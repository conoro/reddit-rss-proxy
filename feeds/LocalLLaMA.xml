<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-06T14:07:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q5eue3</id>
    <title>RTX 6000 Threadripper build drive question</title>
    <updated>2026-01-06T10:16:51+00:00</updated>
    <author>
      <name>/u/Direct_Bodybuilder63</name>
      <uri>https://old.reddit.com/user/Direct_Bodybuilder63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5eue3/rtx_6000_threadripper_build_drive_question/"&gt; &lt;img alt="RTX 6000 Threadripper build drive question" src="https://preview.redd.it/svwnp4vmfpbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e398700113a861a76076a511cd36f501b13cafd4" title="RTX 6000 Threadripper build drive question" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Build:&lt;/p&gt; &lt;p&gt;Motherboard: ASRock WRX90 WS EVO&lt;/p&gt; &lt;p&gt;CPU: Ryzen Threadripper PRO 9985WX&lt;/p&gt; &lt;p&gt;GPU: RTX 6000 MAX-Q x 3&lt;/p&gt; &lt;p&gt;RAM: 768GB (8x96GB) - Vcolor DDR5 6400 TR596G64D452O&lt;/p&gt; &lt;p&gt;Storage: 1. Samsung MZ-V9P2T0B/AM 990 PRO 2TB NVMe Solid State Drive 2. WD_BLACK 8TB SN850X NVMe Gen4 PCIe M.2 2280 WDS800T2XHE 3. Kioxia 30.72TB SSD PSU: Super Flower Leadex Titanium 2800W ATX 3.1 Cooling: Silverstone SST-XE360-TR5 Server AIO Liquid Cooling Case: Phanteks PH-ES620PC_BK02 Enthoo Pro Server Edition&lt;/p&gt; &lt;p&gt;As of this stage I‚Äôve put everything together but I am unsure how to connect the Kioxia SSD. Any help is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Bodybuilder63"&gt; /u/Direct_Bodybuilder63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/svwnp4vmfpbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5eue3/rtx_6000_threadripper_build_drive_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5eue3/rtx_6000_threadripper_build_drive_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T10:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4xfkt</id>
    <title>ROCm running on a ROG Ally X handheld</title>
    <updated>2026-01-05T20:42:13+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4xfkt/rocm_running_on_a_rog_ally_x_handheld/"&gt; &lt;img alt="ROCm running on a ROG Ally X handheld" src="https://external-preview.redd.it/aXhkMmNlcGFlbGJnMd3k2aqwIjNTCgUDqX2GlYiDHPm0ORivpDWQVuZGFBj6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e4704aec14a95a9f6cb17d8d064688683cb637c" title="ROCm running on a ROG Ally X handheld" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We were so busy wondering if we could that we didn‚Äôt think about whether we should&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uqss3psaelbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4xfkt/rocm_running_on_a_rog_ally_x_handheld/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4xfkt/rocm_running_on_a_rog_ally_x_handheld/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T20:42:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4tken</id>
    <title>[Release] EchoChamber - Add AI-Generated Audience Reactions to Your SillyTavern Stories &amp; Conversations</title>
    <updated>2026-01-05T18:23:45+00:00</updated>
    <author>
      <name>/u/mattjb</name>
      <uri>https://old.reddit.com/user/mattjb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've released an extension that generates a dynamic AI-powered reaction feed alongside your SillyTavern conversations and stories. Think of it as adding a live audience to your stories and conversations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; EchoChamber creates real-time AI-generated commentary from virtual audiences as your story unfolds. Whether you want salty Discord chat roasting your plot choices, a viral Twitter feed dissecting every twist, or MST3K-style sarcastic commentary, the extension adapts to match. There are two NSFW avatars (female and male) that react filthily and explicitly, plus a bunch more to choose from (Dumb &amp;amp; Dumber, Thoughtful, HypeBot, Doomscrollers.)&lt;/p&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;10+ Built-in Chat Styles:&lt;/strong&gt; Discord/Twitch chat, Twitter/X threads, Breaking News tickers, Mystery Science Theater 3000, Thoughtful Analysis, Dumb &amp;amp; Dumber, Doomscrollers, HypeBot, and two NSFW advisors (Ava/Kai)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Backend:&lt;/strong&gt; Works with your existing Chat Completion API or runs separately using local models (Ollama, KoboldCPP, LM Studio, vLLM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quick Controls:&lt;/strong&gt; Toggle the feed on/off, switch chat styles, and adjust virtual user count with a convenient bar below your chat&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully Customizable:&lt;/strong&gt; Create your own chat styles by editing Markdown files. Import and share custom styles with the community&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theme Integration:&lt;/strong&gt; Automatically inherits your SillyTavern color scheme&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The extension analyzes your ongoing conversation/story and generates contextual reactions in real-time. The AI responds in character as different audience personas based on the selected chat style, creating an immersive layer of commentary that responds to plot developments, character decisions, and story beats.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; Standard SillyTavern extension process - copy and paste the GitHub URL below in the Extensions panel.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/mattjaybe/SillyTavern-EchoChamber"&gt;https://github.com/mattjaybe/SillyTavern-EchoChamber&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattjb"&gt; /u/mattjb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q4tken"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4tken/release_echochamber_add_aigenerated_audience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4tken/release_echochamber_add_aigenerated_audience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T18:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5g3ye</id>
    <title>Benchmark results for 671B DeepSeek in llama.cpp on 8 x RTX PRO 6000S (layer split mode)</title>
    <updated>2026-01-06T11:28:34+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was run on my modified DeepSeek V3.2 model without lightning indexer tensors, but the performance shall be similar for all 671B DeepSeek models (R1, V3, V3.1, V3.2 with dense attention)&lt;/p&gt; &lt;p&gt;llama.cpp build bd2a93d47 (7643)&lt;/p&gt; &lt;h1&gt;Q4_K_M llama-bench&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;$ ./bin/llama-bench -m /workspace/hf/models--sszymczyk--DeepSeek-V3.2-nolight-GGUF/snapshots/c90cd1a387ba1e3122d4d0f86fe3302ddcf635c8/Q4_K_M/DeepSeek-V3.2-nolight-Q4_K_M-00001-of-00031.gguf -fa 1 -d 0,4096,8192,16384,32768,65536 -p 2048 -n 32 -ub 2048 ... | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 | 1015.31 ¬± 1.87 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 | 40.74 ¬± 0.03 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d4096 | 770.00 ¬± 0.91 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d4096 | 36.41 ¬± 0.06 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d8192 | 625.01 ¬± 1.10 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d8192 | 34.95 ¬± 0.05 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d16384 | 452.01 ¬± 0.83 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d16384 | 32.62 ¬± 0.05 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d32768 | 289.82 ¬± 0.27 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d32768 | 29.50 ¬± 0.03 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d65536 | 168.18 ¬± 0.29 | | deepseek2 671B Q4_K - Medium | 376.71 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d65536 | 24.43 ¬± 0.08 | &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Q4_K_M llama-batched-bench&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;$ ./bin/llama-batched-bench -m /workspace/hf/models--sszymczyk--DeepSeek-V3.2-nolight-GGUF/snapshots/c90cd1a387ba1e3122d4d0f86fe3302ddcf635c8/Q4_K_M/DeepSeek-V3.2-nolight-Q4_K_M-00001-of-00031.gguf -fa 1 -c 150000 -ub 2048 -npp 512,4096,8192 -ntg 32 -npl 1,2,4,8,16 ... | PP | TG | B | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | T s | S t/s | |-------|--------|------|--------|----------|----------|----------|----------|----------|----------| | 512 | 32 | 1 | 544 | 0.864 | 592.30 | 0.829 | 38.60 | 1.693 | 321.23 | | 512 | 32 | 2 | 1088 | 1.143 | 895.77 | 1.798 | 35.60 | 2.941 | 369.92 | | 512 | 32 | 4 | 2176 | 1.788 | 1145.25 | 2.456 | 52.11 | 4.245 | 512.66 | | 512 | 32 | 8 | 4352 | 3.389 | 1208.62 | 3.409 | 75.11 | 6.798 | 640.23 | | 512 | 32 | 16 | 8704 | 6.573 | 1246.26 | 4.539 | 112.80 | 11.112 | 783.27 | | 4096 | 32 | 1 | 4128 | 4.299 | 952.72 | 0.848 | 37.73 | 5.147 | 801.96 | | 4096 | 32 | 2 | 8256 | 8.603 | 952.21 | 1.860 | 34.41 | 10.463 | 789.05 | | 4096 | 32 | 4 | 16512 | 17.167 | 954.39 | 2.563 | 49.93 | 19.730 | 836.88 | | 4096 | 32 | 8 | 33024 | 34.149 | 959.56 | 3.666 | 69.83 | 37.815 | 873.30 | | 4096 | 32 | 16 | 66048 | 68.106 | 962.27 | 5.028 | 101.83 | 73.134 | 903.11 | | 8192 | 32 | 1 | 8224 | 9.739 | 841.13 | 0.883 | 36.24 | 10.622 | 774.22 | | 8192 | 32 | 2 | 16448 | 19.508 | 839.87 | 1.928 | 33.19 | 21.436 | 767.30 | | 8192 | 32 | 4 | 32896 | 39.028 | 839.61 | 2.681 | 47.75 | 41.708 | 788.71 | | 8192 | 32 | 8 | 65792 | 77.945 | 840.80 | 3.916 | 65.37 | 81.860 | 803.71 | | 8192 | 32 | 16 | 131584 | 156.066 | 839.85 | 5.554 | 92.19 | 161.619 | 814.16 | &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Q8_0 llama-bench&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;| model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 | 1026.43 ¬± 0.96 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 | 28.56 ¬± 0.01 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d4096 | 779.80 ¬± 1.98 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d4096 | 26.28 ¬± 0.03 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d8192 | 630.27 ¬± 0.64 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d8192 | 25.51 ¬± 0.02 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d16384 | 453.90 ¬± 0.11 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d16384 | 24.26 ¬± 0.02 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d32768 | 290.33 ¬± 0.14 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d32768 | 22.47 ¬± 0.02 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d65536 | 168.11 ¬± 0.82 | | deepseek2 671B Q8_0 | 664.29 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d65536 | 19.33 ¬± 0.05 | &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Q8_0 llama-batched-bench&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | B | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | T s | S t/s | |-------|--------|------|--------|----------|----------|----------|----------|----------|----------| | 512 | 32 | 1 | 544 | 0.872 | 587.42 | 1.165 | 27.46 | 2.037 | 267.09 | | 512 | 32 | 2 | 1088 | 1.148 | 892.32 | 2.193 | 29.19 | 3.340 | 325.70 | | 512 | 32 | 4 | 2176 | 1.764 | 1160.95 | 2.981 | 42.95 | 4.745 | 458.63 | | 512 | 32 | 8 | 4352 | 3.350 | 1222.52 | 4.225 | 60.60 | 7.575 | 574.51 | | 4096 | 32 | 1 | 4128 | 4.286 | 955.68 | 1.186 | 26.98 | 5.472 | 754.37 | | 4096 | 32 | 2 | 8256 | 8.582 | 954.59 | 2.248 | 28.47 | 10.830 | 762.34 | | 4096 | 32 | 4 | 16512 | 17.107 | 957.74 | 3.105 | 41.22 | 20.212 | 816.94 | | 4096 | 32 | 8 | 33024 | 34.101 | 960.91 | 4.534 | 56.47 | 38.635 | 854.78 | | 8192 | 32 | 1 | 8224 | 9.767 | 838.77 | 1.222 | 26.19 | 10.988 | 748.42 | | 8192 | 32 | 2 | 16448 | 19.483 | 840.93 | 2.322 | 27.56 | 21.806 | 754.30 | | 8192 | 32 | 4 | 32896 | 38.985 | 840.53 | 3.256 | 39.31 | 42.241 | 778.77 | | 8192 | 32 | 8 | 65792 | 77.914 | 841.13 | 4.828 | 53.02 | 82.742 | 795.14 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5g3ye/benchmark_results_for_671b_deepseek_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5g3ye/benchmark_results_for_671b_deepseek_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5g3ye/benchmark_results_for_671b_deepseek_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T11:28:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q586jv</id>
    <title>Backend agnostic llama.cpp support for Kimi-Linear-48B-A3B</title>
    <updated>2026-01-06T03:58:53+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previous experimental support only works with CPU and CUDA. So I implemented a ggml only version such that it can work on all platforms. &lt;/p&gt; &lt;p&gt;You can download the gguf from&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and download the code from&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/ymcki/llama.cpp"&gt;https://github.com/ymcki/llama.cpp&lt;/a&gt; --branch Kimi-Linear&lt;/p&gt; &lt;p&gt;Please feel free to report any bugs you find.&lt;/p&gt; &lt;p&gt;Thanks github's cacaview for his initial version, Aaryan-Kapoor's fixes and pwilkin's qwen3-next implementation to make this possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q586jv/backend_agnostic_llamacpp_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q586jv/backend_agnostic_llamacpp_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q586jv/backend_agnostic_llamacpp_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T03:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q565on</id>
    <title>rtx pro 6000 x4 sandwich stacking thermal test</title>
    <updated>2026-01-06T02:28:03+00:00</updated>
    <author>
      <name>/u/Comfortable-Plate467</name>
      <uri>https://old.reddit.com/user/Comfortable-Plate467</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q565on/rtx_pro_6000_x4_sandwich_stacking_thermal_test/"&gt; &lt;img alt="rtx pro 6000 x4 sandwich stacking thermal test" src="https://b.thumbs.redditmedia.com/BtLgJGuWxykWcHtAUfpjVgILr-qb2wB-2qTdfinvx_c.jpg" title="rtx pro 6000 x4 sandwich stacking thermal test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3bmz27263nbg1.jpg?width=2936&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=daa745c7cf02b038e6fd0781b11291cbe28b6198"&gt;https://preview.redd.it/3bmz27263nbg1.jpg?width=2936&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=daa745c7cf02b038e6fd0781b11291cbe28b6198&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f4gkexwb3nbg1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82a361aad4b83a48b152cabb573fc48288290334"&gt;https://preview.redd.it/f4gkexwb3nbg1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82a361aad4b83a48b152cabb573fc48288290334&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f9nz0ywb3nbg1.png?width=1814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de3afbbf77b40bcba9b8eda366e934feb9f4eb10"&gt;https://preview.redd.it/f9nz0ywb3nbg1.png?width=1814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de3afbbf77b40bcba9b8eda366e934feb9f4eb10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR: Under ~200W for each inference load, the top GPU runs about ~10¬∞C hotter than the bottom GPU. So yeah, fine for inference, but probably not usable for training in the summer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Plate467"&gt; /u/Comfortable-Plate467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q565on/rtx_pro_6000_x4_sandwich_stacking_thermal_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q565on/rtx_pro_6000_x4_sandwich_stacking_thermal_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q565on/rtx_pro_6000_x4_sandwich_stacking_thermal_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T02:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4vz16</id>
    <title>Achieving 30x Real-Time Transcription on CPU . Multilingual STT Openai api endpoint compatible. Plug and play in Open-webui - Parakeet</title>
    <updated>2026-01-05T19:49:08+00:00</updated>
    <author>
      <name>/u/SlightPossibility331</name>
      <uri>https://old.reddit.com/user/SlightPossibility331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been a huge fan of Whisper Large V3 since it came out. it‚Äôs been my reliable workhorse for a long time. But recently, I found a new setup that has completely redefined what I thought was possible for local transcription, especially on a CPU.&lt;/p&gt; &lt;p&gt;I‚Äôm now achieving 30x real-time speeds on an i7-12700KF. To put that in perspective: it processes one minute of audio in just 2 seconds. Even on my older i7-4790, I‚Äôm still seeing a solid 17x real-time factor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes this special?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is powered by &lt;strong&gt;NVIDIA Parakeet TDT 0.6B V3, (in ONNX Format)&lt;/strong&gt; an incredible multilingual model that matches Whisper Large V3 accuracy - and honestly, I‚Äôve found its punctuation to be even better in some cases. It features robust multilingual capabilities with &lt;strong&gt;automatic language detection&lt;/strong&gt;. The model can automatically identify and transcribe speech in any of the &lt;strong&gt;25 supported languages&lt;/strong&gt; without requiring manual language specification:&lt;/p&gt; &lt;p&gt;Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Spanish, Swedish, Ukrainian&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to use it&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve built a frontend to help you capture and transcribe on the fly. However, you can also use the API endpoint to plug this directly into Open-WebUI or any project compatible with the OpenAI API.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/groxaxo/parakeet-tdt-0.6b-v3-fastapi-openai"&gt;&lt;strong&gt;https://github.com/groxaxo/parakeet-tdt-0.6b-v3-fastapi-openai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know what you think and feel free to contribute .I Will keep this project constantly updated so it becomes the new faster-whisper for CPU (Intel)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits &amp;amp; Gratitude&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This project stands on the shoulders of some amazing work:&lt;/p&gt; &lt;p&gt;NVIDIA: For developing the original Parakeet model.&lt;/p&gt; &lt;p&gt;The ONNX team: For the optimization tools that make this speed possible on standard hardware.&lt;/p&gt; &lt;p&gt;Shadowfita: For the excellent original English only FASTAPI Repo that laid the groundwork.&lt;/p&gt; &lt;p&gt;Groxaxo: For his incredible dedication and hard work in pushing this project forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlightPossibility331"&gt; /u/SlightPossibility331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T19:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5bj24</id>
    <title>MedAIBase/AntAngelMed ¬∑ Hugging Face</title>
    <updated>2026-01-06T06:50:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5bj24/medaibaseantangelmed_hugging_face/"&gt; &lt;img alt="MedAIBase/AntAngelMed ¬∑ Hugging Face" src="https://a.thumbs.redditmedia.com/DXGBQTUJAmcQdYQ0FnrMLWQR5y32zBZYDsYdlIz8e-4.jpg" title="MedAIBase/AntAngelMed ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ant Health and others have just open‚Äësourced a medical language model: AntAngelMed.&lt;/p&gt; &lt;p&gt;It‚Äôs based on a Ling‚Äëflash‚Äë2.0 MoE architecture, with 100B total parameters and 6.1B activated parameters. On H20 it achieves inference speeds over 200 tokens/s and supports a 128K context window.&lt;/p&gt; &lt;p&gt;On HealthBench, the open‚Äësource medical evaluation benchmark released by OpenAI, it ranks first among open‚Äësource models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kniszybpeobg1.jpg?width=1120&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28c15613eb7888735cd5e07ae5e7d9efa8249b3b"&gt;https://preview.redd.it/kniszybpeobg1.jpg?width=1120&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28c15613eb7888735cd5e07ae5e7d9efa8249b3b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ssg175lqeobg1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30e74dcd0d97436060426d980c95f0a3a13514f3"&gt;https://preview.redd.it/ssg175lqeobg1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30e74dcd0d97436060426d980c95f0a3a13514f3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ki5525sreobg1.jpg?width=608&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=115f1d3043219a765408bd5727f7f02bb6bb0b3e"&gt;https://preview.redd.it/ki5525sreobg1.jpg?width=608&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=115f1d3043219a765408bd5727f7f02bb6bb0b3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MedAIBase/AntAngelMed"&gt;https://huggingface.co/MedAIBase/AntAngelMed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MedAIBase/AntAngelMed/tree/main"&gt;https://github.com/MedAIBase/AntAngelMed/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MedAIBase/AntAngelMed-FP8"&gt;https://huggingface.co/MedAIBase/AntAngelMed-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5bj24/medaibaseantangelmed_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5bj24/medaibaseantangelmed_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5bj24/medaibaseantangelmed_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T06:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q50g37</id>
    <title>How do we tell them..? :/</title>
    <updated>2026-01-05T22:34:11+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q50g37/how_do_we_tell_them/"&gt; &lt;img alt="How do we tell them..? :/" src="https://preview.redd.it/0mykulscxlbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac9cb2a24636ea77e1a6421eb9c7ac1a58329fe4" title="How do we tell them..? :/" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not funny really, I couldn't think of a better flair...&lt;/p&gt; &lt;p&gt;I have never tried to discuss things where a model would refuse to cooperate, I just woke up one day and thought what GLM (the biggest model I can run locally, using unsloth's IQ2_M) would think of it. I didn't expect it to go this way, I think we all wish it was fiction. How do we break the news to local LLMs? I gave up rephasing the prompt after three tries.&lt;/p&gt; &lt;p&gt;Anyways, 128GB DDR5 paired with an RTX 4060 8GB using an old 0.3.30 LMStudio on Windows 11 to yield the 2.2 ts seen, I am happy with the setup. Will migrate inference to Ubuntu soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0mykulscxlbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q50g37/how_do_we_tell_them/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q50g37/how_do_we_tell_them/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T22:34:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q57txn</id>
    <title>We built an open source memory framework that doesn't rely on embeddings. Just open-sourced it</title>
    <updated>2026-01-06T03:42:23+00:00</updated>
    <author>
      <name>/u/Consistent_Design72</name>
      <uri>https://old.reddit.com/user/Consistent_Design72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, wanted to share something we‚Äôve been hacking on for a while.&lt;/p&gt; &lt;p&gt;It‚Äôs called &lt;strong&gt;memU&lt;/strong&gt; ‚Äî an agentic memory framework for LLMs / AI agents.&lt;/p&gt; &lt;p&gt;Most memory systems I‚Äôve seen rely heavily on embedding search: you store everything as vectors, then do similarity lookup to pull ‚Äúrelevant‚Äù context. That works fine for simple stuff, but it starts breaking down when you care about things like &lt;strong&gt;time&lt;/strong&gt;, &lt;strong&gt;sequences&lt;/strong&gt;, or more complex relationships.&lt;/p&gt; &lt;p&gt;So we tried a different approach. Instead of &lt;em&gt;only&lt;/em&gt; doing embedding search, memU lets the model &lt;strong&gt;read actual memory files directly&lt;/strong&gt;. We call this &lt;em&gt;non-embedding search&lt;/em&gt;. The idea is that LLMs are pretty good at reading structured text already ‚Äî so why not lean into that instead of forcing everything through vector similarity?&lt;/p&gt; &lt;p&gt;High level, the system has three layers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Resource layer&lt;/strong&gt; ‚Äì raw data (text, images, audio, video)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Memory item layer&lt;/strong&gt; ‚Äì extracted fine-grained facts/events&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Memory category layer&lt;/strong&gt; ‚Äì themed memory files the model can read directly&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One thing that‚Äôs been surprisingly useful: the memory structure can &lt;strong&gt;self-evolve&lt;/strong&gt;. Stuff that gets accessed a lot gets promoted, stuff that doesn‚Äôt slowly fades out. No manual pruning, just usage-based reorganization.&lt;/p&gt; &lt;p&gt;It‚Äôs pretty lightweight, all prompts are configurable, and it‚Äôs easy to adapt to different agent setups. Right now it supports text, images, audio, and video.&lt;/p&gt; &lt;p&gt;Open-source repo is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;https://github.com/NevaMind-AI/memU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have a hosted version at &lt;a href="https://app.memu.so"&gt;https://app.memu.so&lt;/a&gt; if you don‚Äôt want to self-host, but the OSS version is fully featured.&lt;/p&gt; &lt;p&gt;Happy to answer questions about how it works, tradeoffs vs embeddings, or anything else. Also very open to feedback ‚Äî we know it‚Äôs not perfect yet üôÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Design72"&gt; /u/Consistent_Design72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q57txn/we_built_an_open_source_memory_framework_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q57txn/we_built_an_open_source_memory_framework_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q57txn/we_built_an_open_source_memory_framework_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T03:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5fhpv</id>
    <title>Training can be possible on 12 GB RAM + 3 GB VRAM.</title>
    <updated>2026-01-06T10:54:32+00:00</updated>
    <author>
      <name>/u/Ok-Type-7663</name>
      <uri>https://old.reddit.com/user/Ok-Type-7663</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes. Training is possible on 12 GB RAM + 3 GB VRAM. I've created a model on a PC with a GTX 1050. IT'S POSSIBLE! But only 0.6B. &lt;a href="https://huggingface.co/Erik22TY/Nebulos-Distill-Qwen3-0.6B"&gt;https://huggingface.co/Erik22TY/Nebulos-Distill-Qwen3-0.6B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Type-7663"&gt; /u/Ok-Type-7663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fhpv/training_can_be_possible_on_12_gb_ram_3_gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fhpv/training_can_be_possible_on_12_gb_ram_3_gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fhpv/training_can_be_possible_on_12_gb_ram_3_gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T10:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q52miw</id>
    <title>I just saw Intel embrace local LLM inference in their CES presentation</title>
    <updated>2026-01-06T00:00:43+00:00</updated>
    <author>
      <name>/u/Mundane-Light6394</name>
      <uri>https://old.reddit.com/user/Mundane-Light6394</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After watching Nvidia show off their massive cloud inference machine while ignoring the existence of local inference I was pleasantly surprised by the message Intel was sending. Intel flipped the script and talked about how local inference in the future because of user privacy, control, model responsiveness and cloud bottlenecks. &lt;/p&gt; &lt;p&gt;I have read countless posts on here about how local inference is dead because Nvidia switched to a cloud first strategy but this might just be temporary because others are apparently thrilled by the idea of building us the hardware we want. And they are leaning into it so who knows what the future brings. Local inference clearly isn't as dead as some want us to believe and it might even become a lot bigger in the near future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane-Light6394"&gt; /u/Mundane-Light6394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q52miw/i_just_saw_intel_embrace_local_llm_inference_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q52miw/i_just_saw_intel_embrace_local_llm_inference_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q52miw/i_just_saw_intel_embrace_local_llm_inference_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T00:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5f1jz</id>
    <title>Liquid AI released LFM2.5 1.2B Instruct</title>
    <updated>2026-01-06T10:28:50+00:00</updated>
    <author>
      <name>/u/KaroYadgar</name>
      <uri>https://old.reddit.com/user/KaroYadgar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/"&gt; &lt;img alt="Liquid AI released LFM2.5 1.2B Instruct" src="https://preview.redd.it/e1qsc3urhpbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55542d5e224e490c112febafbc853ee412d376d2" title="Liquid AI released LFM2.5 1.2B Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, we release LFM2.5, our most capable family of tiny on-device foundation models.&lt;/p&gt; &lt;p&gt;It‚Äôs built to power reliable on-device agentic applications: higher quality, lower latency, and broader modality support in the ~1B parameter class.&lt;/p&gt; &lt;p&gt;&amp;gt; LFM2.5 builds on our LFM2 device-optimized hybrid architecture&lt;br /&gt; &amp;gt; Pretraining scaled from 10T ‚Üí 28T tokens&lt;br /&gt; &amp;gt; Expanded reinforcement learning post-training&lt;br /&gt; &amp;gt; Higher ceilings for instruction following&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaroYadgar"&gt; /u/KaroYadgar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e1qsc3urhpbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T10:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1q502bi</id>
    <title>Rubin uplifts from CES conference going on now</title>
    <updated>2026-01-05T22:19:51+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q502bi/rubin_uplifts_from_ces_conference_going_on_now/"&gt; &lt;img alt="Rubin uplifts from CES conference going on now" src="https://preview.redd.it/zgs8qc8kvlbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=100fa835e15e141c2bf711a1c174360f0ecee07a" title="Rubin uplifts from CES conference going on now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty exciting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zgs8qc8kvlbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q502bi/rubin_uplifts_from_ces_conference_going_on_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q502bi/rubin_uplifts_from_ces_conference_going_on_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T22:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4s8t3</id>
    <title>llama.cpp performance breakthrough for multi-GPU setups</title>
    <updated>2026-01-05T17:37:58+00:00</updated>
    <author>
      <name>/u/Holiday-Injury-9397</name>
      <uri>https://old.reddit.com/user/Holiday-Injury-9397</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/"&gt; &lt;img alt="llama.cpp performance breakthrough for multi-GPU setups" src="https://preview.redd.it/ohxtu0l8hkbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7eb4f66bc390eed56e0c3715fc2510ee8e1fa305" title="llama.cpp performance breakthrough for multi-GPU setups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While we were enjoying our well-deserved end-of-year break, the &lt;strong&gt;ik_llama.cpp&lt;/strong&gt; project (a performance-optimized fork of llama.cpp) achieved a breakthrough in local LLM inference for multi-GPU configurations, delivering a massive performance leap ‚Äî not just a marginal gain, but a 3x to 4x speed improvement.&lt;br /&gt; While it was already possible to use multiple GPUs to run local models, previous methods either only served to pool available VRAM or offered limited performance scaling. However, the ik_llama.cpp team has introduced a new execution mode (split mode graph) that enables the simultaneous and maximum utilization of multiple GPUs.&lt;br /&gt; Why is it so important? With GPU and memory prices at an all-time high, this is a game-changer. We no longer need overpriced high-end enterprise cards; instead, we can harness the collective power of multiple low-cost GPUs in our homelabs, server rooms, or the cloud.&lt;/p&gt; &lt;p&gt;&lt;em&gt;If you are interested, details are&lt;/em&gt; &lt;a href="https://medium.com/@jagusztinl/04c83a66feb2?sk=bad7534bdad1e771a9f61c76c8b0df50"&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday-Injury-9397"&gt; /u/Holiday-Injury-9397 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ohxtu0l8hkbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T17:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5gklh</id>
    <title>So I've been losing my mind over document extraction in insurance for the past few years and I finally figured out what the right approach is.</title>
    <updated>2026-01-06T11:53:43+00:00</updated>
    <author>
      <name>/u/GloomyEquipment2120</name>
      <uri>https://old.reddit.com/user/GloomyEquipment2120</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing document extraction for insurance for a while now and honestly I almost gave up on it completely last year. Spent months fighting with accuracy issues that made no sense until I figured out what I was doing wrong.&lt;/p&gt; &lt;p&gt;everyone's using llms or tools like LlamaParse for extraction and they work fine but then you put them in an actual production env and accuracy just falls off a cliff after a few weeks. I kept thinking I picked the wrong tools or tried to brute force my way through (Like any distinguished engineer would do XD) but it turned out to be way simpler and way more annoying.&lt;/p&gt; &lt;p&gt;So if you ever worked in an information extraction project you already know that most documents have literally zero consistency. I don't mean like &amp;quot;oh the formatting is slightly different&amp;quot; , I mean every single document is structured completely differently than all the others.&lt;/p&gt; &lt;p&gt;For example in my case : a workers comp FROI from California puts the injury date in a specific box at the top. Texas puts it in a table halfway down. New York embeds it in a paragraph. Then you get medical bills where one provider uses line items, another uses narrative format, another has this weird hybrid table thing. And that's before you even get to the faxed-sideways handwritten nightmares that somehow still exist in 2026???&lt;/p&gt; &lt;p&gt;Sadly llms have no concept of document structure. So when you ask about details in a doc it might pull from the right field, or from some random sentence, or just make something up. &lt;/p&gt; &lt;p&gt;After a lot of headaches and honestly almost giving up completely, I came across a process that might save you some pain, so I thought I'd share it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Stop throwing documents at your extraction model blind. Build a classifier that figures out document type first (FROI vs medical bill vs correspondence vs whatever). Then route to type specific extraction. This alone fixed like 60% of my accuracy problems. (Really This is the golden tip ... a lot of people under estimate classification)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Don't just extract and hope. Get confidence scores for each field. &amp;quot;I'm 96% sure this is the injury date, 58% sure on this wage calc&amp;quot; Auto-process anything above 90%, flag the rest. This is how you actually scale without hiring people to validate everything AI does.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Layout matters more than you think. Vision-language models that actually see the document structure perform way better than text only approaches. I switched to Qwen2.5-VL and it was night and day.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fine-tune on your actual documents. Generic models choke on industry-specific stuff. Fine-tuning with LoRA takes like 3 hours now and accuracy jumps 15-20%. Worth it every time.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When a human corrects an extraction, feed that back into training. Your model should get better over time. (This will save you the struggle of having to recreate your process from scratch each time)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Wrote a little blog with more details about this implementation if anyone wants it &amp;quot;I know... Shameless self promotion). ( link in comments) &lt;/p&gt; &lt;p&gt;Anyway this is all the stuff I wish someone had told me when I was starting. Happy to share or just answer questions if you're stuck on this problem. Took me way too long to figure this out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GloomyEquipment2120"&gt; /u/GloomyEquipment2120 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gklh/so_ive_been_losing_my_mind_over_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gklh/so_ive_been_losing_my_mind_over_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gklh/so_ive_been_losing_my_mind_over_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T11:53:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5gii4</id>
    <title>DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available</title>
    <updated>2026-01-06T11:50:35+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/"&gt; &lt;img alt="DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available" src="https://external-preview.redd.it/jIuhV6ttH6YnzKpDkVMMUOo_Jh6zJvPJHB18Vyt60hU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0e98de02fdfc544b4060d48c9cf0b20834c057" title="DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It runs on regular llama.cpp builds (no extra support for DeepSeek V3.2 is needed).&lt;/p&gt; &lt;p&gt;Only Q8_0 and Q4_K_M are available.&lt;/p&gt; &lt;p&gt;Use DeepSeek V3.2 Exp jinja template saved to a file to run this model by passing options: &lt;code&gt;--jinja --chat-template-file ds32-exp.jinja&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here's the template I used in my tests: &lt;a href="https://pastebin.com/4cUXvv35"&gt;https://pastebin.com/4cUXvv35&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note that tool calls will most likely not work with this template - they are different between DS 3.2-Exp and DS 3.2.&lt;/p&gt; &lt;p&gt;I ran &lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; on Q4_K_M quant deployed in llama-server (40 prompts per each difficulty level), results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 | |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:| | 1 | deepseek/deepseek-v3.2 | 0.988 | 1.000 | 1.000 | 1.000 | 0.950 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The model got only 2 answers wrong with most difficult graph size (192). It looks like it performed even a bit better than the original DeepSeek V3.2 with sparse attention tested via API:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 | |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:| | 1 | deepseek/deepseek-v3.2 | 0.956 | 1.000 | 1.000 | 0.975 | 0.850 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;From my testing so far disabling sparse attention does not hurt the model intelligence.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;Edit: &lt;strong&gt;s/lightning attention/lightning indexer/&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/sszymczyk/DeepSeek-V3.2-nolight-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T11:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5cj4e</id>
    <title>LTX-2 Open Sourced</title>
    <updated>2026-01-06T07:51:17+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5cj4e/ltx2_open_sourced/"&gt; &lt;img alt="LTX-2 Open Sourced" src="https://external-preview.redd.it/x6ZxUm50mGHUPTJN69bpngmKLxle6Qp5s56OxgDYluY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01cbac9642a85763962d850066ed2fb90c751b9d" title="LTX-2 Open Sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5cj4e/ltx2_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5cj4e/ltx2_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T07:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4x5e9</id>
    <title>For the first time in 5 years, Nvidia will not announce any new GPUs at CES ‚Äî company quashes RTX 50 Super rumors as AI expected to take center stage</title>
    <updated>2026-01-05T20:31:51+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/"&gt; &lt;img alt="For the first time in 5 years, Nvidia will not announce any new GPUs at CES ‚Äî company quashes RTX 50 Super rumors as AI expected to take center stage" src="https://external-preview.redd.it/co15yfRaj9eX-MR7sOLzYRAR6dcajD3En5Canm81iC0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79072b51c96dcd261d563abdade03dc37cac0511" title="For the first time in 5 years, Nvidia will not announce any new GPUs at CES ‚Äî company quashes RTX 50 Super rumors as AI expected to take center stage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welp, in case anyone had any hopes.&lt;/p&gt; &lt;p&gt;No RTX 50 Super cards, very limited supply of the 5070Ti, 5080, and 5090, and now rumors that Nvidia will bring back the 3060 to prop demand.&lt;/p&gt; &lt;p&gt;Meanwhile &lt;a href="https://www.tomshardware.com/pc-components/ram/newegg-bundles-usd1-460-128gb-ddr5-ram-kit-with-usd50-starbucks-gift-card-drink-coffee-while-you-game-retailer-says-as-memory-hits-rtx-5080-pricing"&gt;DDR5 prices continue to climb, with 128GB kits now costing $1460&lt;/a&gt;. Storage prices have also gone through the roof.&lt;/p&gt; &lt;p&gt;I'm very lucky to have more than enough hardware for all my LLM and homelab needs but at the same time, I don't see any path forward if I want to upgrade in the next 3 years, and hope my gear continues to run without any major issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/for-the-first-time-in-5-years-nvidia-will-not-announce-any-new-gpus-at-ces-company-quashes-rtx-50-super-rumors-as-ai-expected-to-take-center-stage"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T20:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5fs95</id>
    <title>Artificial Analysis just refreshed their global model indices</title>
    <updated>2026-01-06T11:10:20+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fs95/artificial_analysis_just_refreshed_their_global/"&gt; &lt;img alt="Artificial Analysis just refreshed their global model indices" src="https://b.thumbs.redditmedia.com/khnyMcPkgJnHEIEq8WBEM3-umVMrZJXjtfMCDoV1-_c.jpg" title="Artificial Analysis just refreshed their global model indices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The v4.0 mix includes: GDPval-AA, ùúè¬≤-Bench Telecom, Terminal-Bench Hard, SciCode, AA-LCR, AA-Omniscience, IFBench, Humanity's Last Exam, GPQA Diamond, CritPt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;REMOVED&lt;/strong&gt;: MMLU-Pro, AIME 2025, LiveCodeBench, and probably Global-MMLU-Lite.&lt;/p&gt; &lt;p&gt;I did the math on the weights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agents + Terminal Use = ~42%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scientific Reasoning = 25%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Omniscience/Hallucination = 12.5%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; They literally prioritized Terminal-Bench over algorithmic coding (LiveCodeBench is gone, SciCode only).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, the benchmark has shifted to being purely corporate. It doesn't measure &amp;quot;Intelligence&amp;quot; anymore, it measures &amp;quot;How good is this model at being an office clerk?&amp;quot;. If a model isn't fine-tuned to perfectly output JSON for tool calls (like DeepSeek-V3.2-Speciale), it gets destroyed in the rankings even if it's smarter. &lt;/p&gt; &lt;p&gt;They are still updating it, so there may be inaccuracies.&lt;/p&gt; &lt;p&gt;&lt;a href="https://artificialanalysis.ai/?models=gpt-oss-120b%2Cgpt-5-2-non-reasoning%2Cgpt-5-2%2Cgpt-5-1%2Cgpt-oss-20b%2Cllama-4-maverick%2Cgemini-3-pro%2Cgemini-3-flash%2Cgemini-3-flash-reasoning%2Cclaude-opus-4-5%2Cclaude-4-5-sonnet-thinking%2Cclaude-4-5-sonnet%2Cclaude-opus-4-5-thinking%2Cmistral-large-3%2Cdeepseek-r1%2Cdeepseek-v3-2%2Cdeepseek-v3-2-reasoning%2Cgrok-4%2Cgrok-4-1-fast%2Cgrok-4-1-fast-reasoning%2Cnova-2-0-pro-reasoning-medium%2Cnova-2-0-lite-reasoning-medium%2Clfm2-1-2b%2Cminimax-m2-1%2Cnvidia-nemotron-3-nano-30b-a3b-reasoning%2Ckimi-k2-thinking%2Ckimi-k2-0905%2Colmo-3-1-32b-think%2Colmo-3-7b-instruct%2Cmimo-v2-flash-reasoning%2Ckat-coder-pro-v1%2Cmi-dm-k-2-5-pro-dec28%2Cglm-4-5-air%2Cglm-4-6v-reasoning%2Cglm-4-7%2Cglm-4-7-non-reasoning%2Capriel-v1-6-15b-thinker%2Cqwen3-235b-a22b-instruct-2507-reasoning%2Cqwen3-next-80b-a3b-reasoning%2Cqwen3-coder-30b-a3b-instruct%2Cqwen3-235b-a22b-instruct-2507%2Cqwen3-0.6b-instruct%2Cglm-4-6&amp;amp;intelligence-category=reasoning-vs-non-reasoning&amp;amp;media-leaderboards=text-to-video&amp;amp;omniscience=omniscience-index&amp;amp;speed=intelligence-vs-speed#artificial-analysis-intelligence-index#artificial-analysis-intelligence-index"&gt;AA Link with my list models&lt;/a&gt; | &lt;a href="https://artificialanalysis.ai/"&gt;Artificial Analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q5fs95"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fs95/artificial_analysis_just_refreshed_their_global/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fs95/artificial_analysis_just_refreshed_their_global/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T11:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5e010</id>
    <title>Supertonic2: Lightning Fast, On-Device, Multilingual TTS</title>
    <updated>2026-01-06T09:24:47+00:00</updated>
    <author>
      <name>/u/ANLGBOY</name>
      <uri>https://old.reddit.com/user/ANLGBOY</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/"&gt; &lt;img alt="Supertonic2: Lightning Fast, On-Device, Multilingual TTS" src="https://external-preview.redd.it/aTZxcnNkeXU1cGJnMYKGJdezLzYbef1CYRrcNdCGvvmVdrxf390KMohjzSE6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e823aedbe91e1e801fe2a44cca03b41024e1d44" title="Supertonic2: Lightning Fast, On-Device, Multilingual TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I want to share that Supertonic now supports 5 languages:&lt;br /&gt; ÌïúÍµ≠Ïñ¥ ¬∑ Espa√±ol ¬∑ Fran√ßais ¬∑ Portugu√™s ¬∑ English&lt;/p&gt; &lt;p&gt;It‚Äôs an open-weight TTS model designed for extreme speed, minimal footprint, and flexible deployment. You can also use it for commercial use!&lt;/p&gt; &lt;p&gt;Here are key features:&lt;/p&gt; &lt;p&gt;(1) Lightning fast ‚Äî RTF 0.006 on M4 Pro&lt;/p&gt; &lt;p&gt;(2) Lightweight ‚Äî 66M parameters&lt;/p&gt; &lt;p&gt;(3) On-device TTS ‚Äî Complete privacy, zero network latency&lt;/p&gt; &lt;p&gt;(4) Flexible deployment ‚Äî Runs on browsers, PCs, mobiles, and edge devices&lt;/p&gt; &lt;p&gt;(5) 10 preset voices ‚Äî Pick the voice that fits your use cases&lt;/p&gt; &lt;p&gt;(6) Open-weight model ‚Äî Commercial use allowed (&lt;a href="https://huggingface.co/Supertone/supertonic-2/blob/main/LICENSE"&gt;OpenRAIL-M&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I hope Supertonic is useful for your projects.&lt;/p&gt; &lt;p&gt;[Demo] &lt;a href="https://huggingface.co/spaces/Supertone/supertonic-2"&gt;https://huggingface.co/spaces/Supertone/supertonic-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Model] &lt;a href="https://huggingface.co/Supertone/supertonic-2"&gt;https://huggingface.co/Supertone/supertonic-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Code] &lt;a href="https://github.com/supertone-inc/supertonic"&gt;https://github.com/supertone-inc/supertonic&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANLGBOY"&gt; /u/ANLGBOY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k40jciwu5pbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T09:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5a0if</id>
    <title>Liquid Ai released LFM2.5, family of tiny on-device foundation models.</title>
    <updated>2026-01-06T05:27:54+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/"&gt; &lt;img alt="Liquid Ai released LFM2.5, family of tiny on-device foundation models." src="https://preview.redd.it/flk7mfltznbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f4889f71adc7a8ebe91cfeb42042aae8fd240db" title="Liquid Ai released LFM2.5, family of tiny on-device foundation models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/collections/LiquidAI/lfm25"&gt;https://huggingface.co/collections/LiquidAI/lfm25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs built to power reliable on-device agentic applications: higher quality, lower latency, and broader modality support in the ~1B parameter class.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;LFM2.5 builds on LFM2 device-optimized hybrid architecture Pretraining scaled from 10T ‚Üí 28T tokens Expanded reinforcement learning post-training Higher ceilings for instruction following&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;5 open-weight model instances from a single architecture:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;General-purpose instruct model Japanese-optimized chat model Vision-language model Native audio-language model (speech in/out) Base checkpoints for deep customization&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/flk7mfltznbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T05:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5dnyw</id>
    <title>Performance improvements in llama.cpp over time</title>
    <updated>2026-01-06T09:03:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"&gt; &lt;img alt="Performance improvements in llama.cpp over time" src="https://preview.redd.it/lsqwma772pbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5492d32ea47c504ec0399a9c2a02a046df6fc0" title="Performance improvements in llama.cpp over time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lsqwma772pbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T09:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
