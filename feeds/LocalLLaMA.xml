<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-16T14:38:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o8529x</id>
    <title>Qwen3-30B-A3B 2507 Instruct vs Thinking</title>
    <updated>2025-10-16T12:40:23+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got an Oneplus 12 phone with 24GB RAM. Want to run Qwen3-30-A3B on it but I find that there are two versions: Instruct and Thinking&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;According to the charts published in the above links:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Instruct&lt;/th&gt; &lt;th align="left"&gt;Thinking&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA&lt;/td&gt; &lt;td align="left"&gt;70.4&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME25&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;td align="left"&gt;85.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench V6&lt;/td&gt; &lt;td align="left"&gt;43.2&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Arena-Hard V2&lt;/td&gt; &lt;td align="left"&gt;69.0&lt;/td&gt; &lt;td align="left"&gt;56.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BCFLv3&lt;/td&gt; &lt;td align="left"&gt;65.1&lt;/td&gt; &lt;td align="left"&gt;72.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;My understanding is that GPQA is general knowledge, AIME25 is Math, LiveCodeBench is coding, Arena-Hard is a predictor of how well it will perform at LMArena and BCFL is about tool calling.&lt;/p&gt; &lt;p&gt;I want to know which one should I use. Since I am not going to do math, coding and tool calling on my phone and mostly want to query it for knowledge, so maybe Instruct is better for my use case as GPQA is not much different and Arena-Hard might mean Instruct follows instruction better and fewer frustration for me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T12:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7liam</id>
    <title>Llamacpp Model Loader GUI for noobs</title>
    <updated>2025-10-15T20:01:21+00:00</updated>
    <author>
      <name>/u/CabinetNational3461</name>
      <uri>https://old.reddit.com/user/CabinetNational3461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt; &lt;img alt="Llamacpp Model Loader GUI for noobs" src="https://preview.redd.it/msr7wyiwxbvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf3cb84527273f0b3b22fdcb8c887bfed231273" title="Llamacpp Model Loader GUI for noobs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I a noob at this LLM stuff and recently switched from LM Studio/Ollama to llamacpp and loving it so far as far as speed/performance. One thing I dislike is how tedious it is to modify and play around with the parameters and using command line so I vibe coded some python code using Gemini 2.5 Pro for something easier to mess around with. I attached the code, sample model files and commands. I am using window 10 FYI. I had Gemini gen up some doc as am not much of a writer so here it is:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Llama.cpp Model Launcher is a powerful desktop GUI that transforms the complex llama-server.exe command line into an intuitive, point-and-click experience. Effortlessly launch models, dynamically edit every parameter in a visual editor, and manage a complete library of your model configurations. Designed for both beginners and power users, it provides a centralized dashboard to streamline your workflow and unlock the full potential of Llama.cpp without ever touching a terminal.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intuitive Graphical Control:&lt;/strong&gt; Ditch the terminal. Launch, manage, and shut down the llama-server with simple, reliable button clicks, eliminating the risk of command-line typos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Parameter Editor:&lt;/strong&gt; Visually build and modify launch commands in real-time. Adjust values in text fields, toggle flags with checkboxes, and add new parameters on the fly without memorizing syntax.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Configuration Management:&lt;/strong&gt; Build and maintain a complete library of your models. Effortlessly add new profiles, edit names and parameters, and delete old configurations, all from within the application.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-Time Monitoring:&lt;/strong&gt; Instantly know the server's status with a colored indicator (Red, Yellow, Green) and watch the live output log to monitor model loading, API requests, and potential errors as they happen.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrated Documentation:&lt;/strong&gt; Access a complete Llama.cpp command reference and a formatted user guide directly within the interface, eliminating the need to search for external help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Running the Application&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are two primary ways to run this application:&lt;/p&gt; &lt;p&gt;Method 1: Run from Python Source&lt;/p&gt; &lt;p&gt;This method is ideal for developers or users who have Python installed and are comfortable with a code editor.&lt;/p&gt; &lt;p&gt;Method 2: Compile to a Standalone Executable (.exe)&lt;/p&gt; &lt;p&gt;This method packages the application into a single `.exe` file that can be run on any Windows machine without needing Python installed.&lt;/p&gt; &lt;p&gt;code: &lt;a href="https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link"&gt;https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;help_file: &lt;a href="https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link"&gt;https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sample_moldel_commands: &lt;a href="https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link"&gt;https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope someone find it useful&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CabinetNational3461"&gt; /u/CabinetNational3461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/msr7wyiwxbvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x6oi</id>
    <title>SillyTavern for Academic RAG or Alternatives for RAG GUI</title>
    <updated>2025-10-16T04:46:06+00:00</updated>
    <author>
      <name>/u/combrade</name>
      <uri>https://old.reddit.com/user/combrade</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m honestly kinda tempted with SillyTavern’s Lore and World features . It’s kinda like isolating an LLM with an advanced system prompt and persona . I sometimes have an issue with LLMs where they often refuse to report something that is ahead of their knowledge base such as “who is President” even if I give it several articles for RAG with the latest news(just an example not my use case). I feel like it’s Lorebook and World kinda can isolate and refine an LLM output to avoid that . &lt;/p&gt; &lt;p&gt;ST has the most advanced GUI I’ve ever seen with all its neat features like Persona and World . &lt;/p&gt; &lt;p&gt;I’ve been working on this project for my PhD building a RAG vector DB for this research question . I have a MCP tool Vector server running local that’s almost done . The final setup is just a front end so I can give a demo to my department. In the backend , I’ll be using MLflow for reporting the RAG metrics we need .&lt;/p&gt; &lt;p&gt;OpenWebUI is kinda 50-60% there , it was a little annoying setting up the MCP but it works and might require a slightly more powerful Cloud Instance for more users in the future . I’ve been going through SillyTavern’s custom features and it seems really advanced the way you can customize things . &lt;/p&gt; &lt;p&gt;Please be upfront and tell me if this a badshit idea that will have my department head requesting my API logs (Just kidding about this ). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/combrade"&gt; /u/combrade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x6oi/sillytavern_for_academic_rag_or_alternatives_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x6oi/sillytavern_for_academic_rag_or_alternatives_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x6oi/sillytavern_for_academic_rag_or_alternatives_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o85ezo</id>
    <title>My Terminal Project</title>
    <updated>2025-10-16T12:56:19+00:00</updated>
    <author>
      <name>/u/Last-Shake-9874</name>
      <uri>https://old.reddit.com/user/Last-Shake-9874</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o85ezo/my_terminal_project/"&gt; &lt;img alt="My Terminal Project" src="https://external-preview.redd.it/rqjl4n71YeVv91gDGs73XboCWOeNQR52idaZx7k5D5Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e62e735f73e873009d510635bfa123a1120f9e2" title="My Terminal Project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/something-i-made-v0-1rw8yscfygvf1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b6c0fb8b6bf1ae2a431939e7ca2c282304b6a19"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cd03zdrb1hvf1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f40a55f30719f99f3233e9890447c656a7fc261"&gt;https://preview.redd.it/cd03zdrb1hvf1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f40a55f30719f99f3233e9890447c656a7fc261&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So as a developer I wanted a terminal that can catch the errors and exceptions without me having to copy it and ask AI what must I do? So I decided to create one! This is a simple test I created just to showcase it but believe me when it comes to npm debug logs there is always a bunch of text to go through when hitting a error, still in early stages with it but have the basics going already, Connects to 7 different providers (ollama and lm studio included) Can create tabs, use as a terminal so anything you normally do will be there. So what do you guys/girls think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Last-Shake-9874"&gt; /u/Last-Shake-9874 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o85ezo/my_terminal_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o85ezo/my_terminal_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o85ezo/my_terminal_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T12:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8309h</id>
    <title>Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB</title>
    <updated>2025-10-16T10:55:14+00:00</updated>
    <author>
      <name>/u/ThingRexCom</name>
      <uri>https://old.reddit.com/user/ThingRexCom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8309h/looking_for_a_good_agentic_coding_model_that_fits/"&gt; &lt;img alt="Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB" src="https://preview.redd.it/236l5p6ofgvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a063cc9464e783ed553ea182282f276ea5023727" title="Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a huge fan of agentic coding using CLI (i.e., Gemini CLI). I want to create a local setup on Apple M1 Max 32 GB providing similar experience.&lt;/p&gt; &lt;p&gt;Currently, my best setup is Opencode + llama.cpp + gpt-oss-20b.&lt;/p&gt; &lt;p&gt;I have tried other models from HF marked as compatible with my hardware, but most of them failed to start:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable) ggml_metal_synchronize: error: command buffer 0 failed with status 5 error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory) /private/tmp/llama.cpp-20251013-5280-4lte0l/ggml/src/ggml-metal/ggml-metal-context.m:241: fatal error &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any recommendation regarding the LLM and fine-tuning my setup is very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThingRexCom"&gt; /u/ThingRexCom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/236l5p6ofgvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8309h/looking_for_a_good_agentic_coding_model_that_fits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8309h/looking_for_a_good_agentic_coding_model_that_fits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T10:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o85rjo</id>
    <title>Hosting for internal GPT Question</title>
    <updated>2025-10-16T13:11:18+00:00</updated>
    <author>
      <name>/u/lowci</name>
      <uri>https://old.reddit.com/user/lowci</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to host an LLM on-prem for an organization that will serve as an internal GPT. My question is what size of model and hardware would be effective for this? The organization has around 700 employees so I would assume concurrency of around 400 would be sufficient but I would like input as hardware is not my specialty for this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lowci"&gt; /u/lowci &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o85rjo/hosting_for_internal_gpt_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o85rjo/hosting_for_internal_gpt_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o85rjo/hosting_for_internal_gpt_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7z3sn</id>
    <title>Use evaluations to find the best local model for your use case!</title>
    <updated>2025-10-16T06:43:30+00:00</updated>
    <author>
      <name>/u/evalProtocol</name>
      <uri>https://old.reddit.com/user/evalProtocol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"&gt; &lt;img alt="Use evaluations to find the best local model for your use case!" src="https://external-preview.redd.it/5Yyi7FZfBglJXdTAq5ctyvLjdHtxUhbYkAAZztvAOSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555cf9a9171ccbc0dd2a187ee6851a61b8931671" title="Use evaluations to find the best local model for your use case!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I am Benny, I have been working on &lt;a href="http://evalprotocol.io"&gt;evalprotocol.io&lt;/a&gt; for a while now, and we recently published a post on using evaluations to pick the best local model to get your job done &lt;a href="https://fireworks.ai/blog/llm-judge-eval-protocol-ollama"&gt;https://fireworks.ai/blog/llm-judge-eval-protocol-ollama&lt;/a&gt; . The SDK is here &lt;a href="https://github.com/eval-protocol/python-sdk"&gt;https://github.com/eval-protocol/python-sdk&lt;/a&gt; , totally open source, and would love to figure out how to best work together with everyone. Please give it a try and let me know if you have any feedback!&lt;/p&gt; &lt;p&gt;(btw not familiar with the self promotion rule here, the SDK is totally open source, if this is not ok feel free to delete the post)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x5fupedf6fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3087b9d6f9c43b534cb38ac8f513e5f66b4ea005"&gt;https://preview.redd.it/x5fupedf6fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3087b9d6f9c43b534cb38ac8f513e5f66b4ea005&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evalProtocol"&gt; /u/evalProtocol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T06:43:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x25o</id>
    <title>Ollama v0.12.6 finally includes Vulkan support</title>
    <updated>2025-10-16T04:39:06+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"&gt; &lt;img alt="Ollama v0.12.6 finally includes Vulkan support" src="https://external-preview.redd.it/cjCC50drSVvsSjC6BG0LlHPfYq9pihhGvZz2PN90ZFQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c878fa25eb39194203fe7a493bd33e19b95d026c" title="Ollama v0.12.6 finally includes Vulkan support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.12.6-rc0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7miyx</id>
    <title>Just ordered new 3090 TI from MicroCenter 🤔</title>
    <updated>2025-10-15T20:39:56+00:00</updated>
    <author>
      <name>/u/GravyPoo</name>
      <uri>https://old.reddit.com/user/GravyPoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt; &lt;img alt="Just ordered new 3090 TI from MicroCenter 🤔" src="https://preview.redd.it/mzozs3957cvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb5a83e22f624acd437f0414ec334d5a460f063d" title="Just ordered new 3090 TI from MicroCenter 🤔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GravyPoo"&gt; /u/GravyPoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mzozs3957cvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7ep8a</id>
    <title>Apple M5 Officially Announced: is this a big deal?</title>
    <updated>2025-10-15T15:48:34+00:00</updated>
    <author>
      <name>/u/ontorealist</name>
      <uri>https://old.reddit.com/user/ontorealist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;em&gt;Edit: To be clear, only the *&lt;/em&gt;base** M5 has been announced. My question is primarily about whether M5 &lt;strong&gt;Pro&lt;/strong&gt; and higher-end M5 chips with more high bandwidth memory, etc. are more compelling compared to PC builds for inference given the confirmed specs for the base M5.*)&lt;/p&gt; &lt;p&gt;If I’m understanding correctly:&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;3.5x faster AI performance&lt;/strong&gt; compared to the M4 (though the exact neural engine improvements aren’t yet confirmed)&lt;br /&gt; • &lt;strong&gt;153 GB/s memory bandwidth&lt;/strong&gt; (~30% improvement)&lt;br /&gt; • &lt;strong&gt;4x increase in GPU compute&lt;/strong&gt;&lt;br /&gt; • &lt;strong&gt;Unified memory architecture&lt;/strong&gt;, eliminating the need for CPU↔GPU data transfers, as with previous gens&lt;/p&gt; &lt;p&gt;Even if the neural accelerators on the base M5 aren’t dedicated matmul units (which seems unlikely given the A19 Pro), will this translate into noticeably faster prompt processing speeds?&lt;/p&gt; &lt;p&gt;At $1,600 for an entry-level 16GB M5 ($2K for 32GB), serious inference workloads feels limiting, especially when compared to refurbished M-series models with more RAM. That said, it seems like a solid choice for new users exploring local AI experiences, particularly when working with sub-30B models for RAG or large context windows at faster speeds. That, along with another LM Studio feature in the press release, is a good sign, no? &lt;/p&gt; &lt;p&gt;Do the specs / pricing represent a meaningful upgrade for anyone considering the M5 Pro, Max, or Ultra? I’d love to hear others’ thoughts.&lt;/p&gt; &lt;p&gt;Read the announcement &lt;a href="https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontorealist"&gt; /u/ontorealist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T15:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe3g</id>
    <title>Matthew McConaughey LLaMa</title>
    <updated>2025-10-15T22:34:38+00:00</updated>
    <author>
      <name>/u/ContextualNina</name>
      <uri>https://old.reddit.com/user/ContextualNina</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We thought it would be fun to build something for Matthew McConaughey, based on his recent Rogan podcast interview.&lt;/p&gt; &lt;p&gt;&amp;quot;Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations, so he can ask it questions and get answers based solely on that information, without any outside influence.&amp;quot;&lt;/p&gt; &lt;p&gt;Pretty classic RAG/context engineering challenge, right? And we use a fine-tuned Llama model in this setup, which also happens to be the most factual and grounded LLM according to the FACTS benchmark (link in comment), Llama-3-Glm-V2. &lt;/p&gt; &lt;p&gt;Here's how we built it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;We found public writings, podcast transcripts, etc, as our base materials to upload as a proxy for the all the information Matthew mentioned in his interview (of course our access to such documents is very limited compared to his).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent ingested those to use as a source of truth&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;We configured the agent to the specifications that Matthew asked for in his interview. Note that we already have the most grounded language model (GLM) as the generator, and multiple guardrails against hallucinations, but additional response qualities can be configured via prompt.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Now, when you converse with the agent, it knows to only pull from those sources instead of making things up or use its other training data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;However, the model retains its overall knowledge of how the world works, and can reason about the responses, in addition to referencing uploaded information verbatim.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent is powered by Contextual AI's APIs, and we deployed the full web application on Vercel to create a publicly accessible demo.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ContextualNina"&gt; /u/ContextualNina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.alrightalrightalright.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o86h5j</id>
    <title>What MoE model sizes and capabilities are currently missing in the open weight ecosystem?</title>
    <updated>2025-10-16T13:41:40+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who trains models, I’d love to know if you have specific requests for model size or capabilities you’d like to see in a (fully) open MoE model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7rchv</id>
    <title>LLama.cpp GPU Support on Android Device</title>
    <updated>2025-10-15T23:56:47+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt; &lt;img alt="LLama.cpp GPU Support on Android Device" src="https://a.thumbs.redditmedia.com/VeG6UZmL3mBW6TmARr_WVpKD3xQ0T0XIPkiAj730lQ8.jpg" title="LLama.cpp GPU Support on Android Device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have figured out a way to Use Android - GPU for LLAMA.CPP&lt;br /&gt; I mean it is not what you would expect like boost in tk/s but it is good for background work mostly&lt;/p&gt; &lt;p&gt;and i didn't saw much of a difference in both GPU and CPU mode&lt;/p&gt; &lt;p&gt;i was using &lt;a href="https://huggingface.co/Menlo/Lucy-128k-gguf/tree/main"&gt;lucy-128k&lt;/a&gt; model, i mean i am also using k-v cache + state file saving so yaa that's all that i got&lt;br /&gt; love to hear more about it from you guys : )&lt;/p&gt; &lt;p&gt;here is the relevant post : &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7rchv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T23:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7mhf5</id>
    <title>Google &amp; Yale release C2S Scale, a Gemma-based model for cell analysis</title>
    <updated>2025-10-15T20:38:17+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is Omar, from the Gemma team.&lt;/p&gt; &lt;p&gt;I'm super excited to share this research based on Gemma. Today, we're releasing a 27B model for single-cell analysis. This model generated hypotheses about how cancer cells behave, and we were able to confirm the predictions with experimental validation in living cells. This reveals a promising new pathway for developing therapies to fight cancer. &lt;/p&gt; &lt;p&gt;This applications of open models for medical use cases are super exciting for me. It's one of many examples of how open models can change the world&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmers… totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmers… totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmers… totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o83b2i</id>
    <title>Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell</title>
    <updated>2025-10-16T11:11:58+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt; &lt;img alt="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" src="https://b.thumbs.redditmedia.com/YJxZs33mPxgOa7fkc4zIPMKaCT3QAzLt14Sr270bH7A.jpg" title="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: NVIDIA RTX Pro 6000 Blackwell Edition (96GB VRAM)&lt;/p&gt; &lt;p&gt;- Driver: 580.95.05&lt;/p&gt; &lt;p&gt;- CUDA: 13.0&lt;/p&gt; &lt;p&gt;- Compute Capability: 9.0 (Blackwell)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782"&gt;https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;p&gt;- vLLM: v0.11.1rc2.dev72+gf7d318de2 (nightly)&lt;/p&gt; &lt;p&gt;- Attention Backend: **FlashInfer** (with JIT autotuning)&lt;/p&gt; &lt;p&gt;- Quantization: FP8 W8A8&lt;/p&gt; &lt;p&gt;- Python: 3.12.12&lt;/p&gt; &lt;p&gt;- PyTorch with CUDA 12.4 backend (forward compatible with CUDA 13.0 driver)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T11:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o808av</id>
    <title>I fine-tuned Qwen3-VL (4B &amp; 8B) on a free Colab instance using TRL (SFT and GRPO)!</title>
    <updated>2025-10-16T07:58:52+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created a couple of notebook that work for free on Colab (T4 GPU) to fine-tune the new Qwen3-VL small and dense vision-language models (4B and 8B). Both the Instruct and Thinking variants are supported.&lt;/p&gt; &lt;p&gt;They use &lt;strong&gt;TRL&lt;/strong&gt;, which handles most of the training complexity so you can focus entirely on the specific task you want to fine-tune for.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SFT&lt;/strong&gt; notebook: fine-tunes with a dataset to refine the model's response style: &lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GRPO&lt;/strong&gt; notebook: includes two reward functions to make the non-reasoning model learn to reason (&lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb&lt;/a&gt;): &lt;ol&gt; &lt;li&gt;A tag-based reward that checks for &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;answer&amp;gt;&lt;/code&gt; sections.&lt;/li&gt; &lt;li&gt;A length-based reward that discourages overthinking and checks correctness.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both notebooks can be run on a free Colab instance, but can also be scaled up for more advanced setups. The notebooks can also be accessed here: &lt;a href="https://github.com/huggingface/trl/tree/main/examples/notebooks"&gt;https://github.com/huggingface/trl/tree/main/examples/notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback and experiments are welcome!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T07:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b5i4</id>
    <title>Apple unveils M5</title>
    <updated>2025-10-15T13:34:26+00:00</updated>
    <author>
      <name>/u/Agreeable-Rest9162</name>
      <uri>https://old.reddit.com/user/Agreeable-Rest9162</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt; &lt;img alt="Apple unveils M5" src="https://preview.redd.it/5ehnojlm2avf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbc46c6e19f88c18588d2f5384d7fb2dd4717f50" title="Apple unveils M5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the iPhone 17 AI accelerators, most of us were expecting the same tech to be added to M5. Here it is! Lets see what M5 Pro &amp;amp; Max will add. The speedup from M4 to M5 seems to be around 3.5x for prompt processing. &lt;/p&gt; &lt;p&gt;Faster SSDs &amp;amp; RAM:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Additionally, with up to 2x faster SSD performance than the prior generation, the new 14-inch MacBook Pro lets users load a local LLM faster, and they can now choose up to 4TB of storage. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;150GB/s of unified memory bandwidth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Rest9162"&gt; /u/Agreeable-Rest9162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ehnojlm2avf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x7ss</id>
    <title>GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm</title>
    <updated>2025-10-16T04:47:49+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt; &lt;img alt="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" src="https://b.thumbs.redditmedia.com/vckUDC6hgDdCUoZhjQ0jn7hdsNydHHYam8wvadpbBSo.jpg" title="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517"&gt;https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran benchmark of cpatonn/GLM-4.5-Air-AWQ-4bit on a single Pro 6000 with vllm. Nvidia Driver Version: 580.95.05 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7gpr8</id>
    <title>Got the DGX Spark - ask me anything</title>
    <updated>2025-10-15T17:02:50+00:00</updated>
    <author>
      <name>/u/sotech117</name>
      <uri>https://old.reddit.com/user/sotech117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt; &lt;img alt="Got the DGX Spark - ask me anything" src="https://preview.redd.it/9mr835ne4bvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42dc8e85dcff8b55d4174e98495bb8d2d144fd7d" title="Got the DGX Spark - ask me anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there’s anything you want me to benchmark (or want to see in general), let me know, and I’ll try to reply to your comment. I will be playing with this all night trying a ton of different models I’ve always wanted to run. &lt;/p&gt; &lt;p&gt;(&amp;amp; shoutout to microcenter my goats!)&lt;/p&gt; &lt;p&gt;edit: Downloading and setting up a ton of requested programs models currently. Should have some llm and image/video gen numbers tonight.&lt;br /&gt; Gonna do more of the &amp;quot;exotic&amp;quot; requests tomorrow. Over the weekend I'll try to reply to everyone I missed and do whatever I forgot to do!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotech117"&gt; /u/sotech117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mr835ne4bvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o866vl</id>
    <title>PaddleOCR-VL, is better than private models</title>
    <updated>2025-10-16T13:29:48+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt; &lt;img alt="PaddleOCR-VL, is better than private models" src="https://b.thumbs.redditmedia.com/X4U2z8D2mUefEIEuBe11hVIGCJrPT-oF1EOiA8c0dyw.jpg" title="PaddleOCR-VL, is better than private models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19"&gt;https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o866vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o82kta</id>
    <title>NVIDIA DGX Spark – A Non-Sponsored Review (Strix Halo Comparison, Pros &amp; Cons)</title>
    <updated>2025-10-16T10:30:22+00:00</updated>
    <author>
      <name>/u/Corylus-Core</name>
      <uri>https://old.reddit.com/user/Corylus-Core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;NVIDIA DGX Spark – A Non-Sponsored Review (Strix Halo Comparison, Pros &amp;amp; Cons)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Pww8rIzr1pg"&gt;https://www.youtube.com/watch?v=Pww8rIzr1pg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corylus-Core"&gt; /u/Corylus-Core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T10:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe1u</id>
    <title>gigaResearch</title>
    <updated>2025-10-15T22:34:35+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt; &lt;img alt="gigaResearch" src="https://preview.redd.it/nb2hmgqircvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71c101f2683e8df117cbc2a9abd685bcac5cbce0" title="gigaResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nb2hmgqircvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o84b36</id>
    <title>Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm</title>
    <updated>2025-10-16T12:04:11+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt; &lt;img alt="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" src="https://b.thumbs.redditmedia.com/tevD9Ijs-RE6cQcddGazJZY--0Lzdu42Yn4uKrDPRjI.jpg" title="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Power limit set to 450w &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Context (1K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 88.4 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;652 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 5.65s → 7.65s (1→10 users)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Long Context (256K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 22.0 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;115.5 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 22.7s → 43.2s (1→10 users)&lt;/li&gt; &lt;li&gt;Still able to handle 10 concurrent requests!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Sweet Spot (32K-64K context):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;64K @ 10 users: 311 tok/s total, 31 tok/s per user&lt;/li&gt; &lt;li&gt;32K @ 10 users: 413 tok/s total, 41 tok/s per user&lt;/li&gt; &lt;li&gt;Best balance of context length and throughput&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FP8 quantization really shines here - getting 115 tok/s aggregate at 256K context with 10 users is wild, even with the power constraint. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836"&gt;https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T12:04:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81rvs</id>
    <title>Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub</title>
    <updated>2025-10-16T09:41:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt; &lt;img alt="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" src="https://b.thumbs.redditmedia.com/h0BE1gNO8S-6xv6b1X5IIoHb8CSHWZoS7YxS0LFbuxA.jpg" title="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: How a Gemma model helped discover a new potential cancer therapy pathway - We’re launching a new 27 billion parameter foundation model for single-cell analysis built on the Gemma family of open models.: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt;&lt;br /&gt; Scientific preprint on bioRxiv: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;br /&gt; Code on GitHub: &lt;a href="https://github.com/vandijklab/cell2sentence"&gt;https://github.com/vandijklab/cell2sentence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o81rvs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T09:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
