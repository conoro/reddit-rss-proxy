<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-02T15:48:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1omjqhy</id>
    <title>Has anyone been able to run LLMs on the new Intel NPUs?</title>
    <updated>2025-11-02T15:10:35+00:00</updated>
    <author>
      <name>/u/Triq1</name>
      <uri>https://old.reddit.com/user/Triq1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking at the new Intel CPUs, particularly the laptop ones. They advertise '40+ TOPS' (Core Ultra 7 285V) and I was wondering if anyone has had any success with these for on-device LLM, in particular for coding tasks. I'm looking at 7-22B models mostly, but I'm not up to date with just how big decent models are these days.&lt;/p&gt; &lt;p&gt;I've seen some stuff about IPEX-LLM, but it seems to be relatively uncommon and it's not clear whether the NPU is actually faster than the iGPU. I'd appreciate some experience from people who've actually tried and used it.&lt;/p&gt; &lt;p&gt;I'm new to this space so it's possible I've missed a clear information source, go easy on me üòõ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Triq1"&gt; /u/Triq1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjqhy/has_anyone_been_able_to_run_llms_on_the_new_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjqhy/has_anyone_been_able_to_run_llms_on_the_new_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omjqhy/has_anyone_been_able_to_run_llms_on_the_new_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:10:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1omk31z</id>
    <title>LLM Codebase to Impacted features</title>
    <updated>2025-11-02T15:23:57+00:00</updated>
    <author>
      <name>/u/Yeasappaa</name>
      <uri>https://old.reddit.com/user/Yeasappaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, first time building a Gen AI system here...&lt;/p&gt; &lt;p&gt;I'm trying to make a &amp;quot;Code to Impacted Feature mapper&amp;quot; using LLM reasoning..&lt;/p&gt; &lt;p&gt;Can I build a Knowledge Graph or RAG for my microservice codebase that's tied to my features...&lt;/p&gt; &lt;p&gt;What I'm really trying to do is, I'll have a Feature.json like this: name: Feature_stats_manager, component: stats, description: system stats collector&lt;/p&gt; &lt;p&gt;This mapper file will go in with the codebase to make a graph...&lt;/p&gt; &lt;p&gt;When new commits happen, the graph should update, and I should see the Impacted Feature for the code in my commit..&lt;/p&gt; &lt;p&gt;I'm totally lost on how to build this Knowledge Graph with semantic understanding...&lt;/p&gt; &lt;p&gt;Is my whole approach even right??&lt;/p&gt; &lt;p&gt;Would love some ideas..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yeasappaa"&gt; /u/Yeasappaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk31z/llm_codebase_to_impacted_features/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk31z/llm_codebase_to_impacted_features/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omk31z/llm_codebase_to_impacted_features/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:23:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1omk68k</id>
    <title>Local llm on NPU</title>
    <updated>2025-11-02T15:27:21+00:00</updated>
    <author>
      <name>/u/Cokodayo</name>
      <uri>https://old.reddit.com/user/Cokodayo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got a pretty decent laptop (zenbook s13) with an Intel core ultra 7 155U processor. it has an NPU built in, but I have been unable to get it working on my arch Linux setup. They do have official drivers for Ubuntu and I can get the NPU driver from aur, but I have had no luck getting them working. Has anyone got a similar setup or have used the NPU to run small models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cokodayo"&gt; /u/Cokodayo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk68k/local_llm_on_npu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk68k/local_llm_on_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omk68k/local_llm_on_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1om26g2</id>
    <title>Why don‚Äôt more apps run AI locally?</title>
    <updated>2025-11-01T23:21:27+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been seeing more talk about running small LLMs locally on phones.&lt;/p&gt; &lt;p&gt;Almost every new phone ships with dedicated AI hardware (NPU,GPU, etc). Still, very few apps seem to use them to run models on-device.&lt;/p&gt; &lt;p&gt;What‚Äôs holding local inference back on mobile in your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T23:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1olqjxj</id>
    <title>Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B</title>
    <updated>2025-11-01T15:23:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt; &lt;img alt="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" src="https://external-preview.redd.it/_7BYCEiuSe8H_fldVM7chLfCb5j0ciz_pk_F5HpmBuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9f337f24ae40e158287e6812e9408e53add8ae" title="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1om03mi</id>
    <title>AMD EPYC 4565P is a beast</title>
    <updated>2025-11-01T21:50:40+00:00</updated>
    <author>
      <name>/u/coding9</name>
      <uri>https://old.reddit.com/user/coding9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven‚Äôt seen too much coverage on these CPUs but I got a system with it. I can get over 15t/s on gpt-oss 20b with cpu only on 5600mhz ecc ram. &lt;/p&gt; &lt;p&gt;Pretty surprised it‚Äôs this good with the avx 512 instruction set. &lt;/p&gt; &lt;p&gt;Anyone else using these or have any thoughts?&lt;/p&gt; &lt;p&gt;Edit: this wasn‚Äôt purchased for inference so I‚Äôm just excited it can do some basic stuff with it as well&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding9"&gt; /u/coding9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T21:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oma5ws</id>
    <title>OCR models: HF demos vs local performance</title>
    <updated>2025-11-02T06:20:38+00:00</updated>
    <author>
      <name>/u/SubstantialSock8002</name>
      <uri>https://old.reddit.com/user/SubstantialSock8002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt; &lt;img alt="OCR models: HF demos vs local performance" src="https://b.thumbs.redditmedia.com/mIhQZi1GFRBjVDMNiNavyg3R-VaEfLzvZJo7nl-gqMc.jpg" title="OCR models: HF demos vs local performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last few days, I've been testing every OCR model under the sun to compare performance. I'd get amazing results on the HuggingFace Space demos, but when running locally, the models would hallucinate or output garbage.&lt;/p&gt; &lt;p&gt;The latest model I tried running locally was MinerU 2.5, and it had the same issue, even with the exact gradio demo provided in the repo as the hosted version. However, I then switched from the default pipeline backend to vlm-transformers, and it performed as well as the hosted version.&lt;/p&gt; &lt;p&gt;Has anyone else experienced similar issues? I haven't found a fix for others, but so far I've tried docling granite, deepseek ocr, paddleocr vl, and olmocr, with the same common theme: hosted works, local fails.&lt;/p&gt; &lt;p&gt;Here's an example image I used, along with the outputs for MinerU with both backends.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1e78yfvkdsyf1.jpg?width=370&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80636d3d1b6b063014c7e14cd6f748247c8edc9"&gt;https://preview.redd.it/1e78yfvkdsyf1.jpg?width=370&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80636d3d1b6b063014c7e14cd6f748247c8edc9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pipeline output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;# The Daily&lt;/p&gt; &lt;p&gt;# Martians invade earth&lt;/p&gt; &lt;p&gt;Incredible as it may seem, headed towards the North Ren it has been confimed that Pole and Santa Claus was foll a lat ge martian invasion taken hostage by the imp tonight. invaders.&lt;/p&gt; &lt;p&gt;Afterwards they split apart First vessels were sighted in order to approach most over Great Britain, major cities around the Denmark and Norway earth. The streets filled as already in the late evening thousands fled their from where, as further homes, many only wearing reports indicate, the fleet their pajamas...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vlm-transformers output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;# The Daily&lt;/p&gt; &lt;p&gt;Sunday, August 30, 2006&lt;/p&gt; &lt;p&gt;# Martians invade earth&lt;/p&gt; &lt;p&gt;Incredible as it may seem, it has been confirmed that a large martian invasion fleet has landed on earth tonight.&lt;/p&gt; &lt;p&gt;First vessels were sighted over Great Britain, Denmark and Norway already in the late evening from where, as further reports indicate, the fleet&lt;/p&gt; &lt;p&gt;headed towards the North Pole and Santa Claus was taken hostage by the invaders.&lt;/p&gt; &lt;p&gt;Afterwards they split apart in order to approach most major cities around the earth. The streets filled as thousands fled their homes, many only wearing their pajamas...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialSock8002"&gt; /u/SubstantialSock8002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T06:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1omi7v3</id>
    <title>[Question] Best open-source coder LLM (local) that can plan &amp; build a repo from scratch?</title>
    <updated>2025-11-02T14:09:51+00:00</updated>
    <author>
      <name>/u/Admirable-Crow-1480</name>
      <uri>https://old.reddit.com/user/Admirable-Crow-1480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all ‚Äî I‚Äôm looking for recommendations for an &lt;strong&gt;open-source, fully local&lt;/strong&gt; coder LLM that can &lt;strong&gt;plan, scaffold, and iteratively build a brand-new repository from scratch&lt;/strong&gt; (not just single-file edits).&lt;/p&gt; &lt;h1&gt;What ‚Äúbuild from scratch‚Äù means to me&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Propose an initial architecture (folders/modules), then create the files&lt;/li&gt; &lt;li&gt;Implement a working MVP (e.g., API + basic frontend or CLI) and iterate&lt;/li&gt; &lt;li&gt;Add tests, a basic CI workflow, and a &lt;code&gt;README&lt;/code&gt; with run instructions&lt;/li&gt; &lt;li&gt;Produce &lt;strong&gt;small, targeted diffs&lt;/strong&gt; for revisions (or explain file-by-file changes)&lt;/li&gt; &lt;li&gt;Handle multi-step tasks without losing context across many files&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Nice-to-haves&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Long context support (so it can reason over many files)&lt;/li&gt; &lt;li&gt;Solid TypeScript/Python skills (but language-agnostic is fine)&lt;/li&gt; &lt;li&gt;Works well with agent tooling (e.g., editor integrations), but I‚Äôm fine running via CLI/server if that‚Äôs better&lt;/li&gt; &lt;li&gt;Support for common quant formats (GGUF/AWQ/GPTQ) and mainstream runtimes (vLLM, TGI, llama.cpp/Ollama, ExLlamaV2)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hard requirements&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open-source license&lt;/strong&gt; (no cloud reliance)&lt;/li&gt; &lt;li&gt;Runs &lt;strong&gt;locally&lt;/strong&gt; on my box (see specs below)&lt;/li&gt; &lt;li&gt;Good at &lt;strong&gt;planning+execution&lt;/strong&gt;, not just autocompleting single files&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My PC specs (high level)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Gigabyte (NVIDIA)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: ASUS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Samsung&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power Supply&lt;/strong&gt;: MSI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Case&lt;/strong&gt;: Fractal Design&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: Kingston&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;: Thermaltake&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accessory&lt;/strong&gt;: SanDisk&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: Micro Center&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;What I‚Äôm hoping you can share&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Model + &lt;strong&gt;quant&lt;/strong&gt; you recommend (e.g., ‚ÄúQwen-coder X-B AWQ 4-bit‚Äù or ‚ÄúDeepSeek-Coder-V2 16-bit on vLLM‚Äù)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runtime&lt;/strong&gt; you use (Ollama / llama.cpp / vLLM / TGI / ExLlamaV2) + any key flags&lt;/li&gt; &lt;li&gt;Typical &lt;strong&gt;context window&lt;/strong&gt; and what project size it comfortably handles&lt;/li&gt; &lt;li&gt;Any &lt;strong&gt;prompt patterns&lt;/strong&gt; or workflows that helped you get full repo scaffolding working (bonus: examples or repos)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Want a &lt;strong&gt;local, open-source coder LLM&lt;/strong&gt; that can &lt;strong&gt;plan + scaffold + implement a repo from zero&lt;/strong&gt; with solid multi-file reasoning. Please share your model/quant/runtime combos and tips. Thanks! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Crow-1480"&gt; /u/Admirable-Crow-1480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omi7v3/question_best_opensource_coder_llm_local_that_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omi7v3/question_best_opensource_coder_llm_local_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omi7v3/question_best_opensource_coder_llm_local_that_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T14:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1olmj9z</id>
    <title>Bought MI50 32 Gb from Alibaba. Did I get scammed?</title>
    <updated>2025-11-01T12:26:47+00:00</updated>
    <author>
      <name>/u/Moist_Toto</name>
      <uri>https://old.reddit.com/user/Moist_Toto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt; &lt;img alt="Bought MI50 32 Gb from Alibaba. Did I get scammed?" src="https://preview.redd.it/v3w8clon2nyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ef938653e71635a81d9e3e2eaf625cfbf73033e" title="Bought MI50 32 Gb from Alibaba. Did I get scammed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I bought 8 MI50 32Gb units from someone on Alibaba.&lt;/p&gt; &lt;p&gt;After spending some time to figure out Linux and the software stack, I entered the 'amd-smi static' command in the terminal.&lt;/p&gt; &lt;p&gt;The result is quite frightening, here it is: &lt;/p&gt; &lt;p&gt;especially the bottom part product name saying &amp;quot;16GB&amp;quot;, my heart skipped a beat. Is this something driver related or am I screwed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist_Toto"&gt; /u/Moist_Toto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v3w8clon2nyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T12:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1omjgzg</id>
    <title>Can I run open source local LLM trained on specific dataset ?</title>
    <updated>2025-11-02T15:00:20+00:00</updated>
    <author>
      <name>/u/hugo_mdn</name>
      <uri>https://old.reddit.com/user/hugo_mdn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there!&lt;/p&gt; &lt;p&gt;I'm quite new to local LLM, so maybe this question will look dumb to you.&lt;/p&gt; &lt;p&gt;I don't like how ChatGPT is going because it's trained on the whole internet, and it's less and less precise. When I'm looking for very particular information in programming, culture, or anything else, it's not accurate, or using the good sources. And also, I'm not really a fan of privacy terms of OpenAI and other online models.&lt;/p&gt; &lt;p&gt;So my question is, could I run LLM locally (yes), and use a very specific dataset of trusted sources, like Wikipedia, books, very specific health and science websites, programming websites, etc..? And if yes, are there any excellent datasets available? Because I don't really want to add millions of websites and sources one by one.&lt;/p&gt; &lt;p&gt;Thanks in advance for your time and have a nice day :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hugo_mdn"&gt; /u/hugo_mdn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1olouiw</id>
    <title>TIL: For long-lived LLM sessions, swapping KV Cache to RAM is ~10x faster than recalculating it. Why isn't this a standard feature?</title>
    <updated>2025-11-01T14:12:57+00:00</updated>
    <author>
      <name>/u/Shoddy-Tutor9563</name>
      <uri>https://old.reddit.com/user/Shoddy-Tutor9563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was diving into how vLLM and similar inference servers work and had a thought about optimizing memory for long-lived but inactive chat sessions. The standard approach seems to be either keeping the KV Cache in precious VRAM or evicting it and recalculating from scratch when the user returns. I think there might be a better way.&lt;/p&gt; &lt;p&gt;Here's the core idea: Implement a swapping mechanism for the KV Cache of inactive sessions, moving it from VRAM to system RAM (and back), instead of deleting it.&lt;/p&gt; &lt;p&gt;We always focus on the high cost of moving data between CPU and GPU, but we often forget the cost of recalculating that data. Let's do a quick back-of-the-napkin comparison for a Qwen3-4B-like model with a 16k token context:&lt;/p&gt; &lt;p&gt;Scenario: A user's session becomes inactive. Their 16k-token KV Cache is evicted. Later, they return. We need to restore their context.&lt;/p&gt; &lt;p&gt;¬∑ Option A: Recalculate the KV Cache (Standard Approach) ¬∑ This requires a full &amp;quot;prefill&amp;quot; pass over the entire 16k token prompt. ¬∑ Estimated Time: ~1.5 to 3 seconds on a modern GPU. ¬∑ Option B: Swapping (Proposed Approach) ¬∑ We simply copy the ~4 GB of KV Cache data from system RAM back to VRAM over PCIe. ¬∑ Estimated Time: ~200-400 ms (on PCIe 4.0).&lt;/p&gt; &lt;p&gt;The math is pretty compelling. Swapping is roughly 7-15x faster than a full recalculation. For a user, waiting 200ms for their chat history to &amp;quot;wake up&amp;quot; is a much better experience than waiting 2+ seconds.&lt;/p&gt; &lt;p&gt;This wouldn't be for high-throughput, always-online inference, but specifically for managing many long-lived sessions (e.g., support chatbots, document analysis with breaks, multi-user systems with intermittent activity). It's a classic space-time tradeoff, but in this case, using slightly more &amp;quot;space&amp;quot; (system RAM) saves a huge amount of &amp;quot;time&amp;quot; (latency on reactivation).&lt;/p&gt; &lt;p&gt;So, I have two main questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Did I mess up my calculations or reasoning anywhere? Are there hidden costs or architectural limitations (e.g., in vLLM, PyTorch, or CUDA) that make this swapping idea less practical than it seems on paper?&lt;/li&gt; &lt;li&gt;Has anyone seen or heard of implementations doing this? I know vLLM's PagedAttention is genius for VRAM management, but I haven't found anything about spilling over to CPU RAM. Are there any forks, research papers, or other inference engines exploring this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keen to hear your thoughts and correct any misunderstandings I might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy-Tutor9563"&gt; /u/Shoddy-Tutor9563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T14:12:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1omf6p7</id>
    <title>Next evolution of agentic memory</title>
    <updated>2025-11-02T11:44:42+00:00</updated>
    <author>
      <name>/u/Any-Cockroach-3233</name>
      <uri>https://old.reddit.com/user/Any-Cockroach-3233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every new AI startup says they've &amp;quot;solved memory&amp;quot;&lt;/p&gt; &lt;p&gt;99% of them just dump text into a vector DB&lt;/p&gt; &lt;p&gt;I wrote about why that approach is broken, and how agents can build human-like memory instead&lt;/p&gt; &lt;p&gt;Link in the comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cockroach-3233"&gt; /u/Any-Cockroach-3233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T11:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ome1la</id>
    <title>When Five Dumb AIs Beat One Smart AI: The Case for Multi-Agent Systems</title>
    <updated>2025-11-02T10:35:30+00:00</updated>
    <author>
      <name>/u/SuspiciousFile9845</name>
      <uri>https://old.reddit.com/user/SuspiciousFile9845</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/@ksramalakshmi/when-five-dumb-ais-beat-one-smart-ai-the-case-for-multi-agent-systems-47b72ac5d7da"&gt;https://medium.com/@ksramalakshmi/when-five-dumb-ais-beat-one-smart-ai-the-case-for-multi-agent-systems-47b72ac5d7da&lt;/a&gt;&lt;br /&gt; What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuspiciousFile9845"&gt; /u/SuspiciousFile9845 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T10:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1omgi3f</id>
    <title>Why does Image Recognition work in llama-server but not through Open WebUI?</title>
    <updated>2025-11-02T12:53:06+00:00</updated>
    <author>
      <name>/u/pixelterpy</name>
      <uri>https://old.reddit.com/user/pixelterpy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt; &lt;img alt="Why does Image Recognition work in llama-server but not through Open WebUI?" src="https://preview.redd.it/8fjfilvybuyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2447588056ef1b6e5514cbfcc3152a09667d1816" title="Why does Image Recognition work in llama-server but not through Open WebUI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pixelterpy"&gt; /u/pixelterpy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8fjfilvybuyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T12:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1om8wdf</id>
    <title>Do you have any "AI toy projects"?</title>
    <updated>2025-11-02T05:04:35+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt; &lt;img alt="Do you have any &amp;quot;AI toy projects&amp;quot;?" src="https://external-preview.redd.it/ZW1iMzQxdDB4cnlmMeDjq5qHZ2-J4399WQRf0LNpnjuXZ3nb5V768lbHJFZZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=904f40fa717f5b1bf416cec3465963053f8e879b" title="Do you have any &amp;quot;AI toy projects&amp;quot;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I share my toy project as an example: &lt;a href="https://github.com/PasiKoodaa/TextTube"&gt;https://github.com/PasiKoodaa/TextTube&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe in 10-15 years most streaming services will be replaced by local AI content creators.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iy7yl0u0xryf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T05:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1omk3bz</id>
    <title>I want to start my First homelab LLM</title>
    <updated>2025-11-02T15:24:15+00:00</updated>
    <author>
      <name>/u/MediumAd7537</name>
      <uri>https://old.reddit.com/user/MediumAd7537</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to start a small homelab to understand how LLMs work, and I need some advice:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄãRegarding hardware, I'm looking for something very small and not very expandable, and energy-efficient. An expandable option could also be considered, but my current budget is limited to under ‚Ç¨1000.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-‚Äã I primarily want to start understanding how they work, so I probably won't need a top-tier or even mid-range configuration.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄãThis PC/Server will only be accessed remotely to communicate with the AI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÄãAfter i want to make It my own personal assistant:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;‚ÄãVarious information retrieval (I need to decide the specific topic);&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚ÄãA technical assistant I can consult with;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚ÄãUnderstanding how to train them.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÄãI am not an engineer, but I would like to explore this for fun.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediumAd7537"&gt; /u/MediumAd7537 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhjf2</id>
    <title>Looking for models I can run on 16gbs of ram.</title>
    <updated>2025-11-02T13:40:26+00:00</updated>
    <author>
      <name>/u/Think_Question_6677</name>
      <uri>https://old.reddit.com/user/Think_Question_6677</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm aware ram is slow, but I'd like to try out some models on my laptop.&lt;/p&gt; &lt;p&gt;What are the best general purpose and coding models out there that will fit on 16gbs of ram and run on cpu (or an mx350 from nvidia)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Think_Question_6677"&gt; /u/Think_Question_6677 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ome16n</id>
    <title>LEAP: Ifm2-2.6b running locally on my RM11 Pro+</title>
    <updated>2025-11-02T10:34:48+00:00</updated>
    <author>
      <name>/u/ANG3LBEATZ</name>
      <uri>https://old.reddit.com/user/ANG3LBEATZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt; &lt;img alt="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" src="https://external-preview.redd.it/dWk5bXFrd2pudHlmMS2vRIL-FSqGaAZYDE4hFOYMDU1BKOfLu6Jj8jaoH7vM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06ec81f2aecb737f1ef77aba433757f6df108957" title="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;uploading this by the request&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANG3LBEATZ"&gt; /u/ANG3LBEATZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qeszvwvjntyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T10:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1om81j1</id>
    <title>glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues</title>
    <updated>2025-11-02T04:17:09+00:00</updated>
    <author>
      <name>/u/akirose1004</name>
      <uri>https://old.reddit.com/user/akirose1004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt; &lt;img alt="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" src="https://b.thumbs.redditmedia.com/7K256kODiuLD_3gm4UtRLndAWItovQwEv3qnrZaI3FI.jpg" title="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc"&gt;https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was running GLM 4.5 Air on my MacBook M4 Max with LM Studio, but &lt;strong&gt;tool calls weren't working properly&lt;/strong&gt;, which meant I couldn't use qwen-code CLI. I wanted to use an OpenAI-compatible interface, and this constant friction frustrated me enough to build a solution.&lt;/p&gt; &lt;p&gt;A proxy server that &lt;strong&gt;automatically converts GLM's XML-formatted tool calls to OpenAI-compatible format&lt;/strong&gt;. Now you can use any OpenAI-compatible client (like qwen-code) with GLM seamlessly!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full OpenAI API compatibility&lt;/li&gt; &lt;li&gt;Automatic conversion of GLM's XML &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; format to OpenAI JSON format&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Multiple tool calls and complex JSON argument parsing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Point any OpenAI-compatible client (qwen-code, LangChain, etc.) to this address and use GLM 4.5 Air as if it were OpenAI!&lt;/p&gt; &lt;h1&gt;üîó GitHub&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/akirose/glm-proxy"&gt;https://github.com/akirose/glm-proxy&lt;/a&gt; (MIT License)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you're using GLM 4.5 with LM Studio, no more tool call headaches!&lt;/strong&gt; üòä&lt;/p&gt; &lt;p&gt;Feedback and suggestions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akirose1004"&gt; /u/akirose1004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1omaa4i</id>
    <title>OCR Testing Tool maybe Open Source it?</title>
    <updated>2025-11-02T06:28:09+00:00</updated>
    <author>
      <name>/u/No-Fig-8614</name>
      <uri>https://old.reddit.com/user/No-Fig-8614</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a quick OCR tool, what it does is you choose a file then a OCR model to use. Its free to use on this test site. What it does is upload the document -&amp;gt; turns to base64-&amp;gt; OCR Model -&amp;gt; extraction model. The extraction model is a larger model (In this case GLM4.6) to create key value extractions, then format it into json output. Eventually could add API's and user management. &lt;a href="https://parasail-ocr-pipeline.azurewebsites.net/"&gt;https://parasail-ocr-pipeline.azurewebsites.net/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For PDF's I put a pre-processing library that will cut the pdf into pages/images then send it to the OCR model then combine it after.&lt;/p&gt; &lt;p&gt;The status bar needs work because it will produce the OCR output first but then takes another minute for the auto schema (key/value) creation, then modify the JSON).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any feedback on it would be great on it!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is no user segregation so any document uploaded anyone else can see.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Fig-8614"&gt; /u/No-Fig-8614 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T06:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1olytpd</id>
    <title>Qwen3-VL is impressive!</title>
    <updated>2025-11-01T20:56:56+00:00</updated>
    <author>
      <name>/u/KraiiFox</name>
      <uri>https://old.reddit.com/user/KraiiFox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt; &lt;img alt="Qwen3-VL is impressive!" src="https://external-preview.redd.it/d2xhbXRjcGxscHlmMUowvrHmMIpZo4AiauGE1Mcv4FXKd8bkFKJe4QU1BrJL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46a71b7402921f97028babf1e571a097b79a162c" title="Qwen3-VL is impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KraiiFox"&gt; /u/KraiiFox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfcu47ollpyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1omcjct</id>
    <title>Running Local LLM's Fascinates me - But I'm Absolutely LOST</title>
    <updated>2025-11-02T08:57:28+00:00</updated>
    <author>
      <name>/u/WhatsGoingOnERE</name>
      <uri>https://old.reddit.com/user/WhatsGoingOnERE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I watched PewDiePie‚Äôs new video and now I‚Äôm obsessed with the idea of running models locally. He had a ‚Äúcouncil‚Äù of AIs talking to each other, then voting on the best answer. You can also fine tune and customise stuff, which sounds unreal.&lt;/p&gt; &lt;p&gt;Here‚Äôs my deal. I already pay for GPT-5 Pro and Claude Max and they are great. I want to know if I would actually see better performance by doing this locally, or if it‚Äôs just a fun rabbit hole.&lt;/p&gt; &lt;p&gt;Basically want to know if using these local models gets better results for anyone vs the best models available online, and if not, what are the other benefits?&lt;/p&gt; &lt;p&gt;I know privacy is a big one for some people, but lets ignore that for this case.&lt;/p&gt; &lt;p&gt;My main use cases are for business (SEO, SaaS, general marketing, business idea ideation, etc), and coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhatsGoingOnERE"&gt; /u/WhatsGoingOnERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T08:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhijb</id>
    <title>Unhinged Uncensored Model Evolution: Feedback on Satyr V0.1 to Shape Future Releases!</title>
    <updated>2025-11-02T13:39:19+00:00</updated>
    <author>
      <name>/u/ThePantheonUnbound</name>
      <uri>https://old.reddit.com/user/ThePantheonUnbound</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôm the creator of the unhinged and uncensored Satyr model (soon to be a model series). A couple of days ago, I noticed a Reddit post about a new uncensored model release called Apollo V0.1 by &lt;a href="/u/AllThingsIntel"&gt;u/AllThingsIntel&lt;/a&gt;. I tested it and found it to be as uncensored as my model, but more capable and versatile as a general assistant (without any extreme biases or a tendency to turn every single prompt NSFW). That‚Äôs the direction I want future Satyr releases to take, but I noticed far fewer interactions with their posts and far fewer downloads than my model has, which is a bit confusing to say the least.&lt;/p&gt; &lt;p&gt;People who have tested and used both models, please leave feedback on what you liked in each of the two, so I can understand the preferred direction for the Satyr model series.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePantheonUnbound"&gt; /u/ThePantheonUnbound &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1olxijp</id>
    <title>List of interesting open-source models released this month.</title>
    <updated>2025-11-01T20:03:07+00:00</updated>
    <author>
      <name>/u/Acrobatic-Tomato4862</name>
      <uri>https://old.reddit.com/user/Acrobatic-Tomato4862</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been tracking the latest AI model releases and wanted to share a curated list of AI models released this month.&lt;/p&gt; &lt;p&gt;Credit to &lt;a href="/u/duarteeeeee"&gt;u/duarteeeeee&lt;/a&gt; for finding all these models.&lt;/p&gt; &lt;p&gt;Here's a chronological breakdown of some of the most interesting open models released around October 1st - 31st, 2025:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;October 1st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;&lt;strong&gt;LFM2-Audio-1.5B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Low-latency, end-to-end audio foundation model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;&lt;strong&gt;KaniTTS-370M&lt;/strong&gt;&lt;/a&gt; (NineNineSix): Fast, open-source TTS for real-time applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 2nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models"&gt;&lt;strong&gt;Granite 4.0&lt;/strong&gt;&lt;/a&gt; (IBM): Hyper-efficient, hybrid models for enterprise use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;&lt;strong&gt;NeuTTS Air&lt;/strong&gt;&lt;/a&gt; (Neuphonic Speech): On-device TTS with instant voice cloning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 3rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://simular.ai/articles/agent-s3"&gt;&lt;strong&gt;Agent S3&lt;/strong&gt;&lt;/a&gt; (Simular): Open framework for human-like computer use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B"&gt;&lt;strong&gt;Ming-UniVision-16B-A3B&lt;/strong&gt;&lt;/a&gt; (Ant Group): Unified vision understanding, generation, editing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/character-ai/Ovi"&gt;&lt;strong&gt;Ovi (TTV/ITV)&lt;/strong&gt;&lt;/a&gt; (Character.AI / Yale): Open-source framework for offline talking avatars.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;&lt;strong&gt;CoDA-v0-Instruct&lt;/strong&gt;&lt;/a&gt; (Salesforce AI Research): Bidirectional diffusion model for code generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 4th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-30B-A3B-Instruct&lt;/strong&gt;&lt;/a&gt; (Alibaba): Powerful vision-language model for agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DecartAI/Decart-XR"&gt;&lt;strong&gt;DecartXR&lt;/strong&gt;&lt;/a&gt; (Decart AI): Open-source Quest app for realtime video-FX.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 7th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;&lt;strong&gt;LFM2-8B-A1B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient on-device mixture-of-experts model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVision"&gt;&lt;strong&gt;Hunyuan-Vision-1.5-Thinking&lt;/strong&gt;&lt;/a&gt; (Tencent): Multimodal &amp;quot;thinking on images&amp;quot; reasoning model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bageldotcom/paris"&gt;&lt;strong&gt;Paris&lt;/strong&gt;&lt;/a&gt; (Bagel Network): Decentralized-trained open-weight diffusion model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/cumulo-autumn/StreamDiffusion"&gt;&lt;strong&gt;StreamDiffusionV2&lt;/strong&gt;&lt;/a&gt; (UC Berkeley, MIT, et al.): Open-source pipeline for real-time video streaming.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 8th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ai21labs/Jamba-v0.1"&gt;&lt;strong&gt;Jamba Reasoning 3B&lt;/strong&gt;&lt;/a&gt; (AI21 Labs): Small hybrid model for on-device reasoning.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;&lt;strong&gt;Ling-1T / Ring-1T&lt;/strong&gt;&lt;/a&gt; (Ant Group): Trillion-parameter thinking/non-thinking open models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TingtingLiao/mimix"&gt;&lt;strong&gt;Mimix&lt;/strong&gt;&lt;/a&gt; (Research): Framework for multi-character video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 9th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;&lt;strong&gt;UserLM-8b&lt;/strong&gt;&lt;/a&gt; (Microsoft): Open-weight model simulating a &amp;quot;user&amp;quot; role.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RadicalNumerics/RND1"&gt;&lt;strong&gt;RND1-Base-0910&lt;/strong&gt;&lt;/a&gt; (Radical Numerics): Experimental diffusion language model (30B MoE).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 10th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;&lt;strong&gt;KAT-Dev-72B-Exp&lt;/strong&gt;&lt;/a&gt; (Kwaipilot): Open-source experimental model for agentic coding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 12th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pbihao.github.io/projects/DreamOmni2/"&gt;&lt;strong&gt;DreamOmni2&lt;/strong&gt;&lt;/a&gt; (ByteDance): Multimodal instruction-based image editing/generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 13th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/streaming-vlm"&gt;&lt;strong&gt;StreamingVLM&lt;/strong&gt;&lt;/a&gt; (MIT Han Lab): Real-time understanding for infinite video streams.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 14th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL"&gt;&lt;strong&gt;Qwen3-VL-4B / 8B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Efficient, open vision-language models for edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 16th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt;&lt;/a&gt; (Baidu): Lightweight 109-language document parsing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;&lt;strong&gt;MobileLLM-Pro&lt;/strong&gt;&lt;/a&gt; (Meta): 1B parameter on-device model (128k context).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0"&gt;&lt;strong&gt;FlashWorld&lt;/strong&gt;&lt;/a&gt; (Tencent): Fast (5-10 sec) 3D scene generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 17th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash-preview"&gt;&lt;strong&gt;LLaDA2.0-flash-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE diffusion model for reasoning/code.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 20th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;strong&gt;DeepSeek-OCR&lt;/strong&gt;&lt;/a&gt; (DeepseekAI): Open-source model for optical context-compression.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/krea-ai/realtime-video"&gt;&lt;strong&gt;Krea Realtime 14B&lt;/strong&gt;&lt;/a&gt; (Krea AI): 14B open-weight real-time video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 21st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-2B / 32B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Open, dense VLMs for edge and cloud.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.14876"&gt;&lt;strong&gt;BADAS-Open&lt;/strong&gt;&lt;/a&gt; (Nexar): Ego-centric collision prediction model for ADAS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 22nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient vision-language model for edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-1"&gt;&lt;strong&gt;HunyuanWorld-1.1&lt;/strong&gt;&lt;/a&gt; (Tencent): 3D world generation from multi-view/video.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PokeeAI/pokee_research_7b"&gt;&lt;strong&gt;PokeeResearch-7B&lt;/strong&gt;&lt;/a&gt; (Pokee AI): Open 7B deep-research agent (search/synthesis).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025"&gt;&lt;strong&gt;olmOCR-2-7B-1025&lt;/strong&gt;&lt;/a&gt; (Allen Institute for AI): Open-source, single-pass PDF-to-structured-text model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 23rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://ltx.video/"&gt;&lt;strong&gt;LTX 2&lt;/strong&gt;&lt;/a&gt; (Lightricks): Open-source 4K video engine for consumer GPUs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025"&gt;&lt;strong&gt;LightOnOCR-1B&lt;/strong&gt;&lt;/a&gt; (LightOn): Fast, 1B-parameter open-source OCR VLM.&lt;/li&gt; &lt;li&gt;&lt;a href="https://holo-cine.github.io/"&gt;&lt;strong&gt;HoloCine&lt;/strong&gt;&lt;/a&gt; (Research): Model for holistic, multi-shot cinematic narratives.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 24th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/tahoebio/tahoe-x1"&gt;&lt;strong&gt;Tahoe-x1&lt;/strong&gt;&lt;/a&gt; (Tahoe Therapeutics): 3B open-source single-cell biology model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PRIME-RL/P1-30B-A3B"&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/a&gt; (PRIME-RL): Model mastering Physics Olympiads with RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 25th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;&lt;strong&gt;LongCat-Video&lt;/strong&gt;&lt;/a&gt; (Meituan): 13.6B open model for long video generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/html/2510.19944v1"&gt;&lt;strong&gt;Seed 3D 1.0&lt;/strong&gt;&lt;/a&gt; (ByteDance): Generates simulation-grade 3D assets from images.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 27th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.minimax.io/news/minimax-m2"&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;&lt;/a&gt; (Minimax): Open-sourced intelligence engine for agentic workflows.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;&lt;strong&gt;Ming-flash-omni-Preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE omni-modal model for perception.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;&lt;strong&gt;LLaDA2.0-mini-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 16B MoE diffusion model for language.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 28th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-ColBERT-350M"&gt;&lt;strong&gt;LFM2-ColBERT-350M&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Multilingual &amp;quot;late interaction&amp;quot; RAG retriever model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-350m"&gt;&lt;strong&gt;Granite 4.0 Nano (1B / 350M)&lt;/strong&gt;&lt;/a&gt; (IBM): Smallest open models for on-device use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/HKUDS/ViMax"&gt;&lt;strong&gt;ViMax&lt;/strong&gt;&lt;/a&gt; (HKUDS): Agentic framework for end-to-end video creation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://build.nvidia.com/nvidia/nemotron-nano-12b-v2-vl/modelcard"&gt;&lt;strong&gt;Nemotron Nano v2 VL&lt;/strong&gt;&lt;/a&gt; (NVIDIA): 12B open model for multi-image/video understanding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 29th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://openai.com/index/introducing-gpt-oss-safeguard/"&gt;&lt;strong&gt;gpt-oss-safeguard&lt;/strong&gt;&lt;/a&gt; (OpenAI): Open-weight reasoning models for safety classification.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/morphicfilms/frames-to-video"&gt;&lt;strong&gt;Frames to Video&lt;/strong&gt;&lt;/a&gt; (Morphic): Open-source model for keyframe video interpolation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Bria-AI/FIBO"&gt;&lt;strong&gt;Fibo&lt;/strong&gt;&lt;/a&gt; (Bria AI): SOTA open-source model (trained on licensed data).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ByteDance/Ouro-2.6B-Thinking"&gt;&lt;strong&gt;Bytedance Ouro 2.6b thinking and non thinking&lt;/strong&gt;&lt;/a&gt;: Small language models that punch above their weight. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 30th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/baaivision/Emu3.5"&gt;&lt;strong&gt;Emu3.5&lt;/strong&gt;&lt;/a&gt; (BAAI): Native multimodal model as a world learner.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;&lt;strong&gt;Kimi-Linear-48B-A3B&lt;/strong&gt;&lt;/a&gt; (Moonshot AI): Long-context model using a linear-attention mechanism.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/BlinkDL/RWKV-CHN-2/summary"&gt;&lt;strong&gt;RWKV-7 G0a3 7.2B&lt;/strong&gt;&lt;/a&gt; (BlinkDL): A multilingual RNN-based large language model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/alibaba/UI-Ins"&gt;&lt;strong&gt;UI-Ins-32B / 7B&lt;/strong&gt;&lt;/a&gt; (Alibaba): GUI grounding agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please correct me if I have misclassified/mislinked any of the above models. This is my first post, so I am expecting there might be some mistakes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic-Tomato4862"&gt; /u/Acrobatic-Tomato4862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhby8</id>
    <title>Qwen 3 max thinking released.</title>
    <updated>2025-11-02T13:31:06+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try it &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
