<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-01T11:38:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qsvede</id>
    <title>Local Model or Groq Support</title>
    <updated>2026-02-01T10:49:49+00:00</updated>
    <author>
      <name>/u/shalako_damien</name>
      <uri>https://old.reddit.com/user/shalako_damien</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am struggling to get this working on local model. With Anthropic and OpenAi I am running out of credits and it's almost feels like a money guzzling application invented by error or designed by open of the big companies itself !! No offense....I have already thrown good money at the Apis and it's just does not seem enough. Have anyone fot this working on groq or a local model. I am having a 5090 GPU that is dying to serve clawd&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shalako_damien"&gt; /u/shalako_damien &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvede/local_model_or_groq_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvede/local_model_or_groq_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvede/local_model_or_groq_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T10:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qss3yo</id>
    <title>Looking For AI Tools To Synthesize Multiple PDF's</title>
    <updated>2026-02-01T07:39:40+00:00</updated>
    <author>
      <name>/u/GTSaketh</name>
      <uri>https://old.reddit.com/user/GTSaketh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a couple pdfs(around 100) with various topics on the same subject and research, and I want to combine all of the information into one PDF. &lt;/p&gt; &lt;p&gt;Is there any AI that can do it for free but with full privacy?&lt;/p&gt; &lt;p&gt;By the way, I do not mean summarize. I want all the information to remain but neatly organized, essentially what I am looking for is a tool/ai that reads all pdfs and creates its own structured pdf as if it were a book.&lt;/p&gt; &lt;p&gt;I know it's too much to ask something like this for free but it's just for a hobby, I have a gaming laptop aswell so I am ok with local options aswell(preferably with a guide).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTSaketh"&gt; /u/GTSaketh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qss3yo/looking_for_ai_tools_to_synthesize_multiple_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qss3yo/looking_for_ai_tools_to_synthesize_multiple_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qss3yo/looking_for_ai_tools_to_synthesize_multiple_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T07:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrwo9v</id>
    <title>Here it goes</title>
    <updated>2026-01-31T08:12:32+00:00</updated>
    <author>
      <name>/u/gotkush</name>
      <uri>https://old.reddit.com/user/gotkush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"&gt; &lt;img alt="Here it goes" src="https://preview.redd.it/pchjv5z88ngg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46ff31a155e1a7011f67f91d666503ef5cbcdf51" title="Here it goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My friend sold me his mining unit that he never got to use. He had it at his mom’s house and his mom moved out of town so he let me keep it. Was gonna part it out but I think it’s my new project. It has 8 RTx 3090 which has 24gbvram I would just need to upgrade the mobo cpu ram and the est j found was around 2500 for mobo 5900ryzen 256gb ram. It has 4 1000w power, would just need to get 8 pci risers so i can have each gou run at pcie4.0 x16. What donyoi guys think ? U think its over kill, im bery interested in havin my own ai sandbkx. Wouldnlike to get eveyones r thoughts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gotkush"&gt; /u/gotkush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pchjv5z88ngg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T08:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qshv8g</id>
    <title>Better perfs with ik_llama.cpp + Minimax M2.1 (multi RTX3090) + sm graph</title>
    <updated>2026-01-31T23:30:25+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qshv8g/better_perfs_with_ik_llamacpp_minimax_m21_multi/"&gt; &lt;img alt="Better perfs with ik_llama.cpp + Minimax M2.1 (multi RTX3090) + sm graph" src="https://b.thumbs.redditmedia.com/yuNRw6MKOGToAQilGJsKgUHsjtUZGbZWYcXkXoLtwqs.jpg" title="Better perfs with ik_llama.cpp + Minimax M2.1 (multi RTX3090) + sm graph" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following some quite recent posts about -sm graph performances with ik_llama.cpp I made few tests but at that time Minimax was not uspported with that.&lt;/p&gt; &lt;p&gt;But I just have seen &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/1195"&gt;this PR&lt;/a&gt; and it is much better now!&lt;/p&gt; &lt;p&gt;I'm on a multi RTX 3090 setup and following is the command (any suggestion on args is welcomed):&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m 'MiniMax-M2.1-UD-Q4_K_XL-00001-of-00003.gguf' \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-sm graph \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-fa 1 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--n-gpu-layers 99 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--no-mmap \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-c 160000 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-b 2048 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ub 1024 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ctk q4_0 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ctv q4_0 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--jinja&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/907g680norgg1.png?width=1761&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d032d70ee5d8b4954e33f8c905a267bbc0f1da2d"&gt;perfs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This project seems to move very fast so from now on I will pay much more attention to it, ik rocks!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qshv8g/better_perfs_with_ik_llamacpp_minimax_m21_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qshv8g/better_perfs_with_ik_llamacpp_minimax_m21_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qshv8g/better_perfs_with_ik_llamacpp_minimax_m21_multi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T23:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrzyaz</id>
    <title>I found that MXFP4 has lower perplexity than Q4_K_M and Q4_K_XL.</title>
    <updated>2026-01-31T11:27:30+00:00</updated>
    <author>
      <name>/u/East-Engineering-653</name>
      <uri>https://old.reddit.com/user/East-Engineering-653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post was originally written in Korean and then translated into English using ChatGPT.&lt;br /&gt; Hello, I am currently serving LLM models using a Tesla P40 and llama.cpp. When running models in the 30–32B range, I usually rely on 4-bit quantization. Until now, I primarily used Q4_K_XL, and if Q4_K_XL was not available, I used Q4_K_M instead. I initially avoided MXFP4 quantization because, compared to other 4-bit quantization methods, it has a smaller size, so I naturally assumed its accuracy would be lower. However, out of curiosity sparked by MXFP4’s fast speed, I compared Q4_K_M, Q4_K_XL, and MXFP4 quantization methods for the GLM-4.7-Flash and Nemotron-3-nano models using the &lt;code&gt;llama-perplexity&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;Below are the commands used, along with the Python code and command used to generate the dataset. The dataset generation command was created using ChatGPT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import argparse import os import re import sys import urllib.request from pathlib import Path import random def download(url: str, dst: Path) -&amp;gt; None: dst.parent.mkdir(parents=True, exist_ok=True) with urllib.request.urlopen(url) as r, open(dst, &amp;quot;wb&amp;quot;) as f: f.write(r.read()) def normalize_text(text: str, mode: str) -&amp;gt; str: text = text.replace(&amp;quot;\r\n&amp;quot;, &amp;quot;\n&amp;quot;).replace(&amp;quot;\r&amp;quot;, &amp;quot;\n&amp;quot;) if mode == &amp;quot;ppl&amp;quot;: text = re.sub(r&amp;quot;\n\s*\n+&amp;quot;, &amp;quot;\n&amp;quot;, text) text = re.sub(r&amp;quot;[ \t]+&amp;quot;, &amp;quot; &amp;quot;, text) text = text.strip() + &amp;quot;\n&amp;quot; return text if mode == &amp;quot;line&amp;quot;: lines = [] for line in text.split(&amp;quot;\n&amp;quot;): line = line.strip() if not line: continue line = re.sub(r&amp;quot;[ \t]+&amp;quot;, &amp;quot; &amp;quot;, line) lines.append(line) return &amp;quot;\n&amp;quot;.join(lines) + &amp;quot;\n&amp;quot; raise ValueError(f&amp;quot;unknown mode: {mode}&amp;quot;) def take_prefix(text: str, max_chars: int | None) -&amp;gt; str: if max_chars is None: return text if max_chars &amp;lt;= 0: return &amp;quot;&amp;quot; return text[:max_chars] def sample_lines(text: str, n_lines: int, seed: int) -&amp;gt; str: random.seed(seed) lines = [ln for ln in text.split(&amp;quot;\n&amp;quot;) if ln.strip()] if n_lines &amp;lt;= 0 or n_lines &amp;gt;= len(lines): return &amp;quot;\n&amp;quot;.join(lines) + &amp;quot;\n&amp;quot; sampled = random.sample(lines, n_lines) return &amp;quot;\n&amp;quot;.join(sampled) + &amp;quot;\n&amp;quot; def main(): ap = argparse.ArgumentParser() g = ap.add_mutually_exclusive_group(required=True) g.add_argument(&amp;quot;--url&amp;quot;, help=&amp;quot;download source url&amp;quot;) g.add_argument(&amp;quot;--infile&amp;quot;, help=&amp;quot;local input file path&amp;quot;) ap.add_argument(&amp;quot;--out&amp;quot;, required=True, help=&amp;quot;output text file path&amp;quot;) ap.add_argument(&amp;quot;--mode&amp;quot;, choices=[&amp;quot;ppl&amp;quot;, &amp;quot;line&amp;quot;], default=&amp;quot;ppl&amp;quot;, help=&amp;quot;ppl: keep newlines but collapse blanks/spaces, line: one sentence per line style&amp;quot;) ap.add_argument(&amp;quot;--max-chars&amp;quot;, type=int, default=None, help=&amp;quot;optional: cut the output to first N characters (fast/low-memory eval)&amp;quot;) ap.add_argument(&amp;quot;--sample-lines&amp;quot;, type=int, default=None, help=&amp;quot;optional: sample N non-empty lines uniformly (good for quick comparison)&amp;quot;) ap.add_argument(&amp;quot;--seed&amp;quot;, type=int, default=42) args = ap.parse_args() out_path = Path(args.out) if args.url: tmp = out_path.with_suffix(out_path.suffix + &amp;quot;.download&amp;quot;) download(args.url, tmp) in_path = tmp else: in_path = Path(args.infile) try: raw = in_path.read_text(encoding=&amp;quot;utf-8&amp;quot;, errors=&amp;quot;replace&amp;quot;) except Exception as e: print(f&amp;quot;failed to read input: {e}&amp;quot;, file=sys.stderr) sys.exit(1) text = normalize_text(raw, args.mode) if args.sample_lines is not None: text = sample_lines(text, args.sample_lines, args.seed) text = take_prefix(text, args.max_chars) out_path.parent.mkdir(parents=True, exist_ok=True) out_path.write_text(text, encoding=&amp;quot;utf-8&amp;quot;) if args.url: try: os.remove(in_path) except OSError: pass print(f&amp;quot;wrote: {out_path} ({out_path.stat().st_size} bytes)&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 wikitext_prep.py \ --url https://cosmo.zip/pub/datasets/wikitext-2-raw/wiki.test.raw \ --out /data/wikitext2_test.txt \ --mode ppl \ --max-chars 2000000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using the command below, I measured the perplexity of the quantized models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity -m modelname.gguf -f wikitext2_test.txt -c 32768 -b 4096 -fa on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The table below summarizes the test results, which were also organized using ChatGPT. The actual &lt;code&gt;llama-perplexity&lt;/code&gt; output is quite long, so it is attached separately below. For reference, Q4_K_M and Q4_K_XL were measured simultaneously, and after a llama.cpp update, Q4_K_XL and MXFP4 were measured simultaneously. Because the testing time was very long and the perplexity of Q4_K_XL was similar before and after the update, I assumed that the perplexity of Q4_K_M would also not be significantly affected by build changes.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Q4_K_M (Unsloth)&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL (previous)&lt;/th&gt; &lt;th align="left"&gt;MXFP4_MOE&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL (current)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp build&lt;/td&gt; &lt;td align="left"&gt;7803&lt;/td&gt; &lt;td align="left"&gt;7803&lt;/td&gt; &lt;td align="left"&gt;7896&lt;/td&gt; &lt;td align="left"&gt;7896&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF file type&lt;/td&gt; &lt;td align="left"&gt;Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;Q4_K – Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;16.31 GiB&lt;/td&gt; &lt;td align="left"&gt;15.79 GiB&lt;/td&gt; &lt;td align="left"&gt;16.31 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BPW&lt;/td&gt; &lt;td align="left"&gt;4.89&lt;/td&gt; &lt;td align="left"&gt;4.68&lt;/td&gt; &lt;td align="left"&gt;4.53&lt;/td&gt; &lt;td align="left"&gt;4.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (final)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;16.1745 ± 0.1870&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.8605 ± 0.1823&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;10.7235 ± 0.1052&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.7309 ± 0.1803&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt eval speed&lt;/td&gt; &lt;td align="left"&gt;64.39 tok/s&lt;/td&gt; &lt;td align="left"&gt;64.37 tok/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68.20 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.73 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ms/token&lt;/td&gt; &lt;td align="left"&gt;15.53 ms&lt;/td&gt; &lt;td align="left"&gt;15.54 ms&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.66 ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.76 ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Time per pass (ETA)&lt;/td&gt; &lt;td align="left"&gt;529.38 s&lt;/td&gt; &lt;td align="left"&gt;530.05 s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;501.55 s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;502.66 s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU self (total)&lt;/td&gt; &lt;td align="left"&gt;20811 MiB&lt;/td&gt; &lt;td align="left"&gt;20056 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;17874 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;18552 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU model buffer&lt;/td&gt; &lt;td align="left"&gt;17284.84 MiB&lt;/td&gt; &lt;td align="left"&gt;16529.37 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15852.01 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16529.37 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KV cache size&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3196 MiB&lt;/strong&gt; (K 1692 + V 1504)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3196 MiB&lt;/strong&gt; (K 1692 + V 1504)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1692 MiB&lt;/strong&gt; (K 1692 + V 0)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1692 MiB&lt;/strong&gt; (K 1692 + V 0)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU free (log-based)&lt;/td&gt; &lt;td align="left"&gt;3406 MiB&lt;/td&gt; &lt;td align="left"&gt;4162 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;6342 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5666 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Load time&lt;/td&gt; &lt;td align="left"&gt;9.90 s&lt;/td&gt; &lt;td align="left"&gt;9.55 s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.13 s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;43.72 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mmap / direct_io&lt;/td&gt; &lt;td align="left"&gt;mmap off / direct_io on&lt;/td&gt; &lt;td align="left"&gt;mmap off / direct_io on&lt;/td&gt; &lt;td align="left"&gt;mmap on / direct_io off&lt;/td&gt; &lt;td align="left"&gt;mmap on / direct_io off&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;[1]&lt;/th&gt; &lt;th align="left"&gt;[2]&lt;/th&gt; &lt;th align="left"&gt;[3]&lt;/th&gt; &lt;th align="left"&gt;[4]&lt;/th&gt; &lt;th align="left"&gt;[5]&lt;/th&gt; &lt;th align="left"&gt;[6]&lt;/th&gt; &lt;th align="left"&gt;Final PPL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;15.2952&lt;/td&gt; &lt;td align="left"&gt;15.1950&lt;/td&gt; &lt;td align="left"&gt;15.7101&lt;/td&gt; &lt;td align="left"&gt;14.8037&lt;/td&gt; &lt;td align="left"&gt;14.5891&lt;/td&gt; &lt;td align="left"&gt;16.1745&lt;/td&gt; &lt;td align="left"&gt;16.1745 ± 0.1870&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL (previous)&lt;/td&gt; &lt;td align="left"&gt;14.7572&lt;/td&gt; &lt;td align="left"&gt;14.4954&lt;/td&gt; &lt;td align="left"&gt;15.0386&lt;/td&gt; &lt;td align="left"&gt;14.1713&lt;/td&gt; &lt;td align="left"&gt;14.1425&lt;/td&gt; &lt;td align="left"&gt;15.8605&lt;/td&gt; &lt;td align="left"&gt;15.8605 ± 0.1823&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;10.1764&lt;/td&gt; &lt;td align="left"&gt;10.1296&lt;/td&gt; &lt;td align="left"&gt;10.4917&lt;/td&gt; &lt;td align="left"&gt;9.8666&lt;/td&gt; &lt;td align="left"&gt;9.8629&lt;/td&gt; &lt;td align="left"&gt;10.7235&lt;/td&gt; &lt;td align="left"&gt;10.7235 ± 0.1052&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL (current)&lt;/td&gt; &lt;td align="left"&gt;14.4241&lt;/td&gt; &lt;td align="left"&gt;14.2673&lt;/td&gt; &lt;td align="left"&gt;14.8671&lt;/td&gt; &lt;td align="left"&gt;14.0460&lt;/td&gt; &lt;td align="left"&gt;14.0444&lt;/td&gt; &lt;td align="left"&gt;15.7309&lt;/td&gt; &lt;td align="left"&gt;15.7309 ± 0.1803&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Below is a table comparing MXFP4 and Q4_K_XL quantization methods on the Nemotron-3-nano model. This table was also created using ChatGPT.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Q4_K_XL (previous)&lt;/th&gt; &lt;th align="left"&gt;MXFP4 (current)&lt;/th&gt; &lt;th align="left"&gt;Change (MXFP4 − Q4_K_XL)&lt;/th&gt; &lt;th align="left"&gt;Meaning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;7.7090&lt;/td&gt; &lt;td align="left"&gt;7.5294&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-0.1796&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;MXFP4 is lower → based on this corpus, “less accuracy loss (or more accurate)”&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL error (±)&lt;/td&gt; &lt;td align="left"&gt;0.05361&lt;/td&gt; &lt;td align="left"&gt;0.05198&lt;/td&gt; &lt;td align="left"&gt;-0.00163&lt;/td&gt; &lt;td align="left"&gt;Uncertainty is nearly identical&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt eval speed&lt;/td&gt; &lt;td align="left"&gt;763.26 tok/s&lt;/td&gt; &lt;td align="left"&gt;797.79 tok/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+34.53 tok/s (+4.5%)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;MXFP4 is slightly faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Time per pass&lt;/td&gt; &lt;td align="left"&gt;24.74 s/pass&lt;/td&gt; &lt;td align="left"&gt;23.45 s/pass&lt;/td&gt; &lt;td align="left"&gt;-1.29 s/pass&lt;/td&gt; &lt;td align="left"&gt;MXFP4 is slightly shorter&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU model memory&lt;/td&gt; &lt;td align="left"&gt;21537 MiB&lt;/td&gt; &lt;td align="left"&gt;16782 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-4755 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;MXFP4 uses &lt;strong&gt;significantly less model memory&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU free VRAM&lt;/td&gt; &lt;td align="left"&gt;2286 MiB&lt;/td&gt; &lt;td align="left"&gt;7040 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+4754 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Available VRAM increases greatly&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU context memory&lt;/td&gt; &lt;td align="left"&gt;143 MiB&lt;/td&gt; &lt;td align="left"&gt;143 MiB&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;Same due to identical &lt;code&gt;n_ctx&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU compute buffer&lt;/td&gt; &lt;td align="left"&gt;271 MiB&lt;/td&gt; &lt;td align="left"&gt;271 MiB&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;Same&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Host usage (total)&lt;/td&gt; &lt;td align="left"&gt;268 MiB&lt;/td&gt; &lt;td align="left"&gt;394 MiB&lt;/td&gt; &lt;td align="left"&gt;+126 MiB&lt;/td&gt; &lt;td align="left"&gt;Difference is small and of limited significance&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I rewrote this post to add the Nemotron-3-nano benchmark, and in the previous post, one user commented that perplexity and tool calling or coding are completely different domains. They mentioned that using the HumanEval benchmark would provide values more directly related to tool calling and coding performance. If I get the chance, I plan to test again using the HumanEval benchmark in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qrwnd4/comment/o2rape9/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qrwnd4/comment/o2rape9/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To be honest, after seeing these benchmark results, I hoped that perplexity would be directly related to coding and tool calling performance, so it is a bit disappointing.&lt;br /&gt; If anyone has other opinions, I would appreciate it if you could share them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Engineering-653"&gt; /u/East-Engineering-653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T11:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsf4mc</id>
    <title>Woo Hoo! New to me hardware, I think I am now part of club mediocre.</title>
    <updated>2026-01-31T21:39:33+00:00</updated>
    <author>
      <name>/u/Dented_Steelbook</name>
      <uri>https://old.reddit.com/user/Dented_Steelbook</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsf4mc/woo_hoo_new_to_me_hardware_i_think_i_am_now_part/"&gt; &lt;img alt="Woo Hoo! New to me hardware, I think I am now part of club mediocre." src="https://b.thumbs.redditmedia.com/S7Ruy-X01J1o2hWzEJU2dKQYmweAcUX_qyxplg4qjTU.jpg" title="Woo Hoo! New to me hardware, I think I am now part of club mediocre." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got a used machine and don’t know what to do with it. Already having trouble getting a keyboard to work, thought I could just hook a usb cable to my wireless one, but it doesn’t seem to do anything. I need a dedicated one anyways, so I am off to Best Buy. It looks fairly clean, would you just blow out any dust or leave it alone?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dented_Steelbook"&gt; /u/Dented_Steelbook &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qsf4mc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsf4mc/woo_hoo_new_to_me_hardware_i_think_i_am_now_part/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsf4mc/woo_hoo_new_to_me_hardware_i_think_i_am_now_part/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T21:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsbkpe</id>
    <title>M4 Max 128 GB vs Strix halo 128 GB</title>
    <updated>2026-01-31T19:22:46+00:00</updated>
    <author>
      <name>/u/dever121</name>
      <uri>https://old.reddit.com/user/dever121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;Which one is the best device for inference: Mac studio 128 GB vs. GMKtec EVO-X2 AI Mini PC Ryzen Al Max+ 395 (128 GB). I am looking for a prod environment, so speed is a must, plus sometimes small fine-tuning jobs are also required.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dever121"&gt; /u/dever121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbkpe/m4_max_128_gb_vs_strix_halo_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbkpe/m4_max_128_gb_vs_strix_halo_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbkpe/m4_max_128_gb_vs_strix_halo_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T19:22:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsi8n2</id>
    <title>Why no NVFP8 or MXFP8?</title>
    <updated>2026-01-31T23:45:58+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why is there no interest in NVFP8 or MXFP8 in llama.cpp or VLLM or from anyone quantizing models?&lt;/p&gt; &lt;p&gt;These formats should be more accurate than standard FP8 and are accelerated on Blackwell&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsi8n2/why_no_nvfp8_or_mxfp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsi8n2/why_no_nvfp8_or_mxfp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsi8n2/why_no_nvfp8_or_mxfp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T23:45:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrsy4q</id>
    <title>How close are open-weight models to "SOTA"? My honest take as of today, benchmarks be damned.</title>
    <updated>2026-01-31T04:49:42+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"&gt; &lt;img alt="How close are open-weight models to &amp;quot;SOTA&amp;quot;? My honest take as of today, benchmarks be damned." src="https://preview.redd.it/k38sg20q7mgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1d06005e56d7a9be9a1c820f7096aa2805c52dc" title="How close are open-weight models to &amp;quot;SOTA&amp;quot;? My honest take as of today, benchmarks be damned." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k38sg20q7mgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T04:49:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsw2wn</id>
    <title>How much improvement has there been (or seems likely to happen in the future) for clustering mac computers than have Thunderbolt-4 ports (not Thunderbolt-5). I realize the big breakthrough with RDMA last month was for Thunderbolt-5, but I am curious about Thunderbolt-4 mac clusters.</title>
    <updated>2026-02-01T11:28:47+00:00</updated>
    <author>
      <name>/u/MistressMedium123lb</name>
      <uri>https://old.reddit.com/user/MistressMedium123lb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, back in December when there was all that buzz about RDMA, and Exo and the big RDMA improvement for clustering macs, but only macs that had Thunderbolt-5, I didn't look into it much at the time, but, from what I remembered, it seemed like in the past, if you clustered a bunch of mac minis (or similar macs with Thunderbolt 4 connections), you could pool their memory and run bigger models, but, not only would you not gain any speed from the clustering, but instead you would more like lose a bunch of speed, and it would run something like 10 times slower than what a single mac with that amount of memory would be able to do on its own.&lt;/p&gt; &lt;p&gt;Even that was still kind of interesting, actually, since sometimes I don't mind a 10x slowdown if it means I get to use a bigger, more powerful model, but, obviously hard to be nearly as excited about that as a Thunderbolt-5 RDMA cluster that not only doesn't slow down 10x, but instead more like speeds up 2x.&lt;/p&gt; &lt;p&gt;But, I don't really know anything about clustering, or vLLM, or really, hardly anything about computers or running AI models, as I am fairly new to this, and don't have a background in computers.&lt;/p&gt; &lt;p&gt;I do have several mac computers though, (mostly cheap base model mac minis with thunderbolt 4 ports), and I am kind of curious about non-Thunderbolt-5 mac clustering.&lt;/p&gt; &lt;p&gt;One thing that recently made me a bit more curious is, I heard that maybe it doesn't necessarily have to be some big 20x or 10x slowdown when you cluster them on Thunderbolt-4, that maybe that's only if you do it wrong, or that maybe some other sorts of advancements got made, even regarding Thunderbolt-4, not in as good or official of a way as what happened with Thunderbolt-5 and RDMA, but, better than nothing, and also that more improvements for clustering macs with Thunderbolt-4 might be coming in the near future.&lt;/p&gt; &lt;p&gt;Well, since there are probably a lot of people on here who have two or more base mac minis or lower level macs, but don't have numerous mac studios, or people in mixed situations with it (1 mac studio, and 1 or more base mac minis), I figured maybe there are others who might be curious about this, or know something about it. &lt;/p&gt; &lt;p&gt;So, is it still like a 10x-20x slowdown to cluster the non-Thunderbolt-5 macs? Or is it not quite that bad? Does it seem like even-speed clustering (or even speed-gain clustering) could be on the horizon for Thunderbolt-4 (in a non-official way, rather than coming through Apple, I mean)? What is the best current setup to get the best speeds from a Thunderbolt-4 mac cluster? What seems the most promising thing, and thing I should be checking, if I want to see if any breakthroughs happen for Thunderbolt-4 mac clustering performance? And what should I read or where should I start if I want to learn more about clustering in general, for using LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MistressMedium123lb"&gt; /u/MistressMedium123lb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsw2wn/how_much_improvement_has_there_been_or_seems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsw2wn/how_much_improvement_has_there_been_or_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsw2wn/how_much_improvement_has_there_been_or_seems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T11:28:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs27hf</id>
    <title>g-HOOT in the Machine</title>
    <updated>2026-01-31T13:21:43+00:00</updated>
    <author>
      <name>/u/TheVeryNearFuture</name>
      <uri>https://old.reddit.com/user/TheVeryNearFuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt; &lt;img alt="g-HOOT in the Machine" src="https://preview.redd.it/z78lvao9rogg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae89cf23b154560e4ea34ce2fff5ea8a457a781b" title="g-HOOT in the Machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.14805"&gt;https://arxiv.org/abs/2507.14805&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheVeryNearFuture"&gt; /u/TheVeryNearFuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z78lvao9rogg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qspi72</id>
    <title>"Vibe Testing" — using LLMs to pressure-test spec docs before writing code, and it actually works</title>
    <updated>2026-02-01T05:17:33+00:00</updated>
    <author>
      <name>/u/Opposite-Pea-7615</name>
      <uri>https://old.reddit.com/user/Opposite-Pea-7615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;has anyone tried feeding a bunch of design/spec documents into context and asking it to trace through a realistic scenario step by step? &lt;/p&gt; &lt;p&gt;we test code obsessively — unit tests, integration tests, e2e, the whole thing. but the specs that *define* what the code should do? we just review those in a meeting. maybe two people read them carefully. i started wondering if you could use LLMs to basically &amp;quot;unit test&amp;quot; your specs the same way you test code. been calling it &amp;quot;vibe testing&amp;quot; — like vibe coding but for the planning phase, you write a scenario and let the model vibe its way through your docs and tell you where things break down. &lt;/p&gt; &lt;p&gt;the idea is simple: write a concrete scenario with a real persona and specific failure modes, dump all your spec docs into context, and ask the model to trace through it step by step. for each step it tells you which spec covers the behavior, and flags anything that's a gap (spec is silent), a conflict (two specs disagree), or an ambiguity (spec is unclear). &lt;/p&gt; &lt;p&gt;so we had about 15 spec docs for a system — auth, payments, inventory, orders, notifications etc. reviewed them multiple times across the team. felt ready to build. &lt;/p&gt; &lt;p&gt;i wrote up a short scenario — customer on mobile, payment gets declined, enters a different card, expects confirmation email — and dumped everything into context. &lt;/p&gt; &lt;p&gt;it caught a bunch of stuff nobody noticed in review: &lt;/p&gt; &lt;p&gt;- payment spec says &amp;quot;retry 3 times with exponential backoff&amp;quot; but the user is entering a *new* card, not retrying the same one. is that a retry? new attempt? idempotency key reset? spec doesn't say. we all assumed &amp;quot;obviously new attempt&amp;quot; but it's literally not written down &lt;/p&gt; &lt;p&gt;- inventory holds stock for 5 min. payment retry can take 6+. someone else can buy your items while you're still entering your card number. two specs with contradictory timing, neither references the other &lt;/p&gt; &lt;p&gt;- auth tokens expire in 15 min, checkout on a bad connection can take longer, no refresh flow defined &lt;/p&gt; &lt;p&gt;- payment succeeds but if the order service hiccups you've charged someone with no order record and there's no rollback defined &lt;/p&gt; &lt;p&gt;every one of these would have been a painful rewrite-level discovery weeks into building. the model found them in minutes because it's doing something we're bad at — holding all 15 docs in working memory and cross-referencing them without filling in gaps from experience. when a human reads &amp;quot;retry 3 times&amp;quot; your brain goes &amp;quot;yeah obviously we handle the new card case&amp;quot; and moves on. the model just says &amp;quot;this isn't defined&amp;quot; which is exactly what you want for this kind of testing. &lt;/p&gt; &lt;p&gt;some notes after trying this on a few projects: &lt;/p&gt; &lt;p&gt;- you need the context window for this. all the docs + scenario need to fit. this is one of the few cases where 100k+ context actually matters and isn't just a benchmark number&lt;br /&gt; - failure paths find way more gaps than happy paths. &amp;quot;what happens when X breaks&amp;quot; is where specs fall apart&lt;br /&gt; - pedantic models work better here. you want something that follows instructions literally and doesn't try to be helpful by filling in assumptions. more literal = better for this task&lt;br /&gt; - 4-5 scenarios varying user type, device, failure mode gives surprisingly good coverage. and specs that no scenario touches are themselves interesting — if no realistic user story hits a spec, why does it exist?&lt;br /&gt; - i've tried this with a few different models/sizes and it works as long as context is big enough and it can follow structured prompts &lt;/p&gt; &lt;p&gt;put the methodology + prompt template on github if anyone wants to mess with it: &lt;a href="http://github.com/knot0-com/vibe-testing"&gt;github.com/knot0-com/vibe-testing&lt;/a&gt; — nothing fancy, just a structured prompt you can use with whatever you're running locally &lt;/p&gt; &lt;p&gt;anyone have recommendations for which models handle this kind of long-context cross-referencing well? feels like it could be a decent real-world benchmark — &amp;quot;here's 10 docs with a planted contradiction, find it&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Opposite-Pea-7615"&gt; /u/Opposite-Pea-7615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qspi72/vibe_testing_using_llms_to_pressuretest_spec_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qspi72/vibe_testing_using_llms_to_pressuretest_spec_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qspi72/vibe_testing_using_llms_to_pressuretest_spec_docs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T05:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsvqy6</id>
    <title>An image is worth a 1000 words? ClawdBot vs Kubernetes</title>
    <updated>2026-02-01T11:09:56+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvqy6/an_image_is_worth_a_1000_words_clawdbot_vs/"&gt; &lt;img alt="An image is worth a 1000 words? ClawdBot vs Kubernetes" src="https://preview.redd.it/uzi0h1wi8vgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c27d459fc1ba8972b8be37eb656fa46bfaa39477" title="An image is worth a 1000 words? ClawdBot vs Kubernetes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uzi0h1wi8vgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvqy6/an_image_is_worth_a_1000_words_clawdbot_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvqy6/an_image_is_worth_a_1000_words_clawdbot_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T11:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsub12</id>
    <title>[OSS] Kakveda – Failure intelligence &amp; pre-flight warnings for LLM systems</title>
    <updated>2026-02-01T09:47:40+00:00</updated>
    <author>
      <name>/u/Street_Pop9758</name>
      <uri>https://old.reddit.com/user/Street_Pop9758</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing Kakveda, an open-source project that explores failure intelligence&lt;/p&gt; &lt;p&gt;for LLM and agent-based systems.&lt;/p&gt; &lt;p&gt;It focuses on remembering recurring failure modes and providing pre-flight&lt;/p&gt; &lt;p&gt;“this failed before” warnings instead of treating failures as logs.&lt;/p&gt; &lt;p&gt;Runs locally via Docker Compose.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/prateekdevisingh/kakveda"&gt;https://github.com/prateekdevisingh/kakveda&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://kakveda.com"&gt;https://kakveda.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback on the idea and architecture.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Pop9758"&gt; /u/Street_Pop9758 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsub12/oss_kakveda_failure_intelligence_preflight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsub12/oss_kakveda_failure_intelligence_preflight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsub12/oss_kakveda_failure_intelligence_preflight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T09:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsvgsh</id>
    <title>some uncensored models</title>
    <updated>2026-02-01T10:53:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since there haven’t been any (major) new local model releases lately, let’s check what uncensored models are available on Hugging Face. There are different abliteration methods, so varioud models can behave quite differently. Unfortunately, I can’t find any Nemotron-3 Nano variants.&lt;/p&gt; &lt;p&gt;Which one do you use?&lt;/p&gt; &lt;p&gt;GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF"&gt;https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Huihui-GLM-4.7-Flash-abliterated-GGUF"&gt;https://huggingface.co/mradermacher/Huihui-GLM-4.7-Flash-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Olafangensan/GLM-4.7-Flash-heretic-GGUF"&gt;https://huggingface.co/Olafangensan/GLM-4.7-Flash-heretic-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT OSS 20B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf"&gt;https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf"&gt;https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2"&gt;https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/p-e-w_gpt-oss-20b-heretic-GGUF"&gt;https://huggingface.co/bartowski/p-e-w_gpt-oss-20b-heretic-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT OSS 120B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF"&gt;https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemma 12B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DreamFast/gemma-3-12b-it-heretic"&gt;https://huggingface.co/DreamFast/gemma-3-12b-it-heretic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemma 27B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-i1-GGUF"&gt;https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 30B A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-30B-A3B-abliterated-v2"&gt;https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-30B-A3B-abliterated-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 8B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen3-8B-Hivemind-Instruct-Heretic-Abliterated-Uncensored-NEO-Imatrix-GGUF"&gt;https://huggingface.co/DavidAU/Qwen3-8B-Hivemind-Instruct-Heretic-Abliterated-Uncensored-NEO-Imatrix-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 32B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-VL-32B-Instruct-heretic-v2-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-VL-32B-Instruct-heretic-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Qwen3-32B-abliterated"&gt;https://huggingface.co/huihui-ai/Qwen3-32B-abliterated&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T10:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjya0</id>
    <title>Just wanted to post about a cool project, the internet is sleeping on.</title>
    <updated>2026-02-01T00:59:08+00:00</updated>
    <author>
      <name>/u/daLazyModder</name>
      <uri>https://old.reddit.com/user/daLazyModder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/frothywater/kanade-tokenizer"&gt;https://github.com/frothywater/kanade-tokenizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is a audio tokenizer that has been optimized and can do really fast voice cloning. With super fast realtime factor. Can even run on cpu faster then realtime. I vibecoded a fork with gui for gradio and a tkinter realtime gui for it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dalazymodder/kanade-tokenizer"&gt;https://github.com/dalazymodder/kanade-tokenizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly I think it blows rvc out of the water for real time factor and one shotting it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/1G1YU3SvGFsf"&gt;https://vocaroo.com/1G1YU3SvGFsf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/1j630aDND3d8"&gt;https://vocaroo.com/1j630aDND3d8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;example of ljspeech to kokoro voice&lt;/p&gt; &lt;p&gt;the cloning could be better but the rtf is crazy fast considering the quality.&lt;/p&gt; &lt;p&gt;Minor Update: Updated the gui with more clear instructions on the fork and the streaming for realtime works better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daLazyModder"&gt; /u/daLazyModder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjya0/just_wanted_to_post_about_a_cool_project_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjya0/just_wanted_to_post_about_a_cool_project_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjya0/just_wanted_to_post_about_a_cool_project_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T00:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsj8x4</id>
    <title>Beating GPT-2 for &lt;&lt;$100: the nanochat journey · karpathy nanochat · Discussion #481</title>
    <updated>2026-02-01T00:28:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsj8x4/beating_gpt2_for_100_the_nanochat_journey/"&gt; &lt;img alt="Beating GPT-2 for &amp;lt;&amp;lt;$100: the nanochat journey · karpathy nanochat · Discussion #481" src="https://external-preview.redd.it/tRLTF88fq1bbkxeX1rd0tIqyBPPusWm9EVZ_4pC6axI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a46384d561fdba570fc99a8b65c323f9f2075d5" title="Beating GPT-2 for &amp;lt;&amp;lt;$100: the nanochat journey · karpathy nanochat · Discussion #481" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seven years after GPT-2, you can now beat it for &amp;lt;$100.&lt;br /&gt; Andrej Karpathy shows a 3-hour training run on 8×H100 that edges past GPT-2 on the CORE benchmark.&lt;br /&gt; He shares the architecture/optimizer tweaks, the data setup, and a simple script to reproduce it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/481"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsj8x4/beating_gpt2_for_100_the_nanochat_journey/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsj8x4/beating_gpt2_for_100_the_nanochat_journey/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T00:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsh7dz</id>
    <title>Analyzed 5,357 ICLR 2026 accepted papers - here's what the research community is actually working on</title>
    <updated>2026-01-31T23:03:24+00:00</updated>
    <author>
      <name>/u/dippatel21</name>
      <uri>https://old.reddit.com/user/dippatel21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Went through the accepted papers at ICLR 2026 and counted what the research community is actually focusing on. Some findings that seem relevant for people doing local training and fine-tuning:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Alignment methods&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GRPO appears in 157 papers, DPO in only 55&lt;/li&gt; &lt;li&gt;The academic community seems to have largely moved past DPO toward Group Relative Policy Optimization&lt;/li&gt; &lt;li&gt;If you're still using DPO for post-training, might be worth looking into GRPO&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;RLVR over RLHF&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;125 papers on Reinforcement Learning with Verifiable Rewards vs 54 for RLHF&lt;/li&gt; &lt;li&gt;The shift is toward domains where correctness is programmatically checkable (math, code, logic) rather than relying on human preference data&lt;/li&gt; &lt;li&gt;Makes sense for local work since you don't need expensive human annotation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Data efficiency finding&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper called &amp;quot;Nait&amp;quot; (Neuron-Aware Instruction Tuning) shows training on 10% of Alpaca-GPT4, selected by neuron activation patterns, outperforms training on 100%&lt;/li&gt; &lt;li&gt;Implication: most instruction tuning data is redundant. Smart selection &amp;gt; more data&lt;/li&gt; &lt;li&gt;Could matter a lot for compute-constrained local training&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Test-time compute&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;257 papers on test-time training/adaptation/scaling&lt;/li&gt; &lt;li&gt;This is now mainstream, not experimental&lt;/li&gt; &lt;li&gt;Relevant for inference optimization on local hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Mamba/SSMs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;202 papers mention Mamba or state space models&lt;/li&gt; &lt;li&gt;Not dead, still an active research direction&lt;/li&gt; &lt;li&gt;Worth watching for potential attention alternatives that run better on consumer hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Security concern for agents&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MCP Security Bench shows models with better instruction-following are MORE vulnerable to prompt injection via tool outputs&lt;/li&gt; &lt;li&gt;The &amp;quot;capability-vulnerability paradox&amp;quot; - something to consider if you're building local agents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;123 papers on hallucination, 125 on factuality&lt;/li&gt; &lt;li&gt;Still unsolved but heavily researched&lt;/li&gt; &lt;li&gt;One interesting approach treats it as retrieval grounding rather than generation problem&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are your thoughts on the trend? Noticed anything interesting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dippatel21"&gt; /u/dippatel21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsh7dz/analyzed_5357_iclr_2026_accepted_papers_heres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsh7dz/analyzed_5357_iclr_2026_accepted_papers_heres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsh7dz/analyzed_5357_iclr_2026_accepted_papers_heres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T23:03:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjqdl</id>
    <title>Are small models actually getting more efficient?</title>
    <updated>2026-02-01T00:49:26+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;’m trying to understand whether small models (say, sub-1 GB or around that range) are genuinely getting &lt;em&gt;smarter&lt;/em&gt;, or if hard size limits mean they’ll always hit a ceiling.&lt;/p&gt; &lt;p&gt;My long-term hope is that we eventually see a small local model reach something close to &lt;strong&gt;Gemini 2.5–level reasoning&lt;/strong&gt;, at least for constrained tasks. The use case I care about is games: I’d love to run an LLM locally inside a game to handle logic, dialogue, and structured outputs.&lt;/p&gt; &lt;p&gt;Right now my game depends on an API model (Gemini 3 Flash). It works great, but obviously that’s not viable for selling a game long-term if it requires an external API.&lt;/p&gt; &lt;p&gt;So my question is:&lt;br /&gt; Do you think we’ll see, in the not-too-distant future, a &lt;strong&gt;small local model&lt;/strong&gt; that can reliably:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate strict JSON&lt;/li&gt; &lt;li&gt;Reason at roughly Gemini 3 Flash levels (or close)&lt;/li&gt; &lt;li&gt;Handle large contexts (ideally 50k–100k tokens)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Or are we fundamentally constrained by model size here, with improvements mostly coming from scale rather than efficiency?&lt;/p&gt; &lt;p&gt;Curious to hear thoughts from people following quantization, distillation, MoE, and architectural advances closely.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjqdl/are_small_models_actually_getting_more_efficient/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjqdl/are_small_models_actually_getting_more_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjqdl/are_small_models_actually_getting_more_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T00:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qssxhx</id>
    <title>Research: vllm-mlx on Apple Silicon achieves 21% to 87% higher throughput than llama.cpp</title>
    <updated>2026-02-01T08:26:21+00:00</updated>
    <author>
      <name>/u/Synor</name>
      <uri>https://old.reddit.com/user/Synor</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synor"&gt; /u/Synor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2601.19139v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qssxhx/research_vllmmlx_on_apple_silicon_achieves_21_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qssxhx/research_vllmmlx_on_apple_silicon_achieves_21_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T08:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsenpy</id>
    <title>Don’t buy b60 for LLMs</title>
    <updated>2026-01-31T21:21:10+00:00</updated>
    <author>
      <name>/u/damirca</name>
      <uri>https://old.reddit.com/user/damirca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kinda regret buying b60. I thought that 24gb for 700 eur is a great deal, but the reality is completely different.&lt;/p&gt; &lt;p&gt;For starters, I live with a custom compiled kernel with the patch from an Intel dev to solve ffmpeg crashes.&lt;/p&gt; &lt;p&gt;Then I had to install the card into a windows machine in order to get GPU firmware updated (under Linux one need v2.0.19 of fwupd which is not available in Ubuntu yet) to solve the crazy fan speed on the b60 even when the temp of the gpu is 30 degrees Celsius.&lt;/p&gt; &lt;p&gt;But even after solving all of this, the actual experience doing local LLM on b60 is meh.&lt;/p&gt; &lt;p&gt;On llama.cpp the card goes crazy every time it does inference: fans go super high then low, the high again. The speed is about 10-15tks at best in models like mistral 14b. The noise level is just unbearable.&lt;/p&gt; &lt;p&gt;So the only reliable way is intel’s llm-scaler, but as of now it’s based on vllm 0.11.1 whereas latest version of vllm is 0.15. So Intel is like 6 months behind which is an eternity in this AI bubble times. For example any of new mistral models are not supported and one cannot run them on vanilla vllm too.&lt;/p&gt; &lt;p&gt;With llm-scaler the behavior of the card is ok: when it’s doing inference the fan goes louder and stays louder as long is it’s needed. The speed is like 20-25 tks on qwen3 VL 8b. However there are only some models that work with llm-scaler and most of them only with fp8, so for example qwen3 VL 8b after some requests processed with 16k length takes 20gb. That kinda bad: you have 24gb of vram but you cannot run normally 30b model with q4 quant and has to stick with 8b model with fp8.&lt;/p&gt; &lt;p&gt;Overall I think XFX 7900XTX would have been much better deal: same 24gb, 2x faster, in Dec the price was only 50 eur more than b60, it can run newest models with newest llama.cpp versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/damirca"&gt; /u/damirca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T21:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsrscu</id>
    <title>Can 4chan data REALLY improve a model? TURNS OUT IT CAN!</title>
    <updated>2026-02-01T07:20:46+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt; &lt;img alt="Can 4chan data REALLY improve a model? TURNS OUT IT CAN!" src="https://a.thumbs.redditmedia.com/kH2rOREckIffGxPYl8Dxt7JwePnVvjX39wCJzfAooO0.jpg" title="Can 4chan data REALLY improve a model? TURNS OUT IT CAN!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hear me out, no one (really) knows how these things work.&lt;/p&gt; &lt;p&gt;A few days ago, I released &lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B"&gt;Assistant_Pepe_8B&lt;/a&gt;, you can read the discussion in &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;this thread&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I trained it on an extended &lt;strong&gt;4chan dataset&lt;/strong&gt;, on an abliterated base, but what I didn't expect was to get this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457"&gt;https://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a3bby1yd1ugg1.png?width=2980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f050bbd512a12a359626af79ccebcd2d2445877"&gt;https://preview.redd.it/a3bby1yd1ugg1.png?width=2980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f050bbd512a12a359626af79ccebcd2d2445877&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Somehow, &lt;strong&gt;against all common sense&lt;/strong&gt;, the model &lt;strong&gt;outperformed&lt;/strong&gt; nvidia's nemotron, the base it was trained on. This is usually the other way around. You take a smart base, tune a model on it, and accept the sacrifice of some intelligence to give it flavor.&lt;/p&gt; &lt;p&gt;At first I thought &amp;quot;OK nice, a coincidence, who cares?&amp;quot;&lt;/p&gt; &lt;p&gt;But then I looked more closely at the scores:&lt;/p&gt; &lt;p&gt;1) The abliterated base &lt;strong&gt;scored higher&lt;/strong&gt; than the base.&lt;br /&gt; 2) The finetune scored even &lt;strong&gt;higher than both&lt;/strong&gt;.&lt;br /&gt; 3) The finetune was literally on an extremely noise 4chan dataset, it should have eaten glue.&lt;/p&gt; &lt;p&gt;And then I remembered something: the original, gpt4chan (by Yannic Kilcher) scored especially high in truthfulness (that was b4 benchmaxxing).&lt;/p&gt; &lt;p&gt;So I took a closer look on recent models I released; the abliterated Impish_LLAMA_4B not only outperformed the base tune (the unabliterated one), it also changed its political alignment (you can check for yourself the UGI stats, I feel like I spammed enough images). &lt;/p&gt; &lt;p&gt;People were initially joking about the &amp;quot;alignment tax&amp;quot;, I think there's a none trivial substance in all of this. It seems to me just above a marginal error or statistical noise.&lt;/p&gt; &lt;p&gt;Oh, and the KL divergence for Impish_LLAMA_4B was :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;0.01 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T07:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsn78m</id>
    <title>Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site</title>
    <updated>2026-02-01T03:25:12+00:00</updated>
    <author>
      <name>/u/georgemoore13</name>
      <uri>https://old.reddit.com/user/georgemoore13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"&gt; &lt;img alt="Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site" src="https://external-preview.redd.it/bfjA2IIU81Nyg02gojI5OSsrZO__DkXhWMj0JjL44MA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b3c6a4dfe6f5b662bd4fcb95ee7ed9af6ee839f" title="Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/georgemoore13"&gt; /u/georgemoore13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/exposed-moltbook-database-let-anyone-take-control-of-any-ai-agent-on-the-site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T03:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
