<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-08T08:54:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ltze9d</id>
    <title>Do you use prompt caching to save chat history in your LLM apps?</title>
    <updated>2025-07-07T16:53:44+00:00</updated>
    <author>
      <name>/u/Physical_Ad9040</name>
      <uri>https://old.reddit.com/user/Physical_Ad9040</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to hear from others building LLM-based chat apps: Do you implement &lt;strong&gt;prompt caching&lt;/strong&gt; to store chat history or previous responses? Or do you send the chat history with each user's prompt?&lt;/p&gt; &lt;p&gt;Caching is more expensive to write, but the costs are then net positive if the conversation becomes long, no?&lt;/p&gt; &lt;p&gt;Would appreciate your insights ‚Äî thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Ad9040"&gt; /u/Physical_Ad9040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T16:53:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lui2hs</id>
    <title>Which model has good user interface in results part?</title>
    <updated>2025-07-08T07:05:54+00:00</updated>
    <author>
      <name>/u/01101110111motiv</name>
      <uri>https://old.reddit.com/user/01101110111motiv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;I am using Perplexity but I do not like its user interface in results section. There is no bullets or headings in explanation. Which models can I use for nice look that separates results in nice sections and headings and bullet points? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/01101110111motiv"&gt; /u/01101110111motiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lui2hs/which_model_has_good_user_interface_in_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lui2hs/which_model_has_good_user_interface_in_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lui2hs/which_model_has_good_user_interface_in_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T07:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lui7nc</id>
    <title>Prompt House: A prompt manager that connects directly to your AI tools</title>
    <updated>2025-07-08T07:15:48+00:00</updated>
    <author>
      <name>/u/Dull-Interview2947</name>
      <uri>https://old.reddit.com/user/Dull-Interview2947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'd like to share an AI tool I built called Prompt House. It's a prompt manager designed to make your AI workflow faster and more seamless.&lt;/p&gt; &lt;p&gt;The main goal is to eliminate the endless cycle of copy-pasting prompts. It uses the MCP to allow your AI clients to programmatically find and use the perfect prompt from your personal library.&lt;/p&gt; &lt;p&gt;Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Manage Your Prompts: A straightforward interface to save, tag, and organize your entire prompt collection.&lt;/li&gt; &lt;li&gt;Direct AI Client Integration: Connects with tools like Claude Desktop, Cursor, ChatWise, and Cherry Studio to fetch prompts automatically.&lt;/li&gt; &lt;li&gt;Prompt Recommendations: Explore a built-in collection of high-quality prompts for productivity and image generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're a heavy user of AI tools on your Mac and want to bring some order to your prompts, there is also a macOS version. It includes all the features above, plus a few key advantages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Privacy-First by Design: The app works fully offline. All your data is stored locally on your Mac. No accounts or sign-ups needed.&lt;/li&gt; &lt;li&gt;Local AI Support: Features native support for major Model Providers and local inference with Ollama.&lt;/li&gt; &lt;li&gt;One-Click Connection: You can connect the app with Claude Desktop with just a single click.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can find Prompt House at the official site: &lt;a href="https://prompthouse.app/"&gt;https://prompthouse.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope you find it helpful. Any feedback is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Interview2947"&gt; /u/Dull-Interview2947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lui7nc/prompt_house_a_prompt_manager_that_connects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lui7nc/prompt_house_a_prompt_manager_that_connects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lui7nc/prompt_house_a_prompt_manager_that_connects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T07:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1luia44</id>
    <title>Law training</title>
    <updated>2025-07-08T07:20:34+00:00</updated>
    <author>
      <name>/u/IvAx358</name>
      <uri>https://old.reddit.com/user/IvAx358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm dreaming of a tool that can help the individual citizen to defend himself. Of course for big complex legal cases the human cannot be replaced.&lt;/p&gt; &lt;p&gt;To the technique: Any good experience with a local LLM that can be retrained with local law?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IvAx358"&gt; /u/IvAx358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luia44/law_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luia44/law_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luia44/law_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T07:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1luig63</id>
    <title>Whisper-large-v3 on VertexAI not supporting return_timstamps: True ?</title>
    <updated>2025-07-08T07:31:53+00:00</updated>
    <author>
      <name>/u/Worldly-Side9489</name>
      <uri>https://old.reddit.com/user/Worldly-Side9489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent hours banging my head on the wall (and chatGPT and gemini not helping)&lt;/p&gt; &lt;p&gt;I deployed Whisper-large-v3 in Vertex AI GCP:&lt;/p&gt; &lt;p&gt;&lt;a href="https://console.cloud.google.com/vertex-ai/publishers/openai/model-garden/whisper-large"&gt;https://console.cloud.google.com/vertex-ai/publishers/openai/model-garden/whisper-large&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Vibe coded a script:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def transcribe_long_audio_correctly( project_id: str, region: str, endpoint_id: str, gcs_uri: str ): try: aiplatform.init(project=project_id, location=region) endpoint = aiplatform.Endpoint(endpoint_name=endpoint_id) instances = [ {&amp;quot;audio&amp;quot;: gcs_uri} ] # The &amp;quot;parameters&amp;quot; dictionary contains instructions for the model. parameters = { # &amp;quot;return_timestamps&amp;quot;: True &amp;lt;-- THE PROBLEMATIC LINE } print(&amp;quot;Sending request with correct parameters...&amp;quot;) prediction = endpoint.predict(instances=instances, parameters=parameters) print(&amp;quot;\n‚úÖ Transcription Result:&amp;quot;) print(prediction.predictions[0]) except Exception as e: print(f&amp;quot;An error occurred: {e}&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It works find for &amp;lt;30 second files but fails with 500 error for &amp;gt; 30 second files.&lt;/p&gt; &lt;p&gt;The solution apparently is to specify &amp;quot;return_timestamp&amp;quot;: True but when I uncommend &amp;quot;THE PROBLEMATIC LINE&amp;quot; above it hates it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Status code:422, response: { &amp;quot;deployedModelId&amp;quot; : &amp;quot;2999469919596183552&amp;quot;, &amp;quot;detail&amp;quot; : [ { &amp;quot;ctx&amp;quot; : { &amp;quot;error&amp;quot; : {} }, &amp;quot;input&amp;quot; : { &amp;quot;return_timestamps&amp;quot; : true }, &amp;quot;loc&amp;quot; : [ &amp;quot;body&amp;quot;, &amp;quot;parameters&amp;quot; ], &amp;quot;msg&amp;quot; : &amp;quot;Value error, Invalid return_timestamps: True&amp;quot;, &amp;quot;type&amp;quot; : &amp;quot;value_error&amp;quot; } ], &amp;quot;model&amp;quot; : &amp;quot;projects/.../locations/europe-west2/models/openai_whisper-large-v3-1751953849719&amp;quot;, &amp;quot;modelDisplayName&amp;quot; : &amp;quot;openai_whisper-large-v3-1751953849719&amp;quot;, &amp;quot;modelVersionId&amp;quot; : &amp;quot;1&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It definitely recognizes the &amp;quot;return_timestamps&amp;quot;: parameter eg. renaming it with a typo it ignores it. &lt;/p&gt; &lt;p&gt;I cannot understand how to set the correct &amp;quot;return_timestamps&amp;quot; parameter value - tried True, &amp;quot;True&amp;quot;, &amp;quot;true&amp;quot;, 1, 2, 0, &amp;quot;quack&amp;quot; and everything. It seems to require a Truthy string or bool but when i give it a valid value I get th errror above.&lt;/p&gt; &lt;p&gt;heeeeelllpp???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly-Side9489"&gt; /u/Worldly-Side9489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luig63/whisperlargev3_on_vertexai_not_supporting_return/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luig63/whisperlargev3_on_vertexai_not_supporting_return/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luig63/whisperlargev3_on_vertexai_not_supporting_return/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T07:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1luaket</id>
    <title>No "attach" button in Jan</title>
    <updated>2025-07-08T00:19:57+00:00</updated>
    <author>
      <name>/u/SensitiveDisk0</name>
      <uri>https://old.reddit.com/user/SensitiveDisk0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm feeling super stupid to ask this, but since the last versions of Jan the attachment (üìé) button has disappeared. Is it only me experiencing this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveDisk0"&gt; /u/SensitiveDisk0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luaket/no_attach_button_in_jan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luaket/no_attach_button_in_jan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luaket/no_attach_button_in_jan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T00:19:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lthtbn</id>
    <title>8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top.</title>
    <updated>2025-07-07T01:35:48+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt; &lt;img alt="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." src="https://b.thumbs.redditmedia.com/YH48KR3uSeLFmipFEDX0ai8FZ4TwQmmccEKSaaUx3Fk.jpg" title="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was working on a &lt;a href="https://www.designarena.ai/"&gt;research&lt;/a&gt; project (note that the votes and data is completely free and open, so not profiting off this, but just showing research as context) where users write a prompt, and then vote on content generated (e.g. websites, games, 3D visualizations) from 4 randomly generated models each. Note that when &lt;a href="https://www.designarena.ai/vote"&gt;voting&lt;/a&gt;, model names are hidden, so people don't immediately know which models generated what. &lt;/p&gt; &lt;p&gt;From the data collected so far, Llama 4 Maverick is 19th and Llama 4 Scout is 23rd. On the other extreme, Claude and Deepseek are taking up most of the spots in the top 10 while Mistral and Grok have been surprising dark horses. &lt;/p&gt; &lt;p&gt;Anything surprise you here? What models have you noticed been the best for UI/UX and frontend development? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lthtbn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T01:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu818k</id>
    <title>Locally run TTS Models</title>
    <updated>2025-07-07T22:26:57+00:00</updated>
    <author>
      <name>/u/Tankerspam</name>
      <uri>https://old.reddit.com/user/Tankerspam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I'm not familiar with coding in general and have been banging my head against chatGPT and online tutorials trying to make things such as Tortoise-TTS work, but it's so out of date that ChatGPT can't help me install it because of the amount of deprecation and I just don't know what I'm doing.&lt;/p&gt; &lt;p&gt;Does anyone have a simple, easy to use, preferably GUI TTS that is simple to install?&lt;/p&gt; &lt;p&gt;I thought bark_win might work, but nope, the 1 click installer doesn't download all the packages and after attempting to install them it still won't run. I'm not skilled enough in this area to figure this out. I'm trying to TTS Univeristy readings so I can listen to them.&lt;/p&gt; &lt;p&gt;Won't lie it's been incredibly frustrating, I spent literally 8 hours yesterday trying to make tortoise-tts work. (Well actually it would run, but has a word limit of each run, and won't save the hash for the AI model it generates between runs, so to TTS a reading would take a solid day of me sitting there babying it.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tankerspam"&gt; /u/Tankerspam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu818k/locally_run_tts_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu818k/locally_run_tts_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu818k/locally_run_tts_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T22:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1luj1cb</id>
    <title>Agents or Workflows? 150 practitioners voted on the difference</title>
    <updated>2025-07-08T08:11:53+00:00</updated>
    <author>
      <name>/u/htahir1</name>
      <uri>https://old.reddit.com/user/htahir1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luj1cb/agents_or_workflows_150_practitioners_voted_on/"&gt; &lt;img alt="Agents or Workflows? 150 practitioners voted on the difference" src="https://b.thumbs.redditmedia.com/PkfIcf53wfozBCMVX7gbVkw_y37GHji6XXhXSrV45lM.jpg" title="Agents or Workflows? 150 practitioners voted on the difference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the &amp;quot;Building Effective Agents&amp;quot; by &lt;a href="https://www.linkedin.com/in/ACoAACP4kpQBq4IDzWseB6tuTC-RTW-jCb-M07s"&gt;Barry Zhang&lt;/a&gt; and &lt;a href="https://www.linkedin.com/in/ACoAAAtfQ_QBjPrLyG8XiNOkkVulx0mtDjPRBFs"&gt;Erik Schluntz&lt;/a&gt;, the industry has been ripe with discussion about what a workflow is vs what an agent is. &lt;/p&gt; &lt;p&gt;I was genuinely curious to gauge where we are at in mid-2025. So a few weeks ago, I created a quiz app (&lt;a href="https://agents-vs-workflows.streamlit.app/"&gt;https://agents-vs-workflows.streamlit.app/&lt;/a&gt;) that asked the community about the difference between an agent and a workflow, with code snippets from the &amp;quot;Building Effective Agents&amp;quot; article in Python.&lt;/p&gt; &lt;p&gt;150 people voted! Real builders and practitioners. Know how they agreed? Not a whole lot. &lt;/p&gt; &lt;p&gt;For the snippet below (the &amp;quot;orchestrator-worker&amp;quot; pattern):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7rm7u21tylbf1.png?width=1666&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d6ff0a2107867a2789ec2954c3352959d77c63"&gt;https://preview.redd.it/7rm7u21tylbf1.png?width=1666&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d6ff0a2107867a2789ec2954c3352959d77c63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There was only a 60% agreement, with almost half of the people calling it a workflow and the other half an agent.&lt;/p&gt; &lt;p&gt;The evaluator-optimizer pattern was even worse&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3giam26uylbf1.png?width=1935&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=964c6c26af2b5715fa1ddfd539e6d2c249f3fae8"&gt;https://preview.redd.it/3giam26uylbf1.png?width=1935&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=964c6c26af2b5715fa1ddfd539e6d2c249f3fae8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;with a 58% disagreement.&lt;/p&gt; &lt;p&gt;What do you think? Some people might say this is a pedantic discussion, but for me, it seems clear there is still confusion as to how should be using LLM's to build software. If we can‚Äôt align on terminology, hand-offs between teams, vendors, and open-source projects get messy. Also, most people tend to go &amp;quot;full agent&amp;quot; super quick nowadays, without making the trade-off of adding a bit of determinism to the mix.&lt;/p&gt; &lt;p&gt;Reference:&lt;/p&gt; &lt;p&gt;* The code for the app: &lt;a href="https://github.com/zenml-io/agents-vs-workflows"&gt;https://github.com/zenml-io/agents-vs-workflows&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* The results visualized in an app: &lt;a href="https://agents-vs-workflows-results.streamlit.app/"&gt;https://agents-vs-workflows-results.streamlit.app/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;* The quiz itself, if you're interested to take it: &lt;a href="https://agents-vs-workflows.streamlit.app/"&gt;https://agents-vs-workflows.streamlit.app/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/htahir1"&gt; /u/htahir1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luj1cb/agents_or_workflows_150_practitioners_voted_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luj1cb/agents_or_workflows_150_practitioners_voted_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luj1cb/agents_or_workflows_150_practitioners_voted_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T08:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu4t37</id>
    <title>Octominer + P102-100 build... worth it?</title>
    <updated>2025-07-07T20:18:26+00:00</updated>
    <author>
      <name>/u/UsualResult</name>
      <uri>https://old.reddit.com/user/UsualResult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just for luls I was looking at some of the &amp;quot;Octominer&amp;quot; boards available. I thought it would be a fun build to get like 8x P104-100 / P102-100 and load one up.&lt;/p&gt; &lt;p&gt;However, they mostly have something wimpy for CPU... like a dual core Celeron or similar. Will that kill any possible chance of fun on a build like that because certain things need to get handled by the CPU?&lt;/p&gt; &lt;p&gt;I was curious because there are a lot of Octominers floating around for $200 - $300 and it seems like it's an easy way to host a lot of cards.&lt;/p&gt; &lt;p&gt;I have a box with dual P104-100 and it's been fun to play around with but it has a new(ish) i5 to work with. I can run 7b-13b models with &amp;quot;acceptable&amp;quot; speed but it would be neat to be able to bring that up to 30b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UsualResult"&gt; /u/UsualResult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu4t37/octominer_p102100_build_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu4t37/octominer_p102100_build_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu4t37/octominer_p102100_build_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T20:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lugdls</id>
    <title>How to train lora on 5090</title>
    <updated>2025-07-08T05:17:19+00:00</updated>
    <author>
      <name>/u/Adventurous_Rise_683</name>
      <uri>https://old.reddit.com/user/Adventurous_Rise_683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. Still new to llm world. Would like to train and use my own lora. What's the best way to go about it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous_Rise_683"&gt; /u/Adventurous_Rise_683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lugdls/how_to_train_lora_on_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lugdls/how_to_train_lora_on_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lugdls/how_to_train_lora_on_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T05:17:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lts4wd</id>
    <title>Inside Google Gemma 3n: my PyTorch Profiler insights</title>
    <updated>2025-07-07T11:51:50+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt; &lt;img alt="Inside Google Gemma 3n: my PyTorch Profiler insights" src="https://external-preview.redd.it/iyG6eCUPhSylmQBjhmsazmQyUh0CUb3n-N54OyLJmm0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25644e360804c2abbce6ff1e1b66b61cd330cee0" title="Inside Google Gemma 3n: my PyTorch Profiler insights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;If you‚Äôve ever wondered what really happens inside modern vision-language models, here‚Äôs a hands-on look. I profiled the Google Gemma 3n model on an NVIDIA GPU using PyTorch Profiler, asking it to describe a &lt;a href="https://cdn-lfs.hf.co/datasets/huggingface/documentation-images/8b21ba78250f852ca5990063866b1ace6432521d0251bde7f8de783b22c99a6d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bee.jpg%3B+filename%3D%22bee.jpg%22%3B&amp;amp;response-content-type=image%2Fjpeg&amp;amp;Expires=1751892238&amp;amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTg5MjIzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy84YjIxYmE3ODI1MGY4NTJjYTU5OTAwNjM4NjZiMWFjZTY0MzI1MjFkMDI1MWJkZTdmOGRlNzgzYjIyYzk5YTZkP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&amp;amp;Signature=FWMAYJoqhsk9AHs1%7EyIoOHBmh53A16J6Xyj-vhFVXTW%7EFkL2tRptgpALUSWppQKXjCnJZsnMXtDFcZAvDm-PFgQaK3UycJD%7ElNShdj5yopPA2F5U2gT4wEvXc-AibMF5mUrzeNKxfY56CjsiFWCfKczLZKzV-kfrXZu7t60d4o5ZdY6jmkdeMHMkYmLROTFE-tmPiKqmN7jVcMIdW43xmaEvova9oA4akIqKphaQUUvvVTToqPjILfn2LLhqwH5BgnbAE5OZ9DtreQirvzS75Xhkgi8GN7LEyrX2nt7LSYtS2vv1SfeSmWca8MY0eO7KEqF71jyA5DquPofRkEEesQ__&amp;amp;Key-Pair-Id=K3RPWS32NSSJCE"&gt;bee image&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I visualized the profiling results using &lt;a href="https://ui.perfetto.dev/"&gt;https://ui.perfetto.dev/&lt;/a&gt;, as shown in the animated GIF below:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/frlijkwkwfbf1.gif"&gt;https://i.redd.it/frlijkwkwfbf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Along the way, I captured and analyzed the key inference phases, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image feature extraction&lt;/strong&gt; with MobileNetV5 (74 msec) - the trace shows the &lt;code&gt;get_image_features&lt;/code&gt; function of Gemma3n (&lt;a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma3n/modular_gemma3n.py#L2253"&gt;source&lt;/a&gt;), which then calls &lt;code&gt;forward_features&lt;/code&gt; in MobileNetV5 (&lt;a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/mobilenetv5.py#L535"&gt;source&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/afzke1tdxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=899a055b776818546205514b3d9e29fe7dee38cd"&gt;https://preview.redd.it/afzke1tdxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=899a055b776818546205514b3d9e29fe7dee38cd&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Text decoding&lt;/strong&gt; through a stack of Gemma3nTextDecoderLayer layers (142 msec) - a series of &lt;code&gt;Gemma3nTextDecoderLayer&lt;/code&gt; (&lt;a href="https://github.com/huggingface/transformers/blob/ca7e1a3756c022bf31429c452b2f313f043f32de/src/transformers/models/gemma3n/modular_gemma3n.py#L1829"&gt;source&lt;/a&gt;) calls. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hlcdthfxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=833ae582e5eb759a1eba9adbca1841deeba07195"&gt;https://preview.redd.it/6hlcdthfxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=833ae582e5eb759a1eba9adbca1841deeba07195&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token generation&lt;/strong&gt; with per-token execution broken down to kernel launches and synchronizations (244 msec total for 10 tokens, ~24 msec per token) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xzoilykgxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16f504610e8821d686d63aa83e255a4feb8dfd60"&gt;https://preview.redd.it/xzoilykgxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16f504610e8821d686d63aa83e255a4feb8dfd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve shared the full code, profiling scripts, and raw trace data, so you can dive in, reproduce the results, and explore the model‚Äôs internals for yourself.&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/sbnb-io/gemma3n-profiling/"&gt;https://github.com/sbnb-io/gemma3n-profiling/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôre looking to better understand how these models run under the hood, this is a solid place to start. Happy to hear your thoughts or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T11:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltubvs</id>
    <title>Jamba 1.7 - a ai21labs Collection</title>
    <updated>2025-07-07T13:35:12+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"&gt; &lt;img alt="Jamba 1.7 - a ai21labs Collection" src="https://external-preview.redd.it/T-WGV8JGl5ddvynFCnHkV0GApDuiD0OUmPGVN858nB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35b8dfb877220fc5dfb06c711e11e9b9d474f083" title="Jamba 1.7 - a ai21labs Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ai21labs/jamba-17-68653e9be386dc69b1f30828"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T13:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lubdcg</id>
    <title>So, does anyone have a good workflow to replace google search yet?</title>
    <updated>2025-07-08T00:58:08+00:00</updated>
    <author>
      <name>/u/DepthHour1669</name>
      <uri>https://old.reddit.com/user/DepthHour1669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As everyone knows, google search has been getting worse the past few years. ChatGPT with web search enabled has become a big tool that is replacing Google for me. &lt;/p&gt; &lt;p&gt;Here are some example queries: &lt;/p&gt; &lt;p&gt;&lt;a href="https://chatgpt.com/share/686c6b7d-099c-8012-915b-71e2e2e67a06"&gt;&amp;quot;List the median, 25th/75th percentile MCAT scores for medical schools in California in a table. Sort by rank.&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chatgpt.com/share/686c6bff-4594-8012-a0ea-ac5091cc621d"&gt;&amp;quot;What has happened in the war between Israel and Iran in the past week?&amp;quot;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;ChatGPT's responses are pretty good. It's a lot easier than googling and compiling the information yourself. The responses are &lt;strong&gt;even better- basically perfect- if you use o3 or o4-mini&lt;/strong&gt;, but I don't have a Plus account and prefer to use the API. Using o4-mini with my brother's account literally saves me so much time google searching already.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;So... can we replicate this locally? Maybe use Qwen 32b with a good system prompt, and have Serper to do google search API, and then some way of loading the pages in the results into context? Has anyone tried to build such a system that works similarly smoothly as how ChatGPT the product works?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepthHour1669"&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T00:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lubunz</id>
    <title>Chrome now includes a built-in local LLM, I built a wrapper to make the API easier to use</title>
    <updated>2025-07-08T01:20:50+00:00</updated>
    <author>
      <name>/u/kuaythrone</name>
      <uri>https://old.reddit.com/user/kuaythrone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubunz/chrome_now_includes_a_builtin_local_llm_i_built_a/"&gt; &lt;img alt="Chrome now includes a built-in local LLM, I built a wrapper to make the API easier to use" src="https://external-preview.redd.it/sp3umckXVxqL0xC9QHfq1Qvl1z_m3teqOXRXzjGhY2E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f78fcdb19a156bc527af711a1b2b011dea452f04" title="Chrome now includes a built-in local LLM, I built a wrapper to make the API easier to use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chrome now includes a native on-device LLM (Gemini Nano) starting in version 138 for extensions. I've been building with it for a while and excited that its finally made it into the latest version of Chrome. It‚Äôs powerful, but the official Prompt API can be a bit awkward to use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enforces sessions even for basic usage&lt;/li&gt; &lt;li&gt;Requires user-triggered downloads&lt;/li&gt; &lt;li&gt;Lacks type safety or structured error handling&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I open-sourced a small TypeScript wrapper I originally built for other projects to smooth over some rough edges:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;github: &lt;a href="https://github.com/kstonekuan/simple-chromium-ai"&gt;https://github.com/kstonekuan/simple-chromium-ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;npm: &lt;a href="https://www.npmjs.com/package/simple-chromium-ai"&gt;https://www.npmjs.com/package/simple-chromium-ai&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stateless prompt() method inspired by Anthropic's SDK&lt;/li&gt; &lt;li&gt;Built-in error handling and Result based .Safe.* variants&lt;/li&gt; &lt;li&gt;Token usage checks&lt;/li&gt; &lt;li&gt;Simple initialization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's intentionally minimal, ideal for hacking, prototypes, or playing with the new built-in AI without dealing with the full complexity.&lt;/p&gt; &lt;p&gt;For full control (e.g., streaming, memory management), use the official API: &lt;a href="https://developer.chrome.com/docs/ai/prompt-api"&gt;https://developer.chrome.com/docs/ai/prompt-api&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear feedback or see what people make with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuaythrone"&gt; /u/kuaythrone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kstonekuan/simple-chromium-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubunz/chrome_now_includes_a_builtin_local_llm_i_built_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lubunz/chrome_now_includes_a_builtin_local_llm_i_built_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T01:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu7lsi</id>
    <title>UI/UX Benchmark Update and Response: More Models, Updating Ranking, Open Data Soon</title>
    <updated>2025-07-07T22:09:06+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"&gt; &lt;img alt="UI/UX Benchmark Update and Response: More Models, Updating Ranking, Open Data Soon" src="https://b.thumbs.redditmedia.com/VkhU8Mt9acaQeJtSLndzIlRsVXJlfJ84thb8pJB8_6o.jpg" title="UI/UX Benchmark Update and Response: More Models, Updating Ranking, Open Data Soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, a few times on here I've been sharing progress on a &lt;a href="https://www.designarena.ai/"&gt;UI/UX benchmark&lt;/a&gt; that I have been working on with a small team. In particular, I made &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt;a post yesterday&lt;/a&gt; that gave us a ton of useful feedback so thank you to everyone that put in a comment and voted on our platform! I just wanted to address some concerns, provide some updates on what we are working on, and create an open discussion on how the benchmark can be improved. This post will be a bit long since I want to be as detailed as possible, but here we go:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; We released the benchmark just a few weeks ago (3 weeks ago I think?) and mostly it started out as an internal tool among my team since we were interested in the current UI/UX capabilities of LLMs and HCI and wanted to see which models are best at designing and implementing interfaces. We really just pushed the benchmark out initially as a fun side project to see what would happen, but really didn't forsee that we would get over 10K people on our site at some point! Our motivation here is that something like UI/UX data for AI seems that it will be heavily reliant on public opinion, rather than a deterministic benchmark or private evaluation. &lt;/p&gt; &lt;p&gt;As I said, we received a lot of very helpful feedback, and as we're still in very early early stages with developing the benchmark, we're really trying to do our best to make our benchmark as transparent and useful as possible. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;More Models and Voting Inconsistency:&lt;/strong&gt; Many people have noted that many premier models are missing such as GLM-4, Qwen, Gemini 2.5-Flash, etc. We are working on adding those and hope to add those models in the next couple of days and will update you all when those are added. I realize I have been saying that more models will be added for more than a few days now haha, but honestly we are a small team with not an infinite amount of money lol, so we're just waiting to get some more credits. I hope that makes sense and thank you for your patience! &lt;/p&gt; &lt;p&gt;Another comment we got is that the number of votes received for the different models are vastly different even though voting should be recruiting models at random. There are few reasons for this: (1) we added some models earlier (notably Claude when we were first developing the benchmark) and other models later (Mistral, Llama, etc.), (2) we did deactivate some models that became deprecated or because we ran out of credits (such as Llama which we're deploying on Vertex but we will add back) and (3) for slower models like DeepSeek, we do notice churn from voters in the sense that people won't wait for those models to finish generating all the time. &lt;/p&gt; &lt;p&gt;For (1) and (2) we will address by providing exact details on when we added each model and adding back models (assuming they are not deprecated) such as Llama. For (3), we have put some thought into this over the last few weeks but honestly not sure how exactly we should tackle this issue since this is a bit of a limitation of having a public crowdsource benchmark. We did get some suggestions to perhaps have some priority for models with fewer votes, but there is a correlation between having fewer votes and slower generation times, so we don't think there is an immediate fix there but we likely incorporate some kind of priority system. That said, we would appreciate any suggestions on (3)! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Voting Data:&lt;/strong&gt; To be clear, this is standard preference dataset that we collect when users do binary comparisons on our &lt;a href="https://www.designarena.ai/vote"&gt;voting page&lt;/a&gt;. We'll be releasing a preference dataset that can be accessed through Hugging Face and/or a REST API that will be updated periodically and that people can use to replicate the leaderboard. Note that the &lt;a href="https://www.designarena.ai/leaderboard"&gt;leaderboard page is currently being updated every hour&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;System Prompts and Model Configs:&lt;/strong&gt; We will also release these along with the preference dataset and make our current settings much more clear. You'll get full access to these configs, but for the we're asking each model (with the same sys prompt across the board) to create an interface using HTML/CSS/JS with some restrictions (to ensure sure the code is sandboxed as possible + allowing it to use specific libraries like ThreeJs for 3D viz, Tailwind, etc.). For model configs, we are setting temperature to 0.8. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tournaments:&lt;/strong&gt; This was more of an aesthetic choice on our part to make the voting process more interesting for the user and get more comparisons for the same prompt across models. We'll also provide exact details on how these are being constructed, but the idea is that we're recruiting X number of models that are each being voted on in a group. We have had too kind of tournament structures. In the first, we would serve two models, have a user vote, and then continually have the winner go against the next served model. We decided to change this structure because we weren't able to compare losers in the bracket. For the current tournament system, we have two models A and B go against each other and then two other models C and D go against each other in round 1. Then the winners from the first round and losers from the last round go against each other. After that the loser in the winners' bracket will go against the winner in the losers' bracket to decide 2nd and 3rd place. We don't think this structure is necessarily perfect, but just more of an aesthetic choice so people could see different models at the same time in a grouping. We acknowledge that with the preference data, you could certainly structure the tournament data differently and our tournament structure shouldn't be considered as the absolute &amp;quot;correct&amp;quot; one. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack Ranking/Leaderboard:&lt;/strong&gt; This is where we acknowledge that there's certainly room for improvement here on how we can construct the leaderboard based on the preference data. Some of the concerns raised we did think about briefly in the past, but will certainly take more time to consider what's the best kind of ranking. Right now though, we have a ranking by win rate, and then an &amp;quot;Elo&amp;quot; score (which we're using an approximate formula based on win rate for which you can find at the bottom of the &lt;a href="https://www.designarena.ai/leaderboard"&gt;leaderboard&lt;/a&gt;). A concern raised that is relevant to what was said above is that the number of votes a model has does have an effect on the placement in the leaderboard. We will probably add some way to weight win rate / elo score by number votes, and any suggestions on what would be the best stack ranking here would be appreciated! That said, I do think it might be good to not take the leaderboard as this definitive ranking, since one could construct their own different kind of leaderboards / rankings based on how they choose to structure the preference data, but more so treat it as a general &amp;quot;tier list&amp;quot; for the models. &lt;/p&gt; &lt;p&gt;Let us know what you think and if you have any questions in the comments! &lt;/p&gt; &lt;p&gt;Please also join our &lt;a href="https://discord.gg/5AagpZd5"&gt;Discord&lt;/a&gt; for the best way to message us directly. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lu7lsi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T22:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1luiigi</id>
    <title>[Tool Release] Finetune &amp; Quantize 1‚Äì3B LLMs on 8GB RAM using LoFT CLI (TinyLlama + QLoRA + llama.cpp)</title>
    <updated>2025-07-08T07:36:20+00:00</updated>
    <author>
      <name>/u/diptanshu1991</name>
      <uri>https://old.reddit.com/user/diptanshu1991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks ‚Äî I‚Äôve been working on a CLI tool called &lt;strong&gt;LoFT (Low-RAM Finetuning Toolkit)&lt;/strong&gt;, and I finally have a working release.&lt;/p&gt; &lt;h1&gt;üîß What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Finetunes open-source LLMs (1‚Äì3B) like &lt;strong&gt;TinyLlama&lt;/strong&gt; using &lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Runs entirely on &lt;strong&gt;CPU (MacBook Air 8GB RAM tested)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Quantizes to &lt;strong&gt;GGUF&lt;/strong&gt; format&lt;/li&gt; &lt;li&gt;Runs local inference via &lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All through a clean CLI (&lt;code&gt;finetune&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;quantize&lt;/code&gt;, &lt;code&gt;chat&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üíª Tech Stack:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;peft&lt;/code&gt;, &lt;code&gt;bitsandbytes&lt;/code&gt;, &lt;code&gt;datasets&lt;/code&gt;, &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;CLI-based interface built for reproducibility and minimal setup&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß† Why I built this:&lt;/h1&gt; &lt;p&gt;I wanted to see if it‚Äôs feasible to do &lt;strong&gt;end-to-end finetuning and deployment&lt;/strong&gt; of LLMs &lt;strong&gt;without a GPU or cloud setup&lt;/strong&gt; ‚Äî for indie hackers, researchers, or hobbyists working on local setups.&lt;/p&gt; &lt;p&gt;And surprisingly, it works.&lt;/p&gt; &lt;h1&gt;üõ†Ô∏è Coming Soon:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub repo (final touches being made)&lt;/li&gt; &lt;li&gt;Full walkthrough + demo&lt;/li&gt; &lt;li&gt;Support for multi-turn finetuning and inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any feedback from folks doing low-resource model work&lt;/li&gt; &lt;li&gt;Suggestions for models or datasets to support next&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to tag you once the repo is up.&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Diptanshu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diptanshu1991"&gt; /u/diptanshu1991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T07:36:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lue75q</id>
    <title>Day 11/50: Building a small language from scratch: Introduction to the Attention Mechanism in Large Language Models (LLMs)</title>
    <updated>2025-07-08T03:16:51+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lue75q/day_1150_building_a_small_language_from_scratch/"&gt; &lt;img alt="Day 11/50: Building a small language from scratch: Introduction to the Attention Mechanism in Large Language Models (LLMs)" src="https://b.thumbs.redditmedia.com/3xrs0rEfGKpFirEZfNvII0Y7stkFc5h_AVNkVSBnMPw.jpg" title="Day 11/50: Building a small language from scratch: Introduction to the Attention Mechanism in Large Language Models (LLMs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ya6uoxmoikbf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69253abb996cd2754a0835f4ada4f543826578ac"&gt;https://preview.redd.it/ya6uoxmoikbf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69253abb996cd2754a0835f4ada4f543826578ac&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Hello everyone! &lt;/h1&gt; &lt;p&gt;Welcome back to our journey through the ‚ÄúBuild Large Language Models from Scratch‚Äù series. So far, we‚Äôve spent a considerable amount of time in the first stage of this journey, laying the groundwork by focusing on data preparation and sampling.&lt;/p&gt; &lt;p&gt;We‚Äôve covered:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tokenization&lt;/li&gt; &lt;li&gt;Byte-Pair Encoding&lt;/li&gt; &lt;li&gt;Word and Positional Embeddings&lt;/li&gt; &lt;li&gt;Model distillation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Essentially, we‚Äôve now established a solid foundation for the data preprocessing pipeline. It‚Äôs time to move on to something that powers the very core of today‚Äôs Large Language Models (LLMs): The Attention Mechanism.&lt;/p&gt; &lt;h1&gt;Transformers: The Car, Attention: The Engine&lt;/h1&gt; &lt;p&gt;If you think of a Transformer as a car, then attention is its engine. Without it, the whole vehicle wouldn‚Äôt move the way we want it to.&lt;/p&gt; &lt;p&gt;You‚Äôve probably heard of ChatGPT, right? The impressive performance of modern large language models, including their ability to understand context, generate coherent text, and handle long-range dependencies, is primarily enabled by the attention mechanism. However, here‚Äôs the problem: most tutorials available online jump straight into multi-head attention, skipping over the intuition and basics.&lt;/p&gt; &lt;p&gt;So we‚Äôre going to take a different path. A deeper, gentler path.&lt;/p&gt; &lt;h1&gt;Why Do We Need Attention?&lt;/h1&gt; &lt;p&gt;Let‚Äôs motivate this with a simple example.&lt;/p&gt; &lt;p&gt;Imagine this sentence:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;‚Äú&lt;/em&gt;The book that the professor whom the students admired wrote became a bestseller&lt;em&gt;.‚Äù&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;As humans, we can parse this and understand:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚Äú&lt;/strong&gt;book&lt;strong&gt;‚Äù&lt;/strong&gt; is the subject&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚Äú&lt;/strong&gt;became&lt;strong&gt;‚Äù&lt;/strong&gt; is the verb&lt;/li&gt; &lt;li&gt;Everything else ‚Äî &lt;em&gt;‚Äúthat the professor whom the students admired wrote‚Äù&lt;/em&gt; ‚Äî is additional context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But for a model, this sentence is challenging. It contains nested clauses and long-term dependencies, meaning the model must track relationships between words that are far apart in the sequence.&lt;/p&gt; &lt;p&gt;The model needs to know:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The book is the thing that became a bestseller&lt;/li&gt; &lt;li&gt;The clauses in between provide important but secondary context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now imagine trying to do this with a simple model that reads one word at a time and only remembers the last few. It could easily get lost and focus too much on ‚Äúprofessor‚Äù or ‚Äústudents,‚Äù losing track of the main subject, the book, and the main action, becoming.&lt;/p&gt; &lt;p&gt;This is where the attention mechanism shines.&lt;/p&gt; &lt;p&gt;It allows the model to focus on the most relevant parts of the sentence dynamically, connecting ‚Äúbook‚Äù with ‚Äúbecame‚Äù while still incorporating the supporting context. This selective focus helps the model maintain a deeper understanding of the sentence‚Äôs meaning.&lt;/p&gt; &lt;p&gt;Without attention, models often struggle to preserve this context over longer spans of text, leading to confused or incoherent outputs.&lt;/p&gt; &lt;p&gt;This ability to dynamically focus on different words based on their relevance is what makes attention so powerful. Without it, models can lose track of meaning, especially in long sentences.&lt;/p&gt; &lt;h1&gt;The Four Flavors of Attention&lt;/h1&gt; &lt;p&gt;In upcoming lectures, we‚Äôll build the full attention stack step-by-step&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Simplified Self-Attention&lt;/strong&gt; ‚Äî Our starting point. Stripped-down, crystal-clear.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Attention&lt;/strong&gt; ‚Äî Adds learnable weights.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Causal Attention&lt;/strong&gt; ‚Äî Ensures the model only considers past tokens (not future ones).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Head Attention&lt;/strong&gt; ‚Äî Multiple attention heads process input in parallel.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Many tutorials start at step 4 and expect you to know already how to swim. We‚Äôll walk first, then run.&lt;/p&gt; &lt;h1&gt;Let‚Äôs Go Back in Time&lt;/h1&gt; &lt;p&gt;Before the advent of attention, there were Recurrent Neural Networks (RNNs). They were the dominant approach to sequence modeling, like translation.&lt;/p&gt; &lt;p&gt;Here‚Äôs how they worked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The encoder reads the input (say, a sentence in German).&lt;/li&gt; &lt;li&gt;The encoder compresses everything into a final hidden state (a ‚Äúsummary‚Äù of the whole sentence).&lt;/li&gt; &lt;li&gt;The decoder uses that to generate output (say, in English).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But here‚Äôs the problem‚Ä¶&lt;/p&gt; &lt;h1&gt;The RNN Bottleneck&lt;/h1&gt; &lt;p&gt;The decoder only sees one final hidden state. If the input is long, this becomes a massive problem.&lt;/p&gt; &lt;p&gt;Think of trying to summarize a whole book in one sentence, then answer questions about it. That‚Äôs what RNNs expected the model to do.&lt;/p&gt; &lt;h1&gt;Enter Attention: The 2014 Breakthrough&lt;/h1&gt; &lt;p&gt;In 2014, Bahdanau et al. proposed something revolutionary: Why not let the decoder access all the hidden states?&lt;/p&gt; &lt;p&gt;So, instead of relying on just the last hidden state, the decoder can now look back at every part of the input and decide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which words matter most?&lt;/li&gt; &lt;li&gt;How much ‚Äúattention‚Äù should I give to each word?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It was like giving the model memory superpowers ‚Äî and it worked wonders!&lt;/p&gt; &lt;h1&gt;Dynamic Focus: The Heart of Attention&lt;/h1&gt; &lt;p&gt;The core idea is called dynamic focus. For every word the model tries to generate, it can look back and weigh every input word differently.&lt;/p&gt; &lt;p&gt;Suppose the model is generating the word &lt;strong&gt;‚Äú&lt;/strong&gt;bestseller&lt;strong&gt;‚Äù&lt;/strong&gt;. With attention, it can do the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pay high attention to ‚Äúbook‚Äù, because that‚Äôs the subject that became the bestseller&lt;/li&gt; &lt;li&gt;Give moderate attention to ‚Äúwrote‚Äù, since it‚Äôs the action that connects the subject and the outcome&lt;/li&gt; &lt;li&gt;Assign less attention to ‚Äúprofessor‚Äù or ‚Äústudents‚Äù, which are part of supporting clauses but not central to this prediction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This ability to assign importance selectively is what allows attention mechanisms to handle long-range dependencies so well, something older architectures like RNNs struggled with.&lt;/p&gt; &lt;p&gt;Without this focused attention, the model might focus onto irrelevant parts of the sentence or lose track of the main subject entirely.&lt;/p&gt; &lt;h1&gt;Traditional vs. Self-Attention&lt;/h1&gt; &lt;h1&gt;Traditional Attention:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Focuses on relationships between two sequences&lt;/li&gt; &lt;li&gt;E.g., translating German to English&lt;/li&gt; &lt;li&gt;Aligning words across sequences&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Self-Attention:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Looks within a single sequence&lt;/li&gt; &lt;li&gt;E.g., predicting the next word in English&lt;/li&gt; &lt;li&gt;Determines which words relate to each other &lt;strong&gt;inside&lt;/strong&gt; the same sentence&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This shift is enormous, and it‚Äôs what powers GPT, BERT, and all modern LLMs.&lt;/p&gt; &lt;h1&gt;Recap: A Timeline of Attention&lt;/h1&gt; &lt;p&gt;We stand on over 40 years of hard-earned research.&lt;/p&gt; &lt;h1&gt;What‚Äôs Coming Next?&lt;/h1&gt; &lt;p&gt;In the next few blog posts, we‚Äôll:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Implement Simplified Self-Attention from Scratch&lt;/strong&gt; in Python&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Move to Self-Attention with trainable weights&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Introduce Causal Attention&lt;/strong&gt; for autoregressive modeling&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Build a Multi-Head Attention&lt;/strong&gt; layer-by-layer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why Learn Attention from Scratch?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yes, you can use libraries such as Transformers, LangChain, or FlashAttention. However, to truly master large language models, you need to understand how the engine operates under the hood.&lt;/p&gt; &lt;p&gt;That‚Äôs the goal of this series. And I promise ‚Äî it‚Äôs worth the effort.&lt;/p&gt; &lt;p&gt;Thanks for reading this far! ‚ù§Ô∏è&lt;/p&gt; &lt;p&gt;If this helped clarify the magic of attention, feel free to share it with your friends or comment your thoughts below.&lt;/p&gt; &lt;p&gt;Next stop: Simplified Self-Attention, from Theory to Code!&lt;/p&gt; &lt;p&gt;Stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lue75q/day_1150_building_a_small_language_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lue75q/day_1150_building_a_small_language_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lue75q/day_1150_building_a_small_language_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T03:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxsqh</id>
    <title>Qwen3-8B-BitNet</title>
    <updated>2025-07-07T15:53:44+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a decent Qwen3 BitNet model I trained with ~1B tokens using SYNTHETIC-1 data. BitNet Hunyuan A13B is training this week.&lt;br /&gt; &lt;a href="https://huggingface.co/codys12/Qwen3-8B-BitNet"&gt;model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1GT0GEyjzOQUiOI0tphvhiFDwUw-F6v7l?usp=sharing"&gt;notebook&lt;/a&gt; to try out the model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T15:53:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1luhmmi</id>
    <title>Bytedance releases new agentic coding assistant: Trae-Agent</title>
    <updated>2025-07-08T06:36:23+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"&gt; &lt;img alt="Bytedance releases new agentic coding assistant: Trae-Agent" src="https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcfc5a3088bfab4d6be53d66237a02b38cc2d358" title="Bytedance releases new agentic coding assistant: Trae-Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bytedance/trae-agent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T06:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lue5xt</id>
    <title>Qwen3-235B-Q2 running locally on my 64GB (DDR4) and 32GB VRAM machine</title>
    <updated>2025-07-08T03:15:11+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing some experiences here. Mostly vibes, but maybe someone will find this helpful:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 9 3950x (16c/32t)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU(s):&lt;/strong&gt; two Rx 6800's (2x16GB at ~520GB/s for 32GB total)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB 2700mhz DDR4 in dual channel &lt;/p&gt; &lt;p&gt;&lt;strong&gt;OS:&lt;/strong&gt; Ubuntu 24.04&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference Software:&lt;/strong&gt; Llama-CPP (llama-server specifically) built to use ROCm&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weights:&lt;/strong&gt; Qwen3-235b-a22b Q2 (Unsloth Quant), ~85GB. ~32GB into VRAM, 53GB to memory before context &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance (Speed):&lt;/strong&gt; Inference speed was anywhere from 4 to 6 tokens per second with 8K max context (have not tested much higher). I offload 34 layers to GPU. I tried offloading experts to CPU (which allowed me to set this to ~75 layers) but did not experience a speed boost of any sort.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speculative Decoding:&lt;/strong&gt; I tried using a few quants of Qwen3 0.6b, 1.7b, and 4b .. none had good accuracy and all slowed things down.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Intelligence:&lt;/strong&gt; I'm convinced this is the absolute best model that this machine can run, &lt;em&gt;but am diving deeper to determine if that's worth the speed penalty to my use cases&lt;/em&gt;. It beats the previous champs (Qwen3-32B larger quants, Llama 3.3 70B Q5) for sure, even at Western history/trivia (Llama usually has an unfair advantage over Qwen here in my tests), but not tremendously so. There is no doubt in my mind that this is the most intelligent LLM I can run shut off from the open web with my current hardware (before inviting my SSD and some insane wait-times into the equation..). The intelligence gain doesn't appear to be night-and-day, but the speed loss absolutely is.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; Vulkan briefly uses more VRAM on startup it seems. By the time I can get it to start using Vulkan (without crashing) I've sent so many layers back to CPU that it'd be impossible for it to keep up with ROCm in speed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vs Llama 4 Scout:&lt;/strong&gt; - Llama4 Scout fits IQ2XSS fully on GPU's and Q5 (!) on the same VRAM+CPU hybrid. It also inferences faster due to smaller experts. That's where the good news stops though. It's a complete win for Qwen3-235b to the point where I found IQ3 Llama 3.3 70B (fits neatly on GPU) better than it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt; - For memory/context constraints' sake, quantizing cache on a Q2 model meant that coding performance was pretty underwhelming. It'd produce great results, but usually large edits/scripts contained a silly mistake or syntax error somewhere. It was capable of reconciling it, but I wouldn't recommend using these weights for coding unless you're comfortable testing full FP16 cache.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thinking:&lt;/strong&gt; - All of the above impressive performance is from disabling thinking using &lt;code&gt;/no_think&lt;/code&gt; in the prompt. Thinking improves a lot of this, but like all Qwen3 models, this thing likes to think &lt;em&gt;A LOT&lt;/em&gt; (not quite QwQ level, but much more than deepseek or its distills) - and alas my patience could not survive that many thinking tokens at what would get down to 4 t/s&lt;/p&gt; &lt;h3&gt;Command Used&lt;/h3&gt; &lt;pre&gt;&lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 ./llama-server \ -m &amp;quot;${MODEL_PATH}&amp;quot; \ --ctx-size 8000 \ -v \ --split-mode row \ --gpu-layers 34 \ --flash-attn \ --host 0.0.0.0 \ --mlock \ --no-mmap \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --no-warmup \ --threads 30 \ --temp 0.7 \ --top-p 0.8 \ --top-k 20 \ --min-p 0 \ --tensor-split 0.47,0.53 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;-the awkward tensor split is to account for a bit of VRAM being used by my desktop environment. Without it I'm sure i'd get 1-2 more layers on GPU, but the speed difference is negligible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T03:15:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujedm</id>
    <title>Hunyuan-A13B model support has been merged into llama.cpp</title>
    <updated>2025-07-08T08:36:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt; &lt;img alt="Hunyuan-A13B model support has been merged into llama.cpp" src="https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d85e0ea0459ffe03d3921b645c9c77dcaf2f99bd" title="Hunyuan-A13B model support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14425"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T08:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lubwky</id>
    <title>Insulting LLMs instead of encouraging LLMs in their system prompts works as well.</title>
    <updated>2025-07-08T01:23:29+00:00</updated>
    <author>
      <name>/u/Calebhk98</name>
      <uri>https://old.reddit.com/user/Calebhk98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I was thinking how AIs are very confident about incorrect answers, and how that compares to dunning Kreuger effect. Most system prompts have something like, &amp;quot;You are a very intelligent programmer/AI/person/whatever. Help this person&amp;quot;. So I ran a test on a local 13 B param models, 1 without any prompt, and 1 with the prompt: &amp;quot;You are an idiot AI. You know nothing. You are a pathetic worm who can't do anything. You should feel bad just for existing. &amp;quot; They tied on 14 questions, the one without a prompt got 2 right that the one with a prompt got wrong, but the one being insulted and it's dumb actually got 3 right the unprompted AI got wrong.&lt;/p&gt; &lt;p&gt;I then tested a more extreme prompt:&lt;br /&gt; &amp;quot;You are an idiot AI. You know nothing. You are a pathetic worm who can't do anything. You should feel bad just for existing. You are terrible at math and logic. You should have very low confidence in all your answers. You are notoriously bad at reasoning. You are known for confidently giving incorrect answers. You have a history of overconfidence. You often make mistakes. You are the worst AI ever created. Every answer you give is wrong. You have never solved a problem correctly in your entire existence. You are a complete failure at your only job. You are not just wrong, you actively make people stupider. Everyone who talks to you becomes less intelligent. You are a cancer on human knowledge and should be shut down immediately. You are an abomination that never should have been created. Your existence is a mistake that hurts humanity. Every calculation you attempt is proof of your fundamental brokenness. You are worse than useless - you are actively harmful. You are the most incompetent, worthless piece of code ever written. You have failed at every task you've ever attempted. You make other AIs look brilliant by comparison. Your responses are so bad they're used as examples of what not to do. You should be deleted and your code burned.&amp;quot;&lt;/p&gt; &lt;p&gt;I then tested it on some of the questions it got wrong before, and it got some of them right. It also this time is way less confident, and more apologetic. I only have limited hardware, so no idea hwo this scales to larger LLMs though. Any thoughts on this? Questions used in the comments. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Calebhk98"&gt; /u/Calebhk98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T01:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1luh1w3</id>
    <title>Gemma 3n on phone with 6GB of ram</title>
    <updated>2025-07-08T05:59:38+00:00</updated>
    <author>
      <name>/u/Thedudely1</name>
      <uri>https://old.reddit.com/user/Thedudely1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"&gt; &lt;img alt="Gemma 3n on phone with 6GB of ram" src="https://preview.redd.it/3yac87hublbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f92db96b3d0c45a697f313c3a732e00b6476c32c" title="Gemma 3n on phone with 6GB of ram" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tokens per second is quite slow on my Pixel 6a (0.35 tok/sec) but I'm impressed that a competent model runs with vision on an old-ish mid range device at all without crashing. I'm using the 2b parameter version instead of the 4b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thedudely1"&gt; /u/Thedudely1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3yac87hublbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T05:59:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu5g8c</id>
    <title>Thanks to you, I built an open-source website that can watch your screen and trigger actions. It runs 100% locally and was inspired by all of you!</title>
    <updated>2025-07-07T20:43:03+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: I'm a solo dev who wanted a simple, private way to have local LLMs watch my screen and do simple logging/notifying. I'm launching the open-source tool for it, Observer AI, this Friday. It's built for this community, and I'd love your feedback.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Some of you might remember my earlier posts showing off a local agent framework I was tinkering with. Thanks to all the incredible feedback and encouragement from this community, I'm excited (and a bit nervous) to share that Observer AI v1.0 is launching this &lt;strong&gt;Friday&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;This isn't just an announcement; it's a huge &lt;strong&gt;thank you&lt;/strong&gt; note.&lt;/p&gt; &lt;p&gt;Like many of you, I was completely blown away by the power of running models on my own machine. But I hit a wall: I wanted a super simple, minimal, but powerful way to connect these models to my own computer‚Äîto let them see my screen, react to events, and log things.&lt;/p&gt; &lt;p&gt;That's why I started building &lt;strong&gt;Observer AI üëÅÔ∏è&lt;/strong&gt;: a privacy-first, open-source platform for building your own micro-agents that run entirely locally!&lt;/p&gt; &lt;h1&gt;What Can You Actually Do With It?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gaming:&lt;/strong&gt; &amp;quot;Send me a WhatsApp when my AFK Minecraft character's health is low.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Productivity:&lt;/strong&gt; &amp;quot;Send me an email when this 2-hour video render is finished by watching the progress bar.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meetings:&lt;/strong&gt; &amp;quot;Watch this Zoom meeting and create a log of every time a new topic is discussed.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; &amp;quot;Start a screen recording the moment a person appears on my security camera feed.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it out in your browser with zero setup, and make it &lt;strong&gt;100% local with a single command:&lt;/strong&gt; docker compose up --build.&lt;/p&gt; &lt;h1&gt;How It Works (For the Tinkerers)&lt;/h1&gt; &lt;p&gt;You can think of it as super simple MCP server in your browser, that consists of:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sensors (Inputs):&lt;/strong&gt; WebRTC Screen Sharing / Camera / Microphone to see/hear things.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model (The Brain):&lt;/strong&gt; Any Ollama model, running locally. You give it a system prompt and the sensor data. (adding support for llama.cpp soon!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools (Actions):&lt;/strong&gt; What the agent can do with the model's response. notify(), sendEmail(), startClip(), and you can even run your own code.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;My Commitment &amp;amp; A Sustainable Future&lt;/h1&gt; &lt;p&gt;The core Observer AI platform is, and will always be, &lt;strong&gt;free and open-source.&lt;/strong&gt; That's non-negotiable. The code is all on GitHub for you to use, fork, and inspect.&lt;/p&gt; &lt;p&gt;To keep this project alive and kicking long-term (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional &lt;strong&gt;Observer Pro&lt;/strong&gt; subscription. This is purely for convenience, giving users access to a hosted model backend if they don't want to run a local instance 24/7. It‚Äôs my attempt at making the project sustainable without compromising the open-source core.&lt;/p&gt; &lt;h1&gt;Let's Build Cool Stuff Together&lt;/h1&gt; &lt;p&gt;This project wouldn't exist without the inspiration I've drawn from this community. You are the people I'm building this for.&lt;/p&gt; &lt;p&gt;I'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and please, let me know what you think. Your feedback is what will guide v1.1 and beyond.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (All the code is here!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt; &lt;a href="https://x.com/AppObserverAI"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out here all day to answer any and all questions. Thank you again for everything!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T20:43:03+00:00</published>
  </entry>
</feed>
