<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-12T01:57:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pjv5wz</id>
    <title>How to properly run gpt-oss-120b on multiple GPUs with llama.cpp?</title>
    <updated>2025-12-11T11:24:23+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjv5wz/how_to_properly_run_gptoss120b_on_multiple_gpus/"&gt; &lt;img alt="How to properly run gpt-oss-120b on multiple GPUs with llama.cpp?" src="https://a.thumbs.redditmedia.com/NKfUY6B0Eh9KpkLKmc3U7W6WmEuLH87BkO7W0z4f-k8.jpg" title="How to properly run gpt-oss-120b on multiple GPUs with llama.cpp?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SOLVED. Results below.&lt;/p&gt; &lt;p&gt;Hello, I need some advice on how to get the gpt-oss-120b running optimally on multiple GPUs setup.&lt;/p&gt; &lt;p&gt;The issue is that in my case, the model is not getting automagically distributed across two GPUs.&lt;/p&gt; &lt;p&gt;My setup is an old Dell T7910 with dual E5-2673 v4 80cores total, 256gb ddr4 and dual RTX 3090. Posted photos some time ago. Now the AI works in a VM hosted on Proxmox with both RTX and a NVMe drive passed through. NUMA is selected, CPU is host (kvm options). Both RTX3090 are power limited to 200W.&lt;/p&gt; &lt;p&gt;I'm using either freshly compiled llama.cpp with cuda or dockerized llama-swap:cuda.&lt;/p&gt; &lt;p&gt;First attempt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin/llama-server --host 0.0.0.0 --port 8080 -m gpt-oss-120b.gguf --n-gpu-layers 999 --n-cpu-moe 24 --ctx-size 65536 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Getting around 1..2tps, CPUs seem way too old and slow. Only one of the GPUs is fully utilized: like 1st: 3GB/24GB, 2nd: 23GB/24GB&lt;/p&gt; &lt;p&gt;After some fiddling with parameters, tried to spread tensors across both GPUs. Getting between 7tps to 13tps or so, say 10tps on average.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --port ${PORT} -m /models/gpt-oss-120b-MXFP4_MOE.gguf --n-gpu-layers 999 --n-cpu-moe 10 --tensor-split 62,38 --main-gpu 0 --split-mode row --ctx-size 32768 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Third version, according to unsloth tutorial, both GPUs are equally loaded, getting speed up to 10tps, seems slightly slower than the manual tensor split.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --port ${PORT} -m /models/gpt-oss-120b-MXFP4_MOE.gguf --n-gpu-layers 999 --ctx-size 32768 -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --threads -1 --temp 1.0 --min-p 0.0 --top-p 1.0 --top-k 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any suggestions how to adjust to get it working faster?&lt;/p&gt; &lt;p&gt;Interestingly, my dev vm on i9 11th gen, 64GB ram, 1x RTX 3090 , full power gets... 15tps which i think is great, despite having a single GPU.&lt;/p&gt; &lt;p&gt;// Edit&lt;/p&gt; &lt;p&gt;WOAH! 25tps on average! :o&lt;/p&gt; &lt;p&gt;Seems, NUMA is the culprit, apart from the system being old garbage :)&lt;/p&gt; &lt;p&gt;- Changed the VM setup and pinned it to ONE specific CPUs, system has 2x40 cpus, i set the VM to use 1x40&lt;br /&gt; - Memory binding to a numa node&lt;/p&gt; &lt;p&gt;PVE VM config&lt;/p&gt; &lt;pre&gt;&lt;code&gt;agent: 1 bios: ovmf boot: order=virtio0 cores: 40 cpu: host,flags=+aes cpuset: 0-40 efidisk0: zfs:vm-1091-disk-0,efitype=4m,pre-enrolled-keys=1,size=1M hostpci0: 0000:03:00,pcie=1 hostpci1: 0000:04:00,pcie=1 hostpci2: 0000:a4:00,pcie=1 ide2: none,media=cdrom machine: q35 memory: 65536 balloon: 0 meta: creation-qemu=9.0.2,ctime=1738323496 name: genai01 net0: virtio=BC:24:11:7F:30:EB,bridge=vmbr0,tag=102 affinity: 0-19,40-59 numa: 1 numa0: cpus=0-19,40-59,hostnodes=0,memory=65536,policy=bind onboot: 1 ostype: l26 scsihw: virtio-scsi-single smbios1: uuid=bb4a79de-e68c-4225-82d7-6ee6e2ef58fe sockets: 1 virtio0: zfs:vm-1091-disk-1,iothread=1,size=32G virtio1: zfs:vm-1091-disk-2,iothread=1,size=1T vmgenid: 978f6c1e-b6fe-4e33-9658-950dadbf8c07 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Docker compose&lt;/p&gt; &lt;pre&gt;&lt;code&gt;services: llama: container_name: llama image: ghcr.io/mostlygeek/llama-swap:cuda restart: unless-stopped privileged: true networks: - genai-network ports: - 9090:8080 volumes: - ./llama-swap-config.yaml:/app/config.yaml - /nvme/gguf:/models - /sys/devices/system/node:/sys/devices/system/node deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LLama Swap&lt;/p&gt; &lt;pre&gt;&lt;code&gt; gpt-oss-120b: cmd: &amp;gt; llama-server --port ${PORT} -m /models/gpt-oss-120b-MXFP4_MOE.gguf --n-gpu-layers 999 --ctx-size 32768 -fa on -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --threads -1 --temp 1.0 --min-p 0.0 --top-p 1.0 --top-k 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now i usually get between 22 to 26tps, so over 2x faster :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i5e4yzdzbl6g1.png?width=949&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2e97c525fc0bc429e83ce8654ef0511305c5c53"&gt;https://preview.redd.it/i5e4yzdzbl6g1.png?width=949&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2e97c525fc0bc429e83ce8654ef0511305c5c53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rwczwyv1cl6g1.png?width=678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f3cecd0e4088f24ddfd6323e9e9782c03717c65"&gt;https://preview.redd.it/rwczwyv1cl6g1.png?width=678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f3cecd0e4088f24ddfd6323e9e9782c03717c65&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8y269d39cl6g1.png?width=1856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24284942fd8ddc628b3dce8a690f828c42341d9"&gt;https://preview.redd.it/8y269d39cl6g1.png?width=1856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24284942fd8ddc628b3dce8a690f828c42341d9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjv5wz/how_to_properly_run_gptoss120b_on_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjv5wz/how_to_properly_run_gptoss120b_on_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjv5wz/how_to_properly_run_gptoss120b_on_multiple_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T11:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjbhyz</id>
    <title>I bought a Grace-Hopper server for €7.5k on Reddit and converted it into a desktop.</title>
    <updated>2025-12-10T19:10:24+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/"&gt; &lt;img alt="I bought a Grace-Hopper server for €7.5k on Reddit and converted it into a desktop." src="https://a.thumbs.redditmedia.com/l8ZeQnDUHvPd48PfD2ysm_06ihJr4BzLyTdV3VevN80.jpg" title="I bought a Grace-Hopper server for €7.5k on Reddit and converted it into a desktop." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking for a big upgrade for the brain for my &lt;a href="https://github.com/dnhkng/GlaDOS"&gt;GLaDOS Project&lt;/a&gt;, and so when I stumbled across a Grace-Hopper system being sold for 10K euro on here on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; , my first thought was “obviously fake.” My second thought was “I wonder if he’ll take 7.5K euro?”.&lt;/p&gt; &lt;p&gt;This is the story of how I bought enterprise-grade AI hardware designed for liquid-cooled server racks that was converted to air cooling, and then back again, survived multiple near-disasters (including GPUs reporting temperatures of 16 million degrees), and ended up with a desktop that can run 235B parameter models at home. It’s a tale of questionable decisions, creative problem-solving, and what happens when you try to turn datacenter equipment into a daily driver.&lt;/p&gt; &lt;p&gt;If you’ve ever wondered what it takes to run truly large models locally, or if you’re just here to watch someone disassemble $80,000 worth of hardware with nothing but hope and isopropanol, you’re in the right place.&lt;/p&gt; &lt;p&gt;You can read the &lt;a href="https://dnhkng.github.io/posts/hopper/"&gt;full story here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pjbhyz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T19:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjiihv</id>
    <title>FlashAttention implementation for non Nvidia GPUs. AMD, Intel Arc, Vulkan-capable devices</title>
    <updated>2025-12-10T23:47:56+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjiihv/flashattention_implementation_for_non_nvidia_gpus/"&gt; &lt;img alt="FlashAttention implementation for non Nvidia GPUs. AMD, Intel Arc, Vulkan-capable devices" src="https://preview.redd.it/xfshykn1rg6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f22c5f3ba9ca217baf0c5fe898f76c61cea36afa" title="FlashAttention implementation for non Nvidia GPUs. AMD, Intel Arc, Vulkan-capable devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We built a flashattention library that is for non Nvidia GPUs that will solve the age old problem of not having CUDA backend for running ML models on AMD and intel ARC and Metal would love a star on the GitHub PRs as well and share it with your friends too. &amp;quot;&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/AuleTechnologies/Aule-Attention"&gt;https://github.com/AuleTechnologies/Aule-Attention&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sharing Yeabsira work so you can speedup your systems too :)&lt;br /&gt; Created by: &lt;a href="https://www.linkedin.com/in/yeabsira-teshome-1708222b1/"&gt;https://www.linkedin.com/in/yeabsira-teshome-1708222b1/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xfshykn1rg6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjiihv/flashattention_implementation_for_non_nvidia_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjiihv/flashattention_implementation_for_non_nvidia_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T23:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkaqjl</id>
    <title>Mistral Vibe CLI which is the smallest local llm that you can run ?</title>
    <updated>2025-12-11T22:16:36+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Devstral-Small-2-24B-Instruct-2512-Q4_K_M works of course but it's very slow, for me Qwen3-4B-Instruct-2507-Q4_K_M is the best because it's very fast and it also supports tool calling, other bigger models could work but most are painfully slow or use a different style of tool calling&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkaqjl/mistral_vibe_cli_which_is_the_smallest_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkaqjl/mistral_vibe_cli_which_is_the_smallest_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkaqjl/mistral_vibe_cli_which_is_the_smallest_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T22:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj51tu</id>
    <title>You can now train LLMs 3x faster with 30% less memory! (&lt;3.9GB VRAM)</title>
    <updated>2025-12-10T15:12:39+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj51tu/you_can_now_train_llms_3x_faster_with_30_less/"&gt; &lt;img alt="You can now train LLMs 3x faster with 30% less memory! (&amp;lt;3.9GB VRAM)" src="https://preview.redd.it/831ky7k47e6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02ff40ce13155be048e6de2935672da6c685da75" title="You can now train LLMs 3x faster with 30% less memory! (&amp;lt;3.9GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey [&lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;]()! We're excited to release new Triton kernels and smart auto packing support to enable you to train models 3x (sometimes even &lt;strong&gt;5x&lt;/strong&gt;) faster with &lt;strong&gt;30-90% less VRAM&lt;/strong&gt; - all with &lt;strong&gt;no accuracy degradation&lt;/strong&gt;. Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This means you can now train LLMs like Qwen3-4B not only on just &lt;strong&gt;3.9GB VRAM&lt;/strong&gt;, but also 3x faster&lt;/li&gt; &lt;li&gt;But how? It's all due to our new custom RoPE and MLP Triton kernels, plus our new smart auto uncontaminated packing integration&lt;/li&gt; &lt;li&gt;Speed and VRAM optimizations will depend on your setup (e.g. dataset)&lt;/li&gt; &lt;li&gt;You'll also see improved SFT loss stability and more predictable GPU utilization&lt;/li&gt; &lt;li&gt;No need to enable these new additions as they're smartly enabled by default. e.g. auto padding-free uncontaminated packing is on for all training runs without any accuracy changes. Benchmarks show training losses match non-packing runs exactly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Detailed breakdown of optimizations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;2.3x faster QK Rotary Embedding&lt;/strong&gt; fused Triton kernel with packing support&lt;/li&gt; &lt;li&gt;Updated SwiGLU, GeGLU kernels with &lt;strong&gt;int64 indexing for long context&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2.5x to 5x faster uncontaminated packing&lt;/strong&gt; with xformers, SDPA, FA3 backends&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2.1x faster padding free, 50% less VRAM&lt;/strong&gt;, 0% accuracy change&lt;/li&gt; &lt;li&gt;We launched Unsloth with a Triton RoPE kernel in Dec, 2023. We’ve now merged the two Q/K kernels into one and added variable-length RoPE for pad-free packing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can read our educational blogpost for detailed analysis, benchmarks and more: &lt;a href="https://docs.unsloth.ai/new/3x-faster-training-packing"&gt;https://docs.unsloth.ai/new/3x-faster-training-packing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And you can of course train any model using our new features and kernels via our free fine-tuning notebooks: &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks"&gt;https://docs.unsloth.ai/get-started/unsloth-notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To update Unsloth to automatically make training faster, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And to enable manual packing support (we already do padding free which should already provide a boost!) do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastLanguageModel from trl import SFTTrainer, SFTConfig model, tokenizer = FastLanguageModel.from_pretrained(&amp;quot;unsloth/Qwen3-14B&amp;quot;) trainer = SFTTrainer( model = model, processing_class = tokenizer, train_dataset = dataset, args = SFTConfig(..., packing = True,), ) trainer.train() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a lovely rest of the week! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/831ky7k47e6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj51tu/you_can_now_train_llms_3x_faster_with_30_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj51tu/you_can_now_train_llms_3x_faster_with_30_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T15:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjgce6</id>
    <title>Collection of every GPU from AMD and Nvidia</title>
    <updated>2025-12-10T22:16:50+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjgce6/collection_of_every_gpu_from_amd_and_nvidia/"&gt; &lt;img alt="Collection of every GPU from AMD and Nvidia" src="https://external-preview.redd.it/MzhpZ2MzNWhiZzZnMeox36vPvVseHB_QUv5VRvdrDYl5WPoW2X7NoNtQuiRo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e3a4022d440147dce466061d61c141df597394e" title="Collection of every GPU from AMD and Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source &lt;a href="https://youtu.be/g7MpS0X9Ru0?si=aLz_7sOnqUEuNgpa"&gt;https://youtu.be/g7MpS0X9Ru0?si=aLz_7sOnqUEuNgpa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ohsswl4hbg6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjgce6/collection_of_every_gpu_from_amd_and_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjgce6/collection_of_every_gpu_from_amd_and_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T22:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj8kb6</id>
    <title>Mistral AI drops 3x as many LLMs in a single week as OpenAI did in 6 years</title>
    <updated>2025-12-10T17:24:38+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the GGUF links to Mistral AI’s &amp;quot;collected works&amp;quot; from the past week – all ready for local use:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cutting-edge coding models:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 24B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- 123B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Devstral-2-123B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Devstral-2-123B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top-tier reasoning models – perfectly sized for consumer hardware:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 3B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Ministral-3-3B-Reasoning-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Ministral-3-3B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- 8B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Ministral-3-8B-Reasoning-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Ministral-3-8B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- 14B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Reasoning-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Powerful instruct models for local setups:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 3B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Ministral-3-3B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Ministral-3-3B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- 8B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Ministral-3-8B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Ministral-3-8B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- 14B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Ministral-3-14B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral’s most advanced instruct model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 675B parameters: &lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Licensing:&lt;/strong&gt; All models under Apache 2.0, Devstral 2 with a modified MIT license.&lt;/p&gt; &lt;p&gt;What an insane achievement for a company that’s still small compared to OpenAI! Huge thanks to Mistral AI! &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj8kb6/mistral_ai_drops_3x_as_many_llms_in_a_single_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj8kb6/mistral_ai_drops_3x_as_many_llms_in_a_single_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj8kb6/mistral_ai_drops_3x_as_many_llms_in_a_single_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T17:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pju9ob</id>
    <title>New era for fine-tuning is on the horizon</title>
    <updated>2025-12-11T10:28:15+00:00</updated>
    <author>
      <name>/u/uhuge</name>
      <uri>https://old.reddit.com/user/uhuge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A paper released at &lt;a href="https://arxiv.org/abs/2512.05117"&gt;https://arxiv.org/abs/2512.05117&lt;/a&gt; , no code yet &lt;/p&gt; &lt;p&gt;Authors claim you can take a bunch of fine-tuned models of the same architecture and create new task/domain specific variants by just setting a few dozens numbers on each of the internal layer.&lt;/p&gt; &lt;p&gt;You'd have the performance just a bit lowered, but your whole Q30A3 library of teens of variants would be just those 15 gigs, each variant represented in a floppy-friendly chunk of numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uhuge"&gt; /u/uhuge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pju9ob/new_era_for_finetuning_is_on_the_horizon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pju9ob/new_era_for_finetuning_is_on_the_horizon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pju9ob/new_era_for_finetuning_is_on_the_horizon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T10:28:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pke7ai</id>
    <title>Running the latest multimodal models on ANE across iOS and macOS</title>
    <updated>2025-12-12T00:51:13+00:00</updated>
    <author>
      <name>/u/Material_Shopping496</name>
      <uri>https://old.reddit.com/user/Material_Shopping496</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; fam, we’re excited to release NexaSDK for iOS and macOS — the first and only runtime that runs the latest SOTA multimodal models fully on Apple Neural Engine, CPU and GPU across iPhones and Macbooks.&lt;/p&gt; &lt;h1&gt;Key features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Models with ANE support &lt;ul&gt; &lt;li&gt;Embedding: EmbedNeural (Multimodal Embedding)&lt;/li&gt; &lt;li&gt;LLM: Granite-Micro (IBM), Ministral3-3B (Mistral), Gemma3 (Google), Qwen3-0.6B / 4B (Qwen)&lt;/li&gt; &lt;li&gt;CV: PaddleOCR (Baidu)&lt;/li&gt; &lt;li&gt;ASR: Parakeet v3 (NVIDIA)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Simple setup: 3 lines of code to get started&lt;/li&gt; &lt;li&gt;9× energy efficiency compared to CPU and GPU&lt;/li&gt; &lt;li&gt;Easy integration with simple Swift API usage.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it out:&lt;/h1&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/NexaAI/nexasdk-mobile-iOS-framework/tree/main"&gt;https://github.com/NexaAI/nexasdk-mobile-iOS-framework/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://docs.nexa.ai/nexa-sdk-ios/overview"&gt;https://docs.nexa.ai/nexa-sdk-ios/overview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love your feedback — and tell us which model you want on ANE next. We iterate fast.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pke7ai/video/0g6fbarg5o6g1/player"&gt;https://reddit.com/link/1pke7ai/video/0g6fbarg5o6g1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Material_Shopping496"&gt; /u/Material_Shopping496 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pke7ai/running_the_latest_multimodal_models_on_ane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pke7ai/running_the_latest_multimodal_models_on_ane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pke7ai/running_the_latest_multimodal_models_on_ane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T00:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk2okg</id>
    <title>Why do I feel like LLMs in general, both local and cloud, try to do too much at once and that's why they make a lot of mistakes?</title>
    <updated>2025-12-11T17:00:21+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs are essentially chatty encyclopedias but the way their responses are trained makes me feel like they're stretching themselves too thin, like they're trying &lt;em&gt;too&lt;/em&gt; hard to be helpful. &lt;/p&gt; &lt;p&gt;For example, if you have something like gpt-oss-120b running locally and you ask it how to debug an issue with your script, it tries to be helpful by giving you a long-ass, multi-step response that may or may not be correct.&lt;/p&gt; &lt;p&gt;I've come to realize that I think they would be more helpful if they were trained to take things one step at a time instead of forcibly generating a lengthy response that might be a nothingburger.&lt;/p&gt; &lt;p&gt;If you receive advice from the LLM that involves multiple steps, it can be overwhelming and verbose, not to mention you have to understand the tools you supposedly need to use per the LLM, which turns into a learning process &lt;em&gt;within&lt;/em&gt; a learning process and might actually get you nowhere closer to your goal. &lt;/p&gt; &lt;p&gt;I think such verbose responses are great &lt;code&gt;AI -&amp;gt; AI&lt;/code&gt;, but not &lt;code&gt;AI -&amp;gt; Human&lt;/code&gt;. I feel like it would be more helpful instead to address humans with short, concise, bite-sized responses that walk you through the steps needed one-by-one because despite their worldly knowledge, I genuinely haven't found those types of responses to be very helpful. It takes too long to read, too hard to understand everything at once and might actually be incorrect in the end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk2okg/why_do_i_feel_like_llms_in_general_both_local_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk2okg/why_do_i_feel_like_llms_in_general_both_local_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk2okg/why_do_i_feel_like_llms_in_general_both_local_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk80cd</id>
    <title>Questions LLMs usually get wrong</title>
    <updated>2025-12-11T20:26:52+00:00</updated>
    <author>
      <name>/u/DustinKli</name>
      <uri>https://old.reddit.com/user/DustinKli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on custom benchmarks and want to ask everyone for examples of questions they like to ask LLMs (or tasks to have them do) that they always or almost always get wrong. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DustinKli"&gt; /u/DustinKli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk80cd/questions_llms_usually_get_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk80cd/questions_llms_usually_get_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk80cd/questions_llms_usually_get_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T20:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbmqe</id>
    <title>TFLOPS by GPU</title>
    <updated>2025-12-11T22:55:15+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not a professional ML engineer/researcher, I just enjoy ML/AI development as a hobby (still, it would be nice if this knowledge could be transferred to a real job). Just like many people in this sub, I was debating with myself on the idea of buying myself a PC, or buying a DGX Spark, or a mini PC with a Strix Halo, or just renting a cloud one.&lt;/p&gt; &lt;p&gt;Using free GPUs on Google Colab and Kaggle sometimes feels like enough for me, but it's slow. So I decided to run a quick benchmark on different GPUs to see what the actual difference is, and what I would miss for being stingy.&lt;/p&gt; &lt;p&gt;The benchmark script &lt;a href="https://x.com/awnihannun/status/1982880363765768288"&gt;was taken&lt;/a&gt; from Awni Hannun's tweet (MLX co-author), it's basically do matrix multiplications on two BF16 8192x8192 matrices.&lt;/p&gt; &lt;p&gt;Disclaimer: I know just TFLOPS alone is not enough when it come to performance (memory bandwidth, power consumption, other factors like RAM/CPU,...), but it's still make a sense for a quick comparison.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;strong&gt;Device&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;TFLOPS&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Time (ms)&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;B200&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;1629.45&lt;/td&gt; &lt;td&gt;306.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;H200 SXM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;680.32&lt;/td&gt; &lt;td&gt;734.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;MI300X (ROCm)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;464.90&lt;/td&gt; &lt;td&gt;1075.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;L40S&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;209.75&lt;/td&gt; &lt;td&gt;2383.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX 5090&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;207.254&lt;/td&gt; &lt;td&gt;2428.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX 4090&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;152.89&lt;/td&gt; &lt;td&gt;3270.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX PRO 6000 WK&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;136.53&lt;/td&gt; &lt;td&gt;3662.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;A40&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;110.386&lt;/td&gt; &lt;td&gt;4529.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX 3090&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;70.86&lt;/td&gt; &lt;td&gt;7055.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;L4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;56.66&lt;/td&gt; &lt;td&gt;8823.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Tesla V100&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;10.15&lt;/td&gt; &lt;td&gt;49242.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kaggle P100&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;5.708&lt;/td&gt; &lt;td&gt;87594.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;M2 Max MBP 64GB&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;4.796&lt;/td&gt; &lt;td&gt;104246.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google Colab T4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;2.314&lt;/td&gt; &lt;td&gt;216094.496&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kaggle 2xT4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;2.177&lt;/td&gt; &lt;td&gt;229686.30&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The code was modified to run on MPS for macbook. ON the AMD one, no modification needed, run on ROCm.&lt;/p&gt; &lt;p&gt;Also, some numbers I found online, on other devices that I could not confirmed myself:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;strong&gt;Device&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;TFLOPS&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DGX Spark&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;M5 MBP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;It would be nice if someone with other devices can run the test and confirm that the numbers are correct.&lt;/p&gt; &lt;p&gt;After looking at the numbers, I feel like a Strix Halo miniPC (even 64GB) would be more than enough, and if I ever feel the need for CUDA, then adding a 3090 will do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbmqe/tflops_by_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbmqe/tflops_by_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbmqe/tflops_by_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T22:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3pgk</id>
    <title>Dude, Where's My GGUF? - For some models</title>
    <updated>2025-12-11T17:39:31+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From last 3 months. Just sharing models' threads from this sub. I see tickets/PR(llama.cpp support queue) for few models.&lt;/p&gt; &lt;p&gt;I didn't include non-commercial licensed models like Apple's.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pj343j/nous_research_just_open_source_nomos_1_a/"&gt;NousResearch/nomos-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"&gt;CycleCoreTechnologies/maaza-nlm-orchestrator-9.6m-v1.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"&gt;deepseek-ai/DeepSeek-V3.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1paj4m8/trained_a_chess_llm_locally_that_beats_gpt5/"&gt;daavidhauser/chess-bot-3000&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt;deepseek-ai/DeepSeek-Math-V2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;inclusionAI/LLaDA2.0-flash &amp;amp; inclusionAI/LLaDA2.0-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p3e0mp/gptusenet_an_81millionparameter_model_trained_on/"&gt;HDTenEightyP/GPT-Usenet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p37q0h/sensenovasi_scaling_spatial_intelligence_with/"&gt;sensenova/sensenova-si&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"&gt;allenai - rl-research/DR-Tulu-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ol2odj/i_fine_tuned_a_small_model_to_help_with_reasoning/"&gt;joeyzero/Qwen3-4B-Reasoning-Backfill-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"&gt;ByteDance/Ouro 1.4B &amp;amp; 2.6B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/"&gt;moonshotai/Kimi-Linear-48B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ojvgsx/manifestai_releases_brumby14bbase_weights_claims/"&gt;manifestai/Brumby-14B-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt;inference-net/Schematron-3B &amp;amp; Schematron-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT : Point of this thread is randomly coders could help on proceed further because many coders are active on these LLM related subs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3pgk/dude_wheres_my_gguf_for_some_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3pgk/dude_wheres_my_gguf_for_some_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3pgk/dude_wheres_my_gguf_for_some_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk4lrc</id>
    <title>235 contributors from around the world to gather one of the largest robotics dataset (46 different robots - 250 hours - 26M frames)</title>
    <updated>2025-12-11T18:13:38+00:00</updated>
    <author>
      <name>/u/Wide-Screen-4632</name>
      <uri>https://old.reddit.com/user/Wide-Screen-4632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4lrc/235_contributors_from_around_the_world_to_gather/"&gt; &lt;img alt="235 contributors from around the world to gather one of the largest robotics dataset (46 different robots - 250 hours - 26M frames)" src="https://preview.redd.it/b7fxmkoi8m6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5f95ad80194c85e5b971768cd862e7be77f763e" title="235 contributors from around the world to gather one of the largest robotics dataset (46 different robots - 250 hours - 26M frames)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the dataset: &lt;a href="https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3"&gt;https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide-Screen-4632"&gt; /u/Wide-Screen-4632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b7fxmkoi8m6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4lrc/235_contributors_from_around_the_world_to_gather/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4lrc/235_contributors_from_around_the_world_to_gather/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T18:13:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk1yih</id>
    <title>SOLVE_TRI extension to more dimensions by pwilkin · Pull Request #17793 · ggml-org/llama.cpp</title>
    <updated>2025-12-11T16:32:05+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk1yih/solve_tri_extension_to_more_dimensions_by_pwilkin/"&gt; &lt;img alt="SOLVE_TRI extension to more dimensions by pwilkin · Pull Request #17793 · ggml-org/llama.cpp" src="https://external-preview.redd.it/mNzRlPgkKLb5qvqWrnpnvuvOayD0W4Jv9VgNz7gUwSE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9c8a5712c66f04c75e6db44fc20f48b01ddd046" title="SOLVE_TRI extension to more dimensions by pwilkin · Pull Request #17793 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;before:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;jacek@AI-SuperComputer:~$ /home/jacek/git/llama.cpp/build_2025.12.11/bin/llama-bench -m /mnt/models2/Qwen_Qwen3-Next-80B-A3B-Instruct-Q6_K_L-00001-of-00002.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 3 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3next 80B.A3B Q6_K | 61.20 GiB | 79.67 B | CUDA | 99 | pp512 | 562.56 ± 1.53 | | qwen3next 80B.A3B Q6_K | 61.20 GiB | 79.67 B | CUDA | 99 | tg128 | 43.09 ± 0.14 | build: c6f6e4f96 (7359) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;after:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;jacek@AI-SuperComputer:~$ /home/jacek/git/llama.cpp/build_2025.12.11_tri/bin/llama-bench -m /mnt/models2/Qwen_Qwen3-Next-80B-A3B-Instruct-Q6_K_L-00001-of-00002.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 3 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3next ?B Q6_K | 61.20 GiB | 79.67 B | CUDA | 99 | pp512 | 737.65 ± 4.16 | | qwen3next ?B Q6_K | 61.20 GiB | 79.67 B | CUDA | 99 | tg128 | 43.08 ± 0.18 | build: 08a003e18 (7352) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17793"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk1yih/solve_tri_extension_to_more_dimensions_by_pwilkin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk1yih/solve_tri_extension_to_more_dimensions_by_pwilkin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T16:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk1zpq</id>
    <title>Is IQ4_XS closer to Q4 or Q3 in terms of quality?</title>
    <updated>2025-12-11T16:33:18+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. There are a very &lt;em&gt;very&lt;/em&gt; old threads that don't quite come to a consensus on this.&lt;/p&gt; &lt;p&gt;Assume that everything is loaded into VRAM and no layers are offloaded to CPU+system memory.&lt;/p&gt; &lt;p&gt;Wondering what your experiences have been?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk1zpq/is_iq4_xs_closer_to_q4_or_q3_in_terms_of_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk1zpq/is_iq4_xs_closer_to_q4_or_q3_in_terms_of_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk1zpq/is_iq4_xs_closer_to_q4_or_q3_in_terms_of_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T16:33:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk4e27</id>
    <title>Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source</title>
    <updated>2025-12-11T18:05:23+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt; &lt;img alt="Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source" src="https://b.thumbs.redditmedia.com/kKZ3ZoCKjqFucLu9U3zcx9kw9f07ItjqFMxxpwplBXs.jpg" title="Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, thanks for your suggestions of what models to evaluate! Still working on some, but we've just added Kimi K2 thinking and the two new mistral models. Turns out Kimi K2 Thinking takes the top, surpassing minimax by 2.4%pts (that's 12 task instances). The devstral models fall in the middle, but they are currently freely available on the mistral API!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7d8p912z5m6g1.png?width=4071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6688a0a1b7583b3c78097fbb75c31618cbe46b21"&gt;https://preview.redd.it/7d8p912z5m6g1.png?width=4071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6688a0a1b7583b3c78097fbb75c31618cbe46b21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All of these results are independently evaluated with the exact same (minimal) agent. So it is expected that the numbers are lower than what companies typically report.&lt;/p&gt; &lt;p&gt;Note the asterisk with the cost for Kimi K2 thinking, it is calculated based on the official API pricing information, but the actual cost that was billed seemed lower (but also the cost portal seemed buggy, so not sure what to trust here—for now it's calculated based on the number of tokens same as all the other reported). Anyone know what could be causing any discrepancies?&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking and the devstral models are the exact opposite in terms of steps: Kimi K2 takes the least steps to iterate of all models, devstral the most.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/37akv7ra6m6g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab53c4ba03c2f013f21fc9115a53e87e111db10"&gt;https://preview.redd.it/37akv7ra6m6g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab53c4ba03c2f013f21fc9115a53e87e111db10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're thinking about limiting runtimes to conserve costs/time, here's how performance scales with step limits (even with Kimi, you still want to run for 125-150 steps on hard problems).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tdoe4zh6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3803a5c3567ebb0ffee73c5245b3ff92d02e7ec"&gt;https://preview.redd.it/6tdoe4zh6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3803a5c3567ebb0ffee73c5245b3ff92d02e7ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And this would translate in the following cost-performance plot (where deepseek is still hard to beat). We didn't put the mistral models in here because they're only free temporarily. Of course those are just your API costs, so if you're running on your own hardware, you can ignore this plot:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd9gseql6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f78011f256fa2019627b1b89962ec418593163d"&gt;https://preview.redd.it/fd9gseql6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f78011f256fa2019627b1b89962ec418593163d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have all the trajectories/logs updated if you're curious how each model solves things. They're available from the &amp;quot;Trajs&amp;quot; column on &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As always, you can reproduce our numbers using &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt; (there's a page in the tutorial).&lt;/p&gt; &lt;p&gt;Any new models we should add? (there's still some recommendations from last time that I didn't get to yet). Or any other information we should add ? (we've started collecting latency information as of recently).&lt;/p&gt; &lt;p&gt;Also curious if things like the number of steps a model takes etc. show up in your workflows. Depending on how closely users are in the loop behavior is probably quite different. Also would be interested if you have any qualitative observations about the model behaviors and how they differ (if there's interesting observations, we could see if we can add more information about them for the next releases based on all the agent trajectories we collect)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T18:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3cky</id>
    <title>Shisa V2.1: Improved Japanese (JA/EN) Models (1.2B-70B)</title>
    <updated>2025-12-11T17:25:49+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're celebrating the 2 year anniversary of our original &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/18cwh4n/shisa_7b_a_new_jaen_bilingual_model_based_on/"&gt;Shisa V1&lt;/a&gt; with an updated set of &lt;a href="https://huggingface.co/collections/shisa-ai/shisa-v21"&gt;Shisa V2.1&lt;/a&gt; JA/EN bilingual models.&lt;/p&gt; &lt;p&gt;Shisa V2.1 introduces new and improved 8B, 14B, and 70B dense models with a big performance bump to our previous &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz2lll/shisa_v2_a_family_of_new_jaen_bilingual_models/"&gt;Shisa V2 releases&lt;/a&gt;, as well as new 1.2B (LFM2-based) and 3B (Llama 3.2-based) models. Each of these are class-leading in Japanese language capabilities for their size. Our new V2.1 14B beats the old V2 70B and the new V2.1 70B model gets very close to our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;Shisa V2 405B&lt;/a&gt;! These aren't reasoning or coding models, but if you're looking for an open model that is especially strong at natural/native Japanese, maybe give these a spin.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;License&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Context Length&lt;/th&gt; &lt;th align="left"&gt;JA AVG&lt;/th&gt; &lt;th align="left"&gt;EN AVG&lt;/th&gt; &lt;th align="left"&gt;JA-MT Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-lfm2-1.2b"&gt;shisa-v2.1-lfm2-1.2b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1.2B&lt;/td&gt; &lt;td align="left"&gt;32K&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;27.6&lt;/td&gt; &lt;td align="left"&gt;6.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-llama3.2-3b"&gt;shisa-v2.1-llama3.2-3b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;57.9&lt;/td&gt; &lt;td align="left"&gt;43.2&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-qwen3-8b"&gt;shisa-v2.1-qwen3-8b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;32K/128K&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;td align="left"&gt;57.8&lt;/td&gt; &lt;td align="left"&gt;8.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MIT&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-unphi4-14b"&gt;shisa-v2.1-unphi4-14b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;14B&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;72.6&lt;/td&gt; &lt;td align="left"&gt;57.7&lt;/td&gt; &lt;td align="left"&gt;9.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-llama3.3-70b"&gt;shisa-v2.1-llama3.3-70b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;73.1&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;9.26&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For those that just want to kick the tires, we have &lt;a href="https://chat.shisa.ai/"&gt;https://chat.shisa.ai/&lt;/a&gt; up and running that lets you test and compare V2.1 14B, V2.1 70B, and V2 405B, you might be surprised at just how strong the smaller models are.&lt;/p&gt; &lt;p&gt;These models were all trained on an MI300X node provided by AMD via the &lt;a href="https://www.amd.com/en/developer/resources/cloud-access/amd-developer-cloud.html"&gt;AMD Developer Cloud&lt;/a&gt;. Thanks to all of our compute sponsors, we couldn't keep releasing open models without them. More details (including all sponsors and very detailed eval info) are available on the HF model cards or our &lt;a href="https://shisa.ai/posts/shisa-v2.1/"&gt;announcement post&lt;/a&gt; and mradermacher and others have made GGUFs over the past couple days already for all sizes.&lt;/p&gt; &lt;p&gt;I did want to pull out one interesting bit from the model card, since it's fairly new and unique:&lt;/p&gt; &lt;h3&gt;Cross-Lingual Token Leakage&lt;/h3&gt; &lt;p&gt;While reviewing eval results, we noticed that many models can score highly on Japanese language benchmarks but still output non-Japanese words or sub-words (tokens). Internally we refer to this as Cross-Lingual Token Leakage (CLTL). It has also been referred to more generally as &amp;quot;word-level language confusion&amp;quot; (Marchisio et al., &amp;quot;&lt;a href="https://arxiv.org/abs/2406.20052"&gt;Understanding and Mitigating Language Confusion in LLMs&lt;/a&gt;,&amp;quot; Cohere).&lt;/p&gt; &lt;p&gt;We see many strong multilingual models that exhibit language confusion behavior, but quantifying (and reliably identifying) this issue is harder than one might expect because not only do Japanese and Chinese share Unicode code-planes, but also many valid English words can commonly appear in Japanese text. (Think &amp;quot;AI&amp;quot;, &amp;quot;VR&amp;quot;, or common words and acronyms like &amp;quot;Google&amp;quot; or &amp;quot;NATO&amp;quot;). This is compounded by the fact that even frontier models suffer from “token blindness” - they are often unable to disentangle the meaning from the actual language of the tokens and often fail to recognize wrong-language tokens.&lt;/p&gt; &lt;p&gt;For Shisa V2.1, we have developed a brand-new class of Japanese evaluation benchmark specifically designed to identify CLTL, which can both measure and specifically identify wrong language tokens.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Base Model&lt;/th&gt; &lt;th&gt;Shisa V2.1 Model&lt;/th&gt; &lt;th align="right"&gt;Base Leak %&lt;/th&gt; &lt;th align="right"&gt;Shisa V2.1 Leak %&lt;/th&gt; &lt;th align="right"&gt;Leakage Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Llama-3.2-3B-Instruct&lt;/td&gt; &lt;td&gt;shisa-v2.1-llama3.2-3b&lt;/td&gt; &lt;td align="right"&gt;11.48%&lt;/td&gt; &lt;td align="right"&gt;0.24%&lt;/td&gt; &lt;td align="right"&gt;47.8×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM2-1.2B&lt;/td&gt; &lt;td&gt;shisa-v2.1-lfm2-1.2b&lt;/td&gt; &lt;td align="right"&gt;4.32%&lt;/td&gt; &lt;td align="right"&gt;0.32%&lt;/td&gt; &lt;td align="right"&gt;13.5×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B&lt;/td&gt; &lt;td&gt;shisa-v2.1-qwen3-8b&lt;/td&gt; &lt;td align="right"&gt;2.18%&lt;/td&gt; &lt;td align="right"&gt;0.44%&lt;/td&gt; &lt;td align="right"&gt;5.0×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td&gt;shisa-v2.1-llama3.3-70b&lt;/td&gt; &lt;td align="right"&gt;1.90%&lt;/td&gt; &lt;td align="right"&gt;0.36%&lt;/td&gt; &lt;td align="right"&gt;5.3×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi-4&lt;/td&gt; &lt;td&gt;shisa-v2.1-unphi4-14b&lt;/td&gt; &lt;td align="right"&gt;0.12%&lt;/td&gt; &lt;td align="right"&gt;0.06%&lt;/td&gt; &lt;td align="right"&gt;2.0×&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We believe eliminating both CLTL and language confusion in general is of the utmost importance for deploying LLMs for most Japanese-language production use cases (e.g., translation, customer service, or even basic writing tasks) and we plan to continue to both improve our detection heuristics and to integrate it into all our future evaluation grading, as well as use our better CLTL detection to further improve our training methods. We will be publishing more details in-depth in a future writeup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3znw</id>
    <title>Microsoft analyzed 37.5 million AI conversations in 2025.</title>
    <updated>2025-12-11T17:50:31+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt; &lt;img alt="Microsoft analyzed 37.5 million AI conversations in 2025." src="https://b.thumbs.redditmedia.com/I0gHntPEl9oW4aCmQGgV1BpHx_Jt0xMYNUHzNOY1Pys.jpg" title="Microsoft analyzed 37.5 million AI conversations in 2025." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released their &amp;quot;Copilot Usage Report 2025,&amp;quot; analyzing de-identified data to see how people actually use AI in their daily lives. The results are surprisingly human. Here are the most interesting graphs and takeaways from the report:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &amp;quot;Work Hard, Play Hard&amp;quot; Split&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;People have distinct modes for the week vs. the weekend.&lt;/p&gt; &lt;p&gt;View Graph: Programming vs. Gaming&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: In August, there was a perfect crossover. &amp;quot;Programming&amp;quot; queries rise steadily from Monday to Friday, then tank on Saturday/Sunday. &amp;quot;Gaming&amp;quot; does the exact opposite, dominating the weekends.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The 2 AM Philosophy Club&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The topics we talk about change drastically depending on the time of day.&lt;/p&gt; &lt;p&gt;View Graph: Topic by Hour of Day&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: This radial chart shows that &amp;quot;Travel&amp;quot; queries peak during standard commuting hours. However, &amp;quot;Religion and Philosophy&amp;quot; sees a massive spike in the early morning hours. If you're asking AI about the nature of existence at 3 AM, you aren't alone.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The Valentine's Day Panic&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;February data shows a very specific narrative arc.&lt;/p&gt; &lt;p&gt;View Graph: February Topic Trends&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: &amp;quot;Personal Growth&amp;quot; topics peak in the days leading up to Valentine's Day (people trying to improve themselves?), while &amp;quot;Relationship&amp;quot; queries spike on the day itself (people needing immediate advice).&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Health is King on Mobile&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;When we are on our phones, we are almost always worried about our health.&lt;/p&gt; &lt;p&gt;View Graph: Top Mobile Topics&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: No matter the month, &amp;quot;Health&amp;quot; is consistently the #1 topic for mobile users, far outpacing entertainment or productivity. TL;DR: We use AI to code during the week, survive relationships in February, and serve as a therapist/philosopher late at night.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source: &lt;a href="https://microsoft.ai/news/its-about-time-the-copilot-usage-report-2025/?utm_source=alphasignal&amp;amp;utm_campaign=2025-12-11&amp;amp;lid=bpzfIvhThUltNeQ9&amp;amp;hl=en-GB"&gt;Microsoft AI - The Copilot Usage Report 2025 &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pk3znw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbwco</id>
    <title>EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B</title>
    <updated>2025-12-11T23:06:43+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt; &lt;img alt="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" src="https://b.thumbs.redditmedia.com/jSd0TbO_srnflwRt8ojBram2pVNjfZ3rVlBCwbCJcHQ.jpg" title="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com"&gt;https://eqbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gpt-5.2 writing samples: &lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/gpt-5.2.html"&gt;https://eqbench.com/results/creative-writing-v3/gpt-5.2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;opus-4.5 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html"&gt;https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;mistral-large-3 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html"&gt;https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;nanbeige4-3b writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html"&gt;https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pkbwco"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T23:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkdkjo</id>
    <title>Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b</title>
    <updated>2025-12-12T00:22:10+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt; &lt;img alt="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" src="https://external-preview.redd.it/NDYwbGgydmYybzZnMf2LvdJmBzIyNzEDfN0eOt2yDrF46dRxJq4WcX4O0NUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4363f44505584728345cc48958c232a7ab91036f" title="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A a3b LLM is all you need :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vewmcluf2o6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T00:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjvtgn</id>
    <title>Leaked footage from Meta's post-training strategy meeting.</title>
    <updated>2025-12-11T12:02:11+00:00</updated>
    <author>
      <name>/u/YouCanMake1t</name>
      <uri>https://old.reddit.com/user/YouCanMake1t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt; &lt;img alt="Leaked footage from Meta's post-training strategy meeting." src="https://preview.redd.it/2cbgowoj0i6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8274908702ea2b4e3ee76f7741b54aa24bef73d7" title="Leaked footage from Meta's post-training strategy meeting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouCanMake1t"&gt; /u/YouCanMake1t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2cbgowoj0i6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjw7rj</id>
    <title>Mistral’s Vibe CLI now supports a 200K token context window (previously 100K)</title>
    <updated>2025-12-11T12:23:44+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt; &lt;img alt="Mistral’s Vibe CLI now supports a 200K token context window (previously 100K)" src="https://external-preview.redd.it/ZnNsb2d0dzFpazZnMZt0kKC274AvCvOpM9k0UQCIyB1BQvPjsN5T3o1kO8eQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=841cfe71df83eebc90bd5a8915c65e4a8693db6c" title="Mistral’s Vibe CLI now supports a 200K token context window (previously 100K)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4nxnq6w1ik6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk0ubn</id>
    <title>New in llama.cpp: Live Model Switching</title>
    <updated>2025-12-11T15:49:43+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt; &lt;img alt="New in llama.cpp: Live Model Switching" src="https://external-preview.redd.it/8Hy799ws5wvJKYaRb__KN0TGXYxiPxKG6PuG-1SlIWg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a43f5804fb810225237c9c37046b91c9bbb6451" title="New in llama.cpp: Live Model Switching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/model-management-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T15:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
