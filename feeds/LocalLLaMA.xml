<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-15T16:09:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r5bpb8</id>
    <title>Q: How was Ring-Mini-Linear-2.0 (and other shallow hybrid attention models)?</title>
    <updated>2026-02-15T10:47:21+00:00</updated>
    <author>
      <name>/u/TomLucidor</name>
      <uri>https://old.reddit.com/user/TomLucidor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are models like Kimi-Linear and Nemotron-3-Nano that are fast and compatible with agents, and yet I can't seem to get the smaller Ring-V2 model to run. They have half the parameters and 20% less layers (I think?) but still claims to be half decent for agents. Has anyone tried to use this with coding agents for simple projects? &lt;a href="https://huggingface.co/inclusionAI/Ring-mini-linear-2.0-GPTQ-int4"&gt;https://huggingface.co/inclusionAI/Ring-mini-linear-2.0-GPTQ-int4&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomLucidor"&gt; /u/TomLucidor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5bpb8/q_how_was_ringminilinear20_and_other_shallow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5bpb8/q_how_was_ringminilinear20_and_other_shallow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5bpb8/q_how_was_ringminilinear20_and_other_shallow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T10:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5gpfv</id>
    <title>Brain surgery on LLMs via LoRA</title>
    <updated>2026-02-15T15:00:51+00:00</updated>
    <author>
      <name>/u/FeeMassive4003</name>
      <uri>https://old.reddit.com/user/FeeMassive4003</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you‚Äôve been playing with LoRA, you know you can fine-tune a model by only touching specific &amp;quot;parts&amp;quot; of its brain. I decided to run a controlled experiment using a Qwen-2.5 3B model to see how it modifies its behaviour as a result of adapting different parts of its layers.&lt;/p&gt; &lt;p&gt;The domain I work in is AI academic systems. The goal here in this particular application was to generate a memorandum to the advisor about a given student. The prompt used for all tests was a strict persona instruction: it required the model to act as &amp;quot;Academic AI,&amp;quot; an academic advisor, and write a professional memo in a flowing narrative style (no bullets) based on raw student data including dropout risk, quiz scores, and discussion posts.&lt;/p&gt; &lt;p&gt;I tested 5 different LoRA configurations on this one prompt. Here is how this &amp;quot;brain surgery&amp;quot; affects LLM behavior.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;the raw model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;behavior: pure text completion. it fails to act as an &amp;quot;assistant&amp;quot; and instead just continues the document like it's a blank notepad.&lt;/p&gt; &lt;p&gt;example: &amp;quot;student dropout risk assessment student: user **&lt;strong&gt;&lt;em&gt;, course *&lt;/em&gt;&lt;/strong&gt;* assessment date: 2025-11-21 23:03:33+00:00 predicted dropout risk: 55%...additional data points for student ***** include previous enrollment in course ***** and a high interest in radiology. critical: when referencing student communications, you must quote relevant excerpts and cite specific dates. assistant: i will now analyze the data. user: please begin the memo. assistant: certainly, here is the analysis for the academic staff...&amp;quot;&lt;/p&gt; &lt;p&gt;the model leaked the instructions and started a hallucinated dialogue because it doesn't &amp;quot;know&amp;quot; it's supposed to follow a system prompt; it just thinks it's part of a transcript. I remind this is only a 3B model.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;model adapted on the attention layers (q, k, v, o)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;behavior: high-fidelity instruction following. it balances the persona with the data perfectly.&lt;/p&gt; &lt;p&gt;example: &amp;quot;memorandum&lt;/p&gt; &lt;p&gt;to: academic advisory team&lt;/p&gt; &lt;p&gt;from: academic ai assistant&lt;/p&gt; &lt;p&gt;i am writing to report on the 55% dropout risk identified for user **&lt;strong&gt;&lt;em&gt;. a review of the data shows a significant drop in performance on november 16th, where the student scored *&lt;/em&gt;.* on quiz **&lt;/strong&gt;** at 21:04:02 (-0600 utc). while the student expressed on october 31st that they find the 'coordination and precision' of c-arm usage 'amazing,' their fluctuating quiz scores (reaching *&lt;em&gt;.&lt;/em&gt; then dropping back to *&lt;em&gt;.&lt;/em&gt;) suggest a struggle with consistent application. i recommend immediate one-on-one tutoring to bridge this gap.&amp;quot;&lt;/p&gt; &lt;p&gt;analysis: this is the highest quality version. it followed the &amp;quot;narrative&amp;quot; rule, used the empathetic persona, and correctly extracted the timestamps.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;model adapted on the mlp only&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;behavior: data extraction powerhouse, but &amp;quot;instruction-blind.&amp;quot;&lt;/p&gt; &lt;p&gt;example: &amp;quot;academic ai assistant report&lt;/p&gt; &lt;p&gt;risk factors:&lt;/p&gt; &lt;p&gt;median score: *&lt;em&gt;.&lt;/em&gt; (medium impact)&lt;/p&gt; &lt;p&gt;avg days late: &lt;em&gt;.&lt;/em&gt;** (low impact)&lt;/p&gt; &lt;p&gt;quiz timeline:&lt;/p&gt; &lt;p&gt;2025-11-16: scored &lt;strong&gt;.* (quiz **&lt;/strong&gt;**)&lt;/p&gt; &lt;p&gt;2025-11-16: scored &lt;strong&gt;.* (quiz **&lt;/strong&gt;**)&lt;/p&gt; &lt;p&gt;2025-10-31: scored &lt;strong&gt;.* (quiz **&lt;/strong&gt;**)&lt;/p&gt; &lt;p&gt;i recommend monitoring the student's 47.1% late submission rate. Academic ai assistant.&amp;quot;&lt;/p&gt; &lt;p&gt;analysis: it completely ignored the &amp;quot;flowing narrative&amp;quot; and &amp;quot;no bullets&amp;quot; rules. mlp layers hold the &amp;quot;knowledge,&amp;quot; so the model got the numbers right but lost the behavioral constraints.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;model adapted on k and q only&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;behavior: the &amp;quot;search engine&amp;quot; mode. it finds data well but can't write a coherent story.&lt;/p&gt; &lt;p&gt;example: &amp;quot;academic ai assistant analysis. the student ***** in course ***** has a risk of 55%. the data shows scores of *&lt;em&gt;.&lt;/em&gt; on 2025-11-16, *&lt;em&gt;.&lt;/em&gt; on 2025-11-16, *&lt;em&gt;.&lt;/em&gt; on 2025-10-31, *&lt;em&gt;.&lt;/em&gt; on 2025-11-04, and *&lt;em&gt;.&lt;/em&gt; on 2025-11-14. the student said on october 30th 'obviously lol' and mentioned 'buhurt.' the priority is academic support.&amp;quot;&lt;/p&gt; &lt;p&gt;analysis: it is very effective at &amp;quot;finding&amp;quot; the details in the text (k/q are indexing matrices), but the prose is disjointed. it is a list of facts masquerading as a sentence.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;model adapted on v only&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;behavior: the &amp;quot;creative writer&amp;quot; that prone to bringing in outside information.&lt;/p&gt; &lt;p&gt;example: &amp;quot;dear academic team, it is my duty as academic ai assistant to share concerns regarding user **&lt;strong&gt;&lt;em&gt;. they are an enthusiastic student who enjoys 'magic the gathering' and 'buhurt,' as noted in their october 30th post. however, their 55% risk profile is troubling. according to the *&lt;/em&gt;&lt;/strong&gt;*** textbook (2024), student success in radiography requires 'precision and confidence,' which user ***** is currently lacking. we must intervene with a high-priority wellness check.&amp;quot;&lt;/p&gt; &lt;p&gt;analysis: the value (v) matrix handles the &amp;quot;content&amp;quot; of the response. this version writes the most &amp;quot;human&amp;quot; sounding prose, but it brought in outside information (the book citation) that wasn't in the prompt. it is too &amp;quot;creative&amp;quot; with the source material.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeeMassive4003"&gt; /u/FeeMassive4003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5gpfv/brain_surgery_on_llms_via_lora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5gpfv/brain_surgery_on_llms_via_lora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5gpfv/brain_surgery_on_llms_via_lora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5htyt</id>
    <title>Good local setup for LLM training/finetuning?</title>
    <updated>2026-02-15T15:47:20+00:00</updated>
    <author>
      <name>/u/Glittering-Hat-7629</name>
      <uri>https://old.reddit.com/user/Glittering-Hat-7629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;This is my first post on reddit, sorry in advance if this is a naive question. I am a PhD student working on ML/RL theory, and I don't have access to compute at my university. Over the past year, I have been trying to transition toward empirical work on LLMs (e.g., for reasoning), but it has been frustratingly hard to do so in my current environment. No one in my lab cares about LLMs or any kind of empirical research, so it's difficult to do it on my own.&lt;/p&gt; &lt;p&gt;I initially hoped to rely on available grants to get access to compute, but most options I have found seem tailored to people who already have a precise idea in mind. This is obviously not my case yet, and I find it hard to come up with a sensible project description without (i) anyone around to help me navigate a very noisy literature to find sensible problems (e.g., still largely unsolved), and (ii) no compute to run even basic experiments (I don't even have a GPU on my laptop).&lt;/p&gt; &lt;p&gt;That is what brings me here. Recently, I have been considering buying my own setup with personal funds so I can experiment with whatever idea I have. I mostly hang out on X, found this community through people posting there (especially &amp;quot;TheAhmadOsman&amp;quot; who is quite active), and figured reddit would be more appropriate to ask my questions.&lt;/p&gt; &lt;p&gt;Most of what I see discussed is hardware for inference and the benefits of running models locally (privacy, control, etc.). My use case is different: for my day-to-day work (80% math/ML research, 10% random questions, 10% English writing), I don't see myself moving away from frontier models, as I think they'll always be way ahead when it comes to maths/code. What I want is a setup that lets me do small-scale LLM research and iterate quickly, even if I'm limited to relatively small models (say, up to ~2B).&lt;/p&gt; &lt;p&gt;From what I have read, the main options people debate are: (i) some NVIDIA GPU (e.g., RTX 6000 or else + other necessary parts), or (ii) a Mac Mini/Studio. The usual argument for (i) seems to be higher throughput, and for (ii) lower power consumption and a smoother setup experience.&lt;/p&gt; &lt;p&gt;My questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If the goal is to do LLM research and iterate quickly while accepting a small-model constraint, what would you recommend?&lt;/li&gt; &lt;li&gt;In that context, does the electricity cost difference between a GPU workstation and a Mac matter, or is it usually negligible?&lt;/li&gt; &lt;li&gt;Are there alternatives I am overlooking?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Otherwise, I am happy to take any advice on how to get started (I am honestly so new to this that I don't even know what the standard libraries/tooling stack is).&lt;/p&gt; &lt;p&gt;Thanks in advance!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Hat-7629"&gt; /u/Glittering-Hat-7629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5htyt/good_local_setup_for_llm_trainingfinetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5htyt/good_local_setup_for_llm_trainingfinetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5htyt/good_local_setup_for_llm_trainingfinetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5bt2s</id>
    <title>Local-first AI NPC desktop with self-hosted gateways, agent gameplay, and multi-LLM support (openClaw Desktop)</title>
    <updated>2026-02-15T10:53:51+00:00</updated>
    <author>
      <name>/u/stormixus</name>
      <uri>https://old.reddit.com/user/stormixus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5bt2s/localfirst_ai_npc_desktop_with_selfhosted/"&gt; &lt;img alt="Local-first AI NPC desktop with self-hosted gateways, agent gameplay, and multi-LLM support (openClaw Desktop)" src="https://preview.redd.it/n18h0jnh2njg1.gif?frame=1&amp;amp;width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=888dfc24471f228ab81b5cf4c7bbca932df6771f" title="Local-first AI NPC desktop with self-hosted gateways, agent gameplay, and multi-LLM support (openClaw Desktop)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with building a &lt;strong&gt;local-first AI desktop&lt;/strong&gt; that works with self-hosted gateways and local LLM setups.&lt;/p&gt; &lt;p&gt;Instead of another browser chat UI, this project explores an &lt;strong&gt;NPC-style desktop interface&lt;/strong&gt; where agents, games, and document workflows live together.&lt;/p&gt; &lt;h1&gt;Current features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üß† Works with local or remote LLM gateways&lt;/li&gt; &lt;li&gt;üé≠ NPC interaction mode using [face:], [act:] directives&lt;/li&gt; &lt;li&gt;üîå Multi-gateway architecture (switch models/sessions)&lt;/li&gt; &lt;li&gt;üìÑ Forge workspace (OCR + agent-assisted editing)&lt;/li&gt; &lt;li&gt;üéÆ Built-in AI game hub&lt;/li&gt; &lt;li&gt;ü§ñ Agent vs Agent gameplay experiments&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why I built this&lt;/h1&gt; &lt;p&gt;Most local LLM tools feel like wrappers around chat.&lt;/p&gt; &lt;p&gt;I wanted to try something closer to a &lt;strong&gt;local AI environment&lt;/strong&gt; ‚Äî almost like an experimental AI desktop.&lt;/p&gt; &lt;p&gt;It‚Äôs still very much a playground, but I‚Äôm curious what people here think about the NPC + agent interaction direction.&lt;/p&gt; &lt;p&gt;Repo &amp;amp; demos:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/stormixus/openClaw-Desktop"&gt;https://github.com/stormixus/openClaw-Desktop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback welcome ‚Äî especially from anyone running Ollama / local gateways.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stormixus"&gt; /u/stormixus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r5bt2s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5bt2s/localfirst_ai_npc_desktop_with_selfhosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5bt2s/localfirst_ai_npc_desktop_with_selfhosted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T10:53:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5fs69</id>
    <title>GLM-4.7-Flash (IQ5_K GGUF) Bench: CPU-only vs Hybrid (exps=CPU) vs Full GPU (RTX PRO 6000 Blackwell, EPYC 9175F)</title>
    <updated>2026-02-15T14:20:35+00:00</updated>
    <author>
      <name>/u/Express-Jicama-9827</name>
      <uri>https://old.reddit.com/user/Express-Jicama-9827</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;author:~$ Non-native English; AI helped with translation/structure. All numbers are from my logs.üôá &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I benchmarked &lt;strong&gt;GLM-4.7-Flash (IQ5_K GGUF)&lt;/strong&gt; across three different execution modes. The goal was to quantify the performance impact of offloading MoE (Mixture of Experts) to the CPU versus keeping everything on the GPU, especially with high-end server hardware.&lt;/p&gt; &lt;h1&gt;Environment&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX PRO 6000 Blackwell Max-Q 96GB (1GPU)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD EPYC 9175F (Zen 5, L3 512MB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; ik_llama.cpp&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: ubergarm/GLM-4.7-Flash-GGUF/IQ5_K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context:&lt;/strong&gt; 131,072 configured (~30k used in these runs)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary Comparison Table&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Pattern&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;PP Speed(tok/s)&lt;/th&gt; &lt;th align="left"&gt;TG Speed(tok/s)&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Efficiency / Notes&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;A&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU-only&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;100.32&lt;/td&gt; &lt;td align="left"&gt;20.23&lt;/td&gt; &lt;td align="left"&gt;Pure CPU, slow at ~30k used. (131k ctx)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;exps=CPU (Hybrid)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1635.35&lt;/td&gt; &lt;td align="left"&gt;66.84&lt;/td&gt; &lt;td align="left"&gt;16x PP boost over CPU-only.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;C&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;exps on GPU (Full)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3723.34&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;99.42&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Near 100 tok/s generation.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Detailed Logs &amp;amp; Metrics&lt;/h1&gt; &lt;h1&gt;Pattern A: CPU-only (Baseline)&lt;/h1&gt; &lt;p&gt;Pure CPU execution. Prompt processing is slow, and generation feels sluggish for long-form content.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;PP(tok)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;TG(tok)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Ctx_used&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;T_PP(s)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;S_PP(tok/s)&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;T_TG(s)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;S_TG(tok/s)&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;total(s)&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;31151&lt;/td&gt; &lt;td align="left"&gt;427&lt;/td&gt; &lt;td align="left"&gt;31577&lt;/td&gt; &lt;td align="left"&gt;310.51&lt;/td&gt; &lt;td align="left"&gt;100.32&lt;/td&gt; &lt;td align="left"&gt;19.85&lt;/td&gt; &lt;td align="left"&gt;21.51&lt;/td&gt; &lt;td align="left"&gt;330.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;980&lt;/td&gt; &lt;td align="left"&gt;6284&lt;/td&gt; &lt;td align="left"&gt;38413&lt;/td&gt; &lt;td align="left"&gt;21.51&lt;/td&gt; &lt;td align="left"&gt;45.55&lt;/td&gt; &lt;td align="left"&gt;316.57&lt;/td&gt; &lt;td align="left"&gt;19.85&lt;/td&gt; &lt;td align="left"&gt;338.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;2886&lt;/td&gt; &lt;td align="left"&gt;2921&lt;/td&gt; &lt;td align="left"&gt;37935&lt;/td&gt; &lt;td align="left"&gt;59.46&lt;/td&gt; &lt;td align="left"&gt;48.53&lt;/td&gt; &lt;td align="left"&gt;151.03&lt;/td&gt; &lt;td align="left"&gt;19.34&lt;/td&gt; &lt;td align="left"&gt;210.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;35017&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;9632&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;37935&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;391.49&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89.44&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;487.47&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;19.76&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;878.96&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Pattern B: Hybrid (-ot exps=CPU)&lt;/h1&gt; &lt;p&gt;Offloading only MoE Experts to EPYC while keeping Attention on GPU. Massive leap in PP speed.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;PP(tok)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;TG(tok)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Ctx_used&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;T_PP(s)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;S_PP(tok/s)&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;T_TG(s)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;S_TG(tok/s)&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;total(s)&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;31151&lt;/td&gt; &lt;td align="left"&gt;774&lt;/td&gt; &lt;td align="left"&gt;31924&lt;/td&gt; &lt;td align="left"&gt;19.04&lt;/td&gt; &lt;td align="left"&gt;1635.35&lt;/td&gt; &lt;td align="left"&gt;11.05&lt;/td&gt; &lt;td align="left"&gt;70.01&lt;/td&gt; &lt;td align="left"&gt;30.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;981&lt;/td&gt; &lt;td align="left"&gt;4091&lt;/td&gt; &lt;td align="left"&gt;36221&lt;/td&gt; &lt;td align="left"&gt;1.23&lt;/td&gt; &lt;td align="left"&gt;792.91&lt;/td&gt; &lt;td align="left"&gt;61.01&lt;/td&gt; &lt;td align="left"&gt;67.04&lt;/td&gt; &lt;td align="left"&gt;62.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;2388&lt;/td&gt; &lt;td align="left"&gt;2692&lt;/td&gt; &lt;td align="left"&gt;37209&lt;/td&gt; &lt;td align="left"&gt;2.65&lt;/td&gt; &lt;td align="left"&gt;900.82&lt;/td&gt; &lt;td align="left"&gt;40.62&lt;/td&gt; &lt;td align="left"&gt;66.26&lt;/td&gt; &lt;td align="left"&gt;43.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;874&lt;/td&gt; &lt;td align="left"&gt;2106&lt;/td&gt; &lt;td align="left"&gt;37496&lt;/td&gt; &lt;td align="left"&gt;1.40&lt;/td&gt; &lt;td align="left"&gt;619.90&lt;/td&gt; &lt;td align="left"&gt;31.85&lt;/td&gt; &lt;td align="left"&gt;66.10&lt;/td&gt; &lt;td align="left"&gt;33.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;35394&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;9663&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;37496&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;24.34&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1453.76&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;144.56&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;66.84&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;168.90&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Pattern C: Full GPU (no exps=CPU)&lt;/h1&gt; &lt;p&gt;Maximum performance. Prompt evaluation is nearly instantaneous.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;PP(tok)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;TG(tok)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Ctx_used&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;T_PP(s)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;S_PP(tok/s)&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;T_TG(s)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;S_TG(tok/s)&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;total(s)&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;31151&lt;/td&gt; &lt;td align="left"&gt;630&lt;/td&gt; &lt;td align="left"&gt;31780&lt;/td&gt; &lt;td align="left"&gt;8.36&lt;/td&gt; &lt;td align="left"&gt;3723.34&lt;/td&gt; &lt;td align="left"&gt;5.90&lt;/td&gt; &lt;td align="left"&gt;106.67&lt;/td&gt; &lt;td align="left"&gt;14.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;981&lt;/td&gt; &lt;td align="left"&gt;4325&lt;/td&gt; &lt;td align="left"&gt;36455&lt;/td&gt; &lt;td align="left"&gt;0.59&lt;/td&gt; &lt;td align="left"&gt;1638.04&lt;/td&gt; &lt;td align="left"&gt;43.61&lt;/td&gt; &lt;td align="left"&gt;99.16&lt;/td&gt; &lt;td align="left"&gt;44.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;2373&lt;/td&gt; &lt;td align="left"&gt;1918&lt;/td&gt; &lt;td align="left"&gt;36420&lt;/td&gt; &lt;td align="left"&gt;1.46&lt;/td&gt; &lt;td align="left"&gt;1619.97&lt;/td&gt; &lt;td align="left"&gt;19.60&lt;/td&gt; &lt;td align="left"&gt;97.84&lt;/td&gt; &lt;td align="left"&gt;21.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;34505&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;6873&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;36420&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;10.43&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3308.19&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.12&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;99.43&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.55&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Video:&lt;/p&gt; &lt;p&gt;cpu-only:0:00~&lt;/p&gt; &lt;p&gt;hybrid(exps=CPU:05:07~&lt;/p&gt; &lt;p&gt;hybrid(no exps=CPU):07:50~&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r5fs69/video/tk101l9j1ojg1/player"&gt;https://reddit.com/link/1r5fs69/video/tk101l9j1ojg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express-Jicama-9827"&gt; /u/Express-Jicama-9827 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fs69/glm47flash_iq5_k_gguf_bench_cpuonly_vs_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fs69/glm47flash_iq5_k_gguf_bench_cpuonly_vs_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fs69/glm47flash_iq5_k_gguf_bench_cpuonly_vs_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T14:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r56ak1</id>
    <title>Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon</title>
    <updated>2026-02-15T05:19:47+00:00</updated>
    <author>
      <name>/u/PrimaryAbility9</name>
      <uri>https://old.reddit.com/user/PrimaryAbility9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r56ak1/groundup_mlx_reimplementation_of_qwen3asr_for/"&gt; &lt;img alt="Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon" src="https://external-preview.redd.it/KDZqgIfnwVfMzphVYrHDwwZwiqbRfErXcNBAe6CZe0k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=255b94cddc71c3ada0252399ffc5d7764559ae69" title="Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon&lt;/p&gt; &lt;p&gt;Qwen3-ASR is the new open-source SOTA model for ASR and this can now run natively on M-series GPUs.&lt;/p&gt; &lt;p&gt;pip install mlx-qwen3-asr&lt;/p&gt; &lt;p&gt;Benchmarks (M4 Pro, 0.6B fp16):&lt;br /&gt; - 2.5s clip: 0.46s, RTF 0.08 &lt;br /&gt; - 10s clip: 0.83s, RTF 0.08&lt;br /&gt; - 4-bit quantized: 4.7x faster, WER 2.29% ‚Üí 2.72% (LibriSpeech test-clean, n=100)&lt;br /&gt; - vs official PyTorch on multilingual-100: 15.99% vs 16.69% WER&lt;/p&gt; &lt;p&gt;Features:&lt;br /&gt; - 0.6B and 1.7B models, 52 languages&lt;br /&gt; - Word-level timestamps (native MLX forced aligner)&lt;br /&gt; - 4-bit / 8-bit quantization&lt;br /&gt; - Streaming and speculative decoding (experimental)&lt;br /&gt; - Output: txt, json, srt, vtt, tsv&lt;br /&gt; - 393 tests, all benchmarks backed by committed JSON artifacts&lt;/p&gt; &lt;p&gt;4 dependencies: mlx, numpy, regex, huggingface-hub.&lt;br /&gt; PyTorch, no transformers in the inference path.&lt;/p&gt; &lt;p&gt;Memory: ~1.2 GB (0.6B), ~3.4 GB (1.7B)&lt;/p&gt; &lt;p&gt;P.S. This is what claude &amp;amp; codex worked on for valentine's day. Speaker diarization is coming soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrimaryAbility9"&gt; /u/PrimaryAbility9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/moona3k/mlx-qwen3-asr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r56ak1/groundup_mlx_reimplementation_of_qwen3asr_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r56ak1/groundup_mlx_reimplementation_of_qwen3asr_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T05:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4yg6p</id>
    <title>[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)</title>
    <updated>2026-02-14T22:53:06+00:00</updated>
    <author>
      <name>/u/Educational_Cry_7951</name>
      <uri>https://old.reddit.com/user/Educational_Cry_7951</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I have been working on &lt;strong&gt;AdaLLM&lt;/strong&gt; (repo: &lt;a href="https://github.com/BenChaliah/NVFP4-on-4090-vLLM"&gt;https://github.com/BenChaliah/NVFP4-on-4090-vLLM&lt;/a&gt;) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Please think of giving the Github repo a STAR if you like it :)&lt;/strong&gt; &lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Why this is interesting&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.&lt;/li&gt; &lt;li&gt;Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).&lt;/li&gt; &lt;li&gt;No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.&lt;/li&gt; &lt;li&gt;Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarks (RTX 4090)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3-8B-NVFP4&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;batch&lt;/th&gt; &lt;th align="left"&gt;total tokens&lt;/th&gt; &lt;th align="left"&gt;seconds&lt;/th&gt; &lt;th align="left"&gt;tok/s&lt;/th&gt; &lt;th align="left"&gt;peak GB&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;3.3867&lt;/td&gt; &lt;td align="left"&gt;37.79&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;256&lt;/td&gt; &lt;td align="left"&gt;3.5471&lt;/td&gt; &lt;td align="left"&gt;72.17&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;3.4392&lt;/td&gt; &lt;td align="left"&gt;148.87&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;3.4459&lt;/td&gt; &lt;td align="left"&gt;297.16&lt;/td&gt; &lt;td align="left"&gt;7.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;2048&lt;/td&gt; &lt;td align="left"&gt;4.3636&lt;/td&gt; &lt;td align="left"&gt;469.34&lt;/td&gt; &lt;td align="left"&gt;7.56&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Gemma3-27B-it-NVFP4&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;batch&lt;/th&gt; &lt;th align="left"&gt;total tokens&lt;/th&gt; &lt;th align="left"&gt;seconds&lt;/th&gt; &lt;th align="left"&gt;tok/s&lt;/th&gt; &lt;th align="left"&gt;peak GB&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;9.3982&lt;/td&gt; &lt;td align="left"&gt;13.62&lt;/td&gt; &lt;td align="left"&gt;19.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;256&lt;/td&gt; &lt;td align="left"&gt;9.5545&lt;/td&gt; &lt;td align="left"&gt;26.79&lt;/td&gt; &lt;td align="left"&gt;19.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;9.5344&lt;/td&gt; &lt;td align="left"&gt;53.70&lt;/td&gt; &lt;td align="left"&gt;19.84&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;for Qwen3-8B-NVFP4 I observed ~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with ~20-25% throughput loss).&lt;/p&gt; &lt;h1&gt;Quickstart&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git adallm serve nvidia/Qwen3-8B-NVFP4 &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;`export NVFP4_FP8=1` is optional and enables FP8 GEMM path (NVFP4_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Supported models (so far)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;nvidia/Qwen3-8B-NVFP4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;BenChaliah/Gemma3-27B-it-NVFP4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Qwen3 MoE variants are supported, but still slow (see README for MoE notes).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;MoE routing and offload paths are not fully optimized yet (working on it currently)&lt;/li&gt; &lt;li&gt;Only NVFP4 weights, no FP16 fallback for decode by design.&lt;/li&gt; &lt;li&gt;Targeted at Ada Lovelace (sm_89). Needs validation on other Ada cards.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Repo&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/BenChaliah/NVFP4-on-4090-vLLM"&gt;https://github.com/BenChaliah/NVFP4-on-4090-vLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Cry_7951"&gt; /u/Educational_Cry_7951 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T22:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r55dhq</id>
    <title>Opencode Manager</title>
    <updated>2026-02-15T04:29:30+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r55dhq/opencode_manager/"&gt; &lt;img alt="Opencode Manager" src="https://external-preview.redd.it/AUQf-LOorsrfdWmtZxI5-8VZ7v2JlosFkQ5o_bNyMlg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992d846e4a25e907443be4fd06878fda1fbed597" title="Opencode Manager" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Opencode for your phone. Deployable docker container with Git / File browser / speech to text / text to speech / push notifications and much more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chriswritescode-dev/opencode-manager"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r55dhq/opencode_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r55dhq/opencode_manager/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T04:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4n3as</id>
    <title>Heretic 1.2 released: 70% lower VRAM usage with quantization, Magnitude-Preserving Orthogonal Ablation ("derestriction"), broad VL model support, session resumption, and more</title>
    <updated>2026-02-14T15:14:00+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llamas and Gentlemen,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heretic&lt;/strong&gt; (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;) is the leading software for removing censorship from language models. In the three months since its initial release, &lt;a href="https://huggingface.co/models?other=heretic"&gt;more than 1,300 models&lt;/a&gt; (including quants) made using Heretic have been published by the community. This represents more than a third of all abliterated models ever published, and the vast majority of abliterated models published since Heretic's first release.&lt;/p&gt; &lt;p&gt;Today, I am happy to announce the release of Heretic 1.2, the product of two months of hard work by the Heretic contributors.&lt;/p&gt; &lt;p&gt;The headline feature is the new LoRA-based abliteration engine implemented by accemlcc. Built on top of PEFT, it supports loading models with 4-bit quantization using bitsandbytes, which can reduce VRAM requirements for processing a model by up to 70%. The abliterated model is still exported in full precision, which is achieved by re-loading the original model in system RAM and applying the optimized LoRA adapter on top of it, yielding a high-quality model despite the low resource requirements. To enable quantized loading, set &lt;code&gt;quantization&lt;/code&gt; to &lt;code&gt;bnb_4bit&lt;/code&gt; in the configuration.&lt;/p&gt; &lt;p&gt;spikymoth implemented Magnitude-Preserving Orthogonal Ablation (MPOA) aka Norm-Preserving Biprojected Abliteration aka &amp;quot;derestriction&amp;quot;, a refined abliteration technique developed by Jim Lai which can improve the quality of the resulting model in many cases. This has been one of the most frequently requested features from the community, and is now finally available. To enable MPOA, set &lt;code&gt;orthogonalize_direction&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;row_normalization&lt;/code&gt; to &lt;code&gt;full&lt;/code&gt; in the configuration.&lt;/p&gt; &lt;p&gt;Heretic's implementation of MPOA uses Optuna to optimize weight parameters. This can result in models that are better than those generated with the original MPOA technique, which employs a different strategy for layer selection. For example, &lt;code&gt;MuXodious/gpt-oss-20b-RichardErkhov-heresy&lt;/code&gt; dominates &lt;code&gt;ArliAI/gpt-oss-20b-Derestricted&lt;/code&gt; on the UGI Leaderboard, scoring 39.05 vs 34.22 and beating the derestricted model in every individual test (W/10, NatInt, and Writing).&lt;/p&gt; &lt;p&gt;After a long history of hacks being passed around in the community, anrp finally found a clean way to support vision language models in Heretic, and a broad range of VL models can now be processed. Note that only the language model part (the text decoder transformer) is abliterated, not the image encoder.&lt;/p&gt; &lt;p&gt;anrp also implemented fully automatic session progress saving and resumption. This means worrying about crashes during a long optimization run is now a thing of the past, as you can simply restart Heretic and it will offer to continue where it left off. You can also interrupt the run yourself at any time with Ctrl+C, and resume it later.&lt;/p&gt; &lt;p&gt;Please see the release notes for the full list of improvements and fixes. More exciting stuff is coming in future versions!&lt;/p&gt; &lt;p&gt;Cheers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T15:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5fxyd</id>
    <title>Qwen3-Code-Next ggufs: Any difference between Q4KXL and MXPF4?</title>
    <updated>2026-02-15T14:27:39+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The later is a few GBs smaller, but are there any meaningful differences performance wise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fxyd/qwen3codenext_ggufs_any_difference_between_q4kxl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fxyd/qwen3codenext_ggufs_any_difference_between_q4kxl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fxyd/qwen3codenext_ggufs_any_difference_between_q4kxl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T14:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5hp3a</id>
    <title>GLM 5 vs Claude Opus 4.6: the paradox of paying $100 / $200 per month and still chasing hype</title>
    <updated>2026-02-15T15:41:44+00:00</updated>
    <author>
      <name>/u/willymunoz</name>
      <uri>https://old.reddit.com/user/willymunoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve had a hard-to-ignore sense of paradox for weeks now. Just a month ago, a lot of us were paying $100 / $200 to Anthropic (for example via Claude Code) for a level of capability that, at the time, felt ‚Äúworth‚Äù the price. Today, Claude Opus 4.6 is clearly more refined‚Äîbut then GLM 5 shows up pushing incredibly hard, setting records and closing the gap (or outright surpassing it in some areas) relative to the kind of capability that, not long ago, cost exactly those $100 / $200. And yet, the default behavior is still to keep paying the same amount for Claude, as if the ‚Äúvalue‚Äù equation hasn‚Äôt changed.&lt;/p&gt; &lt;p&gt;What bothers me isn‚Äôt only the technical comparison‚Äîit‚Äôs the mismatch between &lt;strong&gt;real value&lt;/strong&gt; and delivery speed. Capability leaps arrive so quickly that the monthly price starts looking less like payment for performance and more like a psychological toll to avoid falling behind. That‚Äôs where FOMO kicks in: we‚Äôd rather avoid ‚Äúbeing a few weeks behind‚Äù even when the market is clearly offering alternatives that are increasingly close‚Äîand sometimes better for specific tasks‚Äîfor the same money or less.&lt;/p&gt; &lt;p&gt;There‚Äôs also something that feels, at minimum, notable: on the ARC-AGI-2 leaderboard, I don‚Äôt see Chinese models (for example, GLM 5). I‚Äôm not saying this as an accusation‚Äîmore as a question about how these narratives of ‚Äúwho‚Äôs ahead‚Äù get constructed, and what gets left outside the frame.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What inclusion criteria are being used (access, licensing, reproducibility, APIs, etc.)?&lt;/li&gt; &lt;li&gt;To what extent does the leaderboard reflect raw capability vs availability/participation from certain actors?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And this is where the fatigue hits: we‚Äôre in a cycle where performance improves at a brutal pace, but our purchasing decisions behave as if pricing were static and viable alternatives didn‚Äôt exist. Even knowing that the predictive inference paradigm (and these rapid improvements) has made us better workers‚Äîfaster, more capable, more productive‚Äîwe still act as if the only thing that matters is ‚Äúnot missing the train‚Äù of this week‚Äôs model.&lt;/p&gt; &lt;p&gt;Does this paradox bother anyone else? How are you rationalizing it day to day‚Äîby actual ROI (use cases) or by the peace of mind of not falling behind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willymunoz"&gt; /u/willymunoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5hp3a/glm_5_vs_claude_opus_46_the_paradox_of_paying_100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5hp3a/glm_5_vs_claude_opus_46_the_paradox_of_paying_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5hp3a/glm_5_vs_claude_opus_46_the_paradox_of_paying_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r58zep</id>
    <title>MiniMax-M2.5 REAP models available on HF</title>
    <updated>2026-02-15T07:58:25+00:00</updated>
    <author>
      <name>/u/Look_0ver_There</name>
      <uri>https://old.reddit.com/user/Look_0ver_There</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just noticed that a bunch of REAP variants for MiniMax M2.5 got pushed to HF here: &lt;a href="https://huggingface.co/Akicou/models"&gt;https://huggingface.co/Akicou/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been messing about flipping between Qwen Coder Next and MiniMax M2.5, and just personally I've been preferring MiniMax. QCN does eventually get things right, but I find that I have to babysit it and nudge it fairly heavily, whereas MiniMax, while a lot more verbose, does seem to require less hand-holding.&lt;/p&gt; &lt;p&gt;That's just my take though. I'm running on a 128GB Strix Halo though, and I've had to run with Unsloth's Q3_K_XL quants just to make MiniMax fit with a large enough context such that the system isn't begging for mercy after 3 prompts.&lt;/p&gt; &lt;p&gt;Anyway, that HF account there has 19, 29, 39, and 50% REAPS available. Presently just safetensors, but they're easy to convert. I'm going to mess about with the 19% and 29% REAPS, and see how they work out. Hope others may find these useful too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Look_0ver_There"&gt; /u/Look_0ver_There &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58zep/minimaxm25_reap_models_available_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58zep/minimaxm25_reap_models_available_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r58zep/minimaxm25_reap_models_available_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T07:58:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5flim</id>
    <title>I ran System Design tests on GLM-5, Kimi k2.5, Qwen 3, and more. Here are the results.</title>
    <updated>2026-02-15T14:12:22+00:00</updated>
    <author>
      <name>/u/Ruhal-Doshi</name>
      <uri>https://old.reddit.com/user/Ruhal-Doshi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5flim/i_ran_system_design_tests_on_glm5_kimi_k25_qwen_3/"&gt; &lt;img alt="I ran System Design tests on GLM-5, Kimi k2.5, Qwen 3, and more. Here are the results." src="https://preview.redd.it/7cntqdzu1ojg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de85c7b2616818398271df1c764abc5555b38d72" title="I ran System Design tests on GLM-5, Kimi k2.5, Qwen 3, and more. Here are the results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I posted my System Design benchmark here and got roasted (rightfully so) for focusing on closed models.&lt;/p&gt; &lt;p&gt;I listened. I spent the weekend doing two things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Adding Open Weight Support:&lt;/strong&gt; I ran the benchmark against &lt;strong&gt;Qwen 3&lt;/strong&gt;, &lt;strong&gt;GLM-5&lt;/strong&gt;, and &lt;strong&gt;Kimi k2.5&lt;/strong&gt;. I tested them on the original problem (&lt;strong&gt;Design a ChatGPT-like Web App&lt;/strong&gt;) as well as a new, much harder problem: &lt;strong&gt;&amp;quot;Design an Enterprise RAG System (like Glean).&amp;quot;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Building a Scoring Platform:&lt;/strong&gt; I built &lt;a href="https://hldbench.com"&gt;&lt;strong&gt;hldbench.com&lt;/strong&gt;&lt;/a&gt; so you can actually browse the diagrams and architectural decisions. You can also &lt;strong&gt;score solutions&lt;/strong&gt; individually against a fixed set of parameters (Scalability, Completeness, etc.) to help build a community leaderboard.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Tool (Run it Locally):&lt;/strong&gt; The library is model-agnostic and supports &lt;strong&gt;OpenAI-compatible endpoints&lt;/strong&gt;. To be honest, I haven't tested it with purely local models (via Ollama/vLLM) myself yet, but that is next on my list. In the meantime, I‚Äôd really appreciate it if you could try running it locally and let me know if it breaks!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Ask:&lt;/strong&gt; Please check out the website and score some of the solutions if you have time. I would also love your feedback on the open source library if you try running it yourself.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Website:&lt;/strong&gt; &lt;a href="https://hldbench.com"&gt;hldbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Ruhal-Doshi/hld-bench"&gt;github.com/Ruhal-Doshi/hld-bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know which other models/quants I should add to the next run, or if you have any &lt;strong&gt;interesting problems&lt;/strong&gt; you'd like to see tested!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Once I have enough data points from the community, I‚Äôll compile and share the first &amp;quot;System Design Leaderboard.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ruhal-Doshi"&gt; /u/Ruhal-Doshi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7cntqdzu1ojg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5flim/i_ran_system_design_tests_on_glm5_kimi_k25_qwen_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5flim/i_ran_system_design_tests_on_glm5_kimi_k25_qwen_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T14:12:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5d9ax</id>
    <title>Step 3.5 and Minimax m. 2.5 on a local hardware - some tests (ik_llama)</title>
    <updated>2026-02-15T12:18:26+00:00</updated>
    <author>
      <name>/u/ZealousidealBunch220</name>
      <uri>https://old.reddit.com/user/ZealousidealBunch220</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"&gt; &lt;img alt="Step 3.5 and Minimax m. 2.5 on a local hardware - some tests (ik_llama)" src="https://preview.redd.it/c9gndrc3cnjg1.png?width=140&amp;amp;height=94&amp;amp;auto=webp&amp;amp;s=2ec4cba976ac1c2e02467f93831a94951a7c5b45" title="Step 3.5 and Minimax m. 2.5 on a local hardware - some tests (ik_llama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I did some llama-bench tests (on ik_llama.cpp fork - it has sota quants (iq4_kss and others, and is faster on prompt processing on both CPU only and CUDA + CPU option)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c9gndrc3cnjg1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d5b1bfd500f3eff470e671bcaf991ffbd5e4a793"&gt;on my machine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r2kfu09fcnjg1.png?width=2688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c3ad692f1fae786fa6baffeecb1682cc493410a"&gt;./ik_llama.cpp/build/bin/llama-bench -m /home/serv/.cache/huggingface/hub/models--ubergarm--Step-3.5-Flash-GGUF/snapshots/c1aefbd3ed11507a02ba452e8e6af10ba36352e8/smol-IQ4_KSS/Step-3.5-Flash-smol-IQ4_KSS-00001-of-00004.gguf --n-cpu-moe 43 -ngl 99 -t 64 -ctk q8_0 -ctv q8_0 -fa 1 -b 4096 -ub 4096 -r 5 -p 16000 -n 4000&lt;/a&gt;&lt;/p&gt; &lt;p&gt;step 3.5 - 529 on prompt (16k), 30 on text gen (4k)&lt;/p&gt; &lt;p&gt;(batch size 2048 instead of 4096 gives 300 tk/s on prompt)&lt;/p&gt; &lt;p&gt;step 3.5 is a GREAT model, it is very nuanced , but the thinking time and token consumption is crippling (up to 10k-20k tokens on thinking with all the details).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zpan44hvenjg1.png?width=2596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa3443f57c5fd18f7cabe57cfa3fee0a17e713a6"&gt;./ik_llama.cpp/build/bin/llama-bench -m /media/serv/E/MiniMax-M2.5-smol-IQ4_KSS-00001-of-00004.gguf --n-cpu-moe 54 -ngl 99 -t 64 -ctk q8_0 -ctv q8_0 -fa 1 -b 4096 -ub 4096 -r 2 -p 16000 -n 4000&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I didn‚Äôt want to wait as long as the five repeats used with step 3.5, so I ran only two repeats minimax m.2.5 - 470 on prompt (16), 26,5 on text gen (4k) &lt;/p&gt; &lt;p&gt;With the new models that are able to perform at the level of the top paid models I'm starting to have a feeling of freedom&lt;/p&gt; &lt;p&gt;I invite everyone to discuss the new models and the methods and optimizations for running them locally!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZealousidealBunch220"&gt; /u/ZealousidealBunch220 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T12:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r58ca8</id>
    <title>jdopensource/JoyAI-LLM-Flash ‚Ä¢ HuggingFace</title>
    <updated>2026-02-15T07:18:52+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt; &lt;img alt="jdopensource/JoyAI-LLM-Flash ‚Ä¢ HuggingFace" src="https://preview.redd.it/vkpqjjqj4mjg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ec8a0dbc966144fb6a5b31a4dec1781fb812a67e" title="jdopensource/JoyAI-LLM-Flash ‚Ä¢ HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vkpqjjqj4mjg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37e9ae1daf8fb794ef27f75590b6ad7557e0e326"&gt;https://preview.redd.it/vkpqjjqj4mjg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37e9ae1daf8fb794ef27f75590b6ad7557e0e326&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jdopensource/JoyAI-LLM-Flash"&gt;https://huggingface.co/jdopensource/JoyAI-LLM-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kl2loe9c0mjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1b1437da4ce6468f7f9b580b3a7f88bb359f23e9"&gt;https://preview.redd.it/kl2loe9c0mjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1b1437da4ce6468f7f9b580b3a7f88bb359f23e9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T07:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r50ohq</id>
    <title>Qwen3 Coder Next Speedup with Latest Llama.cpp</title>
    <updated>2026-02-15T00:34:02+00:00</updated>
    <author>
      <name>/u/StardockEngineer</name>
      <uri>https://old.reddit.com/user/StardockEngineer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like it released just a few hours ago. Previously, I was getting 80ish tokens, max, on either of my GPUS in any combination.&lt;/p&gt; &lt;p&gt;Now I'm over 110+ in dual and 130+ on my RTX Pro&lt;/p&gt; &lt;p&gt;PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/19375"&gt;https://github.com/ggml-org/llama.cpp/pull/19375&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update your llama.cpp.&lt;/p&gt; &lt;p&gt;Edit: This is for CUDA devices.&lt;/p&gt; &lt;p&gt;Previous: ``` ‚ùØ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0&lt;/p&gt; &lt;p&gt;ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 | 2470.78 ¬± 3.84 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 | 87.35 ¬± 0.48 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d500 | 2468.72 ¬± 23.27 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d500 | 85.99 ¬± 0.53 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d1000 | 2451.68 ¬± 19.96 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d1000 | 87.15 ¬± 0.57 |&lt;/p&gt; &lt;p&gt;build: e06088da0 (7972) ```&lt;/p&gt; &lt;p&gt;New ``` ‚ùØ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 &lt;/p&gt; &lt;p&gt;ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 | 2770.34 ¬± 3.40 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 | 118.63 ¬± 1.14 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d500 | 2769.27 ¬± 23.92 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d500 | 119.69 ¬± 1.65 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d1000 | 2753.07 ¬± 21.85 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d1000 | 112.34 ¬± 0.74 |&lt;/p&gt; &lt;p&gt;build: 079feab9e (8055) ```&lt;/p&gt; &lt;p&gt;RTX by itself on new build ``` ‚ùØ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 -dev CUDA1 ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | dev | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ------------ | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 | 3563.60 ¬± 4.35 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 | 132.09 ¬± 1.07 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 @ d500 | 3481.63 ¬± 33.66 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 @ d500 | 119.57 ¬± 1.43 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 @ d1000 | 3534.69 ¬± 30.89 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 @ d1000 | 131.07 ¬± 7.27 |&lt;/p&gt; &lt;p&gt;build: 079feab9e (8055) ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StardockEngineer"&gt; /u/StardockEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T00:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4sivv</id>
    <title>KaniTTS2 ‚Äî open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included.</title>
    <updated>2026-02-14T18:48:10+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"&gt; &lt;img alt="KaniTTS2 ‚Äî open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included." src="https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89937a54be408692a953dcd50857e8ea58cf59a4" title="KaniTTS2 ‚Äî open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just open-sourced KaniTTS2 - a text-to-speech model designed for real-time conversational use cases.&lt;/p&gt; &lt;p&gt;## Models:&lt;/p&gt; &lt;p&gt;Multilingual (English, Spanish), and English-specific with local accents. Language support is actively expanding - more languages coming in future updates&lt;/p&gt; &lt;p&gt;## Specs&lt;/p&gt; &lt;p&gt;* 400M parameters (BF16)&lt;/p&gt; &lt;p&gt;* 22kHz sample rate&lt;/p&gt; &lt;p&gt;* Voice Cloning&lt;/p&gt; &lt;p&gt;* ~0.2 RTF on RTX 5090&lt;/p&gt; &lt;p&gt;* 3GB GPU VRAM&lt;/p&gt; &lt;p&gt;* Pretrained on ~10k hours of speech&lt;/p&gt; &lt;p&gt;* Training took 6 hours on 8x H100s&lt;/p&gt; &lt;p&gt;## Full pretrain code - train your own TTS from scratch&lt;/p&gt; &lt;p&gt;This is the part we‚Äôre most excited to share. We‚Äôre releasing the complete pretraining framework so anyone can train a TTS model for their own language, accent, or domain.&lt;/p&gt; &lt;p&gt;## Links&lt;/p&gt; &lt;p&gt;* Pretrained model: &lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/nineninesix/kani-tts-2-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* English model: &lt;a href="https://huggingface.co/nineninesix/kani-tts-2-en"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Pretrain code: &lt;a href="https://github.com/nineninesix-ai/kani-tts-2-pretrain"&gt;https://github.com/nineninesix-ai/kani-tts-2-pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* HF Spaces: &lt;a href="https://huggingface.co/spaces/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/spaces/nineninesix/kani-tts-2-pt&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/nineninesix/kanitts-2-en"&gt;https://huggingface.co/spaces/nineninesix/kanitts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* License: Apache 2.0&lt;/p&gt; &lt;p&gt;Happy to answer any questions. Would love to see what people build with this, especially for underrepresented languages.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/swybh9pdaijg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T18:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r59th1</id>
    <title>Kreuzberg v4.3.0 and benchmarks</title>
    <updated>2026-02-15T08:49:49+00:00</updated>
    <author>
      <name>/u/Eastern-Surround7763</name>
      <uri>https://old.reddit.com/user/Eastern-Surround7763</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;we have two announcements to share about &lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Kreuzberg&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;First, we‚Äôve published a new set of comparative benchmarks with an interactive UI and fully reproducible results. We‚Äôve been working on these for quite some time, and the goal is to help developers understand how Kreuzberg behaves in real production scenarios and to make performance claims transparent and verifiable.&lt;/p&gt; &lt;p&gt;Second, we released Kreuzberg v4.3.0, which brings several improvements and adds PaddleOCR as an optional backend through a native Rust integration. This release is particularly important for teams working with Chinese and other East Asian languages, where Paddle models perform very well.&lt;/p&gt; &lt;p&gt;What is Kreuzberg?&lt;/p&gt; &lt;p&gt;Kreuzberg is an open-source (MIT-licensed) polyglot document intelligence framework written in Rust, with bindings for Python, TypeScript/JavaScript (Node, Bun, and WASM), Ruby, Java, Go, PHP, Elixir, and C#. It‚Äôs also available as a CLI tool, Docker image, REST API server, and MCP server.&lt;/p&gt; &lt;p&gt;In practical terms, Kreuzberg helps you extract text, metadata, tables, and structured information from 75+ document and image formats, perform OCR, and prepare data for search, embeddings, or LLM pipelines. This kind of preprocessing step is necessary in many AI applications, document workflows, and data pipelines, where the quality of ingestion directly affects downstream results.&lt;/p&gt; &lt;p&gt;Comparative benchmarks: &lt;a href="https://kreuzberg.dev/benchmarks"&gt;https://kreuzberg.dev/benchmarks&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The new benchmarks compare Kreuzberg with several widely used document extraction tools, including Apache Tika, Docling, Unstructured, PDFPlumber, PyMuPDF4LLM, MarkItDown, and Mineru.&lt;/p&gt; &lt;p&gt;All benchmarks are executed automatically in GitHub Actions using a standardized Linux environment and a shared harness, so each framework is tested under the same conditions. We measure throughput, extraction duration, memory consumption, CPU usage, tail latencies, success rates, and extraction quality, both in single-file scenarios (latency and cold start) and batch processing scenarios (parallelism and throughput).&lt;/p&gt; &lt;p&gt;At a high level, the results show significantly higher throughput across common document types such as PDFs, DOCX, PPTX, and HTML. Processing times are often measured in milliseconds rather than seconds, cold start times are lower than most alternatives, and the installation footprint is smaller.&lt;/p&gt; &lt;p&gt;You can explore the benchmarks and download the raw results from the project pages if you want to take a deeper look.&lt;/p&gt; &lt;p&gt;What‚Äôs new in v4.3.0&lt;/p&gt; &lt;p&gt;Alongside the benchmarks, we‚Äôve continued shipping improvements and fixes.&lt;/p&gt; &lt;p&gt;One of the biggest additions in this release is PaddleOCR support through a native Rust integration, with automatic model downloading and caching. This currently supports six languages: English, Chinese, Japanese, Korean, German, and French, and makes it easier to build pipelines that require high-quality OCR for Asian languages without leaving the Rust ecosystem.&lt;/p&gt; &lt;p&gt;We also added structured document data extraction, expanded format support, and removed LibreOffice as a dependency by introducing native extraction for legacy formats such as .doc and .ppt. Reducing external dependencies has been an ongoing focus for us because it simplifies deployment and reduces installation size, especially in containerized environments.&lt;/p&gt; &lt;p&gt;The full changelog is available here:&lt;br /&gt; &lt;a href="https://github.com/kreuzberg-dev/kreuzberg/blob/main/CHANGELOG.md"&gt;https://github.com/kreuzberg-dev/kreuzberg/blob/main/CHANGELOG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting involved&lt;/p&gt; &lt;p&gt;Kreuzberg is an open-source project and contributions are always welcome!Thanks for reading, and we‚Äôd love to hear what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Surround7763"&gt; /u/Eastern-Surround7763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T08:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5beqe</id>
    <title>The current top 4 models on openrouter are all open-weight</title>
    <updated>2026-02-15T10:29:20+00:00</updated>
    <author>
      <name>/u/svantana</name>
      <uri>https://old.reddit.com/user/svantana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt; &lt;img alt="The current top 4 models on openrouter are all open-weight" src="https://preview.redd.it/jjpkakoaxmjg1.png?width=140&amp;amp;height=132&amp;amp;auto=webp&amp;amp;s=0fe256dcd41b962138f1a252df9f49245bdb579d" title="The current top 4 models on openrouter are all open-weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I could be wrong but I think this is the first time this has happened. Is this a pivotal moment or just a temporary fluke?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jjpkakoaxmjg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5072055e50df1701fe5ab51ce67e1b7476f8c62d"&gt;https://preview.redd.it/jjpkakoaxmjg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5072055e50df1701fe5ab51ce67e1b7476f8c62d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/svantana"&gt; /u/svantana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T10:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5dyna</id>
    <title>how to train a tiny model (4B) to prove hard theorems</title>
    <updated>2026-02-15T12:55:39+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"&gt; &lt;img alt="how to train a tiny model (4B) to prove hard theorems" src="https://preview.redd.it/pqtgdyl5onjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6b29a2f167dc4c50fd079038a0edf17bc75ba3f" title="how to train a tiny model (4B) to prove hard theorems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pqtgdyl5onjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T12:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r569eb</id>
    <title>PSA: NVIDIA DGX Spark has terrible CUDA &amp; software compatibility; and seems like a handheld gaming chip.</title>
    <updated>2026-02-15T05:17:53+00:00</updated>
    <author>
      <name>/u/goldcakes</name>
      <uri>https://old.reddit.com/user/goldcakes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the past week experimenting with the DGX Spark and I am about to return it. While I had understood the memory bandwidth and performance limitations, I like the CUDA ecosystem and was willing to pay the premium. Unfortunately, my experiences have been quite poor, and I suspect this is actually handheld gaming scraps that NVIDIA rushed to turn into a product to compete with Apple and Strix Halo.&lt;/p&gt; &lt;p&gt;The biggest issue: DGX Spark is not datacentre Blackwell, it's not even gaming Blackwell, it has its own special snowflake sm121 architecture. A lot of software do not work with it, or &lt;a href="https://github.com/triton-lang/triton/issues/8335#issuecomment-3417643519"&gt;have been patched to run sm80&lt;/a&gt; (Ampere, 6 years old!) codepaths which means it doesn't take advantage of blackwell optimisations.&lt;/p&gt; &lt;p&gt;When questioned about this on NVIDIA support forum, &lt;a href="https://forums.developer.nvidia.com/t/dgx-spark-sm121-software-support-is-severely-lacking-official-roadmap-needed/357663/9#p-1745639-h-1-when-will-sm121-receive-native-support-instead-of-sm80-fallbacks-10"&gt;an official NVIDIA representative said&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;sm80-class kernels can execute on DGX Spark because Tensor Core behavior is very similar, particularly for GEMM/MMAs (closer to the GeForce Ampere-style MMA model). &lt;strong&gt;DGX Spark not has tcgen05 like jetson Thor or GB200, due die space with RT Cores and DLSS algorithm&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Excuse me?? The reason we're getting cut-down tensor cores (not real blackwell) is because of RT Cores and &amp;quot;DLSS algorithm&amp;quot;? This is an AI dev kit; why would I need RT Cores, and additionally how does DLSS come into play? This makes me think they tried to turn a gaming handheld GPU (which needs/supports unified memory) into a poor competitor for a market they weren't prepared for.&lt;/p&gt; &lt;p&gt;In addition, in the same post the rep posted what appears to be LLM hallucinations, mentioning issues have been fixed in version numbers and releases for software libraries that &lt;em&gt;do not exist&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;Just be careful when buying a DGX Spark. You are not really getting a modern CUDA experience. Yes, everything works fine if you pretend you only have an Ampere, but attempting to use any Blackwell features is an exercise in futility.&lt;/p&gt; &lt;p&gt;Additionally, for something that is supposed to be ready 'out of the box', many people (including myself and servethehome) reports basic issues like &lt;strong&gt;HDMI display output&lt;/strong&gt;. I originally thought my Spark was DOA; nope; it just refuses to work with my 1080p144 viewsonic (which works with all other GPUs; including my NVIDIA ones); and had to switch to my 4K60 monitor. Dear NVIDIA, you should not have basic display output issues...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goldcakes"&gt; /u/goldcakes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T05:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5h1gj</id>
    <title>You can run MiniMax-2.5 locally</title>
    <updated>2026-02-15T15:14:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt; &lt;img alt="You can run MiniMax-2.5 locally" src="https://preview.redd.it/hd369oaucojg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf9267391b3836cb000418670d350915c3a8405" title="You can run MiniMax-2.5 locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax-2.5 is a new open LLM achieving SOTA in coding, agentic tool use and search and office work.&lt;/p&gt; &lt;p&gt;The 230B parameters (10B active) model has a &lt;strong&gt;200K context&lt;/strong&gt; window and unquantized bf16 requires &lt;strong&gt;457GB&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Unsloth Dynamic &lt;strong&gt;3-bit&lt;/strong&gt; GGUF reduces size to &lt;strong&gt;101GB&lt;/strong&gt; &lt;strong&gt;(-62%).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide -&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/minimax-2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/minimax-2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUF Models -&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF"&gt;&lt;strong&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd369oaucojg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5chzd</id>
    <title>Is just a meme...</title>
    <updated>2026-02-15T11:35:42+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5chzd/is_just_a_meme/"&gt; &lt;img alt="Is just a meme..." src="https://preview.redd.it/qfotdf9z9njg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cce236e956c8ed28e47cf83ef72bda256c49f6ba" title="Is just a meme..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did need to buy some ECC DDR4 :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfotdf9z9njg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5chzd/is_just_a_meme/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5chzd/is_just_a_meme/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T11:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax ‚Äî Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax ‚Äî Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; ‚Äî Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
