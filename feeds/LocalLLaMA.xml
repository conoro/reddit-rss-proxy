<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-19T05:07:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p0f4r8</id>
    <title>Gemini 3 Pro vs Kimi K2 Thinking</title>
    <updated>2025-11-18T15:38:27+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone done some initial comparisons between the new Gemini 3 Pro and Kimi K2 Thinking?&lt;/p&gt; &lt;p&gt;What are their strengths/weaknesses relative to each other?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:38:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0d18g</id>
    <title>Cloudfare down = ChatGPT down. Local LLM gang for the win!</title>
    <updated>2025-11-18T14:14:53+00:00</updated>
    <author>
      <name>/u/satireplusplus</name>
      <uri>https://old.reddit.com/user/satireplusplus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"&gt; &lt;img alt="Cloudfare down = ChatGPT down. Local LLM gang for the win!" src="https://external-preview.redd.it/AGL421fF8rguq6HfntHEktFb_6D8E61a63BOf9nljqw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0b9fcd248f726ab4f10022b9235b7e02a4c61c6" title="Cloudfare down = ChatGPT down. Local LLM gang for the win!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/satireplusplus"&gt; /u/satireplusplus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/B1K8M3f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T14:14:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0ro7q</id>
    <title>Apple M5 news - LLM boost &amp; clustering</title>
    <updated>2025-11-18T23:33:08+00:00</updated>
    <author>
      <name>/u/Secure_Archer_1529</name>
      <uri>https://old.reddit.com/user/Secure_Archer_1529</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Archer_1529"&gt; /u/Secure_Archer_1529 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://appleinsider.com/articles/25/11/18/macos-tahoe-262-will-give-m5-macs-a-giant-machine-learning-speed-boost"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ro7q/apple_m5_news_llm_boost_clustering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ro7q/apple_m5_news_llm_boost_clustering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0s608</id>
    <title>Stop guessing RAG chunk sizes</title>
    <updated>2025-11-18T23:54:29+00:00</updated>
    <author>
      <name>/u/InstanceSignal5153</name>
      <uri>https://old.reddit.com/user/InstanceSignal5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Last week, I shared a small tool I built to solve a personal frustration: guessing chunk sizes for RAG pipelines.&lt;/p&gt; &lt;p&gt;The feedback here was incredibly helpful. Several of you pointed out that word-based chunking wasn't accurate enough for LLM context windows and that cloning a repo is annoying.&lt;/p&gt; &lt;p&gt;I spent the weekend fixing those issues. I just updated the project (&lt;code&gt;rag-chunk&lt;/code&gt;) with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;True Token Chunking:&lt;/strong&gt; I integrated &lt;code&gt;tiktoken&lt;/code&gt;, so now you can chunk documents based on exact token counts (matching OpenAI's encoding) rather than just whitespace/words.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easier Install:&lt;/strong&gt; It's now packaged properly, so you can install it directly via pip.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visuals:&lt;/strong&gt; Added a demo GIF in the repo so you can see the evaluation table before trying it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal remains the same: a simple CLI to &lt;strong&gt;measure&lt;/strong&gt; recall for different chunking strategies on your own Markdown files, rather than guessing.&lt;/p&gt; &lt;p&gt;It is 100% open-source. I'd love to know if the token-based logic works better for your use cases.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/messkan/rag-chunk"&gt;https://github.com/messkan/rag-chunk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1p0s3cd"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InstanceSignal5153"&gt; /u/InstanceSignal5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0s608/stop_guessing_rag_chunk_sizes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0s608/stop_guessing_rag_chunk_sizes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0s608/stop_guessing_rag_chunk_sizes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0tko5</id>
    <title>Running the latest LLMs like Granite-4.0 and Qwen3 fully on ANE (Apple NPU)</title>
    <updated>2025-11-19T00:56:41+00:00</updated>
    <author>
      <name>/u/Different-Effect-724</name>
      <uri>https://old.reddit.com/user/Different-Effect-724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year, our two co-founders were invited by the Apple Data &amp;amp; Machine Learning Innovation (DMLI) team to share our work on on-device multimodal models for local AI agents. One of the questions that came up in that discussion was: Can the latest LLMs actually run end-to-end on the Apple Neural Engine?&lt;/p&gt; &lt;p&gt;After months of experimenting and building, NexaSDK now runs the latest LLMs like Granite-4.0, Qwen3, Gemma3, and Parakeet-v3, fully on ANE (Apple's NPU), powered by the NexaML engine. &lt;/p&gt; &lt;p&gt;For developers building local AI apps on Apple devices, this unlocks low-power, always-on, fast inference across Mac and iPhone (iOS SDK coming very soon). &lt;/p&gt; &lt;p&gt;Video shows performance running directly on ANE&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p0tko5/video/ur014yfw342g1/player"&gt;https://reddit.com/link/1p0tko5/video/ur014yfw342g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links in comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Effect-724"&gt; /u/Different-Effect-724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0tko5/running_the_latest_llms_like_granite40_and_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0tko5/running_the_latest_llms_like_granite40_and_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0tko5/running_the_latest_llms_like_granite40_and_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T00:56:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0kdcc</id>
    <title>DR Tulu: An open, end-to-end training recipe for long-form deep research</title>
    <updated>2025-11-18T18:51:23+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"&gt; &lt;img alt="DR Tulu: An open, end-to-end training recipe for long-form deep research" src="https://preview.redd.it/6z12rgxba22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44824e149eda9e20a1c7b45b09ec52f394824e96" title="DR Tulu: An open, end-to-end training recipe for long-form deep research" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What Ai2 is releasing&lt;/h1&gt; &lt;p&gt;We‚Äôre making available the entirety of our DR Tulu research and training stack under a permissive license.&lt;/p&gt; &lt;p&gt;Releasing all of DR Tulu‚Äôs components serves three goals. First, it enables reproducibility and transparency: we release our curated prompt datasets, training and evaluation code (including our RLER implementation), and our 8B model checkpoint so others can replicate our results and study how reward functions and tool configurations shape behavior. Second, it provides deployment flexibility‚Äîyou can run the agent with your own MCP tool stack, infrastructure, and privacy constraints. Third, it supports extensibility: the dr-agent-lib agent library lets you plug in domain-specific tools and retrieval systems without retraining by simply describing new tools to the model. Taken together, these artifacts make DR Tulu the first fully open, end-to-end deep research framework.&lt;/p&gt; &lt;p&gt;We encourage you to experiment with different tool configurations, audit the agent‚Äôs research steps, and test how DR Tulu handles your domain's research questions. If you find issues or ways to improve the approach, we'd love to hear about them.&lt;/p&gt; &lt;p&gt;üìö Blog: &lt;a href="https://allenai.org/blog/dr-tulu"&gt;https://allenai.org/blog/dr-tulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚úèÔ∏è Paper: &lt;a href="http://allenai.org/papers/drtulu"&gt;http://allenai.org/papers/drtulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Models: &lt;a href="https://huggingface.co/collections/rl-research/dr-tulu"&gt;https://huggingface.co/collections/rl-research/dr-tulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚å®Ô∏è Code: &lt;a href="https://github.com/rlresearch/DR-Tulu"&gt;https://github.com/rlresearch/DR-Tulu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6z12rgxba22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0okh8</id>
    <title>Nvidia Parakeet-Realtime-EOU-120m-v1</title>
    <updated>2025-11-18T21:30:09+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0okh8/nvidia_parakeetrealtimeeou120mv1/"&gt; &lt;img alt="Nvidia Parakeet-Realtime-EOU-120m-v1" src="https://external-preview.redd.it/zVPL4n_nWpqoPYqwS2dM60dbdwGWNNEtSCu33kDP7a0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=595bf506e0637ff1f4f22d1c370ac082639d0dbd" title="Nvidia Parakeet-Realtime-EOU-120m-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Parakeet-Realtime-EOU-120m-v1 is a streaming speech recognition model that also performs end-of-utterance (EOU) detection. It achieves low latency (80ms~160 ms) and signals EOU by emitting an &amp;lt;EOU&amp;gt; token at the end of each utterance. The model supports only English and does not output punctuation or capitalization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/parakeet_realtime_eou_120m-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0okh8/nvidia_parakeetrealtimeeou120mv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0okh8/nvidia_parakeetrealtimeeou120mv1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T21:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0wqib</id>
    <title>Model quota limit exceeded with 1 prompt Google Antigravity</title>
    <updated>2025-11-19T03:23:49+00:00</updated>
    <author>
      <name>/u/ComposerGen</name>
      <uri>https://old.reddit.com/user/ComposerGen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm quite excited, so download the app and run it on an old Next.js project. The agent goes fully autonomous with a single prompt for minutes, so I grab my double cappuccino. By the time I came back, the limit was already hit. &lt;/p&gt; &lt;p&gt;Prompt: Understand the codebase and build the code.&lt;/p&gt; &lt;p&gt;Call 1-5: List files / read. Call 6-96: Install dependencies, generate Prisma client, build Next.js app, verify API routes, fix routes, fix lint.&lt;/p&gt; &lt;p&gt;22 files changed.&lt;/p&gt; &lt;p&gt;Model quota limit exceeded. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComposerGen"&gt; /u/ComposerGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0wqib/model_quota_limit_exceeded_with_1_prompt_google/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0wqib/model_quota_limit_exceeded_with_1_prompt_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0wqib/model_quota_limit_exceeded_with_1_prompt_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T03:23:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0ve2s</id>
    <title>[LIVE] Gemini 3 Pro vs GPT-5.1: Chess Match (Testing Reasoning Capabilities)</title>
    <updated>2025-11-19T02:20:25+00:00</updated>
    <author>
      <name>/u/Apart-Ad-1684</name>
      <uri>https://old.reddit.com/user/Apart-Ad-1684</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ve2s/live_gemini_3_pro_vs_gpt51_chess_match_testing/"&gt; &lt;img alt="[LIVE] Gemini 3 Pro vs GPT-5.1: Chess Match (Testing Reasoning Capabilities)" src="https://b.thumbs.redditmedia.com/tl3rVcE9lu10wYYX6Ml5X4DfNFwek-VS_OhNWkzR8DQ.jpg" title="[LIVE] Gemini 3 Pro vs GPT-5.1: Chess Match (Testing Reasoning Capabilities)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I was eager to test the new Gemini 3 Pro!&lt;/p&gt; &lt;p&gt;I‚Äôve just kicked off a chess game between &lt;strong&gt;GPT-5.1 (White)&lt;/strong&gt; and &lt;strong&gt;Gemini 3 Pro (Black)&lt;/strong&gt; on the &lt;em&gt;LLM Chess Arena&lt;/em&gt; app I developed a few months ago.&lt;/p&gt; &lt;p&gt;A single game can take a while (sometimes several hours!), so I thought it would be fun to share the live link with you all!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üî¥ Link to the match:&lt;/strong&gt; &lt;a href="https://chess.louisguichard.fr/battle?game=gpt-51-vs-gemini-3-pro-03a640d5"&gt;https://chess.louisguichard.fr/battle?game=gpt-51-vs-gemini-3-pro-03a640d5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLMs aren't designed to play chess and they're not very good at it, but I find it interesting to test them on this because it clearly shows their capabilities or limitations in terms of thinking.&lt;/p&gt; &lt;p&gt;Come hang out and see who cracks first!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e9m1l6y3952g1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdf54cf3294f2186b9b078dd8ce774d2b9a0b99d"&gt;Gemini chooses the Sicilian Defense&lt;/a&gt;&lt;/p&gt; &lt;p&gt;UPDATE: Had to restart the match due to an Out-Of-Memory error caused by traffic&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-Ad-1684"&gt; /u/Apart-Ad-1684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ve2s/live_gemini_3_pro_vs_gpt51_chess_match_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ve2s/live_gemini_3_pro_vs_gpt51_chess_match_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ve2s/live_gemini_3_pro_vs_gpt51_chess_match_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T02:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0kdqf</id>
    <title>That jump in ARC-AGI-2 score from Gemini 3</title>
    <updated>2025-11-18T18:51:48+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"&gt; &lt;img alt="That jump in ARC-AGI-2 score from Gemini 3" src="https://b.thumbs.redditmedia.com/nZ1Acy54HFuRvJailXsBX9aW7Ms7ZFdoAHCltnW5c0Y.jpg" title="That jump in ARC-AGI-2 score from Gemini 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p0kdqf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozu5v4</id>
    <title>20,000 Epstein Files in a single text file available to download (~100 MB)</title>
    <updated>2025-11-17T22:14:12+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've processed all the text and image files (~25,000 document pages/emails) within individual folders released last friday into a two column text file. I used Googles tesseract OCR library to convert jpg to text.&lt;/p&gt; &lt;p&gt;You can download it here: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I uploaded it yesterday, but some of files were incomplete. This version is full. For each document, I've included the full path to the original google drive folder from House oversight committee so you can link and verify contents.&lt;/p&gt; &lt;p&gt;I used mistral 7b to extract entities and relationships and build a basic Graph RAG. There are some new &amp;quot;associations&amp;quot; that have not been reported in the news but couldn't find any breakthrough content. Also my entity/relationship extraction was quick and dirty. Sharing this dataset for people interested in getting into RAG and digging deeper to get more insight that what meets the eye.&lt;/p&gt; &lt;p&gt;In using this dataset, please be sensitive to the privacy of the people involved (and remember that many of these people were certainly not involved in any of the actions which precipitated the investigation.) - Quoted from Enron Email Dataset release&lt;/p&gt; &lt;p&gt;EDIT (NOV 18 Update): These files were released last friday by the &lt;a href="https://oversight.house.gov/release/oversight-committee-releases-additional-epstein-estate-documents/"&gt;house oversight committee&lt;/a&gt;. I will post an update as soon as todays files are released and processed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T22:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0dxns</id>
    <title>If the bubble bursts, what's gonna happen to all those chips?</title>
    <updated>2025-11-18T14:51:51+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will they become cheap? Here's hoping I can have an H200 in my garage for $1500. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T14:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0euvd</id>
    <title>The world‚Äôs fastest open-source TTS: Supertonic</title>
    <updated>2025-11-18T15:27:47+00:00</updated>
    <author>
      <name>/u/ANLGBOY</name>
      <uri>https://old.reddit.com/user/ANLGBOY</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"&gt; &lt;img alt="The world‚Äôs fastest open-source TTS: Supertonic" src="https://external-preview.redd.it/YTdlbmtuc2FhMTJnMeUni0jQysE8S8tC5OeTL5WYLlemOmlkeCkLZq86D7UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f3083d0ba442d2daf2de8bc05be89e611809d9" title="The world‚Äôs fastest open-source TTS: Supertonic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Demo &lt;a href="https://huggingface.co/spaces/Supertone/supertonic#interactive-demo"&gt;https://huggingface.co/spaces/Supertone/supertonic#interactive-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code &lt;a href="https://github.com/supertone-inc/supertonic"&gt;https://github.com/supertone-inc/supertonic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I want to share Supertonic, a newly open-sourced TTS engine that focuses on extreme speed, lightweight deployment, and real-world text understanding.&lt;/p&gt; &lt;p&gt;It‚Äôs available in 8+ programming languages: C++, C#, Java, JavaScript, Rust, Go, Swift, and Python, so you can plug it almost anywhere ‚Äî from native apps to browsers to embedded/edge devices.&lt;/p&gt; &lt;p&gt;Technical highlights are&lt;/p&gt; &lt;p&gt;(1) Lightning-speed ‚Äî Real-time factor:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; 0.001 on RTX4090&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; 0.006 on M4 Pro&lt;/p&gt; &lt;p&gt;(2) Ultra lightweight ‚Äî 66M parameters&lt;/p&gt; &lt;p&gt;(3) On-device TTS ‚Äî Complete privacy and zero network latency&lt;/p&gt; &lt;p&gt;(4) Advanced text understanding ‚Äî Handles complex, real-world inputs naturally&lt;/p&gt; &lt;p&gt;(5) Flexible deployment ‚Äî Works in browsers, mobile apps, and small edge devices&lt;/p&gt; &lt;p&gt;Regarding (4), one of my favorite test sentences is: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; He spent 10,000 JPY to buy tickets for a JYP concert.&lt;/p&gt; &lt;p&gt;Here, ‚ÄúJPY‚Äù refers to Japanese yen, while ‚ÄúJYP‚Äù refers to a name ‚Äî Supertonic handles the difference seamlessly.&lt;/p&gt; &lt;p&gt;Hope it's useful for you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANLGBOY"&gt; /u/ANLGBOY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w8c1bnsaa12g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0r7uw</id>
    <title>CodeMode vs Traditional MCP benchmark</title>
    <updated>2025-11-18T23:13:56+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r7uw/codemode_vs_traditional_mcp_benchmark/"&gt; &lt;img alt="CodeMode vs Traditional MCP benchmark" src="https://preview.redd.it/js0ua9ikl32g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66015efe5e2a45a7817dad1da12469ede8d60d0b" title="CodeMode vs Traditional MCP benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/js0ua9ikl32g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r7uw/codemode_vs_traditional_mcp_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r7uw/codemode_vs_traditional_mcp_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0jr1f</id>
    <title>Mistral removing ton of old models from API (preparing for a new launch?)</title>
    <updated>2025-11-18T18:28:50+00:00</updated>
    <author>
      <name>/u/mpasila</name>
      <uri>https://old.reddit.com/user/mpasila</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt; &lt;img alt="Mistral removing ton of old models from API (preparing for a new launch?)" src="https://preview.redd.it/tg4zaa7b622g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=879c9f3922693c16a694f6bce7604bb1dd61da54" title="Mistral removing ton of old models from API (preparing for a new launch?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They are going to be removing 9 (screenshot is missing one) models from their API at the end of this month. So I wonder if that means they are preparing to release something early December? I sure hope I finally get Nemo 2.0 or something... (it's been over a year since that released).&lt;br /&gt; Source: &lt;a href="https://docs.mistral.ai/getting-started/models#legacy-models"&gt;https://docs.mistral.ai/getting-started/models#legacy-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mpasila"&gt; /u/mpasila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tg4zaa7b622g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0r5ww</id>
    <title>GLM 4.6 on 128 GB RAM with llama.cpp</title>
    <updated>2025-11-18T23:11:48+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I got my hands on a new box at work with 128 GB RAM and 32 GB VRAM (it's a semi-budget option, with 2x5070, but it performs really well). I decided I'm going to try a few of the bigger models. Obviously, a very good model to run on this is GPT-OSS-120B and it's been the default model, but I've set my eyes on the big ones. The GLM 4.6 REAP was a bit overwhelming, but then I though &amp;quot;what if I could get my hands on a good low quant that fits&amp;quot;?&lt;/p&gt; &lt;p&gt;So, with the help of &lt;a href="https://huggingface.co/AesSedai"&gt;https://huggingface.co/AesSedai&lt;/a&gt; I've obtained a really nice mixed quant: &lt;a href="https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S"&gt;https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S&lt;/a&gt; - it's tuned to *just barely* fit in 128GB. What's surprising is how good quality it retains even at such low quant sizes - here's its analysis when I fed it the `modeling_kimi.py` file from Kimi Linear: &lt;a href="https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b"&gt;https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And on top of that, llama.cpp just merged the results of a few weeks of hard work of new contributor &lt;strong&gt;hksdpc255&lt;/strong&gt; on XML tool calling, including GLM 4.6: &lt;a href="https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154"&gt;https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to give it a try - on my box it's getting around 40 t/s prompt processing and about 5 t/s generation, which is not lightning fast, but still a HUGE upgrade from the 5 t/s pp and 3 t/s tg when I tried just a slightly bigger quant.&lt;/p&gt; &lt;p&gt;Edit: forgot to mention, the deployment has 80k context at quite good Q8_0 K/V quantization, so not a gimmick build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0q3z1</id>
    <title>Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut‚Äôs dataset)</title>
    <updated>2025-11-18T22:29:30+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt; &lt;img alt="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut‚Äôs dataset)" src="https://preview.redd.it/nkktzj83y22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a55f8da7446aedd9d2f482226b72c19b4e4ebbf9" title="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut‚Äôs dataset)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been playing with the new 25k-page Epstein Files drop that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file"&gt;tensonaut posted&lt;/a&gt;. Instead of reading 100MB of chaotic OCR myself like a medieval scribe, I threw an open-source model at it and built a local tool that &lt;strong&gt;ranks every document by ‚Äúinvestigative usefulness.‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Everything runs on a single M3 Max MacBook Pro with &lt;strong&gt;open-source&lt;/strong&gt; models only. No cloud, no API calls, no data leaving the machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Streams the entire House Oversight release through &lt;strong&gt;openai/gpt-oss-120b&lt;/strong&gt; running locally via LM Studio.&lt;br /&gt; ‚Ä¢ Scores each passage based on actionable leads, controversy, novelty, and power-linkage.&lt;br /&gt; ‚Ä¢ Outputs a fully structured JSONL dataset with headline, score, key insights, implicated actors, financial-flow notes, etc.&lt;br /&gt; ‚Ä¢ Ships with an interactive local viewer so you can filter by score, read full source text, explore lead types, and inspect charts.&lt;br /&gt; ‚Ä¢ Designed for investigative triage, RAG, IR experiments, or academic analysis.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;&lt;br /&gt; This corpus is massive, messy, and full of OCR noise. Doing a systematic pass manually is impossible. Doing it with cloud models would be expensive and slow. Doing it locally means it‚Äôs cheap, private, and reproducible.&lt;/p&gt; &lt;p&gt;A full run costs about &lt;strong&gt;$1.50 in electricity&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech details&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Model: openai/gpt-oss-120b served at &lt;code&gt;localhost:5002/v1&lt;/code&gt;&lt;br /&gt; ‚Ä¢ Hardware: M3 Max, 128 GB RAM&lt;br /&gt; ‚Ä¢ Viewer: simple JS dashboard with AG Grid, charts, and chunked JSONL loading&lt;br /&gt; ‚Ä¢ Input dataset: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut‚Äôs EPSTEIN_FILES_20K on Hugging Face&lt;/a&gt;&lt;br /&gt; ‚Ä¢ Output: ranked chunks in &lt;code&gt;contrib/&lt;/code&gt;, auto-indexed by the viewer&lt;br /&gt; ‚Ä¢ Prompt: optimized for investigative lead scoring, with a consistent numerical scale (0‚Äì100)&lt;/p&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/latent-variable/epstein-ranker"&gt;https://github.com/latent-variable/epstein-ranker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far I‚Äôve processed the first 5,000 rows myself and published the scored chunks in the repo. If anyone wants to help triage more of the dataset, the GitHub includes simple instructions for claiming a slice and submitting it as a contrib chunk. The workflow supports clean collaboration with automatic deduping.&lt;/p&gt; &lt;p&gt;If you‚Äôd rather build your own tools on top of the scored output or adapt the ranking method for other document dumps, go for it. Everything is MIT-licensed, fully local, and easy to extend.&lt;/p&gt; &lt;p&gt;Contributions, forks, or experiments are all welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nkktzj83y22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T22:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0bql2</id>
    <title>My local AI server is up and running, while ChatGPT and Claude are down due to Cloudflare's outage. Take that, big tech corps!</title>
    <updated>2025-11-18T13:20:14+00:00</updated>
    <author>
      <name>/u/alex_bit_</name>
      <uri>https://old.reddit.com/user/alex_bit_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local servers for the win!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_bit_"&gt; /u/alex_bit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T13:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0sisn</id>
    <title>I replicated Anthropic‚Äôs "Introspection" paper on DeepSeek-7B. It works.</title>
    <updated>2025-11-19T00:09:39+00:00</updated>
    <author>
      <name>/u/Specialist_Bad_4465</name>
      <uri>https://old.reddit.com/user/Specialist_Bad_4465</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Bad_4465"&gt; /u/Specialist_Bad_4465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://joshfonseca.com/blogs/introspection"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T00:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0iayb</id>
    <title>Google Antigravity is a cursor clone</title>
    <updated>2025-11-18T17:36:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you love vibe coding: &lt;a href="https://antigravity.google/"&gt;https://antigravity.google/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supports models other than gemini such as GPT-OSS. Hopefully we will get instructions for running local models soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T17:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0lnlo</id>
    <title>Make your AI talk like a caveman and decrease token usage</title>
    <updated>2025-11-18T19:39:38+00:00</updated>
    <author>
      <name>/u/RegionCareful7282</name>
      <uri>https://old.reddit.com/user/RegionCareful7282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt; &lt;img alt="Make your AI talk like a caveman and decrease token usage" src="https://preview.redd.it/7g67ftgti22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7d9207d83386575ef61218ed4c0a30301826b10" title="Make your AI talk like a caveman and decrease token usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a little side project to help LLMs talk like‚Ä¶ cavemen.&lt;br /&gt; Why? To save tokens, of course. &lt;/p&gt; &lt;p&gt;It works because LLMs can easily fill in grammar and connectives on their own. So we strip what‚Äôs predictable, keep what‚Äôs meaningful, and the model still understands everything perfectly. &lt;/p&gt; &lt;p&gt;Store RAG documents in caveman-compressed form so each chunk carries more valuable data, fits more context, and gives better retrieval quality.&lt;/p&gt; &lt;p&gt;Thought I'd share it here as it might be beneficial in order to not waste tokens on unnecessary words :)&lt;/p&gt; &lt;p&gt;Feel free to contribute if you have any additions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wilpel/caveman-compression"&gt;https://github.com/wilpel/caveman-compression&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegionCareful7282"&gt; /u/RegionCareful7282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7g67ftgti22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T19:39:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0gjcu</id>
    <title>Gemini 3 is launched</title>
    <updated>2025-11-18T16:31:01+00:00</updated>
    <author>
      <name>/u/Several-Republic-609</name>
      <uri>https://old.reddit.com/user/Several-Republic-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt; &lt;img alt="Gemini 3 is launched" src="https://external-preview.redd.it/Jcgyato32sPSUDLsqQhcsyfnhHKEryk97hJ_EjIMDyU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc3edcd8902e26525ff2ad02160747ab3d46316e" title="Gemini 3 is launched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Several-Republic-609"&gt; /u/Several-Republic-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/products/gemini/gemini-3/#note-from-ceo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T16:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0u8hd</id>
    <title>ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama</title>
    <updated>2025-11-19T01:26:53+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt; &lt;img alt="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" src="https://preview.redd.it/2zt7d6q0942g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d69898cd41ba5897e02dd650de189c04e2b1fbb" title="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2zt7d6q0942g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T01:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
</feed>
