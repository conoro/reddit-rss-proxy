<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-03T06:48:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qtuwe7</id>
    <title>Local model fully replacing subscription service</title>
    <updated>2026-02-02T13:22:41+00:00</updated>
    <author>
      <name>/u/Icy_Distribution_361</name>
      <uri>https://old.reddit.com/user/Icy_Distribution_361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm really impressed with local models on a Macbook Pro M4 Pro with 24GB memory. For my usecase, I don't really see the need anymore for a subscription model. While I'm a pretty heavy user of ChatGPT, I don't really ask complicated questions usually. It's mostly &amp;quot;what does the research say about this&amp;quot;, &amp;quot;who is that&amp;quot;, &amp;quot;how does X work&amp;quot;, &amp;quot;what's the etymology of ...&amp;quot; and so on. I don't really do much extensive writing together with it, or much coding (a little bit sometimes). I just hadn't expected Ollama + GPT-OSS:20b to be as high quality and fast as it is. And yes, I know about all the other local models out there, but I actually like GPT-OSS... I know it gets a lot of crap.&lt;/p&gt; &lt;p&gt;Anyone else considering, or has already, cancelling subscriptions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Distribution_361"&gt; /u/Icy_Distribution_361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu6wcc</id>
    <title>NTTuner - Local Fine-Tuning Made Easy (Unsloth + GUI).</title>
    <updated>2026-02-02T20:38:04+00:00</updated>
    <author>
      <name>/u/Few-Pie5592</name>
      <uri>https://old.reddit.com/user/Few-Pie5592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NTTuner: A fine-tuning framework that implements LoRA/QLoRA and integrates Unsloth for 2-5x faster training&lt;/p&gt; &lt;p&gt;· NTCompanion: A GUI wrapper that lets you prep data, configure training, and test models without touching code&lt;/p&gt; &lt;p&gt;Why I think they're worth checking out:&lt;/p&gt; &lt;p&gt;✅ Actually works on single-GPU setups (tested on RTX 4090/3090)&lt;/p&gt; &lt;p&gt;✅ Integrates Unsloth - getting those memory savings and speed boosts without manual setup&lt;/p&gt; &lt;p&gt;✅ GUI makes dataset preparation much less painful (converts CSV/JSON to proper chat formats)&lt;/p&gt; &lt;p&gt;✅ Active development - noosed is responsive to issues and keeps up with new techniques&lt;/p&gt; &lt;p&gt;✅ Windows-friendly (always a plus for local ML tools)&lt;/p&gt; &lt;p&gt;GitHub links:&lt;/p&gt; &lt;p&gt;· NTTuner: &lt;a href="https://github.com/noosed/NTTuner"&gt;https://github.com/noosed/NTTuner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;· NTCompanion: &lt;a href="https://github.com/noosed/NTCompanion"&gt;https://github.com/noosed/NTCompanion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My experience:&lt;/p&gt; &lt;p&gt;Just fine-tuned a Mistral 7B model on some custom Q&amp;amp;A data. The GUI made formatting my dataset trivial, and training with Unsloth integration was noticeably faster than my previous Axolotl setups. Went from ~12 hours estimated to ~4 hours for the same job.&lt;/p&gt; &lt;p&gt;Who this is for:&lt;/p&gt; &lt;p&gt;· If you want to fine-tune locally but find Axolotl/Ollama-training/etc. too command-line heavy&lt;/p&gt; &lt;p&gt;· If you're tired of manually formatting JSONL files for training&lt;/p&gt; &lt;p&gt;· If you want Unsloth benefits without deep technical setup&lt;/p&gt; &lt;p&gt;· If you're on Windows and want a smooth fine-tuning experience&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Pie5592"&gt; /u/Few-Pie5592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu6wcc/nttuner_local_finetuning_made_easy_unsloth_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu6wcc/nttuner_local_finetuning_made_easy_unsloth_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu6wcc/nttuner_local_finetuning_made_easy_unsloth_gui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T20:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qujz0g</id>
    <title>vllm 0.15.0 docker image error</title>
    <updated>2026-02-03T05:57:11+00:00</updated>
    <author>
      <name>/u/Reasonable_Friend_77</name>
      <uri>https://old.reddit.com/user/Reasonable_Friend_77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was trying the latest version of vllm but i'm having this error and can't find any info on it:&lt;/p&gt; &lt;p&gt;&lt;code&gt; vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] WorkerProc failed to start. vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] Traceback (most recent call last): vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] File &amp;quot;/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py&amp;quot;, line 743, in worker_main vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] worker = WorkerProc(*args, **kwargs) vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] ^^^^^^^^^^^^^^^^^^^^^^^^^^^ vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] File &amp;quot;/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py&amp;quot;, line 569, in __init__ vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] self.worker.init_device() vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] File &amp;quot;/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/worker_base.py&amp;quot;, line 326, in init_device vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] self.worker.init_device() # type: ignore vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] ^^^^^^^^^^^^^^^^^^^^^^^^^ vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] File &amp;quot;/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py&amp;quot;, line 210, in init_device vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] current_platform.set_device(self.device) vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] File &amp;quot;/usr/local/lib/python3.12/dist-packages/vllm/platforms/cuda.py&amp;quot;, line 123, in set_device vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] torch.cuda.set_device(device) vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] File &amp;quot;/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py&amp;quot;, line 567, in set_device vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] torch._C._cuda_setDevice(device) vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] File &amp;quot;/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py&amp;quot;, line 410, in _lazy_init vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] torch._C._cuda_init() vllm-qwen3-vl-nvfp4 | ERROR 02-02 21:49:32 [v1/executor/multiproc_executor.py:772] RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is the docker image and i've latest cuda container toolkit and nvidia driver. OS is ubuntu server 25.&lt;/p&gt; &lt;p&gt;Did anyone see anything like this or have any pointer? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Friend_77"&gt; /u/Reasonable_Friend_77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qujz0g/vllm_0150_docker_image_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qujz0g/vllm_0150_docker_image_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qujz0g/vllm_0150_docker_image_error/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T05:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1quk02j</id>
    <title>I am building an LLM arena inside 0 A.D. so models can battle in real-time RTS matches</title>
    <updated>2026-02-03T05:58:50+00:00</updated>
    <author>
      <name>/u/0xrushi</name>
      <uri>https://old.reddit.com/user/0xrushi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hacked together a little project that lets you control a live 0 A.D. match with LLM agents basically an LLM arena on top of the 0 A.D. game.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/0xrushi/openenv-0ad-bridge"&gt;https://github.com/0xrushi/openenv-0ad-bridge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Agents read an omniscient JSON snapshot of the game state and send low-level commands into the same running match (so you can do stuff like &lt;code&gt;gemini&lt;/code&gt; vs &lt;code&gt;gpt-5&lt;/code&gt; on the same map).&lt;/p&gt; &lt;p&gt;I first tried this on the open-source Age of Empires-style engine &lt;a href="https://github.com/SFTtech/openage"&gt;openage&lt;/a&gt;, but that project has been “almost there” for ~10 years. 0 A.D. felt stable enough, so I rebuilt everything around its RL interface with an OpenEnv-style proxy and some helper tools.&lt;/p&gt; &lt;p&gt;If you’re into agent-y things, I’d love help on better prompts and a cleaner action cookbook (move / econ / build / combat / scout), plus any ideas for fun experiments to run on top.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xrushi"&gt; /u/0xrushi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quk02j/i_am_building_an_llm_arena_inside_0_ad_so_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quk02j/i_am_building_an_llm_arena_inside_0_ad_so_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quk02j/i_am_building_an_llm_arena_inside_0_ad_so_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T05:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu6e7w</id>
    <title>I made a proxy to save your tokens for distillation training</title>
    <updated>2026-02-02T20:19:49+00:00</updated>
    <author>
      <name>/u/FaustAg</name>
      <uri>https://old.reddit.com/user/FaustAg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu6e7w/i_made_a_proxy_to_save_your_tokens_for/"&gt; &lt;img alt="I made a proxy to save your tokens for distillation training" src="https://preview.redd.it/0bh7eaqj35hg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c7660e1de353b5587de48d62690c0c2c55354cc" title="I made a proxy to save your tokens for distillation training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;before I release it I'm thinking that I should give people the ability to share their tokens. I am a little worried that even with opt in it could be a security risk if people don't understand what they're doing, but if even a few dozens of us do share tokens it could lead to some very valuable data for distillation. thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FaustAg"&gt; /u/FaustAg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0bh7eaqj35hg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu6e7w/i_made_a_proxy_to_save_your_tokens_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu6e7w/i_made_a_proxy_to_save_your_tokens_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T20:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu3sz7</id>
    <title>Can your model beat this Motherload clone?</title>
    <updated>2026-02-02T18:48:40+00:00</updated>
    <author>
      <name>/u/JosephCurvin</name>
      <uri>https://old.reddit.com/user/JosephCurvin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu3sz7/can_your_model_beat_this_motherload_clone/"&gt; &lt;img alt="Can your model beat this Motherload clone?" src="https://external-preview.redd.it/OG10Nmp3OWNmNGhnMS4ma4XKyQkWAloPUH0A5TEFsSJAEkk1TA9wcLrLDXE4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2558f84a70c8c0a4a2de66b3e913661528bc560b" title="Can your model beat this Motherload clone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recreated the classic &lt;em&gt;Motherload&lt;/em&gt; Flash game so it can be played by an LLM.&lt;/p&gt; &lt;p&gt;The goal is to mine a specific ore while managing fuel, earning money, buying upgrades, and so on.&lt;/p&gt; &lt;p&gt;Of the models I’ve tested, only Gemini Flash has beaten it—and that happened just once.&lt;/p&gt; &lt;p&gt;Give it a try!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JosephCurwin/motherload-agent"&gt;https://github.com/JosephCurwin/motherload-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JosephCurvin"&gt; /u/JosephCurvin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vwp8rn9cf4hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu3sz7/can_your_model_beat_this_motherload_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu3sz7/can_your_model_beat_this_motherload_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T18:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu1mf9</id>
    <title>Transformer Lab can Now Train Across Clusters of GPUs</title>
    <updated>2026-02-02T17:33:33+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may have seen our open source work called Transformer Lab. Now, we built &lt;strong&gt;Transformer Lab for Teams&lt;/strong&gt; to support AI work that can scale across clusters of GPUs. &lt;/p&gt; &lt;p&gt;After talking to numerous labs and individuals training models beyond a single node we heard:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The frontier labs invest a ton to build and maintain their own proprietary tooling.&lt;/li&gt; &lt;li&gt;Most other AI/ML research teams work with a fragmented landscape of legacy scripts, manual workflows which gets more complicated as you grow your team and run more experiments&lt;/li&gt; &lt;li&gt;Researchers spend almost half their time dealing with logistics. For example, results get lost or rerun because jobs fail before finishing and artifacts aren’t tracked consistently.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How Transformer Lab for Teams is helpful:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Interface:&lt;/strong&gt; A single dashboard to manage data ingestion, model fine-tuning, and evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless Scaling:&lt;/strong&gt; The platform is architected to run locally on personal hardware (Apple Silicon, NVIDIA/AMD GPUs) and seamlessly scale to high-performance computing clusters using orchestrators like Slurm and SkyPilot.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extensibility:&lt;/strong&gt; A flexible plugin system allows researchers to add custom training loops, evaluation metrics, and model architectures without leaving the platform.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Privacy-First:&lt;/strong&gt; The platform processes data within the user's infrastructure, whether on-premise or in a private cloud, ensuring sensitive research data never leaves the lab's control.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simplifying workflows:&lt;/strong&gt; Capabilities that used to require complex engineering are now built-in. &lt;ul&gt; &lt;li&gt;Capturing checkpoints (with auto-restart)&lt;/li&gt; &lt;li&gt;One-line to add hyperparameter sweeps&lt;/li&gt; &lt;li&gt;Storing artifacts in a global object store accessible even after ephemeral nodes terminate.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our goal is to make LLM/Diffusion/Audio training easier as you scale: from a single machine to multi-GPU, multi-node setups. All without rewriting your training code.&lt;/p&gt; &lt;p&gt;The project is &lt;strong&gt;open source and free to use&lt;/strong&gt;. It also works on CLI. &lt;/p&gt; &lt;p&gt;We just launched the beta here: &lt;a href="https://lab.cloud/"&gt;https://lab.cloud/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m one of the maintainers and can walk you through install or even provide a live demo if you’d like. Have a look and let us know how we can make it better for you. &lt;/p&gt; &lt;p&gt;Ask any questions here! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1mf9/transformer_lab_can_now_train_across_clusters_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1mf9/transformer_lab_can_now_train_across_clusters_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1mf9/transformer_lab_can_now_train_across_clusters_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T17:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1quknk3</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-02-03T06:34:38+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknk3/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/QprY89tp4mFiTL240KbwJiLb7VIdNSd5G9AewCG5j5M.jpg" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Z-Image - Controllable Text-to-Image&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Foundation model built for precise control with classifier-free guidance, negative prompting, and LoRA support.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tkuso0j158hg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2c3376942edada97d5dfac59b537cfbda876812"&gt;https://preview.redd.it/tkuso0j158hg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2c3376942edada97d5dfac59b537cfbda876812&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HunyuanImage-3.0-Instruct - Image Generation &amp;amp; Editing&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Image generation and editing model with multimodal fusion from Tencent.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/tencent/HunyuanImage-3.0-Instruct"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7bfx5b5358hg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7976d83afa785388b3c2943f9dc6411608d531e"&gt;https://preview.redd.it/7bfx5b5358hg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7976d83afa785388b3c2943f9dc6411608d531e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LTX-2 LoRA - Image-to-Video Adapter&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-source Image-to-Video adapter LoRA for LTX-2 by MachineDelusions.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/MachineDelusions/LTX-2_Image2Video_Adapter_LoRa"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1quknk3/video/6p93cv4458hg1/player"&gt;https://reddit.com/link/1quknk3/video/6p93cv4458hg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TeleStyle - Style Transfer&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Content-preserving style transfer for images and videos.&lt;/li&gt; &lt;li&gt;&lt;a href="https://tele-ai.github.io/TeleStyle/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1quknk3/video/0arp6bc558hg1/player"&gt;https://reddit.com/link/1quknk3/video/0arp6bc558hg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MOSS-Video-and-Audio - Synchronized Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;32B MoE model generates video and audio in one pass.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/OpenMOSS-Team/MOVA-360p"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1quknk3/video/3ryr1oo658hg1/player"&gt;https://reddit.com/link/1quknk3/video/3ryr1oo658hg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LingBot-World:&lt;/strong&gt; An open-source world simulator for video generation research. - &lt;a href="https://github.com/Robbyant/lingbot-world"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/robbyant/lingbot-world-base-cam"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1quknk3/video/57ub0nwb58hg1/player"&gt;https://reddit.com/link/1quknk3/video/57ub0nwb58hg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-43-models-that?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknk3/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknk3/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quknk3/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T06:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1quknpy</id>
    <title>OSS 120b v GLM 4.7 flash. Is the latter better for anything?</title>
    <updated>2026-02-03T06:34:53+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is GLM 4.7 flash better than OSS 120b for anything? I would normally look for a benchmark but I don't know which ones to trust any more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T06:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtjhc8</id>
    <title>Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2</title>
    <updated>2026-02-02T03:07:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"&gt; &lt;img alt="Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2" src="https://b.thumbs.redditmedia.com/sBia_JVk3vzY7mBVsXJhMax7j8mOpxC8QNyJrUyazbc.jpg" title="Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The newly released Stepfun model Step-3.5-Flash outperforms DeepSeek v3.2 on multiple coding and agentic benchmarks, despite using far fewer parameters.&lt;/p&gt; &lt;p&gt;Step-3.5-Flash: 196B total / 11B active parameters&lt;/p&gt; &lt;p&gt;DeepSeek v3.2: 671B total / 37B active parameters&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qtjhc8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T03:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qua4xb</id>
    <title>Kimi distillation attempt</title>
    <updated>2026-02-02T22:36:51+00:00</updated>
    <author>
      <name>/u/ramendik</name>
      <uri>https://old.reddit.com/user/ramendik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the question of a &amp;quot;small Kimi&amp;quot; arises time and time again. And at least once Moonshot said they would welcome community distills: &lt;a href="https://github.com/MoonshotAI/Kimi-K2/issues/16"&gt;https://github.com/MoonshotAI/Kimi-K2/issues/16&lt;/a&gt; . Sadly I keep missing AMAs to ask their present view of community distills.&lt;/p&gt; &lt;p&gt;I've been interested in the topic for a while, and for the last couple of months was actually trying to do it. I could probably do a lot better, so I'll outline what went on, and the end of the post has a link to my test checkpoint - suggestions of what to change in my process are very mush welcome as is any feedback on the checkpoint. I would also love to learn about other distill projects; so far I know of one, a part of a CoT distill set of leading thinking models: &lt;a href="https://huggingface.co/TeichAI/Qwen3-8B-Kimi-K2-Thinking-Distill"&gt;https://huggingface.co/TeichAI/Qwen3-8B-Kimi-K2-Thinking-Distill&lt;/a&gt; . Compared to what I am trying to do, it seems more technical-oriented and also sources Kimi K2 Thinking while my favourite is K2 Instruct 0905 (never tried the non-0905 though).&lt;/p&gt; &lt;p&gt;To make mistakes cheap (this is my first model trainjing project) and to ensure the result runs on anything, I picked a very small first target/student model, Granite 4.0 hybrid 1B (really 1.5B). It's actually one heck of a 1B, trained on 15T tokens from scratch - not a sequential distill of something bigger like the Gemma and Qwen examples in this size. Granite's expression style is very neutral and quite constrained (it ignores style/persona instructions in the system prompt); but that also means one is not fighting an existing &amp;quot;vibe&amp;quot; when implanting a new one. The Mamba-hybrid nature means it can scale to longer contexts withoug choking, even when running on CPU.&lt;/p&gt; &lt;p&gt;There's the big question of what one is distilling for; I went for vibe/style/conversation (with roleplay a potential addition at a later stage), but of course there are other options. And from there one gets to &amp;quot;where to get the prompts for generation&amp;quot;. The best I could think of was to grab user prompts off existing datasets.&lt;/p&gt; &lt;p&gt;First I generated a max_seq_len 6000 dataset of Kimi K2 Instruct 0905 answers - including some seriously strong prose, based on prompts from &lt;a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk-multilingual8-Qwen3-32B-main-gen"&gt;https://huggingface.co/datasets/HuggingFaceTB/smoltalk-multilingual8-Qwen3-32B-main-gen&lt;/a&gt; (advice seeking category) and the magpie-ultra source in main Smoltalk. I worked out a Qwen-based pipeline to detect typical hallucinations and also to find facts that need verification; I used Gemini 2.5 Flash with grounding to verify the facts and dropped the lines with wrong or dubious claims. &lt;a href="https://huggingface.co/datasets/ramendik/kimify-20251115"&gt;https://huggingface.co/datasets/ramendik/kimify-20251115&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unfortunately, after *a lot* of checkpoints it turned out that such long form won't fly with a 1.5B, at least immediately. The result was always too prone to looping (somehow, ifeval at t=0 is a good looping tendency detector and I have a script that specifically checks for loops and counts them; Granite 4.0 h 1b has &amp;lt;20 loops in ifeval while the long-form trained checkpoionts resulted in around 50).&lt;/p&gt; &lt;p&gt;While training on that dataset and trying to defeat the instabilty, I found a training algorithm, CorDA KPM &lt;a href="https://huggingface.co/docs/peft/v0.18.0/en/developer_guides/lora#corda"&gt;https://huggingface.co/docs/peft/v0.18.0/en/developer_guides/lora#corda&lt;/a&gt; , that makes things much more stable. As the &amp;quot;knowledge&amp;quot; dataset I just use tool calls (a random subset of the xLAM dataset, reformatted for Granite - can publish if there's any need for it); this lets me avoid locking in Granite's style. While it made things better, I eventually had to give up on the long-form dataset, at least for the first stage.&lt;/p&gt; &lt;p&gt;So I generated a larger dataset of smaller answers, using a system prompt to make Kimi birfer but still quite punchy. The typical hallucination filter and fact verifier happened again, and I also filtered out entries where any one assistant message is over 1000 Granite tokens. &lt;a href="https://huggingface.co/datasets/ramendik/kimify-short-20260131"&gt;https://huggingface.co/datasets/ramendik/kimify-short-20260131&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also wanted to buttress instruction following but not to benchmax for ifeval, so I never used ifeval prompts but instead took prompts from &lt;a href="https://huggingface.co/datasets/HuggingFaceH4/ifeval-like-data"&gt;https://huggingface.co/datasets/HuggingFaceH4/ifeval-like-data&lt;/a&gt; - then verified the results of Kimi's generation against the constraints. The result is &lt;a href="https://huggingface.co/datasets/ramendik/kimify-ifeval-like"&gt;https://huggingface.co/datasets/ramendik/kimify-ifeval-like&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My hope is to get a good first checkpoint that has picked up at least the basics of Kimi's stype - and then expand my CorDA KPM dataset with actual text generation in the new style. I would hope that, with the basic style and the new CorDA KPM dataset in place, I can train the next checkpoint on longer samples and on actual multiturn conversations (generated with a red-teaming model). For now it's short-ish single-turn advice-seeking answers and three-turn magpie-ultra-short answers.&lt;/p&gt; &lt;p&gt;So, I made my candidate &amp;quot;stage 1&amp;quot; checkpoint. Unlike baselike Granite, it does change its style on system prompts - this is an emergent behaviour, my dataset has no system prompts. So please test with different system prompts; if you don't supply a system prompt, the Granite tokenizer uses a default one that dampens things a bit (or should I cut that out of the tokenizer?). With the larger dataset, the emergent system prompt plasticity was more pronounced and when &amp;quot;creative&amp;quot; was requested the style got quite exuberant - but the loops made me pull away; I am hoping to bring that back in stage 2 with a &amp;quot;fatter&amp;quot; CorDA KPM.&lt;/p&gt; &lt;p&gt;(I named the project &amp;quot;Miki&amp;quot; and the 1B size &amp;quot;pebble&amp;quot; - there are suitable Granite models for &amp;quot;cobble&amp;quot; and &amp;quot;boulder&amp;quot; but I want to polish the technique on &amp;quot;pebble&amp;quot; first).&lt;/p&gt; &lt;p&gt;The hyperparameters I used - CorDA KPM, r=128 a=256, target_modules = [&amp;quot;q_proj&amp;quot;, &amp;quot;k_proj&amp;quot;, &amp;quot;v_proj&amp;quot;, &amp;quot;o_proj&amp;quot;, &amp;quot;mamba.in_proj&amp;quot;, &amp;quot;mamba.out_proj&amp;quot;] (but notably not the MLP layers - targeting those somehow dilutes any styke impact significantly), Muon optimizer (somehow better on the style), LR=1.5e-5. These gave the best result out of a rather large sweep.&lt;/p&gt; &lt;p&gt;This candidate checkpoint is at &lt;a href="https://huggingface.co/ramendik/miki-pebble-20260131"&gt;https://huggingface.co/ramendik/miki-pebble-20260131&lt;/a&gt; - that's the GGUFs in BF16 and Q8_0 ; if anyone actually needs a lower quant at this size please tell me and I'll bother with the imatrix thing. There is a safetensors version too, at &lt;a href="https://huggingface.co/ramendik/miki-pebble-20260131-safetensors"&gt;https://huggingface.co/ramendik/miki-pebble-20260131-safetensors&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Again, feedback very much appreciated, *especially* what I can do better. Better sources of prompts, anything really. (One thing I'm not changing is the general style/writing/conversational direction; I just don't think I know enough to do a coding or agentic oriented distill). And links to other Kimi distill projects are very welcome too.&lt;/p&gt; &lt;p&gt;P.S. Yeah, I did use a Nano-GPT subscription for the mass-generation waves. It really did a lot to help me afford it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramendik"&gt; /u/ramendik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qua4xb/kimi_distillation_attempt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qua4xb/kimi_distillation_attempt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qua4xb/kimi_distillation_attempt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T22:36:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtqspu</id>
    <title>1 Day Left Until ACE-Step 1.5 — Open-Source Music Gen That Runs on &lt;4GB VRAM Open suno alternative (and yes, i made this frontend)</title>
    <updated>2026-02-02T09:47:32+00:00</updated>
    <author>
      <name>/u/ExcellentTrust4433</name>
      <uri>https://old.reddit.com/user/ExcellentTrust4433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"&gt; &lt;img alt="1 Day Left Until ACE-Step 1.5 — Open-Source Music Gen That Runs on &amp;lt;4GB VRAM Open suno alternative (and yes, i made this frontend)" src="https://external-preview.redd.it/dXBiYXJlb295MWhnMYGTlVfp4XddQFbQ7RXlmhemkMaIRdSQh0Jy7FObZ7qD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34c74a399f0ef7e36cb52af6dda02f6967165407" title="1 Day Left Until ACE-Step 1.5 — Open-Source Music Gen That Runs on &amp;lt;4GB VRAM Open suno alternative (and yes, i made this frontend)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An open-source model with quality approaching Suno v4.5/v5... running locally on a potato GPU. No subscriptions. No API limits. Just you and your creativity. &lt;/p&gt; &lt;p&gt;We're so lucky to be in this era of open-source AI. A year ago this was unthinkable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcellentTrust4433"&gt; /u/ExcellentTrust4433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2geqqfooy1hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T09:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qudqul</id>
    <title>How to prevent MacOS annoying RAM compression behavior</title>
    <updated>2026-02-03T01:04:36+00:00</updated>
    <author>
      <name>/u/Sea_Smoke_7626</name>
      <uri>https://old.reddit.com/user/Sea_Smoke_7626</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I recently bought a MacBook M4 Pro 48GB. And I currently running a Qwen coder 30B in LM Studio all time. It works pretty well, never hit swap. &lt;/p&gt; &lt;p&gt;But what annoying me is that MacOS always tries to compress this llm when llm goes into inactive status, and it seems like this compression process never goes to end so that RAM load indicator is always yellow until I trigger the llm to response my request.&lt;/p&gt; &lt;p&gt;Does this behavior cause any significant problems in long time? or is there any solution to prevent macOS from trying to compress this LLM? &lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zd3i4xl8h6hg1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14eed75559eb851f5396a0d696d3d4b028ba042e"&gt;https://preview.redd.it/zd3i4xl8h6hg1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14eed75559eb851f5396a0d696d3d4b028ba042e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Smoke_7626"&gt; /u/Sea_Smoke_7626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qudqul/how_to_prevent_macos_annoying_ram_compression/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qudqul/how_to_prevent_macos_annoying_ram_compression/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qudqul/how_to_prevent_macos_annoying_ram_compression/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T01:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu1j8f</id>
    <title>ggml-cpu: FA split across kv for faster TG</title>
    <updated>2026-02-02T17:30:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"&gt; &lt;img alt="ggml-cpu: FA split across kv for faster TG" src="https://external-preview.redd.it/R4jvaeUcXiua-hwaogdXuUXVYGR6WfvIUnqzyL6NDik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f1403d66bb0d55b925437fb753efc214331c697" title="ggml-cpu: FA split across kv for faster TG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CPU Flash-Attention decoding speed-up (long contexts).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19209"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T17:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qttq5w</id>
    <title>devstral small is faster and better than glm 4.7 flash for local agentic coding.</title>
    <updated>2026-02-02T12:28:47+00:00</updated>
    <author>
      <name>/u/theghost3172</name>
      <uri>https://old.reddit.com/user/theghost3172</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just realised token per second is not the only thing that matters in agentic coding. glm 4.7 flash is almlst 3x faster but it keeps thinking for way more than 3 times the total tokens it generates so yes at the end devstral small finishes the task slighter faster than glm 4.7 flash. while obiously being much much better at agentic coding.&lt;/p&gt; &lt;p&gt;token efficiency of devstral small has to be discussed more often. its incredble.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theghost3172"&gt; /u/theghost3172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T12:28:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu8pqw</id>
    <title>Anyone else down the "data sovereignty" rabbit hole or am I going crazy?</title>
    <updated>2026-02-02T21:43:48+00:00</updated>
    <author>
      <name>/u/itsnotKelsey</name>
      <uri>https://old.reddit.com/user/itsnotKelsey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it started with just wanting to run models locally so my stuff doesn't get scraped. Now I'm like 3 weeks deep reading about self-sovereign Identity, network state stuff and wondering if there's a way to actually prove your data isn't being touched vs just hoping it isn't. Local models help I guess.. but it still feels like we're just trusting that nothing's phoning home. &lt;/p&gt; &lt;p&gt;Is there anything out there that gives you like actual cryptographic proof your queries aren't being logged? Or am I seriously overthinking this lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnotKelsey"&gt; /u/itsnotKelsey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu8pqw/anyone_else_down_the_data_sovereignty_rabbit_hole/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu8pqw/anyone_else_down_the_data_sovereignty_rabbit_hole/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu8pqw/anyone_else_down_the_data_sovereignty_rabbit_hole/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T21:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu337m</id>
    <title>Kimi K2.5 Thinking is now the top open-weights model on the Extended NYT Connections benchmark</title>
    <updated>2026-02-02T18:24:12+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu337m/kimi_k25_thinking_is_now_the_top_openweights/"&gt; &lt;img alt="Kimi K2.5 Thinking is now the top open-weights model on the Extended NYT Connections benchmark" src="https://b.thumbs.redditmedia.com/lXiHCHVYnN-1i7rZTgAmV5ZD6P72uG9N9KolYMvPk0I.jpg" title="Kimi K2.5 Thinking is now the top open-weights model on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qu337m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu337m/kimi_k25_thinking_is_now_the_top_openweights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu337m/kimi_k25_thinking_is_now_the_top_openweights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T18:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu2z21</id>
    <title>GLM-OCR</title>
    <updated>2026-02-02T18:20:07+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu2z21/glmocr/"&gt; &lt;img alt="GLM-OCR" src="https://external-preview.redd.it/ln9l9VYiqmiIpjy0J_jvzMtD5AaeFLSsBaVe9XdCQEk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4dc223422c5611f0b16cc93726757f1c90444b4" title="GLM-OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder–decoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale image–text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-OCR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu2z21/glmocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu2z21/glmocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T18:20:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qucoid</id>
    <title>Smartest model for 24-28GB vram?</title>
    <updated>2026-02-03T00:19:27+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was super happy to find qwen 30B A3B being so damn clever on my 3090 and then I tried GLM flash 4.7 and I was blown away. Is there any other model that’s smart like this? My use case is using it as an agentic coder but bonus points if it can do rp like GLM flash lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qucoid/smartest_model_for_2428gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qucoid/smartest_model_for_2428gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qucoid/smartest_model_for_2428gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T00:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvo4r</id>
    <title>128GB devices have a new local LLM king: Step-3.5-Flash-int4</title>
    <updated>2026-02-02T13:55:00+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's the HF Repo: &lt;a href="http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4"&gt;http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4&lt;/a&gt; (this is a GGUF repo)&lt;/p&gt; &lt;p&gt;I've been running this LLM for about an hour and it has handled all coding tests I've thrown at it in chat mode. IMO this is as good if not better than GLM 4.7, Minimax 2.1 while being much more efficient. Later I will try some agentic coding to see how it performs, but I already have high hopes for it.&lt;/p&gt; &lt;p&gt;I use a 128GB M1 ultra mac studio and can run it at full context (256k). Not only it is fast, but also super efficient in RAM usage.&lt;/p&gt; &lt;p&gt;*Update: I ran llama-bench with up to 100k prefill. Here are the results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices ggml_metal_library_init: using embedded metal library ggml_metal_library_init: loaded in 0.024 sec ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s) ggml_metal_device_init: GPU name: Apple M1 Ultra ggml_metal_device_init: GPU family: MTLGPUFamilyApple7 (1007) ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_device_init: simdgroup reduction = true ggml_metal_device_init: simdgroup matrix mul. = true ggml_metal_library_init: using embedded metal library ggml_metal_library_init: loaded in 0.024 sec ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s) ggml_metal_device_init: GPU name: Apple M1 Ultra ggml_metal_device_init: GPU family: MTLGPUFamilyApple7 (1007) ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_device_init: simdgroup reduction = true ggml_metal_device_init: simdgroup matrix mul. = true ggml_metal_device_init: has unified memory = true ggml_metal_device_init: has bfloat = true ggml_metal_device_init: has tensor = false ggml_metal_device_init: use residency sets = true ggml_metal_device_init: use shared buffers = true ggml_metal_device_init: recommendedMaxWorkingSetSize = 134217.73 MB | model | size | params | backend | threads | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 | 281.09 ± 1.57 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 | 34.70 ± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d10000 | 248.10 ± 1.08 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d10000 | 31.69 ± 0.04 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d20000 | 222.18 ± 0.49 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d20000 | 30.02 ± 0.04 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d30000 | 200.68 ± 0.78 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d30000 | 28.62 ± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d40000 | 182.86 ± 0.55 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d40000 | 26.89 ± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d50000 | 167.61 ± 0.23 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d50000 | 25.37 ± 0.03 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d60000 | 154.50 ± 0.19 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d60000 | 24.10 ± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d70000 | 143.60 ± 0.29 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d70000 | 22.95 ± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d80000 | 134.02 ± 0.35 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d80000 | 21.87 ± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d90000 | 125.34 ± 0.19 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d90000 | 20.66 ± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d100000 | 117.72 ± 0.07 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d100000 | 19.78 ± 0.01 | build: a0dce6f (24) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is still very usable with 100k prefill, so a good option for CLI coding agents!&lt;/p&gt; &lt;p&gt;You need to build a llama.cpp fork to run it, instructions at the HF repo. Though this model is so good that I believe it will soon be supported by llama.cpp upstream.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:55:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1quhtzi</id>
    <title>I built Qwen3-TTS Studio – Clone your voice and generate podcasts locally, no ElevenLabs needed</title>
    <updated>2026-02-03T04:06:59+00:00</updated>
    <author>
      <name>/u/BC_MARO</name>
      <uri>https://old.reddit.com/user/BC_MARO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt; &lt;img alt="I built Qwen3-TTS Studio – Clone your voice and generate podcasts locally, no ElevenLabs needed" src="https://b.thumbs.redditmedia.com/e2H3gASgajrAcOcnvmlH4NRBeqiOdlfaLk86ZYPzqcg.jpg" title="I built Qwen3-TTS Studio – Clone your voice and generate podcasts locally, no ElevenLabs needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been using Qwen3-TTS and found the existing demo a bit limited for what I wanted to do. So I built a proper interface with fine-grained control and a killer feature: **automated podcast generation**.&lt;/p&gt; &lt;p&gt;**What it does:**&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🎙️ Clone any voice with just a 3-second audio sample&lt;/li&gt; &lt;li&gt;🎚️ Fine-tune parameters (temperature, top-k, top-p) with quality presets&lt;/li&gt; &lt;li&gt;📻 Generate complete podcasts from just a topic – AI writes the script, assigns voices, and synthesizes everything&lt;/li&gt; &lt;li&gt;🌍 10 languages supported (Korean, English, Chinese, Japanese, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98"&gt;https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently uses gpt5.2 for script generation, but the architecture is modular – you can swap in any local LLM (Qwen, Llama, etc.) if you want fully local.&lt;/p&gt; &lt;p&gt;**The TTS runs entirely local** on your machine (macOS MPS / Linux CUDA). No API calls for voice synthesis = unlimited generations, zero cost.&lt;/p&gt; &lt;p&gt;Basically: ElevenLabs-style voice cloning + NotebookLM-style podcast generation, but local.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/bc-dunia/qwen3-TTS-studio"&gt;https://github.com/bc-dunia/qwen3-TTS-studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC_MARO"&gt; /u/BC_MARO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T04:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvp74</id>
    <title>GLM-5 Coming in February! It's confirmed.</title>
    <updated>2026-02-02T13:56:14+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt; &lt;img alt="GLM-5 Coming in February! It's confirmed." src="https://preview.redd.it/rq0meza173hg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71bbd7ed37e31d92af89abf19ffb4ef0e1d8925a" title="GLM-5 Coming in February! It's confirmed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twitter Link: &lt;a href="https://x.com/jietang/status/2018246490775498791?s=20"&gt;https://x.com/jietang/status/2018246490775498791?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq0meza173hg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu7jqi</id>
    <title>GLM releases OCR model</title>
    <updated>2026-02-02T21:01:12+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-OCR"&gt;https://huggingface.co/zai-org/GLM-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy my friends, looks like a banger! GLM cooking hard! Seems like a 1.4B-ish model (0.9B vision, 0.5B language). Must be super fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T21:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
