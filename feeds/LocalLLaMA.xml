<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-21T15:52:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rabo34</id>
    <title>Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)</title>
    <updated>2026-02-20T23:50:26+00:00</updated>
    <author>
      <name>/u/RIP26770</name>
      <uri>https://old.reddit.com/user/RIP26770</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"&gt; &lt;img alt="Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)" src="https://preview.redd.it/heh81bnslqkg1.png?width=140&amp;amp;height=69&amp;amp;auto=webp&amp;amp;s=e8760daf2610cb9721ded68d56da0aa9a90586c4" title="Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a small local-first TTS server with voice cloning and streaming audio output so your LLM can reply back in a cloned voice almost in realtime.&lt;/p&gt; &lt;p&gt;Main reason: I wanted something that could replace ElevenLabs in a fully local stack without API costs or external dependencies.&lt;/p&gt; &lt;p&gt;Works well alongside llama.cpp / OpenAI-compatible endpoints and plugs cleanly into voice bots (I’m using it for Telegram voice replies).&lt;/p&gt; &lt;p&gt;Goals were simple:&lt;/p&gt; &lt;p&gt;-fully local -streaming audio output -voice cloning -lightweight + clean API -easy integration &lt;a href="https://github.com/ai-joe-git/pocket-tts-server"&gt;Pocket-TTS-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Already running it daily for voice-first bots.&lt;/p&gt; &lt;p&gt;Curious if anyone else here is building similar pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIP26770"&gt; /u/RIP26770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rabo34"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1raucof</id>
    <title>Wave Field LLM — O(n log n) attention via wave equation dynamics</title>
    <updated>2026-02-21T15:46:07+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an alternative attention mechanism that treats language as a physical field system instead of using standard O(n²) self-attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; - Tokens are mapped onto a continuous 1D field - Information propagates via damped wave equations: k(t) = exp(-α·t)·cos(ω·t + φ) - Each attention head has just 3 learnable physics parameters (frequency, damping, phase) - Convolution computed via FFT in O(n log n) - Heads self-organize into different roles (local grammar, medium context, long-range)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (WikiText-2, 6M params, character tokenizer):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;PPL&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Complexity&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Standard Transformer&lt;/td&gt; &lt;td&gt;5.9&lt;/td&gt; &lt;td&gt;51.0%&lt;/td&gt; &lt;td&gt;O(n²)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Wave Field V3.5&lt;/td&gt; &lt;td&gt;6.2&lt;/td&gt; &lt;td&gt;50.5%&lt;/td&gt; &lt;td&gt;O(n log n)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;At longer sequences the savings grow: 31x at 2K tokens, 107x at 8K, 367x at 32K.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Known limitations:&lt;/strong&gt; - With BPE tokenizer (8K vocab), there's a significant capacity gap vs standard transformer - This is a model capacity issue at small scale, not an architecture flaw - Currently scaling to 100M params to see if the gap closes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's unique:&lt;/strong&gt; - Every bug during development was found through physics-based diagnostics (energy flow, conservation, causality tests) — not guessing - Cross-head field coupling and wave interference for information routing - Not a Mamba/Hyena variant — different approach entirely&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;https://github.com/badaramoni/wave-field-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the physics, architecture decisions, or results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qa7l</id>
    <title>Kimi has context window expansion ambitions</title>
    <updated>2026-02-20T08:54:10+00:00</updated>
    <author>
      <name>/u/omarous</name>
      <uri>https://old.reddit.com/user/omarous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt; &lt;img alt="Kimi has context window expansion ambitions" src="https://preview.redd.it/3cvl2bdh5mkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e22f6604997ccccf6f6215ae239ab8f8b1dd09c3" title="Kimi has context window expansion ambitions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarous"&gt; /u/omarous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cvl2bdh5mkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:54:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wvg4</id>
    <title>GGML and llama.cpp join HF to ensure the long-term progress of Local AI</title>
    <updated>2026-02-20T14:31:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt; &lt;img alt="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" src="https://external-preview.redd.it/tLGg2WMvFn2R5w7Nf2m6oJPphAYJILLSWaWPLPoW8i4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6bb0cd5000a00c0e28c8ae17203068e5acfb352" title="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;article by Georgi Gerganov, Xuan-Son Nguyen, Aleksander Grygier, Lysandre, Victor Mustar, Julien Chaumond&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-joins-hf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rad3hd</id>
    <title>I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python</title>
    <updated>2026-02-21T00:51:34+00:00</updated>
    <author>
      <name>/u/samaphp</name>
      <uri>https://old.reddit.com/user/samaphp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"&gt; &lt;img alt="I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python" src="https://preview.redd.it/jf8obilpwqkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e68a6c305d0b33063478f23529d27aa4fddbb79" title="I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I evaluated &lt;strong&gt;100+ LLMs&lt;/strong&gt; using a fixed set of questions covering &lt;strong&gt;7 software engineering categories&lt;/strong&gt; from the perspective of a Python developer. This was &lt;strong&gt;not coding tasks&lt;/strong&gt; and not traditional benchmarks, the questions focus on practical engineering reasoning and decision-making. All models were tested against the same prompts, and the results include both qualitative evaluation and &lt;strong&gt;token generation speed&lt;/strong&gt;, because usability over time matters as much as correctness.&lt;/p&gt; &lt;p&gt;Local models were evaluated on an NVIDIA RTX 4060 Ti 16GB using LM Studio, while most cloud models were tested via OpenRouter, with some Anthropic and OpenAI models evaluated directly through their official APIs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; the evaluation questions were collaboratively designed by &lt;strong&gt;ChatGPT 5.2&lt;/strong&gt; and &lt;strong&gt;Claude Opus 4.5&lt;/strong&gt;, including an agreed list of &lt;em&gt;good&lt;/em&gt; and &lt;em&gt;bad&lt;/em&gt; behaviors for each question. Model responses were then evaluated by &lt;strong&gt;gpt-4o-mini&lt;/strong&gt;, which checked each answer against that shared list. The evaluation categories were:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Problem Understanding &amp;amp; Reasoning&lt;/li&gt; &lt;li&gt;System Design &amp;amp; Architecture&lt;/li&gt; &lt;li&gt;API, Data &amp;amp; Domain Design&lt;/li&gt; &lt;li&gt;Code Quality &amp;amp; Implementation&lt;/li&gt; &lt;li&gt;Reliability, Security &amp;amp; Operations&lt;/li&gt; &lt;li&gt;LLM Behavior &amp;amp; Professional Discipline&lt;/li&gt; &lt;li&gt;Engineering Restraint &amp;amp; Practical Judgment&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One thing that surprised me was that some of the &lt;strong&gt;highest-performing models&lt;/strong&gt; were also among the &lt;strong&gt;slowest and most token-heavy&lt;/strong&gt;. Once models pass roughly ~95%, quality differences shrink, and &lt;strong&gt;latency and efficiency become far more important&lt;/strong&gt;. My goal was to identify models I could realistically run &lt;strong&gt;24 hours a day&lt;/strong&gt;, either locally or via a cloud provider, without excessive cost or waiting time. The models I ended up favoriting for Python developer tasks weren't always the cheapest or the top scorers; they were the ones that finished quickly, used tokens efficiently, and still showed consistently good engineering judgment. For example, &lt;strong&gt;GPT 5.1 Codex&lt;/strong&gt; isn't very cheap, but it's very fast and highly token-efficient, which makes it practical for continuous use.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Models I favored (efficient &amp;amp; suitable for my use case)&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Grok 4.1 Fast&lt;/strong&gt;: very fast, disciplined engineering responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 120B&lt;/strong&gt;: strong reasoning with excellent efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash Preview&lt;/strong&gt;: extremely fast and clean&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 20B (local)&lt;/strong&gt;: fast and practical on a consumer GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT 5.1 Codex Mini&lt;/strong&gt;: low verbosity, quick turnaround&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT 5.1 Codex&lt;/strong&gt;: not cheap, but very fast and token-efficient&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;:solid discipline with reasonable latency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 4B (local)&lt;/strong&gt;: small, fast, and surprisingly capable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The full list and the test results are available on this URL: &lt;a href="https://py.eval.draftroad.com"&gt;https://py.eval.draftroad.com&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;⚠️ &lt;strong&gt;Disclaimer:&lt;/strong&gt; these results reflect my personal experience and testing methodology. I may be wrong. Results can vary based on use cases, prompting styles, and evaluation criteria. This should be viewed as a transparent comparison, not a definitive benchmark for python with LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samaphp"&gt; /u/samaphp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jf8obilpwqkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T00:51:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rarnfi</id>
    <title>LLM prompting tricks resource ?</title>
    <updated>2026-02-21T13:53:07+00:00</updated>
    <author>
      <name>/u/jiii95</name>
      <uri>https://old.reddit.com/user/jiii95</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I read a paper today that talks about how duplicating the prompts increases significantly the LLM reponse quality. I was wondering if there are any github repos, or somewhere else where these types of techniques are aggregated for sharing purposes so I keep up with the latest techniques out there ? Thank you very much&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2512.14982"&gt;https://arxiv.org/pdf/2512.14982&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiii95"&gt; /u/jiii95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rarnfi/llm_prompting_tricks_resource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rarnfi/llm_prompting_tricks_resource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rarnfi/llm_prompting_tricks_resource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T13:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rarzp2</id>
    <title>Built an Open-Source DOM-Based AI Browser Agent (No Screenshots, No Backend)</title>
    <updated>2026-02-21T14:07:49+00:00</updated>
    <author>
      <name>/u/KlutzySession3593</name>
      <uri>https://old.reddit.com/user/KlutzySession3593</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with AI browser agents and wanted to try a different approach than the usual screenshot + vision model pipeline.&lt;/p&gt; &lt;p&gt;Most agents today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Take a screenshot&lt;/li&gt; &lt;li&gt;Send it to a multimodal model&lt;/li&gt; &lt;li&gt;Ask it where to click&lt;/li&gt; &lt;li&gt;Repeat&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It works, but it’s slow, expensive, and sometimes unreliable due to pixel ambiguity.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Sarathi AI&lt;/strong&gt;, an open-source Chrome extension that reasons over structured DOM instead of screenshots.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Injects into the page&lt;/li&gt; &lt;li&gt;Assigns unique IDs to visible elements&lt;/li&gt; &lt;li&gt;Extracts structured metadata (tag, text, placeholder, nearby labels, etc.)&lt;/li&gt; &lt;li&gt;Sends a JSON snapshot + user instruction to an LLM&lt;/li&gt; &lt;li&gt;LLM returns structured actions (navigate, click, type, hover, wait, keypress)&lt;/li&gt; &lt;li&gt;Executes deterministically&lt;/li&gt; &lt;li&gt;Loops until &lt;code&gt;completed&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;No vision.&lt;br /&gt; No pixel reasoning.&lt;br /&gt; No backend server.&lt;/p&gt; &lt;p&gt;API keys (OpenAI / Gemini / DeepSeek / custom endpoint) are stored locally in Chrome storage.&lt;/p&gt; &lt;h1&gt;What it currently handles&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Opening Gmail and drafting contextual replies&lt;/li&gt; &lt;li&gt;Filling multi-field forms intelligently (name/email/phone inference)&lt;/li&gt; &lt;li&gt;E-commerce navigation (adds to cart, stops at OTP)&lt;/li&gt; &lt;li&gt;Hover-dependent UI elements&lt;/li&gt; &lt;li&gt;Search + extract + speak workflows&lt;/li&gt; &lt;li&gt;Constraint-aware instructions (e.g., “type but don’t send”)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In my testing it works on ~90% of normal websites.&lt;br /&gt; Edge cases still exist (auth redirects, aggressive anti-bot protections, dynamic shadow DOM weirdness).&lt;/p&gt; &lt;h1&gt;Why DOM-based instead of screenshot-based?&lt;/h1&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Faster iteration loop&lt;/li&gt; &lt;li&gt;Lower token cost&lt;/li&gt; &lt;li&gt;Deterministic targeting via unique IDs&lt;/li&gt; &lt;li&gt;Easier debugging&lt;/li&gt; &lt;li&gt;Structured reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Requires careful DOM parsing&lt;/li&gt; &lt;li&gt;Can break on heavy SPA state transitions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m mainly looking for feedback on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tradeoffs between DOM grounding vs vision grounding&lt;/li&gt; &lt;li&gt;Better loop termination heuristics&lt;/li&gt; &lt;li&gt;Safety constraints for real-world deployment&lt;/li&gt; &lt;li&gt;Handling auth redirect flows more elegantly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/sarathisahoo/sarathi-ai-agent"&gt;https://github.com/sarathisahoo/sarathi-ai-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=5Voji994zYw"&gt;https://www.youtube.com/watch?v=5Voji994zYw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would appreciate technical criticism.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KlutzySession3593"&gt; /u/KlutzySession3593 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rarzp2/built_an_opensource_dombased_ai_browser_agent_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rarzp2/built_an_opensource_dombased_ai_browser_agent_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rarzp2/built_an_opensource_dombased_ai_browser_agent_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T14:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rafo5b</id>
    <title>[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS</title>
    <updated>2026-02-21T02:50:43+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"&gt; &lt;img alt="[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS" src="https://preview.redd.it/j509b0ozgrkg1.png?width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=8c561d0403991ae31c82f4c63a33fcc76d123703" title="[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone.&lt;br /&gt; Quick recap if you're new here: Vellium is an open-source app for creative writing that replaces manual prompt editing with visual controls. Want a slow burn or high tension? Just drag a slider for mood, pacing, or intensity instead of digging through configs. &lt;/p&gt; &lt;p&gt;Just pushed a pretty big update for Vellium (v0.2.8 to v0.3.5). The main focus this time was overhauling the writing mode and making local providers work much smoother.&lt;/p&gt; &lt;p&gt;The writing mode got a huge rework. We finally added a proper book bible, direct DOCX import, and cached book summaries. The sidebar is way more compact now, and the character workspace is much better — you can even use AI to patch-edit your characters directly. We also fixed a bunch of UX stuff, so project deletion and export/download (including inline scenes) are actually reliable now.&lt;/p&gt; &lt;p&gt;For local setups, KoboldCpp integration is fully native now. It supports the &lt;code&gt;provider:memory&lt;/code&gt; field, universal tags, and n-sigma. Payload fields are finally aligned with the official API, and we fixed those annoying model loading issues. Tool calling also properly disables in the UI when KoboldCpp is active.&lt;/p&gt; &lt;p&gt;A few other cool things: we added OpenAI-compatible TTS with a separate model just for translation. There's a new Zen Chat UI mode if you want zero visual distractions. Phrase bans are working properly now, and we turned off the default badwords by default. You also get more control in settings over API parameter forwarding, like sampler forwarding.&lt;/p&gt; &lt;p&gt;Under the hood, multi-character chat is way more stable (add at least one word from char name and he answer first than another). Squashed some runtime data leaks, sorted out the server bundle resolving inside&lt;code&gt;asar&lt;/code&gt;, and added some basic security hardening for local mode. Oh, and the project is now officially MIT licensed!&lt;/p&gt; &lt;p&gt;Grab the release on GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you hit any bugs or have ideas for the next updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rafo5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabg6o</id>
    <title>Qwen3 coder next oddly usable at aggressive quantization</title>
    <updated>2026-02-20T23:41:01+00:00</updated>
    <author>
      <name>/u/CoolestSlave</name>
      <uri>https://old.reddit.com/user/CoolestSlave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;I've been testing the 30b range models but i've been a little disappointed by them (qwen 30b, devstral 2, nemotron etc) as they need a lot of guidance and almost all of them can't correct some mistake they made no matter what.&lt;/p&gt; &lt;p&gt;Then i tried to use qwen next coder at q2 because i don't have enough ram for q4. Oddly enough it does not say nonsense, even better, he one shot some html front page and can correct some mistake by himself when prompting back his mistake.&lt;/p&gt; &lt;p&gt;I've only made shallow testing but it really feel like at this quant, it already surpass all 30b models without sweating.&lt;/p&gt; &lt;p&gt;Do you have any experience with this model ? why is it that good ??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoolestSlave"&gt; /u/CoolestSlave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vywq</id>
    <title>GGML.AI has got acquired by Huggingface</title>
    <updated>2026-02-20T13:54:26+00:00</updated>
    <author>
      <name>/u/Time_Reaper</name>
      <uri>https://old.reddit.com/user/Time_Reaper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt; &lt;img alt="GGML.AI has got acquired by Huggingface" src="https://external-preview.redd.it/l687iazpdDZhrDlIbQBxf8OTcfiJg6WGdsBpv03NqVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9e45ab199a5cdbdf8c5eb1968743c094b946e98" title="GGML.AI has got acquired by Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time_Reaper"&gt; /u/Time_Reaper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/19759"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1raqi5w</id>
    <title>Too many memory implementations, what do you actually use?</title>
    <updated>2026-02-21T12:58:08+00:00</updated>
    <author>
      <name>/u/xeeff</name>
      <uri>https://old.reddit.com/user/xeeff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i swear any time i try to research about what memory implementations/architectures are the best, everyone has their own solution, yet at the same time i struggle finding any genuinely working solution with little friction and setup/implementation time. it's crazy how the only &amp;quot;perfect&amp;quot; memory solutions come from people advertising their own project&lt;/p&gt; &lt;p&gt;what do people ACTUALLY use? i've heard of mem0 before (not so much anymore, seems they died out) and more recently stuff like supermemory, openmemory, etc, but i don't want to spend hours checking each solution just for it to not work (put off from previous experiences)&lt;/p&gt; &lt;p&gt;i'd love to see how people have implemented the memory and the types of tasks they do with their AI agent, and stuff like that. the more information the better&lt;/p&gt; &lt;p&gt;thanks for reading and hoping to see your replies :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xeeff"&gt; /u/xeeff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raqi5w/too_many_memory_implementations_what_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raqi5w/too_many_memory_implementations_what_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raqi5w/too_many_memory_implementations_what_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T12:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabcyp</id>
    <title>A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)</title>
    <updated>2026-02-20T23:37:12+00:00</updated>
    <author>
      <name>/u/spaceman_</name>
      <uri>https://old.reddit.com/user/spaceman_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"&gt; &lt;img alt="A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)" src="https://preview.redd.it/y3n05xxziqkg1.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=9c8cd5f0f499dbda0fd3ab30a338f120aa67e03d" title="A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of Step 3.5 and MiniMax M2.5, we've got two new options for models that barely fit in memory.&lt;/p&gt; &lt;p&gt;To help people figure out which models run best on the platform, I decided to run some llama.cpp benchmarks for a few quants of these models.&lt;/p&gt; &lt;p&gt;I also included some benchmarks for Qwen3-coder-next (since we've been seeing lots of improvement lately), GLM 4.6V &amp;amp; GLM 4.7 Flash, and a few older models like gpt-oss-120b which compete in a similar size space.&lt;/p&gt; &lt;p&gt;My ROCm benchmarks are running against ROCm 7.2 as that is what my distro provides. My device has a Ryzen AI Max+ 395 @ 70W and 128GB of memory. All benchmarks are run at a context depth of 30,000 tokens.&lt;/p&gt; &lt;p&gt;If there's interest in other models or quants, feel free to ask for them in the comments, and I'll see if I can get some running.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spaceman_"&gt; /u/spaceman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rabcyp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:37:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1raall0</id>
    <title>fixed parser for Qwen3-Coder-Next</title>
    <updated>2026-02-20T23:06:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"&gt; &lt;img alt="fixed parser for Qwen3-Coder-Next" src="https://external-preview.redd.it/Y3wE-GVXbELboPM9WQJZOtsZ_aPLgAL7jIOMvAV90UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6015d17bc47511d73e5fb3850ccf0851296f278f" title="fixed parser for Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;another fix for Qwen Next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19765"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajez2</id>
    <title>what are your favorite lesser known models on huggingface</title>
    <updated>2026-02-21T06:01:33+00:00</updated>
    <author>
      <name>/u/EngineeringBright82</name>
      <uri>https://old.reddit.com/user/EngineeringBright82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a professor, I want to expand my students minds by showing them models that are not chatGPT etc. Anyone have some unique / interesting / useful models hosted on huggingface?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EngineeringBright82"&gt; /u/EngineeringBright82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T06:01:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9zt8m</id>
    <title>The top 3 models on openrouter this week ( Chinese models are dominating!)</title>
    <updated>2026-02-20T16:21:50+00:00</updated>
    <author>
      <name>/u/keb_37</name>
      <uri>https://old.reddit.com/user/keb_37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt; &lt;img alt="The top 3 models on openrouter this week ( Chinese models are dominating!)" src="https://preview.redd.it/h4l8zr4rdokg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1cb3201433eea5c7cd862fbc8c0f259e4e6b134" title="The top 3 models on openrouter this week ( Chinese models are dominating!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the first time i see a model exceed 3 trillion tokens per week on openrouter!&lt;/p&gt; &lt;p&gt;the first time i see more than one model exceed a trillion token per week ( it was only grok 4 fast month ago)&lt;/p&gt; &lt;p&gt;the first time i see chinese models destroying US ones like this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keb_37"&gt; /u/keb_37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h4l8zr4rdokg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uuc6</id>
    <title>Deepseek and Gemma ??</title>
    <updated>2026-02-20T13:05:36+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt; &lt;img alt="Deepseek and Gemma ??" src="https://preview.redd.it/84ph0pirenkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d2b363b1900aae44bcfc12c0eeb9d8e2caa7d08" title="Deepseek and Gemma ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/84ph0pirenkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra8omf</id>
    <title>"Gemma, which we will be releasing a new version of soon"</title>
    <updated>2026-02-20T21:50:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"&gt; &lt;img alt="&amp;quot;Gemma, which we will be releasing a new version of soon&amp;quot;" src="https://external-preview.redd.it/9mfj1kMXjQ4Pove4Y8zbrEpz5ffGrhmDZ-YwmsdPJeE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66d6fe6182877c1d801afa8a61aec7616fb8a587" title="&amp;quot;Gemma, which we will be releasing a new version of soon&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;20:17&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/P0enFK4bzLE?si=2hfjhPrT4gbqsZwk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T21:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1raf3dm</id>
    <title>GLM 5 seems to have a "Claude" personality</title>
    <updated>2026-02-21T02:23:22+00:00</updated>
    <author>
      <name>/u/TinyApplet</name>
      <uri>https://old.reddit.com/user/TinyApplet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt; &lt;img alt="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" src="https://preview.redd.it/7nj17cwubrkg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=e0025d5e56387cb178ff1a928dc4a19313407e90" title="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that GLM 5 behaves significantly differently when told it is Claude, as with the following system prompt: &amp;quot;You are Claude, a large language model by Anthropic.&amp;quot; The writing style and personality changes significantly, and it even seems to bypass built-in censorship, as per my second image.&lt;/p&gt; &lt;p&gt;I've also tried a more nonsensical prompt: &amp;quot;You are Tiny, a large language model by Applet&amp;quot; (deliberately avoiding the names of any known models or companies), and, as expected, that didn't yield the same results nor bypassed the model's censorship.&lt;/p&gt; &lt;p&gt;Whether this was intentional on Zhipu's part or not, I can't say; it could be that they did, in fact, include a &amp;quot;Claude&amp;quot; personality in the training dataset, seeing as how they seem to have planned for GLM 5 to work well with Claude Code. It's also possible, of course, that this is emergent behavior, and that the personality changes are merely because GLM 5 has some information, however vague, on its dataset about what Claude is and how it's supposed to behave.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TinyApplet"&gt; /u/TinyApplet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raf3dm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:23:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ramir9</id>
    <title>[Release] Ouro-2.6B-Thinking — first working inference (ByteDance's recurrent "thinking" model, fixed for transformers 4.55)</title>
    <updated>2026-02-21T09:04:04+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance released Ouro-2.6B-Thinking a few weeks ago and it's been tricky to run — the architecture is genuinely unusual and existing GGUFs were producing garbage output because of it.&lt;/p&gt; &lt;p&gt;What makes Ouro different: It's a recurrent Universal Transformer — it runs all 48 layers 4 times per token (192 effective passes). Standard llama.cpp just runs each layer once, so every existing GGUF was broken.&lt;/p&gt; &lt;p&gt;What I fixed:&lt;/p&gt; &lt;p&gt;The original modeling_ouro.py had two bugs incompatible with transformers 4.55:&lt;/p&gt; &lt;p&gt;UniversalTransformerCache inherits from Cache, which defines key_cache as a &lt;a href="/u/property"&gt;u/property&lt;/a&gt; — so self.key_cache = [] in __init__ threw AttributeError: can't set attribute&lt;/p&gt; &lt;p&gt;Missing get_mask_sizes() method required by create_causal_mask() in transformers 4.55+&lt;/p&gt; &lt;p&gt;Patched both, tested output:&lt;/p&gt; &lt;p&gt;User: What is 2+2?&amp;lt;think&amp;gt;Okay, the user asked &amp;quot;What is 2+2?&amp;quot; It's a basic arithmetic problem...Adding 2 and 2 gives 4. That's a fundamental math fact...&amp;lt;/think&amp;gt;The sum of 2 and 2 is **4**.2 + 2 = 4&lt;/p&gt; &lt;p&gt;Performance (NVIDIA L4): ~3.8 t/s, 5.3 GB VRAM (float16)&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed"&gt;https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: uses use_cache=False (full context recompute). KV cache pass-through doesn't work correctly with the 4-loop UT architecture — this is the correct behavior matching early_exit_threshold: 1.0 in the config.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T09:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ranako</id>
    <title>TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF · Hugging Face</title>
    <updated>2026-02-21T09:52:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"&gt; &lt;img alt="TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF · Hugging Face" src="https://external-preview.redd.it/FYfNUuhT3WL90VoAzpzSy8fZEgRuGPVIPMxWk_wBrrg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0ae5ed8bdcc636a4a90b9972c253516a3b8e3bd" title="TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;featured yesterday (by Unsloth and on X) so let's check it out&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T09:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rar6md</id>
    <title>Qwen Code - a powerful open-source coding agent + NO TELEMETRY FORK</title>
    <updated>2026-02-21T13:31:28+00:00</updated>
    <author>
      <name>/u/Undici77</name>
      <uri>https://old.reddit.com/user/Undici77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hey everyone,&lt;/h1&gt; &lt;p&gt;I wanted to share two things: a great open-source project I've been using, and a fork I made for privacy-conscious folks.&lt;/p&gt; &lt;h1&gt;Qwen Code&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/qwen-code"&gt;&lt;strong&gt;https://github.com/QwenLM/qwen-code&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen Code is an open-source CLI coding agent developed by Alibaba's Qwen team. It's essentially their take on tools like Claude Code or Gemini CLI. You run it in your terminal, point it at a project, and it can read, write, and reason about your codebase autonomously.&lt;/p&gt; &lt;p&gt;What makes it particularly interesting is how well it pairs with &lt;strong&gt;LM Studio&lt;/strong&gt; and &lt;strong&gt;Qwen3-Coder&lt;/strong&gt;. If you're running Qwen3-Coder locally via LM Studio, you can point Qwen Code at your local server and get a fully local, offline coding agent with zero API costs. The model is genuinely good at coding tasks, refactoring, debugging, generating boilerplate, explaining code and the combo works surprisingly well.&lt;/p&gt; &lt;p&gt;Setup is straightforward: run LM Studio, load Qwen3-Coder, enable the local server on port 1234, and configure Qwen Code to hit &lt;code&gt;http://localhost:1234&lt;/code&gt;. That's it.&lt;/p&gt; &lt;h1&gt;The problem: telemetry&lt;/h1&gt; &lt;p&gt;Qwen Code, like many tools in this space, ships with telemetry enabled. For those of us who prefer to keep our code and prompts strictly local, this is a dealbreaker.&lt;/p&gt; &lt;h1&gt;My no-telemetry fork&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/undici77/qwen-code-no-telemetry/tree/v0.10.5-no-telemetry"&gt;&lt;strong&gt;https://github.com/undici77/qwen-code-no-telemetry/tree/v0.10.5-no-telemetry&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I forked the project and stripped out all telemetry. Nothing leaves your machine except the requests you explicitly make to your model provider.&lt;/p&gt; &lt;p&gt;Install script or Docker available!&lt;/p&gt; &lt;p&gt;ENJOY!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Undici77"&gt; /u/Undici77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rar6md/qwen_code_a_powerful_opensource_coding_agent_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rar6md/qwen_code_a_powerful_opensource_coding_agent_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rar6md/qwen_code_a_powerful_opensource_coding_agent_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T13:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ram2ov</id>
    <title>How I mapped every High Court of Australia case and their citations (1901-2025)</title>
    <updated>2026-02-21T08:36:59+00:00</updated>
    <author>
      <name>/u/Neon0asis</name>
      <uri>https://old.reddit.com/user/Neon0asis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt; &lt;img alt="How I mapped every High Court of Australia case and their citations (1901-2025)" src="https://preview.redd.it/2mntthxp7tkg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=2eb05b7ded68545504de00ea12ea1305b546acb8" title="How I mapped every High Court of Australia case and their citations (1901-2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve recently begun working on a project to convert entirety of Australian case law and legislation into a LexisNexis-style interlinked legal knowledge graph.&lt;/p&gt; &lt;p&gt;As I’ve experimented with techniques to normalise case citations, I thought it would be cool to turn my work into a neat little visualisation, and explain how you could do the same with your own documents.&lt;/p&gt; &lt;p&gt;So the graph above is a visualisation of a cross-section of a legal knowledge graph I’ve been developing of Australian case law.&lt;/p&gt; &lt;p&gt;Each node represents a High Court of Australia decision. The size of the node reflects how often that case has been cited by other High Court cases. The node's location and clustering comes from mapping each case’s semantic “position” into 3D space, based on its location in a higher-dimensional embedding space.&lt;/p&gt; &lt;h1&gt;How the dataset was built&lt;/h1&gt; &lt;p&gt;To assemble the graph, I downloaded the &lt;a href="https://huggingface.co/datasets/isaacus/open-australian-legal-corpus"&gt;Open Australian Legal Corpus &lt;/a&gt;and ran the &lt;a href="https://docs.isaacus.com/capabilities/enrichment"&gt;Kanon 2 Enricher&lt;/a&gt; to extract citations and additional metadata, such as decision dates and pinpoint references. I then used this additional metadata to repair and improve some of the dataset's missing features.&lt;/p&gt; &lt;p&gt;For roughly 90% of the corpus, I was able to recover and uniquely identify the party names, decision dates, and common aliases.&lt;/p&gt; &lt;p&gt;Using the party names and year as a composite key, I then normalised and deduplicated every citation appearing in High Court decisions. This produced ~20,000 High Court-to-High Court citations.&lt;/p&gt; &lt;p&gt;With the citations linked, I used the &lt;a href="https://docs.isaacus.com/capabilities/embedding"&gt;Kanon 2 Embedder&lt;/a&gt; to generate vector embeddings for each case, and then applied &lt;a href="https://github.com/YingfanWang/PaCMAP"&gt;PaCMAP&lt;/a&gt; (a dimensionality reduction library) to reduce those embeddings down to a 3D representation.&lt;/p&gt; &lt;p&gt;To infer clusters (i.e., broad topical groupings), I ran &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"&gt;K-means &lt;/a&gt;in the original embedding space. To make the clusters interpretable, I used &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TF–IDF&lt;/a&gt; to generate simple semantic labels based on the most characteristic terms in each cluster.&lt;/p&gt; &lt;p&gt;Finally, using the reception labels extracted by the Kanon 2 Enricher, I captured a sentiment-like signal for how cases treat the authorities they cite. Most citations are neutral (grey). Citations that overrule prior High Court authority are marked in red, while supportive citations are shown in green. Because the Enricher extracts these signals natively, that step was straightforward.&lt;/p&gt; &lt;p&gt;With the features extracted and linked, I then vibe coded a lightweight interface to render the network as an interactive node graph.&lt;/p&gt; &lt;h1&gt;What you can see in the result&lt;/h1&gt; &lt;p&gt;Even with around ~7,000 High Court cases, some patterns stand out immediately:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The semantic geometry works surprisingly well.&lt;/strong&gt; Closely related areas of law sit near one another in 3D space. Estate law and land law, for example, tend to cluster tightly (towards the bottom of the structure) while criminal law, which is not related to these fields, occupies the top end of the grap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;You can explore fine-grained subregions interactively.&lt;/strong&gt; In the notebook (linked at the end of the post), there’s a region where several clusters intersect that corresponds strongly to constitutional cases involving Indigenous communities. &lt;em&gt;Mabo v Queensland (No 2)&lt;/em&gt; is one of the best-known cases in that neighbourhood.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The time dimension reflects legal history.&lt;/strong&gt; You can see a shift toward citing domestic authority more heavily after the &lt;a href="https://peo.gov.au/understand-our-parliament/history-of-parliament/history-milestones/australian-parliament-history-timeline/events/australia-act-1986"&gt;Australia Acts 1986&lt;/a&gt;, which helped establish Australia’s judicial independence. Earlier High Court decisions cite UK Privy Council rulings more often and are more visibly shaped by UK common law. This is one reason the earliest cases cite Australian authorities less than you might expect.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Reproducing it&lt;/h1&gt; &lt;p&gt;All code to reproduce the results is on &lt;a href="https://github.com/isaacus-dev/cookbooks/tree/main/cookbooks/semantic-legal-citation-graph"&gt;GitHub,&lt;/a&gt; and the interactive visualisation is embedded directly in the notebook, so you can explore it without running anything locally. If you’d like a guided walkthrough, there’s also a guided tour highlighting landmark cases in Australian constitutional law I have up on &lt;a href="https://youtu.be/in76S6P9xOw?si=hBaPpb0p6HVyjelv"&gt;YouTube&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon0asis"&gt; /u/Neon0asis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mntthxp7tkg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T08:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1raq23i</id>
    <title>they have Karpathy, we are doomed ;)</title>
    <updated>2026-02-21T12:34:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt; &lt;img alt="they have Karpathy, we are doomed ;)" src="https://preview.redd.it/ergzi9d1eukg1.png?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=2005c28094bfd489a487151bba9f5c550c22c55b" title="they have Karpathy, we are doomed ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(added second image for the context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raq23i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T12:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
