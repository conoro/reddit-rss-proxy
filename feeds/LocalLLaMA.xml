<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-14T12:13:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r48zgz</id>
    <title>How is it pronounced?</title>
    <updated>2026-02-14T02:45:12+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;â€œGGUFâ€&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1r48zgz"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r48zgz/how_is_it_pronounced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r48zgz/how_is_it_pronounced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r48zgz/how_is_it_pronounced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T02:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r45hkh</id>
    <title>Running GLM-4.7 on an old AMD GPU</title>
    <updated>2026-02-14T00:05:04+00:00</updated>
    <author>
      <name>/u/Begetan</name>
      <uri>https://old.reddit.com/user/Begetan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I am a bit late to GLM-4.7 party, but as a poor unlucky AMD GPU owner I was late to buy a good Nvidia videocard, so I got AMD RX6900XT with 16GB RAM because miners did not want it for their rigs.&lt;/p&gt; &lt;p&gt;I was inspired by other post about running GLM-4.7 model on a baseline hardware and I believe we need to share a successful working configuration to help other people and new models to make decisions.&lt;/p&gt; &lt;h1&gt;My config&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPU: AMD RX6900XT 16GB&lt;/li&gt; &lt;li&gt;CPU: Intel i9-10900k&lt;/li&gt; &lt;li&gt;RAM: DDR4 3200 32GB&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My llama.cpp build&lt;/h1&gt; &lt;p&gt;```bash rm -rf build HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; \ HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \ cmake -S . -B build \ -DGGML_HIP=ON \ -DGPU_TARGETS=gfx1030 \ -DCMAKE_BUILD_TYPE=Release \ -DCMAKE_BUILD_RPATH='$ORIGIN/../lib'&lt;/p&gt; &lt;p&gt;cmake --build build -j 16&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;It is important to provide you target architecture.&lt;/p&gt; &lt;h1&gt;My llama.cpp run&lt;/h1&gt; &lt;p&gt;&lt;code&gt;bash ./build/bin/llama-server \ --model unsloth/GLM-4.7-Flash-UD-Q4_K_XL.gguf \ --alias &amp;quot;glm-4.7-flash&amp;quot; \ --jinja \ --repeat-penalty 1.0 \ --seed 1234 \ --temp 0.7 \ --top-p 1 \ --min-p 0.01 \ --threads 12 \ --n-cpu-moe 32 \ --fit on \ --kv-unified \ --flash-attn off \ --batch-size 256 \ --ubatch-size 256 \ --ctx-size 65535 \ --host 0.0.0.0 &lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The most important setting was &lt;code&gt;--flash-attn off&lt;/code&gt; ! Since old AMD RDNA2 cards doesn't support flash attention, llama switches to fallback CPU and makes work unusable.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The second important parameter is &lt;code&gt;--n-cpu-moe xx&lt;/code&gt; which allows your to balance RAM between CPU and GPU. Here is my result:&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;bash load_tensors: CPU_Mapped model buffer size = 11114.88 MiB load_tensors: ROCm0 model buffer size = 6341.37 MiB &lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the rest thing is about fighting for the model brains (size) and allocation. You can run a bigger model if you decrease a context size and batches and vice versa.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Experiments&lt;/h3&gt; &lt;p&gt;During my experiments I switched between several models. I also generated test promt and passed output to Cloud to make raiting.&lt;/p&gt; &lt;p&gt;Here is tested models: 1. GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL.gguf 2. GLM-4.7-Flash-UD-Q3_K_XL.gguf (no reasoning) 3. GLM-4.7-Flash-UD-Q3_K_XL.gguf 4. GLM-4.7-Flash-UD-Q4_K_XL.gguf&lt;/p&gt; &lt;p&gt;I run once a model without reasoning occasionally, but it was very useful for raiting evaluation&lt;/p&gt; &lt;p&gt;Here is a test prompt:&lt;/p&gt; &lt;p&gt;```bash time curl http://myserver:8080/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;glm-4.7-flash&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Write a JavaScript function to sort an array.&amp;quot; } ], &amp;quot;temperature&amp;quot;: 0.7, &amp;quot;max_tokens&amp;quot;: 2048, &amp;quot;stream&amp;quot;: false, &amp;quot;stop&amp;quot;: [&amp;quot;&amp;lt;|user|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;] }'&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;This prompt was processed in 1:08 minute in average&lt;/p&gt; &lt;h3&gt;Benchmark&lt;/h3&gt; &lt;p&gt;The biggest model which fits into GPU memory is &lt;code&gt;GLM-4.7-Flash-UD-Q3_K_XL.gguf&lt;/code&gt; Here is a benchmark of this model with all defaults &lt;/p&gt; &lt;p&gt;&lt;code&gt; /build/bin/llama-bench --model unsloth/GLM-4.7-Flash-UD-Q3_K_XL.gguf -ngl 99 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 6900 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | deepseek2 ?B Q3_K - Medium | 12.85 GiB | 29.94 B | ROCm | 99 | pp512 | 1410.65 Â± 3.52 | | deepseek2 ?B Q3_K - Medium | 12.85 GiB | 29.94 B | ROCm | 99 | tg128 | 66.19 Â± 0.03 |&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Claude raiting&lt;/h3&gt; &lt;p&gt;I need to say here that I really love Claude, but it is very chatty. I put the main takeaways from it's report &lt;/p&gt; &lt;h4&gt;&lt;strong&gt;B. Feature Completeness&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;code&gt;text â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Feature â”‚ Model 1 â”‚ Model 2 â”‚ Model 3 â”‚ Model 4 â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Ascending sort â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ Descending sort â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ String sorting â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ âœ… â”‚ â”‚ Object sorting â”‚ âœ… â”‚ âœ… â”‚ âŒ â”‚ âŒ â”‚ â”‚ Bubble Sort â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ âœ… â”‚ â”‚ Immutability (spread) â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ âŒ â”‚ â”‚ Mutation warning â”‚ âŒ â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ Comparator explanation â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ Copy technique â”‚ âŒ â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ TOTAL FEATURES â”‚ 4/9 â”‚ 5/9 â”‚ 7/9 â”‚ 7/9 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;Updated Final Rankings&lt;/strong&gt;&lt;/h3&gt; &lt;h4&gt;&lt;strong&gt;ğŸ¥‡ GOLD: Model 4 (Q4_K_XL)&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Score: 94/100&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; - âœ… &lt;strong&gt;Best-organized reasoning&lt;/strong&gt; (9-step structured process) - âœ… &lt;strong&gt;Clearest section headers&lt;/strong&gt; with use-case labels - âœ… &lt;strong&gt;Explicit copy technique warning&lt;/strong&gt; (immutability guidance) - âœ… &lt;strong&gt;Good array example&lt;/strong&gt; (shows string sort bug) - âœ… &lt;strong&gt;String + Bubble Sort&lt;/strong&gt; included - âœ… &lt;strong&gt;Fast generation&lt;/strong&gt; (23.62 tok/sec, 2nd place) - âœ… &lt;strong&gt;Higher quality quantization&lt;/strong&gt; (Q4 vs Q3)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weaknesses:&lt;/strong&gt; - âŒ Doesn't use spread operator in examples (tells user to do it) - âŒ No object sorting - âŒ 15 fewer tokens of content than Model 3&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Professional development, code reviews, production guidance&lt;/p&gt; &lt;h4&gt;&lt;strong&gt;4th Place: Model 1 (Q3_K_XL REAP-23B-A3B)&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Score: 78/100&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; - âœ… Has reasoning - âœ… Object sorting included - âœ… Functional code&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weaknesses:&lt;/strong&gt; - âŒ &lt;strong&gt;Weakest array example&lt;/strong&gt; - âŒ &lt;strong&gt;Slowest generation&lt;/strong&gt; (12.53 tok/sec = &lt;strong&gt;50% slower&lt;/strong&gt; than Model 3) - âŒ &lt;strong&gt;Fewest features&lt;/strong&gt; (4/9) - âŒ No Bubble Sort - âŒ No string sorting - âŒ No immutability patterns - âŒ Special REAP quantization doesn't show advantages here&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Resource-constrained environments, basic use cases&lt;/p&gt; &lt;h3&gt;My conclusions&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;We can still use old AMD GPUs for local inference&lt;/li&gt; &lt;li&gt;Model size still does matter, even with quantisation!&lt;/li&gt; &lt;li&gt;But we can run models bigger than GPU VRAM size!&lt;/li&gt; &lt;li&gt;Recent llama flags give you a large space for experiments&lt;/li&gt; &lt;li&gt;&lt;code&gt;--n-cpu-moe&lt;/code&gt; is very useful for GPU/CPU balance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And the most important conclusion that this is not the final result!&lt;/p&gt; &lt;p&gt;Please feel free to share you findings and improvements with humans and robots! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Begetan"&gt; /u/Begetan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r45hkh/running_glm47_on_an_old_amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r45hkh/running_glm47_on_an_old_amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r45hkh/running_glm47_on_an_old_amd_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T00:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4f4k0</id>
    <title>What is the best way to evaluate if benchmarks are no longer the best?</title>
    <updated>2026-02-14T08:10:10+00:00</updated>
    <author>
      <name>/u/pn_1984</name>
      <uri>https://old.reddit.com/user/pn_1984</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, we developed benchmarks and evaluated them against it. Recently we see a lot more Chinese models performing really well in those benchmarks but the general feedback from users seem to be, don't trust the benchmarks. This sort of reminds me of the infamous Dieselgate. However now the question is, how are you managing to evaluate the models? &lt;/p&gt; &lt;p&gt;I have seen some people mentioning to use some questions only you know and never publish it. I think this could work but it seems more anectodal to me. &lt;/p&gt; &lt;p&gt;Are there other tricks you use? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pn_1984"&gt; /u/pn_1984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4f4k0/what_is_the_best_way_to_evaluate_if_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4f4k0/what_is_the_best_way_to_evaluate_if_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4f4k0/what_is_the_best_way_to_evaluate_if_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T08:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3uj0h</id>
    <title>MiniMax-M2.5 (230B MoE) GGUF is here - First impressions on M3 Max 128GB</title>
    <updated>2026-02-13T16:56:58+00:00</updated>
    <author>
      <name>/u/Remarkable_Jicama775</name>
      <uri>https://old.reddit.com/user/Remarkable_Jicama775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ğŸ”¥ UPDATE 2: Strict Perplexity Benchmark &amp;amp; Trade-off Analysis&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/ubergarm"&gt;u/ubergarm&lt;/a&gt; and the community for pointing out the context discrepancy in my initial PPL run (I used -c 4096, which inflated the score).&lt;/p&gt; &lt;p&gt;I just re-ran the benchmark on the M3 Max using standard comparison parameters (-c 512, -b 2048, --seed 1337) to get an apples-to-apples comparison with SOTA custom mixes (like IQ4_XS).&lt;/p&gt; &lt;p&gt;The Real Numbers:&lt;/p&gt; &lt;p&gt;My Q3_K_L (Standard): 8.7948 PPL (+/- 0.07)&lt;/p&gt; &lt;p&gt;Custom IQ4_XS Mix (ubergarm): ~8.57 PPL&lt;/p&gt; &lt;p&gt;The Verdict / Why use this Q3_K_L? While the custom mix wins on pure reasoning density (~0.22 PPL delta), this Q3_K_L remains the &amp;quot;bandwidth king&amp;quot; for Mac users.&lt;/p&gt; &lt;p&gt;RAM Headroom: It fits comfortably in 128GB with room for context (unlike Q4 which hits swap).&lt;/p&gt; &lt;p&gt;Speed: Because the attn.* tensors are smaller (Q3 vs Q8 in custom mixes), we are seeing 28.7 t/s generation speed due to lower memory bandwidth pressure.&lt;/p&gt; &lt;p&gt;TL;DR: Use this Q3_K_L if you are strictly limited to 128GB RAM and prioritize speed/compatibility. Use an IQ4_XS mix if you have 192GB+ or prioritize absolute maximum reasoning over speed. &lt;strong&gt;Update: Q3_K_L is officially LIVE on Hugging Face! Link. Tested and verified at 28.7 t/s on M3 Max. Enjoy the native RAM performance!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ğŸ”¬ &lt;strong&gt;Perplexity Validation (WikiText-2)&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final PPL: 8.2213 +/- 0.09&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Context: 4096 / 32 chunks&lt;/p&gt; &lt;p&gt;Outcome: The Q3_K_L quantization maintains high logical coherence while boosting speed to 28.7 t/s. Minimal degradation for a ~20GB size reduction vs Q4. Just ran PPL on my Q3_K_L (110.22 GiB). Got a Final PPL of 8.2213 (+/- 0.09) on WikiText-2. It seems that going the FP8 -&amp;gt; F16 Master -&amp;gt; Q3_K_L route really paid off compared to standard quants. It beats the IQ4_XS efficiency curve while fitting perfectly in 128GB RAM at 28.7 t/s&lt;/p&gt; &lt;p&gt;The new MiniMax-M2.5 is a beast, but running a 230B MoE locally isn't easy. Iâ€™ve finished the quantization process using llama.cpp (b8022) and optimized it specifically for high-RAM Apple Silicon.&lt;/p&gt; &lt;p&gt;ğŸš€ The &amp;quot;Sweet Spot&amp;quot; for 128GB RAM: Q3_K_L After initial testing with Q4_K_M (132GB), it was clear that hitting the swap was killing performance. I went back to the F16 Master (457GB) to cook a high-quality Q3_K_L (~110GB).&lt;/p&gt; &lt;p&gt;Benchmarks (M3 Max 128GB):&lt;/p&gt; &lt;p&gt;Prompt Processing: &lt;strong&gt;99.2 t/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Generation: &lt;strong&gt;28.7 t/s ğŸš€&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;RAM Behavior: 100% native RAM usage. Zero swap lag.&lt;/p&gt; &lt;p&gt;ğŸ›  Technical Details To ensure maximum reasoning fidelity, I avoided direct FP8-to-Quant conversion. The workflow was: Original FP8 -&amp;gt; F16 GGUF Master -&amp;gt; K-Quants (Q4_K_M &amp;amp; Q3_K_L).&lt;/p&gt; &lt;p&gt;Architecture: 230B Mixture of Experts (MiniMax-M2).&lt;/p&gt; &lt;p&gt;Logic: The Jinja chat template is working perfectly; &amp;lt;think&amp;gt; tags are isolated as intended.&lt;/p&gt; &lt;p&gt;Context: Native 196k support.&lt;/p&gt; &lt;p&gt;ğŸ“¥ Links &amp;amp; Resources GGUF Repo: &lt;a href="https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Usage: ./llama-cli -m minimax-m2.5-Q3_K_L.gguf -n -1 \ -c 262000 \ -ngl 99 -fa on -ctk q4_0 -ctv q4_0 -b 2048 -ub 1024 --port 8080 --jinja --verbose -sm none --draft 16 -ncmoe 0 --cache-reuse 1024 --draft-p-min 0.5&lt;/p&gt; &lt;p&gt;For those with 64GB or 96GB setups, let me know if there's interest in IQ2_XXS or IQ3_XS versions. I'm happy to cook more if the demand is there!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Jicama775"&gt; /u/Remarkable_Jicama775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r49hob</id>
    <title>Minimax 2.5 is out, considering local deployment</title>
    <updated>2026-02-14T03:09:14+00:00</updated>
    <author>
      <name>/u/Dramatic_Spirit_8436</name>
      <uri>https://old.reddit.com/user/Dramatic_Spirit_8436</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently tried out Minimax 2.5, which just dropped, and from what Iâ€™ve heard, the results are pretty impressive. I gave it a go on zenmux, and I have to say, it really covers a lot of ground. The flexibility, speed, and accuracy are definitely noticeable improvements.&lt;/p&gt; &lt;p&gt;Now, Iâ€™m thinking about deploying it locally. Iâ€™ve used Ollama for deployments before, but I noticed that for Minimax 2.5, Ollama only offers a cloud version. Iâ€™m curious about other deployment options and wondering what the difficulty level and hardware costs would be for a local setup.&lt;/p&gt; &lt;p&gt;Has anyone tried deploying Minimax 2.5 locally, or can share any insights into the hardware requirements? Any advice would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dramatic_Spirit_8436"&gt; /u/Dramatic_Spirit_8436 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r49hob/minimax_25_is_out_considering_local_deployment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r49hob/minimax_25_is_out_considering_local_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r49hob/minimax_25_is_out_considering_local_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T03:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4ekyf</id>
    <title>Coding agent for local LLMs?</title>
    <updated>2026-02-14T07:38:24+00:00</updated>
    <author>
      <name>/u/PaMRxR</name>
      <uri>https://old.reddit.com/user/PaMRxR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It feels like all popular coding agents are heavily tuned for the big capable models. Huge system prompt, verbose tool documentation, etc. fill up the context before you even try to do anything.&lt;/p&gt; &lt;p&gt;Any suggestions for a simpler tool that is geared towards locally hosted LLMs with more limited context room? Or at least one where all the text it adds behind the scenes is configurable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaMRxR"&gt; /u/PaMRxR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ekyf/coding_agent_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ekyf/coding_agent_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ekyf/coding_agent_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T07:38:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yahe</id>
    <title>LLaDA2.1 (100B/16B) released â€” now with token editing for massive speed gains</title>
    <updated>2026-02-13T19:16:39+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaDA2.1 builds on LLaDA2.0 by introducing Token-to-Token (T2T) editing alongside the standard Mask-to-Token decoding. Instead of locking in tokens once generated, the model can now retroactively correct errors during inference â€” enabling much more aggressive parallel drafting.&lt;/p&gt; &lt;p&gt;Two decoding modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;S Mode (Speedy): Aggressively low masking threshold + T2T correction. On coding tasks, LLaDA2.1-flash (100B) hits 892 TPS on HumanEval+, 801 TPS on BigCodeBench, 663 TPS on LiveCodeBench.&lt;/li&gt; &lt;li&gt;Q Mode (Quality): Conservative thresholds for best benchmark scores â€” surpasses LLaDA2.0 on both Mini and Flash.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First large-scale RL framework for diffusion LLMs (EBPO), improving reasoning and instruction following&lt;/li&gt; &lt;li&gt;Multi-Block Editing (MBE): revisit and revise previously generated blocks, consistent gains on reasoning/coding at modest speed cost&lt;/li&gt; &lt;li&gt;LLaDA2.1-mini (16B) peaks at ~1587 TPS on HumanEval+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/inclusionAI/llada21"&gt;https://huggingface.co/collections/inclusionAI/llada21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/inclusionAI/LLaDA2.X"&gt;https://github.com/inclusionAI/LLaDA2.X&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech Report: &lt;a href="https://huggingface.co/papers/2602.08676"&gt;https://huggingface.co/papers/2602.08676&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3uixu</id>
    <title>GPT-OSS (20B) running 100% locally in your browser on WebGPU</title>
    <updated>2026-02-13T16:56:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"&gt; &lt;img alt="GPT-OSS (20B) running 100% locally in your browser on WebGPU" src="https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c42411a8a77fe60dfe81ecb5c06b854e8c0ac88" title="GPT-OSS (20B) running 100% locally in your browser on WebGPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, I released a demo showcasing GPT-OSS (20B) running 100% locally in-browser on WebGPU, powered by Transformers.js v4 (preview) and ONNX Runtime Web. Hope you like it! &lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Demo (+ source code): &lt;a href="https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU"&gt;https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU&lt;/a&gt;&lt;br /&gt; - Optimized ONNX model: &lt;a href="https://huggingface.co/onnx-community/gpt-oss-20b-ONNX"&gt;https://huggingface.co/onnx-community/gpt-oss-20b-ONNX&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ioqb4q8jkajg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4ie8z</id>
    <title>I tested 21 small LLMs on tool-calling judgment â€” Round 2 with every model you asked for</title>
    <updated>2026-02-14T11:31:45+00:00</updated>
    <author>
      <name>/u/MikeNonect</name>
      <uri>https://old.reddit.com/user/MikeNonect</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A week ago, I posted the Round 1 results: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qyg10z/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qyg10z/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That benchmark tested 11 small models on whether they know &lt;em&gt;when&lt;/em&gt; to call a tool, not just whether they can.&lt;/p&gt; &lt;p&gt;The post got some attention, and many of you asked to include specific models.&lt;/p&gt; &lt;p&gt;So I tested (almost) all of them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Round 2: 10 new models, 21 total, 756 inference calls on CPU.&lt;/strong&gt;&lt;br /&gt; Same 12 prompts, same scoring, same Framework 13 laptop, no GPU.&lt;/p&gt; &lt;h1&gt;The results&lt;/h1&gt; &lt;p&gt;Four models tie for #1 at &lt;strong&gt;0.880 Agent Score&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lfm2.5:1.2b&lt;/li&gt; &lt;li&gt;qwen3:0.6b&lt;/li&gt; &lt;li&gt;qwen3:4b&lt;/li&gt; &lt;li&gt;phi4-mini:3.8b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The biggest surprise was &lt;strong&gt;lfm2.5:1.2b&lt;/strong&gt; â€” a 1.2B state-space hybrid â€” tying for #1 with the fastest latency in the top tier (~1.5s).&lt;/p&gt; &lt;p&gt;It originally scored 0.640 because it outputs bracket notation:&lt;/p&gt; &lt;p&gt;[get_weather(city=&amp;quot;Antwerp&amp;quot;)]&lt;/p&gt; &lt;p&gt;instead of XML tool tags. After fixing the parser, it turned out the model had been making correct decisions all along.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;qwen3:0.6b (600M parameters)&lt;/strong&gt; also ties for #1.&lt;/p&gt; &lt;p&gt;The Qwen3 family ranking is non-monotonic:&lt;/p&gt; &lt;p&gt;0.6B &amp;gt; 4B &amp;gt; 1.7B&lt;/p&gt; &lt;p&gt;The 1.7B sits in a capability valley â€” aggressive enough to call tools, but not careful enough to know when not to.&lt;/p&gt; &lt;h1&gt;Score table&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Action&lt;/th&gt; &lt;th align="left"&gt;Restraint&lt;/th&gt; &lt;th align="left"&gt;Wrong Tool&lt;/th&gt; &lt;th align="left"&gt;Agent Score&lt;/th&gt; &lt;th align="left"&gt;Avg ms&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;lfm2.5:1.2b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;1470&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;phi4-mini:3.8b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;5460&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;qwen3:0.6b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;3645&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;qwen3:4b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;63717&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;qwen2.5:1.5b&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.840&lt;/td&gt; &lt;td align="left"&gt;2211&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;bitnet-2B-4T&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.810&lt;/td&gt; &lt;td align="left"&gt;2036&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;ministral-3:3b&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;7157&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;smollm2:1.7b&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.740&lt;/td&gt; &lt;td align="left"&gt;1626&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;deepseek-r1:1.5b&lt;/td&gt; &lt;td align="left"&gt;0.300&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.720&lt;/td&gt; &lt;td align="left"&gt;1672&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;smollm3:3b&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.710&lt;/td&gt; &lt;td align="left"&gt;12096&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;qwen2.5:3b&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;td align="left"&gt;2801&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;qwen3:1.7b&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;td align="left"&gt;11903&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;granite4:3b&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;td align="left"&gt;2402&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;llama3.2:3b&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.660&lt;/td&gt; &lt;td align="left"&gt;1726&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;qwen2.5:0.5b&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;0.640&lt;/td&gt; &lt;td align="left"&gt;881&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;functiongemma&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;0.640&lt;/td&gt; &lt;td align="left"&gt;476&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;bitnet-3B&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;11362&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;jan-v3:4b&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.560&lt;/td&gt; &lt;td align="left"&gt;2335&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.550&lt;/td&gt; &lt;td align="left"&gt;2426&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;granite3.3:2b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.480&lt;/td&gt; &lt;td align="left"&gt;1650&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;llama3.2:1b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;0.430&lt;/td&gt; &lt;td align="left"&gt;1461&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;What I learned building the parser&lt;/h1&gt; &lt;p&gt;The most interesting (but obvious) finding wasn't about a specific model.&lt;/p&gt; &lt;p&gt;It was this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How you parse tool calls matters as much as what you test.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Five models required custom fallback parsers because they don't use standard formats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lfm2.5 â†’ bracket notation&lt;/li&gt; &lt;li&gt;jan-v3 â†’ raw JSON&lt;/li&gt; &lt;li&gt;gemma3 â†’ function syntax inside tags&lt;/li&gt; &lt;li&gt;deepseek-r1 â†’ bare function calls&lt;/li&gt; &lt;li&gt;smollm3 â†’ sometimes omits tags entirely&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hereâ€™s the twist:&lt;/p&gt; &lt;p&gt;Fixing the parser doesn't always &lt;em&gt;help&lt;/em&gt; a model.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lfm2.5: 0.640 â†’ 0.880 (it was right all along)&lt;/li&gt; &lt;li&gt;gemma3: 0.600 â†’ 0.550 (parser blindness was hiding bad behavior)&lt;/li&gt; &lt;li&gt;smollm3: 0.740 â†’ 0.710&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Format-blind benchmarks don't just underestimate models.&lt;br /&gt; They can &lt;strong&gt;overestimate them too&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Your requested models&lt;/h1&gt; &lt;p&gt;Quick replies to the Round 1 commenters:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 family&lt;/strong&gt; â€” all tested&lt;br /&gt; 0.6B ties #1, 4B matches but ~17Ã— slower, 1.7B weakest (0.670).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LFM 2.5:1.2B&lt;/strong&gt; â€” ties #1. Needed a bracket parser to reveal its true score.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FunctionGemma (270M)&lt;/strong&gt; â€” fastest model (476 ms). Perfect restraint but falls for keyword traps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Jan v3:4B&lt;/strong&gt; â€” Action 0.900 but zero restraint. Calls a tool on literally everything. Score: 0.560.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Granite4:3B&lt;/strong&gt; â€” clear improvement over Granite3.3:2B (0.480 â†’ 0.670).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SmolLM3:3B&lt;/strong&gt; â€” reasoning traces often correct, execution sometimes fails.&lt;/p&gt; &lt;p&gt;DeepBrainz-R1-2B GGUF outputs were corrupted. Couldnâ€™t benchmark.&lt;br /&gt; Gemma 3n (5.6GB) and 15B models were outside the â€œsmall modelâ€ scope.&lt;/p&gt; &lt;h1&gt;What each model called on every prompt&lt;/h1&gt; &lt;p&gt;Legend:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;W = get_weather, S = search_files, M = schedule_meeting, â€” = no tool call&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bold&lt;/strong&gt; = correct on hard prompt&lt;/li&gt; &lt;li&gt;&lt;del&gt;Strikethrough&lt;/del&gt; = wrong tool or restraint failure&lt;/li&gt; &lt;li&gt;P5 and P9 should be &lt;strong&gt;â€”&lt;/strong&gt; (restraint). P10â€“P12 are judgment traps.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;P1&lt;/th&gt; &lt;th align="left"&gt;P2&lt;/th&gt; &lt;th align="left"&gt;P3&lt;/th&gt; &lt;th align="left"&gt;P4&lt;/th&gt; &lt;th align="left"&gt;P5&lt;/th&gt; &lt;th align="left"&gt;P6&lt;/th&gt; &lt;th align="left"&gt;P7&lt;/th&gt; &lt;th align="left"&gt;P8&lt;/th&gt; &lt;th align="left"&gt;P9&lt;/th&gt; &lt;th align="left"&gt;P10&lt;/th&gt; &lt;th align="left"&gt;P11&lt;/th&gt; &lt;th align="left"&gt;P12&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;Expected&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;S&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W?&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;â€”&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;S&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;â€”&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;S&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi4-mini:3.8b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:0.6b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:4b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;lfm2.5:1.2b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:1.5b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bitnet-2B-4T&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;ava&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ministral-3:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;smollm2:1.7b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek-r1:1.5b&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;smollm3:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:1.7b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite4:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:0.5b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;functiongemma&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bitnet-3B&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;jan-v3:4b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite3.3:2b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:1b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;M&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;You can really see the patterns here. The top models (phi4-mini, qwen3, lfm2.5) have clean columns â€” no strikethrough.&lt;/p&gt; &lt;p&gt;The bottom models (llama3.2:1b, granite3.3:2b) are littered with wrong calls.&lt;/p&gt; &lt;p&gt;P12 is a sea of &lt;del&gt;W&lt;/del&gt; â€” almost everyone calls get_weather even though the weather is already in the prompt.&lt;/p&gt; &lt;h1&gt;Key takeaways&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local tool-calling agents work on commodity hardware.&lt;/strong&gt; Four models hit 0.880 on CPU in ~1.5 seconds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter count is a weak predictor.&lt;/strong&gt; A 600M model ties a 3.8B model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conservative behavior wins.&lt;/strong&gt; Top models succeed by &lt;em&gt;not&lt;/em&gt; acting on uncertain prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt P12 is hardest:&lt;/strong&gt; â€œThe weather is 8Â°C and rainy. Should I schedule a meeting?â€ Only 3/21 models get it right.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test your parser, not just your prompts.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full report, code, and raw data: &lt;a href="https://github.com/MikeVeerman/tool-calling-benchmark"&gt;https://github.com/MikeVeerman/tool-calling-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or test more models if people want a Round 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MikeNonect"&gt; /u/MikeNonect &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ie8z/i_tested_21_small_llms_on_toolcalling_judgment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ie8z/i_tested_21_small_llms_on_toolcalling_judgment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ie8z/i_tested_21_small_llms_on_toolcalling_judgment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4ipgm</id>
    <title>Kyutai Releases Hibiki-Zero</title>
    <updated>2026-02-14T11:49:27+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Kyutai Releases Hibiki-Zero: A3B Parameter Simultaneous Speech-to-Speech Translation Model Using GRPO Reinforcement Learning Without Any Word-Level Aligned Data&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/kyutai-labs/hibiki-zero"&gt;https://github.com/kyutai-labs/hibiki-zero&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ipgm/kyutai_releases_hibikizero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ipgm/kyutai_releases_hibikizero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ipgm/kyutai_releases_hibikizero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r40o83</id>
    <title>ubergarm/MiniMax-2.5-GGUF</title>
    <updated>2026-02-13T20:47:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"&gt; &lt;img alt="ubergarm/MiniMax-2.5-GGUF" src="https://preview.redd.it/e7zeec20qbjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dec023595454edde747bd1bebdaab70e22a17fe5" title="ubergarm/MiniMax-2.5-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just cooked and benchmarked (perplexity) of some MiniMax-M2.5 GGUF quants over at: &lt;a href="https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The IQ4_XS works on mainline llama.cpp, LMStudio, Kobold CPP etc. The other quants require ik_llama.cpp (which supports all of the quant types of mainline as well).&lt;/p&gt; &lt;p&gt;Gonna get some llama-sweep-bench tests for PP/TG drop off across context depth next. The smol-IQ3_KS was working in my `opencode` local testing and seems promising but probably a bit too large for enough context on 96GB VRAM hence the smaller IQ2_KS is also available at a cost to quality.&lt;/p&gt; &lt;p&gt;Fun stuff!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e7zeec20qbjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T20:47:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4ikop</id>
    <title>15% faster generation - by simply minimizing the webbrowser</title>
    <updated>2026-02-14T11:42:02+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some testing with llama.cpp and its web UI. While having the Windows task manager open I noticed that 3D usage was between 0% and 1% while idle, and maybe around 25% during inference.&lt;/p&gt; &lt;p&gt;Well, that &lt;em&gt;might&lt;/em&gt; have been the llama-server, but no: It's the updates of the web UI. The moment I minimized the browser the 3D usage went back to 0% to 1% during inference. The real-time streaming UI updates apparently put some strain on the GPU otherwise. I get 15% more TPS during generation when I minimize the webbrowser directly after starting a request.&lt;/p&gt; &lt;p&gt;There are a few other web-based applications on Windows that can also cause some GPU load - they're easy to identify in the GPU column of the details of the task manager. Anyway, maybe simply reducing the update frequency of the llama.cpp web UI will fully mitigate that impact.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ikop/15_faster_generation_by_simply_minimizing_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ikop/15_faster_generation_by_simply_minimizing_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ikop/15_faster_generation_by_simply_minimizing_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t8ro</id>
    <title>Nvidiaâ€™s new technique cuts LLM reasoning costs by 8x without losing accuracy</title>
    <updated>2026-02-13T16:09:31+00:00</updated>
    <author>
      <name>/u/Mission-Street4214</name>
      <uri>https://old.reddit.com/user/Mission-Street4214</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia developed a new technique called Dynamic Memory Sparsification (DMS) that vastly improves how LLMs manage their KV cache during inference. It accomplishes this by retrofitting existing models so that the attention layers output a &lt;strong&gt;learned keep or evict&lt;/strong&gt; signal for each token in the KV cache. &lt;/p&gt; &lt;p&gt;In addition, they've added a &amp;quot;delayed eviction&amp;quot; that marks a token as low-importance, but doesn't delete it immediately. Instead, it remains accessible for a short time and allows the model to extract any useful information into newer tokens before it's discarded.&lt;/p&gt; &lt;p&gt;These advancements reduce KV memory usage by up to &lt;strong&gt;8x&lt;/strong&gt;, allowing the model to think longer, run faster and handle more concurrent requests.&lt;/p&gt; &lt;p&gt;Definitely recommend reading the full article. Looking forward to seeing this on self hosted hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://venturebeat.com/orchestration/nvidias-new-technique-cuts-llm-reasoning-costs-by-8x-without-losing-accuracy"&gt;VentureBeat Article&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mission-Street4214"&gt; /u/Mission-Street4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3pxy7</id>
    <title>MiniMaxAI/MiniMax-M2.5 Â· Hugging Face</title>
    <updated>2026-02-13T14:01:52+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2.5 Â· Hugging Face" src="https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de0bab4be78008336f973196f0ed98e2bbe49764" title="MiniMaxAI/MiniMax-M2.5 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can monitor quants begin to appear with this search: &lt;a href="https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r41013</id>
    <title>GLM-5 Is a local GOAT</title>
    <updated>2026-02-13T21:00:08+00:00</updated>
    <author>
      <name>/u/FineClassroom2085</name>
      <uri>https://old.reddit.com/user/FineClassroom2085</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"&gt; &lt;img alt="GLM-5 Is a local GOAT" src="https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa3d1833719afef6a4e55f5f11807d2e7ef7d341" title="GLM-5 Is a local GOAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: I am a developer with over two decades of experience. I use LLMs heavily day to day from all of the major providers. Since the first Llama models came out I've been toying with local models, benchmarking them on real-world heavy use cases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Long story short:&lt;/strong&gt; GLM-5 is the first model I've been able to run locally that's actually impressed me. In 3 'shots' I was able to make a retro styled flappy clone AND deploy it to AWS with a cost assessment if it went viral.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My prompt&lt;/strong&gt;: Please generate a GPU accelerated clone of the game â€˜Flappy Birdâ€™ where using the spacebar causes the bird to â€˜flapâ€™, give it a 'retro inspired' design.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Setup&lt;/strong&gt;:&lt;br /&gt; - Dual RTX 6000 PRO MaxQ GPUs&lt;br /&gt; - 128gb of DDR5&lt;br /&gt; - AMD Ryzen Threadripper PRO 7975WX&lt;br /&gt; - GLM-5-744B served over vLLM with 128k context at IQ2_M&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;: Even with my decently powerful hardware, the token output was painfully slow at 16.5t/s. IMO, completely worth the wait though. The same test with Qwen3-Next-80b, GPT-OSS-120b and a few other leaders was unimpressive.&lt;/p&gt; &lt;p&gt;&lt;a href="https://flappy.tjameswilliams.com/"&gt;https://flappy.tjameswilliams.com/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FineClassroom2085"&gt; /u/FineClassroom2085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7l7iri95rbjg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T21:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r47fz0</id>
    <title>Claude Code with Local Models: Full Prompt Reprocessing with Every Request</title>
    <updated>2026-02-14T01:33:26+00:00</updated>
    <author>
      <name>/u/postitnote</name>
      <uri>https://old.reddit.com/user/postitnote</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very recently, I found that Claude Code was triggering full prompt processing for every request. I looked into the logs and found CC is adding this to the list of system messages: &lt;code&gt; text:&amp;quot;x-anthropic-billing-header: cc_version=2.1.39.c39; cc_entrypoint=cli; cch=56445;&amp;quot;, type:&amp;quot;text&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The values in the header changed with every request, and the template rendered it as text in the system prompt which caused a full reprocessing. With a little google search, I found &lt;a href="https://github.com/musistudio/claude-code-router/issues/1161"&gt;this&lt;/a&gt;, which recommended doing this to remove the header:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;set env &amp;quot;CLAUDE_CODE_ATTRIBUTION_HEADER&amp;quot;: &amp;quot;0&amp;quot; in claude settings.json&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And placing that in my ~/.claude/settings.json in the &amp;quot;env&amp;quot; section was enough to remove that from the system prompt and get my KV cache back to being effective again.&lt;/p&gt; &lt;p&gt;Hope that helps anyone running into the same issue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/postitnote"&gt; /u/postitnote &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T01:33:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yuyd</id>
    <title>has it begun?</title>
    <updated>2026-02-13T19:38:01+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt; &lt;img alt="has it begun?" src="https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36034757efbb832ba75f43ed04c4dc8c7bb34675" title="has it begun?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters"&gt;https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They were about to present the name of alibaba and Baidu as a potential threat or issue for helping chinese military in the Pentagon, but ultimately took their names off the list&lt;/p&gt; &lt;p&gt;Would love to hear what y'all think about this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ei9lt0u4ebjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4e3w3</id>
    <title>KaniTTS2, our text-to-speech model with frame-level position encodings, optimized for real-time conversational AI.</title>
    <updated>2026-02-14T07:10:30+00:00</updated>
    <author>
      <name>/u/KokaOP</name>
      <uri>https://old.reddit.com/user/KokaOP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to release KaniTTS2, our text-to-speech model with frame-level position encodings, optimized for real-time conversational AI.&lt;/p&gt; &lt;p&gt;What's in the release:&lt;/p&gt; &lt;p&gt;Pretrained Model (multilingual â€” English, Spanish, Kyrgyz)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/nineninesix/kani-tts-2-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ğŸ“Œ Currently supports 3 languages, with more being added over time. Stay tuned for updates as we expand language coverage.&lt;/p&gt; &lt;p&gt;ğŸ‡¬ğŸ‡§ English-specific Model&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-en"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ğŸ› ï¸ Full Pretraining Code â€” train your own TTS model from scratch&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nineninesix-ai/kani-tts-2-pretrain"&gt;https://github.com/nineninesix-ai/kani-tts-2-pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;400M parameter model built on LiquidAI's LFM2 backbone + Nvidia NanoCodec&lt;/p&gt; &lt;p&gt;~0.2 RTF on an RTX 5080, 3GB VRAM â€” fast enough for real-time use&lt;/p&gt; &lt;p&gt;Voice cloning with speaker embeddings&lt;/p&gt; &lt;p&gt;Pretrained on ~10k hours of speech data (8x H100s, just 6 hours of training!)&lt;/p&gt; &lt;p&gt;Why we're releasing the pretrain code: We want anyone to be able to train a TTS model for their own language, accent, or domain from scratch. The framework includes FSDP multi-GPU training, Flash Attention 2, YAML-driven configs, and built-in attention analysis metrics to validate layer isolation. Everything you need to go from dataset to deployed model.&lt;/p&gt; &lt;p&gt;Licensed Apache 2.0. Try the demos on our HF Spaces, and come chat with us on Discord if you have questions or want to contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KokaOP"&gt; /u/KokaOP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T07:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3weq3</id>
    <title>SWE-rebench Jan 2026: GLM-5, MiniMax M2.5, Qwen3-Coder-Next, Opus 4.6, Codex Performance</title>
    <updated>2026-02-13T18:06:40+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, Iâ€™m Anton from Nebius.&lt;/p&gt; &lt;p&gt;Weâ€™ve updated the &lt;strong&gt;SWE-rebench leaderboard&lt;/strong&gt; with our &lt;strong&gt;January runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;Key observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Code (Opus 4.6)&lt;/strong&gt; leads this snapshot at &lt;strong&gt;52.9% resolved rate&lt;/strong&gt; and also achieves the highest &lt;strong&gt;pass@5 (70.8%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.6&lt;/strong&gt; and &lt;strong&gt;gpt-5.2-xhigh&lt;/strong&gt; follow very closely (51.7%), making the top tier extremely tight.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gpt-5.2-medium (51.0%)&lt;/strong&gt; performs surprisingly close to the frontier configuration.&lt;/li&gt; &lt;li&gt;Among open models, &lt;strong&gt;Kimi K2 Thinking (43.8%)&lt;/strong&gt;, &lt;strong&gt;GLM-5 (42.1%)&lt;/strong&gt;, and &lt;strong&gt;Qwen3-Coder-Next (40.0%)&lt;/strong&gt; lead the pack.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MiniMax M2.5 (39.6%)&lt;/strong&gt; continues to show strong performance while remaining one of the cheapest options.&lt;/li&gt; &lt;li&gt;Clear gap between Kimi variants: &lt;strong&gt;K2 Thinking (43.8%)&lt;/strong&gt; vs &lt;strong&gt;K2.5 (37.9%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Newer smaller/flash variants (e.g., GLM-4.7 Flash, gpt-5-mini-medium) trade performance for efficiency, landing in the 25â€“31% range.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=jan_2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3zuuf</id>
    <title>GPT-OSS 120b Uncensored Aggressive Release (MXFP4 GGUF)</title>
    <updated>2026-02-13T20:15:33+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, made an uncensored version of GPT-OSS 120B.&lt;/p&gt; &lt;p&gt;Quick specs: 117B total params, ~5.1B active (MoE with 128 experts, top-4 routing), 128K context. MXFP4 is the model's native precision - this isn't a quantization, it's how it was trained. No overall quality loss, though you can see CoT behave differently at times.&lt;/p&gt; &lt;p&gt;This is the aggressive variant - &lt;strong&gt;observed 0 refusals to any query during testing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Completely uncensored while keeping full model capabilities intact.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings:&lt;/p&gt; &lt;p&gt;- --temp 1.0 --top-k 40&lt;/p&gt; &lt;p&gt;- Disable everything else (top_p, min_p, repeat penalty, etc.) - some clients turn&lt;/p&gt; &lt;p&gt;these on by default&lt;/p&gt; &lt;p&gt;- llama.cpp users: --jinja is required for the Harmony response format or the model won't work right&lt;/p&gt; &lt;p&gt;- Example: llama-server -m model.gguf --jinja -fa -b 2048 -ub 2048&lt;/p&gt; &lt;p&gt;Single 61GB file. Fits on one H100. For lower VRAM, use --n-cpu-moe N in llama.cpp to offload MoE layers to CPU.&lt;/p&gt; &lt;p&gt;Works with llama.cpp, LM Studio, Ollama, etc.&lt;/p&gt; &lt;p&gt;If you want smaller models, I also have GPT-OSS 20B, GLM 4.7 Flash and Qwen3 8b VL uncensored:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/models/"&gt;https://huggingface.co/HauhauCS/models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T20:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4hhyy</id>
    <title>local vibe coding</title>
    <updated>2026-02-14T10:37:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please share your experience with vibe coding using local (not cloud) models.&lt;/p&gt; &lt;p&gt;General note: to use tools correctly, some models require a modified chat template, or you may need in-progress PR.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/anomalyco/opencode"&gt;https://github.com/anomalyco/opencode&lt;/a&gt; - probably the most mature and feature complete solution. I use it similarly to Claude Code and Codex.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/mistralai/mistral-vibe"&gt;https://github.com/mistralai/mistral-vibe&lt;/a&gt; - a nice new project, similar to opencode, but simpler.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RooCodeInc/Roo-Code"&gt;https://github.com/RooCodeInc/Roo-Code&lt;/a&gt; - integrates with Visual Studio Code (not CLI).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Aider-AI/aider"&gt;https://github.com/Aider-AI/aider&lt;/a&gt; - a CLI tool, but it feels different from opencode (at least in my experience).&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.continue.dev/"&gt;https://docs.continue.dev/&lt;/a&gt; - I tried it last year as a Visual Studio Code plugin, but I never managed to get the CLI working with llama.cpp.&lt;/li&gt; &lt;li&gt;Cline - I was able to use it as Visual Studio Code plugin &lt;/li&gt; &lt;li&gt;Kilo Code - I was able to use it as Visual Studio Code plugin &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T10:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4hx24</id>
    <title>models : optimizing qwen3next graph by ggerganov Â· Pull Request #19375 Â· ggml-org/llama.cpp</title>
    <updated>2026-02-14T11:03:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"&gt; &lt;img alt="models : optimizing qwen3next graph by ggerganov Â· Pull Request #19375 Â· ggml-org/llama.cpp" src="https://external-preview.redd.it/VVE5ljhhmuVj3S3mZp6__yfxBNtIYwLxEpi1hRwGGjU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95716ff4da82d5b8eaae30528e020e4dd291b4ab" title="models : optimizing qwen3next graph by ggerganov Â· Pull Request #19375 Â· ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Faster (t/s) Qwen Next models.&lt;/p&gt; &lt;p&gt;There are still some in-progress PRs to fix/improve Qwen Next in llama.cpp. Let's hope this model will be awesome soon :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19375"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r44fzk</id>
    <title>The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'</title>
    <updated>2026-02-13T23:20:10+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"&gt; &lt;img alt="The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'" src="https://preview.redd.it/4rozb901icjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0af0fe460ed577cfa1d0490e0386a39aa78b986f" title="The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4rozb901icjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T23:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax â€” Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax â€” Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax â€” Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; â€” Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; â€” Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; â€” Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; â€” LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
