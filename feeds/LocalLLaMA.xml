<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-05T20:51:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q4wc0u</id>
    <title>Vision centric reasoning</title>
    <updated>2026-01-05T20:01:55+00:00</updated>
    <author>
      <name>/u/klop2031</name>
      <uri>https://old.reddit.com/user/klop2031</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting topic/paper: DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.24165"&gt;https://arxiv.org/abs/2512.24165&lt;/a&gt; &lt;a href="https://huggingface.co/yhx12/DiffThinker"&gt;https://huggingface.co/yhx12/DiffThinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am not an author of this paper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klop2031"&gt; /u/klop2031 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4wc0u/vision_centric_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4wc0u/vision_centric_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4wc0u/vision_centric_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T20:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4wibm</id>
    <title>I built a more user-friendly desktop app for managing and chatting with local LLMs</title>
    <updated>2026-01-05T20:08:18+00:00</updated>
    <author>
      <name>/u/Horizonyu13</name>
      <uri>https://old.reddit.com/user/Horizonyu13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4wibm/i_built_a_more_userfriendly_desktop_app_for/"&gt; &lt;img alt="I built a more user-friendly desktop app for managing and chatting with local LLMs" src="https://b.thumbs.redditmedia.com/pRXnqSrSyAxBIHl5SpUTnDyrhUjGspoq_i15iNG9cyA.jpg" title="I built a more user-friendly desktop app for managing and chatting with local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a personal project I‚Äôve been working on: &lt;strong&gt;Horizon AI Desktop&lt;/strong&gt;, a local-first desktop application designed to interact with &lt;strong&gt;locally installed LLMs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The main goal was to have a clean, fast interface to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chat with local models&lt;/li&gt; &lt;li&gt;Manage installed models from one place&lt;/li&gt; &lt;li&gt;Keep everything &lt;strong&gt;fully offline / private&lt;/strong&gt; (no cloud, no telemetry)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Local LLM chat interface (conversation history, fast switching)&lt;/li&gt; &lt;li&gt;Model management (detect installed models, delete/update them)&lt;/li&gt; &lt;li&gt;Simple, minimal UI focused on usability&lt;/li&gt; &lt;li&gt;Desktop app (not a web wrapper running in the cloud)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; React&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Python (worker-based architecture, not FastAPI)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLMs:&lt;/strong&gt; Local models only (Ollama-compatible setup)&lt;/li&gt; &lt;li&gt;Focus on keeping frontend and backend loosely coupled&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why I‚Äôm posting here&lt;/h1&gt; &lt;p&gt;I‚Äôm mainly looking for &lt;strong&gt;feedback from people who actually run local models daily&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UX improvements you‚Äôd expect from a local LLM manager&lt;/li&gt; &lt;li&gt;Missing features you‚Äôd personally want&lt;/li&gt; &lt;li&gt;Architecture mistakes or things that could scale badly&lt;/li&gt; &lt;li&gt;Anything that feels ‚Äúoff‚Äù compared to your current workflow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is still evolving, but already usable.&lt;br /&gt; If there‚Äôs interest, I‚Äôm open to making it fully open-source and documenting the architecture properly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/GabrielHori/Horizon-AI"&gt;https://github.com/GabrielHori/Horizon-AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer technical questions ‚Äî thanks for taking a look üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Horizonyu13"&gt; /u/Horizonyu13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q4wibm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4wibm/i_built_a_more_userfriendly_desktop_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4wibm/i_built_a_more_userfriendly_desktop_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T20:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4kxs7</id>
    <title>backend sampling has been merged into llama.cpp</title>
    <updated>2026-01-05T12:54:29+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4kxs7/backend_sampling_has_been_merged_into_llamacpp/"&gt; &lt;img alt="backend sampling has been merged into llama.cpp" src="https://external-preview.redd.it/2cWHgmxIHvuopjsCPWeTIoGKCFWNy96-VYIpJVWfrsI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28dd05c0eb1e48f57f8d81d5825967a34e7bc5bf" title="backend sampling has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It means that sampling can now be integrated directly into the computation graph on backends (like CUDA), potentially reducing GPU/CPU data transfers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17004"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4kxs7/backend_sampling_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4kxs7/backend_sampling_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T12:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4ahw1</id>
    <title>Llama 3.3 8B, abliterated to &lt;0.05 KL</title>
    <updated>2026-01-05T03:18:45+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an abliterated version of the allegedly leaked Llama 3.3 8B 128k model that tries to minimize intelligence loss while optimizing for compliance.&lt;/p&gt; &lt;p&gt;Link (BF16 weights):&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Llama-3.3-8B-Instruct-128K_Abliterated"&gt;https://huggingface.co/SicariusSicariiStuff/Llama-3.3-8B-Instruct-128K_Abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Credits: Fizzarolli, p-e-w, some employee @ meta for another successful failure.&lt;/p&gt; &lt;p&gt;Enjoy :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T03:18:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q41bw1</id>
    <title>GLM-Image model from Z.ai is coming</title>
    <updated>2026-01-04T20:54:04+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt; &lt;img alt="GLM-Image model from Z.ai is coming" src="https://preview.redd.it/sm31vizebebg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae576450ba7112c06760ff8cddee6f5bdd7b672" title="GLM-Image model from Z.ai is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43100/files"&gt;https://github.com/huggingface/transformers/pull/43100/files&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sm31vizebebg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4hcsf</id>
    <title>Grafted Titans: a Plug-and-Play Neural Memory for Open-Weight LLMs</title>
    <updated>2026-01-05T09:34:36+00:00</updated>
    <author>
      <name>/u/Forsaken-Park8149</name>
      <uri>https://old.reddit.com/user/Forsaken-Park8149</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hcsf/grafted_titans_a_plugandplay_neural_memory_for/"&gt; &lt;img alt="Grafted Titans: a Plug-and-Play Neural Memory for Open-Weight LLMs" src="https://external-preview.redd.it/yT3YokEiN9WPYwPoTIkN__Yl-gpZQVW4GImTVvFalds.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9763e79f1d5ac10e28dcd72effae05fbce31406" title="Grafted Titans: a Plug-and-Play Neural Memory for Open-Weight LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with &lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt;, specifically trying to replicate the core concept of Google‚Äôs &amp;quot;Titans&amp;quot; architecture (learning a neural memory on the fly) without the massive compute requirement of training a transformer from scratch.&lt;/p&gt; &lt;p&gt;I wanted to see if I could &amp;quot;graft&amp;quot; a trainable memory module onto a &lt;strong&gt;frozen open-weight model&lt;/strong&gt; (Qwen-2.5-0.5B) using a consumer-grade setup (I got Nvidia DGX Spark BlackWell, 128GB)&lt;/p&gt; &lt;p&gt;I‚Äôm calling this architecture &amp;quot;Grafted Titans.&amp;quot; I just finished the evaluation on the BABILong benchmark and the results were very interesting&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model:&lt;/strong&gt; Qwen-2.5-0.5B-Instruct (Frozen weights).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mechanism:&lt;/strong&gt; I appended memory embeddings to the input layer (Layer 0) via a trainable cross-attention gating mechanism. This acts as an adapter, allowing the memory to update recursively while the base model stays static.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Benchmark (BABILong, up to 2k context):&lt;/strong&gt; I used a strict 2-turn protocol.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Turn 1:&lt;/strong&gt; Feed context -&amp;gt; Memory updates -&amp;gt; Context removed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Turn 2:&lt;/strong&gt; Feed question -&amp;gt; Model retrieves answer solely from neural memory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt; I compared my grafted memory against two baselines.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Random Guessing:&lt;/strong&gt; 0.68% Accuracy. Basically all wrong.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vanilla Qwen (Full Context):&lt;/strong&gt; I fed the &lt;em&gt;entire&lt;/em&gt; token context to the standard Qwen model in the prompt. It scored &lt;strong&gt;34.0%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grafted Titans (Memory Only):&lt;/strong&gt; The model saw &lt;em&gt;no&lt;/em&gt; context in the prompt, only the memory state. It scored &lt;strong&gt;44.7%&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It appears the &lt;strong&gt;neural memory module is acting as a&lt;/strong&gt; &lt;strong&gt;denoising filter&lt;/strong&gt;. When a small model like Qwen-0.5B sees 1.5k tokens of text, its attention mechanism gets &amp;quot;diluted&amp;quot; by the noise. The grafted memory, however, compresses that signal into specific vectors, making retrieval sharper than the native attention window.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Signal Dilution:&lt;/strong&gt; Because I'm injecting memory at Layer 0 (soft prompting style), I suspect a vanishing gradient effect as the signal travels up the layers. Future versions need multi-layer injection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Guardrails:&lt;/strong&gt; The memory is currently &amp;quot;gullible.&amp;quot; It treats all input as truth, meaning it's highly susceptible to poisoning in a multi-turn setting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark:&lt;/strong&gt; This was a 2-turn evaluation. Stability in long conversations (10+ turns) is unproven.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm currently cleaning up the code and weights to open-source the entire project (will be under &amp;quot;AI Realist&amp;quot; if you want to search for it later).&lt;/p&gt; &lt;p&gt;Has anyone else experimented with cross-attention adapters for memory retrieval? I'm curious if injecting at the middle layers (e.g., block 12 of 24) would solve the signal dilution issue without destabilizing the frozen weights.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forsaken-Park8149"&gt; /u/Forsaken-Park8149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://msukhareva.substack.com/p/grafted-titans-i-built-a-plug-and"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hcsf/grafted_titans_a_plugandplay_neural_memory_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hcsf/grafted_titans_a_plugandplay_neural_memory_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T09:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4i7m2</id>
    <title>Apple CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning</title>
    <updated>2026-01-05T10:26:56+00:00</updated>
    <author>
      <name>/u/PlasticTourist6527</name>
      <uri>https://old.reddit.com/user/PlasticTourist6527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"&gt; &lt;img alt="Apple CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning" src="https://b.thumbs.redditmedia.com/lr9MpjbY9ZQcfQfKCIYFMregw58svjBmOTHq1XoZPtQ.jpg" title="Apple CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have not seen any discussion about this effort so I'm posting it here.&lt;br /&gt; But it looks like apple tried a new approach at RAG.&lt;br /&gt; Basically they took their own attempt at linguistic compression, it can shrink documents by &lt;strong&gt;32x to 64x&lt;/strong&gt; without losing the important details needed to answer a question.&lt;br /&gt; and the novel thing in my opinion is instead of having a separate retriever and a separate writer, it unifies them. It learns to find the right info and write the answer in one smooth process.&lt;/p&gt; &lt;p&gt;And ofcourse its fully open source.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8kt1oflgibg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a0ced30785edce71970f436406c9105af5b0229"&gt;https://preview.redd.it/l8kt1oflgibg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a0ced30785edce71970f436406c9105af5b0229&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; &lt;a href="https://github.com/apple/ml-clara"&gt;https://github.com/apple/ml-clara&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/apple/CLaRa_multi_stage"&gt;https://huggingface.co/datasets/apple/CLaRa_multi_stage&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/apple/CLaRa-7B-Instruct"&gt;https://huggingface.co/apple/CLaRa-7B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://arxiv.org/pdf/2511.18659"&gt;https://arxiv.org/pdf/2511.18659&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlasticTourist6527"&gt; /u/PlasticTourist6527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T10:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jf67</id>
    <title>TeleChat3-105B-A4.7B-Thinking and TeleChat3-36B-Thinking</title>
    <updated>2026-01-05T11:35:48+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"&gt; &lt;img alt="TeleChat3-105B-A4.7B-Thinking and TeleChat3-36B-Thinking" src="https://b.thumbs.redditmedia.com/diBmQ2hoP7eVo8Z7ziFcczmVLcSbccadB3HOJJfPYhA.jpg" title="TeleChat3-105B-A4.7B-Thinking and TeleChat3-36B-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o810skkwnibg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a3c8fa43b527dea185123cdf3cf7f80ee3e9ddcc"&gt;https://preview.redd.it/o810skkwnibg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a3c8fa43b527dea185123cdf3cf7f80ee3e9ddcc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Xingchen Semantic Large Model TeleChat3 is a large language model developed and trained by the China Telecom Artificial Intelligence Research Institute; this series of models was trained entirely using China computing resources.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Tele-AI/TeleChat3?tab=readme-ov-file"&gt;https://github.com/Tele-AI/TeleChat3?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/collections/TeleAI/TeleChat3"&gt;https://modelscope.cn/collections/TeleAI/TeleChat3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current doesn't have huggingface‚ò†Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T11:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4prue</id>
    <title>Wrote a deep dive on sandboxing for AI agents: containers vs gVisor vs microVMs vs Wasm, and when each makes sense</title>
    <updated>2026-01-05T16:09:32+00:00</updated>
    <author>
      <name>/u/BeowulfBR</name>
      <uri>https://old.reddit.com/user/BeowulfBR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, &lt;/p&gt; &lt;p&gt;I've been working on sandboxing for AI coding agents and kept running into the same confusion: people use &amp;quot;sandbox&amp;quot; to mean four completely different things with different security properties.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.luiscardoso.dev/blog/sandboxes-for-ai"&gt;So, I decided to write what I learned&lt;/a&gt;: the actual predicate differences between containers (shared kernel), gVisor (userspace kernel), microVMs (guest kernel + VMM), and Wasm (no syscall ABI) &lt;/p&gt; &lt;p&gt;The post covers why containers aren't sufficient for hostile code, what &amp;quot;policy leakage&amp;quot; looks like in agent systems and practical tradeoffs for different agent architectures.&lt;/p&gt; &lt;p&gt;I hope it can help people out there building AI applications. &lt;/p&gt; &lt;p&gt;Happy to discuss if you're building agent sandboxes or have run into edge cases I didn't cover&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeowulfBR"&gt; /u/BeowulfBR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4prue/wrote_a_deep_dive_on_sandboxing_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4prue/wrote_a_deep_dive_on_sandboxing_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4prue/wrote_a_deep_dive_on_sandboxing_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T16:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4i19c</id>
    <title>Benchmarking 23 LLMs on Nonogram (Logic Puzzle) Solving Performance</title>
    <updated>2026-01-05T10:16:23+00:00</updated>
    <author>
      <name>/u/mauricekleine</name>
      <uri>https://old.reddit.com/user/mauricekleine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i19c/benchmarking_23_llms_on_nonogram_logic_puzzle/"&gt; &lt;img alt="Benchmarking 23 LLMs on Nonogram (Logic Puzzle) Solving Performance" src="https://preview.redd.it/fdryj8qkaibg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8aa66881e7017590b4656228716be0bf22299ee" title="Benchmarking 23 LLMs on Nonogram (Logic Puzzle) Solving Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the Christmas holidays I went down a rabbit hole and built a benchmark to test how well large language models can solve nonograms (grid-based logic puzzles).&lt;/p&gt; &lt;p&gt;The benchmark evaluates 23 LLMs across increasing puzzle sizes (5x5, 10x10, 15x15).&lt;/p&gt; &lt;p&gt;A few interesting observations: - Performance drops sharply as puzzle size increases - Some models generate code to brute-force solutions - Others actually reason through the puzzle step-by-step, almost like a human - GPT-5.2 is currently dominating the leaderboard&lt;/p&gt; &lt;p&gt;Cost of curiosity: - ~$250 - ~17,000,000 tokens - zero regrets&lt;/p&gt; &lt;p&gt;Everything is fully open source and rerunnable when new models drop. Benchmark: &lt;a href="https://www.nonobench.com"&gt;https://www.nonobench.com&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/mauricekleine/nono-bench"&gt;https://github.com/mauricekleine/nono-bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I mostly built this out of curiosity, but I‚Äôm interested in what people here think: Are we actually measuring reasoning ability ‚Äî or just different problem-solving strategies?&lt;/p&gt; &lt;p&gt;Happy to answer questions or run specific models if people are interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mauricekleine"&gt; /u/mauricekleine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fdryj8qkaibg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i19c/benchmarking_23_llms_on_nonogram_logic_puzzle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i19c/benchmarking_23_llms_on_nonogram_logic_puzzle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T10:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4mxu0</id>
    <title>Upstage has finally posted benchmark results for Solar Open 100B</title>
    <updated>2026-01-05T14:22:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mxu0/upstage_has_finally_posted_benchmark_results_for/"&gt; &lt;img alt="Upstage has finally posted benchmark results for Solar Open 100B" src="https://b.thumbs.redditmedia.com/ydV66xzb188h1ELcBzHRydHMUSlO7VdO7H3iIUSexVg.jpg" title="Upstage has finally posted benchmark results for Solar Open 100B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/solar-open-technical-report.pdf"&gt;https://huggingface.co/upstage/Solar-Open-100B/blob/main/solar-open-technical-report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q4mxu0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mxu0/upstage_has_finally_posted_benchmark_results_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mxu0/upstage_has_finally_posted_benchmark_results_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T14:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4harh</id>
    <title>Introducing Falcon H1R 7B</title>
    <updated>2026-01-05T09:31:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4harh/introducing_falcon_h1r_7b/"&gt; &lt;img alt="Introducing Falcon H1R 7B" src="https://external-preview.redd.it/cp8sHrI0u-v727PXUjUREk9f3_bJczbhgY4L_llZyME.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e151ef7170642c032e8387d6514663a6cc0f364e" title="Introducing Falcon H1R 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This repository presents &lt;strong&gt;Falcon-H1R-7B&lt;/strong&gt;, a reasoning-specialized model built on top of &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Base"&gt;Falcon-H1-7B-Base&lt;/a&gt; and trained via cold-start supervised fine-tuning with long reasoning traces and further enhanced by scaling RL with GRPO. The model demonstrates outstanding performance across various benchmark evaluations, including mathematics, programming, instruction following, and general logic.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/tiiuae/falcon-h1r-7b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4harh/introducing_falcon_h1r_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4harh/introducing_falcon_h1r_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T09:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4xfkt</id>
    <title>ROCm running on a ROG Ally X handheld</title>
    <updated>2026-01-05T20:42:13+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4xfkt/rocm_running_on_a_rog_ally_x_handheld/"&gt; &lt;img alt="ROCm running on a ROG Ally X handheld" src="https://external-preview.redd.it/aXhkMmNlcGFlbGJnMd3k2aqwIjNTCgUDqX2GlYiDHPm0ORivpDWQVuZGFBj6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e4704aec14a95a9f6cb17d8d064688683cb637c" title="ROCm running on a ROG Ally X handheld" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We were so busy wondering if we could that we didn‚Äôt think about whether we should&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uqss3psaelbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4xfkt/rocm_running_on_a_rog_ally_x_handheld/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4xfkt/rocm_running_on_a_rog_ally_x_handheld/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T20:42:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4icio</id>
    <title>Bielik-11B-v3.0-Instruct</title>
    <updated>2026-01-05T10:34:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4icio/bielik11bv30instruct/"&gt; &lt;img alt="Bielik-11B-v3.0-Instruct" src="https://external-preview.redd.it/5cEj5o78oh6TbyHqprpd205PtMWxpwd8yMVStGNcCRo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22861a4ba482c1b57948a7a0af2d215159d5c0d3" title="Bielik-11B-v3.0-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bielik-11B-v3.0-Instruct is a generative text model featuring 11 billion parameters. It is an instruct fine-tuned version of the &lt;a href="https://huggingface.co/speakleash/Bielik-11B-v3-Base-20250730"&gt;Bielik-11B-v3-Base-20250730&lt;/a&gt;. Forementioned model stands as a testament to the unique collaboration between the open-science/open-source project SpeakLeash and the High Performance Computing (HPC) center: ACK Cyfronet AGH. &lt;/p&gt; &lt;p&gt;Developed and trained on multilingual text corpora across &lt;strong&gt;32 European languages&lt;/strong&gt;, with &lt;strong&gt;emphasis on Polish&lt;/strong&gt;, which has been cherry-picked and processed by the SpeakLeash team, this endeavor leverages Polish large-scale computing infrastructure, specifically within the PLGrid environment, and more precisely, the HPC centers: ACK Cyfronet AGH.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct-GGUF"&gt;https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/speakleash/bielik-papers/blob/main/v3/Bielik_11B_v3.pdf"&gt;https://github.com/speakleash/bielik-papers/blob/main/v3/Bielik_11B_v3.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4icio/bielik11bv30instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4icio/bielik11bv30instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T10:34:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4f0tm</id>
    <title>I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc.</title>
    <updated>2026-01-05T07:08:30+00:00</updated>
    <author>
      <name>/u/l33t-Mt</name>
      <uri>https://old.reddit.com/user/l33t-Mt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"&gt; &lt;img alt="I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc." src="https://external-preview.redd.it/aGJ3cmdlMXMyaGJnMfKIu2bgp1pENmKjPeusz-I2kkXf7vs8dV2V756jCzVD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67c688a8e1b37aaeac76f077a39bd2c01ad85859" title="I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You might remember me from LlamaCards a previous program ive built or maybe you've seen some of my agentic computer use posts with Moondream/Minicpm navigation creating reddit posts.&lt;/p&gt; &lt;p&gt;Ive had my head down and I've finally gotten something I wanted to show you all.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EmergentFlow&lt;/strong&gt; - a visual node-based editor for creating AI workflows and agents. The whole execution engine runs in your browser. Its a great sandbox for developing AI workflows.&lt;/p&gt; &lt;p&gt;You just open it and go. No Docker, no Python venv, no dependencies. Connect your Ollama(or other local) instance, paste your API keys for whatever providers you use, and start building. Everything runs client-side - your keys stay in your browser, your prompts go directly to the providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supported:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (just works - point it at localhost:11434, auto-fetches models)&lt;/li&gt; &lt;li&gt;LM Studio + llama.cpp (works once CORS is configured)&lt;/li&gt; &lt;li&gt;OpenAI, Anthropic, Groq, Gemini, DeepSeek, xAI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For edge cases where you hit CORS issues, there's an optional desktop runner that acts as a local proxy. It's open source: &lt;a href="http://github.com/l33tkr3w/EmergentFlow-runner"&gt;github.com/l33tkr3w/EmergentFlow-runner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But honestly most stuff works straight from the browser.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The deal:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It's free. Like, actually free - not &amp;quot;free trial&amp;quot; free. &lt;/p&gt; &lt;p&gt;You get a full sandbox with unlimited use of your own API keys. The only thing that costs credits is if you use my server-paid models (Gemini) because Google charges me for those.&lt;/p&gt; &lt;p&gt;Free tier gets 25 daily credits for server models(Gemini through my API key). &lt;/p&gt; &lt;p&gt;Running Ollama/LMStudio/llama.cpp or BYOK? &lt;strong&gt;Unlimited. Forever. No catch.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I do have a Pro tier ($19/mo) for power users who want more server credits and team collaboration, node/flow gallery - because I'm a solo dev with a kid trying to make this sustainable. But honestly most people here running local models won't need it. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; &lt;a href="https://emergentflow.io/try"&gt;emergentflow.io/try&lt;/a&gt; - no signup, no credit card, just start dragging nodes.&lt;/p&gt; &lt;p&gt;If you run into issues (there will be some), please submit a bug report. Happy to answer questions about how stuff works under the hood.&lt;/p&gt; &lt;p&gt;Support a fellow LocalLlama enthusiast! Updoot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/l33t-Mt"&gt; /u/l33t-Mt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ps5d841s2hbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T07:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4vz16</id>
    <title>Achieving 30x Real-Time Transcription on CPU . Multilingual STT Openai api endpoint compatible. Plug and play in Open-webui - Parakeet</title>
    <updated>2026-01-05T19:49:08+00:00</updated>
    <author>
      <name>/u/SlightPossibility331</name>
      <uri>https://old.reddit.com/user/SlightPossibility331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been a huge fan of Whisper Large V3 since it came out. it‚Äôs been my reliable workhorse for a long time. But recently, I found a new setup that has completely redefined what I thought was possible for local transcription, especially on a CPU.&lt;/p&gt; &lt;p&gt;I‚Äôm now achieving 30x real-time speeds on an i7-12700KF. To put that in perspective: it processes one minute of audio in just 2 seconds. Even on my older i7-4790, I‚Äôm still seeing a solid 17x real-time factor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes this special?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is powered by &lt;strong&gt;NVIDIA Parakeet TDT 0.6B V3, (in ONNX Format)&lt;/strong&gt; an incredible multilingual model that matches Whisper Large V3 accuracy - and honestly, I‚Äôve found its punctuation to be even better in some cases. It features robust multilingual capabilities with &lt;strong&gt;automatic language detection&lt;/strong&gt;. The model can automatically identify and transcribe speech in any of the &lt;strong&gt;25 supported languages&lt;/strong&gt; without requiring manual language specification:&lt;/p&gt; &lt;p&gt;Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Spanish, Swedish, Ukrainian&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to use it&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve built a frontend to help you capture and transcribe on the fly. However, you can also use the API endpoint to plug this directly into Open-WebUI or any project compatible with the OpenAI API.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/groxaxo/parakeet-tdt-0.6b-v3-fastapi-openai"&gt;&lt;strong&gt;https://github.com/groxaxo/parakeet-tdt-0.6b-v3-fastapi-openai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know what you think and feel free to contribute .I Will keep this project constantly updated so it becomes the new faster-whisper for CPU (Intel)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits &amp;amp; Gratitude&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This project stands on the shoulders of some amazing work:&lt;/p&gt; &lt;p&gt;NVIDIA: For developing the original Parakeet model.&lt;/p&gt; &lt;p&gt;The ONNX team: For the optimization tools that make this speed possible on standard hardware.&lt;/p&gt; &lt;p&gt;Shadowfita: For the excellent original English only FASTAPI Repo that laid the groundwork.&lt;/p&gt; &lt;p&gt;Groxaxo: For his incredible dedication and hard work in pushing this project forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlightPossibility331"&gt; /u/SlightPossibility331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T19:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4tken</id>
    <title>[Release] EchoChamber - Add AI-Generated Audience Reactions to Your SillyTavern Stories &amp; Conversations</title>
    <updated>2026-01-05T18:23:45+00:00</updated>
    <author>
      <name>/u/mattjb</name>
      <uri>https://old.reddit.com/user/mattjb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've released an extension that generates a dynamic AI-powered reaction feed alongside your SillyTavern conversations and stories. Think of it as adding a live audience to your stories and conversations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; EchoChamber creates real-time AI-generated commentary from virtual audiences as your story unfolds. Whether you want salty Discord chat roasting your plot choices, a viral Twitter feed dissecting every twist, or MST3K-style sarcastic commentary, the extension adapts to match. There are two NSFW avatars (female and male) that react filthily and explicitly, plus a bunch more to choose from (Dumb &amp;amp; Dumber, Thoughtful, HypeBot, Doomscrollers.)&lt;/p&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;10+ Built-in Chat Styles:&lt;/strong&gt; Discord/Twitch chat, Twitter/X threads, Breaking News tickers, Mystery Science Theater 3000, Thoughtful Analysis, Dumb &amp;amp; Dumber, Doomscrollers, HypeBot, and two NSFW advisors (Ava/Kai)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Backend:&lt;/strong&gt; Works with your existing Chat Completion API or runs separately using local models (Ollama, KoboldCPP, LM Studio, vLLM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quick Controls:&lt;/strong&gt; Toggle the feed on/off, switch chat styles, and adjust virtual user count with a convenient bar below your chat&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully Customizable:&lt;/strong&gt; Create your own chat styles by editing Markdown files. Import and share custom styles with the community&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theme Integration:&lt;/strong&gt; Automatically inherits your SillyTavern color scheme&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The extension analyzes your ongoing conversation/story and generates contextual reactions in real-time. The AI responds in character as different audience personas based on the selected chat style, creating an immersive layer of commentary that responds to plot developments, character decisions, and story beats.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; Standard SillyTavern extension process - copy and paste the GitHub URL below in the Extensions panel.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/mattjaybe/SillyTavern-EchoChamber"&gt;https://github.com/mattjaybe/SillyTavern-EchoChamber&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattjb"&gt; /u/mattjb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q4tken"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4tken/release_echochamber_add_aigenerated_audience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4tken/release_echochamber_add_aigenerated_audience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T18:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4mmiz</id>
    <title>Miromind_ai released Miro Thinker 1.5</title>
    <updated>2026-01-05T14:09:11+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mmiz/miromind_ai_released_miro_thinker_15/"&gt; &lt;img alt="Miromind_ai released Miro Thinker 1.5" src="https://preview.redd.it/8sefq240gjbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af4a29e216377f1704f3b3c5dc81609fdd17916e" title="Miromind_ai released Miro Thinker 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF Link: &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;https://huggingface.co/collections/miromind-ai/mirothinker-v15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Post-trained on top of qwen3 - Available in both 30A3B and 235A22B - Claimed to have great result on BrowserComp - Technical report coming soon - MiT license&lt;/p&gt; &lt;p&gt;Official demo: &lt;a href="https://dr.miromind.ai"&gt;https://dr.miromind.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8sefq240gjbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mmiz/miromind_ai_released_miro_thinker_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mmiz/miromind_ai_released_miro_thinker_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T14:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jnq0</id>
    <title>Falcon H1R 7B, a new reasoning model with 256k context window by the Technology Innovation Institute (TII) in Abu Dhabi</title>
    <updated>2026-01-05T11:48:59+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jnq0/falcon_h1r_7b_a_new_reasoning_model_with_256k/"&gt; &lt;img alt="Falcon H1R 7B, a new reasoning model with 256k context window by the Technology Innovation Institute (TII) in Abu Dhabi" src="https://preview.redd.it/khf18ffgqibg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c79a97cb99a35abb0788f044e1494984c94cd524" title="Falcon H1R 7B, a new reasoning model with 256k context window by the Technology Innovation Institute (TII) in Abu Dhabi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B&lt;/a&gt;&lt;br /&gt; Blog post: &lt;a href="https://huggingface.co/blog/tiiuae/falcon-h1r-7b"&gt;https://huggingface.co/blog/tiiuae/falcon-h1r-7b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/khf18ffgqibg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jnq0/falcon_h1r_7b_a_new_reasoning_model_with_256k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jnq0/falcon_h1r_7b_a_new_reasoning_model_with_256k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T11:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jc99</id>
    <title>What do we think about Gorgon Point (Ryzen AI 9 HX 470)?</title>
    <updated>2026-01-05T11:31:03+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/"&gt; &lt;img alt="What do we think about Gorgon Point (Ryzen AI 9 HX 470)?" src="https://preview.redd.it/6lfowdxxnibg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be0f7d010f4bb84aa8472d0b245abd51e0e8c43b" title="What do we think about Gorgon Point (Ryzen AI 9 HX 470)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new APU is promised to support DDR5-6400 (102.4 GB/s) and LPDDR5X-8533 (136.5 GB/s) which should move some models that were barely usable on Strix Point to the usable territory.&lt;/p&gt; &lt;p&gt;However, it really seems that to utilise these capabilities, manufacturers would have to get chips that are basically inaccessible right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6lfowdxxnibg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T11:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4m6k0</id>
    <title>The Major Release of MiroMind‚Äôs Flagship Search Agent Model, MiroThinker 1.5.</title>
    <updated>2026-01-05T13:50:38+00:00</updated>
    <author>
      <name>/u/wuqiao</name>
      <uri>https://old.reddit.com/user/wuqiao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4m6k0/the_major_release_of_mirominds_flagship_search/"&gt; &lt;img alt="The Major Release of MiroMind‚Äôs Flagship Search Agent Model, MiroThinker 1.5." src="https://external-preview.redd.it/cH2lE5iC3U5CuznHdVEsQrxsFQW9rX4gLlOCeNsa0eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c8e5a6d9fb506d45e380a2b69000398cdfa1e84" title="The Major Release of MiroMind‚Äôs Flagship Search Agent Model, MiroThinker 1.5." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have officially released our self-developed flagship search-based agent model, MiroThinker 1.5.This release delivers significant performance improvements and explores as well as implements predictive use cases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started now:&lt;/strong&gt; &lt;a href="https://dr.miromind.ai/"&gt;&lt;strong&gt;https://dr.miromind.ai/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Leading Performance:&lt;/strong&gt; MiroThinker 1.5 (235B) surpasses ChatGPT-Agent in BrowseComp, ranking among the world's top tier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extreme Efficiency:&lt;/strong&gt; MiroThinker 1.5 (30B) costs only 1/20 of Kimi-K2, delivering faster inference and higher intelligence-to-cost ratio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predict the Future:&lt;/strong&gt; Proprietary ‚ÄúInteractive Scaling‚Äù and ‚ÄúTemporal-Sensitive Training‚Äù enable forward-looking analysis of how macro events trigger chain reactions across the Nasdaq.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully Open-Source:&lt;/strong&gt; Model and code are fully open, immediately unlocking discovery-driven intelligence for free.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Sample Showcase&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Case 1: What major events next week could affect the U.S. Nasdaq Index, and how might each of them impact it?&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea"&gt;https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Case 2: Which film is most likely to receive a Best Picture nomination at the 2026 Oscars?&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22"&gt;https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Case 3: Which team is most likely to make it to the Super Bowl in 2026?&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db"&gt;https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub &lt;strong&gt;:&lt;/strong&gt; &lt;a href="https://github.com/MiroMindAI/MiroThinker"&gt;https://github.com/MiroMindAI/MiroThinker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Discord: &lt;a href="https://discord.gg/F7EQFnYscV"&gt;https://discord.gg/F7EQFnYscV&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;Ôºö&lt;a href="https://github.com/MiroMindAI/MiroThinker/discussions/64"&gt;https://github.com/MiroMindAI/MiroThinker/discussions/64&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuqiao"&gt; /u/wuqiao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4m6k0/the_major_release_of_mirominds_flagship_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4m6k0/the_major_release_of_mirominds_flagship_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T13:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4x5e9</id>
    <title>For the first time in 5 years, Nvidia will not announce any new GPUs at CES ‚Äî company quashes RTX 50 Super rumors as AI expected to take center stage</title>
    <updated>2026-01-05T20:31:51+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/"&gt; &lt;img alt="For the first time in 5 years, Nvidia will not announce any new GPUs at CES ‚Äî company quashes RTX 50 Super rumors as AI expected to take center stage" src="https://external-preview.redd.it/co15yfRaj9eX-MR7sOLzYRAR6dcajD3En5Canm81iC0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79072b51c96dcd261d563abdade03dc37cac0511" title="For the first time in 5 years, Nvidia will not announce any new GPUs at CES ‚Äî company quashes RTX 50 Super rumors as AI expected to take center stage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welp, in case anyone had any hopes.&lt;/p&gt; &lt;p&gt;No RTX 50 Super cards, very limited supply of the 5070Ti, 5080, and 5090, and now rumors that Nvidia will bring back the 3060 to prop demand.&lt;/p&gt; &lt;p&gt;Meanwhile &lt;a href="https://www.tomshardware.com/pc-components/ram/newegg-bundles-usd1-460-128gb-ddr5-ram-kit-with-usd50-starbucks-gift-card-drink-coffee-while-you-game-retailer-says-as-memory-hits-rtx-5080-pricing"&gt;DDR5 prices continue to climb, with 128GB kits now costing $1460&lt;/a&gt;. Storage prices have also gone through the roof.&lt;/p&gt; &lt;p&gt;I'm very lucky to have more than enough hardware for all my LLM and homelab needs but at the same time, I don't see any path forward if I want to upgrade in the next 3 years, and hope my gear continues to run without any major issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/for-the-first-time-in-5-years-nvidia-will-not-announce-any-new-gpus-at-ces-company-quashes-rtx-50-super-rumors-as-ai-expected-to-take-center-stage"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T20:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4s8t3</id>
    <title>llama.cpp performance breakthrough for multi-GPU setups</title>
    <updated>2026-01-05T17:37:58+00:00</updated>
    <author>
      <name>/u/Holiday-Injury-9397</name>
      <uri>https://old.reddit.com/user/Holiday-Injury-9397</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/"&gt; &lt;img alt="llama.cpp performance breakthrough for multi-GPU setups" src="https://preview.redd.it/ohxtu0l8hkbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7eb4f66bc390eed56e0c3715fc2510ee8e1fa305" title="llama.cpp performance breakthrough for multi-GPU setups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While we were enjoying our well-deserved end-of-year break, the &lt;strong&gt;ik_llama.cpp&lt;/strong&gt; project (a performance-optimized fork of llama.cpp) achieved a breakthrough in local LLM inference for multi-GPU configurations, delivering a massive performance leap ‚Äî not just a marginal gain, but a 3x to 4x speed improvement.&lt;br /&gt; While it was already possible to use multiple GPUs to run local models, previous methods either only served to pool available VRAM or offered limited performance scaling. However, the ik_llama.cpp team has introduced a new execution mode (split mode graph) that enables the simultaneous and maximum utilization of multiple GPUs.&lt;br /&gt; Why is it so important? With GPU and memory prices at an all-time high, this is a game-changer. We no longer need overpriced high-end enterprise cards; instead, we can harness the collective power of multiple low-cost GPUs in our homelabs, server rooms, or the cloud.&lt;/p&gt; &lt;p&gt;&lt;em&gt;If you are interested, details are&lt;/em&gt; &lt;a href="https://medium.com/@jagusztinl/04c83a66feb2?sk=bad7534bdad1e771a9f61c76c8b0df50"&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday-Injury-9397"&gt; /u/Holiday-Injury-9397 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ohxtu0l8hkbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T17:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
