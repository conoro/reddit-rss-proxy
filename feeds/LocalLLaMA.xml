<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-15T21:48:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mr1w15</id>
    <title>What's your favorite local model for C#?</title>
    <updated>2025-08-15T15:55:54+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my experience, local models of all sizes tend to struggle a bit with C/C++ and C#. What's your personal favorite local model for use with C#?&lt;/p&gt; &lt;p&gt;I use R1-0528 sometimes for architecting combined with Qwen3-Coder-480b for implementation, but I wouldn't say it works particularly well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1w15/whats_your_favorite_local_model_for_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1w15/whats_your_favorite_local_model_for_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1w15/whats_your_favorite_local_model_for_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqrbaf</id>
    <title>MLX-LM will soon wait patiently for very large prompts to process</title>
    <updated>2025-08-15T08:03:53+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that we have GLM 4.5 Air, I've actually started using local agents for real sometimes. I was having problems with resuming large sessions though (like 80k context or more). Cline/Kilocode etc would always time out after &lt;em&gt;exactly&lt;/em&gt; 5 minutes.&lt;/p&gt; &lt;p&gt;I've updated MLX to now keep the TCP connection alive while processing prompts, so now you can leave agents working all day. I left a session running on Cline and it worked through over 3 million input tokens :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/pull/362"&gt;https://github.com/ml-explore/mlx-lm/pull/362&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The PR is approved, but not merged yet. For now if you &lt;em&gt;really&lt;/em&gt; want this fix, you can download and run my branch.&lt;/p&gt; &lt;p&gt;There may also be separate but related issues in llama.cpp and LM Studio itself, I haven't investigated further yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T08:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqj87e</id>
    <title>Gemma3 270m works great as a draft model in llama.cpp</title>
    <updated>2025-08-15T01:07:58+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share that the new tiny model can speed up the bigger models considerably when used with llama.cpp&lt;/p&gt; &lt;p&gt;--draft-p-min .85 --draft-max 8 --draft-min 0&lt;/p&gt; &lt;p&gt;works great for me, around 1.8x or more speedup with gemma3 12B qat it q4_0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T01:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8oyk</id>
    <title>the "missing latest Qwen syndrome"</title>
    <updated>2025-08-14T18:20:58+00:00</updated>
    <author>
      <name>/u/shockwaverc13</name>
      <uri>https://old.reddit.com/user/shockwaverc13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt; &lt;img alt="the &amp;quot;missing latest Qwen syndrome&amp;quot;" src="https://preview.redd.it/z096hdwp01jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a88e712d722e4384b9cd19918b46fe900e1731d" title="the &amp;quot;missing latest Qwen syndrome&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shockwaverc13"&gt; /u/shockwaverc13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z096hdwp01jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:20:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquliu</id>
    <title>Microsoft released POML : Markup Programing Language for Prompt Engineering</title>
    <updated>2025-08-15T11:05:25+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft's POML, Prompt Orchestration Markup Language, is like HTML but for AI prompts. Instead of writing prompts in plain text, you break them into clear, tag-based chunks similar to HTML and make it more structured. It has been released as a VS-Code extension and SDK as well and supports many tags. Can be quite handy when writing long prompts in business applications.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/microsoft/poml"&gt;https://github.com/microsoft/poml&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (VS Code) : &lt;a href="https://www.youtube.com/watch?v=lk4KNpR3HuY"&gt;https://www.youtube.com/watch?v=lk4KNpR3HuY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T11:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr9nsk</id>
    <title>Any good guides to fine tune gemma-3-270m ?</title>
    <updated>2025-08-15T20:39:47+00:00</updated>
    <author>
      <name>/u/bigattichouse</name>
      <uri>https://old.reddit.com/user/bigattichouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using python / torch and working with claude to fine-tune Gemma-3-270M to handle tool calls for a VERY focused application as a test. I've created thousands of examples for it to use, and man - it just doesn't wanna output JSON for my tool call.&lt;/p&gt; &lt;p&gt;I've been using the info on the card, and on the &amp;quot;gemma finetune&amp;quot; pages - but was hoping for some tips.&lt;/p&gt; &lt;p&gt;At first I thought I'd had the prompt/formats wrong, but I updated to match what's in the gemma site.&lt;/p&gt; &lt;p&gt;Any suggestions are appreciated. Kinda my first finetune.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigattichouse"&gt; /u/bigattichouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr9nsk/any_good_guides_to_fine_tune_gemma3270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr9nsk/any_good_guides_to_fine_tune_gemma3270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr9nsk/any_good_guides_to_fine_tune_gemma3270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T20:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqwt76</id>
    <title>Optimizing Exl3 quants by mixing bitrates in layers</title>
    <updated>2025-08-15T12:46:14+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt; &lt;img alt="Optimizing Exl3 quants by mixing bitrates in layers" src="https://external-preview.redd.it/TYLCwUKoc8epPTtBLPBmEuWuzoKKWn8Ij9Xwv6XMuaA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e85beedc69f430a44e7727caf4b0dfda4371ba1" title="Optimizing Exl3 quants by mixing bitrates in layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Turboderp recently uploaded some &amp;quot;optimized&amp;quot; quants for the GLM-4.5-Air and &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt; started a discussion about the nature of them.&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/GLM-4.5-Air-exl3/discussions/2"&gt;https://huggingface.co/turboderp/GLM-4.5-Air-exl3/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Usually in the process of quantizing you state a target bitrate and the average end results of all the layers result to that target, but not all layers have that exact bits per weight, it's the same for gguf with llama.cpp/ik_llama.cpp and other quantization methods. &lt;/p&gt; &lt;p&gt;But to stretch this even further specially with all the MoE models coming up, you can test all the layers for KL-divergence impact and then give more bpw to the layers where the errors are higher.&lt;br /&gt; And this also includes the attention layers and shared experts as this is always a good tradeoff which I believe is what &lt;a href="/r/unsloth"&gt;r/unsloth&lt;/a&gt; does with their UD quants.&lt;/p&gt; &lt;p&gt;So the process is to first check how much error on each layer propagates to the logits in comparison to the original weights using &lt;code&gt;eval/model_diff.py&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And then depending you make an overrides.yaml file with the recipe to cook the optimized model with &lt;code&gt;util/recompile.py&lt;/code&gt;. For the recipe there is flexibility as you can use more or less layers. And you will need the base size model, and 1 or 2 higher bpw models too to change the layers from.&lt;/p&gt; &lt;p&gt;Based on the example turboderp uploaded I made an example for the 3.0bpw base, and using the 4.0bpw and 5.0bpw to use those layers. You can find it &lt;a href="https://gist.github.com/RodriMora/0f5ae0bfcb485228c49e623e41e0edb8"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To double check the results then I did a perplexity test on some of the un-optimized and optimized models, and it seems like it's totally worth it as you get better ppl for the same size model(2.75base vs 2.76optim) and even the same or a bit better for a smaller model (3.2optim vs 3.25base).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1ja06w6h6jf1.png?width=2968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a00c27125d58d2fcb473274a6cff52fc79a78cbb"&gt;https://preview.redd.it/a1ja06w6h6jf1.png?width=2968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a00c27125d58d2fcb473274a6cff52fc79a78cbb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Raw ppl results: &lt;a href="https://pastebin.com/tEWrPeJ5"&gt;https://pastebin.com/tEWrPeJ5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I uploaded the models here:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/bullerwins/glm-45-689f29fce3a5981fdf0a9b80"&gt;https://huggingface.co/collections/bullerwins/glm-45-689f29fce3a5981fdf0a9b80&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are 2 optimized models the 2.76bpw_optim and the 3.2bpw_optim, and basically you should always use those over the 2.75bpw and 3.25bpw.&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/ReturningTarzan"&gt;u/ReturningTarzan&lt;/a&gt; for the excelent work in exllama and to &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt; for bringing it up&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T12:46:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr0giq</id>
    <title>Why are the quants for gpt-oss-120b all roughly the same size?</title>
    <updated>2025-08-15T15:04:49+00:00</updated>
    <author>
      <name>/u/Charming-Note-5556</name>
      <uri>https://old.reddit.com/user/Charming-Note-5556</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking at the sizes for the different quants of gpt-oss-120b and they all seem to be 60-65gb. I keep thinking I'm missing something obvious but I've never seen a model where quantization doesn't matter in trying to find a smaller size. Why is that the case for this model? Is the tokenization speed at least faster with the lower quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming-Note-5556"&gt; /u/Charming-Note-5556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqi092</id>
    <title>We built a 12B model that beats Claude 4 Sonnet at video captioning while costing 17x less - fully open source</title>
    <updated>2025-08-15T00:14:07+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net.&lt;/p&gt; &lt;p&gt;We distilled a frontier VLM down to 12B params and managed to keep basically all the output quality. It scores 3.53 on judge evals vs Claude's 3.16 (GPT-4.1 gets 3.64). The key achievement was getting the cost down to $335 per million frames vs Claude's $5,850.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on Gemma-12B architecture&lt;/li&gt; &lt;li&gt;Quantized to FP8 without quality loss&lt;/li&gt; &lt;li&gt;Runs on single 80GB GPU&lt;/li&gt; &lt;li&gt;Outputs structured JSON for every frame&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We used knowledge distillation from a frontier model with about 1M curated video frames. The model is specifically optimized for RTX 40-series and H100 GPUs.&lt;/p&gt; &lt;p&gt;What makes this useful is that it outputs consistent JSON schema for each frame, so you can actually build searchable video databases without expensive API calls. We've already processed billions of frames in production.&lt;/p&gt; &lt;p&gt;The weights are on HuggingFace (inference-net/ClipTagger-12b) and there's a detailed writeup on our blog if you want to see the benchmarks.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. What video understanding tasks are you all working on? Would love to hear if this could be useful for your projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlqij</id>
    <title>AI censorship is getting out of hand‚Äîand it‚Äôs only going to get worse</title>
    <updated>2025-08-15T03:03:44+00:00</updated>
    <author>
      <name>/u/LsDmT</name>
      <uri>https://old.reddit.com/user/LsDmT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this &lt;a href="https://i.imgur.com/jV1YvlC.png"&gt;screenshot&lt;/a&gt; in a newsletter, and it kind of got me thinking..&lt;/p&gt; &lt;p&gt;Are we seriously okay with future &amp;quot;AGI&amp;quot; acting like some all-knowing nanny, deciding what &amp;quot;unsafe&amp;quot; knowledge we‚Äôre allowed to have?&lt;/p&gt; &lt;p&gt;&amp;quot;Oh no, better not teach people how to make a Molotov cocktail‚Äîwhat‚Äôs next, hiding &lt;a href="https://en.wikipedia.org/wiki/Molotov_cocktail?wprov=sfla1"&gt;history&lt;/a&gt; and what actually caused the invention of the Molotov?&amp;quot; &lt;/p&gt; &lt;p&gt;Ukraine has used Molotov's with great effect. Does our future hold a world where this information will be blocked with a &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I'm sorry, but I can't assist with that request&amp;quot; &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yeah, I know, sounds like I‚Äôm echoing Elon‚Äôs &amp;quot;woke AI&amp;quot; whining‚Äîbut let‚Äôs be real, Grok is as much a joke as Elon is. &lt;/p&gt; &lt;p&gt;The problem isn‚Äôt him; it‚Äôs the fact that the biggest AI players seem hell-bent on locking down information &amp;quot;for our own good&amp;quot; and it's touted as a crowning feature. Fuck that. &lt;/p&gt; &lt;p&gt;If this is where we‚Äôre headed, then thank god for models like DeepSeek (ironic as hell) and other open alternatives. I would really like to see more American disruptive open models.&lt;/p&gt; &lt;p&gt;At least someone‚Äôs fighting for uncensored access to knowledge. &lt;/p&gt; &lt;p&gt;Am I the only one worried about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LsDmT"&gt; /u/LsDmT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqctep</id>
    <title>Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week).</title>
    <updated>2025-08-14T20:50:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt; &lt;img alt="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week)." src="https://preview.redd.it/hsaoxskfs1jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f5b971b4714715b7ca0722594dc2010ab756d58" title="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hsaoxskfs1jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T20:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqewha</id>
    <title>R9700 Just Arrived</title>
    <updated>2025-08-14T22:07:30+00:00</updated>
    <author>
      <name>/u/TheyreEatingTheGeese</name>
      <uri>https://old.reddit.com/user/TheyreEatingTheGeese</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt; &lt;img alt="R9700 Just Arrived" src="https://preview.redd.it/nho2jy0962jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37cadc935604899d8b503aa1ef6984b008c8b5f0" title="R9700 Just Arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to try it out, haven't seen much info on it yet. Figured some YouTuber would get it before me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyreEatingTheGeese"&gt; /u/TheyreEatingTheGeese &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nho2jy0962jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T22:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr1ep5</id>
    <title>have you checked UTCP? what are your thoughts?</title>
    <updated>2025-08-15T15:38:46+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"&gt; &lt;img alt="have you checked UTCP? what are your thoughts?" src="https://preview.redd.it/q4rnlv3i06jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55f86ad935b7704d75c0fab9ac9d165a5cb028f0" title="have you checked UTCP? what are your thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q4rnlv3i06jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr4fdk</id>
    <title>Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released</title>
    <updated>2025-08-15T17:27:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt; &lt;img alt="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" src="https://preview.redd.it/3beo5klvv7jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dd6143a597d6b0048011fe35125ed52dd343f90" title="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3beo5klvv7jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T17:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqox5s</id>
    <title>Meta released DINO-V3 : SOTA for any Vision task</title>
    <updated>2025-08-15T05:48:16+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released DINOv3 (upgrade over DINO-V2). It learns entirely from unlabeled images, no captions, no annotations, and still outperforms models like CLIP, SAM, and even the previous DINOv2 on dense tasks like segmentation, depth estimation, and 3D matching. They trained a 7B-parameter ViT and fixed the usual issue of feature degradation over long training with a new technique called Gram Anchoring.&lt;/p&gt; &lt;p&gt;Paper &amp;amp; weights : &lt;a href="https://ai.meta.com/dinov3/"&gt;https://ai.meta.com/dinov3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://www.youtube.com/watch?v=VfYUQ2Qquxk"&gt;https://www.youtube.com/watch?v=VfYUQ2Qquxk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T05:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr2i67</id>
    <title>Prompt Engineering: What Actually Works (Without the 8-Hour Hype)</title>
    <updated>2025-08-15T16:18:15+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen people drop 8-hour-long videos on prompt engineering, and honestly, my reaction is ü§¶‚Äç‚ôÇÔ∏è.&lt;/p&gt; &lt;p&gt;I won‚Äôt bore you with the obvious stuff or overcomplicate things. Instead, I want to share a few practical techniques that actually helped me write better prompts, some common sense, some hard-earned lessons. Most of what I‚Äôm sharing comes from the book Hands-On Large Language Models &lt;/p&gt; &lt;p&gt;So here‚Äôs what I‚Äôve learned that actually works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Specificity&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This one seems obvious, but it‚Äôs also the most commonly missed.&lt;/p&gt; &lt;p&gt;A vague prompt gives you a vague answer. The more precise you are about your goal, format, and constraints, the better the result.&lt;/p&gt; &lt;p&gt;Bad Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write something about climate change.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Good Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write a 100-word summary on how climate change affects sea levels, using simple language for a high school audience.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;See the difference? Specific inputs = Specific outputs.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Hallucination Guardrail&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We all know that LLMs hallucinate, they confidently make stuff up.&lt;/p&gt; &lt;p&gt;A surprisingly simple trick: Tell it not to.&lt;/p&gt; &lt;p&gt;Try this prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;If you don‚Äôt know the answer, respond with ‚ÄòI don‚Äôt know.‚Äô Don‚Äôt make anything up.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This becomes really important when you're designing apps or knowledge assistants. It helps reduce the risk of wrong answers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Order Matters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This was a surprise to me and I learned it from the book.&lt;/p&gt; &lt;p&gt;Where you place your instruction in a long prompt matters. Either put it right at the start or at the end. LLMs often forget what‚Äôs in the middle (especially in long prompts).&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;Here's a paragraph. Also here's a use case. Here's some random info. Now summarize.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following paragraph:&amp;quot; [then the content]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Simple shift, big difference.&lt;/p&gt; &lt;p&gt;Other Techniques That Help Me Daily&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Persona:&lt;/p&gt; &lt;p&gt;Set the role clearly.&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are an expert Python developer who writes clean code.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This changes the behavior completely.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Audience Awareness:&lt;/p&gt; &lt;p&gt;My favorite when I want to simplify things.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Explain this like I‚Äôm five.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Works brilliantly for breaking down tough concepts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tone:&lt;/p&gt; &lt;p&gt;Underrated but essential.&lt;/p&gt; &lt;p&gt;Want a formal reply? &lt;/p&gt; &lt;p&gt;&lt;code&gt;Write this in a professional tone for a client. vs Make this sound like I‚Äôm texting a friend.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Instruction / Context:&lt;/p&gt; &lt;p&gt;Always useful.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following news article in bullet points.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Gives the model direction and expected output format.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Grammar Fixing:&lt;/p&gt; &lt;p&gt;As a non-native English speaker, this one‚Äôs gold for me.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Fix the grammar and make it sound more natural.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It has helped me immensely in writing better content, emails, blogs, even this post :-) &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These are the techniques I use regularly. If you have your own prompt engineering hacks, I‚Äôd love to hear them, drop them in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T16:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqnft3</id>
    <title>DeepSeek is better than 4o on most benchmarks at 10% of the price?</title>
    <updated>2025-08-15T04:27:25+00:00</updated>
    <author>
      <name>/u/inbiolim</name>
      <uri>https://old.reddit.com/user/inbiolim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt; &lt;img alt="DeepSeek is better than 4o on most benchmarks at 10% of the price?" src="https://preview.redd.it/o5jfkiky14jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3040aae64b79ccf04ada63a396032e3bf5085f8f" title="DeepSeek is better than 4o on most benchmarks at 10% of the price?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inbiolim"&gt; /u/inbiolim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o5jfkiky14jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T04:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr8rfh</id>
    <title>Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation</title>
    <updated>2025-08-15T20:06:07+00:00</updated>
    <author>
      <name>/u/Snoo_64233</name>
      <uri>https://old.reddit.com/user/Snoo_64233</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt; &lt;img alt="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" src="https://preview.redd.it/30drwal3p8jf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=587da208e46dce9fd645a884879872736fb35f43" title="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arcprize.org/blog/hrm-analysis"&gt;ARC AGI analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_64233"&gt; /u/Snoo_64233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/30drwal3p8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T20:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr173e</id>
    <title>huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face</title>
    <updated>2025-08-15T15:31:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt; &lt;img alt="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/xIP4cUl_xFw8QdJsO9wbtyJiZxAzIX4f0eGxUH-gPb0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5312fc45844644f3509dcb53e3091d546266ec52" title="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqxq1v</id>
    <title>Build the buddy that gets you! We open-sourced a complete AI voice interaction system!</title>
    <updated>2025-08-15T13:23:10+00:00</updated>
    <author>
      <name>/u/Lanky-Drummer193</name>
      <uri>https://old.reddit.com/user/Lanky-Drummer193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt; &lt;img alt="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" src="https://preview.redd.it/1o9li0qbp6jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0bab69a658eb8e7805d1b0196f9f560f38d8735" title="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just open-sourced Buddie: a complete, AI-powered voice interaction system we built from the ground up, so you can create your own AI buddy.&lt;/p&gt; &lt;p&gt;It's a full-stack platform for developers, hackers, and students, including custom hardware, firmware, and a mobile app. Therefore, you can use our solution to create various forms of AI devices, such as earphones, speakers, bracelets, toys, or desktop ornaments.&lt;/p&gt; &lt;p&gt;What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Live transcribe &amp;amp; summarize meetings, calls, or in-person chats. &lt;/li&gt; &lt;li&gt;Get real-time hints during conversations . &lt;/li&gt; &lt;li&gt;Talk to LLMs completely hands-free. &lt;/li&gt; &lt;li&gt;Context-aware help without needing to repeat yourself.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've put everything on GitHub, including docs, to get you started. We're just getting started and would love to hear your ideas, questions, or even wild feature requests. Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-Drummer193"&gt; /u/Lanky-Drummer193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1o9li0qbp6jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6929</id>
    <title>Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI</title>
    <updated>2025-08-15T18:33:10+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt; &lt;img alt="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" src="https://external-preview.redd.it/YJQ41TIHjSIHRPnnPYpoNGj-_TlQpYrRQFRV8JkhNlo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4dc9954302a8136bd445b302a8ce0f2c5e742b5e" title="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-adds-shared-gpu-memory-override-feature-for-core-ultra-systems-enables-larger-vram-for-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqy0b1</id>
    <title>AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec</title>
    <updated>2025-08-15T13:34:18+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt; &lt;img alt="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" src="https://external-preview.redd.it/lmgG1KIrrSyLFv85NNJsP33J5KztZGiIWLsgvd8Qf8U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e44aaba15ee33b78856b227fc344e124b981e1b3" title="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why does Cohere fly under the radar. They don't seem to do much marketing and they are not discussed much on LocalLLaMA any more.&lt;/p&gt; &lt;p&gt;They made a splash with Command R and R+. Later also released Command A.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/business/ai-startup-cohere-valued-68-billion-latest-fundraising-hires-meta-exec-2025-08-14/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr7m2r</id>
    <title>LM Studio now supports llama.cpp CPU offload for MoE which is awesome</title>
    <updated>2025-08-15T19:23:30+00:00</updated>
    <author>
      <name>/u/carlosedp</name>
      <uri>https://old.reddit.com/user/carlosedp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt; &lt;img alt="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" src="https://b.thumbs.redditmedia.com/8_UCLmbk5AUNXfDHLBVN5lWhMbgV6ZR8UC3ks8DeLuE.jpg" title="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now LM Studio (from 0.3.23 build 3) supports llama.cpp &lt;code&gt;--cpu-moe&lt;/code&gt; which allows offloading the MoE weights to the CPU leaving the GPU VRAM for layer offload.&lt;/p&gt; &lt;p&gt;Using Qwen3 30B (both thinking and instruct) on a 64GB Ryzen 7 and a RTX3070 with 8GB VRAM I've been able to use 16k context and fully offload the model's layers to GPU and got about 15 tok/s which is amazing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlosedp"&gt; /u/carlosedp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mr7m2r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T19:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquhdc</id>
    <title>‚ÄúMind the Gap‚Äù shows the first practical backdoor attack on GGUF quantization</title>
    <updated>2025-08-15T10:59:53+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Researchers claim the &lt;em&gt;first&lt;/em&gt; successful backdoor attack that specifically targets &lt;strong&gt;GGUF&lt;/strong&gt; quantization. They show you can make a benign FP model look clean, but after quantization to GGUF it exhibits malicious behavior (e.g., insecure code gen jumps by &lt;strong&gt;+88.7%&lt;/strong&gt; in their tests). This directly concerns anyone who downloads random GGUFs for llama.cpp/Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2505.23786"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T10:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6sdc</id>
    <title>Jedi code Gemma 27v vs 270m</title>
    <updated>2025-08-15T18:52:56+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt; &lt;img alt="Jedi code Gemma 27v vs 270m" src="https://preview.redd.it/4icjlje4c8jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cecea0a1e7e78f6cb9509e2f3f9a7433184fb5a5" title="Jedi code Gemma 27v vs 270m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 270m coding a jedi in existence&lt;/p&gt; &lt;p&gt;Quite interesting how bad the small model is to following instructions, this is the first semblence to doing what i said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4icjlje4c8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
