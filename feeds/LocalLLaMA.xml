<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-23T23:56:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rc3naj</id>
    <title>Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao.</title>
    <updated>2026-02-23T01:13:04+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt; &lt;img alt="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." src="https://external-preview.redd.it/MmJ6MGRjNjA4NWxnMR3Al36Nr886FX7jQ_P96fNg8PSf4Zsku92kjG2XN_qv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8910573e373960eea6962553218ddcd88a9324c" title="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, I was bored so I spent the last two weeks experimenting with vibecoding with local LLMs, namely gpt-oss-120b.&lt;/p&gt; &lt;p&gt;I started with Cline, didn't like it at all because it was overheating my GPU while giving back too little. Codex was even worse, locally, leading to weird CPU switches mid-generation when there was supposed to be enough VRAM to run the model entirely on GPU. Then I tried Claude Code and that's when my expectations were exceeded, &lt;em&gt;big time.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I first started with pygame, and after successfully one-shotting simple games (snake game, etc.) under the same project with the same model I decided to take it another level and use Claude Code with Godot, which was pretty easy to setup in VSCode and their IDE/extension. &lt;/p&gt; &lt;p&gt;Next thing I know, I spend the last two weeks making this game on Godot out of curiosity and using Claude Code to help me Vibecode parts of it along the way, and I came up with this game where you have a useful, snarky NPC that makes fun of you lmao.&lt;/p&gt; &lt;p&gt;The way it works is that the game is going to be gathering contextual information in real-time, e.g. actions taken, events occurring, etc. You can see that in the logs that are printed under the gameplay loop. &lt;/p&gt; &lt;p&gt;The mage then stores each chain of events in a chat history and comments on it every 10 seconds. The AI behavior is hard-coded but it works really well. However, I do plan on adding a hybrid approach where the LLM uses tool calls to make informed decisions depending on the situations, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switching equipment&lt;/li&gt; &lt;li&gt;Healing the player or himself&lt;/li&gt; &lt;li&gt;Pointing out objects of interest&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so forth. I haven't ruled out a Wizard of Oz worldbuilding AI that vibecodes enemies and obstacles throughout the game with tool calls, but that will be for another time.&lt;/p&gt; &lt;p&gt;I'm enjoying this process so I think I might actually finish this game, but we'll see how far I can get. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jl31wp5085lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T01:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcy5wv</id>
    <title>Qwen 3 coder next ud-q8-xl F16 filling up the two orin rpc mesh!</title>
    <updated>2026-02-23T23:47:24+00:00</updated>
    <author>
      <name>/u/braydon125</name>
      <uri>https://old.reddit.com/user/braydon125</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/"&gt; &lt;img alt="Qwen 3 coder next ud-q8-xl F16 filling up the two orin rpc mesh!" src="https://external-preview.redd.it/aWVhZHNnaXl6YmxnMWrUBUxyMMPidJm6SSrNb-W9WQcIAZj84NetEddKYJ3y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=676552353f7d7812a39c29ee412cf53fe6f31836" title="Qwen 3 coder next ud-q8-xl F16 filling up the two orin rpc mesh!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;running great and as you can see here llama.cpp -fit is doing a great job at splitting this evenly . the largest piece of traffic between these two during initial tensor transfer was &amp;lt;5Gbps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/braydon125"&gt; /u/braydon125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hvlsxvdyzblg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T23:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcw4sk</id>
    <title>Qwen 3 Next Coder Hallucinating Tools?</title>
    <updated>2026-02-23T22:27:50+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcw4sk/qwen_3_next_coder_hallucinating_tools/"&gt; &lt;img alt="Qwen 3 Next Coder Hallucinating Tools?" src="https://preview.redd.it/d147gfsolblg1.png?width=140&amp;amp;height=81&amp;amp;auto=webp&amp;amp;s=1d6dc8c61147db13d10fd165518ff8f25fbf00ba" title="Qwen 3 Next Coder Hallucinating Tools?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else experiencing this? I was workshopping a website prototype when I noticed it got stuck in a loop continuously attempting to &amp;quot;make&amp;quot; the website infrastructor itself.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d147gfsolblg1.png?width=1218&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8319a814e843fa052a0bcb5cfaa4219b84af4bc"&gt;Qwen 3 Coder Next hallucinating tool call in LM Studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It went on like this for over an hour, stuck in a loop trying to do these tool calls.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcw4sk/qwen_3_next_coder_hallucinating_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcw4sk/qwen_3_next_coder_hallucinating_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcw4sk/qwen_3_next_coder_hallucinating_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T22:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rckcww</id>
    <title>Benchmarked 4 AI Memory Systems on 600-Turn Conversations - Here Are the Results</title>
    <updated>2026-02-23T15:25:00+00:00</updated>
    <author>
      <name>/u/singh_taranjeet</name>
      <uri>https://old.reddit.com/user/singh_taranjeet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just completed comprehensive benchmarks comparing memory layers for production AI agents. Tested Mem0 against OpenAI Memory, LangMem, and MemGPT across 10 multi-session conversations with 200 questions each.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mem0&lt;/strong&gt;: 66.9% accuracy, 1.4s p95 latency, ~2K tokens per query&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mem0 Graph&lt;/strong&gt;: 68.5% accuracy, 2.6s p95 latency, ~4K tokens (superior temporal reasoning)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Memory&lt;/strong&gt;: 52.9% accuracy, 0.9s p95 latency, ~5K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangMem&lt;/strong&gt;: 58.1% accuracy, 60s p95 latency, ~130 tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MemGPT&lt;/strong&gt;: Results in appendix&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What stands out:&lt;/strong&gt; Mem0 achieved 14 percentage points higher accuracy than OpenAI Memory while maintaining sub-2s response times. The graph variant excels at temporal queries (58.1% vs OpenAI's 21.7%) and multi-hop reasoning.&lt;/p&gt; &lt;p&gt;LangMem's 60-second latency makes it unusable for interactive applications, despite being open source.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; Used LOCOMO dataset with GPT-4o-mini at temperature 0. Evaluated factual consistency, multi-hop reasoning, temporal understanding, and open-domain recall across 26K+ token conversations.&lt;/p&gt; &lt;p&gt;This matters because production agents need memory that persists beyond context windows while maintaining chat-level responsiveness. Current approaches either sacrifice accuracy for speed or become too slow for real-time use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/singh_taranjeet"&gt; /u/singh_taranjeet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T15:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcrw96</id>
    <title>Agentic coding with GLM 5 on Mac M3u 512 gb</title>
    <updated>2026-02-23T19:52:20+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running the MLX 4 bit quant and it's actually quite usable. Obviously not nearly as fast as Claude or another API, especially with prompt processing, but as long as you keep context below 50k or so, it feels very usable with a bit of patience.&lt;/p&gt; &lt;p&gt;Wouldn't work for something where you absolutely need 70k+ tokens in context, both because of context size limitations and the unbearable slowness that happens after you hit a certain amount of context with prompt processing.&lt;/p&gt; &lt;p&gt;For example, I needed it to process about 65k tokens last night. The first 50% finished in 8 minutes (67 t/s), but the second fifty percent took another 18 minutes ( a total of 41 t/s).&lt;/p&gt; &lt;p&gt;Token gen however remains pretty snappy; I don't have an exact t/s but probably between 12 and 20 at these larger context sizes. Opencode is pretty clever about not prompt processing between tasks unnecessarily; so once a plan is created it can output thousands of tokens of code across multiple files in just a few minutes with reasoning in between.&lt;/p&gt; &lt;p&gt;Also with prompt processing usually it's just a couple minutes for it to read a few hundred lines of code per file so the 10 minutes of prompt processing is spread across a planning session. Compaction in opencode however does take a while as it likes to basically just reprocess the whole context. But if you set a modest context size of 50k it should only be about 5 minutes of compaction.&lt;/p&gt; &lt;p&gt;I think MLX or even GGUF may get faster prompt processing as the runtimes are updated for GLM 5, but it will likely not get a TON faster than this. Right now I am running on LM studio so I might already not be getting the latest and greatest performance because us LM studio users wait for official LM studio runtime updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrw96/agentic_coding_with_glm_5_on_mac_m3u_512_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrw96/agentic_coding_with_glm_5_on_mac_m3u_512_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrw96/agentic_coding_with_glm_5_on_mac_m3u_512_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcsoju</id>
    <title>MiniMax 2.5 with 8x+ concurrency using RTX 3090s HW Requirements.</title>
    <updated>2026-02-23T20:19:51+00:00</updated>
    <author>
      <name>/u/BigFoxMedia</name>
      <uri>https://old.reddit.com/user/BigFoxMedia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mratsim/MiniMax-M2.5-BF16-INT4-AWQ/"&gt;https://huggingface.co/mratsim/MiniMax-M2.5-BF16-INT4-AWQ/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I have 7 x RTX 3090s split across 2 Servers. &lt;/p&gt; &lt;p&gt;I will need to buy a minimum of 1 more GPU and a better motherboard ( to support having all 8 on it ) just to test trial this model. &lt;/p&gt; &lt;p&gt;However, I need to be able to serve 4-5 concurrent users that likely will fire off concurrent requests ( Software Engineers ).&lt;/p&gt; &lt;p&gt;So I have to calculate how many GPUS I need and which motherboard to be able to serve at least that capacity. &lt;/p&gt; &lt;p&gt;Since no CPU offloading, I suspect I will need around 12 GPUs but likely can get away with x4 PCIe gen 3.0 speeds since no CPU offloading.&lt;/p&gt; &lt;p&gt;Conversely, I do have 512GB of DDR4 RAM ( 8* Hynix 64GB 4DRx4 PC4-2400T LRDIMM DDR4-19200 ECC Load Reduced Server Memory RAM) or alternatively 768 GB of DDR4 using RDDIM ( not LRDIMM - can't mix and match the two sets * ), with 24 x 16gb = 768GB of DDR4 RAM allowing me to run with just 8 GPUs and partial (minimal ) CPU offload ( KV on GPUs and ~60-80% of weights on GPU, the rest on CPU) - is my best guestimate..&lt;/p&gt; &lt;p&gt;So if I go with a higher end EPYC ROME Motherboard I could offload partially I guess, but I need to make sure I get ~35 t/s per each concurrent request, serving ~4-5 users that's likely ~12-16 req in parallel ( so batch 16 peak ) and I don't know if that's possible with possible with partial CPU offload.&lt;/p&gt; &lt;p&gt;Before I shell out another $3K-$5K ( Mobo Combo + 1/2/3 more GPUs ) I need to get a better idea of what I should expect.&lt;/p&gt; &lt;p&gt;Thanks guys,&lt;/p&gt; &lt;p&gt;Eddie.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigFoxMedia"&gt; /u/BigFoxMedia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcsoju/minimax_25_with_8x_concurrency_using_rtx_3090s_hw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcsoju/minimax_25_with_8x_concurrency_using_rtx_3090s_hw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcsoju/minimax_25_with_8x_concurrency_using_rtx_3090s_hw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcrzbn</id>
    <title>Strix Halo 128Gb: what models, which quants are optimal?</title>
    <updated>2026-02-23T19:55:21+00:00</updated>
    <author>
      <name>/u/DevelopmentBorn3978</name>
      <uri>https://old.reddit.com/user/DevelopmentBorn3978</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strix Halo APU should not benefit from running large models that have been quantized using MXFP4 (as on Blackwell GPUs). So which models at which quants have you found that do shine on this architecture in GPU only mode (i.e. runnable with llama.cpp)? Could it benefit as well from usage of formats for models quantization that are closer to the native FP4/FP8 formats of these chips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DevelopmentBorn3978"&gt; /u/DevelopmentBorn3978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:55:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rckqpp</id>
    <title>Hardware requirements for training a ~3B Model From Scratch locally?</title>
    <updated>2026-02-23T15:39:08+00:00</updated>
    <author>
      <name>/u/Any-Cobbler6161</name>
      <uri>https://old.reddit.com/user/Any-Cobbler6161</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôm a data science master‚Äôs student who‚Äôs posted on here a couple times before over the last year or 2. Now am working on my senior thesis and I‚Äôm trying to figure out the feasibility of training a ~3B parameter transformer model from scratch. So not fine-tuning. I‚Äôm trying to figure out what‚Äôs realistically doable on a home setup within ~6 months. My school is unfortunately is a very small public school and doesn‚Äôt have their own cluster or anything like that. Prior to this I was at a bigger school that did so I was just planning on booking time using theirs but unfortunately last year I had to transfer because I got really sick as they didn‚Äôt make accommodations for folks with medical disability. &lt;/p&gt; &lt;p&gt;Anyways I was thinking about training something in the ball park of 3B Params, 2k context, 25/50b training tokens, in fp16, probably using AdamW. My current system I have designed based on some napkin math is 2x 3090s over nvlink as I already have a Z690 motherboard that supports x8/x8 bifurcation, 1200W PSU, and 64gb of DDR5 RAM. Prior to this I had a rtx 5090 but even though it was crazy fast the 32gb was not enough to hold all the weights, grads, buffers, optimizer states (AdamW), etc. &lt;/p&gt; &lt;p&gt;Just wanted to hop on here and see if anyone here actually trained a 3B model or slightly smaller from scratch at home and if so what GPUs did you use/how did you do it? If you‚Äôve done anything remotely similar (even 1B‚Äì2B scale), I‚Äôd love to hear your setup and how it went.&lt;/p&gt; &lt;p&gt;Appreciate any real-world data points , thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cobbler6161"&gt; /u/Any-Cobbler6161 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T15:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcuaip</id>
    <title>Serious question: do you think Dario (or any other major AI players or political players) have enough power and influence that they will get Chinese local AI and/or local AI in general banned in the U.S.? What do you think the odds are?</title>
    <updated>2026-02-23T21:19:33+00:00</updated>
    <author>
      <name>/u/DeepOrangeSky</name>
      <uri>https://old.reddit.com/user/DeepOrangeSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess I'll put Dario in the title, since he's the most relevant hater of the day, and I guess fairly powerful in regards to this as far as any one specific guy goes, but, obviously if something like this happened, it would involve a lot more people combining their powers than just Dario alone.&lt;/p&gt; &lt;p&gt;Anyway, curious what you think the odds are that this actually happens. And if you were puttings odds per timescale, what would you say (like odds it happens in 2026, vs happens in next 2 years, vs next 3 years, vs never happens at all).&lt;/p&gt; &lt;p&gt;And you can divide the scenarios, like just specifically Chinese local AI (but not non-Chinese local AI) vs just all local AI of any kind (even American), etc.&lt;/p&gt; &lt;p&gt;I wonder if there is about to be a huge run on Seagate and WD hdds where they sell out like crazy that dwarfs even that big openclaw-related run on Mac minis that happened a few weeks ago, as everyone starts trying to hoard a bunch of different quants of all the best open models and even a bunch of quants and versions of all the biggest DeepSeek, GLM, and Kimi ones that they don't even necessarily have enough ram to run yet to future-proof in case it all goes away? Time to buy a bunch of Seagate stock?&lt;/p&gt; &lt;p&gt;Kind of joking about the Seagate aspect, since not that many people use open-weights ai rn, obv, but, anyway, wondering how serious you all think the odds are about the local stuff getting banned&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeepOrangeSky"&gt; /u/DeepOrangeSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T21:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rci9h1</id>
    <title>TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)</title>
    <updated>2026-02-23T14:03:12+00:00</updated>
    <author>
      <name>/u/zakerytclarke</name>
      <uri>https://old.reddit.com/user/zakerytclarke</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"&gt; &lt;img alt="TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)" src="https://external-preview.redd.it/JxyR2-HPTrb177zTD0smUzhI5l6xLW7EKVY2pYpkHxc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf1debba24ef624d732718952f5f5127580705f6" title="TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zakerytclarke"&gt; /u/zakerytclarke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/teapotai/tinyteapot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T14:03:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc6c8m</id>
    <title>Feels like magic. A local gpt-oss 20B is capable of agentic work</title>
    <updated>2026-02-23T03:18:16+00:00</updated>
    <author>
      <name>/u/Vaddieg</name>
      <uri>https://old.reddit.com/user/Vaddieg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt; &lt;img alt="Feels like magic. A local gpt-oss 20B is capable of agentic work" src="https://preview.redd.it/b27xdhewq5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9692be692d82dd176bce38aa1cffe88af9406be" title="Feels like magic. A local gpt-oss 20B is capable of agentic work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gave a try to &lt;a href="https://github.com/zeroclaw-labs/zeroclaw"&gt;zeroclaw&lt;/a&gt; agent (intstead of the bloated and overhyped one). After few hours of fuckery with configs it's finally useful. Both main and embeddings models are running locally.&lt;br /&gt; I carefully read what it's trying to execute in shell, and permit only [relatively] safe tools in config.&lt;br /&gt; So far it can interact with macOS apps, web pages, and local files while keeping all my data private.&lt;br /&gt; gpt-oss 20B has its limits though, it loses focus after 15-20 steps and often needs direct instructions to use persistent memory. It also starts behaving weirdly if tool access has been denied or tool returned some error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vaddieg"&gt; /u/Vaddieg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b27xdhewq5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T03:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rco9v7</id>
    <title>RWKV-7: O(1) memory inference, 16.39 tok/s on ARM Cortex-A76, beats LLaMA 3.2 3B. The local-first architecture nobody is talking about...</title>
    <updated>2026-02-23T17:45:31+00:00</updated>
    <author>
      <name>/u/Sensitive-Two9732</name>
      <uri>https://old.reddit.com/user/Sensitive-Two9732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wrote a deep-dive specifically because the deployment numbers don't get enough attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FREE MEDIUM LINK&lt;/strong&gt;: &lt;a href="https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4"&gt;https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The headline stats for local inference:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;O(1) memory per token, no KV cache at all. Context length does not affect VRAM usage.&lt;/li&gt; &lt;li&gt;16.39 tok/s on ARM Cortex-A76 (7B model). That's a mid-range Android chip.&lt;/li&gt; &lt;li&gt;28.7 tok/s on Snapdragon X Elite (7B). Current-gen Windows on ARM.&lt;/li&gt; &lt;li&gt;RWKV-X hybrid: 1.37x faster than Flash Attention v3 at 128K context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Microsoft already ships Eagle v5 (RWKV-based) on ~1.5 billion Windows machines for on-device tasks. No cloud round-trip.&lt;/p&gt; &lt;p&gt;The compression stack: 4-bit quantized RWKV-7 0.1B runs on microcontrollers. The state size is fixed regardless of how long the conversation runs. For local-first deployment this is a fundamentally different proposition than fitting a Transformer's growing KV cache into limited VRAM.&lt;/p&gt; &lt;p&gt;Weights (Apache 2.0): &lt;a href="https://huggingface.co/collections/RWKV/rwkv-v7"&gt;https://huggingface.co/collections/RWKV/rwkv-v7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to discuss about this. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Two9732"&gt; /u/Sensitive-Two9732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/ai-advances/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcc2fa</id>
    <title>An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding</title>
    <updated>2026-02-23T08:33:22+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt; &lt;img alt="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" src="https://preview.redd.it/wfpxmbhlc7lg1.png?width=140&amp;amp;height=99&amp;amp;auto=webp&amp;amp;s=48d6dec8f7b1fd1a11e8a1b5afbd51040b6a5021" title="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcc2fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T08:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc59ze</id>
    <title>Qwen3's most underrated feature: Voice embeddings</title>
    <updated>2026-02-23T02:28:32+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt; &lt;img alt="Qwen3's most underrated feature: Voice embeddings" src="https://preview.redd.it/zmcs7iysm5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=796016e685c536fbab1ce49b5fec35afeb75f40e" title="Qwen3's most underrated feature: Voice embeddings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you know that Qwen3 TTS utilizes voice embedding for voice cloning?&lt;br /&gt; Your voice is turned into a vector of 1024 dimensions (or 2048 for 1.7b), and based on this vector alone you can get your custom voice.&lt;/p&gt; &lt;p&gt;But the coolest part is that this means that you can use math to modify voices, average voices. You can swap gender, pitch, mix and match voices, and even create an emotion space! This also enables semantic voice search!&lt;/p&gt; &lt;p&gt;The voice embedding model is actually just a tiny encoder with just a few million parameters. I've ripped it out of the voice embedding model so you can use the embedding model standalone. Check out my collection! :D I also have onnx models for optimized web / front-end inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding"&gt;https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice embedings can be used for inference in my vllm-omni fork until it is supported in upstream: &lt;a href="https://github.com/heiervang-technologies/ht-vllm-omni"&gt;https://github.com/heiervang-technologies/ht-vllm-omni&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmcs7iysm5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T02:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcs9vr</id>
    <title>Talking to my to-do list</title>
    <updated>2026-02-23T20:05:37+00:00</updated>
    <author>
      <name>/u/llo7d</name>
      <uri>https://old.reddit.com/user/llo7d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt; &lt;img alt="Talking to my to-do list" src="https://external-preview.redd.it/YnFzdm9lejd2YWxnMWY-tuy7HWwE5y0N4mja7xeEwkxeCiovLgSs8XbE5sB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7139ad7399515809748a8bd26139c3d328ee50f5" title="Talking to my to-do list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing feeding all my to-do list and productivity and having this kinda of desk robot thing as a screen to talk to? all the stuff happens on the pc, the screen is just a display and still for now it is a cloud based ai but I can definitely see this all happening locally in the future &lt;em&gt;(also better for privacy stuff)&lt;/em&gt; man the future is going to be awesome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/llo7d"&gt; /u/llo7d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xplqhdz7valg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rclyvf</id>
    <title>Portable Workstation for Inference</title>
    <updated>2026-02-23T16:24:21+00:00</updated>
    <author>
      <name>/u/neintailedfoxx</name>
      <uri>https://old.reddit.com/user/neintailedfoxx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt; &lt;img alt="Portable Workstation for Inference" src="https://preview.redd.it/j59qyq8sq9lg1.jpg?width=140&amp;amp;height=64&amp;amp;auto=webp&amp;amp;s=f979ae081b50b775630080b53aa9f61d422aa5ed" title="Portable Workstation for Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a new portable workstation for gaming/AI workloads. One of the fans is a 12018 fan bought from aliexpress derived from a fan on the 4090FE, allowing it to provide airflow equivalent to normal 25mm thick fans despite only being 18mm in thickness.&lt;/p&gt; &lt;p&gt;Would've loved to get a Threadripper for additional memory bandwidth, but sadly there aren't any itx Threadripper boards :(&lt;/p&gt; &lt;p&gt;Getting around 150-165 tok/sec running GPT OSS 120B with max context length in LM Studio (Using windows, haven't had time to test in linux yet)&lt;/p&gt; &lt;p&gt;CPU is undervolted using the curve optimizer (-25/-30 per CCD CO) with a +200MHz PBO clock offset, RAM is tuned to 6000MT/s CL28-36-35-30 @ 2233MHz FCLK, and the GPU is undervolted to 0.89v@2700MHz and power limited to 500w.&lt;/p&gt; &lt;p&gt;Temps are good, with the cpu reaching a max temp of around 75c and the GPU never going above 80c even during extremely heavy workloads. Top fans are set to intake, providing airflow to the flipped GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Case:&lt;/strong&gt; FormD T1 2.5 Gunmetal w/ Flipped Travel Kit&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 9 9950X3D&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX PRO 6000 Workstation Edition&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; MSI MPG X870I EDGE TI EVO WIFI&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ram:&lt;/strong&gt; TEAMGROUP T-Force Delta RGB 96 GB DDR5-6800 CL36&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; Crucial T710 4TB, Samsung 990 Pro 4TB, WD Black SN850X 8TB, TEAMGROUP CX2 2TB (Used drives from my previous build since I definitely won't be able to afford all this storage at current prices)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Corsair SF1000&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU Cables:&lt;/strong&gt; Custom Cables from Dreambigbyray&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; CM Masterliquid 240 ATMOS Stealth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neintailedfoxx"&gt; /u/neintailedfoxx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rclyvf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:24:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcnv9h</id>
    <title>GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)</title>
    <updated>2026-02-23T17:31:02+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt; &lt;img alt="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" src="https://preview.redd.it/t89mf46o4alg1.png?width=140&amp;amp;height=89&amp;amp;auto=webp&amp;amp;s=6d3e242b7e37ab99694926c9cefb58fae2a90e45" title="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcnv9h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcr4ju</id>
    <title>Dario Is Scared</title>
    <updated>2026-02-23T19:24:52+00:00</updated>
    <author>
      <name>/u/Doris_Dressy1</name>
      <uri>https://old.reddit.com/user/Doris_Dressy1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"&gt; &lt;img alt="Dario Is Scared" src="https://preview.redd.it/kws4m2dtnalg1.png?width=140&amp;amp;height=65&amp;amp;auto=webp&amp;amp;s=03b607ae75f5f49a95e7b7bdff7b77e0c3749c2e" title="Dario Is Scared" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why did Anthropic choose this exact moment to release that &lt;a href="https://x.com/AnthropicAI/status/2025997928242811253"&gt;statement&lt;/a&gt;? Because he‚Äôs scared. &lt;/p&gt; &lt;p&gt;Ever since OpenClaw launched, token usage from both individuals and model companies has been booming. And yet, on OpenRouter, the top-ranked models are no longer Claude but open-source models like Kimi K2.5 and Minimax M2.5. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kws4m2dtnalg1.png?width=2076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7355e5685d4cafe68bbb0ad1f2deffa69f74a50"&gt;https://preview.redd.it/kws4m2dtnalg1.png?width=2076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7355e5685d4cafe68bbb0ad1f2deffa69f74a50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everyone can see that agents are the future. But Anthropic is losing market share in this area. &lt;/p&gt; &lt;p&gt;Dario keeps talking about AI safety, while on the other side his company runs &lt;a href="https://www.theverge.com/2024/7/25/24205943/anthropic-ai-web-crawler-claudebot-ifixit-scraping-training-data"&gt;crawlers&lt;/a&gt; that ignore robots.txt and overwhelm independent websites, trains on &lt;a href="https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit"&gt;copyrighted material&lt;/a&gt;, and keep trying to ban open-source models, scare people by comparing them to &lt;a href="https://www.axios.com/2026/01/20/anthropic-ceo-admodei-nvidia-chips-china-trump"&gt;nuclear weapons&lt;/a&gt;. His goal is so clear: monopolize the intelligence of the future, and with it, monopolize power. &lt;/p&gt; &lt;p&gt;Yet another Linus moment: fxxk you, Dario!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doris_Dressy1"&gt; /u/Doris_Dressy1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcrb2k</id>
    <title>Hypocrisy?</title>
    <updated>2026-02-23T19:31:17+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt; &lt;img alt="Hypocrisy?" src="https://preview.redd.it/jxutlq8bqalg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d78bab536255787ed1f0bc277f2a7f6d5aea3b" title="Hypocrisy?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jxutlq8bqalg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcseh1</id>
    <title>Fun fact: Anthropic has never open-sourced any LLMs</title>
    <updated>2026-02-23T20:10:06+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a little side project comparing tokenizer efficiency across different companies‚Äô models for multilingual encoding.&lt;/p&gt; &lt;p&gt;Then I saw Anthropic‚Äôs announcement today and suddenly realized: there‚Äôs no way to analyze claude‚Äôs tokenizer lmao!&lt;/p&gt; &lt;p&gt;edit: Google once mentioned in a paper that Gemma and Gemini share the same tokenizer. OpenAI has already open‚Äësourced their tokenizers (and gpt‚Äëoss). And don‚Äôt even get me started on Llama (Llama 5 pls üò≠). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcmlwk</id>
    <title>so is OpenClaw local or not</title>
    <updated>2026-02-23T16:47:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt; &lt;img alt="so is OpenClaw local or not" src="https://preview.redd.it/5rolok0mw9lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bdebee8fd3b3c91999b3592892a73daf47142e" title="so is OpenClaw local or not" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading the comments, I‚Äôm guessing you didn‚Äôt bother to read this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Safety and alignment at Meta Superintelligence.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rolok0mw9lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcvimv</id>
    <title>Distillation when you do it. Training when we do it.</title>
    <updated>2026-02-23T22:04:41+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt; &lt;img alt="Distillation when you do it. Training when we do it." src="https://preview.redd.it/9rc0jqbohblg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05481c4cef786a02ca1e5d0b968e61114727348f" title="Distillation when you do it. Training when we do it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rc0jqbohblg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T22:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpmwn</id>
    <title>Anthropic: "We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax." üö®</title>
    <updated>2026-02-23T18:32:45+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt; &lt;img alt="Anthropic: &amp;quot;We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; üö®" src="https://preview.redd.it/94fbimavfalg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2ad159232448ffd7033d6be4fa96582b674e461" title="Anthropic: &amp;quot;We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; üö®" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94fbimavfalg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
