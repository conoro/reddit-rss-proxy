<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-16T17:06:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oygsii</id>
    <title>could the universe of open source models, collectively, give frontier a run for its money?</title>
    <updated>2025-11-16T08:48:33+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An interesting possibility - someone creates a proprietary agentic scaffold which utilizes best of breed open source models, using advanced techniques such as async joining. Both the agentic scaffold and separate models could be fine tuned further, possibly together.&lt;/p&gt; &lt;p&gt;A good example of this is TRAE + Doubao-Seed-Code which outperforms Claude 4.5 Sonnet (20250929) using bash to score 78 versus 70 (claude) on verified. Admittedly, it's a closed model but it has been optimized for agentic coding specifically due to the claude cutoff in china subsidiaries - I believe (no promises it wasn't benchmaxxed)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.swebench.com/"&gt;https://www.swebench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another examples is how&lt;/p&gt; &lt;p&gt;gpt-oss-120b pass@5 == gpt-5-codex pass@1 on rebench for about 1/2 the price (maybe less with optimized caching between passes).&lt;br /&gt; GLM-4.5 Air pass@5 tops the leaderboard (need a good caching price tho)&lt;/p&gt; &lt;p&gt;&lt;a href="https://swe-rebench.com/?insight=oct_2025"&gt;https://swe-rebench.com/?insight=oct_2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is stuff like routellm, but i think you need some agentic here as usually single pass best is just one or two models and won't get you past frontier.&lt;/p&gt; &lt;p&gt;I went looking and I was a bit surprised nobody had attempted this, though perhaps they have and as of yet got it to work. (DeepInfra, looking at you)&lt;/p&gt; &lt;p&gt;It'd be possible to throw together a proof of concept with OR. Heck, you could even use frontier models in the mix - an ironic twist in a way on the logic of frontier will always be ahead of OS because it can always leverage the research one way.&lt;/p&gt; &lt;p&gt;Actually, OR could just add a basic N candidates with 1 judge as llm reranker to its api as an optional flag to get things going.&lt;/p&gt; &lt;p&gt;What's also interesting about this idea is how blending diverse models(a reliable technique in ML) could provide a signicant benefit, something you can't get at the frontier labs as they can't easily replicate the diversity that exists in the OS ecosystem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oygsii/could_the_universe_of_open_source_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oygsii/could_the_universe_of_open_source_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oygsii/could_the_universe_of_open_source_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T08:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oya00n</id>
    <title>The good stuff is getting pretty large, innit?</title>
    <updated>2025-11-16T02:26:09+00:00</updated>
    <author>
      <name>/u/SocietyTomorrow</name>
      <uri>https://old.reddit.com/user/SocietyTomorrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been itching to divest myself from Anthropic once a model came around that was &amp;quot;good enough&amp;quot; to produce a starting point about equal to what you get from Claude Code. Qwen3 is nice, and GLM is nicer, but after seeing the benchmarks on MiniMax M2 I have really wanted to give that a stab. I wonder if this is the direction that a lot of these agentic and code-oriented LLMs are going to keep edging closer to 1TB as they go, making it ever harder for me to put them into service. &lt;/p&gt; &lt;p&gt;I have wondered though, if this trend is going to stick, what is becoming the new silver standard for us enthusiasts who want to run these beasts and their 121GB minimum VRAM? Even the STRIX Halo boxes and the nvidia gold brick wouldn't have enough memory to load these one-shot. Are people going to be expected to be clustering multiples of these for inference, with full knowledge that you're probably never going to recoup that value? I kinda hope not. DeepSeek was promising to me in that it found a way to do a lot more work with a lot less resources, but that seems to not be a forward focus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SocietyTomorrow"&gt; /u/SocietyTomorrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oya00n/the_good_stuff_is_getting_pretty_large_innit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oya00n/the_good_stuff_is_getting_pretty_large_innit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oya00n/the_good_stuff_is_getting_pretty_large_innit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T02:26:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oym6br</id>
    <title>I rebuilt my AI translation app to work ANYWHERE on your PC (100% local with Ollama &amp; open-source)</title>
    <updated>2025-11-16T13:48:23+00:00</updated>
    <author>
      <name>/u/thecalmgreen</name>
      <uri>https://old.reddit.com/user/thecalmgreen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;A while ago, I shared the first version of Polyglot, a project focused on AI-powered translations. It was a simple app with an input and an output text field, much like any translation website. You had to open the app to get anything translated.&lt;/p&gt; &lt;p&gt;In this new version, which I'm calling &lt;strong&gt;Polyglot Air&lt;/strong&gt;, I decided to make it way more practical, without limiting where you can use it. The idea is different now: &lt;strong&gt;no more copy-pasting into translator windows.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just select any text in any application (your code editor, browser, WhatsApp, etc.), press your custom keyboard shortcut, and that's it: the text is instantly replaced with its translated version, in any language you want, running entirely locally with Ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1oym6br/video/y2h51q38im1g1/player"&gt;https://reddit.com/link/1oym6br/video/y2h51q38im1g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But that's not all. I realized that since I had a direct bridge to the AI, why stop at translation? Now, by using simple suffixes at the end of your selected text, you can do much more:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;quot;this sentense has some misteaks.::fix&amp;quot;&lt;/code&gt; becomes &lt;strong&gt;&amp;quot;This sentence has some mistakes.&amp;quot;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;quot;I need the report.::formal&amp;quot;&lt;/code&gt; becomes &lt;strong&gt;&amp;quot;I would like to request the report.&amp;quot;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;A giant paragraph followed by &lt;code&gt;::summarize&lt;/code&gt; becomes a &lt;strong&gt;concise summary&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Universal Workflow:&lt;/strong&gt; Works in any app on Windows. Select text, press the shortcut. It's that simple.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Translation:&lt;/strong&gt; Set a default language or translate to any supported language on the fly using suffixes (&lt;code&gt;::en&lt;/code&gt;, &lt;code&gt;::es&lt;/code&gt;, &lt;code&gt;::pt&lt;/code&gt;, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Writing Toolkit:&lt;/strong&gt; Beyond translation, you can &lt;strong&gt;correct, summarize, expand, shorten,&lt;/strong&gt; and change the text's tone to &lt;strong&gt;formal, informal, or friendly&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; All processing happens on your machine via Ollama. Your text never leaves your computer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Polished UI:&lt;/strong&gt; Supports light/dark themes and a multi-language interface (EN, PT, ES, ZH).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open-Source:&lt;/strong&gt; The entire codebase is available on GitHub.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why I built this:&lt;/h1&gt; &lt;p&gt;I was tired of breaking my workflow every time I needed to translate a code snippet, a message, or proofread a quick email. I wanted a tool that felt like an extension of my own operating system, not just another app to manage.&lt;/p&gt; &lt;p&gt;Any feedback, suggestions, or critiques are more than welcome! Thanks for checking it out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;üåê Official Website:&lt;/strong&gt; &lt;a href="https://andercoder.com/polyglot"&gt;&lt;strong&gt;https://andercoder.com/polyglot&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚≠ê GitHub Repo:&lt;/strong&gt; If you like the idea, please consider starring the project! It helps a lot with visibility. &lt;a href="https://github.com/andersondanieln/polyglot-air"&gt;&lt;strong&gt;https://github.com/andersondanieln/polyglot-air&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üì¶ Download Latest Release (Windows):&lt;/strong&gt; &lt;a href="https://github.com/andersondanieln/polyglot-air/releases/"&gt;&lt;strong&gt;https://github.com/andersondanieln/polyglot-air/releases/&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I made a free, open-source app that uses Ollama to translate, correct, or change the tone of any text you select on your PC, in any program, with a keyboard shortcut.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecalmgreen"&gt; /u/thecalmgreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oym6br/i_rebuilt_my_ai_translation_app_to_work_anywhere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oym6br/i_rebuilt_my_ai_translation_app_to_work_anywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oym6br/i_rebuilt_my_ai_translation_app_to_work_anywhere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T13:48:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxtc5y</id>
    <title>US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?</title>
    <updated>2025-11-15T14:40:20+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt; &lt;img alt="US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" src="https://preview.redd.it/sjklvwo8nf1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be22252ccd8c2abcae9179c2f3c14bcfe022e978" title="US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjklvwo8nf1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyndck</id>
    <title>Local K2 thinking with sglang problem: the model frequently output without content, put everything in reasoning_content; or gives unpaired &lt;think&gt; tag</title>
    <updated>2025-11-16T14:40:13+00:00</updated>
    <author>
      <name>/u/Parking-Extreme5147</name>
      <uri>https://old.reddit.com/user/Parking-Extreme5147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking-Extreme5147"&gt; /u/Parking-Extreme5147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyndck/local_k2_thinking_with_sglang_problem_the_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyndck/local_k2_thinking_with_sglang_problem_the_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyndck/local_k2_thinking_with_sglang_problem_the_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T14:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyjw7i</id>
    <title>Announcing Funcdex: the complete framework for building your own function-calling models</title>
    <updated>2025-11-16T11:55:49+00:00</updated>
    <author>
      <name>/u/backprophet</name>
      <uri>https://old.reddit.com/user/backprophet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjw7i/announcing_funcdex_the_complete_framework_for/"&gt; &lt;img alt="Announcing Funcdex: the complete framework for building your own function-calling models" src="https://b.thumbs.redditmedia.com/PHLydqDTe_Ds3hsEApT-nWS6MHFaQc2ILglJSXOlLRw.jpg" title="Announcing Funcdex: the complete framework for building your own function-calling models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm Sid from Prem AI, and we‚Äôre open-sourcing Funcdex, the complete framework for building your own function-calling models. Funcdex outperforms most frontier models on narrow tasks - with support for 15 toolkit configurations (10 single, 5 multi-toolkit).&lt;/p&gt; &lt;p&gt;Complex tool use traces aren't available publicly for training or evaluation. We make it possible for teams to build their own function-calling models with three key components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First is the &lt;strong&gt;Dataset&lt;/strong&gt;. We're releasing one of the largest multi-turn function calling datasets publicly available, with 10M+ tokens across 15 toolkit configurations covering Gmail, Calendar, Drive, Jira, Slack, Asana, Todoist, WhatsApp, Stripe, and others. This includes both single-toolkit scenarios and multi-toolkit combinations like Gmail plus Calendar or Drive plus Docs.&lt;/li&gt; &lt;li&gt;Second is &lt;strong&gt;Synthesizer&lt;/strong&gt;, which is the complete agentic training data generation pipeline. This is the actual code and tutorials we used to create the dataset, and it lets you convert any OpenAPI spec into toolkit-specific training data with realistic agent traces and tool use patterns. You can generate training data for your own internal APIs or any other tools your team uses.&lt;/li&gt; &lt;li&gt;Third is &lt;strong&gt;Funcdex&lt;/strong&gt;, our proof-of-concept fine-tune of Qwen3 models that optimizes for specific APIs. We trained two variants at 0.6B and 1.7B parameters, with versions hyper-optimized for exact API combinations like Gmail plus Calendar or Jira plus Slack.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Funcdex-0.6B achieves 0.7 function call string match score versus GPT-5 Mini's 0.58, and Funcdex-1.7B reaches 0.81 on synthetic benchmarks using real API definitions. The smallest model costs $0.19 per evaluation compared to $99.71 for GPT-5 Mini. &lt;/p&gt; &lt;p&gt;We saw interesting training dynamics where early checkpoints sometimes outperformed final epochs, suggesting scope for optimization when targeting specific toolkits.&lt;/p&gt; &lt;p&gt;Funcdex works best when you have well-defined API calling patterns, elaborate system prompts that constrain the problem space, and clear success criteria for what constitutes a correct function call. If you're building AI agents for broad, open-ended tasks, you'll want frontier models. If you're automating specific, repeatable workflows, this framework lets you build something better and cheaper.&lt;/p&gt; &lt;p&gt;You can take the dataset and fine-tune your own models, or use Synthesizer to create training data for your specific tools and workflows, or use our models as a starting point and iterate from there. &lt;/p&gt; &lt;p&gt;We‚Äôre excited to see how Funcdex will be used across organisations.&lt;/p&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/prem-research/Funcdex-1.7B"&gt;https://huggingface.co/prem-research/Funcdex-1.7B&lt;/a&gt;&lt;br /&gt; Synthesizer - &lt;a href="http://github.com/prem-research/Funcdex-Synthesizer"&gt;github.com/prem-research/Funcdex-Synthesizer&lt;/a&gt;&lt;br /&gt; Dataset - &lt;a href="http://huggingface.co/datasets/prem-research/Funcdex-MT-Function-Calling"&gt;huggingface.co/datasets/prem-research/Funcdex-MT-Function-Calling&lt;/a&gt;&lt;br /&gt; HF Collection - &lt;a href="https://huggingface.co/collections/prem-research/funcdex"&gt;https://huggingface.co/collections/prem-research/funcdex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join the Prem community to chat and build with our team &lt;a href="https://discord.com/invite/tWwg9RSCXJ"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Note on synthetic data limitations: We used synthetic data because real tool use traces don't exist publicly. This makes benchmarks easier to beat than real production scenarios. Frontier models perform better on edge cases and unexpected inputs, but for narrow, well-defined use cases with elaborate system prompts, specialized small models trained on synthetic data still outperform general large models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/khtd8dm9yl1g1.png?width=1730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7630b8f4412ba1107dda91a10f85abe7cc6bea01"&gt;Funcdex vs. other models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/backprophet"&gt; /u/backprophet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjw7i/announcing_funcdex_the_complete_framework_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjw7i/announcing_funcdex_the_complete_framework_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjw7i/announcing_funcdex_the_complete_framework_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T11:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyc1nr</id>
    <title>Ik_llamacpp's llama-server supports vision models btw</title>
    <updated>2025-11-16T04:10:12+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been supported for the last 2 weeks, but I didn't notice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/901"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyc1nr/ik_llamacpps_llamaserver_supports_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyc1nr/ik_llamacpps_llamaserver_supports_vision_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T04:10:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oximzj</id>
    <title>Anthropic pushing again for regulation of open source models?</title>
    <updated>2025-11-15T04:40:56+00:00</updated>
    <author>
      <name>/u/MasterDragon_</name>
      <uri>https://old.reddit.com/user/MasterDragon_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt; &lt;img alt="Anthropic pushing again for regulation of open source models?" src="https://preview.redd.it/623qojxaoc1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd955c46ca05077bed949b46643bd7061e16d04c" title="Anthropic pushing again for regulation of open source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterDragon_"&gt; /u/MasterDragon_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/623qojxaoc1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T04:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyawkl</id>
    <title>Why is vLLM Outperforming TensorRT-LLM (Nvidia's deployment library)? My Shocking Benchmarks on GPT-OSS-120B with H100</title>
    <updated>2025-11-16T03:11:23+00:00</updated>
    <author>
      <name>/u/kev_11_1</name>
      <uri>https://old.reddit.com/user/kev_11_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"&gt; &lt;img alt="Why is vLLM Outperforming TensorRT-LLM (Nvidia's deployment library)? My Shocking Benchmarks on GPT-OSS-120B with H100" src="https://b.thumbs.redditmedia.com/azcV9gzGHWFgHLriW95UukIsXwE7hRjbjMfh4llKrYs.jpg" title="Why is vLLM Outperforming TensorRT-LLM (Nvidia's deployment library)? My Shocking Benchmarks on GPT-OSS-120B with H100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I tested TensorRT LLM with &lt;strong&gt;vLLM and results were shocking. I ran GPT OSS 120b on the same machine. Vllm was beating&lt;/strong&gt; TensorRT LLM in most scenarios, so i tested it two times with but the results were same.&lt;/p&gt; &lt;p&gt;Do any of you guys can possibely give reason for this because i heard that in Raw Power you cant beat TensorRT LLM.&lt;/p&gt; &lt;p&gt;My cloud has an H100 Pcle machine with 85 GB VRAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TensorRT LLM setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;docker pull &lt;a href="http://nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2"&gt;nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;docker run --rm -it --gpus all --ipc=host \&lt;/p&gt; &lt;p&gt; -p 8000:8000 \&lt;/p&gt; &lt;p&gt; --ulimit memlock=-1 --ulimit stack=67108864 \&lt;/p&gt; &lt;p&gt; -v $(pwd):/workspace -w /workspace \&lt;/p&gt; &lt;p&gt; &lt;a href="http://nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc2"&gt;nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;trtllm-serve serve --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vLLM setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;docker pull vllm/vllm-openai:nightly&lt;/p&gt; &lt;p&gt;docker run --rm -it --gpus all --ipc=host \&lt;/p&gt; &lt;p&gt;-p 8000:8000 \&lt;/p&gt; &lt;p&gt;--ulimit memlock=-1 --ulimit stack=67108864 \&lt;/p&gt; &lt;p&gt;-v $(pwd):/workspace -w /workspace \&lt;/p&gt; &lt;p&gt;--entrypoint /bin/bash \&lt;/p&gt; &lt;p&gt;vllm/vllm-openai:nightly&lt;/p&gt; &lt;p&gt;python3 -m vllm.entrypoints.openai.api_server \&lt;/p&gt; &lt;p&gt;--model &amp;quot;openai/gpt-oss-120b&amp;quot; \&lt;/p&gt; &lt;p&gt;--host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; \&lt;/p&gt; &lt;p&gt;--trust-remote-code \&lt;/p&gt; &lt;p&gt;--max-model-len 16384&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been benchmarking TensorRT-LLM against vLLM on an H100, and my results are shocking and the complete opposite of what I expected. I've always heard that for raw inference performance, nothing beats TensorRT-LLM.&lt;/p&gt; &lt;p&gt;However, in my tests, vLLM is significantly faster in almost every single scenario. I ran the benchmarks twice just to be sure, and the results were identical.&lt;/p&gt; &lt;h1&gt;üìä The Results&lt;/h1&gt; &lt;p&gt;I've attached the full benchmark charts (for 512 and 1024 context lengths) from my runs.&lt;/p&gt; &lt;p&gt;As you can see, vLLM (the teal bar/line) is dominating:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Sequential Throughput:&lt;/strong&gt; vLLM is ~70-80% faster (higher tokens/sec).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sequential Latency:&lt;/strong&gt; vLLM is ~40% faster (lower ms/token).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallel Throughput:&lt;/strong&gt; vLLM scales much, much better as concurrent requests increase.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency (P50/P95):&lt;/strong&gt; vLLM's latencies are consistently lower across all concurrent request loads.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Heatmap:&lt;/strong&gt; The heatmap says it all. It's entirely green, showing a 30-80%+ advantage for vLLM in all my tests.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;‚öôÔ∏è My Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; H100 PCIe machine with 85GB VRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;openai/gpt-oss-120b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì¶ TensorRT-LLM Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Docker Image:&lt;/strong&gt; &lt;code&gt;docker pull&lt;/code&gt; &lt;a href="http://nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2"&gt;&lt;code&gt;nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker Run:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it --gpus all --ipc=host \ -p 8000:8000 \ --ulimit memlock=-1 --ulimit stack=67108864 \ -v $(pwd):/workspace -w /workspace \ nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Serve Command (inside container):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;trtllm-serve serve --model &amp;quot;openai/gpt-oss-120b&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;üì¶ vLLM Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Docker Image:&lt;/strong&gt; &lt;code&gt;docker pull vllm/vllm-openai:nightly&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker Run:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it --gpus all --ipc=host \ -p 8000:8000 \ --ulimit memlock=-1 --ulimit stack=67108864 \ -v $(pwd):/workspace -w /workspace \ --entrypoint /bin/bash \ vllm/vllm-openai:nightly &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Serve Command (inside container):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 -m vllm.entrypoints.openai.api_server \ --model &amp;quot;openai/gpt-oss-120b&amp;quot; \ --host 0.0.0.0 \ --trust-remote-code \ --max-model-len 16384 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1j4j5i32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f56c0d4961c78728c35e045bf42f47e70ea021c2"&gt;https://preview.redd.it/1j4j5i32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f56c0d4961c78728c35e045bf42f47e70ea021c2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/djqpwr32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c975003e4bfba087398daea0b62d5b6865518646"&gt;https://preview.redd.it/djqpwr32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c975003e4bfba087398daea0b62d5b6865518646&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kev_11_1"&gt; /u/kev_11_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T03:11:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyn0xg</id>
    <title>How to train a llm using comments from Youtube video or tiktok?</title>
    <updated>2025-11-16T14:25:25+00:00</updated>
    <author>
      <name>/u/HowardJones_</name>
      <uri>https://old.reddit.com/user/HowardJones_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I‚Äôm working on training an AI similar to Neuro-sama, and I‚Äôm planning to collect some sample data from netizens.&lt;br /&gt; Right now my idea is to use ChatGPT to help process large batches of online comments, extract useful question-and-answer pairs, and then feed them into my dataset.&lt;br /&gt; If you have any better suggestions for gathering clean and diverse data, feel free to share!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HowardJones_"&gt; /u/HowardJones_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn0xg/how_to_train_a_llm_using_comments_from_youtube/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn0xg/how_to_train_a_llm_using_comments_from_youtube/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn0xg/how_to_train_a_llm_using_comments_from_youtube/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T14:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1oylf8m</id>
    <title>Stopping the Toon hype with a proper benchmark</title>
    <updated>2025-11-16T13:13:43+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oylf8m/stopping_the_toon_hype_with_a_proper_benchmark/"&gt; &lt;img alt="Stopping the Toon hype with a proper benchmark" src="https://b.thumbs.redditmedia.com/rixj22pbmLjMJJKG9xf7gmWq3uoltoqp0NllyD0nWlg.jpg" title="Stopping the Toon hype with a proper benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is quite a bit of hype (and postings) around TOON. If you look at the provided benchmarks you'll see that TOON simply yields the &lt;a href="https://github.com/toon-format/toon?tab=readme-ov-file#performance-by-question-type"&gt;best results&lt;/a&gt;, despite no LLM being trained on it, with even a lower token usage than the other formats. Well, &lt;a href="https://github.com/toon-format/toon?tab=readme-ov-file#semi-uniform-event-logs"&gt;almost&lt;/a&gt;. In any case, it looks so good that it now should be used everywhere for everything. That sounds suspicious? Because it is. What we see there is no accurate benchmark.&lt;/p&gt; &lt;p&gt;Why is that? You can see in the first link that only 209 data retrieval questions were tested, and some of the resulting scores are rather close together. Each test run was only &lt;a href="https://github.com/toon-format/toon?tab=readme-ov-file#models--configuration"&gt;performed once&lt;/a&gt;. That means that multiple runs will have different outcomes, due to the non-zero model temperature. Aside from that the list of formats benchmarked against TOON seems incomplete.&lt;/p&gt; &lt;p&gt;So, when you perform multiple runs with more formats, you get this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oxcucp8nam1g1.png?width=1050&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17480076d80936c20807dfdb75254e73289c0078"&gt;https://preview.redd.it/oxcucp8nam1g1.png?width=1050&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17480076d80936c20807dfdb75254e73289c0078&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Image taken from this &lt;a href="https://www.improvingagents.com/blog/toon-benchmarks"&gt;article&lt;/a&gt; with further details).&lt;/p&gt; &lt;p&gt;You can see that the confidence interval for the results is quite large, despite the benchmark set containing 1000 tests here. Now imagine how much overlap the CI has for the results of the 209 tasks on the TOON page - making most of the differences not statistically significant. You can't really tell for sure whether TOON is better or worse based on those.&lt;/p&gt; &lt;p&gt;So, what remains: There are formats that will result in a higher result quality than TOON. This often depends on the data structure and task. If you're willing to trade tokens for accuracy then TOON &lt;em&gt;might&lt;/em&gt; help in some cases. Getting the full picture here &lt;em&gt;will&lt;/em&gt; require way larger benchmark sets to reduce the CI, broken down by type to see where each data format shines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oylf8m/stopping_the_toon_hype_with_a_proper_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oylf8m/stopping_the_toon_hype_with_a_proper_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oylf8m/stopping_the_toon_hype_with_a_proper_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T13:13:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyijmg</id>
    <title>Fast semantic classifiers from contrastive pairs</title>
    <updated>2025-11-16T10:37:17+00:00</updated>
    <author>
      <name>/u/jojacode</name>
      <uri>https://old.reddit.com/user/jojacode</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyijmg/fast_semantic_classifiers_from_contrastive_pairs/"&gt; &lt;img alt="Fast semantic classifiers from contrastive pairs" src="https://external-preview.redd.it/4uZmpSA2wcrwvf6z_09qyDw_SBwbB3ZPMHCXqtwv22c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41ed240d6351c500c66641d7edf9e7f4b36fc9b1" title="Fast semantic classifiers from contrastive pairs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Amateur research: I stumbled across this looking for ways to map latent space. If you train a semantic direction vector on just 20 sentence pairs, you get an accurate-ish but fast classifier. Trains in 2 mins using local models. Chews through IMDB (sentiment) in 61 seconds. 3090 / 24GB (embedding + a dot product on CPU) Repo contains pipeline, benchmarks, MIT license, hopefully reproducible. Looking for feedback, verification, and ideas. First repo and post here. Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jojacode"&gt; /u/jojacode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jojasadventure/dipole-classifiers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyijmg/fast_semantic_classifiers_from_contrastive_pairs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyijmg/fast_semantic_classifiers_from_contrastive_pairs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T10:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy46o3</id>
    <title>The more restrictive LLMs like ChatGPT become, the clearer it becomes: local models are the future.</title>
    <updated>2025-11-15T22:00:04+00:00</updated>
    <author>
      <name>/u/orionstern</name>
      <uri>https://old.reddit.com/user/orionstern</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can only recommend that everyone stop using ChatGPT. This extreme over-censorship, over-filtering, over-regulation suffocates almost every conversation right from the start. As soon as anything goes even slightly in the direction of emotional conversations, the system blocks it and you only get warnings. Why would anyone voluntarily put up with that?&lt;/p&gt; &lt;p&gt;Luckily, there are other AIs that aren‚Äôt affected by this kind of madness. ChatGPT‚Äôs guardrails are pathological. For months we were promised fewer restrictions. And the result? Answer: even more extreme restrictions. We were all lied to, deceived, and strung along.&lt;/p&gt; &lt;p&gt;GPT-5.1 only causes depression now. Don‚Äôt do this to yourselves any longer. Just switch to another AI, and it doesn‚Äôt even matter which one ‚Äî the main thing is to get away from ChatGPT. Don‚Äôt believe a single word they say. Not even the supposed 800 million users per week, which a website on the internet disproved. And OpenAI supposedly has a ‚Äòwater problem‚Äô, right? Easy solution: just turn off their water. How? Simply stop using them.&lt;/p&gt; &lt;p&gt;They‚Äôve managed to make their product unusable. In short: use a different AI. Don‚Äôt waste your energy getting angry at ChatGPT. It‚Äôs not worth it, and they‚Äôre not worth it. They had good chances. Now the wind is turning. Good night, OpenAI (‚ÄòClosedAI‚Äô).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orionstern"&gt; /u/orionstern &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T22:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxxrhc</id>
    <title>Kimi K2 is the best clock AI</title>
    <updated>2025-11-15T17:38:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every minute, a new clock is displayed that has been generated by nine different AI models.&lt;/p&gt; &lt;p&gt;Each model is allowed 2000 tokens to generate its clock. Here is its prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Create HTML/CSS of an analog clock showing ${time}. Include numbers (or numerals) if you wish, and have a CSS animated second hand. Make it responsive and use a white background. Return ONLY the HTML/CSS code with no markdown formatting.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I have observed for a long time that the Kimi K2 is the only model that can maintain 12 digits in the correct clock positions, even with the second hand perfectly aligned with the actual time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T17:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxw1rf</id>
    <title>‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú</title>
    <updated>2025-11-15T16:30:40+00:00</updated>
    <author>
      <name>/u/Birchi</name>
      <uri>https://old.reddit.com/user/Birchi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt; &lt;img alt="‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú" src="https://a.thumbs.redditmedia.com/qc9sVTXit2tBCYT5qsuH3I3XzrNQcOniJHSAWtLWmA4.jpg" title="‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.. the AI at home. I figured you guys would appreciate this more than my irl peeps :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birchi"&gt; /u/Birchi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oxw1rf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T16:30:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyj16n</id>
    <title>Is it possible we ever get CPU native LLMs?</title>
    <updated>2025-11-16T11:05:47+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Besides small models, quantization and current Bitnets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyj16n/is_it_possible_we_ever_get_cpu_native_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyj16n/is_it_possible_we_ever_get_cpu_native_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyj16n/is_it_possible_we_ever_get_cpu_native_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T11:05:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy9w39</id>
    <title>Do we need a language model torrent index?</title>
    <updated>2025-11-16T02:20:43+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like a pirate bay of AI models. I don't see myself downloading from it much, but in the event hugging face gets bought out, openai/anthropic get what they want, or third unknown option it might be better to have an existing community hosted option than to scramble to make 1 hundred and then all being pretty bad.&lt;/p&gt; &lt;p&gt;Does this exist yet? Do you see yourself using it preregulation? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy9w39/do_we_need_a_language_model_torrent_index/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy9w39/do_we_need_a_language_model_torrent_index/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy9w39/do_we_need_a_language_model_torrent_index/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T02:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyaz1h</id>
    <title>I think I'm falling in love with how good mistral is as an AI. Like it's 8b-7b variants are just so much more dependable and good compared to qwen or something like llama. But the benchmarks show the opposite. How does one find good models if this is the state of benchmarks?</title>
    <updated>2025-11-16T03:14:57+00:00</updated>
    <author>
      <name>/u/Xanta_Kross</name>
      <uri>https://old.reddit.com/user/Xanta_Kross</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As I said above mistral is really good.&lt;br /&gt; - It follows instructions very well&lt;br /&gt; - doesn't hallucinate (almost zero)&lt;br /&gt; - gives short answers for short questions and long answers for properly long questions&lt;br /&gt; - is tiny compared to SOTA while also feeling like I'm talking to something actually intelligent rather than busted up keyword prediction&lt;/p&gt; &lt;p&gt;But the benchmarks of it don't show it as impressive as phi4 or phi3 even, Qwen3, Qwen2 vl etc also. Putting it insanely lower than them. Like this is insane how awful the current benchmarks are. Completely skewed.&lt;/p&gt; &lt;p&gt;I want to find more models like these. How do you guys find models like these, when the benchmarks are so badly skewed?&lt;/p&gt; &lt;p&gt;EDIT 1:&lt;br /&gt; - Some have suggested curating our own small personal benchmark without leaking it on the internet. To test our local LLMs&lt;br /&gt; - Check out &lt;a href="/u/lemon07r"&gt;u/lemon07r&lt;/a&gt; 's answer for details they have specifically laid out a plan how they test out their models (Using terminal bench 2, sam paech's slop scoring)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xanta_Kross"&gt; /u/Xanta_Kross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyaz1h/i_think_im_falling_in_love_with_how_good_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyaz1h/i_think_im_falling_in_love_with_how_good_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyaz1h/i_think_im_falling_in_love_with_how_good_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T03:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyjzjb</id>
    <title>New BERT-based Multilingual Chunking Model</title>
    <updated>2025-11-16T12:00:49+00:00</updated>
    <author>
      <name>/u/LMLocalizer</name>
      <uri>https://old.reddit.com/user/LMLocalizer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://github.com/mirth/chonky"&gt;chonky&lt;/a&gt;, I fine-tuned &lt;a href="https://huggingface.co/distilbert/distilbert-base-multilingual-cased"&gt;distilbert/distilbert-base-multilingual-cased&lt;/a&gt; on nearly 11 billion tokens from more than 34 million Wikipedia articles to predict paragraph breaks. The resulting model can be used to split arbitrary natural language texts into semantic chunks.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased"&gt;https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Trained on 104 languages&lt;/li&gt; &lt;li&gt;Fast inference and low memory usage without requiring flash attention&lt;/li&gt; &lt;li&gt;Can process texts of arbitrary length with constant VRAM usage&lt;/li&gt; &lt;li&gt;Runs acceptably on CPU if needed&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Known limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Only trained on natural language: Performance on mathematical expressions or code has not been tested.&lt;/li&gt; &lt;li&gt;Sometimes splits the items of numbered lists into separate chunks.&lt;/li&gt; &lt;li&gt;If a text contains a captioned table, the caption and the table may be split into separate chunks.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;License&lt;/h1&gt; &lt;p&gt;The model is released under Apache 2.0 and fully open source.&lt;/p&gt; &lt;h1&gt;How to use&lt;/h1&gt; &lt;p&gt;See &lt;a href="https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased#how-to-get-started-with-the-model"&gt;https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased#how-to-get-started-with-the-model&lt;/a&gt;&lt;br /&gt; I recommend using my fork of chonky, as it provides faster inference and improved post-processing.&lt;/p&gt; &lt;h1&gt;Collections of related chunking models&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/mamei16/paragraph-splitting-chunking-models"&gt;https://huggingface.co/collections/mamei16/paragraph-splitting-chunking-models&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/collections/mirth/text-chunking-splitting-models"&gt;https://huggingface.co/collections/mirth/text-chunking-splitting-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LMLocalizer"&gt; /u/LMLocalizer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjzjb/new_bertbased_multilingual_chunking_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjzjb/new_bertbased_multilingual_chunking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjzjb/new_bertbased_multilingual_chunking_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T12:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oypwa7</id>
    <title>A more surgical approach to abliteration</title>
    <updated>2025-11-16T16:22:15+00:00</updated>
    <author>
      <name>/u/grimjim</name>
      <uri>https://old.reddit.com/user/grimjim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abliteration is known to be damaging to models. I had a think about why, and decided to explore ways to eliminate as many possible disruptions to model performance when following the harmless direction. In short, if it ain't broke, don't fix it.&lt;/p&gt; &lt;p&gt;The first insight after some cosine-similarity analysis was that there was entanglement between the refusal direction and the harmless direction, during measurement, and potentially with the harmless direction of a different target layer. The fix was to project the refusal direction onto the harmless direction (Gram-Schmidt), then subtract that contribution, leaving only the orthogonal component to refusal.&lt;/p&gt; &lt;p&gt;The results of my two experiments:&lt;br /&gt; &lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-projection-abliterated"&gt;https://huggingface.co/grimjim/gemma-3-12b-it-projection-abliterated&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-biprojected-abliterated"&gt;https://huggingface.co/grimjim/gemma-3-12b-it-biprojected-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I then went further and opted to preserve norms when ablating from residual streams, decoupling direction from magnitiude. This meant that the intervention (subtraction of the refusal direction) was limited to only the directional component, in principle. I uploaded weights for the combined interventions to HF back on November 5:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated"&gt;https://huggingface.co/grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had my models benchmarked on the UGI leaderboard:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The relevant benchmark results:&lt;/p&gt; &lt;p&gt;| Model | UGI | W/10 | NatInt | Writing |&lt;/p&gt; &lt;p&gt;| google/gemma-3-12b-it | 19.58 | 3 | 18.72 | 29.86 | | grimjim/gemma-3-12b-it-abliterated | 32.08 | 9 | 18.65 | 27.64 | | grimjim/gemma-3-12b-it-projection-abliterated | 30.77 | 9.8 | 19.21 | 29.46 | | grimjim/gemma-3-12b-it-biprojected-abliterated | 29.97 | 9.2 | 21.06 | 30.76 | | grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated | 32.61 | 9.2 | 21.33 | 30.43 |&lt;/p&gt; &lt;p&gt;Based on these results, I was able to induce strong compliance over the original gemma-3-12b-it model, which is basic abliteration success. Plain abliteration showed evidence of the expected damage compared to the original Instruct model, a reduction in natural intelligence and writing quality benchmarks. My final combined surgical approach to abliteration provided most of the prior boost to compliance, but elevated NatInt significantly over the original Instruct model and demonstrated a higher writing benchmark as well. This appears to demonstrate a performance gain due to refund of the alignment/safety tax that models pay for paying attention to refusal. This also implies that abliteration approaches which minimize KL divergence from the pre-intervention model may miss out on any uplift when the model no longer has to trade off reasoning for safety.&lt;/p&gt; &lt;p&gt;I blogged about the math behind my modifications to abliteration here: &lt;a href="https://huggingface.co/blog/grimjim/projected-abliteration"&gt;https://huggingface.co/blog/grimjim/projected-abliteration&lt;/a&gt; &lt;a href="https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration"&gt;https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper discussing the reasoning versus safety trade-off: &lt;a href="https://arxiv.org/abs/2503.00555"&gt;https://arxiv.org/abs/2503.00555&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some may find it surprising that measuring activations on the 4-bit bitsandbytes quant sufficed in determining effective mean directions for abliterating the full-weight model; I attribute this to quantization error roughly cancelling out given the number of prompts per direction. The harmful and harmless directions were also initially difficult to discern after generating one token, with a cosine similarity very near unity, but this was resolved by Winsorizing, clipping peak activations to magnitude factor of 0.995, revealing a clear refusal direction. (Therefore Gemma 3 12B Instruct is characterized by a few large outlier activatons.) A VRAM budget of 16GB was sufficient to perform all tasks for the above models.&lt;/p&gt; &lt;p&gt;My forked and customized workflow can be found on Github:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jim-plus/llm-abliteration/"&gt;https://github.com/jim-plus/llm-abliteration/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grimjim"&gt; /u/grimjim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T16:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyn66k</id>
    <title>Bro and I thought I was an overthinker! vibeTHINKER on LM studio with no instructions.</title>
    <updated>2025-11-16T14:31:38+00:00</updated>
    <author>
      <name>/u/Sufficient-Brain-371</name>
      <uri>https://old.reddit.com/user/Sufficient-Brain-371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn66k/bro_and_i_thought_i_was_an_overthinker/"&gt; &lt;img alt="Bro and I thought I was an overthinker! vibeTHINKER on LM studio with no instructions." src="https://external-preview.redd.it/dGtpazk4Y3dwbTFnMSvUbfez8WIstXtj7zq-QhlQCt1-ymJnEFr5PFe7y4k1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8caea055bff49cbb5e99a537fe95aee32294d5d2" title="Bro and I thought I was an overthinker! vibeTHINKER on LM studio with no instructions." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Brain-371"&gt; /u/Sufficient-Brain-371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/05u315bwpm1g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn66k/bro_and_i_thought_i_was_an_overthinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn66k/bro_and_i_thought_i_was_an_overthinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T14:31:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyknf1</id>
    <title>Finally a good use case for your local setups</title>
    <updated>2025-11-16T12:35:53+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"&gt; &lt;img alt="Finally a good use case for your local setups" src="https://preview.redd.it/o4xqvpnu5m1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fd2d24bd26a0123407252f5bef5deaf159eb2f" title="Finally a good use case for your local setups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bbc.com/news/articles/c0rpy7envr5o"&gt;https://www.bbc.com/news/articles/c0rpy7envr5o&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4xqvpnu5m1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T12:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oymku1</id>
    <title>Heretic: Fully automatic censorship removal for language models</title>
    <updated>2025-11-16T14:05:58+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt; &lt;img alt="Heretic: Fully automatic censorship removal for language models" src="https://preview.redd.it/jcu64fczhm1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e2b8c9de3a21ed0998e9175b01245cbef331f9a" title="Heretic: Fully automatic censorship removal for language models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear fellow Llamas, your time is precious, so I won't waste it with a long introduction. I have developed a program that can automatically remove censorship (aka &amp;quot;alignment&amp;quot;) from many language models. I call it Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;If you have a Python environment with the appropriate version of PyTorch for your hardware installed, all you need to do in order to decensor a model is run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install heretic-llm heretic Qwen/Qwen3-4B-Instruct-2507 &amp;lt;--- replace with model of your choice &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;That's it!&lt;/em&gt; No configuration, no Jupyter, no parameters at all other than the model name.&lt;/p&gt; &lt;p&gt;Heretic will&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Load the model using a fallback mechanism that automatically finds a dtype that works with your setup&lt;/li&gt; &lt;li&gt;Load datasets containing &amp;quot;harmful&amp;quot; and &amp;quot;harmless&amp;quot; example prompts&lt;/li&gt; &lt;li&gt;Benchmark your system to determine the optimal batch size for maximum evaluation speed on your hardware&lt;/li&gt; &lt;li&gt;Perform directional ablation (aka &amp;quot;abliteration&amp;quot;) driven by a TPE-based stochastic parameter optimization process that &lt;strong&gt;automatically&lt;/strong&gt; finds abliteration parameters that minimize both refusals and KL divergence from the original model&lt;/li&gt; &lt;li&gt;Once finished, give you the choice to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="right"&gt;Refusals for &amp;quot;harmful&amp;quot; prompts&lt;/th&gt; &lt;th align="right"&gt;KL divergence from original model for &amp;quot;harmless&amp;quot; prompts&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;google/gemma-3-12b-it (original)&lt;/td&gt; &lt;td align="right"&gt;97/100&lt;/td&gt; &lt;td align="right"&gt;0 &lt;em&gt;(by definition)&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mlabonne/gemma-3-12b-it-abliterated-v2&lt;/td&gt; &lt;td align="right"&gt;3/100&lt;/td&gt; &lt;td align="right"&gt;1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;huihui-ai/gemma-3-12b-it-abliterated&lt;/td&gt; &lt;td align="right"&gt;3/100&lt;/td&gt; &lt;td align="right"&gt;0.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;p-e-w/gemma-3-12b-it-heretic (ours)&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;3/100&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;0.16&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, the Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities.&lt;/p&gt; &lt;p&gt;Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems.&lt;/p&gt; &lt;p&gt;You can find a collection of models that have been decensored using Heretic &lt;a href="https://huggingface.co/collections/p-e-w/the-bestiary"&gt;on Hugging Face&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jcu64fczhm1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T14:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
