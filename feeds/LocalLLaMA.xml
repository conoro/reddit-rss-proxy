<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-19T17:07:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r8pohi</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-02-19T04:31:57+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://preview.redd.it/12la8ajmpdkg1.png?width=140&amp;amp;height=90&amp;amp;auto=webp&amp;amp;s=cac74816a78676e7abbdfae0b17c0b819bb2629d" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3.5-397B-A17B - Native Vision-Language Foundation Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;397B-parameter MoE model (17B active) with hybrid linear attention and native multimodal integration.&lt;/li&gt; &lt;li&gt;Handles document parsing, chart analysis, and visual reasoning without a separate vision encoder.&lt;/li&gt; &lt;li&gt;&lt;a href="https://qwen.ai/blog?id=qwen3.5"&gt;Blog&lt;/a&gt; | &lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/12la8ajmpdkg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d39b1ea44a322f087f3b33e35564a96454f25c9"&gt;https://preview.redd.it/12la8ajmpdkg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d39b1ea44a322f087f3b33e35564a96454f25c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PersonaPlex-7B - Full-Duplex Voice Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA's 7B voice model that listens and speaks simultaneously with natural interruption support.&lt;/li&gt; &lt;li&gt;Eliminates turn-taking latency for real-time voice conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r8pohi/video/8f15ixwnpdkg1/player"&gt;https://reddit.com/link/1r8pohi/video/8f15ixwnpdkg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax M2.5 - Open-Source Productivity Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Frontier model tuned for coding, writing, and structured analysis.&lt;/li&gt; &lt;li&gt;Prioritizes instruction-following accuracy over open-ended chat.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/on0tek5qpdkg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0988ea083b38e580baf2961778187892fd50517a"&gt;https://preview.redd.it/on0tek5qpdkg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0988ea083b38e580baf2961778187892fd50517a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepGen 1.0 - 5B Unified Multimodal Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lightweight model with native visual understanding built into the architecture.&lt;/li&gt; &lt;li&gt;Small enough for consumer hardware.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/deepgenteam/DeepGen-1.0"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m1yn8xxrpdkg1.png?width=2376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b56d294a054b3e38244bdcf0e988abc61a8ffbf"&gt;https://preview.redd.it/m1yn8xxrpdkg1.png?width=2376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b56d294a054b3e38244bdcf0e988abc61a8ffbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-TTS - 1.7B Speech Synthesis&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean, natural speech synthesis with custom voice support.&lt;/li&gt; &lt;li&gt;Open weights from Qwen.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r8pohi/video/qg4slbrvpdkg1/player"&gt;https://reddit.com/link/1r8pohi/video/qg4slbrvpdkg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KaniTTS2 - 400M TTS in 3GB VRAM&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-source text-to-speech that runs on modest local hardware.&lt;/li&gt; &lt;li&gt;400M parameters, optimized for local deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MioTTS-2.6B - Fast English/Japanese TTS&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lightweight text-to-speech optimized for inference speed.&lt;/li&gt; &lt;li&gt;Supports English and Japanese out of the box.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Aratako/MioTTS-2.6B"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ming-flash-omni 2.0 - Multimodal Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;New open multimodal model from InclusionAI.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-2.0"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SoulX-Singer - Zero-Shot Singing Voice Synthesis&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High-quality singing voice synthesis with no fine-tuning required.&lt;/li&gt; &lt;li&gt;Open-source with code on GitHub.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Soul-AILab/SoulX-Singer/tree/main"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/Soul-AILab/SoulX-Singer"&gt;Hugging&lt;/a&gt; Face&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ewez41tzpdkg1.png?width=1016&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9614a31ecd2dd373b2abddd730eee0d4c52cedaa"&gt;https://preview.redd.it/ewez41tzpdkg1.png?width=1016&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9614a31ecd2dd373b2abddd730eee0d4c52cedaa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/last-week-in-multimodal-ai-45-no?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;p&gt;* I was delayed this week but normally i post these roundups on Mondays&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1r8pftg"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T04:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r94192</id>
    <title>Temporary access to Ryzen AI Max 395 (128GB) to test real-world local LLM workflows</title>
    <updated>2026-02-19T16:38:15+00:00</updated>
    <author>
      <name>/u/lazy-kozak</name>
      <uri>https://old.reddit.com/user/lazy-kozak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m considering a Ryzen AI Max 395 (128GB) (most likely Framework Desktop) for local models for coding, but I’d like to test it in my real coding workflows before buying.&lt;br /&gt; Only need short-term access (a weekend or a few days), I guess API key for LM Studio will be enough.&lt;/p&gt; &lt;p&gt;Or maybe anyone knows a company that has a VPS on a Ryzen AI Max 395? I'd rent one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lazy-kozak"&gt; /u/lazy-kozak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94192/temporary_access_to_ryzen_ai_max_395_128gb_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94192/temporary_access_to_ryzen_ai_max_395_128gb_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r94192/temporary_access_to_ryzen_ai_max_395_128gb_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T16:38:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r94lv2</id>
    <title>microgpt playground: Build, train, and run LLMs — directly in your browser</title>
    <updated>2026-02-19T16:59:08+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"&gt; &lt;img alt="microgpt playground: Build, train, and run LLMs — directly in your browser" src="https://external-preview.redd.it/YnNxc2ZxZGllaGtnMbLIzqnNOijabBHIPuWpkRNlVyT41oFEP2h_i--AGtUk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5238c6e57cfb3b7fc15857a41ebdd741d10e22f" title="microgpt playground: Build, train, and run LLMs — directly in your browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Andrej Karpathy's microgpt, I built an educational neural network builder that breaks down &amp;quot;mysterious&amp;quot; LLMs into their primitive components. The goal is to teach people how LLMs are built, by constructing them from the ground up (and then modifying nodes, adding connections, and rewiring the graph). This is mainly just a fun experiment, but maybe there's interest in tooling like this.&lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/webml-community/microgpt-playground"&gt;https://huggingface.co/spaces/webml-community/microgpt-playground&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gikcumdiehkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T16:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r86i3o</id>
    <title>LLMs grading other LLMs 2</title>
    <updated>2026-02-18T15:47:24+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt; &lt;img alt="LLMs grading other LLMs 2" src="https://preview.redd.it/rmq2mwriw9kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07e6fa12e92be2b51d119d6c78ac4e28ccf7e1cb" title="LLMs grading other LLMs 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A year ago I made a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;meta-eval here on the sub&lt;/a&gt;, asking LLMs to grade a few criterias about other LLMs. &lt;/p&gt; &lt;p&gt;Time for the part 2.&lt;/p&gt; &lt;p&gt;The premise is very simple, the model is asked a few ego-baiting questions and other models are then asked to rank it. The scores in the pivot table are normalised.&lt;/p&gt; &lt;p&gt;You can find &lt;a href="https://huggingface.co/datasets/av-codes/cringebench"&gt;all the data on HuggingFace&lt;/a&gt; for your analysis.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rmq2mwriw9kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r91lsj</id>
    <title>A CLI tool to audit vector embeddings!</title>
    <updated>2026-02-19T15:07:19+00:00</updated>
    <author>
      <name>/u/gvij</name>
      <uri>https://old.reddit.com/user/gvij</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Working with embeddings (RAG, semantic search, clustering, recommendations, etc.), means:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate embeddings&lt;/li&gt; &lt;li&gt;Compute cosine similarity&lt;/li&gt; &lt;li&gt;Run retrieval&lt;/li&gt; &lt;li&gt;Hope it &amp;quot;works&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But I stumbled upon the issue of not being able to determine why my RAG responses felt off, retrieval quality being inconsistent and clustering results looked weird.&lt;/p&gt; &lt;p&gt;Debugging embeddings was painful.&lt;/p&gt; &lt;p&gt;To solve this issue, we built this Embedding evaluation CLI tool to &lt;strong&gt;audit embedding spaces&lt;/strong&gt;, not just generate them.&lt;/p&gt; &lt;p&gt;Instead of guessing whether your vectors make sense, it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Detects semantic outliers&lt;/li&gt; &lt;li&gt;Identifies cluster inconsistencies&lt;/li&gt; &lt;li&gt;Flags global embedding collapse&lt;/li&gt; &lt;li&gt;Highlights ambiguous boundary tokens&lt;/li&gt; &lt;li&gt;Generates heatmaps and cluster visualizations&lt;/li&gt; &lt;li&gt;Produces structured reports (JSON / Markdown)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Checkout the tool and feel free to share your feedback:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dakshjain-1616/Embedding-Evaluator"&gt;https://github.com/dakshjain-1616/Embedding-Evaluator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is especially useful for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG pipelines&lt;/li&gt; &lt;li&gt;Vector DB systems&lt;/li&gt; &lt;li&gt;Semantic search products&lt;/li&gt; &lt;li&gt;Embedding model comparisons&lt;/li&gt; &lt;li&gt;Fine-tuning experiments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It surfaces structural problems in the geometry of your embeddings before they break your system downstream.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gvij"&gt; /u/gvij &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91lsj/a_cli_tool_to_audit_vector_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91lsj/a_cli_tool_to_audit_vector_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r91lsj/a_cli_tool_to_audit_vector_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T15:07:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8xw1j</id>
    <title>Built a music generation app that runs 100% on-device using Apple's MLX framework no cloud, no API calls</title>
    <updated>2026-02-19T12:26:46+00:00</updated>
    <author>
      <name>/u/tarunyadav9761</name>
      <uri>https://old.reddit.com/user/tarunyadav9761</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8xw1j/built_a_music_generation_app_that_runs_100/"&gt; &lt;img alt="Built a music generation app that runs 100% on-device using Apple's MLX framework no cloud, no API calls" src="https://external-preview.redd.it/MXBieWV6aXQyZ2tnMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa1b34a4f0817d69092023342fc8ca95d676b85b" title="Built a music generation app that runs 100% on-device using Apple's MLX framework no cloud, no API calls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been following local AI discussions here for a while and wanted to share something I built that fits the ethos of this community pretty well.&lt;/p&gt; &lt;p&gt;I got frustrated with every AI music tool being cloud-based Suno, Stable Audio, AIVA all sending your prompts to their servers, all requiring monthly subscriptions. The moment you stop paying, your workflow breaks.&lt;/p&gt; &lt;p&gt;So I built LoopMaker. It runs entirely on your Mac using Apple's MLX framework. After the initial model download, zero internet required. Nothing leaves your device.&lt;/p&gt; &lt;p&gt;Here's what the stack looks like under the hood:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built natively in Swift for macOS&lt;/li&gt; &lt;li&gt;Uses Apple's MLX framework for on-device inference&lt;/li&gt; &lt;li&gt;Runs fast on M-series chips (M1/M2/M3/M4) generation is actually usable, not 5 minutes per track&lt;/li&gt; &lt;li&gt;Supports up to 4-minute tracks with optional lyrics and vocals&lt;/li&gt; &lt;li&gt;6 genre modes: Lo-Fi, Cinematic, Ambient, Electronic, Hip-Hop, Jazz&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The local AI music generation space is still pretty early compared to LLMs curious if anyone here has experimented with this or knows of other approaches people are using for on-device audio generation.&lt;/p&gt; &lt;p&gt;Happy to go deep on the technical side if anyone's interested.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://tarun-yadav.com/loopmaker"&gt;https://tarun-yadav.com/loopmaker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarunyadav9761"&gt; /u/tarunyadav9761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2vw0xoit2gkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8xw1j/built_a_music_generation_app_that_runs_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8xw1j/built_a_music_generation_app_that_runs_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T12:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8smbk</id>
    <title>Local VLMs (Qwen 3 VL) for document OCR with bounding box detection for PII detection/redaction workflows (blog post and open source app)</title>
    <updated>2026-02-19T07:13:57+00:00</updated>
    <author>
      <name>/u/Sonnyjimmy</name>
      <uri>https://old.reddit.com/user/Sonnyjimmy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"&gt; &lt;img alt="Local VLMs (Qwen 3 VL) for document OCR with bounding box detection for PII detection/redaction workflows (blog post and open source app)" src="https://preview.redd.it/1pwglerfhekg1.jpg?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=635f2955ab9e9dd86f185894924ee6bbe9da3633" title="Local VLMs (Qwen 3 VL) for document OCR with bounding box detection for PII detection/redaction workflows (blog post and open source app)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://seanpedrick-case.github.io/doc_redaction/src/redaction_with_vlm_and_llms.html"&gt;Blog post link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A while ago I made a post here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; asking about using local VLMs for OCR in PII detection/redaction processes for documents (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kspe8c/best_local_model_ocr_solution_for_pdf_document/"&gt;here&lt;/a&gt;). The document redaction process differs from other OCR processes in that we need to identify the bounding boxes of words on the page, as well as the text content, to successfully redact the document.&lt;/p&gt; &lt;p&gt;I have now implemented OCR with bounding box detection into the &lt;a href="https://github.com/seanpedrick-case/doc_redaction"&gt;Document redaction app&lt;/a&gt; I have been working on. The VLM models help with OCR either 1. to extract all text and bounding boxes from the page directly or 2. in combination with a 'traditional' OCR model (PaddleOCR), where Paddle first pulls out accurate line-level bounding boxes, then passes words with low confidence to the VLM in a hybrid approach.&lt;/p&gt; &lt;p&gt;I wanted to use small VLM models such as Qwen 3 VL 8B Instruct for this task to see whether local models that can fit in consumer grade GPUs (i.e. 24GB VRAM or less) could be used for redaction tasks.&lt;/p&gt; &lt;p&gt;My experiments with using VLMs in the redaction OCR process are demonstrated in &lt;a href="https://seanpedrick-case.github.io/doc_redaction/src/redaction_with_vlm_and_llms.html"&gt;this blog post&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pwglerfhekg1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5f443be8011738ed0e186ff06a42602ea399881b"&gt;Unclear text on handwritten note analysed with hybrid PaddleOCR + Qwen 3 VL 8B Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All the examples can be replicated using this &lt;a href="https://huggingface.co/spaces/seanpedrickcase/document_redaction_vlm"&gt;Hugging Face space for free&lt;/a&gt;. The code for the underlying Document Redaction app is available for anyone to view and use, and can be found &lt;a href="https://github.com/seanpedrick-case/doc_redaction"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;My blog post used Qwen 3 VL 8B Instruct as the small VLM for OCR. My conclusion at the moment is that the hybrid PaddleOCR + Qwen 3 VL approach is better than the pure VLM approach for 'difficult' handwritten documents. However, both approaches are not quite there for perfect accuracy.&lt;/p&gt; &lt;p&gt;This conclusion may soon change with the imminent release of the Qwen 3.5 VL models, after which I will redo my analysis and post about it here.&lt;/p&gt; &lt;p&gt;The blog post also shows how VLMs can be used for detecting signatures, and PII in images such as people's faces. I also demonstrate how mid-level local LLMs of ~30GB parameter size (Gemma 27B) can be used to detect custom entities in document text.&lt;/p&gt; &lt;p&gt;Any comments on the approach or the app in general are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sonnyjimmy"&gt; /u/Sonnyjimmy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r90vlx</id>
    <title>Models for FPGA coding?</title>
    <updated>2026-02-19T14:38:28+00:00</updated>
    <author>
      <name>/u/jardin14zip</name>
      <uri>https://old.reddit.com/user/jardin14zip</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to figure out where LLMs can be used for FPGA development. For context, I'm doing research for data acquisition in particle detectors. I've been playing with various models (mostly open but also some proprietary for comparison) to see if they can generate FPGA code (VHDL and/or SystemVerilog). I've only experimented with small components (e.g. &amp;quot;make me a gearbox component in VHDL that will convert 48b frames @ 40 MHz into 32b frames @ 60 MHz&amp;quot;), so nothing where multiple components need to talk to each other. My experience is that at the smaller level (&amp;lt; 100B), LLMs can generate good boilerplate, but the algorithms can be wrong, but they often write a decent testbench. At a larger level (500B+) you tend to get better results for the algorithms. Very model dependent though - some models produce total jank or even just don't go anywhere. GLM4.7 has been my go to, in general, but GPT 5.2 will give solid code (but not open, so booo!).&lt;/p&gt; &lt;p&gt;I'm going to try and do some more serious benchmarking, but interested if there are more in the community with experience here. There are plenty of people doing FPGA development (and ASIC development since it's also SystemVerilog mostly), but the tools are quite immature compared to CPU/GPU land. This goes for the compilers themselves as well as code generation with LLMs. It's an area in need of more open source love, but the cost of the devices is a barrier to entry.&lt;/p&gt; &lt;p&gt;I guess I'm trying to understand the answers to these questions:&lt;/p&gt; &lt;p&gt;- Are LLMs trained on more common languages for training and if more niche languages like VHDL are excluded from training sets?&lt;/p&gt; &lt;p&gt;- Are niche languages more likely to suffer with smaller quants?&lt;/p&gt; &lt;p&gt;- Do you know any (smaller) models particularly good at these languages?&lt;/p&gt; &lt;p&gt;- Do benchmarks exist for niche languages? Everything seems to be python + javascript++&lt;/p&gt; &lt;p&gt;Loving this community. I've learned so much in the last few months. PM me if you want more info on my experience with AI FPGA coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jardin14zip"&gt; /u/jardin14zip &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90vlx/models_for_fpga_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90vlx/models_for_fpga_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r90vlx/models_for_fpga_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1r83irw</id>
    <title>PSA: DDR5 RDIMM price passed the point were 3090 are less expensive per gb..</title>
    <updated>2026-02-18T13:51:04+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;Just wanted to note that RDIMM prices are so wild.. Stacking rdimms starts to be as expensive as stacking 3090s.. But RDIMM don't come with compute included..&lt;/p&gt; &lt;p&gt;What a crazy time, shall we stack rdimms or 3090, what's your take on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r90tab</id>
    <title>Neofold, an idle creature-collector with infinite pets thanks to a local diffusion model</title>
    <updated>2026-02-19T14:35:52+00:00</updated>
    <author>
      <name>/u/enricowereld</name>
      <uri>https://old.reddit.com/user/enricowereld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90tab/neofold_an_idle_creaturecollector_with_infinite/"&gt; &lt;img alt="Neofold, an idle creature-collector with infinite pets thanks to a local diffusion model" src="https://external-preview.redd.it/tv-FRLBNweg_rGRTS20pBKCEL48OpsHgkDLjIAb3IV0.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce393697b71be1f680297625713a29750b336c86" title="Neofold, an idle creature-collector with infinite pets thanks to a local diffusion model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/enricowereld"&gt; /u/enricowereld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://store.steampowered.com/app/4412590/Neofold/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90tab/neofold_an_idle_creaturecollector_with_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r90tab/neofold_an_idle_creaturecollector_with_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8rgcp</id>
    <title>Minimax 2.5 on Strix Halo Thread</title>
    <updated>2026-02-19T06:06:20+00:00</updated>
    <author>
      <name>/u/Equivalent-Belt5489</name>
      <uri>https://old.reddit.com/user/Equivalent-Belt5489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I just tried out Minimax 2.5 on headless Fedora 43 with the kyuz0 rocm nightlies toolbox, Jan 26 firmware, 6.18.9 Kernel, &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/a&gt; there are some changes necessary so it fits in the RAM. Using MiniMax-M2.5-Q3_K_M there is just enough RAM for approx 80k. The quality is really impressive! But its slow! Its almost not usabe, but the quality is so great I would like to continue with it. &lt;/p&gt; &lt;p&gt;Do you have any tips or do you have a faster setup?&lt;/p&gt; &lt;p&gt;I use now this: &lt;code&gt;export HIP_VISIBLE_DEVICES=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_VISIBLE_DEVICES=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_ENABLE_DEVICE_MALLOC=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_ENABLE_UNIFIED_MEMORY=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HSA_OVERRIDE_GFX_VERSION=11.5.1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_FORCE_DEV_KERNARG=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export GGML_HIP_UMA=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_HOST_COHERENT=0&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_TRACE_API=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_LAUNCH_BLOCKING=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m /run/host/data/models/MiniMax-M2.5-Q3_K_M-00001-of-00004.gguf -fa on --no-mmap -c 66600 -ub 1024 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080 --jinja -ngl 99&lt;/code&gt; &lt;/p&gt; &lt;p&gt;However its quite slow, if I let it run longer and with more context i get results like: pp 43 t/s, tg 3 t/s...&lt;/p&gt; &lt;p&gt;In the very beginning with 17k kontext&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 81128.69 ms / 17363 tokens ( 4.67 ms per token, 214.02 tokens per second) eval time = 21508.09 ms / 267 tokens ( 80.55 ms per token, 12.41 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;after 8 toolusages and with 40k context&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 25168.38 ms / 1690 tokens ( 14.89 ms per token, 67.15 tokens per second) eval time = 21207.71 ms / 118 tokens ( 179.73 ms per token, 5.56 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;after long usage its getting down to where it stays (still 40 k context)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 13968.84 ms / 610 tokens ( 22.90 ms per token, 43.67 tokens per second) eval time = 24516.70 ms / 82 tokens ( 298.98 ms per token, 3.34 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;llama-bench&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m /run/host/data/models/MiniMax-M2.5-Q3_K_M-00001-of-00004.gguf -ngl 99 -fa on -ngl 99 ggml_cuda_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | pp512 | 200.82 ± 1.38 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | tg128 | 27.27 ± 0.01 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | pp512 | 200.38 ± 1.53 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | tg128 | 27.27 ± 0.00 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the kyuz vulkan radv toolbox:&lt;/p&gt; &lt;p&gt;The pp is 30% slower, tg a bit faster.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m /run/host/data/models/MiniMax-M2.5-Q3_K_M-00001-of-00004.gguf -ngl 99 -fa on -ngl 99 ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | pp512 | 157.18 ± 1.29 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | tg128 | 32.37 ± 1.67 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | pp512 | 176.17 ± 0.85 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | tg128 | 33.09 ± 0.03 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I try now the Q3_K_XL. I doubt it will improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Belt5489"&gt; /u/Equivalent-Belt5489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8rgcp/minimax_25_on_strix_halo_thread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8rgcp/minimax_25_on_strix_halo_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8rgcp/minimax_25_on_strix_halo_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T06:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r93i0p</id>
    <title>Local iOS voice to text app (alternative to Wispr Flow)</title>
    <updated>2026-02-19T16:18:23+00:00</updated>
    <author>
      <name>/u/Impressive-Sir9633</name>
      <uri>https://old.reddit.com/user/Impressive-Sir9633</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r93i0p/local_ios_voice_to_text_app_alternative_to_wispr/"&gt; &lt;img alt="Local iOS voice to text app (alternative to Wispr Flow)" src="https://external-preview.redd.it/cWpnOWU0cjg4aGtnMWS5e3158whTYNSe1GEK61Oq_uqxznQSR6QLvGe1g5lP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af142f52bab47ba63ad4da62327e9b1aa2b97b47" title="Local iOS voice to text app (alternative to Wispr Flow)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually dictate for 2 to 3 hours everyday in Dragon dictation and until recently used Wispr Flow on my personal devices. Over the last few months, I realized that local Al models can give you the same quality as Wispr Flow with complete privacy and without the ongoing subscription cost. So I built an iOS app, a MacOS app and an Android app.&lt;/p&gt; &lt;p&gt;Testflight link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/e5pcxwyq"&gt;https://testflight.apple.com/join/e5pcxwyq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am happy to offer the app for free to people who offer useful feedback for the test flight app.&lt;/p&gt; &lt;p&gt;We also have a MacOS app with local processing. If desired, users can sync their snippets and dictionary using personal iCloud. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Sir9633"&gt; /u/Impressive-Sir9633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ft3amnq88hkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r93i0p/local_ios_voice_to_text_app_alternative_to_wispr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r93i0p/local_ios_voice_to_text_app_alternative_to_wispr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T16:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8ta57</id>
    <title>I retrained /u/Own-Albatross868's FlashLM v4 "Bolt" model from scratch using GreedyPhrase tokenizer on the full TinyStories dataset. I scaled up to 15M parameters with a 65K vocab, achieving smooth convergence and coherent story generation in just 2.2 hours on an RTX 2080 Ti</title>
    <updated>2026-02-19T07:54:51+00:00</updated>
    <author>
      <name>/u/reditzer</name>
      <uri>https://old.reddit.com/user/reditzer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FlashLM v4 &amp;quot;Bolt&amp;quot; retrained from scratch on the full TinyStories dataset using our &lt;a href="https://github.com/rayonnant-ai/greedyphrase"&gt;GreedyPhrase&lt;/a&gt; tokenizer instead of the original GPT-2 10K tokenizer.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;&lt;a href="https://huggingface.co/changcheng967/flashlm-v4-bolt"&gt;Original&lt;/a&gt;&lt;/th&gt; &lt;th&gt;&lt;a href="https://huggingface.co/rrezel/flashlm-v4-bolt-greedyphrase"&gt;This Run&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Tokenizer&lt;/td&gt; &lt;td&gt;GPT-2 (tiktoken), 10K vocab&lt;/td&gt; &lt;td&gt;GreedyPhrase, 65K vocab&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Parameters&lt;/td&gt; &lt;td&gt;4.3M&lt;/td&gt; &lt;td&gt;15.0M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hardware&lt;/td&gt; &lt;td&gt;2 vCPU (CPU only)&lt;/td&gt; &lt;td&gt;RTX 2080 Ti (GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training time&lt;/td&gt; &lt;td&gt;2 hours&lt;/td&gt; &lt;td&gt;~2.2 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tokens seen&lt;/td&gt; &lt;td&gt;10.6M (2.3% of data)&lt;/td&gt; &lt;td&gt;818M (3.3 epochs)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Best val loss&lt;/td&gt; &lt;td&gt;2.0976&lt;/td&gt; &lt;td&gt;3.9352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Throughput&lt;/td&gt; &lt;td&gt;1,479 tok/s&lt;/td&gt; &lt;td&gt;103,000 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Training Configuration&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Architecture&lt;/td&gt; &lt;td&gt;FlashLM v4 Bolt (ternary gated causal conv)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hidden dim&lt;/td&gt; &lt;td&gt;192&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Blocks&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Conv kernel size&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLU expansion dim&lt;/td&gt; &lt;td&gt;512&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vocab size&lt;/td&gt; &lt;td&gt;65,280 (padded from 65,218 actual)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sequence length&lt;/td&gt; &lt;td&gt;256 tokens&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Effective batch size&lt;/td&gt; &lt;td&gt;64 (micro=16, grad_accum=4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Optimizer&lt;/td&gt; &lt;td&gt;AdamW (weight_decay=0.01)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Peak learning rate&lt;/td&gt; &lt;td&gt;4e-3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LR schedule&lt;/td&gt; &lt;td&gt;Cosine with 500-step warmup&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gradient clipping&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Precision&lt;/td&gt; &lt;td&gt;AMP float16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Total steps&lt;/td&gt; &lt;td&gt;50,000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Dataset&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Source:&lt;/strong&gt; TinyStories (roneneldan/TinyStories), 2.1 GB text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Preprocessing:&lt;/strong&gt; &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; replaced with &lt;code&gt;&amp;lt;/s&amp;gt;&lt;/code&gt; (EOS token ID 3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tokenized size:&lt;/strong&gt; 248M tokens (496 MB binary uint16)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression ratio:&lt;/strong&gt; ~8.88 bytes/token (vs ~4.5 for GPT-2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Train/val split:&lt;/strong&gt; 99.5% / 0.5%&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;h3&gt;Loss Curve&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Step Train Loss Val Loss 0 11.13 — 500 6.73 5.96 1000 5.46 5.12 2500 4.72 4.61 5000 4.43 4.39 10000 4.17 4.19 20000 4.03 4.03 30000 3.95 3.97 40000 3.92 3.95 50000 3.94 3.94 Best — 3.9352 (step 47500) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Best validation loss&lt;/td&gt; &lt;td&gt;3.9352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Token-level perplexity&lt;/td&gt; &lt;td&gt;51.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bits per token&lt;/td&gt; &lt;td&gt;5.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bits per character (estimated)&lt;/td&gt; &lt;td&gt;0.64&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Comparing Val Loss Across Tokenizers&lt;/h3&gt; &lt;p&gt;The raw validation loss numbers are &lt;strong&gt;not directly comparable&lt;/strong&gt; between the original (val_loss 2.10 with 10K vocab) and this run (val_loss 3.94 with 65K vocab) because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Larger vocabulary = harder prediction task.&lt;/strong&gt; Random-chance loss is ln(65280) = 11.09 vs ln(10000) = 9.21. The model must distribute probability over 6.5x more tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fewer tokens per story.&lt;/strong&gt; GreedyPhrase compresses TinyStories at ~9 bytes/token vs ~4.5 bytes/token for GPT-2. Each token carries more information, so predicting the next token is inherently harder.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bits-per-character is the fair comparison.&lt;/strong&gt; At 0.64 BPC this model is competitive with the original's 0.88 BPC, suggesting the GreedyPhrase tokenizer's higher compression ratio pays off in information-theoretic efficiency.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Generation Samples (Step 49,500)&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time there was a little girl named Sarah. She was only three years old and loved exploring. One day Sarah went to the park with her mother. She saw a little boy playing with a ball.&lt;/p&gt; &lt;p&gt;Once upon a time there was a very deep lake. It was great! Every morning he would jump off the water and look for something wonderful.&lt;/p&gt; &lt;p&gt;Once upon a time there was a little girl named Mary. Mary loved animals, especially especially loved the ocean. Every day Mary would go out on a walk around the waves and swimming around on the beach.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3&gt;Prompt: &amp;quot;The little dog&amp;quot;&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;The little dog wanted to protect his bone, so he held it up to the cat and tried to protect him. But the big cat was jealous. It wanted to take the bone from him, but it ran away.&lt;/p&gt; &lt;p&gt;The cat was sad and began to cry. Then, he saw a big hole in the ground and started to shake it. The cat growled and tried to run away. The dog was scared and ran back to the cat. The cat saw the fox and was scared. The cat took the kitten and ran away. The dog was sad. The fox did not get the mitten anymore. The cat was happy and played with Spot and the other friends.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Files&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;File&lt;/th&gt; &lt;th&gt;Size&lt;/th&gt; &lt;th&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;flashlm_v4_bolt_greedyphrase.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;58 MB&lt;/td&gt; &lt;td&gt;Final model (step 50,000)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;best.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;172 MB&lt;/td&gt; &lt;td&gt;Best checkpoint with optimizer state (step 47,500)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;checkpoint.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;172 MB&lt;/td&gt; &lt;td&gt;Latest periodic checkpoint&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;tinystories.tokens&lt;/code&gt;&lt;/td&gt; &lt;td&gt;496 MB&lt;/td&gt; &lt;td&gt;Tokenized dataset (uint16 binary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;model.py&lt;/code&gt;&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;td&gt;Model architecture&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;train.py&lt;/code&gt;&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;td&gt;Training script&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Observations&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Convergence was smooth.&lt;/strong&gt; Loss dropped from 11.13 to ~3.94 over 50K steps with no instability, despite ternary weight quantization via straight-through estimators.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The loss curve was still slowly declining at 50K steps.&lt;/strong&gt; Extended training or a second cosine cycle could improve results further.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;GreedyPhrase's long phrases help coherence.&lt;/strong&gt; With ~9 bytes/token, the 256-token context window covers ~2,300 characters (~400 words), much more than the original's ~1,150 characters. This gives the model more context per sequence.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The larger embedding table dominates parameter count.&lt;/strong&gt; 65K vocab x 192 dim = 12.5M parameters in the embedding alone (84% of total), vs 1.9M for the original's 10K vocab. The model body (blocks) is identical.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Throughput benefited from GPU + AMP.&lt;/strong&gt; At 103K tokens/sec on an RTX 2080 Ti, this is 70x faster than the original's 1.5K tokens/sec on CPU, allowing 3.3 full epochs in roughly the same wall-clock time.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reditzer"&gt; /u/reditzer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8ectu</id>
    <title>I plugged a $30 radio into my Mac mini and told my AI "connect to this" — now I control my smart home and send voice messages over radio with zero internet</title>
    <updated>2026-02-18T20:30:14+00:00</updated>
    <author>
      <name>/u/anvarazizov</name>
      <uri>https://old.reddit.com/user/anvarazizov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;So I live in Ukraine during the war. Power goes out a lot here – russia regularly attacks our power grid. When it happens, internet dies, cell towers go dark, and suddenly all my smart home stuff and AI tools become useless. Got tired of it, so I did something kind of ridiculous.&lt;/p&gt; &lt;p&gt;I bought two Lilygo T-Echo radios (~$30 each, LoRa 433MHz, running Meshtastic firmware). Plugged one into my always-on Mac mini via USB. Took the other one as my portable radio. Then I opened up my OpenClaw AI agent and basically said: &amp;quot;hey, there's a Meshtastic radio plugged in. Figure it out.&amp;quot;&lt;/p&gt; &lt;p&gt;And it did.&lt;/p&gt; &lt;h1&gt;What happened next&lt;/h1&gt; &lt;p&gt;It identified the Meshtastic device, installed the CLI, configured an encrypted channel, and then – without me writing a single line of code – built a full Python listener daemon that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Monitors the radio 24/7 for incoming messages&lt;/li&gt; &lt;li&gt;Routes them intelligently: if internet is up, forwards to Discord where a cloud AI responds. If internet is down, routes everything to local models via Ollama&lt;/li&gt; &lt;li&gt;Uses phi4-mini as a lightweight intent classifier (&amp;quot;is this a smart home command or a question?&amp;quot;) and gemma3:12b for actual answers ()&lt;/li&gt; &lt;li&gt;Talks to Home Assistant so I can control lights, read sensors, check who's home — all over radio&lt;/li&gt; &lt;li&gt;Auto-chunks responses to fit the 200-char LoRa limit&lt;/li&gt; &lt;li&gt;Watches an outbox folder – if the AI needs to alert me about something (like a power outage), it drops a message file there and the listener transmits it over LoRa&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole thing just worked. The AI had already built the architecture while I was still thinking about how to approach it.&lt;/p&gt; &lt;h1&gt;The voice thing (this is the cool part)&lt;/h1&gt; &lt;p&gt;Then I added one more feature. If I prefix a Meshtastic message with &lt;code&gt;SAY:&lt;/code&gt;, the listener takes the text, calls Home Assistant's TTS service, and plays it through my HA Voice PE speaker at home. In Ukrainian.&lt;/p&gt; &lt;p&gt;So I can be walking around with a T-Echo in my pocket, completely off-grid, type &lt;code&gt;SAY: Привіт, я скоро буду вдома&lt;/code&gt; (Hi, I'll come back home soon) – and my house literally speaks. No internet anywhere in the chain. Just radio waves → Mac mini → TTS → speaker.&lt;/p&gt; &lt;p&gt;Honestly didn't expect it to feel this magical.&lt;/p&gt; &lt;h1&gt;The stack&lt;/h1&gt; &lt;p&gt;Everything's open source except Claude (which is only used when internet is available):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenClaw&lt;/strong&gt; – you know what is this &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meshtastic&lt;/strong&gt; – LoRa mesh networking firmware. The magic sauce for off-grid communication – open source, encrypted, and any Meshtastic radio can relay messages to extend range&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lilygo T-Echo&lt;/strong&gt; – the $30 radio hardware running Meshtastic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; – you know as well&lt;/li&gt; &lt;li&gt;&lt;strong&gt;phi4-mini&lt;/strong&gt; – lightweight router/classifier&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gemma3:12b&lt;/strong&gt; – the actual brain for offline responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Home Assistant&lt;/strong&gt; – smart home + TTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HA Voice PE&lt;/strong&gt; – the speaker that reads messages aloud&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mac mini M4 16GB&lt;/strong&gt; – always-on server, running on battery backup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;T-Echo (portable) │ LoRa 433MHz, encrypted ▼ T-Echo (USB) → Mac mini │ ├── SAY: prefix → HA TTS → Voice PE speaker ├── AI: prefix → phi4-mini → gemma3:12b (always local) ├── status → Home Assistant sensors ├── Online? → forward to Discord (cloud AI) └── Offline? → route everything to local Ollama models Outbox: AI drops .msg files → listener sends over LoRa (power outage alerts, reminders, etc.) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's next&lt;/h1&gt; &lt;p&gt;I'm thinking about where this goes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mesh AI network&lt;/strong&gt; – Meshtastic is a mesh protocol, every radio relays. Multiple nodes running local LLMs could create a neighborhood-scale AI network with zero internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger local models&lt;/strong&gt; – looking at upgrading hardware for 30B+ parameter models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dead man's switch&lt;/strong&gt; — auto-alert if I don't check in within a time window&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anvarazizov"&gt; /u/anvarazizov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T20:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8mh8m</id>
    <title>How do you get more GPUs than your motheboard natively supports?</title>
    <updated>2026-02-19T02:00:35+00:00</updated>
    <author>
      <name>/u/WizardlyBump17</name>
      <uri>https://old.reddit.com/user/WizardlyBump17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am planning on building an AI server for myself and I want to have 8 GPUs. The problem is that all motherboards I reaserched (FCLGA4710), dont have 8 PCIe slots, with the one with most slots having only 6. I have seen some people here with a lot of GPUs and I am pretty sure they dont have a motherboard with slots for all of them, as I remember some of the GPUs being far from the motherboard. I have done some research and I found out about risers and something about connecting the GPU using an USB, but I couldnt understand how everything works together. Anyone to help with that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WizardlyBump17"&gt; /u/WizardlyBump17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T02:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8yvu5</id>
    <title>I built an eBPF tracer to monitor AI agents the same way you'd monitor malware in a sandbox</title>
    <updated>2026-02-19T13:14:07+00:00</updated>
    <author>
      <name>/u/M4r10_h4ck</name>
      <uri>https://old.reddit.com/user/M4r10_h4ck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;TL;DR: AI agents control their own application logs, which makes those logs useless for security monitoring. We applied the malware sandboxing principle (observe from a layer the subject can't see) and built Azazel, an open-source eBPF-based runtime tracer for containerized AI agents.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;If you're running autonomous AI agents in containers, you probably have application-level logging. The agent reports what tools it called, what it returned, maybe some reasoning traces.&lt;/p&gt; &lt;p&gt;The issue: the agent controls those logs. It writes what it chooses to write.&lt;/p&gt; &lt;p&gt;This is the same fundamental problem in malware analysis, if the subject controls its own reporting, the reporting is worthless. The solution there has been around for decades: observe from the kernel, a layer the subject cannot reach, disable, or detect.&lt;/p&gt; &lt;p&gt;We asked: why isn't anyone doing this for AI agents?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Azazel attaches 19 eBPF hook points (tracepoints + kprobes) to a target container and captures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full process tree with argv, PIDs, parent PIDs (&lt;code&gt;process_exec&lt;/code&gt;, &lt;code&gt;process_clone&lt;/code&gt;, &lt;code&gt;process_exit&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;File operations with pathnames and byte counts (&lt;code&gt;file_open&lt;/code&gt;, &lt;code&gt;file_read&lt;/code&gt;, &lt;code&gt;file_write&lt;/code&gt;, &lt;code&gt;file_rename&lt;/code&gt;, &lt;code&gt;file_unlink&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Network activity including DNS detection via kprobe on &lt;code&gt;udp_sendmsg&lt;/code&gt; (&lt;code&gt;net_connect&lt;/code&gt;, &lt;code&gt;net_bind&lt;/code&gt;, &lt;code&gt;net_dns&lt;/code&gt;, etc.)&lt;/li&gt; &lt;li&gt;Security-relevant events: &lt;code&gt;ptrace&lt;/code&gt;, &lt;code&gt;mmap&lt;/code&gt; with W+X flags, kernel module loads&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything comes out as NDJSON.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The agent cannot detect it, cannot disable it, cannot interfere with it. eBPF runs in kernel space, outside the agent's address space, invisible to any syscall it can invoke.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="http://github.com/beelzebub-labs/azazel"&gt;github.com/beelzebub-labs/azazel&lt;/a&gt;&lt;br /&gt; Full write-up: &lt;a href="http://beelzebub.ai/blog/azazel-runtime-tracing-for-ai-agents"&gt;beelzebub.ai/blog/azazel-runtime-tracing-for-ai-agents&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/M4r10_h4ck"&gt; /u/M4r10_h4ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8yvu5/i_built_an_ebpf_tracer_to_monitor_ai_agents_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8yvu5/i_built_an_ebpf_tracer_to_monitor_ai_agents_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8yvu5/i_built_an_ebpf_tracer_to_monitor_ai_agents_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T13:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8jjtq</id>
    <title>More quantization visualization types (repost)</title>
    <updated>2026-02-18T23:51:43+00:00</updated>
    <author>
      <name>/u/copingmechanism</name>
      <uri>https://old.reddit.com/user/copingmechanism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt; &lt;img alt="More quantization visualization types (repost)" src="https://preview.redd.it/af1o3s52cckg1.gif?frame=1&amp;amp;width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=399ab3abe9aebeae4217cd2925119b0a76b11883" title="More quantization visualization types (repost)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by this post from &lt;a href="/u/VoidAlchemy"&gt;u/VoidAlchemy&lt;/a&gt; a few months back: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Intrusive thoughts had me try to reproduce and extend the work to include more quantization types, with/without imatrix, and some PPL/KLD measurements to see what an &amp;quot;efficient&amp;quot; quantization looks like. MXFP4 really doesn't like to participate in this sort of experiment, I don't have much faith this is a very accurate representation of the quant but oh-well.&lt;/p&gt; &lt;p&gt;The (vibe) code for this is here &lt;a href="https://codeberg.org/mailhost/quant-jaunt"&gt;https://codeberg.org/mailhost/quant-jaunt&lt;/a&gt; along with a sample of summary output (from lenna.bmp) and some specifications that might help keep the vibes on track.&lt;/p&gt; &lt;p&gt;*reposted to respect Lenna's retirement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/copingmechanism"&gt; /u/copingmechanism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r8jjtq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T23:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r90b3a</id>
    <title>TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)</title>
    <updated>2026-02-19T14:14:54+00:00</updated>
    <author>
      <name>/u/cdr420</name>
      <uri>https://old.reddit.com/user/cdr420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"&gt; &lt;img alt="TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)" src="https://external-preview.redd.it/hbTO-tYQddJ91PQtz4lVLYP0Q8-ANtjAM3Y5l6F90rs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0c22835d34c9eb0407b7050d83b0e37a482e243" title="TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cdr420"&gt; /u/cdr420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chrisrobison/textweb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r92o58</id>
    <title>Seems Microsoft is really set on not repeating a Sidney incident</title>
    <updated>2026-02-19T15:47:39+00:00</updated>
    <author>
      <name>/u/frubberism</name>
      <uri>https://old.reddit.com/user/frubberism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"&gt; &lt;img alt="Seems Microsoft is really set on not repeating a Sidney incident" src="https://preview.redd.it/n9127fik2hkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1986cde3166639e1bbd65f72b426304a1b47739" title="Seems Microsoft is really set on not repeating a Sidney incident" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frubberism"&gt; /u/frubberism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9127fik2hkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T15:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8vhhq</id>
    <title>ZUNA "Thought-to-Text": a 380M-parameter BCI foundation model for EEG data (Apache 2.0)</title>
    <updated>2026-02-19T10:11:39+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt; &lt;img alt="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" src="https://preview.redd.it/4knvh57lefkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e0e8a3a25b920023bf1670c3f5ded76380521f2" title="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- Technical paper: &lt;a href="https://zyphra.com/zuna-technical-paper"&gt;https://zyphra.com/zuna-technical-paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Technical blog: &lt;a href="https://zyphra.com/post/zuna"&gt;https://zyphra.com/post/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Hugging Face: &lt;a href="https://huggingface.co/Zyphra/ZUNA"&gt;https://huggingface.co/Zyphra/ZUNA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- GitHub: &lt;a href="https://github.com/Zyphra/zuna"&gt;https://github.com/Zyphra/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zyphra on 𝕏: &lt;a href="https://x.com/ZyphraAI/status/2024114248020898015"&gt;https://x.com/ZyphraAI/status/2024114248020898015&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4knvh57lefkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T10:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8qh08</id>
    <title>I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X</title>
    <updated>2026-02-19T05:13:10+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt; &lt;img alt="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" src="https://preview.redd.it/97driy8r0ekg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=43dd9d0bf4042a0843c9b3d69c60aedb8cfa6185" title="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm absolutely sure of it. The same usual suspects, the same language, the same who stole from whom the next million dollar ideas. It's insane. NFT-bros are now peddling openclawd crypto schemes. It's all the same BS quasi-tech lingo wrapped into neverending posts with meme-like pictures full of slogans, and graphs that literally means less than nothing, that lead back to 'blockchain, blah, blah blah, agentic, blah, blah, prediction markets&amp;quot;. I have enough of this.&lt;/p&gt; &lt;p&gt;Is this the sign of a real bubble? In the fall people were talking on X about how AI is in a bubble - which is never the time for bubbles to burst. But now every grifter discovered AI agents. Now, normally it takes 1-2 years to get from one stage to another, (sorry I'm old) but we are in a super accelerated scenario. Felt like 1998 in fall. It feels we jumped to 2000 suddenly. So IDK. Smells like a bubble is expanding rapidly. Where is my thumbtack?&lt;/p&gt; &lt;p&gt;Is&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/97driy8r0ekg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=037d07f7ab4c22bb2356a92c036939830cabe611"&gt;AGI is coming on X (Sign of something?)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r91akx</id>
    <title>llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp</title>
    <updated>2026-02-19T14:55:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"&gt; &lt;img alt="llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp" src="https://external-preview.redd.it/XOQiRlpUmQ-RDXu2-0vDquJiP5LaHys1ZKIynjJHt5g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8565cf091cdef74b5a5c04fff074756133a2db" title="llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19726"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8pztp</id>
    <title>Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2026-02-19T04:48:29+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt; &lt;img alt="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/Z3FpM3Y4czRyZGtnMWkMiFyATszvzYKXXKWtHcR48BLv2WbhyR3IwK5gi6zR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08b665759677acd0f60146592eee9094aea60bda" title="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;New Kitten models are out. Kitten ML has released open source code and weights for three new tiny expressive TTS models - 80M, 40M, 14M (all Apache 2.0)&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.com/invite/VJ86W4SURW"&gt;https://discord.com/invite/VJ86W4SURW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face - Kitten TTS V0.8:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mini 80M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-mini-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Micro 40M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-micro-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-micro-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Nano 14M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The smallest model is less than 25 MB, and around 14M parameters. All models have a major quality upgrade from previous versions, and can run on just CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Eight expressive voices:&lt;/strong&gt; 4 female and 4 male voices across all three models. They all have very high expressivity, with 80M being the best in quality. English support in this release, multilingual coming in future releases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super-small in size:&lt;/strong&gt; The 14M model is just 25 megabytes. 40M and 80M are slightly bigger, with high quality and expressivity even for longer chunks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs literally anywhere lol:&lt;/strong&gt; Forget &amp;quot;no GPU required.&amp;quot; This is designed for resource-constrained edge devices. Great news for GPU-poor folks like us.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open source (hell yeah!):&lt;/strong&gt; The models can be used for free under Apache 2.0.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unlocking on-device voice agents and applications:&lt;/strong&gt; Matches cloud TTS quality for most use cases, but runs entirely on-device (can also be hosted on a cheap GPU). If you're building voice agents, assistants, or any local speech application, no API calls needed. Free local inference. Just ship it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What changed from V0.1 to V0.8:&lt;/strong&gt; Higher quality, expressivity, and realism. Better training pipelines and 10x larger datasets.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rzgwarr4rdkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T04:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
