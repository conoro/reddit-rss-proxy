<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-17T03:25:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1niz3yk</id>
    <title>LING-MINI-2 QUANTIZED</title>
    <updated>2025-09-17T00:50:51+00:00</updated>
    <author>
      <name>/u/Chance_Camp3720</name>
      <uri>https://old.reddit.com/user/Chance_Camp3720</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While we wait for the quantization of llama.cpp we can use the chatllm.cpp library&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/RiverkanIT/Ling-mini-2.0-Quantized/tree/main"&gt;https://huggingface.co/RiverkanIT/Ling-mini-2.0-Quantized/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance_Camp3720"&gt; /u/Chance_Camp3720 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niz3yk/lingmini2_quantized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niz3yk/lingmini2_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niz3yk/lingmini2_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T00:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1niumtq</id>
    <title>Local Image Generators for AMD?</title>
    <updated>2025-09-16T21:38:40+00:00</updated>
    <author>
      <name>/u/WigWoo2</name>
      <uri>https://old.reddit.com/user/WigWoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What Local AI can I use with AMD? I got the 7900 XTX with 24GB of VRAM and I'd like to find an uncensored AI model I can get running on my PC&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WigWoo2"&gt; /u/WigWoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niumtq/local_image_generators_for_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niumtq/local_image_generators_for_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niumtq/local_image_generators_for_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T21:38:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni2chb</id>
    <title>Qwen3-Next 80b MLX (Mac) runs on latest LM Studio</title>
    <updated>2025-09-15T23:59:55+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was excited to see this work. About 35 tps on my M1 Mac Studio 64 gb. Takes about 42 gb. Edit: &lt;a href="https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T23:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj1wfk</id>
    <title>Modding guide for adding memory to RTX 4090 to 48GB</title>
    <updated>2025-09-17T03:02:54+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1wfk/modding_guide_for_adding_memory_to_rtx_4090_to/"&gt; &lt;img alt="Modding guide for adding memory to RTX 4090 to 48GB" src="https://external-preview.redd.it/DpKOqXut-KT_6hWgkRjfCS5aX47tB7t7JkG73-ejo4M.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8ff6a599c0a6bb62c8d054c25e4c20c597a3a2b" title="Modding guide for adding memory to RTX 4090 to 48GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/forums/threads/nvidia-geforce-rtx-4090-gets-a-48-gb-mod-and-step-by-step-tutorial.340880/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1wfk/modding_guide_for_adding_memory_to_rtx_4090_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1wfk/modding_guide_for_adding_memory_to_rtx_4090_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T03:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nivpb5</id>
    <title>Anyone heard of Zenith Alpha?</title>
    <updated>2025-09-16T22:21:20+00:00</updated>
    <author>
      <name>/u/Acrobatic_Initial665</name>
      <uri>https://old.reddit.com/user/Acrobatic_Initial665</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivpb5/anyone_heard_of_zenith_alpha/"&gt; &lt;img alt="Anyone heard of Zenith Alpha?" src="https://b.thumbs.redditmedia.com/JVzs3AyBHaIlEWRfgPeTyagKdZG5Z2fx7KhAHEKD7SE.jpg" title="Anyone heard of Zenith Alpha?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/n1be0tfeqlpf1.png?width=2398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cce598144e12b8a9a7923900e05b74c199eb4b21"&gt;https://preview.redd.it/n1be0tfeqlpf1.png?width=2398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cce598144e12b8a9a7923900e05b74c199eb4b21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Was playing around on design arena and a model I've never seen before called Zenith Alpha kept coming up in the tournaments -- anyone know what it is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic_Initial665"&gt; /u/Acrobatic_Initial665 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivpb5/anyone_heard_of_zenith_alpha/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivpb5/anyone_heard_of_zenith_alpha/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nivpb5/anyone_heard_of_zenith_alpha/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ninoo3</id>
    <title>Has anyone tried Intel/Qwen3-Next-80B-A3B-Instruct-int4-mixed-AutoRound?</title>
    <updated>2025-09-16T17:20:46+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When can we expect llama.cpp support for this model?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Intel/Qwen3-Next-80B-A3B-Instruct-int4-mixed-AutoRound"&gt;https://huggingface.co/Intel/Qwen3-Next-80B-A3B-Instruct-int4-mixed-AutoRound&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ninoo3/has_anyone_tried/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ninoo3/has_anyone_tried/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ninoo3/has_anyone_tried/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T17:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1niysfm</id>
    <title>The best fine-tunable real time TTS</title>
    <updated>2025-09-17T00:36:14+00:00</updated>
    <author>
      <name>/u/AwkwardBoysenberry26</name>
      <uri>https://old.reddit.com/user/AwkwardBoysenberry26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am searching a good open source TTS model to fine tune it on a specific voice dataset of 1 hour.I find that kokoro is good but I couldn‚Äôt find a documentation about it‚Äôs fine-tuning,also if the model supports non verbal expressions such as [laugh],[sigh],ect‚Ä¶ would be better (not a requirement).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AwkwardBoysenberry26"&gt; /u/AwkwardBoysenberry26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niysfm/the_best_finetunable_real_time_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niysfm/the_best_finetunable_real_time_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niysfm/the_best_finetunable_real_time_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T00:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1niq0t6</id>
    <title>Transformer Lab now supports training text-to-speech (TTS) models</title>
    <updated>2025-09-16T18:45:02+00:00</updated>
    <author>
      <name>/u/OriginalSpread3100</name>
      <uri>https://old.reddit.com/user/OriginalSpread3100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niq0t6/transformer_lab_now_supports_training/"&gt; &lt;img alt="Transformer Lab now supports training text-to-speech (TTS) models" src="https://b.thumbs.redditmedia.com/2MIGuVgcvZQwK9ZOCk_TutcuzAFF9JN3qdD2htUwW5M.jpg" title="Transformer Lab now supports training text-to-speech (TTS) models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/s21p6omknkpf1.gif"&gt;https://i.redd.it/s21p6omknkpf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We just shipped text to speech (TTS) support in Transformer Lab.&lt;/p&gt; &lt;p&gt;That means you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fine-tune open source TTS models on your own dataset&lt;/li&gt; &lt;li&gt;Clone a voice in one-shot from just a single reference sample&lt;/li&gt; &lt;li&gt;Train &amp;amp; generate speech locally on NVIDIA and AMD GPUs, or generate on Apple Silicon&lt;/li&gt; &lt;li&gt;Use the same UI you‚Äôre already using for LLMs and diffusion model trains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you‚Äôve been curious about training speech models locally, this makes it easier to get started.&lt;/p&gt; &lt;p&gt;Here‚Äôs how to get started along with easy to follow examples: &lt;a href="https://transformerlab.ai/blog/text-to-speech-support"&gt;https://transformerlab.ai/blog/text-to-speech-support&lt;/a&gt;&lt;/p&gt; &lt;p&gt; Please let me know if you have any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalSpread3100"&gt; /u/OriginalSpread3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niq0t6/transformer_lab_now_supports_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niq0t6/transformer_lab_now_supports_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niq0t6/transformer_lab_now_supports_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T18:45:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nidixx</id>
    <title>Think twice before spending on GPU?</title>
    <updated>2025-09-16T10:16:07+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen team is shifting paradigm. Qwen Next is probably first big step of many that Qwen (and other chinese labs) are taking towards sparse models, because they do not have the required GPUs to train on.&lt;/p&gt; &lt;p&gt;10% of the training cost, 10x inference throughout, 512 experts, ultra long context (though not good enough yet).&lt;/p&gt; &lt;p&gt;They have a huge incentive to train this model further (on 36T tokens instead of 15T). They will probably release the final checkpoint in coming months or even weeks. Think of the electricity savings running (and on idle) a pretty capable model. We might be able to run a qwen 235B equivalent locally on a hardware under $1500. 128GB of RAM could be enough for the models this year and it's easily upgradable to 256GB for the next.&lt;/p&gt; &lt;p&gt;Wdyt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T10:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj1anf</id>
    <title>used gaming machine vs new ai max+ ?</title>
    <updated>2025-09-17T02:33:32+00:00</updated>
    <author>
      <name>/u/green__1</name>
      <uri>https://old.reddit.com/user/green__1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My existing desktop believes that cutting edge storage technology is chiselling things into stone tablets, so it's time to upgrade to the current millennium. I haven't yet played with local LLMs, but I want to run a local LLM general assistant to learn more about this, and to have better control of my data. I also want the ability to do some image generation, though I'm unsure how much I'll use that part.&lt;/p&gt; &lt;p&gt;I'm a linux user, and this will be my main desktop in addition to AI use, I'm not really a gamer though, so the rest of my usage is not too resource intensive (hence surviving thus far on ancient tech).&lt;/p&gt; &lt;p&gt;My budget is about $3,000-$4,000 CAD (about $2,000-$3,000 USD). I'm seeing some nice used machines on marketplace with RTX 4060ti through RTX 5080 available in that price range with decent specs otherwise&lt;br /&gt; But I'm also hearing hype about the new AMD ai max+ machines which also seem to fit the budget, and I sure like the idea of the lower power use, especially given that the rest of my non-ai use won't be too resource intensive.&lt;/p&gt; &lt;p&gt;I'm hearing 2 conflicting things for AI though:&lt;/p&gt; &lt;p&gt;1) the only thing that matters is vram, nothing else matters&lt;br /&gt; 2) you must use nvidia, that's all that matters&lt;/p&gt; &lt;p&gt;So obviously the ai max+ has a ton more vram than any nvidia card I can afford, but it's not nvidia... so how much priority should I put on 1) vs 2)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/green__1"&gt; /u/green__1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1anf/used_gaming_machine_vs_new_ai_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1anf/used_gaming_machine_vs_new_ai_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1anf/used_gaming_machine_vs_new_ai_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T02:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nif778</id>
    <title>Unofficial VibeVoice finetuning code released!</title>
    <updated>2025-09-16T11:47:48+00:00</updated>
    <author>
      <name>/u/Downtown-Accident-87</name>
      <uri>https://old.reddit.com/user/Downtown-Accident-87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this on discord: &lt;a href="https://github.com/voicepowered-ai/VibeVoice-finetuning"&gt;https://github.com/voicepowered-ai/VibeVoice-finetuning&lt;/a&gt;&lt;br /&gt; I will try training a lora soon, I hope it works :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Downtown-Accident-87"&gt; /u/Downtown-Accident-87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj0p7r</id>
    <title>ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code.</title>
    <updated>2025-09-17T02:04:53+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj0p7r/archgw_0312_model_aliases_allow_clients_to_use/"&gt; &lt;img alt="ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." src="https://preview.redd.it/y12ej5klrmpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49d0c97a765a892fb138b713ad332655f5cddfab" title="ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added this lightweight abstraction to &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; to decouple app code from specific model names. Instead of sprinkling hardcoded model names like&lt;code&gt;gpt-4o-mini&lt;/code&gt; or &lt;code&gt;llama3.2&lt;/code&gt; everywhere, you point to an &lt;em&gt;alias&lt;/em&gt; that encodes intent, and allows you to test new models, swap out the config safely without having to do codewide search/replace every time you want to experiment with a new model or version.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.summarize.v1 ‚Üí cheap/fast summarization arch.v1 ‚Üí default ‚Äúlatest‚Äù general-purpose model arch.reasoning.v1 ‚Üí heavier reasoning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The app calls the alias, not the vendor. Swap the model in config, and the entire system updates without touching code. Of course, you would want to use models compatible. Meaning if you map an embedding model to an alias, when the application expects a chat model, it won't be a good day. &lt;/p&gt; &lt;p&gt;Where are we headed with this...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Guardrails -&amp;gt; Apply safety, cost, or latency rules at the alias level: arch.reasoning.v1:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.reasoning.v1: target: gpt-oss-120b guardrails: max_latency: 5s block_categories: [‚Äújailbreak‚Äù, ‚ÄúPII‚Äù] &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Fallbacks -&amp;gt; Provide a chain if a model fails or hits quota:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.summarize.v1: target: gpt-4o-mini fallback: llama3.2 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt; Traffic splitting &amp;amp; canaries -&amp;gt; Let an alias fan out traffic across multiple targets:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.v1: targets: - model: llama3.2 weight: 80 - model: gpt-4o-mini weight: 20 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y12ej5klrmpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj0p7r/archgw_0312_model_aliases_allow_clients_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj0p7r/archgw_0312_model_aliases_allow_clients_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T02:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nirkn1</id>
    <title>Roo Code and Qwen3 Next is Not Impressive</title>
    <updated>2025-09-16T19:42:06+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I wanted to share my experience with the thinking and instruct versions of the new Qwen3 Next model. Both run impressively well on my computer, delivering fast and reasonably accurate responses outside the Roo code development environment.&lt;/p&gt; &lt;p&gt;However, their performance in the Roo code environment is less consistent. While both models handle tool calling effectively, the instruct model struggles with fixing issues, and the thinking model takes excessively long to process solutions, making other models like GLM Air more reliable in these cases.&lt;/p&gt; &lt;p&gt;Despite these challenges, I‚Äôm optimistic about the model‚Äôs potential, especially given its longer context window. I‚Äôm eager for the GGUF releases and believe increasing the active parameters could enhance accuracy.&lt;/p&gt; &lt;p&gt;Thanks for reading! I‚Äôd love to hear your thoughts. And if if you recommend another set of tools to use with Qwen3 Next other than roo, please do share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nirkn1/roo_code_and_qwen3_next_is_not_impressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nirkn1/roo_code_and_qwen3_next_is_not_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nirkn1/roo_code_and_qwen3_next_is_not_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T19:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1niktfz</id>
    <title>VoxCPM-0.5B</title>
    <updated>2025-09-16T15:34:15+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"&gt; &lt;img alt="VoxCPM-0.5B" src="https://external-preview.redd.it/r3qnehuYhIo41bAc9p8n4efqIezTbTJqzszutOT9598.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c21b8a7fb420d7443519db438480fdc9bd7c71a4" title="VoxCPM-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Supports both Regular text and Phoneme input. Seems promising!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T15:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nipox6</id>
    <title>Fine-tuning Small Language models/ qwen2.5 0.5 B</title>
    <updated>2025-09-16T18:33:01+00:00</updated>
    <author>
      <name>/u/Mysterious_Ad_3788</name>
      <uri>https://old.reddit.com/user/Mysterious_Ad_3788</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"&gt; &lt;img alt="Fine-tuning Small Language models/ qwen2.5 0.5 B" src="https://preview.redd.it/hoplx2colkpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35a01423a78f194d98f7f162fddd9b55eec0fee6" title="Fine-tuning Small Language models/ qwen2.5 0.5 B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been up all week trying to fine-tune a small language model using Unsloth, and I've experimented with RAG. I generated around 1,500 domain-specific questions, but my LLM is still hallucinating. Below is a summary of my training setup and data distribution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Epochs&lt;/strong&gt;: 20 (training stops around epoch 11)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learning rate&lt;/strong&gt;: 1e-4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Warmup ratio&lt;/strong&gt;: 0.5&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Max sequence length&lt;/strong&gt;: 4096&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA rank&lt;/strong&gt;: 32&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA alpha&lt;/strong&gt;: 16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Includes both positive and negative QA-style examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite this setup, hallucinations persist the model dont even know what it was finetuned on. Can anyone help me understand what I might be doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Ad_3788"&gt; /u/Mysterious_Ad_3788 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hoplx2colkpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T18:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj1htj</id>
    <title>one day, this will work ü§û</title>
    <updated>2025-09-17T02:43:16+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1htj/one_day_this_will_work/"&gt; &lt;img alt="one day, this will work ü§û" src="https://preview.redd.it/mmwwmtdh1npf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70bca1b832c2d49c694f77abe7ad5ee6414235e3" title="one day, this will work ü§û" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mmwwmtdh1npf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1htj/one_day_this_will_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj1htj/one_day_this_will_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T02:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nipldx</id>
    <title>Ktransformers now supports qwen3-next</title>
    <updated>2025-09-16T18:29:30+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"&gt; &lt;img alt="Ktransformers now supports qwen3-next" src="https://external-preview.redd.it/GCXHZq6UgvHr-07Ef7MzKApM7hyb5aZQRF1Wd5lmCZ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45539a92e2924b07b2035937feb0f51a09d5cc5e" title="Ktransformers now supports qwen3-next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was a few days ago but I haven't seen it mentioned here so I figured I'd post it. They claim 6GB of vram usage with 320GB of system memory. Hopefully in the future the system memory requirements can be brought down if they support quantized variants.&lt;/p&gt; &lt;p&gt;I think this could be the ideal way to run it on low vram systems in the short term before llamacpp gets support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Qwen3-Next.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T18:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nijikb</id>
    <title>Inference will win ultimately</title>
    <updated>2025-09-16T14:46:07+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt; &lt;img alt="Inference will win ultimately" src="https://preview.redd.it/jp7ada3lhjpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2651ab9359b4d75a0e7c1c55003fec8ea92f4fdb" title="Inference will win ultimately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;inference is where the real value shows up. it‚Äôs where models are actually used at scale.&lt;/p&gt; &lt;p&gt;A few reasons why I think this is where the winners will be: ‚Ä¢Hardware is shifting. Morgan Stanley recently noted that more chips will be dedicated to inference than training in the years ahead. The market is already preparing for this transition. ‚Ä¢Open-source is exploding. Meta‚Äôs Llama models alone have crossed over a billion downloads. That‚Äôs a massive long tail of developers and companies who need efficient ways to serve all kinds of models. ‚Ä¢Agents mean real usage. Training is abstract , inference is what everyday people experience when they use agents, apps, and platforms. That‚Äôs where latency, cost, and availability matter. ‚Ä¢Inefficiency is the opportunity. Right now GPUs are underutilized, cold starts are painful, and costs are high. Whoever cracks this at scale , making inference efficient, reliable, and accessible , will capture enormous value.&lt;/p&gt; &lt;p&gt;In short, inference isn‚Äôt just a technical detail. It‚Äôs where AI meets reality. And that‚Äôs why inference will win.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jp7ada3lhjpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nis417</id>
    <title>Alibaba Tongyi released open-source (Deep Research) Web Agent</title>
    <updated>2025-09-16T20:02:18+00:00</updated>
    <author>
      <name>/u/kahlil29</name>
      <uri>https://old.reddit.com/user/kahlil29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face link to weights : &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kahlil29"&gt; /u/kahlil29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ali_TongyiLab/status/1967988004179546451?s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis417/alibaba_tongyi_released_opensource_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nis417/alibaba_tongyi_released_opensource_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T20:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nis0za</id>
    <title>Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face</title>
    <updated>2025-09-16T19:59:13+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"&gt; &lt;img alt="Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face" src="https://external-preview.redd.it/Br8d0DO81Y2NXG6ObCzOPqMnemqzEFVfpKOIf-1Xb3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f864231608ef7f1e9dcabddf98002a4ed64cb7df" title="Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T19:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nit4v6</id>
    <title>Granite 4 release today? Collection updated with 8 private repos.</title>
    <updated>2025-09-16T20:40:36+00:00</updated>
    <author>
      <name>/u/ironwroth</name>
      <uri>https://old.reddit.com/user/ironwroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt; &lt;img alt="Granite 4 release today? Collection updated with 8 private repos." src="https://preview.redd.it/ihwp4dy78lpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=310d508b27499694f225a40decad5893a979dfda" title="Granite 4 release today? Collection updated with 8 private repos." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ironwroth"&gt; /u/ironwroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ihwp4dy78lpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T20:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1niwb8l</id>
    <title>500,000 public datasets on Hugging Face</title>
    <updated>2025-09-16T22:46:45+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt; &lt;img alt="500,000 public datasets on Hugging Face" src="https://preview.redd.it/rokftav6vlpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26f96c62b0cfcf4ab8d9a212645ed0b0f54e16e2" title="500,000 public datasets on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rokftav6vlpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nivz2n</id>
    <title>We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo</title>
    <updated>2025-09-16T22:32:32+00:00</updated>
    <author>
      <name>/u/Josiahhenryus</name>
      <uri>https://old.reddit.com/user/Josiahhenryus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt; &lt;img alt="We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo" src="https://external-preview.redd.it/ZDZxemk3OWFzbHBmMVMFq2pfv69EmnrpZl789HXOOBvSofKD3EML3NWxX5eD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=517811a91d95e57b2a6e0b44c23976628615830f" title="We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ongoing research out of Derive DX Labs in Lafayette, Louisiana. We‚Äôve been experimenting with efficiency optimizations and managed to get a 2B parameter chain-of-thought model running on iPhone with ~400‚Äì500MB RAM, fully offline.&lt;/p&gt; &lt;p&gt;I‚Äôm not super active on Reddit, so please don‚Äôt kill me if I‚Äôm slow to respond to comments ‚Äî but I‚Äôll do my best to answer questions.&lt;/p&gt; &lt;p&gt;[Correction: Meant Gemma-3N not Gemini-3B]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Josiahhenryus"&gt; /u/Josiahhenryus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6rczu79aslpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nixynv</id>
    <title>The Qwen of Pain.</title>
    <updated>2025-09-16T23:58:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt; &lt;img alt="The Qwen of Pain." src="https://preview.redd.it/0px1banw6mpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8edc833e57220e0c00a8b11ba32c881974742ef1" title="The Qwen of Pain." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0px1banw6mpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T23:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nifajh</id>
    <title>I bought a modded 4090 48GB in Shenzhen. This is my story.</title>
    <updated>2025-09-16T11:52:20+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt; &lt;img alt="I bought a modded 4090 48GB in Shenzhen. This is my story." src="https://external-preview.redd.it/1vD_R63iqu4vnM_qQf7pZNwXb9dy_UDc_Gl2j3LnTpU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5102c5612db16c04c26877a1e72e86700648e25" title="I bought a modded 4090 48GB in Shenzhen. This is my story." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe"&gt;https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few years ago, before ChatGPT became popular, I managed to score a Tesla P40 on eBay for around $150 shipped. With a few tweaks, I installed it in a Supermicro chassis. At the time, I was mostly working on video compression and simulation. It worked, but the card consistently climbed to 85¬∞C.&lt;/p&gt; &lt;p&gt;When DeepSeek was released, I was impressed and installed Ollama in a container. With 24GB of VRAM, it worked‚Äîbut slowly. After trying Stable Diffusion, it became clear that an upgrade was necessary.&lt;/p&gt; &lt;p&gt;The main issue was finding a modern GPU that could actually &lt;strong&gt;fit&lt;/strong&gt; in the server chassis. Standard 4090/5090 cards are designed for desktops: they're too large, and the power plug is inconveniently placed on top. After watching the LTT video featuring a modded 4090 with 48GB (and a follow-up from Gamers Nexus), I started searching the only place I knew might have one: Alibaba.com.&lt;/p&gt; &lt;p&gt;I contacted a seller and got a quote: &lt;strong&gt;CNY 22,900&lt;/strong&gt;. Pricey, but cheaper than expected. However, Alibaba enforces VAT collection, and I‚Äôve had bad experiences with DHL‚Äîthere was a non-zero chance I‚Äôd be charged twice for taxes. I was already over ‚Ç¨700 in taxes and fees.&lt;/p&gt; &lt;p&gt;Just for fun, I checked &lt;a href="http://Trip.com"&gt;Trip.com&lt;/a&gt; and realized that for the same amount of money, I could fly to Hong Kong and back, with a few days to explore. After confirming with the seller that they‚Äôd meet me at their business location, I booked a flight and an Airbnb in Hong Kong.&lt;/p&gt; &lt;p&gt;For context, I don‚Äôt speak Chinese at all. Finding the place using a Chinese address was tricky. Google Maps is useless in China, Apple Maps gave some clues, and Baidu Maps was beyond my skill level. With a little help from DeepSeek, I decoded the address and located the place in an industrial estate outside the city center. Thanks to Shenzhen‚Äôs extensive metro network, I didn‚Äôt need a taxi.&lt;/p&gt; &lt;p&gt;After arriving, the manager congratulated me for being the first foreigner to find them unassisted. I was given the card from a large batch‚Äîthey‚Äôre clearly producing these in volume at a factory elsewhere in town (I was proudly shown videos of the assembly line). I asked them to retest the card so I could verify its authenticity.&lt;/p&gt; &lt;p&gt;During the office tour, it was clear that their next frontier is repurposing old mining cards. I saw a large collection of NVIDIA Ampere mining GPUs. I was also told that modded 5090s with over 96GB of VRAM are in development.&lt;/p&gt; &lt;p&gt;After the test was completed, I paid in cash (a &lt;em&gt;lot&lt;/em&gt; of banknotes!) and returned to Hong Kong with my new purchase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
