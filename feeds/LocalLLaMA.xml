<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-26T04:16:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pux0yc</id>
    <title>We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.</title>
    <updated>2025-12-24T20:50:16+00:00</updated>
    <author>
      <name>/u/vox-deorum</name>
      <uri>https://old.reddit.com/user/vox-deorum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt; &lt;img alt="We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found." src="https://b.thumbs.redditmedia.com/4t8jQ0I5zf5Jo9d6oRUo6_gfae4uPfqghHDLniBqMTU.jpg" title="We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/zaib4up4s79g1.gif"&gt;GLM-4.6 Playing Civilization V + Vox Populi (Replay)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We had GPT-OSS-120B and GLM-4.6 playing 1,408 full Civilization V games (with Vox Populi/Community Patch activated). In a nutshell: LLMs set strategies for Civilization V's algorithmic AI to execute. Here is what we found&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ftox05oo5e9g1.png?width=3201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8181b507060b45caab07acc36ba82d80eb65f1d"&gt;An overview of our system and results (figure fixed thanks to the comments)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; It is now possible to get open-source LLMs to play end-to-end Civilization V games (the m. They are not beating algorithm-based AI on a very simple prompt, but they do play quite differently.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The boring result:&lt;/strong&gt; With a simple prompt and little memory, both LLMs did slightly better in the best score they could achieve within each game (+1-2%), but slightly worse in win rates (-1~3%). Despite the large number of games run (2,207 in total, with 919 baseline games), neither metric is significant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The surprising part:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pure-LLM or pure-RL approaches &lt;a href="https://arxiv.org/abs/2401.10568"&gt;[1]&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2502.20807"&gt;[2]&lt;/a&gt; couldn't get an AI to play and survive full Civilization games. With our hybrid approach, LLMs can survive as long as the game goes (~97.5% LLMs, vs. ~97.3% the in-game AI). The model can be as small as OSS-20B in our internal test.&lt;/p&gt; &lt;p&gt;Moreover, the two models developed &lt;strong&gt;completely different playstyles&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OSS-120B went full warmonger: +31.5% more Domination victories, -23% fewer Cultural victories compared to baseline&lt;/li&gt; &lt;li&gt;GLM-4.6 played more balanced, leaning into both Domination and Cultural strategies&lt;/li&gt; &lt;li&gt;Both models preferred &lt;strong&gt;Order&lt;/strong&gt; (&lt;strong&gt;communist-like&lt;/strong&gt;, ~24% more likely) ideology over &lt;strong&gt;Freedom&lt;/strong&gt; (democratic-like)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cost/latency (OSS-120B):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~53,000 input / 1,500 output tokens per turn&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~$0.86/game&lt;/strong&gt; (OpenRouter pricing as of 12/2025)&lt;/li&gt; &lt;li&gt;Input tokens scale linearly as the game state grows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output stays flat: models don't automatically &amp;quot;think harder&amp;quot; in the late game.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Watch more:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper link: &lt;a href="https://arxiv.org/abs/2512.18564"&gt;https://arxiv.org/abs/2512.18564&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/1.Civ5Replay"&gt;Example save 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/2.Civ5Replay"&gt;Example save 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/3.Civ5Replay"&gt;Example save 3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Vox Deorum system is 100% open-sourced and currently in beta testing&lt;/li&gt; &lt;li&gt;GitHub Repo: &lt;a href="https://github.com/CIVITAS-John/vox-deorum"&gt;https://github.com/CIVITAS-John/vox-deorum&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub Release: &lt;a href="https://github.com/CIVITAS-John/vox-deorum/releases"&gt;https://github.com/CIVITAS-John/vox-deorum/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Works with any &lt;strong&gt;OpenAI-compatible local providers&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tccdt44oq79g1.png?width=2291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b8a4fe5871db4d2bf00f417acd13de3e688037f"&gt;We exposed the game as a MCP server, so your agents can play the game with you&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Your thoughts are greatly appreciated:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's a good way to express the game state more efficiently? Consider a late-game turn where you have 20+ cities and 100+ units. Easily 50k+ tokens. Could multimodal help?&lt;/li&gt; &lt;li&gt;How can we get LLMs to play better? I have considered RAG, but there is really little data to &amp;quot;retrieve&amp;quot; here. Possibly self-play + self-reflection + long-term memory?&lt;/li&gt; &lt;li&gt;How are we going to design strategy games if LLMs are to play with you? I have put an LLM spokesperson for civilizations as an example, but there is surely more to do?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Join us:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I am hiring a PhD student for Fall '26, and we are expanding our game-related work rapidly. Shoot me a DM if you are interested!&lt;/li&gt; &lt;li&gt;I am happy to collaborate with anyone interested in furthering this line of work.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vox-deorum"&gt; /u/vox-deorum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T20:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvly0g</id>
    <title>I made a CLI to train LLMs in 2 commands (no PyTorch boilerplate)</title>
    <updated>2025-12-25T20:21:50+00:00</updated>
    <author>
      <name>/u/NoHotel8779</name>
      <uri>https://old.reddit.com/user/NoHotel8779</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I made a CLI to train LLMs super easily, instead of lots of pytorch boilerplate you just&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cleanai --init-config config.json cleanai --new --config config.json --pretrain --train &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's super easy to use, made in C with no ml libs, the source is available on GitHub along with an install script (&lt;a href="https://github.com/willmil11/cleanai-c"&gt;https://github.com/willmil11/cleanai-c&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Interesting stuff: - init-config asks you questions and explains everything so no need to worry about that. - there's a checkpoint CLI every epoch to stop training, test the model or make adjustments, if you're not here training auto continues after 30 seconds - for windows users, use wsl2&lt;/p&gt; &lt;p&gt;Note: for install script you need fish shell:&lt;/p&gt; &lt;p&gt;Debian/Ubuntu:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install fish &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Arch/Manjaro:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo pacman -S fish &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Fedora/RHEL:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo dnf install fish &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;openSUSE:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo zypper install fish &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alpine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apk add fish &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;macOS (Homebrew):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew install fish &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And make sure your clang is not cosplaying as GCC if you have it. (Sometimes some distros like to have clang aliased as gcc, my install script should tell you if that's the case and ask you for the real GCC command)&lt;/p&gt; &lt;p&gt;Merry Christmas y'all :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoHotel8779"&gt; /u/NoHotel8779 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvly0g/i_made_a_cli_to_train_llms_in_2_commands_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvly0g/i_made_a_cli_to_train_llms_in_2_commands_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvly0g/i_made_a_cli_to_train_llms_in_2_commands_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T20:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvsdhl</id>
    <title>Highly accurate local LLM for SQL analytics on large production datasets</title>
    <updated>2025-12-26T01:44:13+00:00</updated>
    <author>
      <name>/u/NoBlackberry3264</name>
      <uri>https://old.reddit.com/user/NoBlackberry3264</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm working on &lt;strong&gt;SQL analytics locally&lt;/strong&gt; for my company, using &lt;strong&gt;large, real production datasets&lt;/strong&gt;.&lt;br /&gt; My &lt;strong&gt;top priority is accuracy and correctness&lt;/strong&gt;, not creativity or speed.&lt;/p&gt; &lt;p&gt;I‚Äôm specifically looking for a &lt;strong&gt;local LLM&lt;/strong&gt; that is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Highly accurate in SQL generation&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Strong at &lt;strong&gt;analytical reasoning&lt;/strong&gt; (aggregations, joins, window functions)&lt;/li&gt; &lt;li&gt;Consistent with &lt;strong&gt;large schemas&lt;/strong&gt; and avoids hallucinated tables/columns&lt;/li&gt; &lt;li&gt;Reliable for &lt;strong&gt;business-critical analytics&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Suitable for &lt;strong&gt;on-prem / local deployment&lt;/strong&gt; (no cloud)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Use cases include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Writing complex analytical SQL queries&lt;/li&gt; &lt;li&gt;Interpreting business questions into correct SQL&lt;/li&gt; &lt;li&gt;Validating and improving existing queries&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoBlackberry3264"&gt; /u/NoBlackberry3264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvsdhl/highly_accurate_local_llm_for_sql_analytics_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvsdhl/highly_accurate_local_llm_for_sql_analytics_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvsdhl/highly_accurate_local_llm_for_sql_analytics_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T01:44:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1puyq9r</id>
    <title>Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record</title>
    <updated>2025-12-24T22:14:48+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"&gt; &lt;img alt="Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record" src="https://external-preview.redd.it/jvYBeKyc_OM28gIhUnO6GEg0WpsjQzWHJmf00gHtJBw.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d32d447dab802e0a1aec9574d5282648b995cf17" title="Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/12/24/nvidia-buying-ai-chip-startup-groq-for-about-20-billion-biggest-deal.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T22:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvlm4x</id>
    <title>I made a CLI to train LLMs in 2 commands (no PyTorch boilerplate)</title>
    <updated>2025-12-25T20:06:08+00:00</updated>
    <author>
      <name>/u/NoHotel8779</name>
      <uri>https://old.reddit.com/user/NoHotel8779</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I made a CLI to train LLMs super easily, instead of lots of pytorch boilerplate you just &lt;code&gt;bash cleanai --init-config config.json cleanai --new --config config.json --pretrain --train &lt;/code&gt; It's super easy to use, made in C with no ml libs, the source is available on GitHub along with an install script (&lt;a href="https://github.com/willmil11/cleanai-c"&gt;https://github.com/willmil11/cleanai-c&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Interesting stuff: - init-config asks you questions and explains everything so no need to worry about that. - there's a checkpoint CLI every epoch to stop training, test the model or make adjustments, if you're not here training auto continues after 30 seconds - for windows users, use wsl2&lt;/p&gt; &lt;p&gt;Note: for install script you need fish shell: Debian/Ubuntu: &lt;code&gt;bash sudo apt install fish &lt;/code&gt; Arch/Manjaro: &lt;code&gt;bash sudo pacman -S fish &lt;/code&gt; Fedora/RHEL: &lt;code&gt;bash sudo dnf install fish &lt;/code&gt; openSUSE: &lt;code&gt;bash sudo zypper install fish &lt;/code&gt; Alpine: &lt;code&gt;bash sudo apk add fish &lt;/code&gt; macOS (Homebrew): &lt;code&gt;bash brew install fish &lt;/code&gt; And make sure your clang is not cosplaying as GCC if you have it. (Sometimes some distros like to have clang aliased as gcc, my install script should tell you if that's the case and ask you for the real GCC command)&lt;/p&gt; &lt;p&gt;Merry Christmas y'all :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoHotel8779"&gt; /u/NoHotel8779 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvlm4x/i_made_a_cli_to_train_llms_in_2_commands_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvlm4x/i_made_a_cli_to_train_llms_in_2_commands_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvlm4x/i_made_a_cli_to_train_llms_in_2_commands_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T20:06:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvrzrm</id>
    <title>NOTICE - ROMED8-2T MOTHERBOARD USERS - Please read, don't melt cables..</title>
    <updated>2025-12-26T01:24:02+00:00</updated>
    <author>
      <name>/u/gittb</name>
      <uri>https://old.reddit.com/user/gittb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvrzrm/notice_romed82t_motherboard_users_please_read/"&gt; &lt;img alt="NOTICE - ROMED8-2T MOTHERBOARD USERS - Please read, don't melt cables.." src="https://b.thumbs.redditmedia.com/p1Dg-ixoo8TYAs6XGEe9DT01q84wIO5OdrJHvwgBYKo.jpg" title="NOTICE - ROMED8-2T MOTHERBOARD USERS - Please read, don't melt cables.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please, if you're using this motherboard, read closely. I learned this the hard way. Pretty scary to walk into the server closet and see a glowing orange light where there shouldn't be one..&lt;/p&gt; &lt;p&gt;On page 31 of the manual, it reads:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3uvl88hp9g9g1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef038f1b39dd4aa67f53dcaaf9e479113f236628"&gt;https://preview.redd.it/3uvl88hp9g9g1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef038f1b39dd4aa67f53dcaaf9e479113f236628&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is not a suggestion, and you WILL melt you power board power supply cable.&lt;/p&gt; &lt;p&gt;Each GPU pulls 75 watts through the PCIe connector on the motherboard, it will overdraw the 12v supply from the main ATX connector.&lt;/p&gt; &lt;p&gt;There is a small white 6 pin PCI connector on the front side of the board to plug an auxiliary 6 pin adapter into.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7it89w69ag9g1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f345086eacaf4cce918ce0cade168b405a0e03e"&gt;https://preview.redd.it/7it89w69ag9g1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f345086eacaf4cce918ce0cade168b405a0e03e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gittb"&gt; /u/gittb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvrzrm/notice_romed82t_motherboard_users_please_read/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvrzrm/notice_romed82t_motherboard_users_please_read/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvrzrm/notice_romed82t_motherboard_users_please_read/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T01:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvqs54</id>
    <title>Local LLM concurrency question: ‚Äúsatellite orchestration‚Äù works, but LM Studio serializes requests and kills parallelism</title>
    <updated>2025-12-26T00:22:07+00:00</updated>
    <author>
      <name>/u/marcosomma-OrKA</name>
      <uri>https://old.reddit.com/user/marcosomma-OrKA</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvqs54/local_llm_concurrency_question_satellite/"&gt; &lt;img alt="Local LLM concurrency question: ‚Äúsatellite orchestration‚Äù works, but LM Studio serializes requests and kills parallelism" src="https://preview.redd.it/hhq4pkyvyf9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4dc74878a58429528049d89af26fdcc97e470317" title="Local LLM concurrency question: ‚Äúsatellite orchestration‚Äù works, but LM Studio serializes requests and kills parallelism" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm experimenting with a ‚Äústream orchestration‚Äù pattern for live assistants, where the chat-facing agent stays responsive while background agents continuously enrich state.&lt;/p&gt; &lt;p&gt;The mental model is the attached diagram: there is one &lt;strong&gt;Executor&lt;/strong&gt; (the only agent that talks to the user) and multiple &lt;strong&gt;Satellite agents&lt;/strong&gt; around it. Satellites do not produce user output. They only produce structured patches to a shared state.&lt;/p&gt; &lt;p&gt;What satellites do (scope, and why I think it matters)&lt;/p&gt; &lt;p&gt;In a live customer-care style conversation you cannot keep growing a single mega prompt. It becomes slow, expensive, and less reliable. So instead of stuffing everything into one system prompt, I split responsibilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;Executor&lt;/strong&gt; is optimized for low latency and stable voice. It handles ‚Äúrespond now‚Äù.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Satellites&lt;/strong&gt; run in parallel and keep the internal state fresh: &lt;ul&gt; &lt;li&gt;rolling summary (so the executor does not re-ingest the whole transcript)&lt;/li&gt; &lt;li&gt;intent / stage tracking (what the user is trying to do now)&lt;/li&gt; &lt;li&gt;constraints / guardrails (policy or compliance signals)&lt;/li&gt; &lt;li&gt;you can add more: escalation risk, next-best-action hints, entity extraction, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The orchestrator runs a small cadence loop. When satellites patch state, the orchestrator &lt;strong&gt;re-composes&lt;/strong&gt; the executor prompt from invariants (identity, refusal policy, permissions) plus the latest state sections (summary, intent, constraints). Then it &lt;strong&gt;swaps the executor instance&lt;/strong&gt; internally. The chat layer stays continuous for the user, but the executor‚Äôs internal context stays fresh.&lt;/p&gt; &lt;p&gt;My logs show this swap and patch cycle clearly, for example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;satellites enabled (&lt;code&gt;roles: [&amp;quot;summarizer&amp;quot;, &amp;quot;intent&amp;quot;, &amp;quot;compliance&amp;quot;]&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;periodic cadence ticks&lt;/li&gt; &lt;li&gt;state patches (&lt;code&gt;context_update&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;executor swaps (&lt;code&gt;executor_swap&lt;/code&gt; with reasons like &lt;code&gt;state_delta_threshold&lt;/code&gt; / &lt;code&gt;satellite_patch&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;rebuilt prompt (&lt;code&gt;prompt_debug&lt;/code&gt; includes Summary and constraints) orka_debug_console_20251226_010‚Ä¶&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The problem: LM Studio is serializing my ‚Äúparallel‚Äù calls&lt;/p&gt; &lt;p&gt;OrKa uses asyncio and fires the HTTP requests concurrently. You can see multiple TCP connects starting at the same time in the log (several &lt;code&gt;connect_tcp.started host='localhost' port=1234&lt;/code&gt; lines back-to-back), which corresponds to executor + satellites being scheduled together.&lt;/p&gt; &lt;p&gt;But LM Studio appears to execute actual generations one-by-one internally (threaded queue), so my satellites block behind the executor generation. Result: the architecture is parallel at the orchestrator level, but effectively serial at the model server level. That breaks the whole point of satellites, because satellites are supposed to ‚Äúcompute in the background‚Äù while the executor streams.&lt;/p&gt; &lt;p&gt;What I‚Äôm looking for&lt;/p&gt; &lt;p&gt;If you have experience running local models with real concurrency (or at least good batching) behind an OpenAI-compatible endpoint, what would you recommend?&lt;/p&gt; &lt;p&gt;Concretely, I want one of these behaviors:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;true concurrent decoding (multiple sequences progressing at once), or&lt;/li&gt; &lt;li&gt;continuous batching that lets multiple requests share throughput without head-of-line blocking, or&lt;/li&gt; &lt;li&gt;a practical setup that isolates the executor from satellites so the executor stays fast.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ideas I‚Äôm considering (please correct or improve)&lt;/p&gt; &lt;p&gt;Running multiple backends and routing:&lt;br /&gt; Keep the executor on one model server instance, satellites on another (different port/process, possibly smaller model). This avoids the executor being stuck behind satellite work and vice versa. If LM Studio is fundamentally single-queue per model, this might be the simplest.&lt;/p&gt; &lt;p&gt;Switch server:&lt;br /&gt; Use a server that supports parallel slots / continuous batching. vLLM is the obvious one on GPU for concurrency/throughput. On CPU, llama.cpp server has options around parallel sequences and batching (if anyone has a proven configuration for OpenAI-compatible chat completions, I‚Äôd like to hear it).&lt;/p&gt; &lt;p&gt;Change scheduling:&lt;br /&gt; If the backend is serial anyway, I can change the orchestrator to run satellites opportunistically (after the executor finishes, or every N turns, or only when triggers fire). But this is a downgrade: it turns ‚Äústream orchestration‚Äù into ‚Äústaggered orchestration‚Äù.&lt;/p&gt; &lt;p&gt;Question for the community&lt;/p&gt; &lt;p&gt;If you were building a local, streaming assistant with satellites, what would you do to get real parallelism?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is LM Studio known to serialize generation per model instance no matter what?&lt;/li&gt; &lt;li&gt;Is there a setting in LM Studio that actually allows multiple concurrent generations?&lt;/li&gt; &lt;li&gt;What local OpenAI-compatible servers have you personally seen handle concurrent requests well?&lt;/li&gt; &lt;li&gt;Any recommended architecture pattern for ‚Äúone streaming executor + background satellites‚Äù on a single machine?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôll attach the full logs and the diagram with the post. The relevant events to look for in the log are &lt;code&gt;executor_swap&lt;/code&gt;, &lt;code&gt;context_update&lt;/code&gt;, &lt;code&gt;prompt_debug&lt;/code&gt;, and the multiple concurrent &lt;code&gt;connect_tcp.started&lt;/code&gt; entries.&lt;/p&gt; &lt;p&gt;Real OrKA logs: &lt;a href="https://raw.githubusercontent.com/marcosomma/orka-reasoning/refs/heads/feat/streaming_orchestration/docs/streaming_logs/orka_debug_console_20251226_010734.log"&gt;https://raw.githubusercontent.com/marcosomma/orka-reasoning/refs/heads/feat/streaming_orchestration/docs/streaming_logs/orka_debug_console_20251226_010734.log&lt;/a&gt;&lt;br /&gt; OrKA branch where streaming is implemented if you want to check out the code:&lt;br /&gt; &lt;a href="https://github.com/marcosomma/orka-reasoning/tree/feat/streaming_orchestration"&gt;https://github.com/marcosomma/orka-reasoning/tree/feat/streaming_orchestration&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marcosomma-OrKA"&gt; /u/marcosomma-OrKA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hhq4pkyvyf9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvqs54/local_llm_concurrency_question_satellite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvqs54/local_llm_concurrency_question_satellite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T00:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvescs</id>
    <title>GLM 4.7 is not on lmarena anymore</title>
    <updated>2025-12-25T14:44:48+00:00</updated>
    <author>
      <name>/u/Sooqrat</name>
      <uri>https://old.reddit.com/user/Sooqrat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why is that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sooqrat"&gt; /u/Sooqrat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvescs/glm_47_is_not_on_lmarena_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvescs/glm_47_is_not_on_lmarena_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvescs/glm_47_is_not_on_lmarena_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T14:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvk3d9</id>
    <title>HOWTO: Running the best models on a dual RTX Pro 6000 rig with vLLM (192 GB VRAM)</title>
    <updated>2025-12-25T18:55:57+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ground rules: We want speed (tens or hundreds of tokens/sec) and everything fitting into available VRAM&lt;/p&gt; &lt;h1&gt;How to install vLLM stable&lt;/h1&gt; &lt;p&gt;Prerequisite: &lt;a href="https://forum.level1techs.com/t/wip-blackwell-rtx-6000-pro-max-q-quickie-setup-guide-on-ubuntu-24-04-lts-25-04/230521"&gt;Ubuntu 24.04 and the proper NVIDIA drivers&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir vllm cd vllm uv venv --python 3.12 --seed source .venv/bin/activate uv pip install vllm --torch-backend=auto &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How to install vLLM nightly&lt;/h1&gt; &lt;p&gt;Prerequisite: &lt;a href="https://forum.level1techs.com/t/wip-blackwell-rtx-6000-pro-max-q-quickie-setup-guide-on-ubuntu-24-04-lts-25-04/230521"&gt;Ubuntu 24.04 and the proper NVIDIA drivers&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir vllm-nightly cd vllm-nightly uv venv --python 3.12 --seed source .venv/bin/activate uv pip install -U vllm \ --torch-backend=auto \ --extra-index-url https://wheels.vllm.ai/nightly &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How to download models&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;mkdir /models cd /models uv venv --python 3.12 --seed source .venv/bin/activate pip install huggingface_hub # To download a model after going to /models and running source .venv/bin/activate mkdir /models/awq hf download cyankiwi/Devstral-2-123B-Instruct-2512-AWQ-4bit --local-dir /models/awq/cyankiwi-Devstral-2-123B-Instruct-2512-AWQ-4bit &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;If setting tensor-parallel-size 2 fails in vLLM&lt;/h1&gt; &lt;p&gt;I spent two months debugging why I cannot start vLLM with tp 2 (--tensor-parallel-size 2). It was always hanging because the two GPUs could not communicate with each other. I would only see this output in the terminal:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[shm_broadcast.py:501] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization). &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is my hardware:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CPU: AMD Ryzen 9 7950X3D 16-Core Processor Motherboard: ROG CROSSHAIR X670E HERO GPU: Dual NVIDIA RTX Pro 6000 (each at 96 GB VRAM) RAM: 192 GB DDR5 5200 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And here was the solution:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo vi /etc/default/grub At the end of GRUB_CMDLINE_LINUX_DEFAULT add md_iommu=on iommu=pt like so: GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet splash md_iommu=on iommu=pt&amp;quot; sudo update-grub &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Devstral 2 123B&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/cyankiwi/Devstral-2-123B-Instruct-2512-AWQ-4bit"&gt;cyankiwi/Devstral-2-123B-Instruct-2512-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: vllm-nightly on December 25th, 2025&lt;/p&gt; &lt;pre&gt;&lt;code&gt;hf download cyankiwi/Devstral-2-123B-Instruct-2512-AWQ-4bit --local-dir /models/awq/cyankiwi-Devstral-2-123B-Instruct-2512-AWQ-4bit vllm serve \ /models/awq/cyankiwi-Devstral-2-123B-Instruct-2512-AWQ-4bit \ --served-model-name Devstral-2-123B-Instruct-2512-AWQ-4bit \ --enable-auto-tool-choice \ --tool-call-parser mistral \ --max-num-seqs 4 \ --max-model-len 262144 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;zai-org/GLM-4.5-Air-FP8&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air-FP8"&gt;zai-org/GLM-4.5-Air-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve \ /models/original/GLM-4.5-Air-FP8 \ --served-model-name GLM-4.5-Air-FP8 \ --max-num-seqs 10 \ --max-model-len 128000 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --tool-call-parser glm45 \ --reasoning-parser glm45 \ --enable-auto-tool-choice \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;zai-org/GLM-4.6V-FP8&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/zai-org/GLM-4.6V-FP8"&gt;zai-org/GLM-4.6V-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve \ /models/original/GLM-4.6V-FP8/ \ --served-model-name GLM-4.6V-FP8 \ --tensor-parallel-size 2 \ --tool-call-parser glm45 \ --reasoning-parser glm45 \ --enable-auto-tool-choice \ --max-num-seqs 10 \ --max-model-len 131072 \ --mm-encoder-tp-mode data \ --mm_processor_cache_type shm \ --allowed-local-media-path / \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;QuantTrio/MiniMax-M2-AWQ&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/QuantTrio/MiniMax-M2-AWQ"&gt;QuantTrio/MiniMax-M2-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve \ /models/awq/QuantTrio-MiniMax-M2-AWQ \ --served-model-name MiniMax-M2-AWQ \ --max-num-seqs 10 \ --max-model-len 128000 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --pipeline-parallel-size 1 \ --enable-auto-tool-choice \ --tool-call-parser minimax_m2 \ --reasoning-parser minimax_m2_append_think \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;OpenAI gpt-oss-120b&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;openai/gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;p&gt;Note: We are running this on a single GPU&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve \ /models/original/openai-gpt-oss-120b \ --served-model-name gpt-oss-120b \ --tensor-parallel-size 1 \ --pipeline-parallel-size 1 \ --data-parallel-size 2 \ --max_num_seqs 20 \ --max-model-len 131072 \ --gpu-memory-utilization 0.85 \ --tool-call-parser openai \ --reasoning-parser openai_gptoss \ --enable-auto-tool-choice \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Qwen/Qwen3-235B-A22B&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-GPTQ-Int4"&gt;Qwen/Qwen3-235B-A22B-GPTQ-Int4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve \ /models/gptq/Qwen-Qwen3-235B-A22B-GPTQ-Int4 \ --served-model-name Qwen3-235B-A22B-GPTQ-Int4 \ --reasoning-parser deepseek_r1 \ --enable-auto-tool-choice \ --tool-call-parser hermes \ --swap-space 16 \ --max-num-seqs 10 \ --max-model-len 32768 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ"&gt;QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve \ /models/awq/QuantTrio-Qwen3-235B-A22B-Thinking-2507-AWQ \ --served-model-name Qwen3-235B-A22B-Thinking-2507-AWQ \ --reasoning-parser deepseek_r1 \ --enable-auto-tool-choice \ --tool-call-parser hermes \ --swap-space 16 \ --max-num-seqs 10 \ --max-model-len 262144 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;nvidia/Qwen3-235B-A22B-NVFP4&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/nvidia/Qwen3-235B-A22B-NVFP4"&gt;nvidia/Qwen3-235B-A22B-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;p&gt;Note: NVFP4 is slow on vLLM and RTX Pro 6000 (sm120)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;hf download nvidia/Qwen3-235B-A22B-NVFP4 --local-dir /models/nvfp4/nvidia/Qwen3-235B-A22B-NVFP4 vllm serve \ /models/nvfp4/nvidia/Qwen3-235B-A22B-NVFP4 \ --served-model-name Qwen3-235B-A22B-NVFP4 \ --reasoning-parser deepseek_r1 \ --enable-auto-tool-choice \ --tool-call-parser hermes \ --swap-space 16 \ --max-num-seqs 10 \ --max-model-len 40960 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;QuantTrio/Qwen3-VL-235B-A22B-Thinking-AWQ&lt;/h1&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/QuantTrio/Qwen3-VL-235B-A22B-Thinking-AWQ"&gt;Qwen3-VL-235B-A22B-Thinking-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM version tested: 0.12.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve \ /models/awq/QuantTrio-Qwen3-VL-235B-A22B-Thinking-AWQ \ --served-model-name Qwen3-VL-235B-A22B-Thinking-AWQ \ --reasoning-parser deepseek_r1 \ --enable-auto-tool-choice \ --tool-call-parser hermes \ --swap-space 16 \ --max-num-seqs 1 \ --max-model-len 262144 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --host 0.0.0.0 \ --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cross-posted from my blog: &lt;a href="https://www.ovidiudan.com/2025/12/25/dual-rtx-pro-6000-llm-guide.html"&gt;Guide on installing and running the best models on a dual RTX Pro 6000 rig with vLLM&lt;/a&gt; (I am not selling or promoting anything)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvk3d9/howto_running_the_best_models_on_a_dual_rtx_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvk3d9/howto_running_the_best_models_on_a_dual_rtx_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvk3d9/howto_running_the_best_models_on_a_dual_rtx_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T18:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvta3k</id>
    <title>An unnoficial and easy implementation of Nested Learning paradigm(Ali Behrouz et al, and other Google Researchers)</title>
    <updated>2025-12-26T02:31:37+00:00</updated>
    <author>
      <name>/u/Big-Welcome-3169</name>
      <uri>https://old.reddit.com/user/Big-Welcome-3169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i know this isn't a Local LLM Topic, but i need help with scaling it to a bigger model and train on a bigger dataset and language modeling, here is the link: &lt;a href="https://github.com/WindOfNature/Nested-Learning"&gt;https://github.com/WindOfNature/Nested-Learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The proof of concept there is just on scikit learn(digit) and the accuracy is bad, i think this is because of the CMS bottlenecking the vision(because CMS mutating i think?), or because no CNN and small dim(128) and small max samples(200) So i need help with trying to scale it to larger model and task such as: * Language Modeling(Generative/Autoregressive Chatbots,etc) * Larger Vision task(ImageNet)&lt;/p&gt; &lt;p&gt;and etc, Hope you guys enjoyed it(if anyone reading this), Feel free to Issues and PR to help improve this framework.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Welcome-3169"&gt; /u/Big-Welcome-3169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvta3k/an_unnoficial_and_easy_implementation_of_nested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvta3k/an_unnoficial_and_easy_implementation_of_nested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvta3k/an_unnoficial_and_easy_implementation_of_nested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T02:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv8dbb</id>
    <title>GLM 4.7 has now taken #2 on Website Arena</title>
    <updated>2025-12-25T07:52:46+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/"&gt; &lt;img alt="GLM 4.7 has now taken #2 on Website Arena" src="https://preview.redd.it/el2uxr8y2b9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=448cd56dc1a8abbef104c6aa6d319f88783428fb" title="GLM 4.7 has now taken #2 on Website Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is #1 overall amongst all open weight models and ranks just behind Gemini 3 Pro Preview, a 15-place jump from GLM 4.6&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/el2uxr8y2b9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T07:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvfj87</id>
    <title>LFM2-2.6B-Exp is an experimental checkpoint built on LFM2-2.6B using pure reinforcement learning by Liquid AI</title>
    <updated>2025-12-25T15:22:53+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvfj87/lfm226bexp_is_an_experimental_checkpoint_built_on/"&gt; &lt;img alt="LFM2-2.6B-Exp is an experimental checkpoint built on LFM2-2.6B using pure reinforcement learning by Liquid AI" src="https://preview.redd.it/xwktkxmsad9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa099d66cbd4ca2898a85f3ca1bc2781e04fa21a" title="LFM2-2.6B-Exp is an experimental checkpoint built on LFM2-2.6B using pure reinforcement learning by Liquid AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/LiquidAI/LFM2-2.6B-Exp"&gt;https://huggingface.co/LiquidAI/LFM2-2.6B-Exp&lt;/a&gt;&lt;br /&gt; From Liquid AI on ùïè: &lt;a href="https://x.com/liquidai/status/2004190178068296181"&gt;https://x.com/liquidai/status/2004190178068296181&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xwktkxmsad9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvfj87/lfm226bexp_is_an_experimental_checkpoint_built_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvfj87/lfm226bexp_is_an_experimental_checkpoint_built_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T15:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pveluj</id>
    <title>Honestly, has anyone actually tried GLM 4.7 yet? (Not just benchmarks)</title>
    <updated>2025-12-25T14:35:15+00:00</updated>
    <author>
      <name>/u/Empty_Break_8792</name>
      <uri>https://old.reddit.com/user/Empty_Break_8792</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm seeing all these charts claiming GLM 4.7 is officially the ‚ÄúSonnet 4.5 and GPT-5.2 killer‚Äù for coding and math. The benchmarks look insane, but we all know how easy it is to game those for a release day hype cycle.&lt;/p&gt; &lt;p&gt;I‚Äôm specifically curious about using it as a daily driver for complex web development. Most of my work involves managing complex TypeScript code and refactoring legacy React code.&lt;/p&gt; &lt;p&gt;For those of you who have actually hooked the API into an agent like &lt;strong&gt;Kilo Code&lt;/strong&gt; or &lt;strong&gt;OpenCode&lt;/strong&gt; (or even just &lt;strong&gt;Cline&lt;/strong&gt; / &lt;strong&gt;Roo Code&lt;/strong&gt;), how is your experience with it? Please be honest i don't just believe the benchmarks. Tell me if you really use it, and with which agent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Break_8792"&gt; /u/Empty_Break_8792 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T14:35:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvr0w0</id>
    <title>I tested GLM 4.7 and minimax-m2.1 and compared it to CC and Codex</title>
    <updated>2025-12-26T00:34:22+00:00</updated>
    <author>
      <name>/u/jstanaway</name>
      <uri>https://old.reddit.com/user/jstanaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;Claude=best, mimimax-m2.1=excellent (surprised), Codex 5.2-med=very good, GLM-4.7=bad&lt;/p&gt; &lt;p&gt;Ok, so I tested codex5.2-med today and minimax-m2.1 today. I ran these same tests on GLM 4.7 and Claude code (sonnet 4.5 and Haiku 4.5) yesterday. &lt;/p&gt; &lt;p&gt;Lets me add some background to my job I had for it. I tested it on a Vue JS frontend project. I have a parent component with 28 child components which contain different fields in each one. The job was to create one generic component that can be used in place of all 28 components. Heres what needed to happen for this to work out. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Extract the required fields from an existing JSON object I supplied to the model. It needed to extract a specific property and put it into another existing JSON object that stores some hardcoded frontend configuration. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extract some custom text from all 28 of the files for another property that will be added to the existing JSON object in #1. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pass numerous props into the new generic component including all the fields that will be displayed. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Create the generic component that will display the fields that are passed in. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Updated the type related to this data in types file. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Remove the unneeded 28 files. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Make sure the parent component can still submit successfully without modifying any of the existing logic. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Heres the results in the order that they performed from best to worst. Claude was in Claude code, Codex in the Codex CLI. Minimax and GLM-4.7 were in Opencode. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Claude (Sonnet 4.5 planning, Haiku 4.5 implementation). &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;No surprise here, Claude is a beast. Felt like it had the best most comprehensive plan to implement this. Thought of things I left out of the prompt like also extracting and creating a property for footer text that was different in each of the child components. Planned in Sonnet 4.5 and executed in Haiku 4.5. Worked perfectly on first try. Gave a really nice summary at the end outlining how many lines we eliminated etc. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;minimax-m2.1&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Kind of a surprise here. I did NOT expect this model to do this on the first try, especially because I had tested GLM-4.7 first and was let down. Plan had to be refined upon presentation, nothing major. Once I gave it the go ahead it took ~8mins. Worked on first try, no issues. Overall I was impressed. ~50% of context used, total cost $0.13&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Codex 5.2 medium&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Codex asked more refinement questions about the implementation than all the others. Guess this could be good or bad depending on how you look at it. It worked on the first try but changing the value of the dropdown which selects the content for the child component did not work properly after the initial selection. I had to prompt it and it fixed it on the second try in a couple seconds. Overall, pretty much on the first try but I figured it would be cheating if I didn't give credit to the models who actually DID get it on the first try 100%. Total time of implementation once plan approved was like ~10mins. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;GLM-4.7&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Not impressed at all. Did not successfully complete. It messed up my submission code while it got the child component functionality right. I must have prompted it maybe an additional 6-7 times and it never did get it working. It really seemed to get wrapped up in it's own thinking. Based on my experience at least with my small test job I would not use it.&lt;/p&gt; &lt;p&gt;Conclusion&lt;/p&gt; &lt;p&gt;Claude was the best, no surprise there I think. But, for a budget model like minimax I was really surprised. Did it faster than Codex and on the first try. I have ChatGPT Plus and Claude Pro so i probably won't sub to minimax but if I needed a budget model I would definitely start using it, overall impressive. Especially if you consider it should be open source. &lt;/p&gt; &lt;p&gt;I primarily use Haiku 4.5 on my Claude plan, I find it's enough for 80% of my stuff. Ive used sonnet the rest and Opus 4.5 twice since it was released. So, I get quite a bit of usage out of my CC Pro plan. I won't leave ChatGPT, I use it for everything else so Codex is a give in and an excellent option as well. I will add that I do really like the UI of Opencode. I wish CC would adopt the way the thinking is displayed in Opencode. They've improved the way the diffs are highlighted but I feel like they can still improve it more. Anyway, I hope you guys enjoy the read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jstanaway"&gt; /u/jstanaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i_tested_glm_47_and_minimaxm21_and_compared_it_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i_tested_glm_47_and_minimaxm21_and_compared_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i_tested_glm_47_and_minimaxm21_and_compared_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T00:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvpifv</id>
    <title>Steering LLM Behavior Without Fine-Tuning</title>
    <updated>2025-12-25T23:18:18+00:00</updated>
    <author>
      <name>/u/Bakkario</name>
      <uri>https://old.reddit.com/user/Bakkario</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpifv/steering_llm_behavior_without_finetuning/"&gt; &lt;img alt="Steering LLM Behavior Without Fine-Tuning" src="https://external-preview.redd.it/Du7zeDzqswWAolCphXzRi_zj33jOeXB6IU0TL-7DQwc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba4471d708a8d7ead91e841688311fb6555ded1b" title="Steering LLM Behavior Without Fine-Tuning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This video from HuggingFave is a masterpiece!! I thought it should not go unnoticed - despite the good views it has - and share it with you guys. &lt;/p&gt; &lt;p&gt;It shows how you can modify the behavior or the personality of a model at inference time, without fine-tuning or prompt engineering. It‚Äôs inspired by the Golden Gate experiment done by Anthropic. Anthropic‚Äôs researchers changed the behavior of the large language model Claude Sonnet, making it answer as if it were the Golden Gate, no fine tuning whatsoever üòÖ&lt;/p&gt; &lt;p&gt;Enjoy!! And thank you HF and Sabid who made the video üôèüèæ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bakkario"&gt; /u/Bakkario &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://m.youtube.com/watch?v=F2jd5WuT-zg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpifv/steering_llm_behavior_without_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpifv/steering_llm_behavior_without_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T23:18:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvonzn</id>
    <title>Admins, can we create GPU memory tiers</title>
    <updated>2025-12-25T22:35:25+00:00</updated>
    <author>
      <name>/u/ScoreUnique</name>
      <uri>https://old.reddit.com/user/ScoreUnique</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, it happens often that there's people with RTX 6000 PRO commenting on RTX 3050 and the other way around without sometimes realizing what tier performance is expected, can we create a new set of tags that mark different GPU tiers based on VRAM &amp;amp; RAM richness (I suppose most of us use unified memory) &lt;/p&gt; &lt;p&gt;Looking for ideas on how to better organise the sub. Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScoreUnique"&gt; /u/ScoreUnique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvonzn/admins_can_we_create_gpu_memory_tiers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvonzn/admins_can_we_create_gpu_memory_tiers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvonzn/admins_can_we_create_gpu_memory_tiers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T22:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvke55</id>
    <title>llama.cpp's recent updates - --fit flag</title>
    <updated>2025-12-25T19:09:25+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven't updated llama.cpp for last 2 weeks. Liked the new CLI after last time update.&lt;/p&gt; &lt;p&gt;Wanted to mention these PRs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16653"&gt;llama: automatically set parameters not set by the user in such a way that maximizes GPU utilization #16653&lt;/a&gt; - I was waiting for this one. Looks like this one got merged already &amp;amp; also few more related PRs too done with fixes. How many of you used &lt;code&gt;--fit&lt;/code&gt; flag on your llama.cpp commands? Please share your stats on this(Would be nice to see before &amp;amp; after results).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18343"&gt;ggml : optimize cuda cumsum fallback (~2.5x speedup vs CUB) #18343&lt;/a&gt; - This one is from latest update. (As a non-techie) I have no idea what this is &amp;amp; how it works. But the number in title ~2.5x looks nice. PR don't have t/s results with before &amp;amp; after. Somebody please share details on this. I have 4060 Laptop GPU(8GB VRAM).&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/"&gt;Previous thread&lt;/a&gt; from this sub on 1st PR topic. Sorry I had very less context/memory on this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T19:09:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvs8l3</id>
    <title>ASUS Rumored To Enter DRAM Market Next Year</title>
    <updated>2025-12-26T01:36:47+00:00</updated>
    <author>
      <name>/u/Highwaytothebeach</name>
      <uri>https://old.reddit.com/user/Highwaytothebeach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well instead of learning about AI and having a pretty small chince finding a real job with that knoweledge actually seems that right now and in near future the most proffitable is investing in AI and tech stocks. And some people make money when stocks go sharp down.&lt;/p&gt; &lt;p&gt;Because of PC CPUs are locked at max 256 RAM support for too long and also DDR market looks weird lacking higher capacity widelly affordable modules in AI times, I was thinking tons of motherboards , barebones, PSUs and alot of other hardware is just going to hit recycling facilities, despite being reasonably priced.. And found this &lt;a href="https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor"&gt;https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor&lt;/a&gt; Any chance it may be true?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Highwaytothebeach"&gt; /u/Highwaytothebeach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T01:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvgell</id>
    <title>Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)</title>
    <updated>2025-12-25T16:05:14+00:00</updated>
    <author>
      <name>/u/DecodeBytes</name>
      <uri>https://old.reddit.com/user/DecodeBytes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"&gt; &lt;img alt="Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)" src="https://b.thumbs.redditmedia.com/Af4I55UhMYXW5gk6wlYIuAjMVZjoUq-PUY2tOWlSN3E.jpg" title="Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Open Source DeepFabric, a tool that lets you:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pick any MCP server or any given set of Tools&lt;/li&gt; &lt;li&gt;A specific root topic (DevOps, Customer Care, Coding Agent)&lt;/li&gt; &lt;li&gt;Auto-generate a tool calling / reasoning topic specific dataset, with real tool traces executed within isolated webassembly components.&lt;/li&gt; &lt;li&gt;Fine-tune an SLM to become an expert at that specific MCP server using Unsloth's awesome training framework&lt;/li&gt; &lt;li&gt;Evaluate against a training-blind subset of the dataset.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We trained Qwen3-4B to outperform Claude Sonnet 4.5 and Gemini Pro 2.5 against the more challenging to use Blender MCP server.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepFabric Fine Tuned&lt;/td&gt; &lt;td align="left"&gt;93.50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;80.50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Google Gemini Pro 2.5&lt;/td&gt; &lt;td align="left"&gt;47.00%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;The idea is simple:&lt;/strong&gt; frontier models are generalists, but a small model fine-tuned on domain-specific tool calling data can become a specialist that beats them at that specific task.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x6svlmqird9g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e44c8203ce3d7383951397b5ae5b33870ceab7e0"&gt;https://preview.redd.it/x6svlmqird9g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e44c8203ce3d7383951397b5ae5b33870ceab7e0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it yourself on Google Colab using a Free T4:&lt;/strong&gt; &lt;a href="https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq"&gt;https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/always-further/deepfabric"&gt;https://github.com/always-further/deepfabric&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from the community, especially if you decide to generate your own agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DecodeBytes"&gt; /u/DecodeBytes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T16:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvr64e</id>
    <title>A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster.</title>
    <updated>2025-12-26T00:41:51+00:00</updated>
    <author>
      <name>/u/Sudden_Rip7717</name>
      <uri>https://old.reddit.com/user/Sudden_Rip7717</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"&gt; &lt;img alt="A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster." src="https://preview.redd.it/go1uf72v2g9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a49f1c4f3df4fa34d397b34c3fcf1212cd609c5d" title="A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;It has been a challenging year, but it has brought its own blessings too. I am truly grateful to God for so much more than just hardware, but I am also specifically thankful for this opportunity to upgrade my local AI research lab.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I just want to wish everyone here a Merry Christmas! Don't give up on your dreams, be ready to work hard, look boldly into the future, and try to enjoy every single day you live.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Merry Christmas and God bless!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden_Rip7717"&gt; /u/Sudden_Rip7717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go1uf72v2g9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T00:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvr6b0</id>
    <title>Well‚Ä¶ that was hard. Really.</title>
    <updated>2025-12-26T00:42:07+00:00</updated>
    <author>
      <name>/u/getheat</name>
      <uri>https://old.reddit.com/user/getheat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr6b0/well_that_was_hard_really/"&gt; &lt;img alt="Well‚Ä¶ that was hard. Really." src="https://preview.redd.it/eyfif7m23g9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a230c5521cb68c0e5695ebc4ed99d9e13da756b" title="Well‚Ä¶ that was hard. Really." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getheat"&gt; /u/getheat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eyfif7m23g9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr6b0/well_that_was_hard_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr6b0/well_that_was_hard_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T00:42:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvjpmb</id>
    <title>Why I quit using Ollama</title>
    <updated>2025-12-25T18:38:36+00:00</updated>
    <author>
      <name>/u/SoLoFaRaDi</name>
      <uri>https://old.reddit.com/user/SoLoFaRaDi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For about a year, I've used Ollama like... 24/7. It was always my go-to, as it was frequently updated and had support for every model I needed.&lt;/p&gt; &lt;p&gt;Over the past few months, there's been a serious decline in the updates &amp;amp; update content that releases with Ollama. I understand that, and just went about my day, as the maintainers obviously have a life. Cool! Then the **Cloud** update dropped. I saw Ollama as a great model runner, you just download a model and boom. Nope! They decided to combine proprietary models with the models uploaded on their Library. At first, it seemed cool. We can now run AI models that were otherwise impossible to run on consumer hardware, but then I started getting confused. Why did they add in Cloud, what's the point? What were the privacy implications? It just felt like they were adding more and more bloatware into their already massive binaries, so about a month ago, I made the decision, and quit Ollama for good.&lt;/p&gt; &lt;p&gt;I feel like with every update they are seriously straying away from the main purpose of their application; to provide a secure inference platform for LOCAL AI models. I understand they're simply trying to fund their platform with the Cloud option, but it feels like a terrible move from the Ollama maintainers. &lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoLoFaRaDi"&gt; /u/SoLoFaRaDi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T18:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvpkqo</id>
    <title>I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA</title>
    <updated>2025-12-25T23:21:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt; &lt;img alt="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" src="https://external-preview.redd.it/eHAyeXBnM2xvZjlnMcbYDDf5MmPAc5-kZmkvzc1kUbOViw5SF6SuJ_dOojri.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b4e5d7a038251df406b6345161c5136f2011960" title="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1mxlc3lof9g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T23:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
