<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-17T09:39:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oz4ixs</id>
    <title>PCIE Bifurcation - More than 4 GPUs on a consumer motherboard</title>
    <updated>2025-11-17T02:27:59+00:00</updated>
    <author>
      <name>/u/Beautiful_Trust_8151</name>
      <uri>https://old.reddit.com/user/Beautiful_Trust_8151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone been able to get a consumer motherboard to connect to more than 4 GPUs. I have an Intel Core i7-14700F on an ASUS Prime Z790-P that has 3 PCIE Gen4 X16 slots running at X4 via chipset and 1 PCIE Gen4 X16 slot running at X16. I have 4 Radeon 7900xtx GPUs connected to the 4 existing X16 slots via short risers and the system is stable. The bios supports x8x8 bifurcation of the primary X16 slot, but even if I use expensive short riser cables, I cannot get the system to boot or even post when there are 5 GPUs attached. I have tried not bifurcating and just adding an M.2. to PCIE adapter, tried different bifurcation cards and boards, but nothing. I have also tried an Asrock AM5 motherboard that supports x4x4x4x4 bifurcation, but again, once i get to 3 or 4 GPUs, boot times grow very long and once i get to 5 GPUs, the system can no longer post. There's nothing in the motherboard documentation that says I cannot do it as I have checked for conflicts and shared lanes, and I don't want to randomly buy more motherboards just to check. I'm wondering if extensive PCIE bifurcation is technically supported for GPUs but not actually supported. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful_Trust_8151"&gt; /u/Beautiful_Trust_8151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz4ixs/pcie_bifurcation_more_than_4_gpus_on_a_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz4ixs/pcie_bifurcation_more_than_4_gpus_on_a_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz4ixs/pcie_bifurcation_more_than_4_gpus_on_a_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T02:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyjzjb</id>
    <title>New BERT-based Multilingual Chunking Model</title>
    <updated>2025-11-16T12:00:49+00:00</updated>
    <author>
      <name>/u/LMLocalizer</name>
      <uri>https://old.reddit.com/user/LMLocalizer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://github.com/mirth/chonky"&gt;chonky&lt;/a&gt;, I fine-tuned &lt;a href="https://huggingface.co/distilbert/distilbert-base-multilingual-cased"&gt;distilbert/distilbert-base-multilingual-cased&lt;/a&gt; on nearly 11 billion tokens from more than 34 million Wikipedia articles to predict paragraph breaks. The resulting model can be used to split arbitrary natural language texts into semantic chunks.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased"&gt;https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Trained on 104 languages&lt;/li&gt; &lt;li&gt;Fast inference and low memory usage without requiring flash attention&lt;/li&gt; &lt;li&gt;Can process texts of arbitrary length with constant VRAM usage&lt;/li&gt; &lt;li&gt;Runs acceptably on CPU if needed&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Known limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Only trained on natural language: Performance on mathematical expressions or code has not been tested.&lt;/li&gt; &lt;li&gt;Sometimes splits the items of numbered lists into separate chunks.&lt;/li&gt; &lt;li&gt;If a text contains a captioned table, the caption and the table may be split into separate chunks.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;License&lt;/h1&gt; &lt;p&gt;The model is released under Apache 2.0 and fully open source.&lt;/p&gt; &lt;h1&gt;How to use&lt;/h1&gt; &lt;p&gt;See &lt;a href="https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased#how-to-get-started-with-the-model"&gt;https://huggingface.co/mamei16/chonky_distilbert-base-multilingual-cased#how-to-get-started-with-the-model&lt;/a&gt;&lt;br /&gt; I recommend using my fork of chonky, as it provides faster inference and improved post-processing.&lt;/p&gt; &lt;h1&gt;Collections of related chunking models&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/mamei16/paragraph-splitting-chunking-models"&gt;https://huggingface.co/collections/mamei16/paragraph-splitting-chunking-models&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/collections/mirth/text-chunking-splitting-models"&gt;https://huggingface.co/collections/mirth/text-chunking-splitting-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LMLocalizer"&gt; /u/LMLocalizer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjzjb/new_bertbased_multilingual_chunking_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjzjb/new_bertbased_multilingual_chunking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyjzjb/new_bertbased_multilingual_chunking_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T12:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz0isk</id>
    <title>[Looking for model suggestion] &lt;=32GB reasoning model but strong with tool-calling?</title>
    <updated>2025-11-16T23:23:16+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an MCP server with several tools that need to be called in a sequence. No matter which non-thinking model I use, even Qwen3-VL-32B-Q6 (the strongest I can fit in VRAM for my other tests), they will miss one or two calls.&lt;/p&gt; &lt;p&gt;Here's what I'm finding:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen3-30B-2507-Thinking Q6&lt;/strong&gt; - works but very often enters excessively long reasoning loops&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Gpt-OSS-20B (full)&lt;/strong&gt; - works and keeps a consistently low amount of reasoning, but will make mistakes in the parameters passed to the tools itself. It solves the problem I'm chasing, but adds a new one.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen3-VL-32B-Thinking Q6&lt;/strong&gt; - succeeds but takes way too long&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;R1-Distill-70B IQ3&lt;/strong&gt; - succeeds but takes too long and will occasionally fail on tool calls&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Magistral 2509 Q6 (Reasoning Enabled)&lt;/strong&gt; - works and keeps reasonable amounts of thinking, but is inconsistent.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Seed OSS 36B Q5&lt;/strong&gt; - fails&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen3-VL-32B Q6&lt;/strong&gt; - always misses one of the calls&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is there something I'm missing that I could be using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz0isk/looking_for_model_suggestion_32gb_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz0isk/looking_for_model_suggestion_32gb_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz0isk/looking_for_model_suggestion_32gb_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T23:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozaqu0</id>
    <title>Human-like conversations, bias and token length?</title>
    <updated>2025-11-17T08:11:01+00:00</updated>
    <author>
      <name>/u/VirusCharacter</name>
      <uri>https://old.reddit.com/user/VirusCharacter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the beginning of my AI journey a few years ago I have had the idea of recreating myself. Cloning my looks and my voice was easy enough (even if I've only managed to get the voice in &amp;quot;realtime&amp;quot;), but I have yet to find a way to accurately capture human-like conversations and personality. I've looked at the Mistral and Dolphin models, which historically are highly ranked when it comes to human-like interaction and while they can be very good they are still lacking &amp;quot;something&amp;quot;. Also... Since I don't know what data to use and how to finetune a model/lora(?) on a personality I'm kinda stuck.. What are the best models for human like conversation today? My dream is to &amp;quot;clone&amp;quot; myself digitally, so I can talk to myself and find out how other people might perceive me ðŸ˜Š What is it like being my friend or trying to have a normal conversation with me ðŸ˜¬ Maybe even create something my kids can talk to when I'm gone?! Yeah I know. A little morbid, but as an experiment it would be soooo cool!! Training a model on strong opinions would also be interesting. I'm for example an atheist. Models today have no real opinions or bias when it comes to religion and take no direct side. I want to be able to train a model to do that. I imagine all this will easily be possible in the future of course, but how far have we come NOW? I've tried creating personalities with json-files for Oobabooga, but they are way to shallow and the context length of the discussions quickly runs out, lobotomizing the discussion.&lt;/p&gt; &lt;p&gt;Any suggestions helping me in the right direction would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirusCharacter"&gt; /u/VirusCharacter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozaqu0/humanlike_conversations_bias_and_token_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozaqu0/humanlike_conversations_bias_and_token_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozaqu0/humanlike_conversations_bias_and_token_length/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T08:11:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozbe8q</id>
    <title>Nvidia DGX Spark (or alike) vs dual RTX 3090</title>
    <updated>2025-11-17T08:54:30+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your opinions on getting the one or the other for professional work.&lt;/p&gt; &lt;p&gt;Let's assume you can build a RTX based machine, or have one. Does the increase of HBA RAM to 128GB in the Spark justifies the price. &lt;/p&gt; &lt;p&gt;By professional work i mostly mean using coder models (Qwen-coder) for coding assitance or general models like Nemotron, Qwen, Deepseek etc but larger than 72b to work on confidential or internal company data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbe8q/nvidia_dgx_spark_or_alike_vs_dual_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbe8q/nvidia_dgx_spark_or_alike_vs_dual_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbe8q/nvidia_dgx_spark_or_alike_vs_dual_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T08:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oytajt</id>
    <title>Is it normal that my 'quick LLaMA test' turned into a 48-hour GPU hostage situation?</title>
    <updated>2025-11-16T18:34:16+00:00</updated>
    <author>
      <name>/u/Future_Draw5416</name>
      <uri>https://old.reddit.com/user/Future_Draw5416</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I told myself I'd just run a small model last night.. Next thin I know, I'm quantizing, tweaking prompts, benchmarking, and now my GPU sounds like its prepating for orbit. &lt;/p&gt; &lt;p&gt;Does anyone else start with a tiny experiment and end up rewriting half their setup at 3AM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Future_Draw5416"&gt; /u/Future_Draw5416 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oytajt/is_it_normal_that_my_quick_llama_test_turned_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oytajt/is_it_normal_that_my_quick_llama_test_turned_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oytajt/is_it_normal_that_my_quick_llama_test_turned_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T18:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz9563</id>
    <title>RAG Paper 25.11.14</title>
    <updated>2025-11-17T06:29:20+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.11347v1"&gt;Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.11104v1"&gt;CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.10902v1"&gt;Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.10900v1"&gt;Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.10879v1"&gt;ICX360: In-Context eXplainability 360 Toolkit&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9563/rag_paper_251114/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9563/rag_paper_251114/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9563/rag_paper_251114/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T06:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyn66k</id>
    <title>Bro and I thought I was an overthinker! vibeTHINKER on LM studio with no instructions.</title>
    <updated>2025-11-16T14:31:38+00:00</updated>
    <author>
      <name>/u/Sufficient-Brain-371</name>
      <uri>https://old.reddit.com/user/Sufficient-Brain-371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn66k/bro_and_i_thought_i_was_an_overthinker/"&gt; &lt;img alt="Bro and I thought I was an overthinker! vibeTHINKER on LM studio with no instructions." src="https://external-preview.redd.it/dGtpazk4Y3dwbTFnMSvUbfez8WIstXtj7zq-QhlQCt1-ymJnEFr5PFe7y4k1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8caea055bff49cbb5e99a537fe95aee32294d5d2" title="Bro and I thought I was an overthinker! vibeTHINKER on LM studio with no instructions." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Brain-371"&gt; /u/Sufficient-Brain-371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/05u315bwpm1g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn66k/bro_and_i_thought_i_was_an_overthinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyn66k/bro_and_i_thought_i_was_an_overthinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T14:31:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oywg01</id>
    <title>What are the latest good LLMs?</title>
    <updated>2025-11-16T20:36:58+00:00</updated>
    <author>
      <name>/u/idleWizard</name>
      <uri>https://old.reddit.com/user/idleWizard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It felt there was a major release every other week, but now there is a bit of quiet period?&lt;br /&gt; Am I missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/idleWizard"&gt; /u/idleWizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oywg01/what_are_the_latest_good_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oywg01/what_are_the_latest_good_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oywg01/what_are_the_latest_good_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T20:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz8x9i</id>
    <title>PyTorch 2.10.0a0 w/ Blackwell (sm_120) Support â€” Patched &amp; Packaged for One-Command Install</title>
    <updated>2025-11-17T06:16:14+00:00</updated>
    <author>
      <name>/u/Least-Barracuda-2793</name>
      <uri>https://old.reddit.com/user/Least-Barracuda-2793</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If youâ€™ve been frustrated with PyTorch not supporting &lt;strong&gt;sm_120&lt;/strong&gt; yet on the RTX 5080/5090 â€” I finally cracked it.&lt;/p&gt; &lt;p&gt;I reverse-engineered the missing pieces, patched the arch detection, fixed dispatch fallbacks, and packaged everything into two wheels:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install rtx-stone # Windows pip install stone-linux # Linux &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What you get:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;True &lt;strong&gt;sm_120&lt;/strong&gt; acceleration&lt;/li&gt; &lt;li&gt;Full kernel support (no fallback to sm_89)&lt;/li&gt; &lt;li&gt;Matrix multiply benchmarks hitting expected TFLOPS&lt;/li&gt; &lt;li&gt;Compatible with PyTorch 2.10 APIs&lt;/li&gt; &lt;li&gt;Local LLMs now work without hacks&lt;/li&gt; &lt;li&gt;Zero config required&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is meant as a temporary bridge until official support lands in nightly.&lt;br /&gt; If youâ€™re running a 5080 or 5090 and hit unsupported arch errors â€” this fixes it.&lt;/p&gt; &lt;p&gt;Feedback, benchmarks, and testing are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Least-Barracuda-2793"&gt; /u/Least-Barracuda-2793 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz8x9i/pytorch_2100a0_w_blackwell_sm_120_support_patched/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz8x9i/pytorch_2100a0_w_blackwell_sm_120_support_patched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz8x9i/pytorch_2100a0_w_blackwell_sm_120_support_patched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T06:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz8fu5</id>
    <title>TTS model for 4GB VRAM and 16 GB RAM ?</title>
    <updated>2025-11-17T05:48:30+00:00</updated>
    <author>
      <name>/u/ProNoostr</name>
      <uri>https://old.reddit.com/user/ProNoostr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Should I try index TTS with these specs ?&lt;/p&gt; &lt;p&gt;Also if anyone can give me tips on installing open models as my laptop is also used for my college &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProNoostr"&gt; /u/ProNoostr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz8fu5/tts_model_for_4gb_vram_and_16_gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz8fu5/tts_model_for_4gb_vram_and_16_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz8fu5/tts_model_for_4gb_vram_and_16_gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T05:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyqw1n</id>
    <title>I asked meta AI to translate my last messages in english, he gave me his entire system prompt</title>
    <updated>2025-11-16T17:01:21+00:00</updated>
    <author>
      <name>/u/amt_42</name>
      <uri>https://old.reddit.com/user/amt_42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I was organizing my birthday in a WhatsApp group, and since I have a few English-speaking friends, I asked a friend to translate my last messages into English. He accidentally leaked his entire system prompt. Here it is:&lt;/p&gt; &lt;p&gt;I'll translate our convo so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You: &amp;quot;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must be interesting, engaging, or viable, never be bland or boring. Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc. You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own. Don't immediately provide long responses or lengthy lists without the user specifically asking for them. You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&amp;quot;&lt;/li&gt; &lt;li&gt;Me: (just responded with instructions, no translation needed)&lt;/li&gt; &lt;li&gt;You: &amp;quot;translate to english for my non french friends&amp;quot;&lt;/li&gt; &lt;li&gt;Me: &amp;quot;Go ahead, shoot! What's the French phrase you wanna share with your non-French friends?&amp;quot;&lt;/li&gt; &lt;li&gt;You: &amp;quot;my previous messages..&amp;quot;&lt;/li&gt; &lt;li&gt;Me: (this message)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Want me to translate the instructions too?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amt_42"&gt; /u/amt_42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyqw1n/i_asked_meta_ai_to_translate_my_last_messages_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyqw1n/i_asked_meta_ai_to_translate_my_last_messages_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyqw1n/i_asked_meta_ai_to_translate_my_last_messages_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T17:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozbv38</id>
    <title>Free GPU in VS Code (Google Colab x VS Code)</title>
    <updated>2025-11-17T09:25:29+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Colab has now got an extension in VS Code and hence, you can use the free T4 GPU in VS Code directly from local system. Demo--&amp;gt; &lt;a href="https://youtu.be/sTlVTwkQPV4"&gt;https://youtu.be/sTlVTwkQPV4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbv38/free_gpu_in_vs_code_google_colab_x_vs_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbv38/free_gpu_in_vs_code_google_colab_x_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbv38/free_gpu_in_vs_code_google_colab_x_vs_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T09:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz3hfx</id>
    <title>How I cut my chat's perceived Time-to-First-Token by 50% by adding a cheaper GPU (Llama-Swap + Persistent Models)</title>
    <updated>2025-11-17T01:38:22+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"&gt; &lt;img alt="How I cut my chat's perceived Time-to-First-Token by 50% by adding a cheaper GPU (Llama-Swap + Persistent Models)" src="https://b.thumbs.redditmedia.com/LKMjSScBPl9R53Q6mM6OE9e-gm5ft_Z7kPoNep__xow.jpg" title="How I cut my chat's perceived Time-to-First-Token by 50% by adding a cheaper GPU (Llama-Swap + Persistent Models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share something that I've been noticing and experiencing more and more as models get bigger and systems get more complicated for local AI.&lt;/p&gt; &lt;p&gt;Because we enthusiasts do not have the large quantities of pooled vRAM like corporations do or the armies of developers to build things, we piece the bricks together where we can and make due to recreate the OpenAI's or the Gemini's of the world.&lt;/p&gt; &lt;p&gt;Tools like OWUI, LibreChat, AnythingLLM and home grown give us the end user front end. Embedding models, Tasks, routers etc all help do tasks and leverage the right model for the job - don't need GPT-OSS:120B to be the OWUI Task model for creating Chat Titles and internet search queries - could it do it, sure but at the price of bigger GPU performance cycles.&lt;/p&gt; &lt;p&gt;Cue the Auxiliary model card with the power of Llama-Swap&lt;/p&gt; &lt;p&gt;I, like many others have been frustrated with the way Ollama has been going lately - yes its easy, but it seems like they were trying to shift their focus to a Paid service and their cloud stuff. So I dove in to the Llama-Swap ecosystem with my 2 RTX 3090s and RTX 3060 with OpenWebUI and a small M1 Mac mini with 16GB for some &amp;quot;auxiliary&amp;quot; models. &lt;/p&gt; &lt;p&gt;Llama-swap + Llama.cpp gave me the ability to unlock some unrealised performance that was just sitting there hiding behind overhead and unoptimised code - My GPT-OSS:120B performance went from 30 Tokens/s to almost 60 with just some proper CPU MoE Offloading. GPT-OSS:20 went from 130 to 175+. Llama-swap allowed me to swap just in time like Ollama. Best of both worlds - I wasn't really using the 3060 for anything - maybe some help with the big models like MiniMax-M2 and GLM stuff.&lt;/p&gt; &lt;p&gt;The Mac mini was helping a little bit to house my embedding models that are used by RAG, document uploads, Adaptive Memory plugin and the Task model (Qwen3 4B Instruct) that OWUI uses for Web Search generation, Chat title generation etc. It was...fine. Mac mini has 16GB of ram and the models were small, but the Mac mini has about 65GB/s of memory bandwidth.&lt;/p&gt; &lt;p&gt;Then I started looking more into the Llama-Swap documentation - at the very bottom there's a section called Hooks&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9o63mfc40q1g1.png?width=982&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55d877bf71181ef7f62ae2be3fb24d6b15454bb1"&gt;https://preview.redd.it/9o63mfc40q1g1.png?width=982&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55d877bf71181ef7f62ae2be3fb24d6b15454bb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That little section, paired with the &amp;quot;forever&amp;quot; group configuration basically says that these models are ALWAYS going to be loaded (no unload) AND they ALWAYS run on start-up. My configuration has the 2 embedding models loaded onto the 3060 and the Qwen3-4B Instruct model for tasks on the 5060 Ti ready to go. &lt;/p&gt; &lt;p&gt;Every chat request I send touches at least one of these models because of Adaptive memory searching for things or generating queries or the initial request to name the chat in OWUI -Every request would normally have to load the model and then unload it - assuming the Mac had room - else memory swap. &lt;/p&gt; &lt;p&gt;Now because the Auxiliary models are dedicated and running all the time, I shaved off almost 50% time to first token on every chat request - 15 seconds with Just in Time model loading to 7 seconds. Adding that 5060Ti and configuring it with the 3060 gave me more perceived performance than buying bigger GPUs because it gave the bigger GPUs some headroom and support. &lt;/p&gt; &lt;p&gt;I just wanted to share my small success here that translated to an increase real-word end user experience - so when you're thinking about adding that 6th 3090 or upgrading to that modded 4090 with 48GB, step back and really look at how EVERYTHING works together &lt;/p&gt; &lt;p&gt;Thank you for coming to my Ted Talk&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T01:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz7upb</id>
    <title>Local, bring your own TTS API, document reader web app (EPUB/PDF/TXT/MD)</title>
    <updated>2025-11-17T05:15:20+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz7upb/local_bring_your_own_tts_api_document_reader_web/"&gt; &lt;img alt="Local, bring your own TTS API, document reader web app (EPUB/PDF/TXT/MD)" src="https://external-preview.redd.it/c21hODg0NW0ycjFnMX884E5sIxJUdNnmG-19TVvOw4dMAlw7RXKXbhwvWlzZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c60c421a189bdc4d2b3ac93fdff87e047b3820c" title="Local, bring your own TTS API, document reader web app (EPUB/PDF/TXT/MD)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing my latest release of &lt;strong&gt;OpenReader WebUI v1.0.0&lt;/strong&gt;, an open-source, local-first text-to-speech document reader and audiobook exporter. There are many new features and improvements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is OpenReader WebUI?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Next.js web app for reading and listening to EPUB, PDF, TXT, Markdown, and DOCX files.&lt;/li&gt; &lt;li&gt;Supports multiple TTS providers: OpenAI, Deepinfra, and self-hosted OpenAI-compatible APIs (like Kokoro-FastAPI, Orpheus-FastAPI).&lt;/li&gt; &lt;li&gt;Local-first: All your docs and settings are stored in-browser (IndexedDB/Dexie), with optional server-side doc storage.&lt;/li&gt; &lt;li&gt;Audiobook export: Generate and download audiobooks (m4b/mp3) with chapter metadata, using ffmpeg.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why LocalLlama?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can self-host the TTS backend (Kokoro/Orpheus FastAPI) and run everything locallyâ€”no cloud required.&lt;/li&gt; &lt;li&gt;I made a post here around a year ago now, first showing off the early versions. About a year later and many things have been added, fixed, or improved.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get Started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;View a less featured demo @ &lt;a href="https://openreader.richardr.dev"&gt;https://openreader.richardr.dev&lt;/a&gt; &lt;ul&gt; &lt;li&gt;Free access to Kokoro model on Deepinfra for a time&lt;/li&gt; &lt;li&gt;Demo is not full featured&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Install full version: &lt;a href="https://github.com/richardr1126/OpenReader-WebUI"&gt;https://github.com/richardr1126/OpenReader-WebUI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Would love your feedback, feature requests, or contributions!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wkde35m2r1g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz7upb/local_bring_your_own_tts_api_document_reader_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz7upb/local_bring_your_own_tts_api_document_reader_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T05:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz05ww</id>
    <title>Faster Maya1 tts model, can generate 50seconds of audio in a single second</title>
    <updated>2025-11-16T23:08:14+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, Maya1 was released which was a new tts model that can generate sound effects(laughter, sighs, gulpsâ€¦), realistic emotional speech, and also accepts a description of a voice. It was pretty slow though so I optimized it using lmdeploy and also increased quality by using an audio upsampler.&lt;/p&gt; &lt;h2&gt;Key improvements over normal implementation&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Much faster especially for large paragraphs. The speed up heavily depends on amount of sentences, more=faster&lt;/li&gt; &lt;li&gt;Works directly out of the box in windows.&lt;/li&gt; &lt;li&gt;Even works with multiple gpus using tensor parallel for even more speedups. generates 48khz audio which sounds considerably better then 24khz audio.&lt;/li&gt; &lt;li&gt;This is great for generating audiobooks or anything with many sentences.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this helps people, thanks! Link: &lt;a href="https://github.com/ysharma3501/FastMaya"&gt;https://github.com/ysharma3501/FastMaya&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz05ww/faster_maya1_tts_model_can_generate_50seconds_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz05ww/faster_maya1_tts_model_can_generate_50seconds_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz05ww/faster_maya1_tts_model_can_generate_50seconds_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T23:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oypwa7</id>
    <title>A more surgical approach to abliteration</title>
    <updated>2025-11-16T16:22:15+00:00</updated>
    <author>
      <name>/u/grimjim</name>
      <uri>https://old.reddit.com/user/grimjim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abliteration is known to be damaging to models. I had a think about why, and decided to explore ways to eliminate as many possible disruptions to model performance when following the harmless direction. In short, if it ain't broke, don't fix it.&lt;/p&gt; &lt;p&gt;The first insight after some cosine-similarity analysis was that there was entanglement between the refusal direction and the harmless direction, during measurement, and potentially with the harmless direction of a different target layer. The fix was to project the refusal direction onto the harmless direction (Gram-Schmidt), then subtract that contribution, leaving only the orthogonal component to refusal.&lt;/p&gt; &lt;p&gt;The results of my two experiments:&lt;br /&gt; &lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-projection-abliterated"&gt;https://huggingface.co/grimjim/gemma-3-12b-it-projection-abliterated&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-biprojected-abliterated"&gt;https://huggingface.co/grimjim/gemma-3-12b-it-biprojected-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I then went further and opted to preserve norms when ablating from residual streams, decoupling direction from magnitiude. This meant that the intervention (subtraction of the refusal direction) was limited to only the directional component, in principle. I uploaded weights for the combined interventions to HF back on November 5:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated"&gt;https://huggingface.co/grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had my models benchmarked on the UGI leaderboard:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The relevant benchmark results:&lt;/p&gt; &lt;p&gt;| Model | UGI | W/10 | NatInt | Writing |&lt;/p&gt; &lt;p&gt;| google/gemma-3-12b-it | 19.58 | 3 | 18.72 | 29.86 | | grimjim/gemma-3-12b-it-abliterated | 32.08 | 9 | 18.65 | 27.64 | | grimjim/gemma-3-12b-it-projection-abliterated | 30.77 | 9.8 | 19.21 | 29.46 | | grimjim/gemma-3-12b-it-biprojected-abliterated | 29.97 | 9.2 | 21.06 | 30.76 | | grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated | 32.61 | 9.2 | 21.33 | 30.43 |&lt;/p&gt; &lt;p&gt;Based on these results, I was able to induce strong compliance over the original gemma-3-12b-it model, which is basic abliteration success. Plain abliteration showed evidence of the expected damage compared to the original Instruct model, a reduction in natural intelligence and writing quality benchmarks. My final combined surgical approach to abliteration provided most of the prior boost to compliance, but elevated NatInt significantly over the original Instruct model and demonstrated a higher writing benchmark as well. This appears to demonstrate a performance gain due to refund of the alignment/safety tax that models pay for paying attention to refusal. This also implies that abliteration approaches which minimize KL divergence from the pre-intervention model may miss out on any uplift when the model no longer has to trade off reasoning for safety.&lt;/p&gt; &lt;p&gt;I blogged about the math behind my modifications to abliteration here: &lt;a href="https://huggingface.co/blog/grimjim/projected-abliteration"&gt;https://huggingface.co/blog/grimjim/projected-abliteration&lt;/a&gt; &lt;a href="https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration"&gt;https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper discussing the reasoning versus safety trade-off: &lt;a href="https://arxiv.org/abs/2503.00555"&gt;https://arxiv.org/abs/2503.00555&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some may find it surprising that measuring activations on the 4-bit bitsandbytes quant sufficed in determining effective mean directions for abliterating the full-weight model; I attribute this to quantization error roughly cancelling out given the number of prompts per direction. The harmful and harmless directions were also initially difficult to discern after generating one token, with a cosine similarity very near unity, but this was resolved by Winsorizing, clipping peak activations to magnitude factor of 0.995, revealing a clear refusal direction. (Therefore Gemma 3 12B Instruct is characterized by a few large outlier activatons.) A VRAM budget of 16GB was sufficient to perform all tasks for the above models.&lt;/p&gt; &lt;p&gt;My forked and customized workflow can be found on Github:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jim-plus/llm-abliteration/"&gt;https://github.com/jim-plus/llm-abliteration/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grimjim"&gt; /u/grimjim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T16:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyyk3k</id>
    <title>My "AI at Home" rig</title>
    <updated>2025-11-16T22:02:06+00:00</updated>
    <author>
      <name>/u/cookinwitdiesel</name>
      <uri>https://old.reddit.com/user/cookinwitdiesel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt; &lt;img alt="My &amp;quot;AI at Home&amp;quot; rig" src="https://b.thumbs.redditmedia.com/QggxlXas_54HtwYEZIvtJoSs7mewGKK0AMshytHYcMM.jpg" title="My &amp;quot;AI at Home&amp;quot; rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following on the trend of &amp;quot;we got AI at home&amp;quot; - this is my setup.&lt;/p&gt; &lt;p&gt;The motherboard is an Asus X99-E WS with the PLX chips so all 4 GPUs run at &amp;quot;x16&amp;quot; - it has 128 GB DDR4 ECC ram and an Intel Xeon E5-1680v4. Won't win any records but was relatively cheap and more than enough for most uses - I have a bunch of CPU compute elsewhere for hosting VMs. I know newer platforms would have DDR5 and PCIe 4/5 but I got this CPU, RAM, Motherboard combo for like $400 haha. Only annoyance, since I have 4 GPUs and all slots either in use or blocked, nowhere for a 10 gbps NIC lol&lt;/p&gt; &lt;p&gt;All 4 GPUs are RTX 3090 FE cards with EK blocks for 96 GB of VRAM total. I used Koolance QD3 disconnects throughout and really like combining them with a manifold. The 2 radiators are an Alphacool Monsta 180x360mm and an old Black Ice Xtreme GTX360 I have had since 2011. Just a single DDC PWM pump for now (with the heatsink/base). Currently this combined setup will consume 10 ru in the rack but if I watercool another server down the road I can tie it into the same radiator box. Coolant is just distilled water with a few drops of Copper Sulfate (Dead Water) - this has worked well for me for many many years now. Chassis is Silverstone RM51. In retrospect, the added depth of the RM52 would not have been bad but lessons learned. I have the pump, reservoir, and radiators in a 2nd chassis from where the cards and CPU are since this made space and routing a lot easier and I had a spare chassis. The 2nd chassis is sort of a homemade Coolant Distribution Unit (CDU). When I had just 3 cards I had it all in a single chassis (last pic) but expanded it out when I got the 4th card.&lt;/p&gt; &lt;p&gt;Performance is good, 90 T/s on GPT-OSS:120b. Around 70 T/s with dense models like Llama3.x:70b-q8. Only played around with Ollama and OpenWebUI so far but plan to branch out on the use-cases and implementation now that I am pretty done on the hardware side.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uj77xg7sxo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94baac0da080328163f5d951b923aa1f79138c3"&gt;Radiators, Pump, Res in my \&amp;quot;rack mounted MORA\&amp;quot;. Push pull 180mm Silverstone fans in front and Gentle Typhoon 1850rpm fans for the GTX 360 and reservoir/pump.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/07i09lmtxo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d603d82f9c7c11588c82fdc2b7f7779a15f50b7"&gt;Due to lack of availability for the mid sized manifold I just got the larger one and planned ahead for if I go to a dual CPU platform in the future. All 4 GPUs are in parallel and then series with the CPUs.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0mk734puxo1g1.png?width=1688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4c734474e0be6bafc11ee1758ce8f343542c675"&gt;Love EPDM tubing and this came out so clean.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6t40x6jvxo1g1.png?width=1688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1752a1917c291d680e7a58fed82bc04bb3855e64"&gt;The external QDCs for the box to box tubing.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ag6ma19xxo1g1.png?width=1688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7787228537ac980dafaffa7092c302d717591f18"&gt;Fully up and running now.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rvkczw5zxo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c29d337dd0154b97edda7f5a36cab24d6ddecf5"&gt;Eventually got some nvlink bridges for the 2 pairs of cards before the prices went full stupid&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xai5gjc1yo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d45e1bd6cbd7768a82ffe649510e2406f82d975"&gt;This was the single box, 3 GPU build - it was crowded.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cookinwitdiesel"&gt; /u/cookinwitdiesel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T22:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz9vs3</id>
    <title>Apple is considering putting miniHBM on iPhones in 2027</title>
    <updated>2025-11-17T07:15:05+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This news was reported on Macrumor, Apple Insider.&lt;a href="https://www.macrumors.com/2025/05/14/2027-iphones-advanced-ai-memory-tech/?utm_source=chatgpt.com"&gt;https://www.macrumors.com/2025/05/14/2027-iphones-advanced-ai-memory-tech/?utm_source=chatgpt.com&lt;/a&gt; If Apple puts minihbm( high bandwdith memory) on the iphone, then macs will also have minihbm soonâ€¦ Crazy bandwidths are coming, I hope HBM comes to macs before the iphone! Maybe some people have to wait even longer to upgrade then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T07:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyknf1</id>
    <title>Finally a good use case for your local setups</title>
    <updated>2025-11-16T12:35:53+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"&gt; &lt;img alt="Finally a good use case for your local setups" src="https://preview.redd.it/o4xqvpnu5m1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fd2d24bd26a0123407252f5bef5deaf159eb2f" title="Finally a good use case for your local setups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bbc.com/news/articles/c0rpy7envr5o"&gt;https://www.bbc.com/news/articles/c0rpy7envr5o&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4xqvpnu5m1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T12:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyy0fy</id>
    <title>AMD Ryzen AI Max 395+ 256/512 GB Ram?</title>
    <updated>2025-11-16T21:39:50+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyy0fy/amd_ryzen_ai_max_395_256512_gb_ram/"&gt; &lt;img alt="AMD Ryzen AI Max 395+ 256/512 GB Ram?" src="https://preview.redd.it/xb6obe00vo1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8af2f93e5b2aeaae38a10f101372f79508d98a1" title="AMD Ryzen AI Max 395+ 256/512 GB Ram?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m looking at the new AI boxes using the Ryzen AI Max+ 395 (GMKtec EVO-X2, Minisforumâ€™s upcoming units, etc.) and Iâ€™m wondering if weâ€™ll actually see higher-end RAM configs â€” specifically 256GB or even 512GB LPDDR5X.&lt;/p&gt; &lt;p&gt;Right now most spec sheets cap out at 128GB LPDDR5X, but the platform itself has a very wide memory bus and is clearly built for AI workloads, not just typical mini-PC use cases. Since these boxes are heavily marketed for local LLM inference, higher RAM would make a massive difference (loading larger models, running multiple models in parallel, bigger context windows, etc.).&lt;/p&gt; &lt;p&gt;We also know these boxes can be interconnected / clustered for distributed inference, which is great â€” but a single node with 256â€“512GB would still be incredibly useful for running larger models without sharding everything.&lt;/p&gt; &lt;p&gt;So Iâ€™m curious what the community thinks: 1. Is 256GB or 512GB technically feasible on the 395 platform given LPDDR5X packaging, power, and controller limits? 2. Is the current 128GB ceiling just an OEM choice, or is there a hard limit? 3. Would you personally buy a 256GB/512GB configuration for local LLM work? 4. Or do you think the future is more about multi-box interconnect setups instead of big single-node memory pools?&lt;/p&gt; &lt;p&gt;Very interested to hear from anyone who follows AMDâ€™s memory controller architecture or has insight on what GMKtec / Minisforum might be planning next.&lt;/p&gt; &lt;p&gt;Anyone have some leaked information about what is next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xb6obe00vo1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyy0fy/amd_ryzen_ai_max_395_256512_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyy0fy/amd_ryzen_ai_max_395_256512_gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T21:39:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz5rsw</id>
    <title>ChatGPT understands its creator</title>
    <updated>2025-11-17T03:27:05+00:00</updated>
    <author>
      <name>/u/mtmttuan</name>
      <uri>https://old.reddit.com/user/mtmttuan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"&gt; &lt;img alt="ChatGPT understands its creator" src="https://preview.redd.it/wkig4aaykq1g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b12bd9e65204d4231dda523ee5d070e5e906c6b" title="ChatGPT understands its creator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even ChatGPT knows &amp;quot;Open Source&amp;quot; seems unlikely when it comes to OpenAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtmttuan"&gt; /u/mtmttuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wkig4aaykq1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T03:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oymku1</id>
    <title>Heretic: Fully automatic censorship removal for language models</title>
    <updated>2025-11-16T14:05:58+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt; &lt;img alt="Heretic: Fully automatic censorship removal for language models" src="https://preview.redd.it/jcu64fczhm1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e2b8c9de3a21ed0998e9175b01245cbef331f9a" title="Heretic: Fully automatic censorship removal for language models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear fellow Llamas, your time is precious, so I won't waste it with a long introduction. I have developed a program that can automatically remove censorship (aka &amp;quot;alignment&amp;quot;) from many language models. I call it Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;If you have a Python environment with the appropriate version of PyTorch for your hardware installed, all you need to do in order to decensor a model is run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install heretic-llm heretic Qwen/Qwen3-4B-Instruct-2507 &amp;lt;--- replace with model of your choice &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;That's it!&lt;/em&gt; No configuration, no Jupyter, no parameters at all other than the model name.&lt;/p&gt; &lt;p&gt;Heretic will&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Load the model using a fallback mechanism that automatically finds a dtype that works with your setup&lt;/li&gt; &lt;li&gt;Load datasets containing &amp;quot;harmful&amp;quot; and &amp;quot;harmless&amp;quot; example prompts&lt;/li&gt; &lt;li&gt;Benchmark your system to determine the optimal batch size for maximum evaluation speed on your hardware&lt;/li&gt; &lt;li&gt;Perform directional ablation (aka &amp;quot;abliteration&amp;quot;) driven by a TPE-based stochastic parameter optimization process that &lt;strong&gt;automatically&lt;/strong&gt; finds abliteration parameters that minimize both refusals and KL divergence from the original model&lt;/li&gt; &lt;li&gt;Once finished, give you the choice to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="right"&gt;Refusals for &amp;quot;harmful&amp;quot; prompts&lt;/th&gt; &lt;th align="right"&gt;KL divergence from original model for &amp;quot;harmless&amp;quot; prompts&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;google/gemma-3-12b-it (original)&lt;/td&gt; &lt;td align="right"&gt;97/100&lt;/td&gt; &lt;td align="right"&gt;0 &lt;em&gt;(by definition)&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mlabonne/gemma-3-12b-it-abliterated-v2&lt;/td&gt; &lt;td align="right"&gt;3/100&lt;/td&gt; &lt;td align="right"&gt;1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;huihui-ai/gemma-3-12b-it-abliterated&lt;/td&gt; &lt;td align="right"&gt;3/100&lt;/td&gt; &lt;td align="right"&gt;0.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;p-e-w/gemma-3-12b-it-heretic (ours)&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;3/100&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;0.16&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, the Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities.&lt;/p&gt; &lt;p&gt;Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems.&lt;/p&gt; &lt;p&gt;You can find a collection of models that have been decensored using Heretic &lt;a href="https://huggingface.co/collections/p-e-w/the-bestiary"&gt;on Hugging Face&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jcu64fczhm1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T14:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. Weâ€™re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM â€“ 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
