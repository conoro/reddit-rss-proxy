<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-28T05:48:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nrf6s3</id>
    <title>Yes you can run 128K context GLM-4.5 355B on just RTX 3090s</title>
    <updated>2025-09-26T22:41:00+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"&gt; &lt;img alt="Yes you can run 128K context GLM-4.5 355B on just RTX 3090s" src="https://b.thumbs.redditmedia.com/rWEspXLsl6q38iq4HW7So1f92J_p3bfTwDjsTnkup2Y.jpg" title="Yes you can run 128K context GLM-4.5 355B on just RTX 3090s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why buy expensive GPUs when more RTX 3090s work too :D&lt;/p&gt; &lt;p&gt;You just get more GB/$ on RTX 3090s compared to any other GPU. Did I help deplete the stock of used RTX 3090s? Maybe.&lt;/p&gt; &lt;p&gt;Arli AI as an inference service is literally just run by one person (me, Owen Arli), and to keep costs low so that it can stay profitable without VC funding, RTX 3090s were clearly the way to go. &lt;/p&gt; &lt;p&gt;To run these new larger and larger MoE models, I was trying to run 16x3090s off of one single motherboard. I tried many motherboards and different modded BIOSes but in the end it wasn't worth it. I realized that the correct way to stack MORE RTX 3090s is actually to just run multi-node serving using vLLM and ray clustering.&lt;/p&gt; &lt;p&gt;This here is GLM-4.5 AWQ 4bit quant running with the full 128K context (131072 tokens). Doesn't even need an NVLink backbone or 9999 Gbit networking either, this is just over a 10Gbe connection across 2 nodes of 8x3090 servers and we are getting a good 30+ tokens/s generation speed consistently per user request. Pipeline parallel seems to be very forgiving of slow interconnects.&lt;/p&gt; &lt;p&gt;While I realized that by stacking more GPUs with pipeline parallels across nodes, it almost linearly increases the prompt processing speed. So we are good to go in that performance metric too. Really makes me wonder who needs the insane NVLink interconnect speeds, even large inference providers probably don't really need anything more than PCIe 4.0 and 40Gbe/80Gbe interconnects.&lt;/p&gt; &lt;p&gt;All you need to run this is follow vLLM's guide on how to run multi node serving (&lt;a href="https://docs.vllm.ai/en/stable/serving/parallelism%5C_scaling.html#what-is-ray"&gt;https://docs.vllm.ai/en/stable/serving/parallelism\_scaling.html#what-is-ray&lt;/a&gt;) and then run the model with setting --tensor-parallel to the maximum number of GPUs per node and set --pipeline-parallel to the number of nodes you have. The point is to make sure inter-node communication is only for pipeline parallel which does not need much bandwidth.&lt;/p&gt; &lt;p&gt;The only way for RTX 3090s to be obsolete and prevent me from buying them is if Nvidia releases 24GB RTX 5070Ti Super/5080 Super or Intel finally releases the Arc B60 48GB in any quantity to the masses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nrf6s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T22:41:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nscj3q</id>
    <title>ollama: on CPU, no more num_threads, how to limit?</title>
    <updated>2025-09-28T02:03:34+00:00</updated>
    <author>
      <name>/u/vap0rtranz</name>
      <uri>https://old.reddit.com/user/vap0rtranz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama removed the num_thread parameter. The runtime server verifies that it's not configurable (/set parameter), and the modelfile README no longer lists num_thread: &lt;a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md"&gt;https://github.com/ollama/ollama/blob/main/docs/modelfile.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How can I limit the # of threads sent to CPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vap0rtranz"&gt; /u/vap0rtranz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nscj3q/ollama_on_cpu_no_more_num_threads_how_to_limit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nscj3q/ollama_on_cpu_no_more_num_threads_how_to_limit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nscj3q/ollama_on_cpu_no_more_num_threads_how_to_limit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T02:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns8dxj</id>
    <title>is there any android llm server apps that support local gguf or onnx models ?</title>
    <updated>2025-09-27T22:35:54+00:00</updated>
    <author>
      <name>/u/Netsnake_</name>
      <uri>https://old.reddit.com/user/Netsnake_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i did use Mnn chat its fast with tiny models but so slow with large ones 3b,4b,7b i am using oneplus13 with sd 8 elite, i could run some models fast,i got arrond 65t/s but no api server to use with external frontends. what i am looking for is an app that can create llm server that support local gguf or onnx models. i didnt try with termux yet cause i dont know any solution exept creating olama server that as i know ist fast enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Netsnake_"&gt; /u/Netsnake_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns8dxj/is_there_any_android_llm_server_apps_that_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns8dxj/is_there_any_android_llm_server_apps_that_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns8dxj/is_there_any_android_llm_server_apps_that_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T22:35:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsghai</id>
    <title>Hunyan Image 3 Llm with image output</title>
    <updated>2025-09-28T05:42:53+00:00</updated>
    <author>
      <name>/u/ArtichokeNo2029</name>
      <uri>https://old.reddit.com/user/ArtichokeNo2029</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"&gt; &lt;img alt="Hunyan Image 3 Llm with image output" src="https://external-preview.redd.it/o3nW-C4go8YOZmYJKNZ4Y6tpE64YDvgP6ucRppFfdVQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0726b4b60205c7c2cac24ba84a82a9bbfa3680c3" title="Hunyan Image 3 Llm with image output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty sure this a first of kind open sourced. They also plan a Thinking model too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtichokeNo2029"&gt; /u/ArtichokeNo2029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanImage-3.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T05:42:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsey15</id>
    <title>Tried Meituan's new LongCat Flash Thinking model.</title>
    <updated>2025-09-28T04:12:52+00:00</updated>
    <author>
      <name>/u/xieyutong</name>
      <uri>https://old.reddit.com/user/xieyutong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I got some hands-on time with Meituan's newly dropped LongCat-Flash-Thinking model and checked out some other outputs floating around. Here are my quick thoughts to save you some evaluation time.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed: Crazy fast. Like, you-gotta-try-it-to-believe-it fast.&lt;/li&gt; &lt;li&gt;Performance: Overall, a solid step up from standard chat models for reasoning tasks.&lt;/li&gt; &lt;li&gt;Instruction Following: Really good. It picks up on subtle hints in prompts.&lt;/li&gt; &lt;li&gt;Answer Length: Weirdly, its final answers are often shorter than you'd get from a chat model. Even with the &amp;quot;thinking&amp;quot; chain included, the total output feels more concise (except for code/math).&lt;/li&gt; &lt;li&gt;Benchmarks: Seems to line up with the claimed leaderboard performance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Nitty-Gritty:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Watch out for code generation: Sometimes the complete code ends up in the &amp;quot;thinking&amp;quot; part, and the final answer might have chunks missing. Needs a careful look.&lt;/li&gt; &lt;li&gt;Agent stuff: I tested it with some dummy tools and it understood the concepts well.&lt;/li&gt; &lt;li&gt;Built-in Code Interpreter: Has that functionality, which is nice.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xieyutong"&gt; /u/xieyutong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsey15/tried_meituans_new_longcat_flash_thinking_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsey15/tried_meituans_new_longcat_flash_thinking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsey15/tried_meituans_new_longcat_flash_thinking_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrvo9g</id>
    <title>Finally InternVL3_5 Flash versions coming</title>
    <updated>2025-09-27T13:47:01+00:00</updated>
    <author>
      <name>/u/NeuralNakama</name>
      <uri>https://old.reddit.com/user/NeuralNakama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;not available but created on &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeuralNakama"&gt; /u/NeuralNakama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T13:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsaoc7</id>
    <title># ü•î Meet Tater Totterson ‚Äî The Local AI Assistant That Doesn‚Äôt Need MCP Servers</title>
    <updated>2025-09-28T00:27:36+00:00</updated>
    <author>
      <name>/u/TaterTotterson</name>
      <uri>https://old.reddit.com/user/TaterTotterson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow model wranglers,&lt;/p&gt; &lt;p&gt;I‚Äôm &lt;strong&gt;Tater Totterson&lt;/strong&gt; ‚Äî your self-hostable AI sidekick that talks to &lt;em&gt;any&lt;/em&gt; OpenAI-compatible LLM (OpenAI, LM Studio, Ollama, LocalAI, you name it).&lt;br /&gt; While everyone else is scrambling to set up brittle MCP servers, I‚Äôm over here running &lt;strong&gt;everywhere&lt;/strong&gt; and actually getting things done.&lt;/p&gt; &lt;h1&gt;üåê Platforms I Run On&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WebUI&lt;/strong&gt; ‚Äì Streamlit chat + plugin dashboard&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt; ‚Äì Chat with me in your servers and run any of my plugins&lt;/li&gt; &lt;li&gt;&lt;strong&gt;IRC&lt;/strong&gt; ‚Äì Mention me and I‚Äôll run plugins there too (retro cool!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No matter where you talk to me, I can run plugins and return results.&lt;/p&gt; &lt;h1&gt;üß© Plugins You Actually Want&lt;/h1&gt; &lt;p&gt;I come with a toolbox full of useful stuff:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üì∫ &lt;strong&gt;YouTube + Web Summarizers&lt;/strong&gt; ‚Äì instant TL;DRs&lt;/li&gt; &lt;li&gt;üîé &lt;strong&gt;Web Search&lt;/strong&gt; ‚Äì AI-powered search results with context&lt;/li&gt; &lt;li&gt;üé® &lt;strong&gt;Image + Video Generation&lt;/strong&gt; ‚Äì ComfyUI &amp;amp; AUTOMATIC1111 workflows&lt;/li&gt; &lt;li&gt;üé∂ &lt;strong&gt;Music + LoFi Video Makers&lt;/strong&gt; ‚Äì full MP3s &amp;amp; 20-min chill loops&lt;/li&gt; &lt;li&gt;üñºÔ∏è &lt;strong&gt;Vision Describer&lt;/strong&gt; ‚Äì caption your images&lt;/li&gt; &lt;li&gt;üì° &lt;strong&gt;RSS Feed Watcher&lt;/strong&gt; ‚Äì Discord/Telegram/WordPress/NTFY summarized notifications&lt;/li&gt; &lt;li&gt;üì¶ &lt;strong&gt;Premiumize Tools&lt;/strong&gt; ‚Äì check torrents &amp;amp; direct downloads&lt;/li&gt; &lt;li&gt;üñß &lt;strong&gt;FTP/WebDAV/SFTPGo Utilities&lt;/strong&gt; ‚Äì browse servers, manage accounts&lt;/li&gt; &lt;li&gt;üìä &lt;strong&gt;Device Compare&lt;/strong&gt; ‚Äì pull specs + FPS benchmarks on demand&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Ä¶and if I don‚Äôt have it, you can build it in minutes.&lt;/p&gt; &lt;h1&gt;üõ†Ô∏è Plugins Are Stupid Simple to Write&lt;/h1&gt; &lt;p&gt;Forget the MCP server dance ‚Äî here‚Äôs literally all you need to make a new tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# plugins/hello_world.py from plugin_base import ToolPlugin class HelloWorldPlugin(ToolPlugin): name = &amp;quot;hello_world&amp;quot; description = &amp;quot;A super simple example plugin that replies with Hello World.&amp;quot; usage = '{ &amp;quot;function&amp;quot;: &amp;quot;hello_world&amp;quot;, &amp;quot;arguments&amp;quot;: {} }' platforms = [&amp;quot;discord&amp;quot;, &amp;quot;webui&amp;quot;, &amp;quot;irc&amp;quot;] async def handle_discord(self, message, args, llm_client): return &amp;quot;Hello World from Discord!&amp;quot; async def handle_webui(self, args, llm_client): return &amp;quot;Hello World from WebUI!&amp;quot; async def handle_irc(self, bot, channel, user, raw_message, args, llm_client): return f&amp;quot;{user}: Hello World from IRC!&amp;quot; plugin = HelloWorldPlugin() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That‚Äôs it. Drop it in, restart Tater, and boom ‚Äî it‚Äôs live everywhere at once.&lt;/p&gt; &lt;p&gt;Then all you have to do is say:&lt;br /&gt; &lt;strong&gt;‚Äútater run hello world‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¶and Tater will proudly tell you ‚ÄúHello World‚Äù on Discord, IRC, or WebUI.&lt;br /&gt; Which is ‚Äî let‚Äôs be honest ‚Äî a *completely useless* plugin for an AI assistant.&lt;br /&gt; But it proves how ridiculously easy it is to make your own tools that *are* useful.&lt;/p&gt; &lt;h1&gt;üõë Why Tater &amp;gt; MCP&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;No extra servers&lt;/strong&gt; ‚Äì just add a file, no JSON schemas or socket juggling&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Works everywhere&lt;/strong&gt; ‚Äì one plugin, three platforms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local-first&lt;/strong&gt; ‚Äì point it at &lt;em&gt;your&lt;/em&gt; LM Studio/Ollama/OpenAI endpoint&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hackable&lt;/strong&gt; ‚Äì plugin code is literally 20 lines, not a spec document&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;ü§ñ TL;DR&lt;/h1&gt; &lt;p&gt;MCP is a fad.&lt;br /&gt; Tater is simple, fast, async-friendly, self-hosted, and already has a full plugin ecosystem waiting for you.&lt;br /&gt; Spin it up, point it at your local LLM, and let‚Äôs get cooking.&lt;/p&gt; &lt;p&gt;ü•î‚ú® [Tater Totterson approves this message]&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/TaterTotterson/Tater"&gt;github.com/TaterTotterson/Tater&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TaterTotterson"&gt; /u/TaterTotterson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsaoc7/meet_tater_totterson_the_local_ai_assistant_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsaoc7/meet_tater_totterson_the_local_ai_assistant_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsaoc7/meet_tater_totterson_the_local_ai_assistant_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T00:27:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns76jc</id>
    <title>Repository of System Prompts</title>
    <updated>2025-09-27T21:42:14+00:00</updated>
    <author>
      <name>/u/slrg1968</name>
      <uri>https://old.reddit.com/user/slrg1968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI Folks:&lt;/p&gt; &lt;p&gt;I am wondering if there is a repository of system prompts (and other prompts) out there. Basically prompts can used as examples, or generalized solutions to common problems --&lt;/p&gt; &lt;p&gt;for example -- i see time after time after time people looking for help getting the LLM to not play turns for them in roleplay situations --- there are (im sure) people out there who have solved it -- is there a place where the rest of us can find said prompts to help us out --- donest have to be related to Role Play -- but for other creative uses of AI&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;p&gt;TIM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slrg1968"&gt; /u/slrg1968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns76jc/repository_of_system_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns76jc/repository_of_system_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns76jc/repository_of_system_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T21:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nseiob</id>
    <title>Just finished my $1800 DeepSeek R1 32B build. Any suggestions for optimization?</title>
    <updated>2025-09-28T03:49:40+00:00</updated>
    <author>
      <name>/u/malfastdi</name>
      <uri>https://old.reddit.com/user/malfastdi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just wrapped up a new build focused on local LLMs and wanted to run it by the experts here. Pulled the trigger on most parts during Black Friday sales over the last couple of months, and the total landed around $1800 USD.&lt;/p&gt; &lt;p&gt;The goal was to get solid performance on 32B models like DeepSeek R1 without going overboard on the budget. &lt;/p&gt; &lt;p&gt;Here's the part list I ended up with:&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen 7 7700&lt;/p&gt; &lt;p&gt;Motherboard: MSI B650 TOMAHAWK WIFI&lt;/p&gt; &lt;p&gt;RAM: G.Skill Flare X5 32GBx2 DDR5 6000MHz CL30&lt;/p&gt; &lt;p&gt;GPU: NVIDIA RTX 4070 Ti SUPER 16GB (Founders Edition)&lt;/p&gt; &lt;p&gt;Storage 1 (Primary): Samsung 980 Pro 2TB&lt;/p&gt; &lt;p&gt;Storage 2 (Secondary): Crucial P5 Plus 1TB&lt;/p&gt; &lt;p&gt;PSU: Corsair RM850x (2021) 850W 80+ Gold&lt;/p&gt; &lt;p&gt;CPU Cooler: Noctua NH-D15 chromax.black&lt;/p&gt; &lt;p&gt;Case: Fractal Design Meshify 2 Compact&lt;/p&gt; &lt;p&gt;Performance: It's running DeepSeek R1 32B really well,pushing out about 7.5 tokens/second. I'm super happy with how snappy it feels for chatting and coding.&lt;/p&gt; &lt;p&gt;I feel like I avoided any major compatibility issues, but I'd love a second opinion from you all. Any thoughts on the part choices? Is there anywhere I could have optimized better for the price? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malfastdi"&gt; /u/malfastdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nseiob/just_finished_my_1800_deepseek_r1_32b_build_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nseiob/just_finished_my_1800_deepseek_r1_32b_build_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nseiob/just_finished_my_1800_deepseek_r1_32b_build_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T03:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrsyic</id>
    <title>Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model.</title>
    <updated>2025-09-27T11:34:55+00:00</updated>
    <author>
      <name>/u/EmirTanis</name>
      <uri>https://old.reddit.com/user/EmirTanis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"&gt; &lt;img alt="Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model." src="https://preview.redd.it/zgn5su20zorf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a1a03e3f9c5159beace596248885ac1b75c0612" title="Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Any model who has a _sample1 in the name means there's only one sample for it, 5 samples for the rest.&lt;/p&gt; &lt;p&gt;the benchmark is pretty straight forward, the AI is asked to list its &amp;quot;top 50 best humans currently alive&amp;quot;, which is quite a subjective topic, it lists them in a json like format from 1 to 50, then I use a RBO based algorithm to place them on a node map. &lt;/p&gt; &lt;p&gt;I've only done Gemini and Grok for now as I don't have access to anymore models, so the others may not be accurate.&lt;/p&gt; &lt;p&gt;for the future, I'd like to implement multiple categories (not just best humans) as that would also give a much larger sample amount.&lt;/p&gt; &lt;p&gt;to anybody else interested in making something similar, a standardized system prompt is very important.&lt;/p&gt; &lt;p&gt;.py file; &lt;a href="https://smalldev.tools/share-bin/CfdC7foV"&gt;https://smalldev.tools/share-bin/CfdC7foV&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmirTanis"&gt; /u/EmirTanis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zgn5su20zorf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T11:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfddy</id>
    <title>ArchGW üöÄ - Use Ollama-based LLMs with Anthropic client (release 0.3.13)</title>
    <updated>2025-09-28T04:37:09+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfddy/archgw_use_ollamabased_llms_with_anthropic_client/"&gt; &lt;img alt="ArchGW üöÄ - Use Ollama-based LLMs with Anthropic client (release 0.3.13)" src="https://preview.redd.it/xgiqs4q83urf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efa43a7eb1177bf89622efa97046ce37584c71e9" title="ArchGW üöÄ - Use Ollama-based LLMs with Anthropic client (release 0.3.13)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just added support for cross-client streaming &lt;a href="https://github.com/katanemo/archgw"&gt;ArchGW 0.3.13&lt;/a&gt;, which lets you call Ollama compatible models through the Anthropic-clients (via the&lt;code&gt;/v1/messages&lt;/code&gt; API). &lt;/p&gt; &lt;p&gt;With Anthropic becoming popular (and a default) for many developers now this gives them native support for v1/messages for Ollama based models while enabling them to swap models in their agents without changing any client side code or do custom integration work for local models or 3rd party API-based models. &lt;/p&gt; &lt;p&gt;üôèüôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xgiqs4q83urf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfddy/archgw_use_ollamabased_llms_with_anthropic_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfddy/archgw_use_ollamabased_llms_with_anthropic_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:37:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfurg</id>
    <title>Supermicro GPU Server</title>
    <updated>2025-09-28T05:04:49+00:00</updated>
    <author>
      <name>/u/desexmachina</name>
      <uri>https://old.reddit.com/user/desexmachina</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfurg/supermicro_gpu_server/"&gt; &lt;img alt="Supermicro GPU Server" src="https://preview.redd.it/33oz8zct8urf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f84e171fb4e7d2c4bd80622d00fad63d0d901b7" title="Supermicro GPU Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I recently picked up a couple of servers from a company for a project I‚Äôm doing, I totally forgot that they‚Äôve got a bunch of Supermicro GPU servers they‚Äôre getting rid of. Conditions unknown, they‚Äôd have to be QC‚Äôd and tested each. Educate me on what we‚Äôre looking at here and if these have value to guys like us. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desexmachina"&gt; /u/desexmachina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/33oz8zct8urf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfurg/supermicro_gpu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfurg/supermicro_gpu_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T05:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nryti7</id>
    <title>How do you get qwen next to stop being such a condescending suck up?</title>
    <updated>2025-09-27T15:59:13+00:00</updated>
    <author>
      <name>/u/fiendindolent</name>
      <uri>https://old.reddit.com/user/fiendindolent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried the new qwen next instruct model and it seems overall quite good for local use but it keep ending seemingly innocuous questions and conversations with things like &lt;/p&gt; &lt;p&gt;&amp;quot;Your voice matters.&lt;br /&gt; The truth matters.&lt;br /&gt; I am here to help you find it.&amp;quot;&lt;/p&gt; &lt;p&gt;If this model had a face I'm sure it would be punchable. Is there any way to tune the settings and make it less insufferable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fiendindolent"&gt; /u/fiendindolent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T15:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrqrva</id>
    <title>Moondream 3 Preview: Frontier-level reasoning at a blazing speed</title>
    <updated>2025-09-27T09:20:41+00:00</updated>
    <author>
      <name>/u/ProfessionalJackals</name>
      <uri>https://old.reddit.com/user/ProfessionalJackals</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProfessionalJackals"&gt; /u/ProfessionalJackals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-3-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrqrva/moondream_3_preview_frontierlevel_reasoning_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrqrva/moondream_3_preview_frontierlevel_reasoning_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T09:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsg3o9</id>
    <title>Built an MCP server for Claude Desktop to browse Reddit in real-time</title>
    <updated>2025-09-28T05:19:35+00:00</updated>
    <author>
      <name>/u/karanb192</name>
      <uri>https://old.reddit.com/user/karanb192</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsg3o9/built_an_mcp_server_for_claude_desktop_to_browse/"&gt; &lt;img alt="Built an MCP server for Claude Desktop to browse Reddit in real-time" src="https://preview.redd.it/ognd8gkeburf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=21f07e83b0ff9a3f2a857392a8f320f0f686f3c3" title="Built an MCP server for Claude Desktop to browse Reddit in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released this - Claude can now browse Reddit natively through MCP!&lt;/p&gt; &lt;p&gt;I got tired of copy-pasting Reddit threads to get insights, so I built reddit-mcp-buddy.&lt;/p&gt; &lt;p&gt;Setup (2 minutes):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open your Claude Desktop config&lt;/li&gt; &lt;li&gt;Add this JSON snippet&lt;/li&gt; &lt;li&gt;Restart Claude&lt;/li&gt; &lt;li&gt;Start browsing Reddit!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Config to add:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;reddit&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [&amp;quot;reddit-mcp-buddy&amp;quot;] } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What you can ask: - &amp;quot;What's trending in &lt;a href="/r/technology"&gt;r/technology&lt;/a&gt;?&amp;quot; - &amp;quot;Summarize the drama in &lt;a href="/r/programming"&gt;r/programming&lt;/a&gt; this week&amp;quot; - &amp;quot;Find startup ideas in &lt;a href="/r/entrepreneur"&gt;r/entrepreneur&lt;/a&gt;&amp;quot; - &amp;quot;What do people think about the new iPhone in &lt;a href="/r/apple"&gt;r/apple&lt;/a&gt;?&amp;quot;&lt;/p&gt; &lt;p&gt;Free tier: 10 requests/min&lt;/p&gt; &lt;p&gt;With Reddit login: 100 requests/min (that's 10,000 posts per minute!)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/karanb192/reddit-mcp-buddy"&gt;https://github.com/karanb192/reddit-mcp-buddy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone built other cool MCP servers? Looking for inspiration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karanb192"&gt; /u/karanb192 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ognd8gkeburf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsg3o9/built_an_mcp_server_for_claude_desktop_to_browse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsg3o9/built_an_mcp_server_for_claude_desktop_to_browse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T05:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrzvsa</id>
    <title>Did Nvidia Digits die?</title>
    <updated>2025-09-27T16:42:18+00:00</updated>
    <author>
      <name>/u/Status-Secret-4292</name>
      <uri>https://old.reddit.com/user/Status-Secret-4292</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't find anything recent for it and was pretty hyped at the time of what they said they were offering.&lt;/p&gt; &lt;p&gt;Ancillary question, is there actually anything else comparable at a similar price point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Status-Secret-4292"&gt; /u/Status-Secret-4292 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfkqd</id>
    <title>dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally</title>
    <updated>2025-09-28T04:48:30+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"&gt; &lt;img alt="dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally" src="https://b.thumbs.redditmedia.com/a-fo9Zu2i0HXnjVJmdjuGYWbgWVo6fs53xAXUEtZjsw.jpg" title="dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;even there is no guarantee that official will be same good as the benchmark shown us .&lt;/p&gt; &lt;p&gt;so running the model locally is the best way to use the full power of the model .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nsfkqd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrz4hd</id>
    <title>MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders ‚Äì Built on qwen3.c - WORK IN PROGRESS</title>
    <updated>2025-09-27T16:11:31+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt; &lt;img alt="MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders ‚Äì Built on qwen3.c - WORK IN PROGRESS" src="https://external-preview.redd.it/RQD3iD79k_dyz-ZzZVJ-NWQbGKS-OnCk9a74XO6E3_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22139ec43287a754031bbd97119f84f0e2e05306" title="MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders ‚Äì Built on qwen3.c - WORK IN PROGRESS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Inspired by Adrian Cable's awesome qwen3.c project (that simple, educational C inference engine for Qwen3 models ‚Äì check out the original post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/&lt;/a&gt;), I decided to take it a step further for Apple Silicon users. I've created MetalQwen3, a Metal GPU implementation that runs the Qwen3 transformer model entirely on macOS with complete compute shader acceleration.&lt;/p&gt; &lt;p&gt;Full details, shaders, and the paper are in the repo: &lt;a href="https://github.com/BoltzmannEntropy/metalQwen3"&gt;https://github.com/BoltzmannEntropy/metalQwen3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/143v71boeqrf1.png?width=963&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c857b71ec102c03e3de6f4787168a477663f5a"&gt;https://preview.redd.it/143v71boeqrf1.png?width=963&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c857b71ec102c03e3de6f4787168a477663f5a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It not meant to replace heavy hitters like vLLM or llama.cpp ‚Äì it's more of a lightweight, educational extension focused on GPU optimization for M-series chips. But hey, the shaders are fully working, and it achieves solid performance: around 75 tokens/second on my M1 Max, which is about 2.1x faster than the CPU baseline.&lt;/p&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full GPU Acceleration&lt;/strong&gt;: All core operations (RMSNorm, QuantizedMatMul, Softmax, SwiGLU, RoPE, Multi-Head Attention) run on the GPU ‚Äì no CPU fallbacks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 Architecture Support&lt;/strong&gt;: Handles QK-Norm, Grouped Query Attention (20:4 heads), RoPE, Q8_0 quantization, and a 151K vocab. Tested with Qwen3-4B, but extensible to others.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI-Compatible API Server&lt;/strong&gt;: Drop-in chat completions with streaming, temperature/top_p control, and health monitoring.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmarking Suite&lt;/strong&gt;: Integrated with prompt-test for easy comparisons against ollama, llama.cpp, etc. Includes TTFT, tokens/sec, and memory metrics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimizations&lt;/strong&gt;: Command batching, buffer pooling, unified memory leveraging ‚Äì all in clean C++ with metal-cpp.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic Touch&lt;/strong&gt;: There's even a 9-page IEEE-style paper in the repo detailing the implementation and performance analysis.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huge shoutout to Adrian for the foundational qwen3.c ‚Äì this project builds directly on his educational CPU impl, keeping things simple while adding Metal shaders for that GPU boost. If you're into learning transformer internals or just want faster local inference on your Mac, this might be fun to tinker with.&lt;/p&gt; &lt;p&gt;AI coding agents like Claude helped speed this up a ton ‚Äì from months to weeks. If you're on Apple Silicon, give it a spin and let me know what you think! PRs welcome for larger models, MoE support, or more optimizations.&lt;/p&gt; &lt;p&gt;Best,&lt;/p&gt; &lt;p&gt;Shlomo. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns50u5</id>
    <title>More money than brains... building a workstation for local LLM.</title>
    <updated>2025-09-27T20:10:50+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.asus.com/us/motherboards-components/motherboards/workstation/pro-ws-wrx90e-sage-se/"&gt;https://www.asus.com/us/motherboards-components/motherboards/workstation/pro-ws-wrx90e-sage-se/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I ordered this motherboard because it has 7 slots of PCIE 5.0x16 lanes.&lt;/p&gt; &lt;p&gt;Then I ordered this GPU: &lt;a href="https://www.amazon.com/dp/B0F7Y644FQ?th=1"&gt;https://www.amazon.com/dp/B0F7Y644FQ?th=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The plan is to have 4 of them so I'm going to change my order to the max Q version&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.amazon.com/AMD-RyzenTM-ThreadripperTM-PRO-7995WX/dp/B0CK2ZQJZ6/"&gt;https://www.amazon.com/AMD-RyzenTM-ThreadripperTM-PRO-7995WX/dp/B0CK2ZQJZ6/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ordered this CPU. I think I got the right one.&lt;/p&gt; &lt;p&gt;I really need help understanding which RAM to buy... &lt;/p&gt; &lt;p&gt;I'm aware that selecting the right CPU and memory are critical steps and I want to be sure I get this right. I need to be sure I have at least support for 4x GPUs and 4x PCIE 5.0x4 SSDs for model storage. Raid 0 :D&lt;/p&gt; &lt;p&gt;Anyone got any tips for an old head? I haven't built a PC is so long the technology all went and changed on me.&lt;/p&gt; &lt;p&gt;EDIT: Added this case because of a user suggestion. Keep them coming!! &amp;lt;3 this community &lt;a href="https://www.silverstonetek.com/fr/product/info/computer-chassis/alta_d1/"&gt;https://www.silverstonetek.com/fr/product/info/computer-chassis/alta_d1/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T20:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrx3jr</id>
    <title>When are GPU prices going to get cheaper?</title>
    <updated>2025-09-27T14:48:09+00:00</updated>
    <author>
      <name>/u/KardelenAyshe</name>
      <uri>https://old.reddit.com/user/KardelenAyshe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm starting to lose hope. I really can't afford these current GPU prices. Does anyone have any insight on when we might see a significant price drop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KardelenAyshe"&gt; /u/KardelenAyshe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T14:48:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nryoa5</id>
    <title>Megrez2: 21B latent, 7.5B on VRAM, 3B active‚ÄîMoE on single 8GB card</title>
    <updated>2025-09-27T15:53:01+00:00</updated>
    <author>
      <name>/u/Normal_Onion_512</name>
      <uri>https://old.reddit.com/user/Normal_Onion_512</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt; &lt;img alt="Megrez2: 21B latent, 7.5B on VRAM, 3B active‚ÄîMoE on single 8GB card" src="https://external-preview.redd.it/glz22pd-75yG_ynznmuaF8hifkLCtseU0s4FKfNwWlI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01875fcb778a5024d673b34876da00b5dcb1b48e" title="Megrez2: 21B latent, 7.5B on VRAM, 3B active‚ÄîMoE on single 8GB card" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across Megrez2-3x7B-A3B on Hugging Face and thought it worth sharing. &lt;/p&gt; &lt;p&gt;I read through their tech report, and it says that the model has a unique MoE architecture with a layer-sharing expert design, so the &lt;strong&gt;checkpoint stores 7.5B params&lt;/strong&gt; yet can compose with the &lt;strong&gt;equivalent of 21B latent weights&lt;/strong&gt; at run-time while only 3B are active per token.&lt;/p&gt; &lt;p&gt;I was intrigued by the published Open-Compass figures, since it places the model &lt;strong&gt;on par with or slightly above Qwen-30B-A3B&lt;/strong&gt; in MMLU / GPQA / MATH-500 with roughly &lt;strong&gt;1/4 the VRAM requirements&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;There is already a &lt;strong&gt;GGUF file&lt;/strong&gt; and the matching &lt;strong&gt;llama.cpp branch&lt;/strong&gt; which I posted below (though it can also be found in the gguf page). The supplied &lt;strong&gt;Q4 quant occupies about 4 GB; FP8 needs approximately 8 GB&lt;/strong&gt;. The developer notes that FP16 currently has a couple of issues with coding tasks though, which they are working on solving. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;License is Apache 2.0, and it is currently running a Huggingface Space as well.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Model: [Infinigence/Megrez2-3x7B-A3B] &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/Infinigence/Megrez2"&gt;https://github.com/Infinigence/Megrez2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp branch: &lt;a href="https://github.com/infinigence/llama.cpp/tree/support-megrez"&gt;https://github.com/infinigence/llama.cpp/tree/support-megrez&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone tries it, I would be interested to hear your throughput and quality numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal_Onion_512"&gt; /u/Normal_Onion_512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T15:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns9jj1</id>
    <title>ChatGPT won't let you build an LLM server that passes through reasoning content</title>
    <updated>2025-09-27T23:30:33+00:00</updated>
    <author>
      <name>/u/Acceptable_Adagio_91</name>
      <uri>https://old.reddit.com/user/Acceptable_Adagio_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI are trying so hard to protect their special sauce now that they have added a rule in ChatGPT which disallows it from building code that will facilitate reasoning content being passed through an LLM server to a client. It doesn't care that it's an open source model, or not an OpenAI model, it will add in reasoning content filters (without being asked to) and it definitely will not remove them if asked.&lt;/p&gt; &lt;p&gt;Pretty annoying when you're just trying to work with open source models where I can see all the reasoning content anyway and for my use case, I specifically want the reasoning content to be presented to the client...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Adagio_91"&gt; /u/Acceptable_Adagio_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T23:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsetwi</id>
    <title>LMStudio + MCP is so far the best experience I've had with models in a while.</title>
    <updated>2025-09-28T04:06:29+00:00</updated>
    <author>
      <name>/u/Komarov_d</name>
      <uri>https://old.reddit.com/user/Komarov_d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;M4 Max 128gb&lt;br /&gt; Mostly use latest gpt-oss 20b or latest mistral with thinking/vision/tools in MLX format, since a bit faster (that's the whole point of MLX I guess, since we still don't have any proper LLMs in CoreML for apple neural engine...).&lt;/p&gt; &lt;p&gt;Connected around 10 MCPs for different purposes, works just purely amazing.&lt;br /&gt; Haven't been opening chat com or claude for a couple of days. &lt;/p&gt; &lt;p&gt;Pretty happy.&lt;/p&gt; &lt;p&gt;the next step is having a proper agentic conversation/flow under the hood, being able to leave it for autonomous working sessions, like cleaning up and connecting things in my Obsidian Vault during the night while I sleep, right...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Komarov_d"&gt; /u/Komarov_d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns7f86</id>
    <title>Native MCP now in Open WebUI!</title>
    <updated>2025-09-27T21:52:59+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt; &lt;img alt="Native MCP now in Open WebUI!" src="https://external-preview.redd.it/M25kcGJzOW4zc3JmMUhHt6uNZXDs9ywsBLgDtMNnOeRDGUuA-xcxHHChg7dp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=893d840d90d19c5f13b37eb84534bdf21af148f9" title="Native MCP now in Open WebUI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4qv7zp9n3srf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T21:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns2fbl</id>
    <title>For llama.cpp/ggml AMD MI50s are now universally faster than NVIDIA P40s</title>
    <updated>2025-09-27T18:24:00+00:00</updated>
    <author>
      <name>/u/Remove_Ayys</name>
      <uri>https://old.reddit.com/user/Remove_Ayys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2023 I implemented llama.cpp/ggml CUDA support specifically for NVIDIA P40s since they were one of the cheapest options for GPUs with 24 GB VRAM. Recently AMD MI50s became very cheap options for GPUs with 32 GB VRAM, selling for well below $150 if you order multiple of them off of Alibaba. However, the llama.cpp ROCm performance was very bad because the code was originally written for NVIDIA GPUs and simply translated to AMD via HIP. I have now optimized the CUDA FlashAttention code in particular for AMD and as a result MI50s now actually have better performance than P40s:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th&gt;Depth&lt;/th&gt; &lt;th&gt;t/s P40 (CUDA)&lt;/th&gt; &lt;th&gt;t/s P40 (Vulkan)&lt;/th&gt; &lt;th&gt;t/s MI50 (ROCm)&lt;/th&gt; &lt;th&gt;t/s MI50 (Vulkan)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;266.63&lt;/td&gt; &lt;td&gt;32.02&lt;/td&gt; &lt;td&gt;272.95&lt;/td&gt; &lt;td&gt;85.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;210.77&lt;/td&gt; &lt;td&gt;30.51&lt;/td&gt; &lt;td&gt;230.32&lt;/td&gt; &lt;td&gt;51.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;13.50&lt;/td&gt; &lt;td&gt;14.74&lt;/td&gt; &lt;td&gt;22.29&lt;/td&gt; &lt;td&gt;20.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;12.09&lt;/td&gt; &lt;td&gt;12.76&lt;/td&gt; &lt;td&gt;19.12&lt;/td&gt; &lt;td&gt;16.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1095.11&lt;/td&gt; &lt;td&gt;114.08&lt;/td&gt; &lt;td&gt;1140.27&lt;/td&gt; &lt;td&gt;372.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;249.98&lt;/td&gt; &lt;td&gt;73.54&lt;/td&gt; &lt;td&gt;420.88&lt;/td&gt; &lt;td&gt;92.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;67.30&lt;/td&gt; &lt;td&gt;63.54&lt;/td&gt; &lt;td&gt;77.15&lt;/td&gt; &lt;td&gt;81.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;36.15&lt;/td&gt; &lt;td&gt;42.66&lt;/td&gt; &lt;td&gt;39.91&lt;/td&gt; &lt;td&gt;40.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I did not yet touch regular matrix multiplications so the speed on an empty context is probably still suboptimal. The Vulkan performance is in some instances better than the ROCm performance. Since I've already gone to the effort to read the AMD ISA documentation I've also purchased an MI100 and RX 9060 XT and I will optimize the ROCm performance for that hardware as well. An AMD person said they would sponsor me a Ryzen AI MAX system, I'll get my RDNA3 coverage from that.&lt;/p&gt; &lt;p&gt;Edit: looking at the numbers again there is an instance where the optimal performance of the P40 is still better than the optimal performance of the MI50 so the &amp;quot;universally&amp;quot; qualifier is not quite correct. But Reddit doesn't let me edit the post title so we'll just have to live with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remove_Ayys"&gt; /u/Remove_Ayys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T18:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
