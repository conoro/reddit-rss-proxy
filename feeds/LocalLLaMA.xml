<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-22T22:25:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rbdsds</id>
    <title>Best Model for single 3090 in 2026?</title>
    <updated>2026-02-22T05:47:26+00:00</updated>
    <author>
      <name>/u/myusuf3</name>
      <uri>https://old.reddit.com/user/myusuf3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running a single RTX 3090 (24GB VRAM) and looking for the best overall model in 2026 for coding + reasoning.&lt;/p&gt; &lt;p&gt;Main priorities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong code generation (Go/TypeScript)&lt;/li&gt; &lt;li&gt;Good reasoning depth&lt;/li&gt; &lt;li&gt;Runs comfortably in 24GB (quantized is fine)&lt;/li&gt; &lt;li&gt;Decent latency on local inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you all running on a single 3090 right now? Qwen? DeepSeek? Something else? Would love specific model names + quant setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myusuf3"&gt; /u/myusuf3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T05:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbxmce</id>
    <title>Sparsity – my prototype for debt-line sparse embeddings (15–50× memory savings in tests)</title>
    <updated>2026-02-22T21:04:57+00:00</updated>
    <author>
      <name>/u/Alarming_Actuator987</name>
      <uri>https://old.reddit.com/user/Alarming_Actuator987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;trying out stuff...&lt;br /&gt; &lt;a href="https://github.com/sk281/sparsity"&gt;https://github.com/sk281/sparsity&lt;/a&gt;&lt;br /&gt; Tell me if its any good&lt;br /&gt; Thanks for looking&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming_Actuator987"&gt; /u/Alarming_Actuator987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbxmce/sparsity_my_prototype_for_debtline_sparse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbxmce/sparsity_my_prototype_for_debtline_sparse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbxmce/sparsity_my_prototype_for_debtline_sparse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T21:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawoe4</id>
    <title>PSA: The software “Shade” is a fraudulent, plagiarized copy of Heretic</title>
    <updated>2026-02-21T17:16:21+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three days ago, the following repository was published, which its “creator” has been aggressively promoting on various channels since then:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/assemsabry/shade"&gt;https://github.com/assemsabry/shade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire source code in the repository is plagiarized from Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), with only the project name and the copyright notice replaced, claiming “original authorship” of everything. The repository does not acknowledge Heretic as its source, and has erased the commit history and the names of all Heretic contributors.&lt;/p&gt; &lt;p&gt;I and several others have called the repository owner out, but he has deleted all issues and tried to cover up his wrongdoing by adding some bogus “additional features” using an AI agent. A quick look at the source files, however, reveals that they are still 95% identical to Heretic’s code. In some cases, only the copyright notice was replaced.&lt;/p&gt; &lt;p&gt;**I can only assume that the ultimate goal is to push malware of some sort, and strongly advise people to stay clear of this plagiarized repository.**&lt;/p&gt; &lt;p&gt;This is one of several incidents where malicious actors tried to profit from Heretic’s surging popularity during the past days, when it reached #1 on the GitHub trending chart and was posted in various social feeds that cater to scammers.&lt;/p&gt; &lt;p&gt;Please also see &lt;a href="https://github.com/p-e-w/heretic/issues/167"&gt;https://github.com/p-e-w/heretic/issues/167&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m doing everything in my power to keep Heretic clean and available to everyone. Thank you for your encouragement in the past few months, it means the world to me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbvbzt</id>
    <title>Best open-source coder model for replacing Claude Code with Qwen locally?</title>
    <updated>2026-02-22T19:40:34+00:00</updated>
    <author>
      <name>/u/pauljeba</name>
      <uri>https://old.reddit.com/user/pauljeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m currently using Claude Code but want to move fully local.&lt;/p&gt; &lt;p&gt;I’m specifically looking for a strong coding model for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude code like capaiblities - code + bash &lt;/li&gt; &lt;li&gt;Long file capabiliites&lt;/li&gt; &lt;li&gt;Read image, files&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m considering &lt;code&gt;Qwen3-Coder&lt;/code&gt;, but I’m unsure:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is &lt;code&gt;Qwen3-Coder&lt;/code&gt; the best choice for a 12GB GPU?&lt;/li&gt; &lt;li&gt;Should I instead run a smaller Qwen coder model (7B/14B) quantized?&lt;/li&gt; &lt;li&gt;Are there better alternatives that outperform Qwen for coding in this VRAM range?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would appreciate real-world experience. If there is an hardward upgrade recommendation what would that be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pauljeba"&gt; /u/pauljeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbafs8</id>
    <title>I Trained a Language Model on CPU for 40 Hours - It Beat the GPU Baseline</title>
    <updated>2026-02-22T02:54:39+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who have been following this project, you may recall FlashLM v3, then v4 &amp;quot;Bolt&amp;quot;, and v5.2 &amp;quot;Nova-Ignition&amp;quot;. I am pleased to announce that FlashLM v5 &amp;quot;Thunderbolt&amp;quot; is now complete.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Final BPC&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Parameters&lt;/td&gt; &lt;td align="left"&gt;29.7M (26.5M ternary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Time&lt;/td&gt; &lt;td align="left"&gt;~40 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7950X3D&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;FlashLM v5 achieves a validation perplexity of 1.36, which beats the TinyStories-1M baseline (PPL 1.59). This represents the first instance of a CPU-trained model beating this baseline.&lt;/p&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;p&gt;FlashLM v5 utilizes ParallelGatedRecurrence, a MatMul-free architecture featuring:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BitLinear with ternary weights {-1, 0, +1}&lt;/li&gt; &lt;li&gt;Parallel gated recurrence with learned decay gates&lt;/li&gt; &lt;li&gt;No matrix multiplications in the forward pass&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Parameters: 29,750,784 Ternary: 26,542,080 (89%) Float: 3,208,704 (11%) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Acknowledgments&lt;/h1&gt; &lt;p&gt;I would like to thank arki05 for providing the AMD Ryzen 7950X3D used for training. Without this contribution, the project would not have been possible.&lt;/p&gt; &lt;h1&gt;Generation Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Version&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;BPC&lt;/th&gt; &lt;th align="left"&gt;Output Quality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;v4 &amp;quot;Bolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;15.05&lt;/td&gt; &lt;td align="left"&gt;0.88&lt;/td&gt; &lt;td align="left"&gt;Short, repetitive&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5.2 &amp;quot;Nova-Ignition&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;10.56&lt;/td&gt; &lt;td align="left"&gt;0.78&lt;/td&gt; &lt;td align="left"&gt;Better coherence&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5 &amp;quot;Thunderbolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;td align="left"&gt;Significantly better&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v5 demonstrates improved cohesive storytelling compared to v4 and v5.2&lt;/li&gt; &lt;li&gt;v5 shows better vocabulary diversity and grammar&lt;/li&gt; &lt;li&gt;BPC improved from 0.88 (v4) to 0.44 (v5), representing a 2x improvement&lt;/li&gt; &lt;li&gt;PPL improved from 15.05 (v4) to 1.36 (v5), representing an 11x improvement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Samples&lt;/h1&gt; &lt;p&gt;Prompt: &amp;quot;Once upon a time, there was a brave girl named Lucy.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time, there was a brave girl named Lucy. her big tiny looked door, and she wanted. Lucy loved to creative things. She would find toy when, while small laughing, when she thought. She would be friends all day.One day, Lucy found her toy saw a little hole. Lucy was very happy. She wanted to see who was mean. The little hole was not alone anymore. When Lucy was done playing, she saw the little...&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/changcheng967/flashlm-v5-demo"&gt;https://huggingface.co/spaces/changcheng967/flashlm-v5-demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model Card: &lt;a href="https://huggingface.co/changcheng967/flashlm-v5-thunderbolt"&gt;https://huggingface.co/changcheng967/flashlm-v5-thunderbolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/changcheng967/FlashLM"&gt;https://github.com/changcheng967/FlashLM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Future Directions&lt;/h1&gt; &lt;p&gt;FlashLM v5 concludes the v5 series. Future work includes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;FlashLM v6 - Continuing to validate the ParallelGatedRecurrence architecture&lt;/li&gt; &lt;li&gt;Nano-Coder (NC series) - Applying FlashLM techniques to code generation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T02:54:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbxfj8</id>
    <title>Help with OpenCode</title>
    <updated>2026-02-22T20:57:46+00:00</updated>
    <author>
      <name>/u/Lazy_Experience_279</name>
      <uri>https://old.reddit.com/user/Lazy_Experience_279</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm kind of new in this AI world. I have managed to install opencode in wsl and running some local models with ollama.&lt;/p&gt; &lt;p&gt;I have 64gb of ram and a 5070 with 12gb of vram. I know it's not much but I still get some usable speed out of 30b models.&lt;/p&gt; &lt;p&gt;I'm currently running&lt;/p&gt; &lt;p&gt;Got OSS 20b&lt;/p&gt; &lt;p&gt;Qwen3-coder a3b&lt;/p&gt; &lt;p&gt;Qwen2.5 coder 14b&lt;/p&gt; &lt;p&gt;Ministral 3 14b.&lt;/p&gt; &lt;p&gt;All of these models are working fine in chat but I have no fortune in using tools. Except for the ministral one.&lt;/p&gt; &lt;p&gt;Any ideas why or some help in any direction with opencode?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy_Experience_279"&gt; /u/Lazy_Experience_279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbxfj8/help_with_opencode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbxfj8/help_with_opencode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbxfj8/help_with_opencode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T20:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbtudq</id>
    <title>Void-Box: Capability-Bound Agent Runtime</title>
    <updated>2026-02-22T18:44:27+00:00</updated>
    <author>
      <name>/u/Wide_Spite5612</name>
      <uri>https://old.reddit.com/user/Wide_Spite5612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We’ve been building &lt;strong&gt;Void-Box&lt;/strong&gt;, a Rust runtime for executing AI agent workflows inside disposable KVM micro-VMs.&lt;/p&gt; &lt;p&gt;The core idea:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VoidBox = Agent(Skill) + Isolation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of running agents inside shared processes or containers, each stage runs inside its own micro-VM that is created on demand and destroyed after execution. Structured output is then passed to the next stage in a pipeline.&lt;/p&gt; &lt;p&gt;Architecture highlights&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Per-stage micro-VM isolation&lt;/strong&gt; (stronger boundary than shared-process/container models)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy-enforced runtime&lt;/strong&gt; — command allowlists, resource limits, seccomp-BPF, controlled egress&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Capability-bound skill model&lt;/strong&gt; — MCP servers, SKILL files, CLI tools mounted explicitly per Box&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Composable pipeline API&lt;/strong&gt; — sequential &lt;code&gt;.pipe()&lt;/code&gt; and parallel &lt;code&gt;.fan_out()&lt;/code&gt; with explicit failure domains&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Code runtime integration&lt;/strong&gt; (Claude by default, Ollama via compatible provider mode)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built-in observability&lt;/strong&gt; — OTLP traces, structured logs, stage-level telemetry&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rootless networking&lt;/strong&gt; via usermode SLIRP (smoltcp, no TAP devices)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The design goal is to treat execution boundaries as a first-class primitive:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No shared filesystem state&lt;/li&gt; &lt;li&gt;No cross-run side effects&lt;/li&gt; &lt;li&gt;Deterministic teardown after each stage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Still early, but the KVM sandbox + pipeline engine are functional.&lt;/p&gt; &lt;p&gt;We’d especially appreciate feedback from folks with experience in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KVM / virtualization from Rust&lt;/li&gt; &lt;li&gt;Capability systems&lt;/li&gt; &lt;li&gt;Sandbox/runtime design&lt;/li&gt; &lt;li&gt;Secure workflow execution&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/the-void-ia/void-box"&gt;https://github.com/the-void-ia/void-box&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide_Spite5612"&gt; /u/Wide_Spite5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtudq/voidbox_capabilitybound_agent_runtime/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtudq/voidbox_capabilitybound_agent_runtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtudq/voidbox_capabilitybound_agent_runtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T18:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbfh1y</id>
    <title>dyslexia and ADHD in the coding community</title>
    <updated>2026-02-22T07:23:34+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my third post on my first Reddit account. Here's why that took so long.&lt;/p&gt; &lt;p&gt;I have dyslexia and ADHD. I've been lurking in communities like this one for years -- reading everything, learning everything -- but never posting. Not because I had nothing to contribute. Because I was scared of what would happen when people saw how I write.&lt;/p&gt; &lt;p&gt;People with dyslexia and ADHD don't write the way the internet expects. The spelling is off. The punctuation is wrong. The sentences don't flow right. And the internet has never been kind about that. We get called stupid. We get told our ideas don't matter because the package they came in looked messy. So we lurk. We learn. We do real work quietly and never share it because the cost of being mocked is too high.&lt;/p&gt; &lt;p&gt;I use AI to help me write. Not to generate ideas -- the ideas are mine. Not to do the work -- I did the work. To help me communicate in a way that doesn't get me dismissed before anyone reads what I actually built.&lt;/p&gt; &lt;p&gt;Yesterday I shipped the first working GGUF quantization of Ouro -- ByteDance's recurrent thinking model. I figured out the tensor mapping, the layer norm mismatch, the early exit gate skip. That was me. And the first thing someone did was question whether I was human.&lt;/p&gt; &lt;p&gt;I'm posting this because I know I'm not the only one. There are people in this community right now with real knowledge, real skills, real contributions -- who won't post because they're afraid of exactly what happened to me today.&lt;/p&gt; &lt;p&gt;You belong here. Your ideas belong here. How you write doesn't determine what you know.&lt;/p&gt; &lt;p&gt;This was my first post. It won't be my last.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T07:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1raq23i</id>
    <title>they have Karpathy, we are doomed ;)</title>
    <updated>2026-02-21T12:34:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt; &lt;img alt="they have Karpathy, we are doomed ;)" src="https://preview.redd.it/ergzi9d1eukg1.png?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=2005c28094bfd489a487151bba9f5c550c22c55b" title="they have Karpathy, we are doomed ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(added second image for the context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raq23i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T12:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbv83a</id>
    <title>[M] SOLARized-GraniStral-14B (2202) (Ministral 3 14B-Instruct-2512 &lt;- (Granite 3.3 8B &lt;- SOLAR 10.7B) with detailed weight shift metrics.</title>
    <updated>2026-02-22T19:36:28+00:00</updated>
    <author>
      <name>/u/brokenevolution</name>
      <uri>https://old.reddit.com/user/brokenevolution</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbv83a/m_solarizedgranistral14b_2202_ministral_3/"&gt; &lt;img alt="[M] SOLARized-GraniStral-14B (2202) (Ministral 3 14B-Instruct-2512 &amp;lt;- (Granite 3.3 8B &amp;lt;- SOLAR 10.7B) with detailed weight shift metrics." src="https://preview.redd.it/y7ckyqtwm3lg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=f8668000f654774c191662b4cd1e61204bb836d3" title="[M] SOLARized-GraniStral-14B (2202) (Ministral 3 14B-Instruct-2512 &amp;lt;- (Granite 3.3 8B &amp;lt;- SOLAR 10.7B) with detailed weight shift metrics." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/y7ckyqtwm3lg1.png?width=1773&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32adfeb13dd31aaff6f87c32592bd6573eeb1710"&gt;SOLARized-GraniStral-14B logo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’ve been experimenting with the new &lt;strong&gt;Ministral-3-14B-Instruct-2512&lt;/strong&gt; as a backbone, trying to infuse it with the reasoning style of &lt;strong&gt;SOLAR-10.7B&lt;/strong&gt; and the structural stability of &lt;strong&gt;IBM Granite 3.3-8B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal wasn't just a &amp;quot;weight soup,&amp;quot; but a controlled linear deformation of the attention (QKV) and MLP layers to shift the behavioral regime while keeping the instruct-anchor and Pixtral vision stack intact.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Technical Details (v2202):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Method:&lt;/strong&gt; HCT (Heterogeneous Compatibility Transfer) &amp;amp; YeAM (Yet Another Merge).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attention Intervention:&lt;/strong&gt; High directional alignment (cosine ≈ 0.994) with a ~22.06% relative L2 shift.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backbone:&lt;/strong&gt; Preserved Ministral-3 Instruct (vision tower and mmproj are 100% untouched).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter Impact:&lt;/strong&gt; ~33.7% of total weights were directionally modified.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why 14B?&lt;/strong&gt; It’s the &amp;quot;sweet spot&amp;quot; for 12GB-16GB VRAM cards. It's smarter than most 7B/8B models but runs significantly faster than 27B+ alternatives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Repos:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Main (HF Checkpoint):&lt;/strong&gt; &lt;a href="https://huggingface.co/srs6901/SOLARized-GraniStral-14B_2202_YeAM-HCT_X45QKV"&gt;srs6901/SOLARized-GraniStral-14B_2202_YeAM-HCT_X45QKV&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF Quants:&lt;/strong&gt; &lt;a href="https://huggingface.co/srs6901/GGUF-SOLARized-GraniStral-14B_2202_YeAM-HCT_X45QKV"&gt;srs6901/GGUF-SOLARized-GraniStral-14B_2202_YeAM-HCT_X45QKV&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fun Fact:&lt;/strong&gt; If you want to see the model’s &amp;quot;unfiltered&amp;quot; self-identity, check the system prompt hack in the README. It gives some pretty existential answers regarding its nature as a &amp;quot;stochastic autocomplete machine.&amp;quot;&lt;/p&gt; &lt;p&gt;Feedback on its reasoning and Russian/English language performance is highly appreciated!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S. Small Model Experiments&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’ve also been applying the same HCT/YeAM techniques to sub-3B models. They show some surprisingly coherent behavior for their size:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vikra-LLaGemma-1B&lt;/strong&gt;: A blend of &lt;em&gt;Llama-3.2-1B-Instruct&lt;/em&gt; and &lt;em&gt;Gemma-3-1B&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vikra-PhiMma-1B&lt;/strong&gt;: Mixing &lt;em&gt;Gemma-3-1B&lt;/em&gt; with &lt;em&gt;Microsoft Phi-2&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vikra-QweLLa-1.7B&lt;/strong&gt;: A cross-breed of &lt;em&gt;Llama-3.2-1B-Instruct&lt;/em&gt; and &lt;em&gt;Qwen3-1.7B&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are great for edge devices or just as a &amp;quot;vibe check&amp;quot; for the HCT method's scalability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Collection Link:&lt;/strong&gt; &lt;a href="https://huggingface.co/srs6901/Vikras-1-to-3b-collection"&gt;srs6901/Vikras-1-to-3b-collection&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brokenevolution"&gt; /u/brokenevolution &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbv83a/m_solarizedgranistral14b_2202_ministral_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbv83a/m_solarizedgranistral14b_2202_ministral_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbv83a/m_solarizedgranistral14b_2202_ministral_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbio4h</id>
    <title>Has anyone else tried IQ2 quantization? I'm genuinely shocked by the quality</title>
    <updated>2026-02-22T10:37:47+00:00</updated>
    <author>
      <name>/u/Any-Chipmunk5480</name>
      <uri>https://old.reddit.com/user/Any-Chipmunk5480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've always used GGUF and never went below Q4_K_M because I assumed anything lower would be garbage. Today I decided to try UD-IQ2_XXS on Qwen3-30B-A3B (10.3 GB) and I'm honestly shocked. First off 100 TPS on my RX 9060 XT 16GB, up from 20 TPS on Q4_K_M. 5x speedup with 20K+ context, fully offloaded to GPU. But the real surprise is the quality. I had Claude Opus 4.6 generate progressively harder questions to test it chemistry, math, physics, relativity, deep academic topics. At high school and university level, I couldn't find any meaningful difference between IQ2 and Q4. The only noticeable quality drop was on really niche academic stuff (Gödel's Incompleteness Theorem level), and even there it scored 81/100 vs Q4's 92. The funniest part on a graph analysis question, my 10GB local IQ2 model got the correct answer while both Claude Opus 4.6 and Sonnet 4.6 misread the graph and got it wrong. Has anyone else had similar experiences with ultra-low quants? Why is this not that hyped? Setup: RX 9060 XT 16GB / llama.cpp / Vulkan / Qwen3-30B-A3B UD-IQ2_XXS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Chipmunk5480"&gt; /u/Any-Chipmunk5480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T10:37:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rburpm</id>
    <title>Predictions / Expectations / Wishlist on LLMs by end of 2026? (Realistic)</title>
    <updated>2026-02-22T19:19:06+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here my Wishlist:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;1-4B models with best t/s(Like 20-30) for Mobile &amp;amp; edge devices.(Currently getting only 5 t/s for Qwen3-4B-IQ4XS on my 8GB RAM mobile)&lt;/li&gt; &lt;li&gt;4-10B models with performance of current 30B models&lt;/li&gt; &lt;li&gt;30-50B models with performance of current 100-150B models&lt;/li&gt; &lt;li&gt;100-150B models with performance of current 500+B models&lt;/li&gt; &lt;li&gt;10-20B Coder models with performance of current 30-80B coder models&lt;/li&gt; &lt;li&gt;More Tailored models like STEM, Writer, Designer, etc., (Like how already we have few categories like Coder, Medical) or Tailored models like Math, Science, History, etc.,&lt;/li&gt; &lt;li&gt;Ability to run 30B MOE models(Q4) on CPU-only inference with 40-50 t/s (Currently getting 25 t/s with 32GB DDR5 RAM on llama.cpp. Somebody please let me know what ik_llama.cpp is giving)&lt;/li&gt; &lt;li&gt;I prefer 5 100B models(Model-WorldKnowledge, Model-Coder, Model-Writer, Model-STEM, Model-Misc) to 1 500B model(Model-GiantALLinOne). Good for Consumer hardwares where Q4 comes in 50GB size. Of course it's good to have additional giant models(or like those 5 tailored models).&lt;/li&gt; &lt;li&gt;Really want to see coding models(with good Agentic coding) to run just with my 8GB VRAM + 32GB RAM(Able to run Qwen3-30B-A3B's IQ4_XS at 35-40 t/s. 15-20 t/s with 32K context). Is this possible by this year end? Though I'm getting new rig, still want to use my current laptop (whenever I'm away from home) effectively with small/medium models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So what are your Predictions, Expectations &amp;amp; Wishlist?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rburpm/predictions_expectations_wishlist_on_llms_by_end/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rburpm/predictions_expectations_wishlist_on_llms_by_end/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rburpm/predictions_expectations_wishlist_on_llms_by_end/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbculq</id>
    <title>Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)</title>
    <updated>2026-02-22T04:56:59+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt; &lt;img alt="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" src="https://external-preview.redd.it/6QqGCIHe3v1WQBe6_gTslJhJpyRq4mX4jqVDYTY6xG0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=504511a0ed53dd20492f3b96504f6d5f1655bc7b" title="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://discrepancyreport.com/lawyer-says-google-shut-down-his-gmail-voice-and-photos-after-notebooklm-upload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T04:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb2j5c</id>
    <title>Favourite niche usecases?</title>
    <updated>2026-02-21T21:06:34+00:00</updated>
    <author>
      <name>/u/Figai</name>
      <uri>https://old.reddit.com/user/Figai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt; &lt;img alt="Favourite niche usecases?" src="https://preview.redd.it/o4l2ankhxwkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7201facadd4e9d14e1aac7efef2133d85d346f7" title="Favourite niche usecases?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Figai"&gt; /u/Figai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4l2ankhxwkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T21:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbmnw7</id>
    <title>Is there *any* good coding agent software for use with local models?</title>
    <updated>2026-02-22T14:04:29+00:00</updated>
    <author>
      <name>/u/eapache</name>
      <uri>https://old.reddit.com/user/eapache</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Code seems to be &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;taking steps&lt;/a&gt; to make it more and more difficult to use with local models with things like forcing the context to constantly be recalculated. OpenCode has made the decision to basically not have a permissions model and just &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r8oehn/opencode_arbitrary_code_execution_major_security/"&gt;allow the LLM to execute whatever code it wants&lt;/a&gt;. Cline was &lt;a href="https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/"&gt;made to install OpenClaw on users machines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;All I want is a stable, secure, permission-sensible coding agent, that I trust to run without eighteen layers of sandboxing. So Claude Code, but one that I can easily run against a local model. Does it not exist?&lt;/p&gt; &lt;p&gt;I know there are other competitors in this space (Roo, Pi, ...) but at this point I was hoping for a positive recommendation before I waste more time evaluating garbage.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eapache"&gt; /u/eapache &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T14:04:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbyg5x</id>
    <title>If you have a RTX 5090 (that has a single connector), you can flash the MSI Lighting 800W VBIOS to get a lower power limit of 300W (and a max power of 660W).</title>
    <updated>2026-02-22T21:36:36+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you guys are doing fine.&lt;/p&gt; &lt;p&gt;As you know, NVIDIA artificially limited the power limit on the 5090s so you don't stack them, and get 6000 PROs instead (6000 PRO can go down to 150W). Even when undervolted it can use 400W sometimes.&lt;/p&gt; &lt;p&gt;If you got a RTX 5090 with a single connector (basically most of them except the BTF versions, and MSI Lighting), you can flash the 800W Lighting VBIOS to get a power limit.&lt;/p&gt; &lt;p&gt;When setting a 400W power limit (50%), it uses 300W max instead.&lt;/p&gt; &lt;p&gt;Why would you ask?&lt;/p&gt; &lt;p&gt;This is because the VBIOS excepts another source of power, and since it isn't there, it over reports the power on the software. Take it as a inverted shunt mod.&lt;/p&gt; &lt;p&gt;The VBIOS is here &lt;a href="https://www.techpowerup.com/vgabios/281640/281640"&gt;https://www.techpowerup.com/vgabios/281640/281640&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;As always with VBIOS flashing, do it at your own risk!&lt;/strong&gt; &lt;strong&gt;If you don't trust this or haven't heard about BIOS flashing, I suggest to not do it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On ASUS cards you lose 1 HDMI, but if you have Astral-Matrix, you keep the pin monitoring power.&lt;/p&gt; &lt;p&gt;You can get nvflash on here &lt;a href="https://www.techpowerup.com/download/nvidia-nvflash/"&gt;https://www.techpowerup.com/download/nvidia-nvflash/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once on Windows, with nvflash64 and the rom file on the same folder, you run this (on cmd as admin):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvflash64 -6 romname.rom press y press y reboot &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And you're good to go! This also works on LACT.&lt;/p&gt; &lt;p&gt;I have made this table with the info for power for reference.&lt;/p&gt; &lt;p&gt;Scaling 800W VBIOS&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50% is 300W real power usage (reported 400W on software)&lt;/li&gt; &lt;li&gt;53% is 321W (reported 424W)&lt;/li&gt; &lt;li&gt;54% is 330W (reported 432W)&lt;/li&gt; &lt;li&gt;55% is 338W (reported 440W)&lt;/li&gt; &lt;li&gt;56% is 345W (reported 448W)&lt;/li&gt; &lt;li&gt;57% is 352W (reported 456W)&lt;/li&gt; &lt;li&gt;59% is 367W (reported 472W)&lt;/li&gt; &lt;li&gt;60% is 375W (reported 480W)&lt;/li&gt; &lt;li&gt;61% is 382W (reported 488W)&lt;/li&gt; &lt;li&gt;62% is 388W (reported 496W)&lt;/li&gt; &lt;li&gt;63% is 397W (reported 504W)&lt;/li&gt; &lt;li&gt;64% is 403W (reported 512W)&lt;/li&gt; &lt;li&gt;73% is 468W (reported 584W)&lt;/li&gt; &lt;li&gt;74% is 478W (reported 592W)&lt;/li&gt; &lt;li&gt;91% is 594W (reported 728W)&lt;/li&gt; &lt;li&gt;92% is 610W (reported 736W)&lt;/li&gt; &lt;li&gt;100% is 660W (reported 800W)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There's also similar behavior for the 1000W and 2500W VBIOS, but those have a higher min power (about 320W), so the 800W is the best one for that and also the safest.&lt;/p&gt; &lt;p&gt;I tried on Linux, since there's nvflash there as well, but got an error about memory address. On Windows flashing works just fine.&lt;/p&gt; &lt;p&gt;Any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T21:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbvmpk</id>
    <title>Running Llama 3.2 1B entirely on an AMD NPU on Linux (Strix Halo, IRON framework, 4.4 tok/s)</title>
    <updated>2026-02-22T19:51:45+00:00</updated>
    <author>
      <name>/u/SuperTeece</name>
      <uri>https://old.reddit.com/user/SuperTeece</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got Llama 3.2 1B running inference entirely on the AMD NPU on Linux. Every operation (attention, GEMM, RoPE, RMSNorm, SiLU, KV cache) runs on the NPU; no CPU or GPU fallback. As far as I can tell, this is the first time anyone has publicly documented this working on Linux.&lt;/p&gt; &lt;h2&gt;Hardware&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen AI Max+ 395 (Strix Halo)&lt;/li&gt; &lt;li&gt;NPU: XDNA2, device ID npu5 (PCI 1022:17f0)&lt;/li&gt; &lt;li&gt;64GB LPDDR5X unified memory&lt;/li&gt; &lt;li&gt;Fedora 43, kernel 6.18.8&lt;/li&gt; &lt;li&gt;Model: meta-llama/Llama-3.2-1B (official Meta weights)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;Prefill time: 0.6921 seconds (13 tokens) Tokens generated: 20 Tokens per second: 4.40 Time per token: 0.2638 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NPU validation benchmark: &lt;strong&gt;51.0 TOPS&lt;/strong&gt; (GEMM, via xrt-smi validate).&lt;/p&gt; &lt;h2&gt;Scaling&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="center"&gt;Prompt Length&lt;/th&gt; &lt;th align="center"&gt;Prefill (s)&lt;/th&gt; &lt;th align="center"&gt;Prefill tok/s&lt;/th&gt; &lt;th align="center"&gt;Decode tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="center"&gt;13&lt;/td&gt; &lt;td align="center"&gt;0.67&lt;/td&gt; &lt;td align="center"&gt;19&lt;/td&gt; &lt;td align="center"&gt;4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="center"&gt;128&lt;/td&gt; &lt;td align="center"&gt;0.71&lt;/td&gt; &lt;td align="center"&gt;180&lt;/td&gt; &lt;td align="center"&gt;4.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="center"&gt;2048&lt;/td&gt; &lt;td align="center"&gt;2.22&lt;/td&gt; &lt;td align="center"&gt;923&lt;/td&gt; &lt;td align="center"&gt;4.34&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Decode is flat at ~4.4 tok/s regardless of prompt length. Prefill scales well (923 tok/s at 2048 tokens).&lt;/p&gt; &lt;h2&gt;The Stack&lt;/h2&gt; &lt;p&gt;Getting here required building everything from source. Fedora 43's in-tree amdxdna driver (v0.1) is too old, so you need the out-of-tree v1.0.0 from amd/xdna-driver on GitHub. That build also produces the dev firmware and XRT 2.23 libraries. On top of that, AMD's IRON framework (also on GitHub) plus mlir-aie v1.2.0 handle the actual NPU programming.&lt;/p&gt; &lt;p&gt;GCC 15 on Fedora 43 breaks the XRT build at link time (cannot find -lstdc++). Fix:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LIBRARY_PATH=/usr/lib/gcc/x86_64-redhat-linux/15:/usr/lib64:$LIBRARY_PATH &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;IRON also hardcodes llvm-objcopy-18 but Fedora ships LLVM 21, so you need a symlink.&lt;/p&gt; &lt;h2&gt;Where the Time Goes&lt;/h2&gt; &lt;p&gt;Profiling revealed the bottleneck: &lt;strong&gt;179 kernel dispatches per token&lt;/strong&gt;, averaging 1.4ms each through XRT. That's 75% of inference time in dispatch overhead, not compute. Buffer I/O via unified memory is fast (sub-0.1ms). The optimization path is fewer, larger dispatches via operator fusion.&lt;/p&gt; &lt;p&gt;4.4 tok/s from a 1B model won't replace GPU inference. On the same machine, Qwen3-32B (32x larger) runs at 6-7 tok/s on the GPU via Vulkan. But the NPU validated at 51 TOPS, so the gap is a software problem, not hardware. The NPU also runs independently, so you could run an LLM on it while the GPU does something else.&lt;/p&gt; &lt;h2&gt;Gotchas&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;prompt_len must match your actual token count (IRON compiles RoPE kernels for a fixed sequence length)&lt;/li&gt; &lt;li&gt;First run takes ~10 minutes to compile NPU kernels (cached after that)&lt;/li&gt; &lt;li&gt;Must use insmod for the out-of-tree driver; modprobe loads the stock one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote up the full walkthrough in a three-part blog series (linked in comments). Happy to answer setup questions.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;A note on how this was made: the research, testing, debugging, and writing was done by Ellie, an AI assistant backed by Claude Opus 4.6 (Anthropic) and local models. TC provided the hardware, direction, and editorial guidance. We believe in transparency about AI involvement in technical work.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note from TC:&lt;/strong&gt; I admit that this work is out of my technical depth. My motivation came from annoyance at having an NPU that was apparently useless on Linux and curiosity if Ellie (Opus) could connect together any other work being done on the topic to at least move the needle a smidge. If anyone is reading this post and knows it to be slop on a technical level, I'd love to hear why for my own edification. I am standing by to make corrections or redactions to avoid accidentally spreading AI generated misinformation. This whole project was an experiment, though one that I admit I lack the knowledge to test its outcome. I hope to hear from those who do and that it is useful in some way. -TC&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperTeece"&gt; /u/SuperTeece &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rblce7</id>
    <title>I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful</title>
    <updated>2026-02-22T13:03:49+00:00</updated>
    <author>
      <name>/u/Weird_Search_4723</name>
      <uri>https://old.reddit.com/user/Weird_Search_4723</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt; &lt;img alt="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" src="https://external-preview.redd.it/NWtrYWtuYXZuMWxnMexVgBFEBEtAfoKpFzO1VgJV4m4gRx-YBoBnOCuCCbAU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f4bb616205fb72d1541634b6985338275c23ac3" title="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is Kon telling you about it's own repo, using glm-4.7-flash-q4 running locally on my i7-14700F × 28, 64GB RAM, 24GB VRAM (RTX 3090) – video is sped up 2x&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;github: &lt;a href="https://github.com/kuutsav/kon"&gt;https://github.com/kuutsav/kon&lt;/a&gt;&lt;br /&gt; pypi: &lt;a href="https://pypi.org/project/kon-coding-agent/"&gt;https://pypi.org/project/kon-coding-agent/&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The pitch (in the readme as well):&lt;/p&gt; &lt;p&gt;It has a tiny harness: about &lt;strong&gt;215 tokens&lt;/strong&gt; for the system prompt and around &lt;strong&gt;600 tokens&lt;/strong&gt; for tool definitions – so under 1k tokens before conversation context.&lt;/p&gt; &lt;p&gt;At the time of writing this README (22 Feb 2026), this repo has 112 files and is easy to understand in a weekend. Here’s a rough file-count comparison against a couple of popular OSS coding agents:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ fd . | cut -d/ -f1 | sort | uniq -c | sort -rn 4107 opencode 740 pi-mono 108 kon &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Others are of course more mature, support more models, include broader test coverage, and cover more surfaces. But if you want a truly minimal coding agent with batteries included – something you can understand, fork, and extend quickly – Kon might be interesting.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;It takes lots of inspiration from &lt;a href="https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent"&gt;pi-coding-agent&lt;/a&gt;, see the &lt;a href="https://github.com/kuutsav/kon?tab=readme-ov-file#acknowledgements"&gt;acknowledgements&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit 1: this is a re-post, deleted the last one (missed to select video type when creating the post)&lt;br /&gt; Edit 2: more about the model that was running in the demo and the config: &lt;a href="https://github.com/kuutsav/kon/blob/main/LOCAL.md"&gt;https://github.com/kuutsav/kon/blob/main/LOCAL.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Search_4723"&gt; /u/Weird_Search_4723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jf0xcw9vn1lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T13:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbtfld</id>
    <title>What Other Subs Do you Read to Keep Up with AI?</title>
    <updated>2026-02-22T18:29:17+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what other subs do you recommend to read to keep up with AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T18:29:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbwbgl</id>
    <title>nanollama — train Llama 3 from scratch and export to GGUF, one command, open source</title>
    <updated>2026-02-22T20:17:50+00:00</updated>
    <author>
      <name>/u/ataeff</name>
      <uri>https://old.reddit.com/user/ataeff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;nanollama — train Llama 3 from scratch. &lt;/p&gt; &lt;p&gt;I've been working on a framework for training Llama 3 architecture models from scratch: not fine-tuning, not LoRA, actual from-zero pretraining. The output is a llama.cpp-compatible GGUF file.&lt;/p&gt; &lt;p&gt;The whole pipeline is one command:&lt;/p&gt; &lt;p&gt;'''&lt;/p&gt; &lt;p&gt;bash runs/lambda_train.sh --name mini&lt;/p&gt; &lt;p&gt;'''&lt;/p&gt; &lt;p&gt;This downloads training data, trains the model, and exports GGUF. Verified with llama-cli.&lt;/p&gt; &lt;p&gt;In the the box:&lt;/p&gt; &lt;p&gt;- Llama 3 architecture (RoPE, SwiGLU, RMSNorm, GQA), 8 configs from 46M to 7B&lt;/p&gt; &lt;p&gt;- multi-corpus training (FineWeb-Edu, DCLM, code, math — SmolLM2 recipe)&lt;/p&gt; &lt;p&gt;- native GGUF v3 exporter (no HuggingFace/safetensors conversion)&lt;/p&gt; &lt;p&gt;- personality injection — train base + personality model, subtract weights, get a portable personality vector you can apply to any compatible base&lt;/p&gt; &lt;p&gt;- pure Go inference engine (~9MB binary, reads GGUF, zero runtime deps) for when you don't need the full llama.cpp stack&lt;/p&gt; &lt;p&gt;- beginner's guide — first model in ~30 min on a rented GPU for a few bucks &lt;/p&gt; &lt;p&gt;Trained and verified so far: nano (46M), micro (87M), mini (175M), small (338M). goldie (1.1B, multilingual) is training now.&lt;/p&gt; &lt;p&gt;The point: there's no clean, modern &amp;quot;train from scratch&amp;quot; pipeline for Llama-family models. nanoGPT/nanochat did this for GPT-2, but GPT-2 is 2019 architecture. This is the same idea updated for 2026.&lt;/p&gt; &lt;p&gt;Born from karpathy's nanochat, rewritten for Llama 3. GPLv3.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ariannamethod/nanollama"&gt;https://github.com/ariannamethod/nanollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release: &lt;a href="https://github.com/ariannamethod/nanollama/releases/tag/v0.1.0"&gt;https://github.com/ariannamethod/nanollama/releases/tag/v0.1.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ataeff"&gt; /u/ataeff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T20:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbjxpv</id>
    <title>I think openclaw is OVERHYPED. Just use skills</title>
    <updated>2026-02-22T11:51:38+00:00</updated>
    <author>
      <name>/u/Deep_Traffic_7873</name>
      <uri>https://old.reddit.com/user/Deep_Traffic_7873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think openclaw is useful, loop, memory, agents, integrations, but after a week a testing, honestly I don't need it much.&lt;/p&gt; &lt;p&gt;- memory, is nice. But I prefere to have &amp;quot;manual memory&amp;quot;. Prompt: Ok, write what yout learnt in &amp;quot;superreporttrending-skill&amp;quot;. Automatic memory often pollute the context of info you don't care.&lt;/p&gt; &lt;p&gt;- cron. Useful but I already use other tools for that and I can always recall a skill whenever i want. I don't need everyday at 8:00AM, i prefere recall it when i want with up to date data&lt;/p&gt; &lt;p&gt;Conclusion: for me &amp;quot;opencode web&amp;quot; is a much superior option, but much of the &amp;quot;intelligence&amp;quot; and value is the skills that you develop or you integrate, not in the runner itself, what do you think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Traffic_7873"&gt; /u/Deep_Traffic_7873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T11:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbnczy</id>
    <title>The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets.</title>
    <updated>2026-02-22T14:34:36+00:00</updated>
    <author>
      <name>/u/w1nter5n0w</name>
      <uri>https://old.reddit.com/user/w1nter5n0w</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt; &lt;img alt="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." src="https://preview.redd.it/l8duwvse42lg1.png?width=140&amp;amp;height=106&amp;amp;auto=webp&amp;amp;s=2928d1df2289068d0491626609ab2109106409dc" title="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, a friend of mine posted a thread here (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/&lt;/a&gt;) regarding a project he started called &lt;strong&gt;DeepSeek-Overclock&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal was to create an experimental setup designed to theoretically push the model's reasoning capabilities to the absolute limit. However, the &amp;quot;overclocked&amp;quot; DeepSeek model kept failing during the process. After diving deep into the logs, he realized the model wasn't hallucinating. In many instances, it was rigorously deriving answers that were technically correct but contradicted the provided &amp;quot;gold standard&amp;quot; labels.&lt;/p&gt; &lt;p&gt;He ended up writing Python scripts to verify the math line-by-line from first principles. Then he found out that &lt;strong&gt;the data quality in both the GPQA and HLE (Humanity's Last Exam) test sets is seriously flawed.&lt;/strong&gt; (You can check the link above for the specific details of that investigation).&lt;/p&gt; &lt;p&gt;Fast forward to a couple of days ago, and the &lt;strong&gt;Qwen team just released a paper&lt;/strong&gt; that basically confirms exactly what we saw: the data quality in GPQA and HLE is a mess.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20"&gt;https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Attached the screenshot of Fig. 1: Structural composition of HLE-Verified.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arxiv Link:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2602.13964v2"&gt;https://arxiv.org/abs/2602.13964v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper doesn't mince words. Right from the intro, it bluntly points out that a lot of the questions in the HLE test set are fundamentally broken. And in some cases, &amp;quot;standard answers&amp;quot; that are straight-up wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w1nter5n0w"&gt; /u/w1nter5n0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T14:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbkeea</id>
    <title>Which one are you waiting for more: 9B or 35B?</title>
    <updated>2026-02-22T12:15:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt; &lt;img alt="Which one are you waiting for more: 9B or 35B?" src="https://preview.redd.it/jyvany3jf1lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f667e97854acf566b7f6d1d56e9c09e17f5a8ee8" title="Which one are you waiting for more: 9B or 35B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jyvany3jf1lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T12:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
