<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-19T01:12:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r8fk3h</id>
    <title>Cosmos-Reason2 running on Jetson Orin Nano Super</title>
    <updated>2026-02-18T21:15:02+00:00</updated>
    <author>
      <name>/u/No-Dragonfly6246</name>
      <uri>https://old.reddit.com/user/No-Dragonfly6246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;About a month ago NVIDIA released Cosmos-Reason2 (&lt;a href="https://github.com/nvidia-cosmos/cosmos-reason2?utm_source=chatgpt.com"&gt;https://github.com/nvidia-cosmos/cosmos-reason2&lt;/a&gt;), with official support aimed at DGX Spark, H100, GB200 and Jetson AGX Thor.&lt;/p&gt; &lt;p&gt;We just pushed a heavily quantized (and highly accurate) version of nvidia/Cosmos-Reason2-2B and together with some other tricks Cosmos Reason 2 now runs on the &lt;strong&gt;full Jetson lineup,&lt;/strong&gt; including the most affordable and constrained stuff (Orin Nano Super).&lt;/p&gt; &lt;p&gt;HF Link with models, instructions, and benchmarks: &lt;a href="https://huggingface.co/embedl/Cosmos-Reason2-2B-W4A16"&gt;https://huggingface.co/embedl/Cosmos-Reason2-2B-W4A16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’ll be releasing more optimized Cosmos variants over the next few weeks, along with additional performance improvements. Two questions for the sub that would greatly help us align this with community interest:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There’s no clear &amp;quot;standard&amp;quot; for running models on Jetson (llama.cpp limited for VLMs and Jetson, TensorRT-LLM is heavy, etc.). We added vLLM support following NVIDIA’s direction. What are people's preferences?&lt;/li&gt; &lt;li&gt;For edge VLM deployments, what’s the first bottleneck you hit: weights, vision encoding, or KV cache/context length?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Dragonfly6246"&gt; /u/No-Dragonfly6246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8fk3h/cosmosreason2_running_on_jetson_orin_nano_super/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8fk3h/cosmosreason2_running_on_jetson_orin_nano_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8fk3h/cosmosreason2_running_on_jetson_orin_nano_super/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T21:15:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8j5y9</id>
    <title>Building an opensource Living Context Engine</title>
    <updated>2026-02-18T23:34:54+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8j5y9/building_an_opensource_living_context_engine/"&gt; &lt;img alt="Building an opensource Living Context Engine" src="https://external-preview.redd.it/ZHd2bTh0MWE0Y2tnMTSWAiwU5Zm-wtwyH8ihCsjzyh8lS1uR-vc1xsvFK1G5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f1bce7edd49f4a18ed90900fc16965b12bf26c1" title="Building an opensource Living Context Engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I m working on this opensource project gitnexus, have posted about it here before too, I have just published a CLI tool which will index your repo locally and expose it through MCP ( skip the video 30 seconds to see claude code integration ). &lt;/p&gt; &lt;p&gt;Got some great idea from comments before and applied it, pls try it and give feedback. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;br /&gt; It creates knowledge graph of codebases, make clusters, process maps. Basically skipping the tech jargon, the idea is to make the tools themselves smarter so LLMs can offload a lot of the retrieval reasoning part to the tools, making LLMs much more reliable. I found haiku 4.5 was able to outperform opus 4.5 using its MCP on deep architectural context.&lt;/p&gt; &lt;p&gt;Therefore, it can accurately do auditing, impact detection, trace the call chains and be accurate while saving a lot of tokens especially on monorepos. LLM gets much more reliable since it gets Deep Architectural Insights and AST based relations, making it able to see all upstream / downstream dependencies and what is located where exactly without having to read through files.&lt;/p&gt; &lt;p&gt;Also you can run gitnexus wiki to generate an accurate wiki of your repo covering everything reliably ( highly recommend minimax m2.5 cheap and great for this usecase )&lt;/p&gt; &lt;p&gt;repo wiki of gitnexus made by gitnexus :-) &lt;a href="https://gistcdn.githack.com/abhigyantrumio/575c5eaf957e56194d5efe2293e2b7ab/raw/index.html#other"&gt;https://gistcdn.githack.com/abhigyantrumio/575c5eaf957e56194d5efe2293e2b7ab/raw/index.html#other&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Webapp: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt; (A ⭐ would help a lot :-) )&lt;/p&gt; &lt;p&gt;to set it up:&lt;br /&gt; 1&amp;gt; npm install -g gitnexus&lt;br /&gt; 2&amp;gt; on the root of a repo or wherever the .git is configured run gitnexus analyze&lt;br /&gt; 3&amp;gt; add the MCP on whatever coding tool u prefer, right now claude code will use it better since I gitnexus intercepts its native tools and enriches them with relational context so it works better without even using the MCP. &lt;/p&gt; &lt;p&gt;Also try out the skills - will be auto setup when u run gitnexus analyze&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;mcp&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;gitnexus&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;args&amp;quot;: [&amp;quot;-y&amp;quot;, &amp;quot;gitnexus@latest&amp;quot;, &amp;quot;mcp&amp;quot;]&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;Everything is client sided both the CLI and webapp ( webapp uses webassembly to run the DB engine, AST parsers etc )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ctke3t1a4ckg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8j5y9/building_an_opensource_living_context_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8j5y9/building_an_opensource_living_context_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T23:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8biu3</id>
    <title>AnythingLLM Desktop works across your entire OS with local models</title>
    <updated>2026-02-18T18:45:52+00:00</updated>
    <author>
      <name>/u/tcarambat</name>
      <uri>https://old.reddit.com/user/tcarambat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8biu3/anythingllm_desktop_works_across_your_entire_os/"&gt; &lt;img alt="AnythingLLM Desktop works across your entire OS with local models" src="https://external-preview.redd.it/MzI4cTJobGZxYWtnMWcTOysjh4KRAQS1HqUZTuY8uTJ3Gln28lnaxHmPC-Xx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b13f3d8c0d97541f079273e47f93ffe51c7ab640" title="AnythingLLM Desktop works across your entire OS with local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Tim from AnythingLLM here!) &lt;/p&gt; &lt;p&gt;Today, we released &lt;a href="https://anythingllm.com/desktop"&gt;AnythingLLM Desktop v1.11.0&lt;/a&gt; and it is a step towards our new direction that becomes more of an extension of your OS and less of a sandboxed app.&lt;/p&gt; &lt;p&gt;Now with a simple customized keybind you can open an overlay that instantly has access to your open apps and screen. This works for both multi-modal &lt;strong&gt;but also&lt;/strong&gt; non-vision enabled models.&lt;/p&gt; &lt;p&gt;This functionality is all on top of all the stuff people use AnythingLLM for already: Chatting with documents, RAG, agents, MCPs, and more. This panel also has awareness of any &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qk1u6h/we_added_an_ondevice_ai_meeting_note_taker_into/"&gt;Meeting transcripts&lt;/a&gt; you might have too!&lt;/p&gt; &lt;p&gt;This is all done using on-device models and pipelines - using a local model you can have a fully on-device experience. In that demo I am using Qwen3-VL 4B Instruct (Q4) on a Macbook M4 Pro but you can really bring in any model or provider you want.&lt;/p&gt; &lt;p&gt;By default, everything AnythingLLM does can be customized but is on-device first with the option to bring your own key to use whatever you like to use for inference (Ollama, LM Studio, OpenAi, etc). We also bench on old (and bad) hardware that env on underpowered devices you can still have some semblance of a great experience.&lt;/p&gt; &lt;p&gt;We are trying to &amp;quot;simplify&amp;quot; our entire experience but still allow power-users like on this sub to get that customization they always require. We also have an &lt;a href="https://github.com/Mintplex-Labs/anything-llm"&gt;OSS MIT license multi-user server based version&lt;/a&gt; of AnythingLLM if you are looking for something more hostable on a VM or something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tcarambat"&gt; /u/tcarambat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/onupvglfqakg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8biu3/anythingllm_desktop_works_across_your_entire_os/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8biu3/anythingllm_desktop_works_across_your_entire_os/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T18:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7r7zr</id>
    <title>GLM-5 Technical Report</title>
    <updated>2026-02-18T02:51:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"&gt; &lt;img alt="GLM-5 Technical Report" src="https://preview.redd.it/phk5j82g36kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a4da644c3d3988eba39a218faf8a811456998b3" title="GLM-5 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Presenting the GLM-5 Technical Report!&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2602.15763"&gt;http://arxiv.org/abs/2602.15763&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After the launch of GLM-5, we’re pulling back the curtain on how it was built. Key innovations include:&lt;/p&gt; &lt;p&gt;- DSA Adoption: Significantly reduces training and inference costs while preserving long-context fidelity&lt;/p&gt; &lt;p&gt;- Asynchronous RL Infrastructure: Drastically improves post-training efficiency by decoupling generation from training&lt;/p&gt; &lt;p&gt;- Agent RL Algorithms: Enables the model to learn from complex, long-horizon interactions more effectively&lt;/p&gt; &lt;p&gt;Through these innovations, GLM-5 achieves SOTA performance among open-source models, with particularly strong results in real-world software engineering tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phk5j82g36kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T02:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8kybv</id>
    <title>Best coding models (or other models) one can run on an rtx5070ti (16gb vram) with of 64gb RAM</title>
    <updated>2026-02-19T00:51:53+00:00</updated>
    <author>
      <name>/u/cmdr-William-Riker</name>
      <uri>https://old.reddit.com/user/cmdr-William-Riker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just playing around. I am aware that this isn't going to be anything groundbreaking you can run on hardware like this, but I am curious if there are any small models that have any genuine use for coding in particular or other use cases if not that could fit in moderate consumer hardware yet. I've run Deepseek and llama 8b models, which are definitely good, but I was actually able to run those models on an rtx3050 with 8gb of vram and 32gb of ram easily. I'm just wondering if there are any models that can make use of slightly better hardware that I have now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmdr-William-Riker"&gt; /u/cmdr-William-Riker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8kybv/best_coding_models_or_other_models_one_can_run_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8kybv/best_coding_models_or_other_models_one_can_run_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8kybv/best_coding_models_or_other_models_one_can_run_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T00:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8jgwv</id>
    <title>I built a local AI dev assistant with hybrid RAG (vector + knowledge graph) that works with any Ollama model</title>
    <updated>2026-02-18T23:48:03+00:00</updated>
    <author>
      <name>/u/ikchain</name>
      <uri>https://old.reddit.com/user/ikchain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I've been using Claude Code as my main dev tool for months, but I got tired of burning tokens on repetitive tasks, generating docstrings, basic code reviews, answering questions about my own stack. So I built something local to handle that.&lt;/p&gt; &lt;p&gt;Fabrik-Codek is a model-agnostic local assistant that runs on top of Ollama. The interesting part isn't the chat wrapper, it's what's underneath:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hybrid RAG: combines LanceDB (vector search) with a NetworkX knowledge graph. So when you ask a question, it pulls context from both semantic similarity AND entity relationships&lt;/li&gt; &lt;li&gt;Data Flywheel: every interaction gets captured automatically. The system learns how you work over time&lt;/li&gt; &lt;li&gt;Extraction Pipeline: automatically builds a knowledge graph from your training data, technical decisions, and even Claude Code session transcripts (thinking blocks)&lt;/li&gt; &lt;li&gt;&lt;p&gt;REST API: 7 FastAPI endpoints with optional API key auth, so any tool (or agent) can query your personal knowledge base&lt;/p&gt; &lt;p&gt;Works with Qwen, Llama, DeepSeek, Codestral, Phi, Mistral... whatever you have in Ollama. Just --model flag or change the .env.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's not going to replace Claude or GPT for complex tasks, but for day-to-day stuff where you want zero latency, zero cost, and your data staying on your machine, it's been really useful for me.&lt;/p&gt; &lt;p&gt;413 tests, MIT license, ~3k LOC.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ikchain/Fabrik-Codek"&gt;https://github.com/ikchain/Fabrik-Codek&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, especially on the hybrid RAG approach. First time publishing something open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikchain"&gt; /u/ikchain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jgwv/i_built_a_local_ai_dev_assistant_with_hybrid_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jgwv/i_built_a_local_ai_dev_assistant_with_hybrid_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jgwv/i_built_a_local_ai_dev_assistant_with_hybrid_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T23:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r877tl</id>
    <title>Vibe Check: Latest models on AMD Strix Halo</title>
    <updated>2026-02-18T16:13:31+00:00</updated>
    <author>
      <name>/u/bhamm-lab</name>
      <uri>https://old.reddit.com/user/bhamm-lab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing a bunch of recent drops on my AMD homelab (Ryzen AI Max+ 395 + R9700) with a very non-scientific “vibe check” workflow (Roo Code + Open WebUI).&lt;/p&gt; &lt;p&gt;A few standouts that replaced my old stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi Linear 48B Instruct&lt;/strong&gt; as a daily-driver generalist.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 Coder Next&lt;/strong&gt; as my new coding model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q2_K_XL&lt;/strong&gt; on huge models is… surprisingly not trash? (Still too slow for HITL, but decent for background tasks like summarization or research).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full write-up and latency numbers here: &lt;a href="https://site.bhamm-lab.com/blogs/upgrade-models-feb26/"&gt;https://site.bhamm-lab.com/blogs/upgrade-models-feb26/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious what other people are running with limited hardware and what use cases work for them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bhamm-lab"&gt; /u/bhamm-lab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r877tl/vibe_check_latest_models_on_amd_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r877tl/vibe_check_latest_models_on_amd_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r877tl/vibe_check_latest_models_on_amd_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T16:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7y86d</id>
    <title>Gemma 27B/12B/4B/1B finetunes from DavidAU (20 models)</title>
    <updated>2026-02-18T09:13:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Gemma 3 (1b, 4b, 12b and 27b) - Uncensored full Reasoning/Thinking models fine tuned using top distill datasets.&lt;/p&gt; &lt;p&gt;20 Gemma 3 models 1B, 4B, 12B and 27B with full reasoning using GLM 4.7 Flash, GPT, Claude and Gemini datasets and more fully fine tuned using Unsloth.&lt;/p&gt; &lt;p&gt;Most models are Heretic'ed (uncensored) first, and tuned second.&lt;br /&gt; This vastly improves the model.&lt;/p&gt; &lt;p&gt;Models are also bench marked and in almost all cases exceed org model metrics - and in some cases by a lot.&lt;/p&gt; &lt;p&gt;Enjoy the freedom and more powerful THINKING/REASONING and UNCENSORED Gemma 3s !&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored"&gt;https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DavidAU on reddit: &lt;a href="/u/Dangerous_Fix_5526/"&gt;u/Dangerous_Fix_5526/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T09:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r81w8n</id>
    <title>(Google) On Surprising Effectiveness of Masking Updates in Adaptive Optimizers</title>
    <updated>2026-02-18T12:38:51+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r81w8n/google_on_surprising_effectiveness_of_masking/"&gt; &lt;img alt="(Google) On Surprising Effectiveness of Masking Updates in Adaptive Optimizers" src="https://external-preview.redd.it/ta2IiH0S_hDLLmbY2FJbCETWnyAZ9mwNCBtInzZsN24.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffd08352f3df59650efb4f607d20fabe9a962ba0" title="(Google) On Surprising Effectiveness of Masking Updates in Adaptive Optimizers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2602.15322"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r81w8n/google_on_surprising_effectiveness_of_masking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r81w8n/google_on_surprising_effectiveness_of_masking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T12:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r85z1t</id>
    <title>Even with Opus 4.6 and massive context windows, this is still the only thing that saves my production pipelines</title>
    <updated>2026-02-18T15:27:51+00:00</updated>
    <author>
      <name>/u/tdeliev</name>
      <uri>https://old.reddit.com/user/tdeliev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85z1t/even_with_opus_46_and_massive_context_windows/"&gt; &lt;img alt="Even with Opus 4.6 and massive context windows, this is still the only thing that saves my production pipelines" src="https://preview.redd.it/esofp8nbu9kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3395f6dda8e8eaa898f976c6258bdb37d5c27231" title="Even with Opus 4.6 and massive context windows, this is still the only thing that saves my production pipelines" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all got excited when the new reasoning models dropped. Better at following instructions, longer context, fewer hallucinations. Great.&lt;/p&gt; &lt;p&gt;Still seeing agentic workflows fail at basic deterministic logic because teams treat the LLM as a CPU instead of what it is — a reasoning engine.&lt;/p&gt; &lt;p&gt;After the bug I shared on Monday (RAG pipeline recommending a candidate based on a three-year-old resume), I made my team go back to basics. Wrote a checklist I’ve been calling the Delegation Filter.&lt;/p&gt; &lt;p&gt;The first question does most of the heavy lifting:&lt;/p&gt; &lt;p&gt;“Is the outcome deterministic?”&lt;/p&gt; &lt;p&gt;If yes — don’t use an LLM. I don’t care if it’s GPT-5 or Opus 4.6. Write a SQL query. Deterministic code is free and correct every time. Probabilistic models are expensive and correct most of the time. For tasks where “most of the time” isn’t good enough, that gap will bite you.&lt;/p&gt; &lt;p&gt;Am I the only one who feels like we’re forgetting how to write regular code because the models got too good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tdeliev"&gt; /u/tdeliev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/esofp8nbu9kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85z1t/even_with_opus_46_and_massive_context_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r85z1t/even_with_opus_46_and_massive_context_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8g0iw</id>
    <title>MiniMax-M2.5-REAP from cerebras</title>
    <updated>2026-02-18T21:32:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2.5-REAP-172B-A10B"&gt;https://huggingface.co/cerebras/MiniMax-M2.5-REAP-172B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2.5-REAP-139B-A10B"&gt;https://huggingface.co/cerebras/MiniMax-M2.5-REAP-139B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;REAP are smaller versions of models that you can fit on your setup and be happy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8g0iw/minimaxm25reap_from_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8g0iw/minimaxm25reap_from_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8g0iw/minimaxm25reap_from_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T21:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8cc72</id>
    <title>Model: support GLM-OCR merged! LLama.cpp</title>
    <updated>2026-02-18T19:15:13+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19677"&gt;https://github.com/ggml-org/llama.cpp/pull/19677&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't wait to test!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8cc72/model_support_glmocr_merged_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8cc72/model_support_glmocr_merged_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8cc72/model_support_glmocr_merged_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T19:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r89a4y</id>
    <title>Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing</title>
    <updated>2026-02-18T17:26:07+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"&gt; &lt;img alt="Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing" src="https://preview.redd.it/jdgxyzrhdakg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=7956ef19d5a35f0a082596b3cb054ece5781faf7" title="Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of digging through SillyTavern's config every time I wanted to change the tone of a scene. So I built my own thing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; sliders instead of prompts. Want slow burn? Drag pacing down. High tension? Push intensity up. The app handles prompt injections behind the scenes. There are presets too if you don't want to tweak manually.&lt;/p&gt; &lt;p&gt;Chat with an inspector panel: Mood, Pacing, Intensity, Dialogue Style, Initiative, Descriptiveness, Unpredictability, Emotional Depth. All visual, no prompt editing needed.&lt;/p&gt; &lt;p&gt;Writer mode for longer stuff. Each chapter gets its own controls: Tone, Pacing, POV, Creativity, Tension, Detail, Dialogue Share. You can generate, expand, rewrite or summarize scenes. Generation runs in the background so you can chat while it writes.&lt;/p&gt; &lt;p&gt;Characters are shared between chat and writing. Build one in chat, drop them into a novel. Imports ST V2 cards and JSON. Avatars pull from Chub.&lt;/p&gt; &lt;p&gt;Lorebooks with keyword activation. MCP tool calling with per-function toggles. Multi-agent chat with auto turn switching. File attachments and vision in chat. Export to MD/DOCX.&lt;/p&gt; &lt;p&gt;Works with Ollama, LM Studio, OpenAI, OpenRouter, or any compatible endpoint. Light and dark themes. English, Russian, Chinese, Japanese.&lt;/p&gt; &lt;p&gt;Still rough around the edges but actively developing. Would love feedback.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r89a4y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T17:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8157s</id>
    <title>Qwen 3.5 MXFP4 quants are coming - confirmed by Junyang Lin</title>
    <updated>2026-02-18T12:01:58+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most here are aware that OpenAI did something very well with their GPT-Oss release - they trained their model in 4 bit and delivered native mxfp4 quants which means a lot higher quality than the typical Unsloth and Bartowski quants of bf16 models. Google did it too with Gemma 3 QAT which was very well received by the community. Super excited for it, this is definately the right direction to take!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/2024002713579651245"&gt;https://x.com/JustinLin610/status/2024002713579651245&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T12:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r87ou8</id>
    <title>UPDATE#3: repurposing 800 RX 580s converted to AI cluster</title>
    <updated>2026-02-18T16:30:14+00:00</updated>
    <author>
      <name>/u/rasbid420</name>
      <uri>https://old.reddit.com/user/rasbid420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone, posting an update on the ETH mining farm conversion project. last time i posted we were still figuring out what to even do with 800 rx 580s (mix of 4gb and 8gb sapphire nitro+ and pulse cards) sitting in an old ethereum mining farm&lt;/p&gt; &lt;p&gt;so the tldr is we think we finally found a good use case. maybe two actually.&lt;/p&gt; &lt;p&gt;the fundamental problem with these gpus is the interdevice communication. they have good usable vram 8GB but low pcie speeds, low memory bandwith, and each card sitting on its a celeron g3950 board with 8gb of system ram. you cant do tensor parallelism across nodes with these things. we tried, its not happening. the latency between devices kills anything... so we had to completely rethink the approach. instead of trying to make them work together on one big model through parallelism on a node or even RPC in network, we treat each gpu as a completely independant inference worker. one model per gpu, one request at a time, working in parallel across a cluster.&lt;/p&gt; &lt;p&gt;getting llama.cpp to run on gfx803 polaris in 2026 is... an experience. rocm support for more than one card is dismal for these cards and the biggest issue still is &amp;quot;PCI-E ATOMICS support&amp;quot;... we can't build llama.cpp with a HIP backend because we have 6 cards on each rig and it doesn't see more than one card...&lt;/p&gt; &lt;p&gt;so we went with vulkan and tested and benchmarked internally all the possible permutations and combinations with vulkan / ubuntu&lt;/p&gt; &lt;p&gt;and came up with the most optimal settings to run and build llama.cpp's vulkan for rx580 support&lt;/p&gt; &lt;p&gt;so our dockerfile_v43 that builds the entire graphics stack from source looks like this:&lt;/p&gt; &lt;p&gt;- libdrm 2.4.121 from source&lt;/p&gt; &lt;p&gt;- wayland 1.22 from source&lt;/p&gt; &lt;p&gt;- mesa 24.2.0 from source with llvm 15 and the radv vulkan driver&lt;/p&gt; &lt;p&gt;- vulkan sdk 1.3.283&lt;/p&gt; &lt;p&gt;- then llama.cpp on top of all that&lt;/p&gt; &lt;p&gt;we had to build with GGML_NATIVE=ON because avx2/fma produces a binary that segfaults on every worker node because celerons dont have avx. we had to explicitly disable everything except sse4.2:&lt;/p&gt; &lt;p&gt;-DGGML_NATIVE=OFF -DGGML_AVX=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF -DGGML_F16C=OFF -DGGML_SSE42=ON&lt;/p&gt; &lt;p&gt;CXXFLAGS=&amp;quot;-march=x86-64 -mtune=generic&amp;quot;&lt;/p&gt; &lt;p&gt;the model we use is qwen3-vl-8b-instruct which is a visual language model. the q4 quantization fits on a single 8gb card with room for 6k context tokens. we run 4 tiers of quantization across the fleet: q4 on 1 gpu, q8 on 2 gpus, bf16 on 3 or 6 gpus for quality escalation AND / OR bigger context&lt;/p&gt; &lt;p&gt;&lt;strong&gt;use case #1: mass document OCR / visual document understanding&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;we can process large documents like textbooks, medical literature, legal docs for high quality text extractions. the pdf gets split into individual pages, each page gets converted to an image and sent to a seperate gpu for visual understanding. you can get 200 gpus to process 200 pages simultaneously.&lt;/p&gt; &lt;p&gt;our quality benchmark is a clinical opthalmology of 966 pages of dense medical terminology, complex diagrams, photographic plates, multi-column layouts, tables, cursive annotations. the works. doing this through openai api with a visual model costs about $12 per run. we do it for roughly $0.50 in electricity at our local hydro rate of $0.065/kwh. thats 24x cheaper on opex and the capex is essentially nothing because we already had the hardware sitting there from the mining days. cards cost us like $80 per 8gb of vram vs $365/gb if you compare with an h100.&lt;/p&gt; &lt;p&gt;quality wise, its honestly comparable for document understanding work. cursive text, messy handwriting, charts, tables, images, the quantized qwen3-vl handles it.&lt;/p&gt; &lt;p&gt;the escalation path goes: tier 1 (q4, 175 dpi) &amp;gt; tier 2 (q8, 200 dpi) &amp;gt; tier 3 (bf16, 250 dpi) &amp;gt; tier 4 (bf16 on 6 gpus, 300 dpi). after 3 retries we accept degraded quality if it's impossible work but it works suprisingly well... most pages resolve on tier 1, only the really nasty scans escalate up.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;use case #2: video frame analysis (work in progress)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;this is the next thing were working on. same architecture but for video. 60 seconds of video at ~13fps = 800 frames. distribute 800 frames across 800 gpus,&lt;/p&gt; &lt;p&gt;each one describes what it sees in that frame. then you do temporal clustering, entity tracking, event extraction, and build a scene summary on top&lt;/p&gt; &lt;p&gt;the idea is to provide an endpoint where users can send video data and get back structured visual analysis. you could build monitoring alerts, safety assessments, quality assurance checks on top of it. stuff that currently costs way too much through traditional api calls to be practical at scale&lt;/p&gt; &lt;p&gt;were still early on this one but the architecture should translate pretty directly from the document pipeline. the hard part will be the temporal synthesis layers on top.&lt;/p&gt; &lt;p&gt;anyway... thats where were at. the mining farm to ai cluster conversion has been a year of pain but we finally have something that we can call useful&lt;/p&gt; &lt;p&gt;the key advantage of this cluster is the low cost of text extraction from documents which in turn can should be fed into a RAG pipeline like a chatgpt window for embedding/vectorization/good high quality chat on top of that document&lt;/p&gt; &lt;p&gt;happy to hear any feedback or any further ideas about this&lt;/p&gt; &lt;p&gt;&lt;a href="https://hyperstract.com"&gt;https://hyperstract.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the system is capable of processing big pdfs of 400 pages per minute but please don't abuse it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rasbid420"&gt; /u/rasbid420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T16:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8d4iq</id>
    <title>model: support GLM-OCR by ngxson · Pull Request #19677 · ggml-org/llama.cpp</title>
    <updated>2026-02-18T19:44:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8d4iq/model_support_glmocr_by_ngxson_pull_request_19677/"&gt; &lt;img alt="model: support GLM-OCR by ngxson · Pull Request #19677 · ggml-org/llama.cpp" src="https://external-preview.redd.it/gy3Bao2ncM4JSj1HjFdjb15hySU2009NljOUnQ4h7EI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b6bc867195227228f82cc5f935733bfd4718813" title="model: support GLM-OCR by ngxson · Pull Request #19677 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr &lt;strong&gt;0.9B OCR model (you can run it on any potato)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder–decoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale image–text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State-of-the-Art Performance&lt;/strong&gt;: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks, including formula recognition, table recognition, and information extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized for Real-World Scenarios&lt;/strong&gt;: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy to Use&lt;/strong&gt;: Fully open-sourced and equipped with a comprehensive &lt;a href="https://github.com/zai-org/GLM-OCR"&gt;SDK&lt;/a&gt; and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19677"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8d4iq/model_support_glmocr_by_ngxson_pull_request_19677/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8d4iq/model_support_glmocr_by_ngxson_pull_request_19677/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T19:44:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8c6th</id>
    <title>FlashLM v4: 4.3M ternary model trained on CPU in 2 hours — coherent stories from adds and subtracts only</title>
    <updated>2026-02-18T19:09:37+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back with v4. Some of you saw v3 — 13.6M params, ternary weights, trained on CPU, completely incoherent output. Went back to the drawing board and rebuilt everything from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;4.3M parameter language model where every weight in the model body is -1, 0, or +1. Trained for 2 hours on a free Deepnote notebook (2 threads, 5GB RAM). No GPU at any point — not for training, not for inference. The model generates coherent children’s stories with dialogue and narrative structure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fair comparison using BPC:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick note on the metric — you can’t directly compare validation loss across models with different tokenizers because the tokenizer changes how many tokens a sentence gets split into. BPC (bits-per-character) fixes this by measuring compression per character of raw text instead of per token. Tokenizer drops out of the equation entirely.&lt;/p&gt; &lt;p&gt;Evaluated on 500 TinyStories validation stories (405K characters):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;FlashLM v4&lt;/th&gt; &lt;th align="left"&gt;TinyStories-1M&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Params&lt;/td&gt; &lt;td align="left"&gt;4.3M (ternary)&lt;/td&gt; &lt;td align="left"&gt;3.7M (float32)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BPC&lt;/td&gt; &lt;td align="left"&gt;0.88&lt;/td&gt; &lt;td align="left"&gt;0.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;2-thread CPU (free tier)&lt;/td&gt; &lt;td align="left"&gt;V100 GPU&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training time&lt;/td&gt; &lt;td align="left"&gt;2 hours&lt;/td&gt; &lt;td align="left"&gt;Hours (GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tokens seen&lt;/td&gt; &lt;td align="left"&gt;10.6M&lt;/td&gt; &lt;td align="left"&gt;~470M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Architecture&lt;/td&gt; &lt;td align="left"&gt;Gated conv + GLU (no attention)&lt;/td&gt; &lt;td align="left"&gt;GPT-Neo (attention)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We’re behind, but we’ve seen 2.3% of their training data and the loss curve was still going down when time ran out. The model is undertrained, not underdesigned.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What changed from v3:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;v3’s fatal flaw was the output layer. 50,257 vocab with d_model=256 meant 86% of training compute went to the softmax projection. The actual ternary model core got 14% of the compute budget. Also trained on FineWeb-Edu which is way too broad for a tiny model — like asking a 4-year-old to memorize Wikipedia.&lt;/p&gt; &lt;p&gt;v4 changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vocab 50K → 10K with weight-tied embeddings, killed the softmax bottleneck&lt;/li&gt; &lt;li&gt;FineWeb-Edu → TinyStories, a focused dataset proven to work at small scale&lt;/li&gt; &lt;li&gt;New token mixer: gated causal depthwise convolution (kernel=8) instead of attention — O(T) not O(T²)&lt;/li&gt; &lt;li&gt;Added ternary GLU feed-forward (SiLU gating, 192→512→192)&lt;/li&gt; &lt;li&gt;RMSNorm instead of LayerNorm&lt;/li&gt; &lt;li&gt;6 blocks, d_model=192, 16.7MB total&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Embedding (10K × 192, float, weight-tied) → 6× BoltBlock: RMSNorm → GatedConvMixer (ternary depthwise conv + gate) + residual RMSNorm → TernaryGLU (ternary gate/up/down, SiLU) + residual → RMSNorm → Output Head (tied to embedding) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No attention anywhere. Token mixing is a gated causal conv with receptive field of 8 per layer (48 across all 6 layers). All linear projections use ternary quantization with straight-through estimator. At inference time the core ops are just adds, subtracts, and zeros.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sample output (step 5000):&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;The [] are UNK tokens from the 10K vocab not covering all TinyStories words — fixable by building vocab from actual corpus frequencies instead of taking the first 10K GPT-2 tokens.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training curve:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Val loss went from 9.2 → 2.10 over 5,199 steps (10.6M tokens). Never plateaued. Speed was ~1,480 tokens/sec on 2 threads.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Step&lt;/th&gt; &lt;th align="left"&gt;Val Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;500&lt;/td&gt; &lt;td align="left"&gt;2.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;2.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2000&lt;/td&gt; &lt;td align="left"&gt;2.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3000&lt;/td&gt; &lt;td align="left"&gt;2.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4000&lt;/td&gt; &lt;td align="left"&gt;2.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5000&lt;/td&gt; &lt;td align="left"&gt;2.10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;What’s next:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Someone in my DMs from the v3 post offered SSH access to a Ryzen 7950X3D (16 cores, 96MB V-Cache, 128GB RAM). Planning to train a scaled-up version (~15M params, d=384, 8 blocks) on that machine for multiple days with a proper frequency-based tokenizer. Target is closing the BPC gap with TinyStories-1M and pushing toward TinyStories-28M territory.&lt;/p&gt; &lt;p&gt;Also planning to release a standalone &lt;a href="http://train.py/"&gt;train.py&lt;/a&gt; so anyone can reproduce this on their own hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model + weights + model card: &lt;a href="https://huggingface.co/changcheng967/flashlm-v4-bolt"&gt;https://huggingface.co/changcheng967/flashlm-v4-bolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://huggingface.co/spaces/changcheng967/flashlm-v4-demo"&gt;https://huggingface.co/spaces/changcheng967/flashlm-v4-demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v3 for comparison: &lt;a href="https://huggingface.co/changcheng967/flashlm-v3-13m"&gt;https://huggingface.co/changcheng967/flashlm-v3-13m&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Code and model are MIT licensed. Happy to answer questions about the architecture or training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8c6th/flashlm_v4_43m_ternary_model_trained_on_cpu_in_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8c6th/flashlm_v4_43m_ternary_model_trained_on_cpu_in_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8c6th/flashlm_v4_43m_ternary_model_trained_on_cpu_in_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T19:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8gb3p</id>
    <title>Do we want the benefits of Ollama API without actually using Ollama?</title>
    <updated>2026-02-18T21:43:03+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8gb3p/do_we_want_the_benefits_of_ollama_api_without/"&gt; &lt;img alt="Do we want the benefits of Ollama API without actually using Ollama?" src="https://preview.redd.it/ye8e5rinobkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fee113ba5ec5804912453771ad9dbfd4c1c4053" title="Do we want the benefits of Ollama API without actually using Ollama?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apps with native Ollama API integration often have smoother setup and model management than what we get with the OpenAI API alone. For example, in Open WebUI (see image), the server is auto-detected on port &lt;code&gt;11434&lt;/code&gt; and you can pull, eject, and check the status of models right from the web ui.&lt;/p&gt; &lt;p&gt;As an experiment this week I added Ollama API support to Lemonade Server. We already had the functions, so I just had to hook them up to &lt;code&gt;/api&lt;/code&gt; endpoints. I think it's pretty neat, so I'm interested to hear what you all think.&lt;/p&gt; &lt;p&gt;Here's how it works:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;First: stop the Ollama service if you have it running&lt;/h1&gt; &lt;h1&gt;Start Lemonade on the Ollama port&lt;/h1&gt; &lt;p&gt;lemonade-server serve --port 11434&lt;/p&gt; &lt;h1&gt;Optional: use any llamacpp binaries you like&lt;/h1&gt; &lt;p&gt;export LEMONADE_LLAMACPP_VULKAN_BIN=/path/to/llama-server-folder&lt;/p&gt; &lt;h1&gt;or&lt;/h1&gt; &lt;p&gt;export LEMONADE_LLAMACPP_ROCM_BIN=/path/to/llama-server-folder&lt;/p&gt; &lt;h1&gt;Optional: use your own GGUFs from llamacpp -hf or LM Studio&lt;/h1&gt; &lt;p&gt;lemonade-server serve --port 11434 --extra-models-dir ~/.cache/llama.cpp&lt;/p&gt; &lt;h1&gt;or&lt;/h1&gt; &lt;p&gt;lemonade-server serve --port 11434 --extra-models-dir ~/.lmstudio/models ```&lt;/p&gt; &lt;p&gt;Then, start Open WebUI and it should auto-detect Lemonade, populate the models list with your GGUF and/or NPU models, and give you access to features that were otherwise Ollama-only.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;Get Lemonade v9.3.4 here&lt;/a&gt; if you want to give it a spin, and let me know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ye8e5rinobkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8gb3p/do_we_want_the_benefits_of_ollama_api_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8gb3p/do_we_want_the_benefits_of_ollama_api_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T21:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r85o89</id>
    <title>Devstral Small 2 24B + Qwen3 Coder 30B: Coders for Every Hardware (Yes, Even the Pi)</title>
    <updated>2026-02-18T15:16:53+00:00</updated>
    <author>
      <name>/u/enrique-byteshape</name>
      <uri>https://old.reddit.com/user/enrique-byteshape</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85o89/devstral_small_2_24b_qwen3_coder_30b_coders_for/"&gt; &lt;img alt="Devstral Small 2 24B + Qwen3 Coder 30B: Coders for Every Hardware (Yes, Even the Pi)" src="https://preview.redd.it/zzlx2eqlr9kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd8d72883f48c2f76c3641eab494e4d6657dfba" title="Devstral Small 2 24B + Qwen3 Coder 30B: Coders for Every Hardware (Yes, Even the Pi)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, &lt;em&gt;ByteShape’s back, alright! Everybody (yeah), you asked for coders (yeah). Everybody get your coders right:&lt;/em&gt; &lt;strong&gt;Devstral-Small-2-24B-Instruct-2512&lt;/strong&gt; (ShapeLearn-optimized for GPU) + &lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt; (optimized for all hardware and patience levels). Alright!&lt;/p&gt; &lt;p&gt;We're back at it with another GGUF quants release, this time focused on coder models and multimodal. We use our technology to find the optimal datatypes per layer to squeeze as much performance out of these models while compromising the least amount of accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Devstral&lt;/strong&gt; is the hero on &lt;strong&gt;RTX 40/50 series&lt;/strong&gt;. Also: it has a &lt;strong&gt;quality cliff ~2.30 bpw,&lt;/strong&gt; but ShapeLearn avoids faceplanting there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the “runs everywhere” option: &lt;strong&gt;Pi 5 (16GB) ~9 TPS&lt;/strong&gt; at ~&lt;strong&gt;90%&lt;/strong&gt; BF16 quality. (If you daily-drive that Pi setup, we owe you a medal.)&lt;/li&gt; &lt;li&gt;Picking a model is annoying: Devstral is &lt;strong&gt;more capable&lt;/strong&gt; but &lt;strong&gt;more demanding&lt;/strong&gt; (dense 24B + bigger KV). If your &lt;strong&gt;context fits&lt;/strong&gt; and TPS is fine → Devstral. Otherwise → Qwen.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;Devstral GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Qwen3 Coder 30B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/"&gt;Blog + plots&lt;/a&gt; (interactive graphs you can hover over and compare to Unsloth's models, with file name comparisons)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt; Qwen GGUFs ship with a &lt;strong&gt;custom template&lt;/strong&gt; that supports parallel tool calling (tested on llama.cpp; same template used for fair comparisons vs Unsloth). If you can sanity-check on different llama.cpp builds/backends and real coding workflows, any feedback will be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/enrique-byteshape"&gt; /u/enrique-byteshape &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzlx2eqlr9kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85o89/devstral_small_2_24b_qwen3_coder_30b_coders_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r85o89/devstral_small_2_24b_qwen3_coder_30b_coders_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:16:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r86i3o</id>
    <title>LLMs grading other LLMs 2</title>
    <updated>2026-02-18T15:47:24+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt; &lt;img alt="LLMs grading other LLMs 2" src="https://preview.redd.it/rmq2mwriw9kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07e6fa12e92be2b51d119d6c78ac4e28ccf7e1cb" title="LLMs grading other LLMs 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A year ago I made a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;meta-eval here on the sub&lt;/a&gt;, asking LLMs to grade a few criterias about other LLMs. &lt;/p&gt; &lt;p&gt;Time for the part 2.&lt;/p&gt; &lt;p&gt;The premise is very simple, the model is asked a few ego-baiting questions and other models are then asked to rank it. The scores in the pivot table are normalised.&lt;/p&gt; &lt;p&gt;You can find &lt;a href="https://huggingface.co/datasets/av-codes/cringebench"&gt;all the data on HuggingFace&lt;/a&gt; for your analysis.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rmq2mwriw9kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8jjtq</id>
    <title>More quantization visualization types (repost)</title>
    <updated>2026-02-18T23:51:43+00:00</updated>
    <author>
      <name>/u/copingmechanism</name>
      <uri>https://old.reddit.com/user/copingmechanism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt; &lt;img alt="More quantization visualization types (repost)" src="https://preview.redd.it/af1o3s52cckg1.gif?frame=1&amp;amp;width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=399ab3abe9aebeae4217cd2925119b0a76b11883" title="More quantization visualization types (repost)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by this post from &lt;a href="/u/VoidAlchemy"&gt;u/VoidAlchemy&lt;/a&gt; a few months back: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Intrusive thoughts had me try to reproduce and extend the work to include more quantization types, with/without imatrix, and some PPL/KLD measurements to see what an &amp;quot;efficient&amp;quot; quantization looks like. MXFP4 really doesn't like to participate in this sort of experiment, I don't have much faith this is a very accurate representation of the quant but oh-well.&lt;/p&gt; &lt;p&gt;The (vibe) code for this is here &lt;a href="https://codeberg.org/mailhost/quant-jaunt"&gt;https://codeberg.org/mailhost/quant-jaunt&lt;/a&gt; along with a sample of summary output (from lenna.bmp) and some specifications that might help keep the vibes on track.&lt;/p&gt; &lt;p&gt;*reposted to respect Lenna's retirement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/copingmechanism"&gt; /u/copingmechanism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r8jjtq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T23:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r83irw</id>
    <title>PSA: DDR5 RDIMM price passed the point were 3090 are less expensive per gb..</title>
    <updated>2026-02-18T13:51:04+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;Just wanted to note that RDIMM prices are so wild.. Stacking rdimms starts to be as expensive as stacking 3090s.. But RDIMM don't come with compute included..&lt;/p&gt; &lt;p&gt;What a crazy time, shall we stack rdimms or 3090, what's your take on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8ectu</id>
    <title>I plugged a $30 radio into my Mac mini and told my AI "connect to this" — now I control my smart home and send voice messages over radio with zero internet</title>
    <updated>2026-02-18T20:30:14+00:00</updated>
    <author>
      <name>/u/anvarazizov</name>
      <uri>https://old.reddit.com/user/anvarazizov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;So I live in Ukraine during the war. Power goes out a lot here – russia regularly attacks our power grid. When it happens, internet dies, cell towers go dark, and suddenly all my smart home stuff and AI tools become useless. Got tired of it, so I did something kind of ridiculous.&lt;/p&gt; &lt;p&gt;I bought two Lilygo T-Echo radios (~$30 each, LoRa 433MHz, running Meshtastic firmware). Plugged one into my always-on Mac mini via USB. Took the other one as my portable radio. Then I opened up my OpenClaw AI agent and basically said: &amp;quot;hey, there's a Meshtastic radio plugged in. Figure it out.&amp;quot;&lt;/p&gt; &lt;p&gt;And it did.&lt;/p&gt; &lt;h1&gt;What happened next&lt;/h1&gt; &lt;p&gt;It identified the Meshtastic device, installed the CLI, configured an encrypted channel, and then – without me writing a single line of code – built a full Python listener daemon that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Monitors the radio 24/7 for incoming messages&lt;/li&gt; &lt;li&gt;Routes them intelligently: if internet is up, forwards to Discord where a cloud AI responds. If internet is down, routes everything to local models via Ollama&lt;/li&gt; &lt;li&gt;Uses phi4-mini as a lightweight intent classifier (&amp;quot;is this a smart home command or a question?&amp;quot;) and gemma3:12b for actual answers ()&lt;/li&gt; &lt;li&gt;Talks to Home Assistant so I can control lights, read sensors, check who's home — all over radio&lt;/li&gt; &lt;li&gt;Auto-chunks responses to fit the 200-char LoRa limit&lt;/li&gt; &lt;li&gt;Watches an outbox folder – if the AI needs to alert me about something (like a power outage), it drops a message file there and the listener transmits it over LoRa&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole thing just worked. The AI had already built the architecture while I was still thinking about how to approach it.&lt;/p&gt; &lt;h1&gt;The voice thing (this is the cool part)&lt;/h1&gt; &lt;p&gt;Then I added one more feature. If I prefix a Meshtastic message with &lt;code&gt;SAY:&lt;/code&gt;, the listener takes the text, calls Home Assistant's TTS service, and plays it through my HA Voice PE speaker at home. In Ukrainian.&lt;/p&gt; &lt;p&gt;So I can be walking around with a T-Echo in my pocket, completely off-grid, type &lt;code&gt;SAY: Привіт, я скоро буду вдома&lt;/code&gt; (Hi, I'll come back home soon) – and my house literally speaks. No internet anywhere in the chain. Just radio waves → Mac mini → TTS → speaker.&lt;/p&gt; &lt;p&gt;Honestly didn't expect it to feel this magical.&lt;/p&gt; &lt;h1&gt;The stack&lt;/h1&gt; &lt;p&gt;Everything's open source except Claude (which is only used when internet is available):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenClaw&lt;/strong&gt; – you know what is this &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meshtastic&lt;/strong&gt; – LoRa mesh networking firmware. The magic sauce for off-grid communication – open source, encrypted, and any Meshtastic radio can relay messages to extend range&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lilygo T-Echo&lt;/strong&gt; – the $30 radio hardware running Meshtastic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; – you know as well&lt;/li&gt; &lt;li&gt;&lt;strong&gt;phi4-mini&lt;/strong&gt; – lightweight router/classifier&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gemma3:12b&lt;/strong&gt; – the actual brain for offline responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Home Assistant&lt;/strong&gt; – smart home + TTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HA Voice PE&lt;/strong&gt; – the speaker that reads messages aloud&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mac mini M4 16GB&lt;/strong&gt; – always-on server, running on battery backup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;T-Echo (portable) │ LoRa 433MHz, encrypted ▼ T-Echo (USB) → Mac mini │ ├── SAY: prefix → HA TTS → Voice PE speaker ├── AI: prefix → phi4-mini → gemma3:12b (always local) ├── status → Home Assistant sensors ├── Online? → forward to Discord (cloud AI) └── Offline? → route everything to local Ollama models Outbox: AI drops .msg files → listener sends over LoRa (power outage alerts, reminders, etc.) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's next&lt;/h1&gt; &lt;p&gt;I'm thinking about where this goes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mesh AI network&lt;/strong&gt; – Meshtastic is a mesh protocol, every radio relays. Multiple nodes running local LLMs could create a neighborhood-scale AI network with zero internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger local models&lt;/strong&gt; – looking at upgrading hardware for 30B+ parameter models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dead man's switch&lt;/strong&gt; — auto-alert if I don't check in within a time window&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anvarazizov"&gt; /u/anvarazizov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T20:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
