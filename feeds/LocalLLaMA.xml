<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-19T16:37:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oas343</id>
    <title>Build Advice - RTX 6000 / 7985WX</title>
    <updated>2025-10-19T15:11:57+00:00</updated>
    <author>
      <name>/u/Direct_Bodybuilder63</name>
      <uri>https://old.reddit.com/user/Direct_Bodybuilder63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there I‚Äôm about to pull the trigger on this on Monday. Is there anything I‚Äôm not taking into account here?&lt;/p&gt; &lt;p&gt;I currently have an 80TB SSD NAS, I‚Äôm debating going 25GbE for network so I can also use it for storage, and am considering adding an additional 7.68 or 15 NVMe U2/U3 SSD.&lt;/p&gt; &lt;p&gt;Is there anything you‚Äôd consider adding here or anything obvious I‚Äôve missed? Thanks. &lt;/p&gt; &lt;p&gt;CPU: Ryzen Threadripper PRO 7985WX ‚Äì 64C/128T, 3.2GHz base / 5.1GHz boost, 256MB L3 Cooler: AIO Liquid for SP3/TR4/TR5 RAM: 64GB DDR5 ECC REG 6400MT/s 512GB total&lt;/p&gt; &lt;p&gt;Storage: 2TB M.2 NVMe (OS) 7.68TB U.2/U.3 NVMe Enterprise SSD 20TB 7200RPM SATA HDD (Enterprise) GPU: NVIDIA RTX PRO 6000 Blackwell Max-Q, 96GB GDDR7, 300W x 2 Networking: 2x 10GbE + 1x GbE IPMI Case: 240x580x560mm, supports 4x double-wide GPUs PCIe Layout: 6x PCIe 5.0 x16 + 1x PCIe 5.0 x8 Motherboard storage: 4x SATA, 4x M.2 NVMe, 2x SlimSAS U.2/U.3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Bodybuilder63"&gt; /u/Direct_Bodybuilder63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oas343/build_advice_rtx_6000_7985wx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oas343/build_advice_rtx_6000_7985wx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oas343/build_advice_rtx_6000_7985wx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T15:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa3u2d</id>
    <title>The size difference of gpt-oss-120b vs it's abliterated version</title>
    <updated>2025-10-18T18:48:10+00:00</updated>
    <author>
      <name>/u/iamkucuk</name>
      <uri>https://old.reddit.com/user/iamkucuk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was away from the locally hosted models, so please forgive my ignorance.&lt;/p&gt; &lt;p&gt;Here are two versions of gpt-oss-120b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;https://ollama.com/library/gpt-oss&lt;/a&gt;&lt;br /&gt; &lt;a href="https://ollama.com/huihui_ai/gpt-oss-abliterated"&gt;https://ollama.com/huihui_ai/gpt-oss-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, one takes 88 GB and the other takes 65 GB, and the difference shows when they are loaded as well. I thought they were both 4-bit. Would someone be able to explain where the discrepancy is coming from? And if any abliterated versions of the original model's quant occupy the same space?&lt;/p&gt; &lt;p&gt;Another question would be, I can see the GGUF versions of gpt-oss. Why would we need GGUF versions, as the model itself already is quantized?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamkucuk"&gt; /u/iamkucuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T18:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oash81</id>
    <title>Best Agentic Coder</title>
    <updated>2025-10-19T15:27:35+00:00</updated>
    <author>
      <name>/u/Huge-Solution-7168</name>
      <uri>https://old.reddit.com/user/Huge-Solution-7168</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve tried Claude code, CLINE, continue, codex. I want to find the best local LLM based Claude code that I can run, have it debug and test/improve the code all by itself. I‚Äôll be using OSS:120b or any recommended model for the DGX Spark, what are yalls recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huge-Solution-7168"&gt; /u/Huge-Solution-7168 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oash81/best_agentic_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oash81/best_agentic_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oash81/best_agentic_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T15:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oatip1</id>
    <title>If the bubble really pops how can that affect local AI models?</title>
    <updated>2025-10-19T16:08:41+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If all this AI bubble talk really comes to an popa after all, how might this affect the development of more local AI models? From what I've seen MoE models still outperforms most models easily, but creating models is still expensive as shit, rather for the planet than their pocket, donation exists anyways.&lt;/p&gt; &lt;p&gt;But the servers these models use to be trained consumes a shitton of load, and I could imagine most big company servers not allowing AI to be trained on their servers anymore considering the massive amounts of models being released every week. Do you think AI would immediately freeze in advancement upon a bubble pop making us have to wait more 80 years for an actual AGI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T16:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa671p</id>
    <title>Could you recommend good LLM models for heavier stories that include NSFW content?</title>
    <updated>2025-10-18T20:19:23+00:00</updated>
    <author>
      <name>/u/Sr_M_Ghost</name>
      <uri>https://old.reddit.com/user/Sr_M_Ghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Deep Seek R2 0528, but I'd like other models that are better suited to this type of content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sr_M_Ghost"&gt; /u/Sr_M_Ghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T20:19:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oatzjf</id>
    <title>N00b looking to get initial hardware to play with</title>
    <updated>2025-10-19T16:27:04+00:00</updated>
    <author>
      <name>/u/dovi5988</name>
      <uri>https://old.reddit.com/user/dovi5988</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have been experimenting for now on &amp;quot;regular machines&amp;quot; (aka with no GPU) and I want to start experimenting a bit. I want to start by experimenting. My priority is working with TTS engines like Chatterbox (&lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;). Over all I am trying to figure out the hardware I should get to start learning and I am clueless. I learn more from playing then from reading docs. Can someone explain to me &amp;quot;like I am five&amp;quot; the quests below?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How GPU's work when it comes to loading models? Like if the model I am loading needs 8GB then do I need a card that has at least 8GB on it to load it?&lt;/li&gt; &lt;li&gt;If I want to run concurrent requests at once (say two requests at once) do I then need a card that has 16GB?&lt;/li&gt; &lt;li&gt;Is it better get a system like a MAC that has unified memory or get multiple cards? Again my goal for now is concurrently TTS. I would like to branch into Speech to Text with the spare time that I have (when I am not generating TTS).&lt;/li&gt; &lt;li&gt;What kind of cards should I look at? I have heard cards like the 4070, 3090 etc. but I am clueless where I start.&lt;/li&gt; &lt;li&gt;Can anyone explain the differences in cards other than the memory capacity? Like how do I know the speed of the card and how does that matter for concurrency and speed of testing.&lt;/li&gt; &lt;li&gt;How do I find out how much memory is needed (for instance for chatterbox). Do you look at the project and try to figure out what's needed or do you run it and find out what it takes?&lt;/li&gt; &lt;li&gt;Would one of these cards work with a Zima board?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For now I just want to experiment and test. I don't care so much about speed as I care about getting my feet wet and seeing what I can do. My current TTS bill with Google is about $150.00 per month and growing and I am wondering if it's time to get some GPU's and do it myself. I am also thinking about getting one of these (&lt;a href="https://marketplace.nvidia.com/en-us/developer/dgx-spark/"&gt;https://marketplace.nvidia.com/en-us/developer/dgx-spark/&lt;/a&gt;) but based on this video (&lt;a href="https://www.youtube.com/watch?v=FYL9e_aqZY0"&gt;https://www.youtube.com/watch?v=FYL9e_aqZY0&lt;/a&gt;) it seems like the bang per buck you get here is more for training. Side note: I have a pile Nvidia Jetsons' though I think they are only 2GB and doubt they can be of any use here.&lt;/p&gt; &lt;p&gt;TIA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dovi5988"&gt; /u/dovi5988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatzjf/n00b_looking_to_get_initial_hardware_to_play_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatzjf/n00b_looking_to_get_initial_hardware_to_play_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oatzjf/n00b_looking_to_get_initial_hardware_to_play_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T16:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oap367</id>
    <title>Total noob here who wants to run a local LLM to build my own coach and therapist chatbot</title>
    <updated>2025-10-19T13:02:02+00:00</updated>
    <author>
      <name>/u/tokyothrowie</name>
      <uri>https://old.reddit.com/user/tokyothrowie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I‚Äôm an absolute beginner when it comes to local LLMs. I‚Äôve been using ChatGPT, Claude, and Perplexity daily, but that‚Äôs about it. I work in hospitality and mostly with English speakers, but English is my second language.&lt;/p&gt; &lt;p&gt;I‚Äôve been thinking about building a local LLM that could act as a personal coach and therapist. I‚Äôve been in therapy with a certified therapist for the past 18 months, and she‚Äôs allowed me to record every session. Having those sessions twice a month has been a game changer for me.&lt;/p&gt; &lt;p&gt;The thing is, I pay around $100 per 45-minute session out of pocket, and I‚Äôm currently focused on paying off some debt. So, I‚Äôd like to reduce my sessions to once every 4‚Äì6 weeks instead and supplement them with something AI-based. My therapist is totally on board with this idea.&lt;/p&gt; &lt;p&gt;My main concern, though, is privacy. I don‚Äôt want to upload any personal data to random AI tools, which is why I want to explore a local setup. The problem is, I can‚Äôt afford new hardware right now I only have a Mac Mini M3 Pro. My goal is to run a local LLM offline, ideally with voice input, and have it push me like David Goggins but also use the same therapeutic techniques my therapist does.&lt;/p&gt; &lt;p&gt;The issue is.. I have zero clue where to start or if this is even possible. I see people on YouTube using tools like NotebookLM for personal stuff like Tiago Forte in one of his videos but I‚Äôm just too paranoid to trust big tech companies with something this personal.&lt;/p&gt; &lt;p&gt;Any advice, resources, or starting points would be super appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tokyothrowie"&gt; /u/tokyothrowie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oap367/total_noob_here_who_wants_to_run_a_local_llm_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oap367/total_noob_here_who_wants_to_run_a_local_llm_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oap367/total_noob_here_who_wants_to_run_a_local_llm_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T13:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaqr7x</id>
    <title>How to Fine tune a LLM to give it a persona?</title>
    <updated>2025-10-19T14:16:50+00:00</updated>
    <author>
      <name>/u/Bruce_spixky</name>
      <uri>https://old.reddit.com/user/Bruce_spixky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to fine tune a LLM for a hospital but I don't know how to get started. I want it to know about my hospital details. Also, When asked &amp;quot;Who are you?&amp;quot; It must say &amp;quot;I am a Chatbot of XYZ Hospital&amp;quot; rather than saying about the base model. Can someone tell me how to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bruce_spixky"&gt; /u/Bruce_spixky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaqr7x/how_to_fine_tune_a_llm_to_give_it_a_persona/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaqr7x/how_to_fine_tune_a_llm_to_give_it_a_persona/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaqr7x/how_to_fine_tune_a_llm_to_give_it_a_persona/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar5du</id>
    <title>I made a multi-provider AI coding agent</title>
    <updated>2025-10-19T14:33:16+00:00</updated>
    <author>
      <name>/u/West-Bottle9609</name>
      <uri>https://old.reddit.com/user/West-Bottle9609</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been building Binharic, an open-source AI coding assistant that runs in the terminal. It's entirely written in TypeScript and uses the AI SDK from Vercel for its agentic logic, including tool use and workflow management.&lt;/p&gt; &lt;p&gt;It supports models from OpenAI, Google, Anthropic, and local ones through Ollama. It has a built-in keyword-based RAG pipeline and can use external tools via the MCP. Many things about the agent are customizable, including its personality. The default persona is a Tech-Priest (from Warhammer 40k), but this can be changed.&lt;/p&gt; &lt;p&gt;Project's GitHub repo: &lt;a href="https://github.com/CogitatorTech/binharic-cli"&gt;https://github.com/CogitatorTech/binharic-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Bottle9609"&gt; /u/West-Bottle9609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa29de</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.2.0</title>
    <updated>2025-10-18T17:47:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt; &lt;img alt="Drummer's Cydonia and Magidonia 24B v4.2.0" src="https://external-preview.redd.it/texRxv_iJ0Ni14pBUMNg-YEbpRERebh0ufaJ753mjSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab199da434abf76101631f6569f8bb80838d47f9" title="Drummer's Cydonia and Magidonia 24B v4.2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magidonia is Cydonia using Magistral 2509 base.&lt;/p&gt; &lt;p&gt;Magidonia variant: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cydonia (Small 3.2) variant: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4.2.0 is an upgrade from 4.1 in regards to creativity. Enjoy!&lt;/p&gt; &lt;p&gt;Does anyone have a base to recommend for finetuning? Waiting for GLM Air 4.6 to come out :^)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;By the way, Huggingface has restricted storage in my account and I'm having a harder time doing my open-source work for the community. I'll be all out of space after a few days of work thanks to their storage restriction. &lt;/p&gt; &lt;p&gt;I tried contacting them via [&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;) but they told me to make my case to [&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;) . I haven't received a response from &lt;em&gt;that&lt;/em&gt; team yet. Other employees I've reached out to recommended that I pay around $200 / mo to get the storage I need, I think.&lt;/p&gt; &lt;p&gt;At this point I believe they're not interested in giving me an exception. I got bundled up with those who upload 1T models, I guess? I'm not sure what to do next, but I might have to start deleting models. Let me know if you guys have any ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaafyq</id>
    <title>3 3090's, room for one more?</title>
    <updated>2025-10-18T23:16:49+00:00</updated>
    <author>
      <name>/u/BusinessBookkeeper63</name>
      <uri>https://old.reddit.com/user/BusinessBookkeeper63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt; &lt;img alt="3 3090's, room for one more?" src="https://preview.redd.it/4wabpxvjcyvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abcb88de7be3c5396eac5805691a1be8d3309133" title="3 3090's, room for one more?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I am currently running 3 3090's and was thinking of adding one more. But as you can see, my case Thermaltake CTE750 Air has some free space, but not sure if it can fit another 3090.&lt;/p&gt; &lt;p&gt;I know, I know, I should have had a server rack but I was looking for a Local AI + relatively decent looking case, so this is what I landed on. The CTE 750 is big enough for 3 3090's, but not sure if I should be doing 4 given temps inside a closed case is probably going to rise quick. The third 3090 needs a custom mount and sits on the side of the case in this picture, but it rests on the intake fans and I have screwed the standing with 3 screws. I have no idea, where I could fit the 4th.&lt;/p&gt; &lt;p&gt;Any suggestions on how I could do 4 3090;s in this case or if anyone has done this before?&lt;/p&gt; &lt;p&gt;Also looking for suggestions on my cooling. Currently it has intake from bottom, front, back and sides and outtake on top only. This is somewhat based on the CTE design, but open to other suggestions. Another option, is to eventually do water cooling to save on some space and keep things cooler, but that's a project kept for December.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BusinessBookkeeper63"&gt; /u/BusinessBookkeeper63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4wabpxvjcyvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T23:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa8klx</id>
    <title>Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models</title>
    <updated>2025-10-18T21:54:34+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt; &lt;img alt="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" src="https://external-preview.redd.it/ZnUwMzR3NXV5eHZmMTNcUS_6HPKChleMDpJ0qQU2p3V675TK2MrxaOnhSll8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=568275d60bed24ec215f8d24a426e506eab3f5d7" title="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jak8lx5uyxvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T21:54:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar69q</id>
    <title>Any resource to understand LLM fine tuning/inference at a medium level to learn about temperature, quanitzation, loss functions, gpu setup?</title>
    <updated>2025-10-19T14:34:15+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any resource you found helpful to learn LLM fine tuning at a medium level so. i can start tinkering by knowing what's happening behind the scenes? Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oas6ki</id>
    <title>Same banchmark, diff results?</title>
    <updated>2025-10-19T15:15:51+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oas6ki/same_banchmark_diff_results/"&gt; &lt;img alt="Same banchmark, diff results?" src="https://a.thumbs.redditmedia.com/t3yYkcIIgvIwZhhOflvRUt-lpf4cslCOMDzxbTUj8T8.jpg" title="Same banchmark, diff results?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted so see which model performs better in benches, ring mini 2.0 or gpt oss 20b (high). So, i searched for a direct comparison. I couldn't find it though, but what i did find was more interesting.&lt;/p&gt; &lt;p&gt;The hugging face card for ring mini 2.0 shows a couple of benchmarks. Benchmarks of ring mini 2.0 vs gpt oss 20b (medium) vs qwen3 8b thinking. So i thought that this model (ring mini 2.0) aint that great coz they were comparing it with gpt oss 20b set to medium thinking budget (not high thinking budget) and a model half the size of ring mini 2.0 (qwen3 8b thinking).&lt;/p&gt; &lt;p&gt;So i looked for benchmarks of gpt oss 20b (high), and i found this:&lt;/p&gt; &lt;p&gt;Gpt oss 20b (medium) scorers 73.33 in AIME 25 (ring mini 2.0's model card) Gpt oss 20b (high) scores only 62 in AIME 25 (artificial intelligence analysis)&lt;/p&gt; &lt;p&gt;Gpt oss 20b (medium) scorers 65.53 in GPQA Diamond (ring mini 2.0's model card) Gpt oss 20b (high) scorers only 62 in GPQA Diamond (artificial intelligence analysis)&lt;/p&gt; &lt;p&gt;So, my questions are:&lt;/p&gt; &lt;p&gt;1)Are these inconsistencies coz of faulty benchmarking or coz gpt oss 20b (medium) is actually better than gpt oss 20b (high) in some cases?&lt;/p&gt; &lt;p&gt;2)Which one is actually better, ring mini 2.0 or gpt oss 20b (high).&lt;/p&gt; &lt;p&gt;If there is a direct comparison than please share it.&lt;/p&gt; &lt;p&gt;[Unsessary coz this is reasonable, high outperforming medium:&lt;/p&gt; &lt;p&gt;Gpt oss 20b (medium) scorers 54.90 in LiveCodeBench (ring mini 2.0's model card) Gpt oss 20b (high) scores 57 in LiveCodeBench (artificial intelligence analysis)]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oas6ki"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oas6ki/same_banchmark_diff_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oas6ki/same_banchmark_diff_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T15:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oat6fh</id>
    <title>Best Current Model for Programming?</title>
    <updated>2025-10-19T15:55:20+00:00</updated>
    <author>
      <name>/u/MurazakiUsagi</name>
      <uri>https://old.reddit.com/user/MurazakiUsagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title says it all. I'm looking to work with Rust, C/C++, Python and Assembly. &lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MurazakiUsagi"&gt; /u/MurazakiUsagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T15:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oapgtj</id>
    <title>I want to have a local llm server for my house - just focused on coding assistant - what would be a reasonable spec for that?</title>
    <updated>2025-10-19T13:19:49+00:00</updated>
    <author>
      <name>/u/gameguy56</name>
      <uri>https://old.reddit.com/user/gameguy56</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't need and am not interested in video/image generation - just want something to work with me on coding stuff. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gameguy56"&gt; /u/gameguy56 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T13:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xiza</id>
    <title>dgx, it's useless , High latency</title>
    <updated>2025-10-18T14:38:06+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt; &lt;img alt="dgx, it's useless , High latency" src="https://preview.redd.it/wwroq3nbtvvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d419416ab7812d4f7c564531795007c015a4c85f" title="dgx, it's useless , High latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ahmad posted a tweet where DGX latency is high : &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19"&gt;https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwroq3nbtvvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oafumx</id>
    <title>Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device</title>
    <updated>2025-10-19T03:51:17+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt; &lt;img alt="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" src="https://external-preview.redd.it/1REAPLhpZknr_BaLbzQgHufo9VWmTuOWut1-PIVgTuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3c98ea4dc27d76e781b86ebeda6b0c583cc503d" title="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T03:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa98jf</id>
    <title>Made a website to track 348 benchmarks across 188 models.</title>
    <updated>2025-10-18T22:22:42+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt; &lt;img alt="Made a website to track 348 benchmarks across 188 models." src="https://preview.redd.it/omjxzqi82yvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df31289ac6e45db697b70c3a9add3e087585f736" title="Made a website to track 348 benchmarks across 188 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been building a website from a while ago in which we track the benchmark results from the official papers / model cards that the labs publish. &lt;/p&gt; &lt;p&gt;I thought it would be interesting to compile everything in one place to fill in the gaps on each model release.&lt;br /&gt; All the data is open in Github and all scores have references to the original posts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-stats.com/benchmarks"&gt;https://llm-stats.com/benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to provide candid feedback. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**We don't think this is the best approach yet**. We're now building a way to replicate the results from the most interesting and useful benchmarks, but we understand that most of them haven't been created yet.&lt;/p&gt; &lt;p&gt;Current benchmarks are too simple and are not testing real capabilities. We're looking to build interesting, real world, independent benchmarks with held out data, but that can be easy to reproduce and extend.&lt;/p&gt; &lt;p&gt;Another thing we're currently doing is benchmarking across different inference providers to monitor and detect changes in quality of their service.&lt;/p&gt; &lt;p&gt;We're currently giving out up to $1k to people that want to explore ideas about new benchmarks / environments. Dm me for more information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/omjxzqi82yvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakrdm</id>
    <title>Drop your underrated models you run LOCALLY</title>
    <updated>2025-10-19T08:52:22+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preferably within the 0.2b -32b range, or MoEs up to 140b&lt;/p&gt; &lt;p&gt;I‚Äôm on a LLM downloading spree, and wanna fill up a 2tb SSD with them. &lt;/p&gt; &lt;p&gt;Can be any use case. Just make sure to mention the use case too &lt;/p&gt; &lt;p&gt;Thank you ‚úåÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahpmx</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:38:21+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt; &lt;img alt="When you have little money but want to run big models" src="https://a.thumbs.redditmedia.com/vj3j4Vjr082yd6wYwhFLXQpEt2Zp3s7yg7spOglJoq8.jpg" title="When you have little money but want to run big models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live in India. Everything is expensive. Importers want hefty margin. Government want hefty tax. Rtx 6000 96gb which is possible to get for 7-8k usd in USA is impossible to find even for 11 lakhs(12-13k usd) in India. So we have a couple of friends 1) Juggad 2) Olx ( indian craigslists) 3) Other similar p2p sites like fb marketplace.&lt;/p&gt; &lt;p&gt;Let me show you what I built. 1) Dell T7910 - it has 7 pci slots. I can only get 5 to work. Found it on fb mp with 256 gb ddr4 2) 5 * 3090 from olx 3) 5 pci raisers amazon. These are hard to find for cheap. 4) 1300 watt additional power supply &lt;/p&gt; &lt;p&gt;There are only 4*3090 in this build 5th slot I am using for nvme extension.&lt;/p&gt; &lt;p&gt;Total cost for this build of 96gb vram is around 3.25 lakhs. ( Around 4.6k usd) This post is just for reference for those who are in a similar boat. Please understand there is a lot of difference between planning and execution. Keep +1 lakhs in hand for things that can go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oamo0k</id>
    <title>Gemma 4</title>
    <updated>2025-10-19T10:53:39+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People are very very excited about the release of gemini 3.0 including me, but im more excited in the gemma family of models since they are based on gemini models and on top of that are open-sourced. And simce Gemini 3.0 is groundbreaking (apparently, like the pelican svg, robot svg, xbox svg, os etc tests), I am very curious about how will the gemma 4 models perform. And also, gemma 4 is going to be a big leap compared to gemma 3 coz It was based on gemini 2.0, not 2.5. So we are getting 2 genarational leaps!&lt;/p&gt; &lt;p&gt;When it will be released??&lt;/p&gt; &lt;p&gt;Gemma 1 was based on gemini 1 and was released ~1-2 months after gemini&lt;/p&gt; &lt;p&gt;Gemma 2 was based on gemini 1.5 and was released ~4 months after gemini 1.5&lt;/p&gt; &lt;p&gt;Gemma 3 was based on gemini 2 and was released ~1-2 months after gemini 2.0&lt;/p&gt; &lt;p&gt;So Gemma 4 might be released ~1-2 months after gemini 3??? Maybe???&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T10:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready üéâ</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready üéâ" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready üéâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;a href="https://www.youtube.com/@stanfordonline/videos"&gt;https://www.youtube.com/@stanfordonline/videos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
