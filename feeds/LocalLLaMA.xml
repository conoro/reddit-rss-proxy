<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-18T13:47:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ppj35l</id>
    <title>Has anyone done extensive testing with reap releases?</title>
    <updated>2025-12-18T05:40:36+00:00</updated>
    <author>
      <name>/u/SillyLilBear</name>
      <uri>https://old.reddit.com/user/SillyLilBear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have only done some basic testing, but I am curious if anyone has done any extensive testing of reaped q4 and q8 releases vs non-reaped versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SillyLilBear"&gt; /u/SillyLilBear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppj35l/has_anyone_done_extensive_testing_with_reap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppj35l/has_anyone_done_extensive_testing_with_reap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppj35l/has_anyone_done_extensive_testing_with_reap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T05:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pplvzz</id>
    <title>[Project] I built a local "System 2" VLM pipeline to mine Autonomous Driving data on a single RTX 3090 (No Cloud APIs). Beats CLIP recall by ~50%.</title>
    <updated>2025-12-18T08:36:42+00:00</updated>
    <author>
      <name>/u/Pale_Location_373</name>
      <uri>https://old.reddit.com/user/Pale_Location_373</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm an independent researcher working on Autonomous Vehicles. I wanted to solve the &amp;quot;Dark Data&amp;quot; problem‚Äîwe have petabytes of driving logs, but finding the weird edge cases (e.g., a wheelchair on the road, sensor glare, passive construction zones) is incredibly hard.&lt;/p&gt; &lt;p&gt;Standard methods use metadata tags (too vague) or CLIP embeddings (spatial blindness). Sending petabytes of video to GPT-4V is impossible due to cost and privacy.&lt;/p&gt; &lt;p&gt;So, I built &lt;strong&gt;Semantic-Drive&lt;/strong&gt;: A local-first, neuro-symbolic data mining engine that runs entirely on consumer hardware (tested on an RTX 3090).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture (&amp;quot;System 2&amp;quot; Inference):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of just asking a VLM to &amp;quot;describe the image,&amp;quot; I implemented a &lt;strong&gt;Judge-Scout&lt;/strong&gt; architecture inspired by recent reasoning models (o1):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Symbolic Grounding (The Eye):&lt;/strong&gt; I use &lt;strong&gt;YOLO-E&lt;/strong&gt; to extract a high-recall text inventory of objects. This is injected into the VLM's context window as a hard constraint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cognitive Analysis (The Scouts):&lt;/strong&gt; I run quantized VLMs (&lt;strong&gt;Qwen3-VL-30B-A3B-Thinking, Gemma-3-27B-IT,&lt;/strong&gt; and &lt;strong&gt;Kimi-VL-A3B-Thinking-2506&lt;/strong&gt;) via &lt;em&gt;llama.cpp&lt;/em&gt;. They perform a Chain-of-Thought &amp;quot;&lt;em&gt;forensic analysis&lt;/em&gt;&amp;quot; to verify if the YOLO objects are actual hazards or just artifacts (like a poster of a person).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference-Time Consensus (The Judge):&lt;/strong&gt; A local &lt;strong&gt;Ministral-3-14B-Instruct-2512&lt;/strong&gt; aggregates reports from multiple scouts. It uses an &lt;strong&gt;Explicit Outcome Reward Model (ORM),&lt;/strong&gt; a Python script that scores generations based on YOLO consistency, to perform a &lt;strong&gt;Best-of-N&lt;/strong&gt; search.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Results (Benchmarked on nuScenes):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recall:&lt;/strong&gt; 0.966 (vs 0.475 for CLIP ViT-L/14).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hallucination:&lt;/strong&gt; Reduced Risk Assessment Error by &lt;strong&gt;51%&lt;/strong&gt; compared to a raw zero-shot VLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; ~$0.85 per 1k frames (Energy) vs ~$30.00 for GPT-4o.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Inference:&lt;/strong&gt; `llama.cpp` server (Dockerized).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; Q4_K_M GGUFs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI:&lt;/strong&gt; Streamlit (for human-in-the-loop verification).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôve open-sourced the whole thing, including the Docker setup and a &amp;quot;Gold Set&amp;quot; benchmark for long-tail mining.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/AntonioAlgaida/Semantic-Drive"&gt;https://github.com/AntonioAlgaida/Semantic-Drive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo (HF Space):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/agnprz/Semantic-Drive-Explorer"&gt;https://huggingface.co/spaces/agnprz/Semantic-Drive-Explorer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper (ArXiv):&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.12012"&gt;https://arxiv.org/abs/2512.12012&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the prompt engineering or the local &amp;quot;System 2&amp;quot; implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pale_Location_373"&gt; /u/Pale_Location_373 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T08:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppqkix</id>
    <title>I got tired of guessing which model to use, so I built this</title>
    <updated>2025-12-18T13:17:13+00:00</updated>
    <author>
      <name>/u/Neat_Confidence_4166</name>
      <uri>https://old.reddit.com/user/Neat_Confidence_4166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqkix/i_got_tired_of_guessing_which_model_to_use_so_i/"&gt; &lt;img alt="I got tired of guessing which model to use, so I built this" src="https://external-preview.redd.it/mnyW6zrW8cPAdQDiTnfw5UBdcY8DdHmNzFbWcTSpQw8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6b2f12cff6e2d053484698505d1a985aa43ff80" title="I got tired of guessing which model to use, so I built this" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on a project called &lt;a href="https://modelator.ai/"&gt;modelator.ai&lt;/a&gt;. It helps you figure out which model actually works best for &lt;em&gt;your&lt;/em&gt; specific use case, creates regression tests to notify you if it starts performing worse (or new models perform better!) and can even create endpoints in the app that allows you to hot swap out models or fine tune parameters based on future test results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A few months ago, I had to build an AI parsing product and had absolutely the worst time trying to pick a model to use. I had a bunch of examples that I KNEW the output I expected and I was stuck manually testing them one at a time across models. I'd just guess based on a few manual tests and painstakingly compare outputs by eye. Then a new model drops, benchmarks look incredible, I'd swap it into my app, and it performs worse on my actual task.&lt;/p&gt; &lt;p&gt;So I built an internal tool that enables you to create a test suite for structured output! (I've since been working on unstructured output as well) All you need to do is simply put your inputs and expected outputs in then it spits out a score, cool visualizations and lets you know which model performs best for your use case. You can also select your preferences across accuracy, latency and cost to get new weighted scores across models. Scoring uses a combination of an AI judge (fine tuned OpenAI model), semantic similarity via embeddings, and algorithmic scoring with various techniques ultimately providing a 0-100 accuracy score.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create test suites against 30ish models across Anthropic, OpenAI, Google, Mistral, Groq, Deepseek (hoping to add more but some of them are $$ just to get access to)&lt;/li&gt; &lt;li&gt;Schematized and unschematized support&lt;/li&gt; &lt;li&gt;Turn your best performing model of choice into an endpoint directly in the app&lt;/li&gt; &lt;li&gt;Create regression tests that notify you if something is off like model drift or if a new model is outperforming yours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;On pricing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can bring your own &lt;strong&gt;API keys and use most of it for free&lt;/strong&gt;! There's a Pro tier if you want to use platform keys and a few more features that use more infra and token costs. I ended up racking up a few hundred dollars in infra and token costs while building this thing so unfortunately can't make it completely free.&lt;/p&gt; &lt;p&gt;Definitely still in beta, so would love any feedback you guys have and if this is something anyone would actually want to use.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neat_Confidence_4166"&gt; /u/Neat_Confidence_4166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://modelator.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqkix/i_got_tired_of_guessing_which_model_to_use_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqkix/i_got_tired_of_guessing_which_model_to_use_so_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2j60</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!</title>
    <updated>2025-12-17T17:29:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 20+ iterations, 3 close calls, we've finally come to a release. The best Cydonia so far. At least that's what the testers at Beaver have been saying.&lt;/p&gt; &lt;p&gt;Peak Cydonia! Served by yours truly.&lt;/p&gt; &lt;p&gt;Small 3.2: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Magistral 1.2: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Most prefer Magidonia, but they're both pretty good!)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;To my patrons,&lt;/p&gt; &lt;p&gt;Earlier this week, I had a difficult choice to make. Thanks to your support, I get to enjoy the freedom you've granted me. Thank you for giving me strength to pursue this journey. I will continue dishing out the best tunes possible for you, truly.&lt;/p&gt; &lt;p&gt;- Drummer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppposw</id>
    <title>NobodyWho: the simplest way to run local LLMs in python</title>
    <updated>2025-12-18T12:33:10+00:00</updated>
    <author>
      <name>/u/ex-ex-pat</name>
      <uri>https://old.reddit.com/user/ex-ex-pat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppposw/nobodywho_the_simplest_way_to_run_local_llms_in/"&gt; &lt;img alt="NobodyWho: the simplest way to run local LLMs in python" src="https://external-preview.redd.it/k_VaO9xVzDs6NTs0GJUhwW7HFwfE1xcIDpypCoxpI_M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2b5e9501263e92458557675d4ef8cefc191e067" title="NobodyWho: the simplest way to run local LLMs in python" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's an ergonomic high-level python library on top of llama.cpp&lt;/p&gt; &lt;p&gt;We add a bunch of need-to-have features on top of libllama.a, to make it much easier to build local LLM applications with GPU inference:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU acceleration with Vulkan (or Metal on MacOS): skip wasting time with pytorch/cuda&lt;/li&gt; &lt;li&gt;threaded execution with an async API, to avoid blocking the main thread for UI&lt;/li&gt; &lt;li&gt;simple tool calling with normal functions: avoid the boilerplate of parsing tool call messages&lt;/li&gt; &lt;li&gt;constrained generation for the parameter types of your tool, to guarantee correct tool calling every time&lt;/li&gt; &lt;li&gt;actually using the upstream chat template from the GGUF file w/ minijinja, giving much improved accuracy compared to the chat template approximations in libllama.&lt;/li&gt; &lt;li&gt;pre-built wheels for Windows, MacOS and Linux, with support for hardware acceleration built-in. Just `pip install` and that's it.&lt;/li&gt; &lt;li&gt;good use of SIMD instructions when doing CPU inference&lt;/li&gt; &lt;li&gt;automatic tokenization: only deal with strings&lt;/li&gt; &lt;li&gt;streaming with normal iterators (async or blocking)&lt;/li&gt; &lt;li&gt;clean context-shifting along message boundaries: avoid crashing on OOM, and avoid borked half-sentences like llama-server does&lt;/li&gt; &lt;li&gt;prefix caching built-in: avoid re-reading old messages on each new generation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's an example of an interactive, streaming, terminal chat interface with NobodyWho:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from nobodywho import Chat, TokenStream chat = Chat(&amp;quot;./path/to/your/model.gguf&amp;quot;) while True: prompt = input(&amp;quot;Enter your prompt: &amp;quot;) response: TokenStream = chat.ask(prompt) for token in response: print(token, end=&amp;quot;&amp;quot;, flush=True) print() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check it out on github: &lt;a href="https://github.com/nobodywho-ooo/nobodywho"&gt;https://github.com/nobodywho-ooo/nobodywho&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ex-ex-pat"&gt; /u/ex-ex-pat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nobodywho-ooo/nobodywho"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppposw/nobodywho_the_simplest_way_to_run_local_llms_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppposw/nobodywho_the_simplest_way_to_run_local_llms_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T12:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pppxec</id>
    <title>What is the real deal with MI50 ?</title>
    <updated>2025-12-18T12:45:47+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've seen MI50 showing up literally everywhere for acceptable prices, but nobody seem to mention them anymore, ChatGPT says:&lt;/p&gt; &lt;p&gt;‚ÄúWorth getting‚Äù vs other 32GB options (the real trade)&lt;/p&gt; &lt;p&gt;The MI50‚Äôs big upside is cheap used 32GB HBM2 + very high bandwidth for memory-bound stuff. &lt;/p&gt; &lt;p&gt;The MI50‚Äôs big downside (and it‚Äôs not small): software support risk.&lt;/p&gt; &lt;p&gt;AMD groups MI50 under gfx906, which entered maintenance mode; ROCm 5.7 was the last ‚Äúfully supported‚Äù release for gfx906, and current ROCm support tables flag gfx906 as not supported. That means you often end up pinning older ROCm, living with quirks, and accepting breakage risk with newer frameworks.&lt;/p&gt; &lt;p&gt;So are those guys obsoleted and that's why are all over the place, or are they still worth buying for inference, fine-tuning and training ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pppxec/what_is_the_real_deal_with_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pppxec/what_is_the_real_deal_with_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pppxec/what_is_the_real_deal_with_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T12:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2rtn</id>
    <title>Nemotron was post-trained to assume humans have reasoning, but they never use it</title>
    <updated>2025-12-17T17:38:58+00:00</updated>
    <author>
      <name>/u/RetiredApostle</name>
      <uri>https://old.reddit.com/user/RetiredApostle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"&gt; &lt;img alt="Nemotron was post-trained to assume humans have reasoning, but they never use it" src="https://preview.redd.it/52423nr8us7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4ca1f0be0378b305c1a58efa1b2f8b99752b5ff" title="Nemotron was post-trained to assume humans have reasoning, but they never use it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetiredApostle"&gt; /u/RetiredApostle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/52423nr8us7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppn7t1</id>
    <title>TIGER: Speech/Cinematic Sound Separation Demo</title>
    <updated>2025-12-18T10:06:14+00:00</updated>
    <author>
      <name>/u/Warm-Professor-9299</name>
      <uri>https://old.reddit.com/user/Warm-Professor-9299</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppn7t1/tiger_speechcinematic_sound_separation_demo/"&gt; &lt;img alt="TIGER: Speech/Cinematic Sound Separation Demo" src="https://external-preview.redd.it/aTM2bmlqNDVzeDdnMcXLO7I6Oh1OfkJnF9rX2m0V5xuhWFP5rjp4KW7s81RB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5becf3159a24e2e869947df70751661db13aa849" title="TIGER: Speech/Cinematic Sound Separation Demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled upon this project that performs really well at separating the BG music, voice and effects from single audio. See for yourself: &lt;a href="https://cslikai.cn/TIGER/"&gt;https://cslikai.cn/TIGER/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Warm-Professor-9299"&gt; /u/Warm-Professor-9299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/amc7d745sx7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppn7t1/tiger_speechcinematic_sound_separation_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppn7t1/tiger_speechcinematic_sound_separation_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnrq5</id>
    <title>Benchmarking AI by making it play a 2D version of Portal! We're building a leaderboard of local LLMs and would love your help</title>
    <updated>2025-12-18T10:41:21+00:00</updated>
    <author>
      <name>/u/Jaxkr</name>
      <uri>https://old.reddit.com/user/Jaxkr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnrq5/benchmarking_ai_by_making_it_play_a_2d_version_of/"&gt; &lt;img alt="Benchmarking AI by making it play a 2D version of Portal! We're building a leaderboard of local LLMs and would love your help" src="https://external-preview.redd.it/eGllMzM4YTd4eDdnMaGojmT3bbZo8yY3-KaCvnapuLu-He8EPgF2CzzXIlwS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c4f279fec3d9815a7b1d416d0073d5b754815d0" title="Benchmarking AI by making it play a 2D version of Portal! We're building a leaderboard of local LLMs and would love your help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We are working on an open source, multiplayer game engine for building environments to train+evaluate AI.&lt;/p&gt; &lt;p&gt;Right now we've mostly focused on testing frontier models, but we want to get the local LLM community involved and benchmark smaller models on these gameplay tasks.&lt;/p&gt; &lt;p&gt;If that sounds interesting to you, check us out at &lt;a href="https://github.com/WorldQL/worldql"&gt;https://github.com/WorldQL/worldql&lt;/a&gt; or &lt;a href="https://discord.gg/nPWVJzZFnP"&gt;join our Discord&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We'd appreciate a star and if you are into running and finetuning models, we'd love your help!&lt;/p&gt; &lt;p&gt;We want to build open source benchmarks and RL environments that are just as good as what the big labs have üòé&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaxkr"&gt; /u/Jaxkr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1n6etx97xx7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnrq5/benchmarking_ai_by_making_it_play_a_2d_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnrq5/benchmarking_ai_by_making_it_play_a_2d_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1porpwd</id>
    <title>Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model</title>
    <updated>2025-12-17T08:49:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt; &lt;img alt="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" src="https://external-preview.redd.it/OXpuN3VqYnE4cTdnMbhg7mfH3BLNBAJzBcqwf-BeiskbYrfqW4XgiIx-FQh0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6475aef4e90b21644bf95a26618a75433c2e08de" title="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Details&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Type:&lt;/strong&gt; Flow-Matching Transformers with Sparse Voxel based 3D VAE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 4 Billion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Single Image&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; 3D Asset &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/microsoft/TRELLIS.2-4B"&gt;https://huggingface.co/microsoft/TRELLIS.2-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo - &lt;a href="https://huggingface.co/spaces/microsoft/TRELLIS.2"&gt;https://huggingface.co/spaces/microsoft/TRELLIS.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post - &lt;a href="https://microsoft.github.io/TRELLIS.2/"&gt;https://microsoft.github.io/TRELLIS.2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g8uco5dq8q7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T08:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnoma</id>
    <title>Qwen3-Coder-REAP mxfp4 quant with custom imatrix dataset</title>
    <updated>2025-12-18T10:36:00+00:00</updated>
    <author>
      <name>/u/spectralyst</name>
      <uri>https://old.reddit.com/user/spectralyst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just posted my first model on huggingface.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spectralyst/Qwen3-Coder-REAP-25B-A3B-MXFP4_MOE-GGUF"&gt;spectralyst/Qwen3-Coder-REAP-25B-A3B-MXFP4_MOE-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's a quant of cerebra's REAP of Qwen3-Coder-30B inspired by the original mxfp4 quant by &lt;a href="https://huggingface.co/noctrex/Qwen3-Coder-REAP-25B-A3B-MXFP4_MOE-GGUF"&gt;noctrex&lt;/a&gt; adding more C/C++ queries to the imatrix dataset while reducing the overall amount of code in the set and adding a bit of math queries to aid with math-based code prompts. The idea is to provide a more balanced calibration with greater emphasis on low-level coding.&lt;/p&gt; &lt;p&gt;From my limited experience, these mxfp4 quants of Qwen3-Coder-REAP-25B are the best coding models that will fit in 16 GB VRAM, although with only 16-24K context. Inference is very fast on Blackwell. Hoping this can prove useful for agentic FIM type stuff.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spectralyst"&gt; /u/spectralyst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnoma/qwen3coderreap_mxfp4_quant_with_custom_imatrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnoma/qwen3coderreap_mxfp4_quant_with_custom_imatrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnoma/qwen3coderreap_mxfp4_quant_with_custom_imatrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We‚Äôll be answering questions live on Thursday, Dec. 18, from 2-3pm PT. Hope to see you there.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppjo5b</id>
    <title>Day 10: 21 Days of Building a Small Language Model: KV Cache</title>
    <updated>2025-12-18T06:14:31+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 10: 21 Days of Building a Small Language Model: KV Cache" src="https://b.thumbs.redditmedia.com/vt9eeanrTHbty0PeG_jN-4PShJ8fGv9qBsfU3Bpopco.jpg" title="Day 10: 21 Days of Building a Small Language Model: KV Cache" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 10 of 21 Days of Building a Small Language Model. The topic for today is the KV cache. Yesterday, we explored multi-head attention and how it allows models to look at sequences from multiple perspectives simultaneously. Today, we'll see why generating text would be impossibly slow without a clever optimization called the Key-Value cache.&lt;/p&gt; &lt;h1&gt;Problem&lt;/h1&gt; &lt;p&gt;To understand why KV cache is necessary, we first need to understand how language models generate text. The process is simple: the model predicts one token at a time, using all previously generated tokens as context.&lt;/p&gt; &lt;p&gt;Let's walk through a simple example. Suppose you prompt the model with: The algorithm processes data&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ketg7dmymw7g1.png?width=1006&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1998bceae61cdc3a85a3c13fd7292dc0f229c280"&gt;https://preview.redd.it/ketg7dmymw7g1.png?width=1006&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1998bceae61cdc3a85a3c13fd7292dc0f229c280&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's what happens step by step:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;First pass&lt;/strong&gt;: The model processes these four tokens through all transformer layers and predicts the next token, say efficiently&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Second pass&lt;/strong&gt;: Now the sequence is. The algorithm processes data efficiently. The model feeds this &lt;em&gt;entire&lt;/em&gt; sequence through all layers again to predict the next token, perhaps by&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Third pass&lt;/strong&gt;: The sequence becomes. The algorithm processes data efficiently by, and this &lt;em&gt;entire&lt;/em&gt; sequence is processed again to predict the next token&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This process can continue for potentially hundreds or thousands of tokens.&lt;/p&gt; &lt;p&gt;Notice something deeply inefficient here: we're repeatedly recomputing attention for all earlier tokens, even though those computations never change.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the first pass, we compute Query (Q), Key (K), and Value (V) vectors for [&amp;quot;The&amp;quot;, &amp;quot;algorithm&amp;quot;, &amp;quot;processes&amp;quot;, &amp;quot;data&amp;quot;]&lt;/li&gt; &lt;li&gt;In the second pass, we recompute Q/K/V for those same four tokens &lt;em&gt;again&lt;/em&gt;, plus &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;In the third pass, we recompute all five previous tokens &lt;em&gt;again&lt;/em&gt;, plus the new one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each iteration repeats 90-99% of the same computation. We're essentially throwing away all the work we did in previous iterations and starting over from scratch.&lt;/p&gt; &lt;p&gt;The problem compounds as sequences grow longer. If you're generating a 1,000-token response:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The first token's attention is computed 1,000 times&lt;/li&gt; &lt;li&gt;The second token's attention is computed 999 times&lt;/li&gt; &lt;li&gt;And so on...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a 100-token sequence, you'd compute Q/K/V a total of 5,050 times (1+2+...+100) when you really only need to do it 100 times (once per token). This massive redundancy is what makes inference slow and expensive without optimization.&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;NOTE:&lt;/strong&gt; KV caching only comes during the inference stage. It does not exist during training or pretraining. The KV cache is purely an inference-time optimization that helps accelerate text generation after the model has been trained. This distinction is critical to understand. The cache is used when the model is generating text, not when it is learning from data.&lt;/p&gt; &lt;h1&gt;Only the last token matters&lt;/h1&gt; &lt;p&gt;Here's something that might not be obvious at first, but changes everything once you see it: when predicting the next token, only the last token's output matters.&lt;/p&gt; &lt;p&gt;Think about what happens at the transformer's output. We get a logits matrix with probability distributions for &lt;em&gt;every&lt;/em&gt; token in the sequence. But for prediction, we only use the last row, the logits for the most recent token.&lt;/p&gt; &lt;p&gt;When processing The algorithm processes data efficiently, we compute logits for all five tokens, but we only care about the logits for efficiently to determine what comes next. The earlier tokens? Their logits get computed and then ignored.&lt;/p&gt; &lt;p&gt;This raises an important question: why not just keep the last token and throw away everything else?&lt;/p&gt; &lt;p&gt;While we only need the last token's logits for prediction, we still need information from all earlier tokens to compute those logits correctly. Remember from Day 9, the attention mechanism needs to look at all previous tokens to create context for the current token.&lt;/p&gt; &lt;p&gt;So we can't simply discard everything. We need a smarter approach: preserve information from earlier tokens in a form that lets us efficiently compute attention for new tokens, without recomputing everything from scratch.&lt;/p&gt; &lt;h1&gt;Solution&lt;/h1&gt; &lt;p&gt;Let's work backward from what we actually need to compute the next token.&lt;/p&gt; &lt;p&gt;To compute the context vector for the latest token (say, &amp;quot;efficiently&amp;quot;), we need:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Attention weights&lt;/strong&gt; for &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value vectors&lt;/strong&gt; for all previous tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And to compute those attention weights, we need:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Query vector&lt;/strong&gt; for &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key vectors&lt;/strong&gt; for all previous tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking at this list reveals an important pattern: we only need all previous key vectors and all previous value vectors. We do NOT need to store previous query vectors. Here's why this distinction matters.&lt;/p&gt; &lt;h1&gt;Why Queries aren't cached&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v15xtcmymw7g1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c629f193faa2f2823f1a17ae906dcc99292fb72"&gt;https://preview.redd.it/v15xtcmymw7g1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c629f193faa2f2823f1a17ae906dcc99292fb72&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the first question that comes to everyone‚Äôs mind. The query vector has a very specific, one time job. It's only used to compute attention weights for the &lt;em&gt;current&lt;/em&gt; token. Once we've done that and combined the value vectors, the query has served its purpose. We never need it again.&lt;/p&gt; &lt;p&gt;Let's trace through what happens with &amp;quot;efficiently&amp;quot;: ‚Ä¢ We compute its query vector to figure out which previous tokens to attend to ‚Ä¢ We compare this query to all the previous keys (from &amp;quot;The&amp;quot;, &amp;quot;algorithm&amp;quot;, &amp;quot;processes&amp;quot;, &amp;quot;data&amp;quot;) ‚Ä¢ We get attention weights and use them to combine the previous value vectors ‚Ä¢ Done. The query is never used again.&lt;/p&gt; &lt;p&gt;When the next token &amp;quot;by&amp;quot; arrives: ‚Ä¢ We'll compute &amp;quot;by&amp;quot;'s NEW query vector for its attention ‚Ä¢ But we WON'T need &amp;quot;efficiently&amp;quot;'s query vector anymore ‚Ä¢ However, we WILL need &amp;quot;efficiently&amp;quot;'s key and value vectors, because &amp;quot;by&amp;quot; needs to attend to &amp;quot;efficiently&amp;quot; and all previous tokens&lt;/p&gt; &lt;p&gt;See the pattern? Each token's query is temporary. But each token's keys and values are permanent. They're needed by every future token.&lt;/p&gt; &lt;p&gt;This is why it's called the KV cache, not the QKV cache.&lt;/p&gt; &lt;p&gt;Here's a helpful mental model: think of the query as asking a question (&amp;quot;What should I pay attention to?&amp;quot;). Once you get your answer, you don't need to ask again. But the keys and values? They're like books in a library. Future tokens will need to look them up, so we keep them around.&lt;/p&gt; &lt;h1&gt;Memory Cost&lt;/h1&gt; &lt;p&gt;While KV cache makes inference dramatically faster, this optimization comes with a significant tradeoff: it requires substantial memory.&lt;/p&gt; &lt;p&gt;The cache must store a key vector and value vector for every layer, every head, and every token in the sequence. These requirements accumulate quickly.&lt;/p&gt; &lt;p&gt;The formula for calculating memory requirements:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;KV Cache Size = layers √ó batch_size √ó num_heads √ó head_dim √ó seq_length √ó 2 √ó 2 Where: ‚Ä¢ First 2: for Keys and Values ‚Ä¢ Second 2: bytes per parameter (FP16 uses 2 bytes) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, let's examine numbers from models to understand the scale of memory requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 1: A 30B Parameter Model&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Layers: 48 ‚Ä¢ Batch size: 128 ‚Ä¢ Total head dimensions: 7,168 ‚Ä¢ Sequence length: 1,024 tokens KV Cache Size = 48 √ó 128 √ó 7,168 √ó 1,024 √ó 2 √ó 2 = ~180 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's 180 GB just for the cache, not even including the model parameters themselves.&lt;/p&gt; &lt;p&gt;For models designed for long contexts, the requirements grow even larger:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 2: A Long Context Model&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Layers: 61 ‚Ä¢ Batch size: 1 ‚Ä¢ Heads: 128 ‚Ä¢ Head dimension: 128 ‚Ä¢ Sequence length: 100,000 tokens KV Cache Size = 61 √ó 1 √ó 128 √ó 128 √ó 100,000 √ó 2 √ó 2 = ~400 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;400 GB represents a massive memory requirement. No single GPU can accommodate this, and even multi-GPU setups face significant challenges.&lt;/p&gt; &lt;p&gt;KV cache memory scales linearly with context length. Doubling the context length doubles the memory requirements, which directly translates to higher costs and fewer requests that can be served in parallel.&lt;/p&gt; &lt;h1&gt;Addressing the Memory Challenge&lt;/h1&gt; &lt;p&gt;The memory constraints of KV cache aren't just theoretical concerns. They're real bottlenecks that have driven significant innovation in several directions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi Query Attention (MQA)&lt;/strong&gt;: What if all attention heads shared one key and one value projection instead of each having its own? Instead of storing H separate key/value vectors per token per layer, you'd store just one that all heads share. Massive memory savings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grouped Query Attention (GQA)&lt;/strong&gt;: A middle ground. Instead of all heads sharing K/V (MQA) or each head having its own (standard multi-head attention), groups of heads share K/V. Better memory than standard attention, more flexibility than MQA.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other Approaches&lt;/strong&gt;: ‚Ä¢ Sparse attention (only attend to relevant tokens) ‚Ä¢ Linear attention (reduce the quadratic complexity) ‚Ä¢ Compression techniques (reduce precision/dimensionality of cached K/V)&lt;/p&gt; &lt;p&gt;All of these innovations address the same fundamental issue: as context length grows, KV cache memory requirements grow proportionally, making very long contexts impractical.&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we uncovered one of the most important optimizations in modern language models. The KV cache is elegant in its simplicity: cache the keys and values for reuse, but skip the queries since they're only needed once.&lt;/p&gt; &lt;p&gt;However, the optimization comes at a cost. The KV cache requires substantial memory that grows with context length. This memory requirement becomes the bottleneck as contexts get longer. The cache solved computational redundancy but created a memory scaling challenge.This tradeoff explains many design decisions in modern language models. Researchers developed MQA, GQA, and other attention variants to address the memory problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T06:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppq8pi</id>
    <title>Z-Image is now the default image model on HuggingChat</title>
    <updated>2025-12-18T13:01:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt; &lt;img alt="Z-Image is now the default image model on HuggingChat" src="https://b.thumbs.redditmedia.com/q04f8-Hq7gSnGXdIq-IW8V70b-2l8sOF10WS2JF_Kks.jpg" title="Z-Image is now the default image model on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Victor M (Hugging Face) on ùïè: &lt;a href="https://x.com/victormustar/status/2001629770329858391?s=20"&gt;https://x.com/victormustar/status/2001629770329858391&lt;/a&gt;&lt;br /&gt; HuggingChat: &lt;a href="https://huggingface.co/chat/"&gt;https://huggingface.co/chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppq8pi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnmca</id>
    <title>AI is great at answers, but terrible at uncertainty and that‚Äôs a bigger problem than hallucinations</title>
    <updated>2025-12-18T10:32:06+00:00</updated>
    <author>
      <name>/u/Mediocre_Common_4126</name>
      <uri>https://old.reddit.com/user/Mediocre_Common_4126</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of the criticism around LLMs focuses on hallucinations, wrong facts, or confidence issues but I think the deeper problem is AI is optimized to sound &lt;em&gt;certain&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In real work, the hardest moments are not when you need an answer. They‚Äôre when you don‚Äôt even know what the right question is yet&lt;/p&gt; &lt;p&gt;The messy parts: half-formed thoughts + contradictory signals + ‚Äúthis feels wrong but I don‚Äôt know why‚Äù backtracking changing your mind mid-way&lt;/p&gt; &lt;p&gt;Humans spend a huge amount of time operating in uncertainty, we explore, we reframe, we circle around the problem&lt;/p&gt; &lt;p&gt;Most training data skips that phase entirely, we feed models clean prompts and polished conclusions, then expect them to handle ambiguity well&lt;/p&gt; &lt;p&gt;That‚Äôs why LLMs often feel impressive but fragile, they jump to conclusions too fast, they don‚Äôt linger in confusion, they optimize for closure, not exploration.&lt;/p&gt; &lt;p&gt;What‚Äôs interesting is that the best human collaborators are the opposite. They slow you down, they ask annoying clarifying questions, they surface blind spots instead of hiding them behind confident language&lt;/p&gt; &lt;p&gt;This made me rethink how AI tools should be built, less ‚Äúgive me the answer‚Äù, more ‚Äúhelp me think without collapsing the space too early‚Äù&lt;/p&gt; &lt;p&gt;Interesting if others have noticed this too. Especially people building tools on top of LLMs or using them for real decision making&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Common_4126"&gt; /u/Mediocre_Common_4126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppntz9</id>
    <title>GLM-V GGUF is out!</title>
    <updated>2025-12-18T10:45:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt; &lt;img alt="GLM-V GGUF is out!" src="https://b.thumbs.redditmedia.com/FZv8pFFPpwEa4qKxoMpu3mE3J-5QIWLQlQCBViK_yvg.jpg" title="GLM-V GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-v"&gt;https://huggingface.co/collections/ggml-org/glm-v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db"&gt;https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp6jhq</id>
    <title>Hey, LocalLLaMa. We need to talk...</title>
    <updated>2025-12-17T20:04:07+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I look on the front page and I see people who have spent time and effort to make something, and they share it willingly. They are getting no upvotes.&lt;/p&gt; &lt;p&gt;We are here because we are &lt;em&gt;local&lt;/em&gt; and we are &lt;em&gt;open source&lt;/em&gt;. Those things &lt;em&gt;depend on people who give us things&lt;/em&gt;, and they don't ask for anything in return, but they &lt;em&gt;need&lt;/em&gt; something in return or they will stop.&lt;/p&gt; &lt;p&gt;Pop your head into the smaller posts where someone is showing work they have done. Give honest and constructive feedback. UPVOTE IT.&lt;/p&gt; &lt;p&gt;The project may be terrible -- encourage them to grow by telling them how they can make it better. &lt;/p&gt; &lt;p&gt;The project may be awesome. They would love to hear how awesome it is. But if you use it, then they would love 100 times more to hear how you use it and how it helps you.&lt;/p&gt; &lt;p&gt;Engage with the people who share their things, and not just with the entertainment. &lt;/p&gt; &lt;p&gt;It take so little effort but it makes so much difference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pper90</id>
    <title>MiraTTS: High quality and fast TTS model</title>
    <updated>2025-12-18T01:55:55+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MiraTTS&lt;/strong&gt; is a high quality LLM based TTS finetune that can generate audio at &lt;strong&gt;100x&lt;/strong&gt; realtime and generate realistic and clear 48khz speech! I heavily optimized it using Lmdeploy and used &lt;a href="https://github.com/ysharma3501/FlashSR"&gt;FlashSR&lt;/a&gt; to enhance the audio.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Incredibly fast: As stated before, over &lt;strong&gt;100x&lt;/strong&gt; realtime!&lt;/li&gt; &lt;li&gt;High quality: Generates realistic and 48khz speech, &lt;strong&gt;much&lt;/strong&gt; clearer then most TTS models and it‚Äôs base model.&lt;/li&gt; &lt;li&gt;Memory efficient: Works with even 6gb vram gpus!&lt;/li&gt; &lt;li&gt;Low latency: Possible latency low as &lt;strong&gt;150ms&lt;/strong&gt;, I have not released code for streaming yet but will release soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basic multilingual versions are already supported, I just need to clean up code. Multispeaker is still in progress, but should come soon. If you have any other issues, I will be happy to fix them.&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/ysharma3501/MiraTTS"&gt;https://github.com/ysharma3501/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model link: &lt;a href="https://huggingface.co/YatharthS/MiraTTS"&gt;https://huggingface.co/YatharthS/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog explaining llm tts models: &lt;a href="https://huggingface.co/blog/YatharthS/llm-tts-models"&gt;https://huggingface.co/blog/YatharthS/llm-tts-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stars/Likes would be appreciated very much, thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T01:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppongx</id>
    <title>Fast on-device Speech-to-text for Home Assistant (open source)</title>
    <updated>2025-12-18T11:34:57+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt; &lt;img alt="Fast on-device Speech-to-text for Home Assistant (open source)" src="https://external-preview.redd.it/6PRNLd3TFMw1DCfYP7618_nVHzwQRPRrDRjMqQg7XGU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff7a166c2a85ced6d24604f32dc307cf599fedf" title="Fast on-device Speech-to-text for Home Assistant (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released &lt;a href="https://github.com/orgs/kroko-ai/repositories"&gt;kroko-onnx-home-assistant &lt;/a&gt; is a &lt;strong&gt;local&lt;/strong&gt; streaming STT pipeline for home assistant.&lt;/p&gt; &lt;p&gt;It's currently just a fork of the excellent &lt;a href="https://github.com/ptbsare/sherpa-onnx-tts-stt"&gt;https://github.com/ptbsare/sherpa-onnx-tts-stt&lt;/a&gt; with support for our models added, hopefully it will be accepted in the main project. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High quality&lt;/li&gt; &lt;li&gt;Real streaming (partial results, low latency)&lt;/li&gt; &lt;li&gt;100% local &amp;amp; privacy-first&lt;/li&gt; &lt;li&gt;optimized for fast CPU inference, even in low resources raspberry pi's&lt;/li&gt; &lt;li&gt;Does not require additional VAD&lt;/li&gt; &lt;li&gt;Home Assistant integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; [&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant%5D()"&gt;https://github.com/kroko-ai/kroko-onnx-home-assistant]()&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to test the model quality before installing: the huggingface models running in the browser is the easiest way: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A big thanks to:&lt;br /&gt; - NaggingDaivy on discord, for the assistance.&lt;br /&gt; - the sherpa-onnx-tts-stt team for adding support for streaming models in record time.&lt;/p&gt; &lt;p&gt;Want us to integrate with your favorite open source project ? Contact us on discord:&lt;br /&gt; &lt;a href="https://discord.gg/TEbfnC7b"&gt;https://discord.gg/TEbfnC7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some releases you may have missed:&lt;br /&gt; - Freewitch Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Asterisk Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Full Asterisk based voicebot running with Kroko streaming models: &lt;a href="https://github.com/hkjarral/Asterisk-AI-Voice-Agent"&gt;https://github.com/hkjarral/Asterisk-AI-Voice-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are still working on the main models, code and documentation as well, but held up a bit with urgent paid work deadlines, more coming there soon too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T11:34:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppm9xm</id>
    <title>NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano</title>
    <updated>2025-12-18T09:03:01+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt; &lt;img alt="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" src="https://external-preview.redd.it/i9rG1D6xcH_2B9JTT5Ak5wKM4ExK483hNq6oNeOkRNo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa9a037e77932298ed68f09b93c42491dd8ab8e0" title="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T09:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy0lb</id>
    <title>Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.</title>
    <updated>2025-12-17T14:33:13+00:00</updated>
    <author>
      <name>/u/themixtergames</name>
      <uri>https://old.reddit.com/user/themixtergames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt; &lt;img alt="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." src="https://external-preview.redd.it/YWpkODI1NDF4cjdnMbxNGAI-puPRf-AP3cgrLxlreCeM4kV742La4OIIHHvj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1dded6d1cdcc956e0916d9926400982637f4d7c" title="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/apple/ml-sharp"&gt;https://github.com/apple/ml-sharp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.10685"&gt;https://arxiv.org/abs/2512.10685&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themixtergames"&gt; /u/themixtergames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2mp7b31xr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T14:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp8vo4</id>
    <title>Nvidia plans heavy cuts to GPU supply in early 2026</title>
    <updated>2025-12-17T21:37:13+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T21:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppl5mw</id>
    <title>Maestro ‚Äì Run AI coding agents autonomously for days (Free/OSS)</title>
    <updated>2025-12-18T07:47:12+00:00</updated>
    <author>
      <name>/u/pedramamini</name>
      <uri>https://old.reddit.com/user/pedramamini</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl5mw/maestro_run_ai_coding_agents_autonomously_for/"&gt; &lt;img alt="Maestro ‚Äì Run AI coding agents autonomously for days (Free/OSS)" src="https://preview.redd.it/6wzh6jbg3x7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98f3e53f3b468ec568a721316f4f89ab00e96544" title="Maestro ‚Äì Run AI coding agents autonomously for days (Free/OSS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing a recent labor of love to the world... Maestro is a cross-platform desktop app for orchestrating your fleet of Al agents. Set them loose on complex tasks, check in from your phone, and let them work while you sleep. Free and open source:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://runmaestro.ai/"&gt;https://runmaestro.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/pedramamini/Maestro"&gt;https://github.com/pedramamini/Maestro&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I strongly prefer interacting with ReAct (reason-act) agents over chat agents. It allows for file-system based memory, tool creation and use, MCP agents, etc. I have so many parallel threads with so many agents that I lose track of them regularly. This was the impetus behind the creation of Maestro. Now all my agents sit side-by-side, each logical thread in its own tab, and keyboard short cuts galore allow me to conduct them all at lighting speed.&lt;/p&gt; &lt;p&gt;The single most powerful feature of the application is the Auto Run capability. Work with Al to generate a series of detailed implementation plans, then execute on them with a fresh context per task, allowing for nonstop uninterrupted execution. The current record is over two days of runtime! Even more powerful, organize multiple Markdown documents into a loop-able Playbook, with one stage creating work for other stages.&lt;/p&gt; &lt;p&gt;Mostly tested on OSX with Claude. Codex and Open Code support was just added today. Please download and send me feedback during your holiday downtime, many thanks in advance.&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;p&gt;-pedram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pedramamini"&gt; /u/pedramamini &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6wzh6jbg3x7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl5mw/maestro_run_ai_coding_agents_autonomously_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl5mw/maestro_run_ai_coding_agents_autonomously_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T07:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
