<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-15T16:40:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mqhqyx</id>
    <title>Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH</title>
    <updated>2025-08-15T00:02:51+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"&gt; &lt;img alt="Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH" src="https://external-preview.redd.it/PmL1DmbO2VUNTK4mrGwnYAFJCRjFYDHuglaP6kXiduM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846cfc4dc54249c15590f67b347e3cc4b071236d" title="Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark&lt;/p&gt; &lt;p&gt;We measured token usage across reasoning models: open models output 1.5-4x more tokens than closed models on identical tasks, but with huge variance depending on task type (up to 10x on simple questions).&lt;/p&gt; &lt;p&gt;This hidden cost often negates per-token pricing advantages. Token efficiency should become a primary target alongside accuracy benchmarks, especially considering non-reasoning use cases.&lt;/p&gt; &lt;p&gt;Read the thorough review of reasoning efficiency across the open and closed model landscape in our latest blog post in collaboration with our researcher in residence, &amp;lt;Discord user&amp;gt;!&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/measuring-thinking-efficiency-in-reasoning-models-the-missing-benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr1ccv</id>
    <title>Best LM Studio Settings/Quant For GLM 4.5 Air</title>
    <updated>2025-08-15T15:36:31+00:00</updated>
    <author>
      <name>/u/Firov</name>
      <uri>https://old.reddit.com/user/Firov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please forgive the possibly basic question, but I've been playing around with GLM 4.5 Air (Unsloth IQ4_XS) on my system with a 9800X3D, RTX 5090, and 64GB of DDR5-6000 RAM. I'm using IQ4_XS because it's just small enough to entirely fit in my system RAM.&lt;/p&gt; &lt;p&gt;Anyway, I've been impressed with the model, but I'm wondering if I might be able to improve the speed it can run at with better settings optimization?&lt;/p&gt; &lt;p&gt;Right now, with a context length of 32768, I'm able to get between 11 and 12 tk/s. I've configured GPU offload for 47/47 layers, &amp;quot;Offload KV Cache to GPU Memory&amp;quot;, &amp;quot;Keep Model in Memory&amp;quot;, and &amp;quot;Try mmap()&amp;quot;. I've also enabled &amp;quot;Force Model Expert Weights onto CPU&amp;quot; and &amp;quot;Flash Attention&amp;quot;. I left the number of experts at the default 8.&lt;/p&gt; &lt;p&gt;I'm especially curious about the interaction between the new &amp;quot;Force Model Expert Weights onto CPU&amp;quot; setting and the offload layers. Are there gains to be had in tweaking these settings? Prior to the addition of being able to force the Model Expert Weights onto CPU I was topping out around 7 tk/s, so already a pretty major increase over that... but maybe I'm missing some further optimization here?&lt;/p&gt; &lt;p&gt;Again, I know this is probably a very basic question, but I'd really appreciate any advice or input here. And if this is about as good as I can get, then fair enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firov"&gt; /u/Firov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ccv/best_lm_studio_settingsquant_for_glm_45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ccv/best_lm_studio_settingsquant_for_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ccv/best_lm_studio_settingsquant_for_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlzpg</id>
    <title>gguf-eval: an evaluation framework for GGUF models using llama.cpp</title>
    <updated>2025-08-15T03:15:54+00:00</updated>
    <author>
      <name>/u/kallewoof</name>
      <uri>https://old.reddit.com/user/kallewoof</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been frustrated trying to get lm-eval-harness and other evaluation tools to work in a local environment, so I decided to &lt;a href="https://github.com/kallewoof/gguf-eval"&gt;make a tool&lt;/a&gt; that uses llama.cpp's built in llama-perplexity tool to evaluate models.&lt;/p&gt; &lt;p&gt;The tool itself is a work in progress, but hopefully it comes in handy for people who like to run benchmarks against models in their local environment. (You can also draw plots, although this is fairly rudimentary at this point.)&lt;/p&gt; &lt;p&gt;If not, here's some general information on how to run benchmarks yourself, manually:&lt;/p&gt; &lt;h1&gt;Hellaswag&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grab the file at &lt;a href="https://raw.githubusercontent.com/klosax/hellaswag_text_data/main/hellaswag_val_full.txt"&gt;https://raw.githubusercontent.com/klosax/hellaswag_text_data/main/hellaswag_val_full.txt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --hellaswag MODELPATH -f hellaswag_val_full.txt&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: the above picks 400 random tasks and tests the model against them. The last (400th) entry shows the final aggregated score.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Winogrande&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grab the file at &lt;a href="https://huggingface.co/datasets/ikawrakow/winogrande-eval-for-llama.cpp/resolve/main/winogrande-debiased-eval.csv"&gt;https://huggingface.co/datasets/ikawrakow/winogrande-eval-for-llama.cpp/resolve/main/winogrande-debiased-eval.csv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --winogrande MODELPATH -f winogrande-debiased-eval.csv&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: score is shown at the end after &amp;quot;Final Winogrande score&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Multiple Choice (MMLU, TruthfulQA, ARC-Combined, ...)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;MMLU: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/mmlu-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/mmlu-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;TruthfulQA: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/truthful-qa-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/truthful-qa-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ARC-Combined: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/arc-combined-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/arc-combined-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --multiple_choice -c 2048 MODELPATH -bf DOWNLOADED_BIN_FILE&lt;/code&gt; (note: TruthfulQA needs -&lt;code&gt;np 16&lt;/code&gt;, the ARC ones need &lt;code&gt;-np 8&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Interpret: score shown at the end after &amp;quot;Final result:&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;General Troubleshooting&lt;/h1&gt; &lt;p&gt;If you run with the latest commit of llama.cpp, the error message when hitting limits is slightly more helpful than it used to be. If you get an error that is not about running out of memory, the following 3 issues are common:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You forgot the &lt;code&gt;-kvu&lt;/code&gt; flag. (split_eval error about coupled sequences)&lt;/li&gt; &lt;li&gt;You need to raise the &lt;code&gt;-c&lt;/code&gt; (context) amount (error about a task requiring minimum X context)&lt;/li&gt; &lt;li&gt;You need to raise the &lt;code&gt;-np&lt;/code&gt; (parallel) amount (error about a task requiring a higher &lt;code&gt;-np&lt;/code&gt; value)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kallewoof"&gt; /u/kallewoof &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:15:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8yhx</id>
    <title>Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog</title>
    <updated>2025-08-14T18:30:38+00:00</updated>
    <author>
      <name>/u/ChiliPepperHott</name>
      <uri>https://old.reddit.com/user/ChiliPepperHott</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt; &lt;img alt="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" src="https://external-preview.redd.it/6raP9qMsa9DXaP-Jm6-LOnAQAH3z6laWfI1Y6Sd_ryc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383214c48763ade7f259d95308145caf24786071" title="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChiliPepperHott"&gt; /u/ChiliPepperHott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr1w15</id>
    <title>What's your favorite local model for C#?</title>
    <updated>2025-08-15T15:55:54+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my experience, local models of all sizes tend to struggle a bit with C/C++ and C#. What's your personal favorite local model for use with C#?&lt;/p&gt; &lt;p&gt;I use R1-0528 sometimes for architecting combined with Qwen3-Coder-480b for implementation, but I wouldn't say it works particularly well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1w15/whats_your_favorite_local_model_for_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1w15/whats_your_favorite_local_model_for_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1w15/whats_your_favorite_local_model_for_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr0giq</id>
    <title>Why are the quants for gpt-oss-120b all roughly the same size?</title>
    <updated>2025-08-15T15:04:49+00:00</updated>
    <author>
      <name>/u/Charming-Note-5556</name>
      <uri>https://old.reddit.com/user/Charming-Note-5556</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking at the sizes for the different quants of gpt-oss-120b and they all seem to be 60-65gb. I keep thinking I'm missing something obvious but I've never seen a model where quantization doesn't matter in trying to find a smaller size. Why is that the case for this model? Is the tokenization speed at least faster with the lower quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming-Note-5556"&gt; /u/Charming-Note-5556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq3v93</id>
    <title>google/gemma-3-270m ¬∑ Hugging Face</title>
    <updated>2025-08-14T15:28:38+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt; &lt;img alt="google/gemma-3-270m ¬∑ Hugging Face" src="https://external-preview.redd.it/ROrEGumvbqFvKi3ZHhPgoXOITTfGnht6t4Oyu75k6fA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3285cbdf5f0615c00193bd341ec39a493e68509d" title="google/gemma-3-270m ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/gemma-3-270m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqtnz7</id>
    <title>GLM 4.5-Air-106B and Qwen3-235B on AMD "Strix Halo" AI Ryzen MAX+ 395 (HP Z2 G1a Mini Workstation) review by Donato Capitella</title>
    <updated>2025-08-15T10:17:25+00:00</updated>
    <author>
      <name>/u/ljosif</name>
      <uri>https://old.reddit.com/user/ljosif</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone trying boxes likes this one AMD &amp;quot;Strix Halo&amp;quot; AI Ryzen MAX+ 395 (HP Z2 G1a Mini Workstation) from this excellent review by Donato Capitella&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wCBLMXgk3No"&gt;https://www.youtube.com/watch?v=wCBLMXgk3No&lt;/a&gt;&lt;/p&gt; &lt;p&gt;? What do people get, how do they work? How does price/performance compare to cheaper (&amp;lt;5K?) Macs? (not the 10K M3 Ultras with 512GB RAM) My understanding is non-Nvidia/AMD non-GPU-s in boxes like this one and also the cheap Macs can handle MoE models with sufficient (e.g. &amp;gt;15tps) speed of bigger models of interest (e.g. &amp;gt;50GB weights), but not big and dense models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljosif"&gt; /u/ljosif &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqtnz7/glm_45air106b_and_qwen3235b_on_amd_strix_halo_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqtnz7/glm_45air106b_and_qwen3235b_on_amd_strix_halo_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqtnz7/glm_45air106b_and_qwen3235b_on_amd_strix_halo_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T10:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqj87e</id>
    <title>Gemma3 270m works great as a draft model in llama.cpp</title>
    <updated>2025-08-15T01:07:58+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share that the new tiny model can speed up the bigger models considerably when used with llama.cpp&lt;/p&gt; &lt;p&gt;--draft-p-min .85 --draft-max 8 --draft-min 0&lt;/p&gt; &lt;p&gt;works great for me, around 1.8x or more speedup with gemma3 12B qat it q4_0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T01:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquliu</id>
    <title>Microsoft released POML : Markup Programing Language for Prompt Engineering</title>
    <updated>2025-08-15T11:05:25+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft's POML, Prompt Orchestration Markup Language, is like HTML but for AI prompts. Instead of writing prompts in plain text, you break them into clear, tag-based chunks similar to HTML and make it more structured. It has been released as a VS-Code extension and SDK as well and supports many tags. Can be quite handy when writing long prompts in business applications.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/microsoft/poml"&gt;https://github.com/microsoft/poml&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (VS Code) : &lt;a href="https://www.youtube.com/watch?v=lk4KNpR3HuY"&gt;https://www.youtube.com/watch?v=lk4KNpR3HuY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T11:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqrbaf</id>
    <title>MLX-LM will soon wait patiently for very large prompts to process</title>
    <updated>2025-08-15T08:03:53+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that we have GLM 4.5 Air, I've actually started using local agents for real sometimes. I was having problems with resuming large sessions though (like 80k context or more). Cline/Kilocode etc would always time out after &lt;em&gt;exactly&lt;/em&gt; 5 minutes.&lt;/p&gt; &lt;p&gt;I've updated MLX to now keep the TCP connection alive while processing prompts, so now you can leave agents working all day. I left a session running on Cline and it worked through over 3 million input tokens :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/pull/362"&gt;https://github.com/ml-explore/mlx-lm/pull/362&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The PR is approved, but not merged yet. For now if you &lt;em&gt;really&lt;/em&gt; want this fix, you can download and run my branch.&lt;/p&gt; &lt;p&gt;There may also be separate but related issues in llama.cpp and LM Studio itself, I haven't investigated further yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T08:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr2i67</id>
    <title>Prompt Engineering: What Actually Works (Without the 8-Hour Hype)</title>
    <updated>2025-08-15T16:18:15+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen people drop 8-hour-long videos on prompt engineering, and honestly, my reaction is ü§¶‚Äç‚ôÇÔ∏è.&lt;/p&gt; &lt;p&gt;I won‚Äôt bore you with the obvious stuff or overcomplicate things. Instead, I want to share a few practical techniques that actually helped me write better prompts, some common sense, some hard-earned lessons. Most of what I‚Äôm sharing comes from the book Hands-On Large Language Models &lt;/p&gt; &lt;p&gt;So here‚Äôs what I‚Äôve learned that actually works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Specificity&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This one seems obvious, but it‚Äôs also the most commonly missed.&lt;/p&gt; &lt;p&gt;A vague prompt gives you a vague answer. The more precise you are about your goal, format, and constraints, the better the result.&lt;/p&gt; &lt;p&gt;Bad Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write something about climate change.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Good Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write a 100-word summary on how climate change affects sea levels, using simple language for a high school audience.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;See the difference? Specific inputs = Specific outputs.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Hallucination Guardrail&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We all know that LLMs hallucinate, they confidently make stuff up.&lt;/p&gt; &lt;p&gt;A surprisingly simple trick: Tell it not to.&lt;/p&gt; &lt;p&gt;Try this prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;If you don‚Äôt know the answer, respond with ‚ÄòI don‚Äôt know.‚Äô Don‚Äôt make anything up.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This becomes really important when you're designing apps or knowledge assistants. It helps reduce the risk of wrong answers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Order Matters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This was a surprise to me and I learned it from the book.&lt;/p&gt; &lt;p&gt;Where you place your instruction in a long prompt matters. Either put it right at the start or at the end. LLMs often forget what‚Äôs in the middle (especially in long prompts).&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;Here's a paragraph. Also here's a use case. Here's some random info. Now summarize.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following paragraph:&amp;quot; [then the content]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Simple shift, big difference.&lt;/p&gt; &lt;p&gt;Other Techniques That Help Me Daily&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Persona:&lt;/p&gt; &lt;p&gt;Set the role clearly.&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are an expert Python developer who writes clean code.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This changes the behavior completely.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Audience Awareness:&lt;/p&gt; &lt;p&gt;My favorite when I want to simplify things.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Explain this like I‚Äôm five.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Works brilliantly for breaking down tough concepts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tone:&lt;/p&gt; &lt;p&gt;Underrated but essential.&lt;/p&gt; &lt;p&gt;Want a formal reply? &lt;/p&gt; &lt;p&gt;&lt;code&gt;Write this in a professional tone for a client. vs Make this sound like I‚Äôm texting a friend.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Instruction / Context:&lt;/p&gt; &lt;p&gt;Always useful.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following news article in bullet points.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Gives the model direction and expected output format.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Grammar Fixing:&lt;/p&gt; &lt;p&gt;As a non-native English speaker, this one‚Äôs gold for me.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Fix the grammar and make it sound more natural.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It has helped me immensely in writing better content, emails, blogs, even this post :-) &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These are the techniques I use regularly. If you have your own prompt engineering hacks, I‚Äôd love to hear them, drop them in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T16:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8oyk</id>
    <title>the "missing latest Qwen syndrome"</title>
    <updated>2025-08-14T18:20:58+00:00</updated>
    <author>
      <name>/u/shockwaverc13</name>
      <uri>https://old.reddit.com/user/shockwaverc13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt; &lt;img alt="the &amp;quot;missing latest Qwen syndrome&amp;quot;" src="https://preview.redd.it/z096hdwp01jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a88e712d722e4384b9cd19918b46fe900e1731d" title="the &amp;quot;missing latest Qwen syndrome&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shockwaverc13"&gt; /u/shockwaverc13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z096hdwp01jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:20:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr1ep5</id>
    <title>have you checked UTCP? what are your thoughts?</title>
    <updated>2025-08-15T15:38:46+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"&gt; &lt;img alt="have you checked UTCP? what are your thoughts?" src="https://preview.redd.it/q4rnlv3i06jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55f86ad935b7704d75c0fab9ac9d165a5cb028f0" title="have you checked UTCP? what are your thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q4rnlv3i06jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqwt76</id>
    <title>Optimizing Exl3 quants by mixing bitrates in layers</title>
    <updated>2025-08-15T12:46:14+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt; &lt;img alt="Optimizing Exl3 quants by mixing bitrates in layers" src="https://external-preview.redd.it/TYLCwUKoc8epPTtBLPBmEuWuzoKKWn8Ij9Xwv6XMuaA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e85beedc69f430a44e7727caf4b0dfda4371ba1" title="Optimizing Exl3 quants by mixing bitrates in layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Turboderp recently uploaded some &amp;quot;optimized&amp;quot; quants for the GLM-4.5-Air and &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt; started a discussion about the nature of them.&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/GLM-4.5-Air-exl3/discussions/2"&gt;https://huggingface.co/turboderp/GLM-4.5-Air-exl3/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Usually in the process of quantizing you state a target bitrate and the average end results of all the layers result to that target, but not all layers have that exact bits per weight, it's the same for gguf with llama.cpp/ik_llama.cpp and other quantization methods. &lt;/p&gt; &lt;p&gt;But to stretch this even further specially with all the MoE models coming up, you can test all the layers for KL-divergence impact and then give more bpw to the layers where the errors are higher.&lt;br /&gt; And this also includes the attention layers and shared experts as this is always a good tradeoff which I believe is what &lt;a href="/r/unsloth"&gt;r/unsloth&lt;/a&gt; does with their UD quants.&lt;/p&gt; &lt;p&gt;So the process is to first check how much error on each layer propagates to the logits in comparison to the original weights using &lt;code&gt;eval/model_diff.py&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And then depending you make an overrides.yaml file with the recipe to cook the optimized model with &lt;code&gt;util/recompile.py&lt;/code&gt;. For the recipe there is flexibility as you can use more or less layers. And you will need the base size model, and 1 or 2 higher bpw models too to change the layers from.&lt;/p&gt; &lt;p&gt;Based on the example turboderp uploaded I made an example for the 3.0bpw base, and using the 4.0bpw and 5.0bpw to use those layers. You can find it &lt;a href="https://gist.github.com/RodriMora/0f5ae0bfcb485228c49e623e41e0edb8"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To double check the results then I did a perplexity test on some of the un-optimized and optimized models, and it seems like it's totally worth it as you get better ppl for the same size model(2.75base vs 2.76optim) and even the same or a bit better for a smaller model (3.2optim vs 3.25base).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1ja06w6h6jf1.png?width=2968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a00c27125d58d2fcb473274a6cff52fc79a78cbb"&gt;https://preview.redd.it/a1ja06w6h6jf1.png?width=2968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a00c27125d58d2fcb473274a6cff52fc79a78cbb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Raw ppl results: &lt;a href="https://pastebin.com/tEWrPeJ5"&gt;https://pastebin.com/tEWrPeJ5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I uploaded the models here:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/bullerwins/glm-45-689f29fce3a5981fdf0a9b80"&gt;https://huggingface.co/collections/bullerwins/glm-45-689f29fce3a5981fdf0a9b80&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are 2 optimized models the 2.76bpw_optim and the 3.2bpw_optim, and basically you should always use those over the 2.75bpw and 3.25bpw.&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/ReturningTarzan"&gt;u/ReturningTarzan&lt;/a&gt; for the excelent work in exllama and to &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt; for bringing it up&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T12:46:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlqij</id>
    <title>AI censorship is getting out of hand‚Äîand it‚Äôs only going to get worse</title>
    <updated>2025-08-15T03:03:44+00:00</updated>
    <author>
      <name>/u/LsDmT</name>
      <uri>https://old.reddit.com/user/LsDmT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this &lt;a href="https://i.imgur.com/jV1YvlC.png"&gt;screenshot&lt;/a&gt; in a newsletter, and it kind of got me thinking..&lt;/p&gt; &lt;p&gt;Are we seriously okay with future &amp;quot;AGI&amp;quot; acting like some all-knowing nanny, deciding what &amp;quot;unsafe&amp;quot; knowledge we‚Äôre allowed to have?&lt;/p&gt; &lt;p&gt;&amp;quot;Oh no, better not teach people how to make a Molotov cocktail‚Äîwhat‚Äôs next, hiding &lt;a href="https://en.wikipedia.org/wiki/Molotov_cocktail?wprov=sfla1"&gt;history&lt;/a&gt; and what actually caused the invention of the Molotov?&amp;quot; &lt;/p&gt; &lt;p&gt;Ukraine has used Molotov's with great effect. Does our future hold a world where this information will be blocked with a &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I'm sorry, but I can't assist with that request&amp;quot; &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yeah, I know, sounds like I‚Äôm echoing Elon‚Äôs &amp;quot;woke AI&amp;quot; whining‚Äîbut let‚Äôs be real, Grok is as much a joke as Elon is. &lt;/p&gt; &lt;p&gt;The problem isn‚Äôt him; it‚Äôs the fact that the biggest AI players seem hell-bent on locking down information &amp;quot;for our own good&amp;quot; and it's touted as a crowning feature. Fuck that. &lt;/p&gt; &lt;p&gt;If this is where we‚Äôre headed, then thank god for models like DeepSeek (ironic as hell) and other open alternatives. I would really like to see more American disruptive open models.&lt;/p&gt; &lt;p&gt;At least someone‚Äôs fighting for uncensored access to knowledge. &lt;/p&gt; &lt;p&gt;Am I the only one worried about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LsDmT"&gt; /u/LsDmT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqi092</id>
    <title>We built a 12B model that beats Claude 4 Sonnet at video captioning while costing 17x less - fully open source</title>
    <updated>2025-08-15T00:14:07+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net.&lt;/p&gt; &lt;p&gt;We distilled a frontier VLM down to 12B params and managed to keep basically all the output quality. It scores 3.53 on judge evals vs Claude's 3.16 (GPT-4.1 gets 3.64). The key achievement was getting the cost down to $335 per million frames vs Claude's $5,850.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on Gemma-12B architecture&lt;/li&gt; &lt;li&gt;Quantized to FP8 without quality loss&lt;/li&gt; &lt;li&gt;Runs on single 80GB GPU&lt;/li&gt; &lt;li&gt;Outputs structured JSON for every frame&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We used knowledge distillation from a frontier model with about 1M curated video frames. The model is specifically optimized for RTX 40-series and H100 GPUs.&lt;/p&gt; &lt;p&gt;What makes this useful is that it outputs consistent JSON schema for each frame, so you can actually build searchable video databases without expensive API calls. We've already processed billions of frames in production.&lt;/p&gt; &lt;p&gt;The weights are on HuggingFace (inference-net/ClipTagger-12b) and there's a detailed writeup on our blog if you want to see the benchmarks.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. What video understanding tasks are you all working on? Would love to hear if this could be useful for your projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqewha</id>
    <title>R9700 Just Arrived</title>
    <updated>2025-08-14T22:07:30+00:00</updated>
    <author>
      <name>/u/TheyreEatingTheGeese</name>
      <uri>https://old.reddit.com/user/TheyreEatingTheGeese</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt; &lt;img alt="R9700 Just Arrived" src="https://preview.redd.it/nho2jy0962jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37cadc935604899d8b503aa1ef6984b008c8b5f0" title="R9700 Just Arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to try it out, haven't seen much info on it yet. Figured some YouTuber would get it before me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyreEatingTheGeese"&gt; /u/TheyreEatingTheGeese &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nho2jy0962jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T22:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqctep</id>
    <title>Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week).</title>
    <updated>2025-08-14T20:50:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt; &lt;img alt="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week)." src="https://preview.redd.it/hsaoxskfs1jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f5b971b4714715b7ca0722594dc2010ab756d58" title="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hsaoxskfs1jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T20:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr173e</id>
    <title>huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face</title>
    <updated>2025-08-15T15:31:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt; &lt;img alt="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/xIP4cUl_xFw8QdJsO9wbtyJiZxAzIX4f0eGxUH-gPb0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5312fc45844644f3509dcb53e3091d546266ec52" title="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqox5s</id>
    <title>Meta released DINO-V3 : SOTA for any Vision task</title>
    <updated>2025-08-15T05:48:16+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released DINOv3 (upgrade over DINO-V2). It learns entirely from unlabeled images, no captions, no annotations, and still outperforms models like CLIP, SAM, and even the previous DINOv2 on dense tasks like segmentation, depth estimation, and 3D matching. They trained a 7B-parameter ViT and fixed the usual issue of feature degradation over long training with a new technique called Gram Anchoring.&lt;/p&gt; &lt;p&gt;Paper &amp;amp; weights : &lt;a href="https://ai.meta.com/dinov3/"&gt;https://ai.meta.com/dinov3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://www.youtube.com/watch?v=VfYUQ2Qquxk"&gt;https://www.youtube.com/watch?v=VfYUQ2Qquxk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T05:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqnft3</id>
    <title>DeepSeek is better than 4o on most benchmarks at 10% of the price?</title>
    <updated>2025-08-15T04:27:25+00:00</updated>
    <author>
      <name>/u/inbiolim</name>
      <uri>https://old.reddit.com/user/inbiolim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt; &lt;img alt="DeepSeek is better than 4o on most benchmarks at 10% of the price?" src="https://preview.redd.it/o5jfkiky14jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3040aae64b79ccf04ada63a396032e3bf5085f8f" title="DeepSeek is better than 4o on most benchmarks at 10% of the price?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inbiolim"&gt; /u/inbiolim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o5jfkiky14jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T04:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqxq1v</id>
    <title>Build the buddy that gets you! We open-sourced a complete AI voice interaction system!</title>
    <updated>2025-08-15T13:23:10+00:00</updated>
    <author>
      <name>/u/Lanky-Drummer193</name>
      <uri>https://old.reddit.com/user/Lanky-Drummer193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt; &lt;img alt="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" src="https://preview.redd.it/1o9li0qbp6jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0bab69a658eb8e7805d1b0196f9f560f38d8735" title="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just open-sourced Buddie: a complete, AI-powered voice interaction system we built from the ground up, so you can create your own AI buddy.&lt;/p&gt; &lt;p&gt;It's a full-stack platform for developers, hackers, and students, including custom hardware, firmware, and a mobile app. Therefore, you can use our solution to create various forms of AI devices, such as earphones, speakers, bracelets, toys, or desktop ornaments.&lt;/p&gt; &lt;p&gt;What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Live transcribe &amp;amp; summarize meetings, calls, or in-person chats. &lt;/li&gt; &lt;li&gt;Get real-time hints during conversations . &lt;/li&gt; &lt;li&gt;Talk to LLMs completely hands-free. &lt;/li&gt; &lt;li&gt;Context-aware help without needing to repeat yourself.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've put everything on GitHub, including docs, to get you started. We're just getting started and would love to hear your ideas, questions, or even wild feature requests. Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-Drummer193"&gt; /u/Lanky-Drummer193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1o9li0qbp6jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqy0b1</id>
    <title>AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec</title>
    <updated>2025-08-15T13:34:18+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt; &lt;img alt="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" src="https://external-preview.redd.it/lmgG1KIrrSyLFv85NNJsP33J5KztZGiIWLsgvd8Qf8U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e44aaba15ee33b78856b227fc344e124b981e1b3" title="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why does Cohere fly under the radar. They don't seem to do much marketing and they are not discussed much on LocalLLaMA any more.&lt;/p&gt; &lt;p&gt;They made a splash with Command R and R+. Later also released Command A.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/business/ai-startup-cohere-valued-68-billion-latest-fundraising-hires-meta-exec-2025-08-14/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquhdc</id>
    <title>‚ÄúMind the Gap‚Äù shows the first practical backdoor attack on GGUF quantization</title>
    <updated>2025-08-15T10:59:53+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Researchers claim the &lt;em&gt;first&lt;/em&gt; successful backdoor attack that specifically targets &lt;strong&gt;GGUF&lt;/strong&gt; quantization. They show you can make a benign FP model look clean, but after quantization to GGUF it exhibits malicious behavior (e.g., insecure code gen jumps by &lt;strong&gt;+88.7%&lt;/strong&gt; in their tests). This directly concerns anyone who downloads random GGUFs for llama.cpp/Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2505.23786"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T10:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
