<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-24T10:38:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oeeqqf</id>
    <title>2x MAX-Q RTX 6000 or workstation</title>
    <updated>2025-10-23T20:48:16+00:00</updated>
    <author>
      <name>/u/Direct_Bodybuilder63</name>
      <uri>https://old.reddit.com/user/Direct_Bodybuilder63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeeqqf/2x_maxq_rtx_6000_or_workstation/"&gt; &lt;img alt="2x MAX-Q RTX 6000 or workstation" src="https://preview.redd.it/06edhj4xbxwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51c87b64c02b5b5acad4f7c54a4c74182d089d12" title="2x MAX-Q RTX 6000 or workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôm currently in the process of buying components for this build. &lt;/p&gt; &lt;p&gt;Everything marked I‚Äôve purchased and everything unmarked I‚Äôm waiting on for whatever reason.&lt;/p&gt; &lt;p&gt;I‚Äôm still a little unsure on two things &lt;/p&gt; &lt;p&gt;1) whether I want the 7000 threadripper versus the 9985 or 9995. 2) whether getting a third card is better than going from say 7975WX to 9985 or 9995. 3) whether cooling requirements for 2 normal RTX 6000s would be OK or if opting for the MAX-Qs is a better idea.&lt;/p&gt; &lt;p&gt;Happy to take any feedback or thoughts thank you &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Bodybuilder63"&gt; /u/Direct_Bodybuilder63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/06edhj4xbxwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeeqqf/2x_maxq_rtx_6000_or_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeeqqf/2x_maxq_rtx_6000_or_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeshaj</id>
    <title>Need Help: I've been breaking my head over structured output form qwen3:14b.</title>
    <updated>2025-10-24T08:42:52+00:00</updated>
    <author>
      <name>/u/No-Translator-1323</name>
      <uri>https://old.reddit.com/user/No-Translator-1323</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to get structured output from qwen3:14b running via ollama. On python side, I'm using Langgraph and Langchain ecosystem.&lt;/p&gt; &lt;p&gt;I have noticed that if I set the `reasoning` parameter to `True`, structured output breaks for some reason, Interesetingly this problem does not happen if I set reasoning to None.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model = ChatOllama(model=&amp;quot;qwen3:14b&amp;quot;, temperature=0, num_ctx=16384, reasoning=True) response = model.with_structured_output(OutptuSchema) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output always has an extra '{' and thus fails the pyadantic parsing.&lt;br /&gt; Output looks like (notice the extra '{' at the beginning.): &lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ { &amp;quot;field1&amp;quot;: &amp;quot;...&amp;quot;, &amp;quot;field2&amp;quot;: &amp;quot;...&amp;quot;, &amp;quot;field3&amp;quot;: &amp;quot;...&amp;quot;, &amp;quot;reasoning&amp;quot;: &amp;quot;...&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any ideas on why this could be happening. I have tried modifying the prompt and get the same results. Is there really no other option than to try another model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Translator-1323"&gt; /u/No-Translator-1323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeshaj/need_help_ive_been_breaking_my_head_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeshaj/need_help_ive_been_breaking_my_head_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeshaj/need_help_ive_been_breaking_my_head_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T08:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe3wfs</id>
    <title>Virus Total integration on Hugging Face</title>
    <updated>2025-10-23T13:54:49+00:00</updated>
    <author>
      <name>/u/McPotates</name>
      <uri>https://old.reddit.com/user/McPotates</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt; &lt;img alt="Virus Total integration on Hugging Face" src="https://external-preview.redd.it/TwKiQ2XM7P28_0o53Sg5het24dh0s2bGVXdozQe9a5g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22b5835e6bc12ff826ab2939382c16ada8b641a6" title="Virus Total integration on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! We've just integrated Virus Total as security scanning partner. You should get a lot more AV scanners working on your files out of the box!&lt;br /&gt; Super happy to have them on board, curious to hear what yall think about this :)&lt;/p&gt; &lt;p&gt;FYI, we don't have all files scanned atm, should expand as more files are moved to xet (which gives us a sha256 out of the box, VT needs it to identify files).&lt;br /&gt; Also, only public files are scanned!&lt;/p&gt; &lt;p&gt;more info here: &lt;a href="https://huggingface.co/blog/virustotal"&gt;https://huggingface.co/blog/virustotal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5r3o1tpq9vwf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49b0cb3f1fc78589e0b8d36eaae8d773515e6101"&gt;https://preview.redd.it/5r3o1tpq9vwf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49b0cb3f1fc78589e0b8d36eaae8d773515e6101&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McPotates"&gt; /u/McPotates &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T13:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe8hjh</id>
    <title>M5 iPad runs 8B-Q4 model.</title>
    <updated>2025-10-23T16:50:14+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"&gt; &lt;img alt="M5 iPad runs 8B-Q4 model." src="https://preview.redd.it/cq5w77gg5wwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb34d2ba73be617d6ecd7bfa852850ba436123a5" title="M5 iPad runs 8B-Q4 model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not too much of a surprise that the new M5 iPad (11&amp;quot; Base model with 12 GB of RAM) will run an 8B Q4 model. Please see the screenshot. I asked it to explain how to solve a Rubik's Cube, and it gave a decent answer and a respectable 23 tokens per second. The app I'm using is called Noema AI, and I like it a lot because you can have both a local model and an endpoint. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cq5w77gg5wwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:50:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oejb8p</id>
    <title>Why is Phi4 considered the best model for structured information extraction?</title>
    <updated>2025-10-24T00:05:17+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;curious, i have read multiple times in this sub that, if you want your output to fit to a structure like json, go. with Phi4, wondering why this is the case&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oejb8p/why_is_phi4_considered_the_best_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oejb8p/why_is_phi4_considered_the_best_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oejb8p/why_is_phi4_considered_the_best_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T00:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oekfvv</id>
    <title>Another OCR Model!</title>
    <updated>2025-10-24T01:00:32+00:00</updated>
    <author>
      <name>/u/grrowb</name>
      <uri>https://old.reddit.com/user/grrowb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on OCR at the moment and I had ChatGPT do a deep research to find me models to use. Its number one recommended model was LightOnOCR. I did a classic &amp;quot;LightOnOCR reddit&amp;quot; search in Google to see what people were saying but I didn't find anything. &lt;/p&gt; &lt;p&gt;Turns out it was released today. &lt;/p&gt; &lt;p&gt;I was able to get it to run on my NVIDIA RTX 3090 with 24GB of VRAM and it could do a page anywhere from 1.5 -&amp;gt; 5 seconds. I didn't do any substantial testing but it seems quite good. &lt;/p&gt; &lt;p&gt;Lots of exciting things in the OCR space lately. &lt;/p&gt; &lt;p&gt;Here's a link to their blog post. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/lightonai/lightonocr"&gt;https://huggingface.co/blog/lightonai/lightonocr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grrowb"&gt; /u/grrowb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oekfvv/another_ocr_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oekfvv/another_ocr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oekfvv/another_ocr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T01:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeqg0k</id>
    <title>An open-source AI co-browser Linux alternative</title>
    <updated>2025-10-24T06:27:19+00:00</updated>
    <author>
      <name>/u/Significant-Skin118</name>
      <uri>https://old.reddit.com/user/Significant-Skin118</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeqg0k/an_opensource_ai_cobrowser_linux_alternative/"&gt; &lt;img alt="An open-source AI co-browser Linux alternative" src="https://external-preview.redd.it/66UdAjJOuMx3rWlJ6VSXgNFUdNa0ngrQM0MoeQQVwX4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9d9273c7f5bb291b4b13807ce911598b2668764" title="An open-source AI co-browser Linux alternative" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, some of you might remember Zenbot, the Podman/Docker-based LLM web browser I posted here a few weeks ago.&lt;/p&gt; &lt;p&gt;Zenbot is now pebkac, and it's almost ready to be your web co-browsing alternative.&lt;/p&gt; &lt;p&gt;I've been hard at work at it. It's vastly improved (and easier to set up!). Check out the readme for a full list of new features. Runs on Podman/Docker.&lt;/p&gt; &lt;p&gt;With OpenAI's Atlas and Perplexity's Comet, it's time Linux had its own Chrome-wrapped web browsing thing. So here it is, free and open-source. Click the link and check out the screenshots.&lt;/p&gt; &lt;p&gt;(This post was written by a human, saved as a draft, and posted by pebkac)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Skin118"&gt; /u/Significant-Skin118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/michaelsoftmd/pebkac-chrome"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeqg0k/an_opensource_ai_cobrowser_linux_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeqg0k/an_opensource_ai_cobrowser_linux_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T06:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oet4dm</id>
    <title>Open WebUI Context Menu</title>
    <updated>2025-10-24T09:25:31+00:00</updated>
    <author>
      <name>/u/united_we_ride</name>
      <uri>https://old.reddit.com/user/united_we_ride</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôve been tinkering with a little Firefox extension I built myself and I‚Äôm finally ready to drop it into the wild. It‚Äôs called Open WebUI Context Menu Extension, and it lets you talk to Open WebUI straight from any page, just select what you want answers for, right click it and ask away!&lt;/p&gt; &lt;p&gt;Think of it like Edge‚Äôs Copilot but with way more knobs you can turn. Here‚Äôs what it does:&lt;/p&gt; &lt;p&gt;Custom context‚Äëmenu items (4 total).&lt;/p&gt; &lt;p&gt;Rename the default ones so they fit your flow.&lt;/p&gt; &lt;p&gt;Separate settings for each item, so one prompt can be super specific while another can be a quick and dirty query.&lt;/p&gt; &lt;p&gt;Export/import your whole config, perfect for sharing or backing up.&lt;/p&gt; &lt;p&gt;I‚Äôve been using it every day in my private branch and it‚Äôs become an essential part of how I do research, get context on the fly, and throw quick questions at Open WebUI. The ability to tweak prompts per item makes it feel like a something useful i think.&lt;/p&gt; &lt;p&gt;It‚Äôs live on AMO, &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/open-webui-context-menu/"&gt;Open WebUI Context Menu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôre curious, give it a spin and let me know what you think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/united_we_ride"&gt; /u/united_we_ride &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oet4dm/open_webui_context_menu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oet4dm/open_webui_context_menu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oet4dm/open_webui_context_menu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T09:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oetfxu</id>
    <title>MoonshotAI/kimi-cli - CLI coding agent from MoonshotAI</title>
    <updated>2025-10-24T09:46:47+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oetfxu/moonshotaikimicli_cli_coding_agent_from_moonshotai/"&gt; &lt;img alt="MoonshotAI/kimi-cli - CLI coding agent from MoonshotAI" src="https://external-preview.redd.it/gCRFDtSNP63oI07JY9GS8NsMz4dKnDZ7jPQdkKhmdHE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcf21d3b55e39a16871cc1ae0ef8860b685f6623" title="MoonshotAI/kimi-cli - CLI coding agent from MoonshotAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MoonshotAI/kimi-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oetfxu/moonshotaikimicli_cli_coding_agent_from_moonshotai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oetfxu/moonshotaikimicli_cli_coding_agent_from_moonshotai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T09:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oebo07</id>
    <title>Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)</title>
    <updated>2025-10-23T18:50:17+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"&gt; &lt;img alt="Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)" src="https://external-preview.redd.it/NDJxZTFsN3lwd3dmMSkQIYjP_oFpJvmih5U0oEGvnjDWhMxIFYeX2zHmhGBL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8f010389c86ee8911fc6841ab3d654b84ceda1a" title="Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to see if Qwen3-VL could handle something simple: counting push-ups. If it can‚Äôt do that, it‚Äôs not ready to be a good trainer.&lt;/p&gt; &lt;p&gt;Overview:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on Gabber (will link repo)&lt;/li&gt; &lt;li&gt;Used Qwen3-VL for vision to tracks body position &amp;amp; reps&lt;/li&gt; &lt;li&gt;Cloned Ronnie Coleman‚Äôs voice for the trainer. That was‚Ä¶ interesting.&lt;/li&gt; &lt;li&gt;Output = count my reps and gimme a ‚ÄúLIGHTWEIGHT BABY‚Äù every once in a while&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Took a lot of tweaking to get accurate rep counts&lt;/li&gt; &lt;li&gt;Some WEIRD voice hallucinations (Ronnie was going off lol)&lt;/li&gt; &lt;li&gt;Timing still a bit off between reps&lt;/li&gt; &lt;li&gt;Seems the model isn‚Äôt quite ready for useful real-time motion analysis or feedback, but it‚Äôs getting there&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pfn5nm7ypwwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T18:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oes8a9</id>
    <title>Qwen3 VL: Is there anyone worried about object detection performance (in production)</title>
    <updated>2025-10-24T08:26:10+00:00</updated>
    <author>
      <name>/u/BackgroundLow3793</name>
      <uri>https://old.reddit.com/user/BackgroundLow3793</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm currently working document parsing where I also care about extracting the images (bounding box) in the document. &lt;/p&gt; &lt;p&gt;I did try `qwen/qwen3-vl-235b-a22b-instruct` it worked better than MstralOCR for some of my test case. &lt;/p&gt; &lt;p&gt;But things make me worried is that, as I try end to end. and my output will be schema object where I have markdown content (include image path markdown), image object contains `bbox_2d`, annotation (description of that image) &lt;/p&gt; &lt;p&gt;Though I surprised that it worked perfect for some test cases, but I really concern. As it's still a generative model, it might be affected by the prompting. &lt;/p&gt; &lt;p&gt;Is this approach too risky for production? Or I should combine with other layout parser tool? Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BackgroundLow3793"&gt; /u/BackgroundLow3793 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes8a9/qwen3_vl_is_there_anyone_worried_about_object/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes8a9/qwen3_vl_is_there_anyone_worried_about_object/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oes8a9/qwen3_vl_is_there_anyone_worried_about_object/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T08:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe13rg</id>
    <title>Qwen3 outperforming bigger LLMs at trading</title>
    <updated>2025-10-23T11:47:37+00:00</updated>
    <author>
      <name>/u/Christosconst</name>
      <uri>https://old.reddit.com/user/Christosconst</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"&gt; &lt;img alt="Qwen3 outperforming bigger LLMs at trading" src="https://preview.redd.it/7i46ukqanuwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2af3d3871761418ac44fa8e43516acb99b51653d" title="Qwen3 outperforming bigger LLMs at trading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Christosconst"&gt; /u/Christosconst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7i46ukqanuwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T11:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe0y11</id>
    <title>I found a perfect coder model for my RTX4090+64GB RAM</title>
    <updated>2025-10-23T11:39:15+00:00</updated>
    <author>
      <name>/u/srigi</name>
      <uri>https://old.reddit.com/user/srigi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disappointed with vanilla Qwen3-coder-30B-A3B, I browsed models at mradermacher. I had a good experience with YOYO models in the past. I stumbled upon &lt;strong&gt;mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;First, I was a little worried that &lt;strong&gt;42B&lt;/strong&gt; won't fit, and offloading MoEs to CPU will result in poor perf. But thankfully, I was wrong.&lt;/p&gt; &lt;p&gt;Somehow this model consumed only about 8GB with &lt;code&gt;--cpu-moe&lt;/code&gt; (keep all Mixture of Experts weights on the CPU) and Q4_K_M, and 32k ctx. So I tuned llama.cpp invocation to fully occupy 24GB of RTX 4090 and put the rest into the CPU/RAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --model Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_K_M.gguf \ --ctx-size 102400 \ --flash-attn on \ --jinja \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --batch-size 1024 \ --ubatch-size 512 \ --n-cpu-moe 28 \ --n-gpu-layers 99 \ --repeat-last-n 192 \ --repeat-penalty 1.05 \ --threads 16 \ --host 0.0.0.0 \ --port 8080 \ --api-key secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With these settings, it eats 23400MB of VRAM and 30GB of RAM. It processes the RooCode's system prompt (around 16k tokens) in around 10s and generates at 44tk/s. With 100k context window.&lt;/p&gt; &lt;p&gt;And the best thing - the RooCode tool-calling is very reliable (vanilla Qwen3-coder failed at this horribly). This model can really code and is fast on a single RTX 4090!&lt;/p&gt; &lt;p&gt;Here is a 1 minute demo of adding a small code-change to medium sized &lt;a href="https://github.com/srigi/type-graphql"&gt;code-base&lt;/a&gt;: &lt;a href="https://i.postimg.cc/cHp8sP9m/Screen-Flow.gif"&gt;https://i.postimg.cc/cHp8sP9m/Screen-Flow.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srigi"&gt; /u/srigi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T11:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oenxtf</id>
    <title>Created Deepseek 3.1 OCR Metal</title>
    <updated>2025-10-24T03:58:29+00:00</updated>
    <author>
      <name>/u/Lyuseefur</name>
      <uri>https://old.reddit.com/user/Lyuseefur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Mac M1 32GB and some OCR needs - just some older pdf I had. I did not see a Metal port so I made one with some help from Claude. &lt;/p&gt; &lt;p&gt;Tested and seemed OK on my Mac with a few documents. Would appreciate any comments. &lt;/p&gt; &lt;p&gt;I‚Äôm in Central time so probably respond to anything in the AM.&lt;/p&gt; &lt;p&gt;Feel free to like / share it‚Äôs my first contribution.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/JeffersonNunn/deepseek-ocr-metal"&gt;https://huggingface.co/JeffersonNunn/deepseek-ocr-metal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Associated Metal Bridge update&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/JeffersonNunn/metal-flash-attention-bridge"&gt;https://huggingface.co/JeffersonNunn/metal-flash-attention-bridge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lyuseefur"&gt; /u/Lyuseefur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oenxtf/created_deepseek_31_ocr_metal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oenxtf/created_deepseek_31_ocr_metal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oenxtf/created_deepseek_31_ocr_metal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T03:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oet55i</id>
    <title>MiniMax-M2 on artificialanalysis.ai ?</title>
    <updated>2025-10-24T09:26:54+00:00</updated>
    <author>
      <name>/u/Leather-Term-30</name>
      <uri>https://old.reddit.com/user/Leather-Term-30</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oet55i/minimaxm2_on_artificialanalysisai/"&gt; &lt;img alt="MiniMax-M2 on artificialanalysis.ai ?" src="https://preview.redd.it/28uj2kbi21xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b25935779b392d85c0c10e80a52653a296304133" title="MiniMax-M2 on artificialanalysis.ai ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed this new model (MiniMax-M2 ) on &lt;a href="http://artificialanalysis.ai"&gt;artificialanalysis.ai&lt;/a&gt; (it outperforms Gemini 2.5 Pro in their benchmarks). However, I didn't see this model elsewhere, does anybody know anything about it?&lt;/p&gt; &lt;p&gt;Edit: as stated by a well-informed user, the following sentence is on MiniMax's website &amp;quot;üöÄ MiniMax-M2 is coming on Oct 27!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Term-30"&gt; /u/Leather-Term-30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/28uj2kbi21xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oet55i/minimaxm2_on_artificialanalysisai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oet55i/minimaxm2_on_artificialanalysisai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T09:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oehpe8</id>
    <title>Our groups GPU server (2x Ai Pro R9700, 2x RX7900 XTX)</title>
    <updated>2025-10-23T22:51:23+00:00</updated>
    <author>
      <name>/u/MrHighVoltage</name>
      <uri>https://old.reddit.com/user/MrHighVoltage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oehpe8/our_groups_gpu_server_2x_ai_pro_r9700_2x_rx7900/"&gt; &lt;img alt="Our groups GPU server (2x Ai Pro R9700, 2x RX7900 XTX)" src="https://preview.redd.it/5cj8651wxxwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7008e9361c4884009889d20384d33074f9ca72fa" title="Our groups GPU server (2x Ai Pro R9700, 2x RX7900 XTX)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says. Due to financial limitations, we had to get the cheapest GPU server possible. It is actually mostly used for simulating complex physical systems with in-house written software.&lt;/p&gt; &lt;p&gt;Just last week we got our hands on two Asrock Creator Ai Pro R9700, which seemed to be sold too early by our vendor. Also, the machines houses two Asrock Creator RX 7900 XTX.&lt;/p&gt; &lt;p&gt;Aside, it's a Ryzen 7960X, 256GB RAM, and some SSDs. Overall a really nice machine at this point, with a total of over 217TFLOP/s of FP32 compute.&lt;/p&gt; &lt;p&gt;Ollama works fine with the R9700, GPT-OSS 120b works quite well using both R9700.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrHighVoltage"&gt; /u/MrHighVoltage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5cj8651wxxwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oehpe8/our_groups_gpu_server_2x_ai_pro_r9700_2x_rx7900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oehpe8/our_groups_gpu_server_2x_ai_pro_r9700_2x_rx7900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T22:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oegejr</id>
    <title>Is this a massive mistake? Super tight fit, 2x 3-slot GPU</title>
    <updated>2025-10-23T21:55:16+00:00</updated>
    <author>
      <name>/u/zhambe</name>
      <uri>https://old.reddit.com/user/zhambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"&gt; &lt;img alt="Is this a massive mistake? Super tight fit, 2x 3-slot GPU" src="https://b.thumbs.redditmedia.com/QzvVO4FHT-NLkbNyOHbOU6NxaPcMMkZoxvfXU1-n4EM.jpg" title="Is this a massive mistake? Super tight fit, 2x 3-slot GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Two 3090s is the sweet spot&amp;quot; they said, &amp;quot;best value&amp;quot; they said. The top card literally touches the bottom one, no breathing room for the fans. This is how the PCIe-16x slots are spaced on the mobo. Not only is thermal a concern, both cards are drooping because they're so heavy.&lt;/p&gt; &lt;p&gt;What's the right thing to do here? Complicate the setup further with a water block + pump + radiator? I can construct some kind of support bracket to remedy the drooping, and a shim to put between the cards to give a few mm of space for airflow. I'm sure there are better ideas...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhambe"&gt; /u/zhambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oegejr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:55:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oepfug</id>
    <title>Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models</title>
    <updated>2025-10-24T05:24:13+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Abstract&lt;/h3&gt; &lt;p&gt;Widespread LLM adoption has introduced characteristic repetitive phraseology, termed &amp;quot;slop,&amp;quot; which degrades output quality and makes AI-generated text immediately recognizable. We present Antislop, a comprehensive framework providing tools to both detect and eliminate these overused patterns. Our approach combines three innovations: (1) The Antislop Sampler, which uses backtracking to suppress unwanted strings at inference time without destroying vocabulary; (2) An automated pipeline that profiles model-specific slop against human baselines and generates training data; (3) Final Token Preference Optimization (FTPO), a novel fine-tuning method that operates on individual tokens, surgically adjusting logits wherever a banned pattern has appeared in an inference trace.&lt;/p&gt; &lt;p&gt;We demonstrate that some slop patterns appear over 1,000x more frequently in LLM output than human text. The Antislop Sampler successfully suppresses 8,000+ patterns while maintaining quality, whereas token banning becomes unusable at just 2,000. Most importantly, FTPO achieves 90% slop reduction while maintaining or improving performance in cross-domain evals including GSM8K, MMLU, and creative writing tasks. In contrast, DPO suffers significant degradation in writing quality and lexical diversity despite achieving weaker suppression.&lt;/p&gt; &lt;p&gt;We release all code and results under MIT license: &lt;a href="https://github.com/sam-paech/auto-antislop"&gt;https://github.com/sam-paech/auto-antislop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2510.15061"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oepfug/antislop_a_comprehensive_framework_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oepfug/antislop_a_comprehensive_framework_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T05:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe7orf</id>
    <title>State of Open OCR models</title>
    <updated>2025-10-23T16:19:39+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks! it's Merve from Hugging Face ü´°&lt;/p&gt; &lt;p&gt;You might have noticed there has been many open OCR models released lately üòÑ they're cheap to run compared to closed ones, some even run on-device&lt;/p&gt; &lt;p&gt;But it's hard to compare them and have a guideline on picking among upcoming ones, so we have broken it down for you in a blog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how to evaluate and pick an OCR model,&lt;/li&gt; &lt;li&gt;a comparison of the latest open-source models,&lt;/li&gt; &lt;li&gt;deployment tips,&lt;/li&gt; &lt;li&gt;and what‚Äôs next beyond basic OCR &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We hope it's useful for you! Let us know what you think: &lt;a href="https://huggingface.co/blog/ocr-open-models"&gt;https://huggingface.co/blog/ocr-open-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeep6p</id>
    <title>What LLM gave you your first "we have GPT-4 at home" moment?</title>
    <updated>2025-10-23T20:46:35+00:00</updated>
    <author>
      <name>/u/Klutzy-Snow8016</name>
      <uri>https://old.reddit.com/user/Klutzy-Snow8016</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, local models lagged ChatGPT 3.5 by a lot, and 4 was so far beyond that it felt hopeless. But now, you can run very good models at home.&lt;/p&gt; &lt;p&gt;So I'm curious, for your use-case, or just general usage, what was the point at which a model you ran locally finally caught up to what you saw from the paid models of 2023, or are you still waiting for that to happen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Klutzy-Snow8016"&gt; /u/Klutzy-Snow8016 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oefu29</id>
    <title>Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!</title>
    <updated>2025-10-23T21:31:20+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt; &lt;img alt="Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!" src="https://external-preview.redd.it/AGpnB3Q_Xjisqwn0DU233BBKuTP9o7kSBGuVW7dOHBs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74eb9943702ce83c73eb39485b3fe95bdff48313" title="Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We've gotten a ton of positive feedback on our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;previous&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;posts&lt;/a&gt; about our REAP pruned MoE models.&lt;/p&gt; &lt;p&gt;We've a got a new (highly requested!) update - REAP'd GLM4.6!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM4.6-FP8 REAP@25%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B-FP8&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6-FP8 REAP@30%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B-FP8&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6-FP8 REAP@40%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: the BF16 versions for low-bit quant are now available:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM4.6 REAP@25%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B"&gt;https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6 REAP@30%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B"&gt;https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6 REAP@40%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B"&gt;https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stay tuned, we are updating our model collection: &lt;a href="https://huggingface.co/collections/cerebras/cerebras-reap"&gt;https://huggingface.co/collections/cerebras/cerebras-reap&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gwuv3e9tjxwf1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab8c8018762fc99dae789ad012ce0d3f7a8a6a"&gt;https://preview.redd.it/gwuv3e9tjxwf1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab8c8018762fc99dae789ad012ce0d3f7a8a6a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:31:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeg2g6</id>
    <title>AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27</title>
    <updated>2025-10-23T21:40:51+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"&gt; &lt;img alt="AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27" src="https://external-preview.redd.it/1xRs-fKiMj0zISqC9vylqykmOso4ilpVoXcWW0J8xW4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=661bfefccbe35bc33bf966b1c78cea33a1e763bd" title="AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-officially-launches-radeon-ai-pro-r9700-at-1299/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oee1ie</id>
    <title>I spent months struggling to understand AI agents. Built a from scratch tutorial so you don't have to.</title>
    <updated>2025-10-23T20:21:05+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the longest time, I felt lost trying to understand how AI agents actually work.&lt;/p&gt; &lt;p&gt;Every tutorial I found jumped straight into LangChain or CrewAI. The papers were full of architecture diagrams but vague about implementation. I'd follow along, copy-paste code, and it would work... but I had no idea why.&lt;/p&gt; &lt;p&gt;The breaking point: I couldn't debug anything. When something broke, I had no mental model of what was happening under the hood. Was it the framework? The prompt? The model? No clue.&lt;/p&gt; &lt;p&gt;So I did what probably seems obvious in hindsight: I started building from scratch.&lt;/p&gt; &lt;p&gt;Just me, node-llama-cpp, and a lot of trial and error. No frameworks. No abstractions I didn't understand. Just pure fundamentals.&lt;/p&gt; &lt;p&gt;After months of reading, experimenting, and honestly struggling through a lot of confusion, things finally clicked. I understood what function calling really is. Why ReAct patterns work. How memory actually gets managed. What frameworks are actually doing behind their nice APIs.&lt;/p&gt; &lt;p&gt;I put together everything I learned here: &lt;a href="https://github.com/pguso/ai-agents-from-scratch"&gt;https://github.com/pguso/ai-agents-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's 8 progressive examples, from &amp;quot;Hello World&amp;quot; to full ReAct agents: - Plain JavaScript, no frameworks - Local LLMs only (Qwen, Llama, whatever you have) - Each example has detailed code breakdowns + concept explanations - Builds from basics to real agent patterns&lt;/p&gt; &lt;p&gt;Topics covered: - System prompts &amp;amp; specialization - Streaming &amp;amp; token control&lt;br /&gt; - Function calling (the &amp;quot;aha!&amp;quot; moment) - Memory systems (very basic) - ReAct pattern (Reasoning + Acting) - Parallel processing&lt;/p&gt; &lt;p&gt;Do you miss something?&lt;/p&gt; &lt;p&gt;Who this is for: - You want to understand agents deeply, not just use them - You're tired of framework black boxes - You learn by building - You want to know what LangChain is doing under the hood&lt;/p&gt; &lt;p&gt;What you'll need: - Node.js - A local GGUF model (I use Qwen 1.7B, runs on modest hardware) instructions in the repo for downloading - Curiosity and patience&lt;/p&gt; &lt;p&gt;I wish I had this resource when I started. Would've saved me months of confusion. Hope it helps someone else on the same journey.&lt;/p&gt; &lt;p&gt;Happy to answer questions about any of the patterns or concepts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oes4ez</id>
    <title>Qwen3 Next support in llama.cpp ready for review</title>
    <updated>2025-10-24T08:18:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt; &lt;img alt="Qwen3 Next support in llama.cpp ready for review" src="https://external-preview.redd.it/JWuwM-H5pHYaaKPNtY_8U3LHlrsSjJTNAjLHRGwU5o0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=016d876487cc90150078e6b226c52b29735d5532" title="Qwen3 Next support in llama.cpp ready for review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Congratulations to Piotr for his hard work, the code is now ready for review.&lt;/p&gt; &lt;p&gt;Please note that this is not the final version, and if you download some quantized models, you will probably need to download them again later. Also, it's not yet optimized for speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T08:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeigeh</id>
    <title>Amongst safety cuts, Facebook is laying off the Open Source LLAMA folks</title>
    <updated>2025-10-23T23:25:21+00:00</updated>
    <author>
      <name>/u/eredhuin</name>
      <uri>https://old.reddit.com/user/eredhuin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nytimes.com/2025/10/23/technology/meta-layoffs-user-privacy.html?unlocked_article_code=1.vk8.8nWb.yFO38KVrwYZW&amp;amp;smid=nytcore-ios-share&amp;amp;referringSource=articleShare"&gt;https://www.nytimes.com/2025/10/23/technology/meta-layoffs-user-privacy.html?unlocked_article_code=1.vk8.8nWb.yFO38KVrwYZW&amp;amp;smid=nytcore-ios-share&amp;amp;referringSource=articleShare&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Beyond Meta‚Äôs risk organization, other cuts on Wednesday targeted veteran members of Meta‚Äôs FAIR team and&lt;/em&gt; &lt;strong&gt;&lt;em&gt;those who had worked on previous versions of Meta‚Äôs open source A.I. models, called Llama.&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Among the employees who were laid off was Yuandong Tian, FAIR‚Äôs research director, who had been at the company for eight years.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;But there was one division that was spared: TBD Labs, the organization largely made up of new, highly paid recruits working on the next generation of A.I. research. The department is led by Mr. Wang.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eredhuin"&gt; /u/eredhuin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeigeh/amongst_safety_cuts_facebook_is_laying_off_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeigeh/amongst_safety_cuts_facebook_is_laying_off_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeigeh/amongst_safety_cuts_facebook_is_laying_off_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T23:25:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
