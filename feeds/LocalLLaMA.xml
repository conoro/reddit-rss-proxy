<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-23T20:24:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oee1ie</id>
    <title>I spent months struggling to understand AI agents. Built a from scratch tutorial so you don't have to.</title>
    <updated>2025-10-23T20:21:05+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the longest time, I felt lost trying to understand how AI agents actually work.&lt;/p&gt; &lt;p&gt;Every tutorial I found jumped straight into LangChain or CrewAI. The papers were full of architecture diagrams but vague about implementation. I'd follow along, copy-paste code, and it would work... but I had no idea why.&lt;/p&gt; &lt;p&gt;The breaking point: I couldn't debug anything. When something broke, I had no mental model of what was happening under the hood. Was it the framework? The prompt? The model? No clue.&lt;/p&gt; &lt;p&gt;So I did what probably seems obvious in hindsight: I started building from scratch.&lt;/p&gt; &lt;p&gt;Just me, node-llama-cpp, and a lot of trial and error. No frameworks. No abstractions I didn't understand. Just pure fundamentals.&lt;/p&gt; &lt;p&gt;After months of reading, experimenting, and honestly struggling through a lot of confusion, things finally clicked. I understood what function calling really is. Why ReAct patterns work. How memory actually gets managed. What frameworks are actually doing behind their nice APIs.&lt;/p&gt; &lt;p&gt;I put together everything I learned here: &lt;a href="https://github.com/pguso/ai-agents-from-scratch"&gt;https://github.com/pguso/ai-agents-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's 8 progressive examples, from &amp;quot;Hello World&amp;quot; to full ReAct agents: - Plain JavaScript, no frameworks - Local LLMs only (Qwen, Llama, whatever you have) - Each example has detailed code breakdowns + concept explanations - Builds from basics to real agent patterns&lt;/p&gt; &lt;p&gt;Topics covered: - System prompts &amp;amp; specialization - Streaming &amp;amp; token control&lt;br /&gt; - Function calling (the &amp;quot;aha!&amp;quot; moment) - Memory systems (very basic) - ReAct pattern (Reasoning + Acting) - Parallel processing&lt;/p&gt; &lt;p&gt;Do you miss something?&lt;/p&gt; &lt;p&gt;Who this is for: - You want to understand agents deeply, not just use them - You're tired of framework black boxes - You learn by building - You want to know what LangChain is doing under the hood&lt;/p&gt; &lt;p&gt;What you'll need: - Node.js - A local GGUF model (I use Qwen 1.7B, runs on modest hardware) instructions in the repo for downloading - Curiosity and patience&lt;/p&gt; &lt;p&gt;I wish I had this resource when I started. Would've saved me months of confusion. Hope it helps someone else on the same journey.&lt;/p&gt; &lt;p&gt;Happy to answer questions about any of the patterns or concepts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oebgam</id>
    <title>GPT-OSS 20B reasoning low vs medium vs high</title>
    <updated>2025-10-23T18:42:09+00:00</updated>
    <author>
      <name>/u/Inevitable_Ant_2924</name>
      <uri>https://old.reddit.com/user/Inevitable_Ant_2924</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that the “low” reasoning setting runs about four times faster than the “high” setting, but I haven’t found any example prompts where “high” succeeds while “low” fails. Do you have any?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Ant_2924"&gt; /u/Inevitable_Ant_2924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebgam/gptoss_20b_reasoning_low_vs_medium_vs_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebgam/gptoss_20b_reasoning_low_vs_medium_vs_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oebgam/gptoss_20b_reasoning_low_vs_medium_vs_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T18:42:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1odk11r</id>
    <title>Strix Halo vs DGX Spark - Initial Impressions (long post with TL;DR at the end)</title>
    <updated>2025-10-22T20:46:12+00:00</updated>
    <author>
      <name>/u/Eugr</name>
      <uri>https://old.reddit.com/user/Eugr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a lot of separate posts about Strix Halo and DGX Spark, but not too many direct comparisons from the people who are actually going to use them for work.&lt;/p&gt; &lt;p&gt;So, after getting Strix Halo and later DGX Spark, decided to compile my initial impressions after using both Strix Halo (GMKTek Evo x2 128GB) and NVidia DGX Spark as an AI developer, in case it would be useful to someone.&lt;/p&gt; &lt;h1&gt;Hardware&lt;/h1&gt; &lt;p&gt;DGX Spark is probably the most minimalist mini-PC I've ever used. &lt;/p&gt; &lt;p&gt;It has absolutely no LEDs, not even in the LAN port, and on/off switch is a button, so unless you ping it over the network or hook up a display, good luck guessing if this thing is on. All ports are in the back, there is no Display Port, only a single HDMI port, USB-C (power only), 3x USB-C 3.2 gen 2 ports, 10G ethernet port and 2x QSFP ports.&lt;/p&gt; &lt;p&gt;The air intake is in the front and exhaust is in the back. It is quiet for the most part, but the fan is quite audible when it's on (but quieter than my GMKTek).&lt;/p&gt; &lt;p&gt;It has a single 4TB PciE 5.0x4 M.2 2242 SSD - SAMSUNG MZALC4T0HBL1-00B07 which I couldn't find anywhere for sale in 2242 form factor, only 2280 version, but DGX Spark only takes 2242 drives. I wish they went with standard 2280 - weird decision, given that it's a mini-PC, not a laptop or tablet. Who cares if the motherboard is an inch longer!&lt;/p&gt; &lt;p&gt;The performance seems good, and gives me 4240.64 MB/sec vs 3118.53 MB/sec on my GMKTek (as measured by hdparm).&lt;/p&gt; &lt;p&gt;It is user replaceable, but there is only one slot, accessible from the bottom of the device. You need to take the magnetic plate off and there are some access screws underneath. &lt;/p&gt; &lt;p&gt;The unit is made of metal, and gets quite hot during high loads, but not unbearable hot like some reviews mentioned. Cools down quickly, though (metal!).&lt;/p&gt; &lt;p&gt;The CPU is 20 core ARM with 10 performance and 10 efficiency cores. I didn't benchmark them, but other reviews CPU show performance similar to Strix Halo.&lt;/p&gt; &lt;h1&gt;Initial Setup&lt;/h1&gt; &lt;p&gt;DGX Spark comes with DGX OS pre-installed (more on this later). You can set it up interactively using keyboard/mouse/display or in headless mode via WiFi hotspot that it creates.&lt;/p&gt; &lt;p&gt;I tried to set it up by connecting my trusted Logitech keyboard/trackpad combo that I use to set up pretty much all my server boxes, but once it booted up, it displayed &amp;quot;Connect the keyboard&amp;quot; message and didn't let me proceed any further. Trackpad portion worked, and volume keys on the keyboard also worked! I rebooted, and was able to enter BIOS (by pressing Esc) just fine, and the keyboard was fully functioning there!&lt;/p&gt; &lt;p&gt;BTW, it has AMI BIOS, but doesn't expose anything interesting other than networking and boot options.&lt;/p&gt; &lt;p&gt;Booting into DGX OS resulted in the same problem. After some googling, I figured that it shipped with a borked kernel that broke Logitech unified setups, so I decided to proceed in a headless mode.&lt;/p&gt; &lt;p&gt;Connected to the Wifi hotspot from my Mac (hotspot SSID/password are printed on a sticker on top of the quick start guide) and was able to continue set up there, which was pretty smooth, other than Mac spamming me with &amp;quot;connect to internet&amp;quot; popup every minute or so. It then proceeded to update firmware and OS packages, which took about 30 minutes, but eventually finished, and after that my Logitech keyboard worked just fine.&lt;/p&gt; &lt;h1&gt;Linux Experience&lt;/h1&gt; &lt;p&gt;DGX Spark runs DGX OS 7.2.3 which is based on Ubuntu 24.04.3 LTS, but uses NVidia's custom kernel, and an older one than mainline Ubuntu LTS uses. So instead of 6.14.x you get 6.11.0-1016-nvidia.&lt;/p&gt; &lt;p&gt;It comes with CUDA 13.0 development kit and NVidia drivers (580.95.05) pre-installed. It also has NVidia's container toolkit that includes docker, and GPU passthrough works well.&lt;/p&gt; &lt;p&gt;Other than that, it's a standard Ubuntu Desktop installation, with GNOME and everything.&lt;/p&gt; &lt;p&gt;SSHd is enabled by default, so after headless install you can connect to it immediately without any extra configuration. &lt;/p&gt; &lt;p&gt;RDP remote desktop doesn't work currently - it connects, but display output is broken.&lt;/p&gt; &lt;p&gt;I tried to boot from Fedora 43 Beta Live USB, and it worked, sort of. First, you need to disable Secure Boot in BIOS. Then, it boots only in &amp;quot;basic graphics mode&amp;quot;, because built-in nvidia drivers don't recognize the chipset. It also throws other errors complaining about chipset, processor cores, etc. &lt;/p&gt; &lt;p&gt;I think I'll try to install it to an external SSD and see if NVidia standard drivers will recognize the chip. There is hope:&lt;/p&gt; &lt;p&gt;============== PLATFORM INFO: ============== IOMMU: Pass-through or enabled Nvidia Driver Info Status: Supported(Nvidia Open Driver Installed) Cuda Driver Version Installed: 13000 Platform: NVIDIA_DGX_Spark, Arch: aarch64(Linux 6.11.0-1016-nvidia) Platform verification succeeded&lt;/p&gt; &lt;p&gt;As for Strix Halo, it's an x86 PC, so you can run any distro you want. I chose Fedora 43 Beta, currently running with kernel 6.17.3-300.fc43.x86_64. Smooth sailing, up-to-date packages.&lt;/p&gt; &lt;h1&gt;Llama.cpp experience&lt;/h1&gt; &lt;h2&gt;DGX Spark&lt;/h2&gt; &lt;p&gt;You need to build it from source as there is no CUDA ARM build, but compiling llama.cpp was very straightforward - CUDA toolkit is already installed, just need to install development tools and it compiles just like on any other system with NVidia GPU. Just follow the instructions, no surprises.&lt;/p&gt; &lt;p&gt;However, when I ran the benchmarks, I ran into two issues.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The model loading was VERY slow. It took 1 minute 40 seconds to load gpt-oss-120b. For comparison, it takes 22 seconds to load on Strix Halo (both from cold, memory cache flushed).&lt;/li&gt; &lt;li&gt;I wasn't getting the same results as ggerganov in this &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;thread&lt;/a&gt;. While PP was pretty impressive for such a small system, TG was matching or even slightly worse than my Strix Halo setup with ROCm.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For instance, here are my Strix Halo numbers, compiled with ROCm 7.10.0a20251017, llama.cpp build 03792ad9 (6816), HIP only, no rocWMMA:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash build/bin/llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-120b-GGUF_gpt-oss-120b-mxfp4-00001-of-00003.gguf -fa 1 -d 0,4096,8192,16384,32768 -p 2048 -n 32 -ub 2048 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 999.59 ± 4.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 47.49 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 824.37 ± 1.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 44.23 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 703.42 ± 1.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 42.52 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 514.89 ± 3.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 39.71 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 348.59 ± 2.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 35.39 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The same command on Spark gave me this:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1816.00 ± 11.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 44.74 ± 0.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 1763.75 ± 6.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 42.69 ± 0.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 1695.29 ± 11.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 40.91 ± 0.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 1512.65 ± 6.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 38.61 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 1250.55 ± 5.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 34.66 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried enabling Unified Memory switch (GGML_CUDA_ENABLE_UNIFIED_MEMORY=1) - it improved model loading, but resulted in even worse performance.&lt;/p&gt; &lt;p&gt;I reached out to ggerganov, and he suggested disabling mmap. I thought I tried it, but apparently not. Well, that fixed it. Model loading improved too - now taking 56 seconds from cold and 23 seconds when it's still in cache.&lt;/p&gt; &lt;p&gt;Updated numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1939.32 ± 4.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 56.33 ± 0.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 1832.04 ± 5.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 52.63 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 1738.07 ± 5.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 48.60 ± 0.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 1525.71 ± 12.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 45.01 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 1242.35 ± 5.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 39.10 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, much better performance both in PP and TG. &lt;/p&gt; &lt;p&gt;As for Strix Halo, mmap/no-mmap doesn't make any difference there.&lt;/p&gt; &lt;h2&gt;Strix Halo&lt;/h2&gt; &lt;p&gt;On Strix Halo, llama.cpp experience is... well, a bit turbulent. &lt;/p&gt; &lt;p&gt;You can download a pre-built version for Vulkan, and it works, but the performance is a mixed bag. TG is pretty good, but PP is not great.&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash build/bin/llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-120b-GGUF_gpt-oss-120b-mxfp4-00001-of-00003.gguf -fa 1 -d 0,4096,8192,16384,32768 -p 2048 -n 32 --mmap 0 -ngl 999 -ub 1024 &lt;/code&gt; &lt;strong&gt;NOTE&lt;/strong&gt;: Vulkan likes batch size of 1024 the most, unlike ROCm that likes 2048 better.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 526.54 ± 4.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 52.64 ± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 438.85 ± 0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 48.21 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 356.28 ± 4.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 45.90 ± 0.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 210.17 ± 2.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 42.64 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 138.79 ± 9.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 36.18 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried toolboxes from kyuz0, and some of them were better, but I still felt that I could squeeze more juice out of it. All of them suffered from significant performance degradation when the context was filling up.&lt;/p&gt; &lt;p&gt;Then I tried to compile my own using the latest ROCm &lt;a href="https://therock-nightly-tarball.s3.amazonaws.com/therock-dist-linux-gfx1151-7.10.0a20251017.tar.gz"&gt;build&lt;/a&gt; from TheRock (on that date).&lt;/p&gt; &lt;p&gt;I also build &lt;a href="https://github.com/ROCm/rocWMMA.git"&gt;rocWMMA&lt;/a&gt; as recommended by kyoz0 (more on that later).&lt;/p&gt; &lt;p&gt;Llama.cpp compiled without major issues - I had to configure the paths properly, but other than that, it just worked. The PP increased dramatically, but TG decreased.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1030.71 ± 2.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 47.84 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 802.36 ± 6.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 39.09 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 615.27 ± 2.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 33.34 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 409.25 ± 0.67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 25.86 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 228.04 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 18.07 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;But the biggest issue is significant performance degradation with long context, much more than you'd expect.&lt;/p&gt; &lt;p&gt;Then I stumbled upon Lemonade SDK and their pre-built llama.cpp. Ran that one, and got much better results across the board. TG was still below Vulkan, but PP was decent and degradation wasn't as bad:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;999.20 ± 3.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32&lt;/td&gt; &lt;td align="right"&gt;47.53 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt;826.63 ± 9.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt;44.24 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt;702.66 ± 2.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt;42.56 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt;505.85 ± 1.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt;39.82 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt;343.06 ± 2.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt;35.50 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So I looked at their compilation options and noticed that they build without rocWMMA. So, I did the same and got similar performance too!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1000.93 ± 1.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 47.46 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 827.34 ± 1.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 44.20 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 701.68 ± 2.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 42.39 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 503.49 ± 0.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 39.61 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 344.36 ± 0.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 35.32 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So far that's the best I could get from Strix Halo. It's very usable for text generation tasks.&lt;/p&gt; &lt;p&gt;Also, wanted to touch multi-modal performance. That's where Spark shines. I don't have any specific benchmarks yet, but image processing is much faster on Spark than on Strix Halo, especially in vLLM.&lt;/p&gt; &lt;h1&gt;VLLM Experience&lt;/h1&gt; &lt;p&gt;Haven't had a chance to do extensive testing here, but wanted to share some early thoughts.&lt;/p&gt; &lt;h2&gt;DGX Spark&lt;/h2&gt; &lt;p&gt;First, I tried to just build vLLM from the source as usual. The build was successful, but it failed with the following error: ptxas fatal : Value 'sm_121a' is not defined for option 'gpu-name'&lt;/p&gt; &lt;p&gt;I decided not to spend too much time on this for now, and just launched vLLM container that NVidia provides through their Docker repository. It is built for DGX Spark, so supports it out of the box.&lt;/p&gt; &lt;p&gt;However, it has version 0.10.1, so I wasn't able to run Qwen3-VL there.&lt;/p&gt; &lt;p&gt;Now, they put the source code inside the container, but it wasn't a git repository - probably contains some NVidia-specific patches - I'll need to see if those could be merged into main vllm code.&lt;/p&gt; &lt;p&gt;So I just checked out vllm main branch and proceeded to build with existing pytorch as usual. This time I was able to run it and launch qwen3-vl models just fine. Both dense and MOE work. I tried FP4 and AWQ quants - everything works, no need to disable CUDA graphs.&lt;/p&gt; &lt;p&gt;The performance is decent - I still need to run some benchmarks, but image processing is very fast.&lt;/p&gt; &lt;h2&gt;Strix Halo&lt;/h2&gt; &lt;p&gt;Unlike llama.cpp that just works, vLLM experience on Strix Halo is much more limited.&lt;/p&gt; &lt;p&gt;My goal was to run Qwen3-VL models that are not supported by llama.cpp yet, so I needed to build 0.11.0 or later. There are some existing containers/toolboxes for earlier versions, but I couldn't use them.&lt;/p&gt; &lt;p&gt;So, I installed ROCm pyTorch libraries from TheRock, some &lt;a href="https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes/blob/main/Dockerfile.vllm-therock-gfx1151-aotriton"&gt;patches&lt;/a&gt; from kyoz0 toolboxes to avoid amdsmi package crash, &lt;a href="https://github.com/ROCm/flash-attention.git"&gt;ROCm FlashAttention&lt;/a&gt; and then just followed vLLM standard installation instructions with existing pyTorch.&lt;/p&gt; &lt;p&gt;I was able to run Qwen3VL dense models with decent (for dense models) speeds, although initialization takes quite some time until you reduce -max-num-seqs to 1 and set tp 1. The image processing is very slow though, much slower than llama.cpp for the same image, but the token generation is about what you'd expect from it.&lt;/p&gt; &lt;p&gt;Again, model loading is faster than Spark for some reason (I'd expect other way around given faster SSD in Spark and slightly faster memory).&lt;/p&gt; &lt;p&gt;I'm going to rebuild vLLM and re-test/benchmark later.&lt;/p&gt; &lt;p&gt;Some observations: - FP8 models don't work - they hang on WARNING 10-22 12:55:04 [fp8_utils.py:785] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/eugr/vllm/vllm/vllm/model_executor/layers/quantization/utils/configs/N=6144,K=2560,device_name=Radeon_8060S_Graphics,dtype=fp8_w8a8,block_shape=[128,128].json - You need to use --enforce-eager, as CUDA graphs crash vLLM. Sometimes it works, but mostly crashes. - Even with --enforce-eager, there are some HIP-related crashes here and there occasionally. - AWQ models work, both 4-bit and 8-bit, but only dense ones. AWQ MOE quants require Marlin kernel that is not available for ROCm.&lt;/p&gt; &lt;h1&gt;Conclusion / TL;DR&lt;/h1&gt; &lt;p&gt;Summary of my initial impressions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DGX Spark is an interesting beast for sure. &lt;ul&gt; &lt;li&gt;Limited extensibility - no USB-4, only one M.2 slot, and it's 2242.&lt;/li&gt; &lt;li&gt;But has 200Gbps network interface.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;It's a first generation of such devices, so there are some annoying bugs and incompatibilities.&lt;/li&gt; &lt;li&gt;Inference wise, the token generation is nearly identical to Strix Halo both in llama.cpp and vllm, but prompt processing is 2-5x higher than Strix Halo. &lt;ul&gt; &lt;li&gt;Strix Halo performance in prompt processing degrades much faster with context.&lt;/li&gt; &lt;li&gt;Image processing takes longer, especially with vLLM.&lt;/li&gt; &lt;li&gt;Model loading into unified RAM is slower on DGX Spark for some reason, both in llama.cpp and vLLM.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Even though vLLM included gfx1151 in the supported configurations, it still requires some hacks to compile it. &lt;ul&gt; &lt;li&gt;And even then, the experience is suboptimal. Initialization time is slow, it crashes, FP8 doesn't work, AWQ for MOE doesn't work.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;If you are an AI developer who uses transformers/pyTorch or you need vLLM - you are better off with DGX Spark (or just a normal GPU build).&lt;/li&gt; &lt;li&gt;If you want a power-efficient inference server that can run gpt-oss and similar MOE at decent speeds, and don't need to process images often, Strix Halo is the way to go.&lt;/li&gt; &lt;li&gt;If you want a general purpose machine, Strix Halo wins too.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eugr"&gt; /u/Eugr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odk11r/strix_halo_vs_dgx_spark_initial_impressions_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odk11r/strix_halo_vs_dgx_spark_initial_impressions_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odk11r/strix_halo_vs_dgx_spark_initial_impressions_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T20:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1odystc</id>
    <title>Hierarchical Agentic RAG: What are your thoughts?</title>
    <updated>2025-10-23T09:33:14+00:00</updated>
    <author>
      <name>/u/Just-Message-9899</name>
      <uri>https://old.reddit.com/user/Just-Message-9899</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odystc/hierarchical_agentic_rag_what_are_your_thoughts/"&gt; &lt;img alt="Hierarchical Agentic RAG: What are your thoughts?" src="https://preview.redd.it/co0m6q6hztwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67cb6fa342de397bddd954c1f689d7420df2c28f" title="Hierarchical Agentic RAG: What are your thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;While exploring techniques to optimize Retrieval-Augmented Generation (RAG) systems, I found the concept of Hierarchical RAG (sometimes called &amp;quot;Parent Document Retriever&amp;quot; or similar).&lt;/p&gt; &lt;p&gt;Essentially, I've seen implementations that use a hierarchical chunking strategy where: 1. Child chunks (smaller, denser) are created and used as retrieval anchors (for vector search). 2. Once the most relevant child chunks are identified, their larger &amp;quot;parent&amp;quot; text portions (which contain more context) are retrieved to be used as context for the LLM.&lt;/p&gt; &lt;p&gt;The idea is that the small chunks improve retrieval precision (reducing &amp;quot;lost in the middle&amp;quot; and semantic drift), while the large chunks provide the LLM with the full context needed for more accurate and coherent answers.&lt;/p&gt; &lt;p&gt;What are your thoughts on this technique? Do you have any direct experience with it?&lt;br /&gt; Do you find it to be one of the best strategies for balancing retrieval precision and context richness?&lt;br /&gt; Are there better/more advanced RAG techniques (perhaps &amp;quot;Agentic RAG&amp;quot; or other routing/optimization strategies) that you prefer?&lt;/p&gt; &lt;p&gt;I found an implementation on GitHub that explains the concept well and offers a practical example. It seems like a good starting point to test the validity of the approach. &lt;/p&gt; &lt;p&gt;Link to the repository: &lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just-Message-9899"&gt; /u/Just-Message-9899 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co0m6q6hztwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odystc/hierarchical_agentic_rag_what_are_your_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odystc/hierarchical_agentic_rag_what_are_your_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T09:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe6hvs</id>
    <title>C++ worth it for a local LLM server implementation? Thinking of switching Lemonade from Python to C++ (demo with voiceover)</title>
    <updated>2025-10-23T15:35:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6hvs/c_worth_it_for_a_local_llm_server_implementation/"&gt; &lt;img alt="C++ worth it for a local LLM server implementation? Thinking of switching Lemonade from Python to C++ (demo with voiceover)" src="https://external-preview.redd.it/YTltZ3NjMDdwdndmMZDefYhOjxF6Oonl06ZspNuuLgsqptkGVQiT3n8KC0wx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10a659c8b24e8137c28c07bd01bb4eb00acb9a39" title="C++ worth it for a local LLM server implementation? Thinking of switching Lemonade from Python to C++ (demo with voiceover)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last 48 hours I've built a proof-of-concept pure C++ implementation of Lemonade. It's going pretty well so I want to get people's thoughts here as the team decides whether to replace the Python implementation.&lt;/p&gt; &lt;p&gt;So far, the ported features are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD NPU, GPU, and CPU support on Windows via Ryzen AI SW 1.6, FastFlowLM, and llama.cpp Vulkan.&lt;/li&gt; &lt;li&gt;OpenAI chat/completions and models endpoints (for Open WebUI compatibility)&lt;/li&gt; &lt;li&gt;Serves the Lemonade web ui and supports most Lemonade API endpoints (load, unload, pull, delete, health) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main benefits of C++ I see are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;All interactions feel much snappier.&lt;/li&gt; &lt;li&gt;Devs can deploy with their apps without needing to ship a Python interpreter.&lt;/li&gt; &lt;li&gt;Install size for the Lemonade server-router itself is 10x smaller (backend engine sizes are unchanged).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The main advantage of Python has always been development speed, especially thanks to the libraries available. However, I've found that coding with Sonnet 4.5 is such a productivity boost that Python no longer has an advantage. (is there an ethical quandary using Sonnet to port a Python project with 67 OSS deps into a C++ project with 3 deps? it's definitely a strange and different way to work...)&lt;/p&gt; &lt;p&gt;Anyways, take a look and I'm curious to hear everyone's thoughts. Not committed to shipping this yet, but if I do it'll of course be open source on the Lemonade github. I would also make sure it works on Linux and macOS with the supported backends (vulkan/rocm/metal). Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wyjbad07pvwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6hvs/c_worth_it_for_a_local_llm_server_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6hvs/c_worth_it_for_a_local_llm_server_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T15:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe2him</id>
    <title>Pokee AI - Opensource 7B model for deep research</title>
    <updated>2025-10-23T12:53:44+00:00</updated>
    <author>
      <name>/u/previse_je_sranje</name>
      <uri>https://old.reddit.com/user/previse_je_sranje</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked it to give me Universities that fit specific criteria. 30 min later it produced a report with sources and really emphasized on verifying my criteria was met. It doesn't feel like just a 7B model, it's pretty good.. or maybe 7B models got too good :D?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/previse_je_sranje"&gt; /u/previse_je_sranje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Pokee_AI/status/1981040897346179256"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe2him/pokee_ai_opensource_7b_model_for_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe2him/pokee_ai_opensource_7b_model_for_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T12:53:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1odi1c0</id>
    <title>Meta lays off 600 employees within AI unit</title>
    <updated>2025-10-22T19:30:58+00:00</updated>
    <author>
      <name>/u/a_slay_nub</name>
      <uri>https://old.reddit.com/user/a_slay_nub</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"&gt; &lt;img alt="Meta lays off 600 employees within AI unit" src="https://external-preview.redd.it/J07EauFcN4nV9LOcRS0eXwdDIcxd3OiFJdlO3Bhl-Rc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b63bb44a8b15efeaf11cd6f80af319b8caa01688" title="Meta lays off 600 employees within AI unit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_slay_nub"&gt; /u/a_slay_nub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/22/meta-layoffs-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T19:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1odwj89</id>
    <title>Qwen3 Next 80B A3B Instruct on RTX 5090</title>
    <updated>2025-10-23T07:03:34+00:00</updated>
    <author>
      <name>/u/lkarlslund</name>
      <uri>https://old.reddit.com/user/lkarlslund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With latest patches you can run the Q2 on 32GB VRAM with 50K context size. Here's how:&lt;/p&gt; &lt;p&gt;Assuming you're running Linux, and have required dev tools installed:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/cturan/llama.cpp.git llama.cpp-qwen3-next cd llama.cpp-qwen3-next git checkout qwen3_next time cmake -B build -DGGML_CUDA=ONgit clone https://github.com/cturan/llama.cpp.git llama.cpp-qwen3-next cd llama.cpp-qwen3-next git checkout qwen3_next time cmake -B build -DGGML_CUDA=ON time cmake --build build --config Release --parallel $(nproc --all) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Grab the model from HuggingFace:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF/tree/main"&gt;https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If all of that went according to plan, launch it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;build/bin/llama-server -m \~/models/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF/Qwen\_\_Qwen3-Next-80B-A3B-Instruct-Q2\_K.gguf --port 5005 --no-mmap -ngl 999 --ctx-size 50000 -fa on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That gives me around 600t/s for prompt parsing and 50-60t/s for generation.&lt;/p&gt; &lt;p&gt;You can also run Q4 with partial CUDA offload, adjust -ngl 30 or whatever VRAM you have available. The performance is not great though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lkarlslund"&gt; /u/lkarlslund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odwj89/qwen3_next_80b_a3b_instruct_on_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odwj89/qwen3_next_80b_a3b_instruct_on_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odwj89/qwen3_next_80b_a3b_instruct_on_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T07:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oec0xm</id>
    <title>What’s the smartest NON thinking model under 40B or so?</title>
    <updated>2025-10-23T19:03:31+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seed 39B is excellent for thinking, but what about non-thinking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T19:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1odx0d4</id>
    <title>Llama.cpp is looking for M5 Neural Accelerator performance testers</title>
    <updated>2025-10-23T07:34:54+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odx0d4/llamacpp_is_looking_for_m5_neural_accelerator/"&gt; &lt;img alt="Llama.cpp is looking for M5 Neural Accelerator performance testers" src="https://external-preview.redd.it/nAPkKB3AZyBbBnQ2EaOzbhk_HVxsulN95BhKMutB8lk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5cdeb2d71479faa99d95242c1a9a704128731920" title="Llama.cpp is looking for M5 Neural Accelerator performance testers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16634"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odx0d4/llamacpp_is_looking_for_m5_neural_accelerator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odx0d4/llamacpp_is_looking_for_m5_neural_accelerator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T07:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe99n3</id>
    <title>llama2 may not be as smart as newer LLMs, but it does have personality LOL</title>
    <updated>2025-10-23T17:19:24+00:00</updated>
    <author>
      <name>/u/junior600</name>
      <uri>https://old.reddit.com/user/junior600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe99n3/llama2_may_not_be_as_smart_as_newer_llms_but_it/"&gt; &lt;img alt="llama2 may not be as smart as newer LLMs, but it does have personality LOL" src="https://preview.redd.it/1uk9ze6f9wwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30def686c934c0d9b9f64b15f568619bcee8c527" title="llama2 may not be as smart as newer LLMs, but it does have personality LOL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I tried running an ancient model by today’s standards for nostalgia, and I’m impressed to see that it still retains its “personality,” lol. These models are obviously very dated by today’s standards, but it’s interesting to see how much the technology has improved in such a short time span. Are you also still using ancient models from time to time? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/junior600"&gt; /u/junior600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1uk9ze6f9wwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe99n3/llama2_may_not_be_as_smart_as_newer_llms_but_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe99n3/llama2_may_not_be_as_smart_as_newer_llms_but_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T17:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe70jh</id>
    <title>Is MLX working with new M5 matmul yet?</title>
    <updated>2025-10-23T15:54:39+00:00</updated>
    <author>
      <name>/u/PracticlySpeaking</name>
      <uri>https://old.reddit.com/user/PracticlySpeaking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a dev so I don't speak git, but &lt;a href="https://creativestrategies.com/research/m5-apple-silicon-its-all-about-the-cache-and-tensors/"&gt;this article&lt;/a&gt; implies that there is &amp;quot;preliminary support&amp;quot; for the M5 GPU matmul hardware in MLX. It references this issue:&lt;/p&gt; &lt;p&gt;[Experiment] Use metal performance primitives by sstame20 · Pull Request #2687 · ml-explore/mlx · GitHub - &lt;a href="https://github.com/ml-explore/mlx/pull/2687"&gt;https://github.com/ml-explore/mlx/pull/2687&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seems not to be in a release (yet) seeing it's only three days old rn.&lt;/p&gt; &lt;p&gt;Or does the OS, compiler/interpreter or framework decide where matmul is actually executed (GPU hardware or software)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticlySpeaking"&gt; /u/PracticlySpeaking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe70jh/is_mlx_working_with_new_m5_matmul_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe70jh/is_mlx_working_with_new_m5_matmul_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe70jh/is_mlx_working_with_new_m5_matmul_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T15:54:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeaucp</id>
    <title>What’s the best and most reliable LLM benchmarking site or arena right now?</title>
    <updated>2025-10-23T18:18:41+00:00</updated>
    <author>
      <name>/u/fflarengo</name>
      <uri>https://old.reddit.com/user/fflarengo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying to make sense of the current landscape of LLM leaderboards like Chatbot Arena, HELM, Hugging Face’s Open LLM Leaderboard, AlpacaEval, Arena-Hard, etc.&lt;/p&gt; &lt;p&gt;Some focus on human preference, others on standardized accuracy, and a few mix both. The problem is, every leaderboard seems to tell a slightly different story. It’s hard to know what actually &lt;em&gt;means&lt;/em&gt; “better.”&lt;/p&gt; &lt;p&gt;What I’m trying to figure out is:&lt;br /&gt; Which benchmarking platform do &lt;strong&gt;you&lt;/strong&gt; personally trust the most and not just for leaderboard bragging rights, but for genuine, day-to-day reflection of how capable or “smart” a model really is?&lt;/p&gt; &lt;p&gt;If you’ve run your own evals or compared models directly, I’d love to hear what lined up (or didn’t) with your real-world experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fflarengo"&gt; /u/fflarengo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeaucp/whats_the_best_and_most_reliable_llm_benchmarking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeaucp/whats_the_best_and_most_reliable_llm_benchmarking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeaucp/whats_the_best_and_most_reliable_llm_benchmarking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T18:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oda8mk</id>
    <title>Qwen team is helping llama.cpp again</title>
    <updated>2025-10-22T14:44:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt; &lt;img alt="Qwen team is helping llama.cpp again" src="https://preview.redd.it/dh1iaky2eowf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=addcf456730d4f5ec04b561980fa9d74dfb18d96" title="Qwen team is helping llama.cpp again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh1iaky2eowf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T14:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe94w6</id>
    <title>Distil NPC: Family of SLMs responsing as NPCs</title>
    <updated>2025-10-23T17:14:32+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe94w6/distil_npc_family_of_slms_responsing_as_npcs/"&gt; &lt;img alt="Distil NPC: Family of SLMs responsing as NPCs" src="https://preview.redd.it/vd644k6p9wwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=511c6a6b9803aa9da5de8e08744f985176d753b0" title="Distil NPC: Family of SLMs responsing as NPCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;we finetuned Google's Gemma 270m (and 1b) small language models specialized in having conversations as non-playable characters (NPC) found in various video games. Our goal is to enhance the experience of interacting in NPSs in games by enabling natural language as means of communication (instead of single-choice dialog options). More details in &lt;a href="https://github.com/distil-labs/Distil-NPCs"&gt;https://github.com/distil-labs/Distil-NPCs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The models can be found here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Data&lt;/h1&gt; &lt;p&gt;We preprocessed an existing NPC dataset (amaydle/npc-dialogue) to make it amenable to being trained in a closed-book QA setup. The original dataset consists of approx 20 examples with&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Character Name&lt;/li&gt; &lt;li&gt;Biography - a very brief bio. about the character&lt;/li&gt; &lt;li&gt;Question&lt;/li&gt; &lt;li&gt;Answer&lt;/li&gt; &lt;li&gt;The inputs to the pipeline are:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and a list of Character biographies.&lt;/p&gt; &lt;h1&gt;Qualitative analysis&lt;/h1&gt; &lt;p&gt;A qualitative analysis offers a good insight into the trained models performance. For example we can compare the answers of a trained and base model below.&lt;/p&gt; &lt;p&gt;Character bio:&lt;/p&gt; &lt;p&gt;Marcella Ravenwood is a powerful sorceress who comes from a long line of magic-users. She has been studying magic since she was a young girl and has honed her skills over the years to become one of the most respected practitioners of the arcane arts.&lt;/p&gt; &lt;p&gt;Question:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Character: Marcella Ravenwood Do you have any enemies because of your magic? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Answer:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Yes, I have made some enemies in my studies and battles. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finetuned model prediction:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The darkness within can be even fiercer than my spells. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Base model prediction:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;question&amp;gt;Character: Marcella Ravenwood Do you have any enemies because of your magic?&amp;lt;/question&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vd644k6p9wwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe94w6/distil_npc_family_of_slms_responsing_as_npcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe94w6/distil_npc_family_of_slms_responsing_as_npcs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T17:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe6y1a</id>
    <title>VT Code — Rust terminal coding agent doing AST-aware edits + local model workflows</title>
    <updated>2025-10-23T15:51:59+00:00</updated>
    <author>
      <name>/u/vinhnx</name>
      <uri>https://old.reddit.com/user/vinhnx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all — I’m the author of &lt;strong&gt;VT Code&lt;/strong&gt;, an open-source Rust CLI/TUI coding agent built around structural code editing (via Tree-sitter + ast-grep) and multi-provider LLM support — including local model workflows via Ollama.&lt;br /&gt; Link: &lt;a href="https://github.com/vinhnx/vtcode"&gt;https://github.com/vinhnx/vtcode&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this is relevant to LocalLLaMA&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local-model ready: you can run it fully offline if you have Ollama + a compatible model.&lt;/li&gt; &lt;li&gt;Agent architecture: modular provider/tool traits, token budgeting, caching, and structural edits.&lt;/li&gt; &lt;li&gt;Editor integration: works with editor context and TUI + CLI control, so you can embed local model workflows into your dev loop.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to try&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cargo install vtcode # or brew install vinhnx/tap/vtcode # or npm install -g vtcode # Local run example: ollama serve vtcode --provider ollama --model qwen3.1:7b ask &amp;quot;Refactor this Rust function into an async Result-returning API.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What I’d like feedback on&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UX and performance when using &lt;strong&gt;local models&lt;/strong&gt; (what works best: hardware, model size, latency)&lt;/li&gt; &lt;li&gt;Safety &amp;amp; policy for tool execution in local/agent workflows (sandboxing, path limits, PTY handling)&lt;/li&gt; &lt;li&gt;Editor integration: how intuitive is the flow from code to agent to edit back in your environment?&lt;/li&gt; &lt;li&gt;Open-source dev workflow: ways to make contributions simpler for add-on providers/models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;License &amp;amp; repo&lt;/strong&gt;&lt;br /&gt; MIT licensed, open for contributions: vinhnx/vtcode on GitHub.&lt;/p&gt; &lt;p&gt;Thanks for reading — happy to dive into any questions or discussions about local model setups, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinhnx"&gt; /u/vinhnx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6y1a/vt_code_rust_terminal_coding_agent_doing_astaware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6y1a/vt_code_rust_terminal_coding_agent_doing_astaware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6y1a/vt_code_rust_terminal_coding_agent_doing_astaware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T15:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe86sk</id>
    <title>I will try to benchmark every LLM + GPU combination you request in the comments</title>
    <updated>2025-10-23T16:38:41+00:00</updated>
    <author>
      <name>/u/Level-Park3820</name>
      <uri>https://old.reddit.com/user/Level-Park3820</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, &lt;/p&gt; &lt;p&gt;I’ve been running benchmarks for different LLM and GPU combinations, and I’m planning to create even more based on your suggestions.&lt;/p&gt; &lt;p&gt;If there’s a specific model + GPU combo you’d like to see benchmarked, drop it in the comments and I’ll try to include it in the next batch. Any ideas or requests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Level-Park3820"&gt; /u/Level-Park3820 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe86sk/i_will_try_to_benchmark_every_llm_gpu_combination/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe86sk/i_will_try_to_benchmark_every_llm_gpu_combination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe86sk/i_will_try_to_benchmark_every_llm_gpu_combination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe8hjh</id>
    <title>M5 iPad runs 8B-Q4 model.</title>
    <updated>2025-10-23T16:50:14+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"&gt; &lt;img alt="M5 iPad runs 8B-Q4 model." src="https://preview.redd.it/cq5w77gg5wwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb34d2ba73be617d6ecd7bfa852850ba436123a5" title="M5 iPad runs 8B-Q4 model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not too much of a surprise that the new M5 iPad (11&amp;quot; Base model with 12 GB of RAM) will run an 8B Q4 model. Please see the screenshot. I asked it to explain how to solve a Rubik's Cube, and it gave a decent answer and a respectable 23 tokens per second. The app I'm using is called Noema AI, and I like it a lot because you can have both a local model and an endpoint. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cq5w77gg5wwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:50:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1odxyb6</id>
    <title>Un-LOCC (Universal Lossy Optical Context Compression), Achieve Up To 3× context compression with 93.65% Accuracy.</title>
    <updated>2025-10-23T08:37:48+00:00</updated>
    <author>
      <name>/u/MaxDev0</name>
      <uri>https://old.reddit.com/user/MaxDev0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odxyb6/unlocc_universal_lossy_optical_context/"&gt; &lt;img alt="Un-LOCC (Universal Lossy Optical Context Compression), Achieve Up To 3× context compression with 93.65% Accuracy." src="https://preview.redd.it/it5cpntkptwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ac961c643c153addf2fd53394331fd81a50f29" title="Un-LOCC (Universal Lossy Optical Context Compression), Achieve Up To 3× context compression with 93.65% Accuracy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I compress LLM context into &lt;strong&gt;images&lt;/strong&gt; instead of text, and let a &lt;strong&gt;vision-language model&lt;/strong&gt; (VLM) “decompress” it by reading the image. In my tests, this yields up to &lt;strong&gt;~2.8:1 token compression at 93.65% accuracy&lt;/strong&gt; on &lt;em&gt;Gemini 2.5-Flash-Lite (Exp 56)&lt;/em&gt;, and &lt;strong&gt;99.26% at 1.7:1&lt;/strong&gt; on &lt;em&gt;Qwen2.5-VL-72B-Instruct (Exp 34)&lt;/em&gt;. Full code, experiments, and replication steps are open-source.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo (please ⭐ if useful):&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC"&gt;https://github.com/MaxDevv/Un-LOCC&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What this is:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Un-LOCC (Universal Lossy Optical Context Compression)&lt;/strong&gt;: a simple, general method to &lt;strong&gt;encode long text context into compact images&lt;/strong&gt;, then &lt;strong&gt;decode with a VLM&lt;/strong&gt;. Think of the VLM as an OCR-plus semantic decompressor.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I render text into a fixed-size PNG (e.g., &lt;strong&gt;324×324&lt;/strong&gt;, Atkinson Hyperlegible ~&lt;strong&gt;13px&lt;/strong&gt;), pass that image to a VLM, and ask it to reproduce the original text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; = normalized Levenshtein similarity (%).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression ratio&lt;/strong&gt; = &lt;em&gt;text tokens ÷ image tokens&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key results (linked to experiments in the repo):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5-Flash-Lite&lt;/strong&gt;: &lt;strong&gt;100% @ 1.3:1&lt;/strong&gt; &lt;em&gt;(Exp 46)&lt;/em&gt; and &lt;strong&gt;~93.65% @ 2.8:1&lt;/strong&gt; &lt;em&gt;(Exp 56)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen2.5-VL-72B-Instruct&lt;/strong&gt;: &lt;strong&gt;99.26% @ 1.7:1&lt;/strong&gt; &lt;em&gt;(Exp 34)&lt;/em&gt;; &lt;strong&gt;~75.56% @ 2.3:1&lt;/strong&gt; &lt;em&gt;(Exp 41)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-VL-235B-a22b-Instruct&lt;/strong&gt;: &lt;strong&gt;95.24% @ 2.2:1&lt;/strong&gt; &lt;em&gt;(Exp 50)&lt;/em&gt;; &lt;strong&gt;~82.22% @ 2.8:1&lt;/strong&gt; &lt;em&gt;(Exp 90)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phi-4-Multimodal&lt;/strong&gt;: &lt;strong&gt;94.44% @ 1.1:1&lt;/strong&gt; &lt;em&gt;(Exps 59, 85)&lt;/em&gt;; &lt;strong&gt;~73.55% @ 2.3:1&lt;/strong&gt; &lt;em&gt;(Exp 61)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;: &lt;strong&gt;95.24% @ 1.7:1&lt;/strong&gt; &lt;em&gt;(Exp 72)&lt;/em&gt;; &lt;strong&gt;~79.71% @ 1.7:1&lt;/strong&gt; &lt;em&gt;(Exp 88)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLaMA-4-Scout&lt;/strong&gt;: &lt;strong&gt;86.57% @ 1.3:1&lt;/strong&gt; &lt;em&gt;(Exp 53)&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;Details, prompts, fonts, and measurement code are in the README. I cite each claim with &lt;strong&gt;(Exp XX)&lt;/strong&gt; so you can verify quickly.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Why this matters:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cheaper context&lt;/strong&gt;: replace expensive text tokens with “image tokens” when a capable VLM sits in the loop.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecturally simple&lt;/strong&gt;: no model modifications are needed, you can use rendering + a VLM you already have.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Composable&lt;/strong&gt;: combine with retrieval, chunking, or multimodal workflows.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I need help with:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Generalization&lt;/strong&gt;: different fonts, colors, and resolutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model coverage&lt;/strong&gt;: more open VLMs; local runs welcome.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge cases&lt;/strong&gt;: math, code blocks, long tables, multilingual.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repro/PRs&lt;/strong&gt;: if you get better ratios or accuracy, please open an issue/PR.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo again (and yes, stars genuinely help discoverability):&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC"&gt;https://github.com/MaxDevv/Un-LOCC&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaxDev0"&gt; /u/MaxDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/it5cpntkptwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odxyb6/unlocc_universal_lossy_optical_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odxyb6/unlocc_universal_lossy_optical_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T08:37:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oebo07</id>
    <title>Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)</title>
    <updated>2025-10-23T18:50:17+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"&gt; &lt;img alt="Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)" src="https://external-preview.redd.it/NDJxZTFsN3lwd3dmMSkQIYjP_oFpJvmih5U0oEGvnjDWhMxIFYeX2zHmhGBL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8f010389c86ee8911fc6841ab3d654b84ceda1a" title="Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to see if Qwen3-VL could handle something simple: counting push-ups. If it can’t do that, it’s not ready to be a good trainer.&lt;/p&gt; &lt;p&gt;Overview:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on Gabber (will link repo)&lt;/li&gt; &lt;li&gt;Used Qwen3-VL for vision to tracks body position &amp;amp; reps&lt;/li&gt; &lt;li&gt;Cloned Ronnie Coleman’s voice for the trainer. That was… interesting.&lt;/li&gt; &lt;li&gt;Output = count my reps and gimme a “LIGHTWEIGHT BABY” every once in a while&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Took a lot of tweaking to get accurate rep counts&lt;/li&gt; &lt;li&gt;Some WEIRD voice hallucinations (Ronnie was going off lol)&lt;/li&gt; &lt;li&gt;Timing still a bit off between reps&lt;/li&gt; &lt;li&gt;Seems the model isn’t quite ready for useful real-time motion analysis or feedback, but it’s getting there&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pfn5nm7ypwwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T18:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe3wfs</id>
    <title>Virus Total integration on Hugging Face</title>
    <updated>2025-10-23T13:54:49+00:00</updated>
    <author>
      <name>/u/McPotates</name>
      <uri>https://old.reddit.com/user/McPotates</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt; &lt;img alt="Virus Total integration on Hugging Face" src="https://external-preview.redd.it/TwKiQ2XM7P28_0o53Sg5het24dh0s2bGVXdozQe9a5g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22b5835e6bc12ff826ab2939382c16ada8b641a6" title="Virus Total integration on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! We've just integrated Virus Total as security scanning partner. You should get a lot more AV scanners working on your files out of the box!&lt;br /&gt; Super happy to have them on board, curious to hear what yall think about this :)&lt;/p&gt; &lt;p&gt;FYI, we don't have all files scanned atm, should expand as more files are moved to xet (which gives us a sha256 out of the box, VT needs it to identify files).&lt;br /&gt; Also, only public files are scanned!&lt;/p&gt; &lt;p&gt;more info here: &lt;a href="https://huggingface.co/blog/virustotal"&gt;https://huggingface.co/blog/virustotal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5r3o1tpq9vwf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49b0cb3f1fc78589e0b8d36eaae8d773515e6101"&gt;https://preview.redd.it/5r3o1tpq9vwf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49b0cb3f1fc78589e0b8d36eaae8d773515e6101&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McPotates"&gt; /u/McPotates &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T13:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1odzuos</id>
    <title>ByteDance new release: Video-As-Prompt</title>
    <updated>2025-10-23T10:37:35+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odzuos/bytedance_new_release_videoasprompt/"&gt; &lt;img alt="ByteDance new release: Video-As-Prompt" src="https://external-preview.redd.it/NmtjemdueXlhdXdmMYm3iTnseSQvWv7pLtSTSL9kyuPriWa9dnRnXyWhtUoO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d57110b1dddf99fa2c1932f5645e1206f259411" title="ByteDance new release: Video-As-Prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Video-As-Prompt-Wan2.1-14B : &lt;a href="https://huggingface.co/ByteDance/Video-As-Prompt-Wan2.1-14B"&gt;HuggingFace link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video-As-Prompt-CogVideoX-5B : &lt;a href="https://huggingface.co/ByteDance/Video-As-Prompt-CogVideoX-5B"&gt;HuggingFace link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video-As-Prompt Core idea: Given a reference video with wanted semantics as a video prompt, Video-As-Prompt animate a reference image with the same semantics as the reference video. &lt;/p&gt; &lt;p&gt;Video-As-Prompt provides two variants, each with distinct trade-offs:&lt;/p&gt; &lt;p&gt;CogVideoX-I2V-5B Strengths: Fewer backbone parameters let us train more steps under limited resources, yielding strong stability on most semantic conditions. Limitations: Due to backbone ability limitation, it is weaker on human-centric generation and on concepts underrepresented in pretraining (e.g., ladudu, Squid Game, Minecraft).&lt;/p&gt; &lt;p&gt;Wan2.1-I2V-14B Strengths: Strong performance on human actions and novel concepts, thanks to a more capable base model. Limitations: Larger model size reduced feasible training steps given our resources, lowering stability on some semantic conditions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rkbtr0wyauwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odzuos/bytedance_new_release_videoasprompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odzuos/bytedance_new_release_videoasprompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T10:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe0y11</id>
    <title>I found a perfect coder model for my RTX4090+64GB RAM</title>
    <updated>2025-10-23T11:39:15+00:00</updated>
    <author>
      <name>/u/srigi</name>
      <uri>https://old.reddit.com/user/srigi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disappointed with vanilla Qwen3-coder-30B-A3B, I browsed models at mradermacher. I had a good experience with YOYO models in the past. I stumbled upon &lt;strong&gt;mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;First, I was a little worried that &lt;strong&gt;42B&lt;/strong&gt; won't fit, and offloading MoEs to CPU will result in poor perf. But thankfully, I was wrong.&lt;/p&gt; &lt;p&gt;Somehow this model consumed only about 8GB with &lt;code&gt;--cpu-moe&lt;/code&gt; (keep all Mixture of Experts weights on the CPU) and Q4_K_M, and 32k ctx. So I tuned llama.cpp invocation to fully occupy 24GB of RTX 4090 and put the rest into the CPU/RAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --model Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_K_M.gguf \ --ctx-size 102400 \ --flash-attn on \ --jinja \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --batch-size 1024 \ --ubatch-size 512 \ --n-cpu-moe 28 \ --n-gpu-layers 99 \ --repeat-last-n 192 \ --repeat-penalty 1.05 \ --threads 16 \ --host 0.0.0.0 \ --port 8080 \ --api-key secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With these settings, it eats 23400MB of VRAM and 30GB of RAM. It processes the RooCode's system prompt (around 16k tokens) in around 10s and generates at 44tk/s. With 100k context window.&lt;/p&gt; &lt;p&gt;And the best thing - the RooCode tool-calling is very reliable (vanilla Qwen3-coder failed at this horribly). This model can really code and is fast on a single RTX 4090!&lt;/p&gt; &lt;p&gt;Here is a 1 minute demo of adding a small code-change to medium sized &lt;a href="https://github.com/srigi/type-graphql"&gt;code-base&lt;/a&gt;: &lt;a href="https://i.postimg.cc/cHp8sP9m/Screen-Flow.gif"&gt;https://i.postimg.cc/cHp8sP9m/Screen-Flow.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srigi"&gt; /u/srigi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T11:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe13rg</id>
    <title>Qwen3 outperforming bigger LLMs at trading</title>
    <updated>2025-10-23T11:47:37+00:00</updated>
    <author>
      <name>/u/Christosconst</name>
      <uri>https://old.reddit.com/user/Christosconst</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"&gt; &lt;img alt="Qwen3 outperforming bigger LLMs at trading" src="https://preview.redd.it/7i46ukqanuwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2af3d3871761418ac44fa8e43516acb99b51653d" title="Qwen3 outperforming bigger LLMs at trading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Christosconst"&gt; /u/Christosconst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7i46ukqanuwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T11:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe7orf</id>
    <title>State of Open OCR models</title>
    <updated>2025-10-23T16:19:39+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks! it's Merve from Hugging Face 🫡&lt;/p&gt; &lt;p&gt;You might have noticed there has been many open OCR models released lately 😄 they're cheap to run compared to closed ones, some even run on-device&lt;/p&gt; &lt;p&gt;But it's hard to compare them and have a guideline on picking among upcoming ones, so we have broken it down for you in a blog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how to evaluate and pick an OCR model,&lt;/li&gt; &lt;li&gt;a comparison of the latest open-source models,&lt;/li&gt; &lt;li&gt;deployment tips,&lt;/li&gt; &lt;li&gt;and what’s next beyond basic OCR &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We hope it's useful for you! Let us know what you think: &lt;a href="https://huggingface.co/blog/ocr-open-models"&gt;https://huggingface.co/blog/ocr-open-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
