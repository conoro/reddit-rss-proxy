<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-12T16:37:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o4gi5j</id>
    <title>sm120 - is like everything gated? (Pre-training my own)</title>
    <updated>2025-10-12T05:05:42+00:00</updated>
    <author>
      <name>/u/exhorder72</name>
      <uri>https://old.reddit.com/user/exhorder72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me say that I’m new to this whole world of lm training and I’ve pretty much learned as I go. For a couple weeks now I’ve been working on a 1.8b param model just chugging along in pre training. I’ve done many a search for a better, more effective strat. Things I read about such as FA2/3, MXFP8/4, some Hopper stuff all seems gated. I set up a nightly torchao build in another venv and getting blocked all around. I mean, sm120 been out for some time, right? Here’s the most stable I’ve come up with to date. If anyone has any advice to share, I would love to hear it:&lt;/p&gt; &lt;p&gt;Ubuntu 22.04 (WSL2 on Win 11) PyTorch 2.8 + CUDA 12.8 / 13.0 drivers (5090 32gb) Transformer Engine 2.8 FP8 linears active cudaMallocAsync allocator enabled Doc-aware SDPA attention (efficient path, flash off) TE RMSNorm swap (+15 % throughput vs baseline) AdamW fused, D2Z LR schedule Training data ≈ 20 B tokens Nemotron HQ mixed with some Nemo Math, The Stack V2 and 2025 Wikipedia.&lt;/p&gt; &lt;p&gt;15 k tokens/s steady @ batch 4 × grad-accum 6, ctx = 2048, loss ≈ 0.7 → 0.5 about 10b tokens chewed on. Had a bad 30k run because for whatever reason I had one or both embed.weight and lm_head.weight tensors blow up on me and since I had them tied, that was a bad day. Since then, smooth sailing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exhorder72"&gt; /u/exhorder72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gi5j/sm120_is_like_everything_gated_pretraining_my_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gi5j/sm120_is_like_everything_gated_pretraining_my_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gi5j/sm120_is_like_everything_gated_pretraining_my_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T05:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ra1k</id>
    <title>Deleted Ollama, but it’s still running on my MacBook</title>
    <updated>2025-10-12T15:00:19+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Wafer81</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Wafer81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm going crazy. I deleted Ollama a few weeks ago to save my battery since it was draining almost all of it. I thought I had completely removed it, every last bit. Apparently not, because this popped up when I turned my MacBook on. Any idea how to fix this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Wafer81"&gt; /u/Puzzleheaded-Wafer81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ra1k/deleted_ollama_but_its_still_running_on_my_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ra1k/deleted_ollama_but_its_still_running_on_my_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ra1k/deleted_ollama_but_its_still_running_on_my_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T15:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4se74</id>
    <title>ComfyUI State of the Art</title>
    <updated>2025-10-12T15:43:51+00:00</updated>
    <author>
      <name>/u/tillybowman</name>
      <uri>https://old.reddit.com/user/tillybowman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;im working as a software dev in a mid sized company. i work with a dedicated AI team, implementing different business cases. &lt;/p&gt; &lt;p&gt;Our current focus and point of attention is really LLMs with text generation capabilities. We rarely needed to do image/video. &lt;/p&gt; &lt;p&gt;I want to get more attention to image generation in the company, especially all the things the default online models like dalle or sora. &lt;/p&gt; &lt;p&gt;things like: - style transfer - image restauration - pose transfer. &lt;/p&gt; &lt;p&gt;I want to give a presentation on comfyUI and what you can do with it. I barely know its capabilities, only by looking at this sub, so what i'm looking for is: - state of the art things you can do in comfy - workflows i can use for a good presentation - tutorials i can follow to implement something interesting &lt;/p&gt; &lt;p&gt;so i'm glad for any helpful links that will help me achieve to give a better presentation of comfy and local models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tillybowman"&gt; /u/tillybowman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4se74/comfyui_state_of_the_art/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4se74/comfyui_state_of_the_art/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4se74/comfyui_state_of_the_art/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T15:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ez13</id>
    <title>I made a plugin to run LLMs on phones</title>
    <updated>2025-10-12T03:39:04+00:00</updated>
    <author>
      <name>/u/Dragneel_passingby</name>
      <uri>https://old.reddit.com/user/Dragneel_passingby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I've been working on a side project to get LLMs (GGUF models) running locally on Android devices using Flutter.&lt;/p&gt; &lt;p&gt;The result is a plugin I'm calling Llama Flutter. It uses llama.cpp under the hood and lets you load any GGUF model from Hugging Face. I built a simple chat app as an example to test it.&lt;/p&gt; &lt;p&gt;I'm sharing this here because I'm looking for feedback from the community. Has anyone else tried building something similar? I'd be curious to know your thoughts on the approach, or any suggestions for improvement.&lt;/p&gt; &lt;p&gt;Video Demo: &lt;a href="https://files.catbox.moe/xrqsq2.mp4"&gt;https://files.catbox.moe/xrqsq2.mp4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example APK: &lt;a href="https://github.com/dragneel2074/Llama-Flutter/blob/master/example-app/app-release.apk"&gt;https://github.com/dragneel2074/Llama-Flutter/blob/master/example-app/app-release.apk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are some of the technical details / features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Uses the latest llama.cpp (as of Oct 2025) with ARM64 optimizations.&lt;/li&gt; &lt;li&gt; Provides a simple Dart API with real-time token streaming.&lt;/li&gt; &lt;li&gt; Supports a good range of generation parameters and several built-in chat templates.&lt;/li&gt; &lt;li&gt; For now, it's Android-only and focused on text generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're interested in checking it out to provide feedback or contribute, the links are below. If you find it useful, a star on GitHub would help me gauge interest.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;* GitHub Repo: &lt;a href="https://github.com/dragneel2074/Llama-Flutter"&gt;https://github.com/dragneel2074/Llama-Flutter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Plugin on pub.dev: &lt;a href="https://pub.dev/packages/llama%5C_flutter%5C_android"&gt;https://pub.dev/packages/llama\_flutter\_android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think? Is local execution of LLMs on mobile something you see a future for in Flutter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragneel_passingby"&gt; /u/Dragneel_passingby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T03:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mkve</id>
    <title>Local open source AI-sheets?</title>
    <updated>2025-10-12T11:24:50+00:00</updated>
    <author>
      <name>/u/SuddenWerewolf7041</name>
      <uri>https://old.reddit.com/user/SuddenWerewolf7041</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mkve/local_open_source_aisheets/"&gt; &lt;img alt="Local open source AI-sheets?" src="https://preview.redd.it/u9r363k61ouf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6801691f6a13702b411221cd785bd9fa2cb388da" title="Local open source AI-sheets?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any solution for local and open source AI that generates content based on an Excel sheet or preferably something web-based?&lt;/p&gt; &lt;p&gt;The use case is to generate content based on other column, try to fill gaps, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuddenWerewolf7041"&gt; /u/SuddenWerewolf7041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u9r363k61ouf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mkve/local_open_source_aisheets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mkve/local_open_source_aisheets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4pgh5</id>
    <title>Question about power-cheap and economical solution for selfhosting</title>
    <updated>2025-10-12T13:44:49+00:00</updated>
    <author>
      <name>/u/XenYaume</name>
      <uri>https://old.reddit.com/user/XenYaume</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I come here because after some research I am currrently thinking of self hosting AI but curious about the hardware to buy;&lt;/p&gt; &lt;p&gt;Originally, I wanted to buy a M1 Max with 32GB of RAM, put some LLM, After some research I am considering &lt;em&gt;Yahboom Jetson Orin Nano Super 8GB Development Board Kit 67TOP&lt;/em&gt; on one hand for my dev needs, running Ministral or Phi. and on one of my server (24GB of RAM) buying a &lt;em&gt;Google Coral USB&lt;/em&gt; for every other stuff which would mostly be &lt;em&gt;stupid questions that i want to be answered fast&lt;/em&gt; running LLama-7B or some fork, which i would share with my gf.&lt;/p&gt; &lt;p&gt;I want to prioritize power consumption, my budget is around 1k EUR, which is the price I could get a M1 Max with 32GB of RAM, second hand.&lt;/p&gt; &lt;p&gt;My question is, what would be better for such budget with power consumption first&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XenYaume"&gt; /u/XenYaume &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4pgh5/question_about_powercheap_and_economical_solution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4pgh5/question_about_powercheap_and_economical_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4pgh5/question_about_powercheap_and_economical_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T13:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4pk9x</id>
    <title>Tracking MCP Server Growth: 1,150+ servers and climbing</title>
    <updated>2025-10-12T13:49:26+00:00</updated>
    <author>
      <name>/u/malderson</name>
      <uri>https://old.reddit.com/user/malderson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4pk9x/tracking_mcp_server_growth_1150_servers_and/"&gt; &lt;img alt="Tracking MCP Server Growth: 1,150+ servers and climbing" src="https://external-preview.redd.it/8Cu81YajYCaT4p00UYebB_W6eiURi1xEFXqNrO8eoW0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e74dae782f8b7043cfc210e3929a58d3e1abbbb1" title="Tracking MCP Server Growth: 1,150+ servers and climbing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malderson"&gt; /u/malderson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://martinalderson.com/posts/tracking-mcp-server-growth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4pk9x/tracking_mcp_server_growth_1150_servers_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4pk9x/tracking_mcp_server_growth_1150_servers_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T13:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4av71</id>
    <title>LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</title>
    <updated>2025-10-12T00:08:28+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: &lt;a href="https://github.com/rbalestr-lab/llm-jepa"&gt;this https URL&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Limitations &lt;/p&gt; &lt;p&gt;Despite its strong accuracy gains, LLM-JEPA introduces two additional hyperparameters. As shown in fig. 7, the optimal configuration may occur at any point in a grid (λ, k), which imposes a significant cost for hyperparameter tuning. While we have not identified an efficient method to explore this space, we empirically observe that adjacent grid points often yield similar accuracy, suggesting the potential for a more efficient tuning algorithm.&lt;/p&gt; &lt;p&gt;The primary bottleneck at present is the 2-fold increase in compute cost during training, which is mitigated by random loss dropout.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.14252"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4av71/llmjepa_large_language_models_meet_joint/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4av71/llmjepa_large_language_models_meet_joint/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T00:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4im2n</id>
    <title>I built an open-source repo to learn and apply AI Agentic Patterns</title>
    <updated>2025-10-12T07:15:09+00:00</updated>
    <author>
      <name>/u/learnwithparam</name>
      <uri>https://old.reddit.com/user/learnwithparam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone 👋&lt;/p&gt; &lt;p&gt;I’ve been experimenting with how AI agents actually &lt;em&gt;work in production&lt;/em&gt; — beyond simple prompt chaining. So I created an &lt;strong&gt;open-source project&lt;/strong&gt; that demonstrates &lt;strong&gt;30+ AI Agentic Patterns&lt;/strong&gt;, each in a single, focused file.&lt;/p&gt; &lt;p&gt;Each pattern covers a core concept like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt Chaining&lt;/li&gt; &lt;li&gt;Multi-Agent Coordination&lt;/li&gt; &lt;li&gt;Reflection &amp;amp; Self-Correction&lt;/li&gt; &lt;li&gt;Knowledge Retrieval&lt;/li&gt; &lt;li&gt;Workflow Orchestration&lt;/li&gt; &lt;li&gt;Exception Handling&lt;/li&gt; &lt;li&gt;Human-in-the-loop&lt;/li&gt; &lt;li&gt;And more advanced ones like Recursive Agents &amp;amp; Code Execution&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ Works with OpenAI, Gemini, Claude, Fireworks AI, Mistral, and even &lt;strong&gt;Ollama&lt;/strong&gt; for local runs.&lt;br /&gt; ✅ Each file is self-contained — perfect for learning or extending.&lt;br /&gt; ✅ Open for contributions, feedback, and improvements!&lt;/p&gt; &lt;p&gt;You can check the full list and examples in the README here:&lt;br /&gt; 🔗 &lt;a href="https://github.com/learnwithparam/ai-agents-pattern"&gt;https://github.com/learnwithparam/ai-agents-pattern&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love your feedback — especially on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Missing patterns worth adding&lt;/li&gt; &lt;li&gt;Ways to make it more beginner-friendly&lt;/li&gt; &lt;li&gt;Real-world examples to expand&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let’s make AI agent design patterns as clear and reusable as software design patterns once were.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/learnwithparam"&gt; /u/learnwithparam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4im2n/i_built_an_opensource_repo_to_learn_and_apply_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4im2n/i_built_an_opensource_repo_to_learn_and_apply_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4im2n/i_built_an_opensource_repo_to_learn_and_apply_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T07:15:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o44u78</id>
    <title>We know the rule of thumb… large quantized models outperform smaller less quantized models, but is there a level where that breaks down?</title>
    <updated>2025-10-11T19:43:54+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ask because I’ve also heard quants below 4 bit are less effective, and that rule of thumb always seemed to compare 4bit large vs 8bit small.&lt;/p&gt; &lt;p&gt;As an example let’s take the large GLM 4.5 vs GLM 4.5 Air. You can have a much higher bitrate with GLM 4.5 Air… but… even with a 2bit quant made by unsloth, GLM 4.5 does quite well for me. &lt;/p&gt; &lt;p&gt;I haven’t figured out a great way to have complete confidence though so I thought I’d ask you all. What’s your rule of thumb when having to weigh a smaller model vs larger model at different quants? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T19:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3opq5</id>
    <title>What the sub feels like lately</title>
    <updated>2025-10-11T06:47:33+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt; &lt;img alt="What the sub feels like lately" src="https://preview.redd.it/92s8znbxifuf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4866bff0d572386ea47fc19d643a6b2261fbdb" title="What the sub feels like lately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92s8znbxifuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mwlp</id>
    <title>How do you benchmark the cognitive performance of local LLM models?</title>
    <updated>2025-10-12T11:43:18+00:00</updated>
    <author>
      <name>/u/LastikPlastic</name>
      <uri>https://old.reddit.com/user/LastikPlastic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’ve been experimenting with running local LLMs (mainly open-weight models from Hugging Face) and I’m curious about &lt;strong&gt;how to systematically benchmark their cognitive performance&lt;/strong&gt; — not just speed or token throughput, but things like reasoning, memory, comprehension, and factual accuracy.&lt;/p&gt; &lt;p&gt;I know about &lt;code&gt;lm-evaluation-harness&lt;/code&gt;, but it’s pretty cumbersome to run manually for each model. I’m wondering if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;there’s any &lt;strong&gt;online tool or web interface&lt;/strong&gt; that can run multiple benchmarks automatically (similar to Hugging Face’s Open LLM Leaderboard, but for local models), or&lt;/li&gt; &lt;li&gt;a more &lt;strong&gt;user-friendly script or framework&lt;/strong&gt; that can test reasoning / logic / QA performance locally without too much setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any suggestions, tools, or workflows you’d recommend?&lt;br /&gt; Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LastikPlastic"&gt; /u/LastikPlastic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwlp/how_do_you_benchmark_the_cognitive_performance_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwlp/how_do_you_benchmark_the_cognitive_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwlp/how_do_you_benchmark_the_cognitive_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:43:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o43qhn</id>
    <title>What rig are you running to fuel your LLM addiction?</title>
    <updated>2025-10-11T18:59:15+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post your shitboxes, H100's, nvidya 3080ti's, RAM-only setups, MI300X's, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4m7yt</id>
    <title>LM Studio no new runtimes since weeks..?</title>
    <updated>2025-10-12T11:04:23+00:00</updated>
    <author>
      <name>/u/therealAtten</name>
      <uri>https://old.reddit.com/user/therealAtten</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pardon the hyperbole and sorry to bother, but since the release of GLM-4.6 on Oct. 30 (that's fourteen days, or two weeks ago), I have been checking daily on LM Studio whether new Runtimes are provided to finally run the successsor to my favourite model, GLM-4.5. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw4sv6/unsloth_glm46_gguf_doesnt_work_in_lm_studio/"&gt;I was told&lt;/a&gt; their current runtime v1.52.1 is based on llama.cpp's b6651, with b6653 (just two releases later) adding support for GLM-4.6. Meanwhile as of writing, llama.cpp is on release b6739.&lt;/p&gt; &lt;p&gt;@ LM Studio, thank you so much for your &lt;em&gt;amazing&lt;/em&gt; platform, and sorry that we cannot contribute to your incessant efforts in proliferating Local LLMs. (obligatory &amp;quot;open-source when?&amp;quot;)&lt;br /&gt; I sincerely hope you are doing alright...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therealAtten"&gt; /u/therealAtten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m7yt/lm_studio_no_new_runtimes_since_weeks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m7yt/lm_studio_no_new_runtimes_since_weeks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m7yt/lm_studio_no_new_runtimes_since_weeks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mwtv</id>
    <title>I benchmarked my Redmagic 9 Pro phone, initially to find out whether the BLAS batch size parameter had an observable effect on performance, and got some interesting results.</title>
    <updated>2025-10-12T11:43:37+00:00</updated>
    <author>
      <name>/u/PurpleWinterDawn</name>
      <uri>https://old.reddit.com/user/PurpleWinterDawn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwtv/i_benchmarked_my_redmagic_9_pro_phone_initially/"&gt; &lt;img alt="I benchmarked my Redmagic 9 Pro phone, initially to find out whether the BLAS batch size parameter had an observable effect on performance, and got some interesting results." src="https://b.thumbs.redditmedia.com/G1HJLM9wP3Xt7l35pbv_SP4NqP7-XzCxO4A5l25VamM.jpg" title="I benchmarked my Redmagic 9 Pro phone, initially to find out whether the BLAS batch size parameter had an observable effect on performance, and got some interesting results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Phone maker and model: Redmagic 9 Pro 512/16GB, released end of Dec. 2023.&lt;/p&gt; &lt;p&gt;Results :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Basically a wash on prompt processing speeds ;&lt;/li&gt; &lt;li&gt;Some interesting results on the 100 tokens generations, including massive outliers I have no explanation for ;&lt;/li&gt; &lt;li&gt;Going from 3840 to 4096 context window sizes increased the PP and generation speeds slightly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Notes :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ran on Termux, KoboldCpp compiled on-device ;&lt;/li&gt; &lt;li&gt;This is the Unsloth Q4_0 quant ;&lt;/li&gt; &lt;li&gt;100% battery. Power consumption stood at around 7.5 to 9W at the wall, factory phone charger losses included ;&lt;/li&gt; &lt;li&gt;Choice of number of threads: going from 3 to 6 threads registered a great boost in speeds, while 7 threads halved the results obtained at 6 threads. 8 threads not tested. Hypothesis: all cores run at the same frequency, and the slowest cores slow the rest too much to be worth adding to the process. KoboldCpp notes &amp;quot;6 threads and 6 BLAS threads&amp;quot; were spawned ;&lt;/li&gt; &lt;li&gt;Choice of quant: Q4_0 allows using the Llama.cpp improvements for ARM with memory interleaving, increasing performance ; I have observed Q4_K_M models running single-digit speeds at under 1k context window usage ;&lt;/li&gt; &lt;li&gt;Choice of KV quant: Q8 was basically for the compromise on memory usage, considering the device used. I only evaluated whether the model was coherent on a random topic repeatedly (&amp;quot;A wolf has entered my house, what do I do? AI: &amp;lt;insert short response here&amp;gt; User: Thank you. Any other advice? AI: &amp;lt;insert 240+ tokens response here&amp;gt;&amp;quot;) before using it for the benchmark ;&lt;/li&gt; &lt;li&gt;FlashAttention: this one I was divided on, but settled on using it because KoboldCpp highly discourages using QuantKV without it, citing possible higher memory usage than without QuantKV at all ;&lt;/li&gt; &lt;li&gt;I highly doubt KoboldCpp uses the Qualcomm Hexagon NPU at all ; it didn't use the integrated GPU either, as trying to compile with LLAMA_VULKAN=1 failed ;&lt;/li&gt; &lt;li&gt;htop reported RAM usage went up from 8.20GB to 10.90GB which corresponds to the model size, while KoboldCpp reported 37.72MiB for llama_context at 4096 context window. I'm surprised by this &amp;quot;small&amp;quot; memory footprint for the context.&lt;/li&gt; &lt;li&gt;This benchmark session took the better time of 8 hours ;&lt;/li&gt; &lt;li&gt;While the memory footprint of the context allowed for testing larger context windows, going all the way to 8192 context window size would take an inordinate amount of time to benchmark.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you think other parameters can improve those charts, I'll be happy to try a few of them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleWinterDawn"&gt; /u/PurpleWinterDawn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o4mwtv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwtv/i_benchmarked_my_redmagic_9_pro_phone_initially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwtv/i_benchmarked_my_redmagic_9_pro_phone_initially/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4p3vj</id>
    <title>Very interesting! OmniInsert — mask-free video insertion of any reference</title>
    <updated>2025-10-12T13:29:16+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New diffusion-transformer method that inserts a referenced subject into a source video &lt;strong&gt;without masks&lt;/strong&gt;, with robust demos and a technique report. Paper + project page are live; repo is up—&lt;strong&gt;eager to test once code &amp;amp; weights drop&lt;/strong&gt;. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Highlights: InsertPipe data pipeline, condition-specific feature injection, progressive training; introduces &lt;strong&gt;InsertBench&lt;/strong&gt;. &lt;a href="https://arxiv.org/abs/2509.17627?utm_source=chatgpt.com"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Status: Apache-2.0 repo; no releases yet; open issue requesting HF models/dataset; arXiv says “code will be released.” &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://phantom-video.github.io/OmniInsert/"&gt;&lt;strong&gt;https://phantom-video.github.io/OmniInsert/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4p3vj/very_interesting_omniinsert_maskfree_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4p3vj/very_interesting_omniinsert_maskfree_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4p3vj/very_interesting_omniinsert_maskfree_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T13:29:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4szf0</id>
    <title>Paper2Video — turn a research paper into a full presentation video (slides, speech, talking head)</title>
    <updated>2025-10-12T16:06:41+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"&gt; &lt;img alt="Paper2Video — turn a research paper into a full presentation video (slides, speech, talking head)" src="https://external-preview.redd.it/14WZnO_akEyJj42YD4L-nD--_D0Ep8o7tXNC2svzt48.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=604ef91a15e04ea2ea32a47e9edd6696a8318d46" title="Paper2Video — turn a research paper into a full presentation video (slides, speech, talking head)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Multi-agent pipeline (“PaperTalker”) that takes a paper + reference &lt;strong&gt;image/audio&lt;/strong&gt; and outputs a polished &lt;strong&gt;presentation video&lt;/strong&gt; (Slides → Subtitles → Speech → Cursor → Talking-Head). &lt;strong&gt;MIT&lt;/strong&gt; licensed, code + benchmark out. &lt;a href="https://github.com/showlab/Paper2Video"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One-command run via &lt;a href="http://pipeline.py"&gt;&lt;code&gt;pipeline.py&lt;/code&gt;&lt;/a&gt;; set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; / &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; (best: GPT-4.1 or Gemini 2.5). Depends on Hallo2 + Paper2Poster. &lt;/li&gt; &lt;li&gt;Recommended: &lt;strong&gt;A6000 48GB&lt;/strong&gt; for end-to-end generation.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Benchmark (&lt;strong&gt;101&lt;/strong&gt; paper–video pairs) + metrics: Meta Similarity, PresentArena, PresentQuiz, IP Memory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b4nd5tfmfpuf1.png?width=835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90777151264bb001c851e64669dcb7b6baae186e"&gt;https://preview.redd.it/b4nd5tfmfpuf1.png?width=835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90777151264bb001c851e64669dcb7b6baae186e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T16:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4l7qk</id>
    <title>I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus</title>
    <updated>2025-10-12T10:02:47+00:00</updated>
    <author>
      <name>/u/alone_musk18</name>
      <uri>https://old.reddit.com/user/alone_musk18</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"&gt; &lt;img alt="I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus" src="https://preview.redd.it/1rpwzkgqmnuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afa5a5bad4ea1892710d6879e8e3673370dee73e" title="I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alone_musk18"&gt; /u/alone_musk18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1rpwzkgqmnuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T10:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4fxer</id>
    <title>PSA: Ollama no longer supports the Mi50 or Mi60</title>
    <updated>2025-10-12T04:32:11+00:00</updated>
    <author>
      <name>/u/TechEnthusiastx86</name>
      <uri>https://old.reddit.com/user/TechEnthusiastx86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ollama/ollama/pull/12481"&gt;https://github.com/ollama/ollama/pull/12481&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama recently upgraded its ROCM version and therefore no longer supports the Mi50 or Mi60.&lt;/p&gt; &lt;p&gt;Their most recent release notes states that &amp;quot;AMD gfx900 and gfx906 (MI50, MI60, etc) GPUs are no longer supported via ROCm. We're working to support these GPUs via Vulkan in a future release.&amp;quot; &lt;/p&gt; &lt;p&gt;This means if you pull the latest version of Ollama you won't be able to use the Mi50 even though Ollama docs still list it as being supported.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechEnthusiastx86"&gt; /u/TechEnthusiastx86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T04:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4sgv5</id>
    <title>Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry</title>
    <updated>2025-10-12T15:46:43+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"&gt; &lt;img alt="Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry" src="https://external-preview.redd.it/brnT1-CiL694NH_ogGrkVOnYdebEpEkUcEFq_sappRI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d425501d8da63b539406ec0adae7cd9c361ea22d" title="Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=r0SalROzO38"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T15:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4hxqe</id>
    <title>KoboldCpp now supports video generation</title>
    <updated>2025-10-12T06:32:40+00:00</updated>
    <author>
      <name>/u/fish312</name>
      <uri>https://old.reddit.com/user/fish312</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"&gt; &lt;img alt="KoboldCpp now supports video generation" src="https://external-preview.redd.it/n7QpKCCkcHUBLj4nC-Lh95amFG6mzdqatT5L5ZA_y1k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f4fc22015a57b49dec0643ef6f0d2a92f83c37" title="KoboldCpp now supports video generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fish312"&gt; /u/fish312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T06:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4qix3</id>
    <title>Claude's system prompt length has now exceeded 30k tokens</title>
    <updated>2025-10-12T14:29:52+00:00</updated>
    <author>
      <name>/u/StableSable</name>
      <uri>https://old.reddit.com/user/StableSable</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"&gt; &lt;img alt="Claude's system prompt length has now exceeded 30k tokens" src="https://external-preview.redd.it/otAtlKXoVGIzRX_D-XS8ef102ismRuSmY-rYGjCWHEI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=345f8e8a9693f1ecf3f281e2c9b37a5656e8634f" title="Claude's system prompt length has now exceeded 30k tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableSable"&gt; /u/StableSable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/asgeirtj/system_prompts_leaks/blob/main/Anthropic/claude-4.5-sonnet.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T14:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4dswr</id>
    <title>HuggingFace storage is no longer unlimited - 12TB public storage max</title>
    <updated>2025-10-12T02:36:39+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case you’ve missed the memo like me, HuggingFace is no longer unlimited.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Type of account&lt;/th&gt; &lt;th&gt;Public storage&lt;/th&gt; &lt;th&gt;Private storage&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Free user or org&lt;/td&gt; &lt;td&gt;Best-effort* usually up to 5 TB for impactful work&lt;/td&gt; &lt;td&gt;100 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;PRO&lt;/td&gt; &lt;td&gt;Up to 10 TB included* ✅ grants available for impactful work†&lt;/td&gt; &lt;td&gt;1 TB + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Team Organizations&lt;/td&gt; &lt;td&gt;12 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Enterprise Organizations&lt;/td&gt; &lt;td&gt;500 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As seen on &lt;a href="https://huggingface.co/docs/hub/en/storage-limits"&gt;https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, they started enforcing it.&lt;/p&gt; &lt;p&gt;—-&lt;/p&gt; &lt;p&gt;For ref. &lt;a href="https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits"&gt;https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T02:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mdkh</id>
    <title>Why has Meta research failed to deliver foundational model at the level of Grok, Deepseek or GLM?</title>
    <updated>2025-10-12T11:13:22+00:00</updated>
    <author>
      <name>/u/External_Natural9590</name>
      <uri>https://old.reddit.com/user/External_Natural9590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They have been in the space for longer - could have atracted talent earlier, their means are comparable to ther big tech. So why have they been outcompeted so heavily? I get they are currently a one generation behind and the chinese did some really clever wizardry which allowed them to squeeze a lot more eke out of every iota. But what about xAI? They compete for the same talent and had to start from the scratch. Or was starting from the scratch actually an advantage here? Or is it just a matter of how many key ex OpenAI employees was each company capable of attracting - trafficking out the trade secrets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Natural9590"&gt; /u/External_Natural9590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mwet</id>
    <title>GPU Poor LLM Arena is BACK! 🎉🎊🥳</title>
    <updated>2025-10-12T11:43:02+00:00</updated>
    <author>
      <name>/u/kastmada</name>
      <uri>https://old.reddit.com/user/kastmada</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt; &lt;img alt="GPU Poor LLM Arena is BACK! 🎉🎊🥳" src="https://external-preview.redd.it/xnvppfD8q4Rvrqs00KT2LLxfAKmO_ypt1REhqFgxlVw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49fb1bf881d9a00b0e731a0269d44b4ea6c31968" title="GPU Poor LLM Arena is BACK! 🎉🎊🥳" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;🚀 GPU Poor LLM Arena is BACK! New Models &amp;amp; Updates!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;First off, a massive apology for the extended silence. Things have been a bit hectic, but the GPU Poor LLM Arena is officially back online and ready for action! Thanks for your patience and for sticking around.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🚀 Newly Added Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Granite 4.0 Small Unsloth (32B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Tiny Unsloth (7B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Micro Unsloth (3B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Thinking 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (30B, 4-bit)&lt;/li&gt; &lt;li&gt;OpenAI gpt-oss Unsloth (20B, 4-bit)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;🚨 Important Notes for GPU-Poor Warriors:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Please be aware that Granite 4.0 Small, Qwen 3 30B, and OpenAI gpt-oss models are quite bulky. Ensure your setup can comfortably handle them before diving in to avoid any performance issues.&lt;/li&gt; &lt;li&gt;I've decided to default to Unsloth GGUFs for now. In many cases, these offer valuable bug fixes and optimizations over the original GGUFs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to see you back in the arena, testing out these new additions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kastmada"&gt; /u/kastmada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
