<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-13T23:24:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qc6gbr</id>
    <title>How to use AI locally to get ahead in a workplace that‚Äôs rolling out AI</title>
    <updated>2026-01-13T23:03:46+00:00</updated>
    <author>
      <name>/u/Prinzen2</name>
      <uri>https://old.reddit.com/user/Prinzen2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I just built my own Nvidia RTX 16 GB GPU powered system. Idea was to test and develop software, scripts etc locally and then see introducing improvements to the company I work for. Initially I planned to use my local system to extract data from our vast repository of pdf documents, and build processes around having said data ready for piping into various tools to get the output we need. &lt;/p&gt; &lt;p&gt;But I got worried about data ex filtration so parked the idea. &lt;/p&gt; &lt;p&gt;I then thought I could leverage my years of Python experience (I‚Äôm not a pro coder nor is my company in the IT sector, but I had gotten some qualifications on Python years back) and use my AI machine for developing other projects. &lt;/p&gt; &lt;p&gt;However ; the company I work for is rolling out CoPilot with its agents etc to across the board and running a big training campaign for everyone. &lt;/p&gt; &lt;p&gt;Ultimate goal is to leverage my domain knowledge plus python / general IT knowledge to gain and advantage and further my career. &lt;/p&gt; &lt;p&gt;But now that gap I had over everyone else in the office is closing rapidly. &lt;/p&gt; &lt;p&gt;Just looking for some general advice and wondering where do I go from here ?&lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prinzen2"&gt; /u/Prinzen2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc6gbr/how_to_use_ai_locally_to_get_ahead_in_a_workplace/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc6gbr/how_to_use_ai_locally_to_get_ahead_in_a_workplace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc6gbr/how_to_use_ai_locally_to_get_ahead_in_a_workplace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T23:03:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc0w13</id>
    <title>What actually breaks when AI agents move from demos into real production workflows</title>
    <updated>2026-01-13T19:34:35+00:00</updated>
    <author>
      <name>/u/saurabhjain1592</name>
      <uri>https://old.reddit.com/user/saurabhjain1592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have been building and evaluating agent-based systems in real production contexts, and one pattern keeps repeating.&lt;/p&gt; &lt;p&gt;The failures are rarely about model quality.&lt;/p&gt; &lt;p&gt;They tend to show up once workflows become multi-step and stateful: retries with side effects, partial execution, permission boundaries across tools, and the inability to answer ‚Äúwhat exactly happened‚Äù after the fact.&lt;/p&gt; &lt;p&gt;A lot of this feels less like an AI problem and more like classic distributed systems failure modes, just amplified by agent autonomy and non-determinism.&lt;/p&gt; &lt;p&gt;I am curious how people here are handling execution control, auditability, and safe failure once agents are allowed to touch real systems.&lt;/p&gt; &lt;p&gt;There is also a longer discussion in a different format for anyone interested: &lt;a href="https://news.ycombinator.com/item?id=46603800"&gt;https://news.ycombinator.com/item?id=46603800&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saurabhjain1592"&gt; /u/saurabhjain1592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc0w13/what_actually_breaks_when_ai_agents_move_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc0w13/what_actually_breaks_when_ai_agents_move_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc0w13/what_actually_breaks_when_ai_agents_move_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T19:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb034t</id>
    <title>GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models</title>
    <updated>2026-01-12T16:49:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/Engram/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T16:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc34r2</id>
    <title>How non-vision LLM handle image vision ?</title>
    <updated>2026-01-13T20:57:52+00:00</updated>
    <author>
      <name>/u/Individual-Source618</name>
      <uri>https://old.reddit.com/user/Individual-Source618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, thanks for all your valuable and very interesting posts on this sub.&lt;/p&gt; &lt;p&gt;It's my 1st post. I was wondering how do non-vision LLMs such as Deepseek v3.2 or GLM-4.7 handle images visions/understanding despite not being multimodal ?&lt;/p&gt; &lt;p&gt;Thank you for your help&lt;/p&gt; &lt;p&gt;EDIT : could the recent Qwen3-VL-embedding be of any help ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Source618"&gt; /u/Individual-Source618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc34r2/how_nonvision_llm_handle_image_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc34r2/how_nonvision_llm_handle_image_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc34r2/how_nonvision_llm_handle_image_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T20:57:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbgdu2</id>
    <title>OSS Alternative to Glean</title>
    <updated>2026-01-13T03:21:07+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"&gt; &lt;img alt="OSS Alternative to Glean" src="https://external-preview.redd.it/cmU5Y2xuYnFiMWRnMWWIQZ2CyIf_Xrmm-Z03F9XkK4MxpC4ND6bEYAzhiTDs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e2103787259340396f1676074d239180e73672b" title="OSS Alternative to Glean" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;In short, Connect any LLM to your internal knowledge sources (Search Engines, Drive, Calendar, Notion and 15+ other connectors) and chat with it in real time alongside your team.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep Agentic Agent&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Local TTS/STT support.&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi Collaborative Chats&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents&lt;/li&gt; &lt;li&gt;Real Time Features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick Start (without oauth connectors)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y63zrbbqb1dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T03:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc5f0q</id>
    <title>What happens when you load two models and let each model take a turn generating a token?</title>
    <updated>2026-01-13T22:23:04+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To really make sure there is no misunderstanding here it is played out:&lt;/p&gt; &lt;p&gt;I like eating hotdogs.&lt;/p&gt; &lt;p&gt;Model 1: I, eat, hot&lt;/p&gt; &lt;p&gt;Model2: like,ing, dogs. &lt;/p&gt; &lt;p&gt;This is a simulation to demonstrate the idea.&lt;/p&gt; &lt;p&gt;So why? And is it worth it?&lt;/p&gt; &lt;p&gt;The first thought that came my mind was clearly it will be slower‚Ä¶ but I wondered if a few adjustments to the software could ensure the context isn‚Äôt fully reprocessed for each model each time.&lt;/p&gt; &lt;p&gt;My next thought was how would two different model families handle this? For example GPT-OSS 120b and GLM-4.6V? What happens when the east meets west? &lt;/p&gt; &lt;p&gt;What happens if you always did inference on a smaller model, but only used it when it predicted the next word with high confidence and/or it was a common word (the, a, an, has, etc.) from the top 200 English words? Would this be faster than a draft model with a larger model and how much less accurate would it be? &lt;/p&gt; &lt;p&gt;One idea that came to mind is the fingerprint of the models would get muddied. How muddied? Only one way to find out. &lt;/p&gt; &lt;p&gt;And here you might get a little grumpy. I‚Äôm still at work and my knowledge to accomplish this is pretty narrow so I can‚Äôt give you this answer‚Ä¶ yet. But a helpful upvote and a comment from you should get this some visibility so that those that have done this or have the knowledge to do so can beat me to providing you and I with an answer. &lt;/p&gt; &lt;p&gt;Have you done something wacky like this? Love to hear your experiences along my these lines. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5f0q/what_happens_when_you_load_two_models_and_let/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5f0q/what_happens_when_you_load_two_models_and_let/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5f0q/what_happens_when_you_load_two_models_and_let/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbm7f4</id>
    <title>Gemma 3 1B qat q4_0 gguf without imatrix and (hopefully) correct metadata</title>
    <updated>2026-01-13T08:39:42+00:00</updated>
    <author>
      <name>/u/Big-Tune-190</name>
      <uri>https://old.reddit.com/user/Big-Tune-190</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm7f4/gemma_3_1b_qat_q4_0_gguf_without_imatrix_and/"&gt; &lt;img alt="Gemma 3 1B qat q4_0 gguf without imatrix and (hopefully) correct metadata" src="https://external-preview.redd.it/FWZ9aA8J9mRVVh8qfDpYOFQpE66ZT01z7wbVnO9_J7w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb710747a9555867465a936ab4df4e692fa30d18" title="Gemma 3 1B qat q4_0 gguf without imatrix and (hopefully) correct metadata" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since this is my very first post here, I would like to apologize in advance if I make any content-related or semantic errors in creating this post (or if it might be irrelevant) and I am grateful for constructive feedback.&lt;/p&gt; &lt;p&gt;TL;DR; (model card)&lt;/p&gt; &lt;p&gt;&lt;code&gt;Q4_0&lt;/code&gt; quantized version of &lt;code&gt;google/gemma-3-1b-it-qat-q4_0-unquantized&lt;/code&gt;, which differs from existing quantizations in the following aspects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;smaller and therefore faster than the original &lt;code&gt;google/gemma-3-1b-it-qat-q4_0-gguf&lt;/code&gt;&lt;/li&gt; &lt;li&gt;quantization without imatrix to avoid interactions with already QAT optimized Q4_0 weights&lt;/li&gt; &lt;li&gt;various fixes regarding model metadata &lt;ul&gt; &lt;li&gt;added &lt;code&gt;tokenizer.ggml.eot_token_id = 106&lt;/code&gt; (&lt;code&gt;&amp;lt;end_of_turn&amp;gt;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;make &lt;code&gt;&amp;lt;start_of_image&amp;gt;&lt;/code&gt; type &lt;code&gt;CONTROL&lt;/code&gt;&lt;/li&gt; &lt;li&gt;make &lt;code&gt;&amp;lt;end_of_image&amp;gt;&lt;/code&gt; type &lt;code&gt;CONTROL&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Created with &lt;code&gt;llama.cpp&lt;/code&gt; &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; release &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7699"&gt;b7699&lt;/a&gt; based on &lt;a href="https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-unquantized/tree/a6692c1945954f4aa39a17b8dfba4a7e62db3d4f"&gt;google/gemma-3-1b-it-qat-q4_0-unquantized@a6692c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inspired by ideas and discussions around &lt;a href="https://huggingface.co/stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small"&gt;stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some more context (why this might be important for others)&lt;/p&gt; &lt;p&gt;I just wanted to briefly inform you that I have provided a new GGUF quantization for the &lt;code&gt;qat-q4_0&lt;/code&gt; snapshot of &lt;code&gt;gemma-3-1b-it&lt;/code&gt;. The reason for this was that I had not found a ready-made GGUF quantization for &lt;code&gt;google/gemma-3-1b-it-qat-q4_0&lt;/code&gt;that was quantized both with correct metadata on one hand and without the use of an imatrix on the other.&lt;/p&gt; &lt;p&gt;Regarding metadata, there has often been an issue in the past with QAT versions of Gemma 3 GGUF where the &lt;code&gt;&amp;lt;end_of_turn&amp;gt;&lt;/code&gt; token was not set in the model metadata, with only &lt;code&gt;&amp;lt;eos&amp;gt;&lt;/code&gt; appearing there instead. There are also quantizations that incorrectly declare certain tokens as &lt;code&gt;USER_DEFINED&lt;/code&gt;, even though they are probably &lt;code&gt;CONTROL&lt;/code&gt; tokens (like &lt;code&gt;&amp;lt;start_of_image&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;end_of_image&amp;gt;&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;Furthermore, it is questionable whether using an importance matrix (imatrix) during the quantization of a QAT snapshot is truly helpful or if it might even have a negative effect. For this reason, I wanted to create a quantization that explicitly functions without the use of an imatrix.&lt;/p&gt; &lt;p&gt;In summary, this is a GGUF Q4_0 quantization of &lt;code&gt;google/gemma-3-1b-it-qat-q4_0-unquantized&lt;/code&gt; without the use of an imatrix and with corrected metadata.&lt;/p&gt; &lt;p&gt;Since I searched for such a version for a long time myself and ultimately decided to create it on my own, I thought this might also be helpful for others, especially since, in my opinion, the very small 1B variant of Gemma 3 is somehow sensitive when it comes to quantization and metadata.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Tune-190"&gt; /u/Big-Tune-190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/msievers/gemma-3-1b-it-qat-q4_0-gguf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm7f4/gemma_3_1b_qat_q4_0_gguf_without_imatrix_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm7f4/gemma_3_1b_qat_q4_0_gguf_without_imatrix_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T08:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbqazx</id>
    <title>MCP, A2A, ACP, UCP - are we sleepwalking into another "standards" war controlled by the same companies?</title>
    <updated>2026-01-13T12:42:06+00:00</updated>
    <author>
      <name>/u/PutPurple844</name>
      <uri>https://old.reddit.com/user/PutPurple844</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic has MCP. Google has A2A. OpenAI has ACP. Google just dropped UCP for commerce.&lt;/p&gt; &lt;p&gt;They're all &amp;quot;open&amp;quot;, but let's be real - the specs are written by the big labs.&lt;/p&gt; &lt;p&gt;Linux Foundation launched AAIF to govern all of this. Founding members? Anthropic, OpenAI, Google, Microsoft. The same players.&lt;/p&gt; &lt;p&gt;MCP is probably the most useful one for local setups - tool connections work regardless of what model you're running. But A2A and the commerce protocols assume you're hitting hosted APIs.&lt;/p&gt; &lt;p&gt;Anyone here running MCP servers with local models? Curious how the auth story works when there's no cloud identity provider in the loop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PutPurple844"&gt; /u/PutPurple844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqazx/mcp_a2a_acp_ucp_are_we_sleepwalking_into_another/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqazx/mcp_a2a_acp_ucp_are_we_sleepwalking_into_another/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqazx/mcp_a2a_acp_ucp_are_we_sleepwalking_into_another/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc1isk</id>
    <title>RTX 6000 Pro (Blackwell) Wouldn‚Äôt POST on MSI Z790-P Pro [FIXED]</title>
    <updated>2026-01-13T19:57:23+00:00</updated>
    <author>
      <name>/u/pfn0</name>
      <uri>https://old.reddit.com/user/pfn0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc1isk/rtx_6000_pro_blackwell_wouldnt_post_on_msi_z790p/"&gt; &lt;img alt="RTX 6000 Pro (Blackwell) Wouldn‚Äôt POST on MSI Z790-P Pro [FIXED]" src="https://a.thumbs.redditmedia.com/jkkh6fUsfysnACmqUZ55ILMb0uVmVBGGnFhg0ocIYE4.jpg" title="RTX 6000 Pro (Blackwell) Wouldn‚Äôt POST on MSI Z790-P Pro [FIXED]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On Friday, I picked up an RTX6000, mobo, nvme, and ram. Recently, I replaced my 13600K in my desktop with a 14700K, and sent the 13600K back to Intel for warranty replacement due to the Vmin shift issue. Everyone knows what happens when you have spare parts, it turns into a whole new build...&lt;/p&gt; &lt;p&gt;I wanted to document this whole experience because there are very few reports out there about Blackwell setups and problems, and the ones that exist are mostly unresolved threads (see &lt;a href="https://forum-en.msi.com/index.php?threads/msi-pro-z790-p-wifi-ddr4-no-boot-with-rtx-pro-blackwell.412240/"&gt;https://forum-en.msi.com/index.php?threads/msi-pro-z790-p-wifi-ddr4-no-boot-with-rtx-pro-blackwell.412240/&lt;/a&gt; and &lt;a href="https://www.reddit.com/r/nvidia/comments/1kt3uoi/finally_got_the_rtx_6000_blackwell_workstation/"&gt;https://www.reddit.com/r/nvidia/comments/1kt3uoi/finally_got_the_rtx_6000_blackwell_workstation/&lt;/a&gt; ). Also because it was something like 12 hours of torture getting it all figured out.&lt;/p&gt; &lt;p&gt;Parts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA RTX 6000 Pro (Blackwell)&lt;/li&gt; &lt;li&gt;MSI Pro Z790‚ÄëP&lt;/li&gt; &lt;li&gt;Meshroom S v2 15L case&lt;/li&gt; &lt;li&gt;128GB DDR5‚Äë6400, Samsung 990 Pro 4TB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After getting the whole system built and put together the RTX 6000 installed, the system wouldn‚Äôt POST at all. EZ Debug LEDs would light up red -&amp;gt; yellow -&amp;gt; red -&amp;gt; yellow and then die, never reaching white or green. Just everything black.&lt;/p&gt; &lt;p&gt;I pulled the RTX 6000 and booted on the iGPU, that posted and dropped me into the UEFI. That also helped me understand how the EZ Debug LEDs should behave:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Red -&amp;gt; Yellow -&amp;gt; White -&amp;gt; Green -&amp;gt; UEFI. With the iGPU, the sequence was perfect. With the RTX 6000, it died, just black after yellow.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once I got into BIOS on the iGPU, I tried the settings that people mentioned in other threads:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Disable CSM for pure UEFI&lt;/li&gt; &lt;li&gt;Enable Above 4GB decoding for crypto mining support (some funky msi option, I don't think I've ever heard of this before)&lt;/li&gt; &lt;li&gt;Disable ReBAR&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The blackwell board doesn't seem to be able to negotiate rebar with the mobo, whatever, all disabled.&lt;/p&gt; &lt;p&gt;So... I reinstalled the RTX6000 and it POSTs, wow... then... I updated the BIOS... shit. The card wouldn't POST anymore... then I tried the iGPU, that shit wouldn't work either, the graphics would constantly get busted in BIOS everytime the iGPU booted up.&lt;/p&gt; &lt;p&gt;Since the RTX6000 and iGPU both wouldn't boot up into a working state, I pulled out my old old old Geforce 760 and plugged it in, and it POST fine and dropped into UEFI just fine. At this point, I tried downgrading BIOS just to see if iGPU would work, it didn't, same corrupt graphics in BIOS issue, and the blackwell wouldn't POST at all either. I took a look at the settings again and saw that CSM was still disabled, but the other settings for &amp;gt;4GB decoding and disabling rebar were reset. I put them back into place, reinstalled the RTX6000, and that shit POSTs again.&lt;/p&gt; &lt;p&gt;Key takeaways from this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stay away from MSI, they have broken GPU support in this situation. And they refuse to acknowledge it, other than saying that they will not support the RTX6000 on a consumer board, despite it being a standard PCIE5 card.&lt;/li&gt; &lt;li&gt;iGPU is also broken under MSI when CSM is disabled for pure UEFI&lt;/li&gt; &lt;li&gt;BIOS updates wipes settings that leaves the blackwell card unusable and the system in a broken state unless the card is pulled and another discrete gpu is put in, maybe other Z790 boards would work with just iGPU, I haven't tried.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I spent like 12 hours figuring this all out, so I'm going to use the mobo as is for a few more days while I get the sytem fully built, then I'll replace it with another Z790 from someone else, hopefully I don't have as much of a pain with it. But upon further shopping, sadly, it looks like the Z790-P is the only board available locally for me that supports 64gb ram sticks. All the other Z790 boards 128-192GB of ram max&lt;/li&gt; &lt;li&gt;I've finished setting up Debian13 and Steam. Trying to get 4K120 working on my TV, but no luck with that yet, ugh.&lt;/li&gt; &lt;li&gt;Setting up vLLM, Docker, ComfyUI, etc. Already have llama.cpp running, but would prefer a more solid/production type of setup.&lt;/li&gt; &lt;li&gt;I started running some models, and qwen3-vl 235b in Q5/Q6 quants... I need more ram, these models put me at exactly my full system ram on both gpu and dram and barely enough for anything else. llama.cpp with &lt;code&gt;--fit on --fit-target 8192 --fit-ctx CTXSIZE --mlock&lt;/code&gt; are gamechangers, this lets the dense part of the LLM sit in gpu, some moe in gpu, and the rest offloaded to sysram. It's not great performance, but I can still get something like 5-8 tokens/second running on ~200GB model sizes. I want to get another 128gb of ram so that I can go up to about 250GB models and still leave some room for other tasks in sysram. or maybe adjust the gpu/cpu allocation more so that I can run other models in vram such as SD or LTX-2 concurrently&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pfn0"&gt; /u/pfn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qc1isk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc1isk/rtx_6000_pro_blackwell_wouldnt_post_on_msi_z790p/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc1isk/rtx_6000_pro_blackwell_wouldnt_post_on_msi_z790p/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T19:57:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc23oc</id>
    <title>Seline V0.1.4 - Codex OAuth</title>
    <updated>2026-01-13T20:18:46+00:00</updated>
    <author>
      <name>/u/Diligent-Builder7762</name>
      <uri>https://old.reddit.com/user/Diligent-Builder7762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc23oc/seline_v014_codex_oauth/"&gt; &lt;img alt="Seline V0.1.4 - Codex OAuth" src="https://external-preview.redd.it/dzY1NnUyMjVkNmRnMUmSYbXnweP7RjR2NYMpOSLE5Z9tkRJZWPp5KiKGLBp5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a641b15ee2a772d4b2945cec38444fedeadb6b9a" title="Seline V0.1.4 - Codex OAuth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys!&lt;/p&gt; &lt;p&gt;Today I heard a quote; &amp;quot;In a world where answers are abundant and cheap, the questions become valuable.&amp;quot; And that is exactly what we are trying to solve with Seline! It adds value to your inputs/prompts + more. It grounds your doc/code base, and enhances your prompts with advanced rag algorithms + tools. Still improving and rocking PRs and commits at work lately with my own agent!&lt;/p&gt; &lt;p&gt;Feedback is much welcomed.&lt;/p&gt; &lt;p&gt;Seline v0.1.4 has been released with exciting new features and improvements:&lt;/p&gt; &lt;p&gt;New Features:&lt;br /&gt; - OpenAI Codex has been added as a new LLM provider with OAuth authentication.&lt;br /&gt; - Advanced vector search configuration has been introduced, including hybrid search, chunking, reranking, query processing, and file limits.&lt;br /&gt; - An embedding setup flow has been added during agent creation.&lt;br /&gt; - Tool dependency tracking now includes visual indicators for prerequisites.&lt;br /&gt; - A multi-step onboarding wizard has been implemented to guide users through setup, covering provider selection, authentication, and preference customization.&lt;br /&gt; - An onboarding gate ensures new users complete the initial setup before accessing the app.&lt;br /&gt; - A &amp;quot;Memory&amp;quot; section in settings allows management of preference defaults across visual, communication, and workflow styles.&lt;br /&gt; - Authentication flows have been integrated to support multiple AI providers with customizable setup options.&lt;/p&gt; &lt;p&gt;Improvements:&lt;br /&gt; - Vector search routing has been simplified; hybrid search now automatically activates when enabled.&lt;br /&gt; - Token refresh mechanisms have been enhanced for background maintenance, ensuring users remain logged in.&lt;/p&gt; &lt;p&gt;Chores:&lt;br /&gt; - The legacy vector search V2 percentage rollout setting has been removed.&lt;/p&gt; &lt;p&gt;TLDR: Lots of models, + good performance + beautiful well visible UI + enhanced onboarding and terrible UX + save your data on your device (its crazy nice if we think about we get to keep and save all our inputs and outputs in nicely structured way to dbs, hence we are building our future datasets. Not just handing them freely to the third parties... I mean I love creating datasets and tuning models, and I profit from it a lot with image/diffusion models. I have been a long time fine tuner and can't wait to see the days where I will be fine-tuning my own coding models from my own queries.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tercumantanumut/seline"&gt;https://github.com/tercumantanumut/seline&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Builder7762"&gt; /u/Diligent-Builder7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qk5zb315d6dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc23oc/seline_v014_codex_oauth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc23oc/seline_v014_codex_oauth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T20:18:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc687e</id>
    <title>Any claude cowork qlternative worth checking?</title>
    <updated>2026-01-13T22:54:47+00:00</updated>
    <author>
      <name>/u/marsxyz</name>
      <uri>https://old.reddit.com/user/marsxyz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not a dev, hence cowork really appeal to me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marsxyz"&gt; /u/marsxyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc687e/any_claude_cowork_qlternative_worth_checking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc687e/any_claude_cowork_qlternative_worth_checking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc687e/any_claude_cowork_qlternative_worth_checking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbmtuw</id>
    <title>Best LLM model for 128GB of VRAM?</title>
    <updated>2026-01-13T09:19:32+00:00</updated>
    <author>
      <name>/u/Professional-Yak4359</name>
      <uri>https://old.reddit.com/user/Professional-Yak4359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My work requires the LLM to read tons of technical documents at a time and to provide insights (50 pages typically). I have a system of 8 x 5070 Ti running vllm (I need the prompt processing speed with at least 64k or 128k context). Right now I am running qwen3-32b and gptoss:120b but I am wondering if there are better choices than these two. &lt;/p&gt; &lt;p&gt;Any suggestion would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Yak4359"&gt; /u/Professional-Yak4359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbmtuw/best_llm_model_for_128gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbmtuw/best_llm_model_for_128gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbmtuw/best_llm_model_for_128gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T09:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbjbrf</id>
    <title>baichuan-inc/Baichuan-M3-235B ¬∑ Hugging Face</title>
    <updated>2026-01-13T05:46:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbjbrf/baichuanincbaichuanm3235b_hugging_face/"&gt; &lt;img alt="baichuan-inc/Baichuan-M3-235B ¬∑ Hugging Face" src="https://external-preview.redd.it/-zZCICdRLYRGcsTSpa_79bNF1i9sBC7auVQLA_P7iG8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16acadd715d2f48039128191cb574c9204d186a6" title="baichuan-inc/Baichuan-M3-235B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M3-235B#%F0%9F%8C%9F-model-overview"&gt;&lt;/a&gt;üåü Model Overview&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Baichuan-M3&lt;/strong&gt; is Baichuan AI's new-generation medical-enhanced large language model, a major milestone following &lt;a href="https://github.com/baichuan-inc/Baichuan-M2-32B"&gt;Baichuan-M2&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In contrast to prior approaches that primarily focus on static question answering or superficial role-playing, Baichuan-M3 is trained to explicitly model the &lt;strong&gt;clinical decision-making process&lt;/strong&gt;, aiming to improve usability and reliability in real-world medical practice. Rather than merely producing &amp;quot;plausible-sounding answers&amp;quot; or high-frequency vague recommendations like &amp;quot;you should see a doctor soon,&amp;quot; the model is trained to &lt;strong&gt;proactively acquire critical clinical information&lt;/strong&gt;, &lt;strong&gt;construct coherent medical reasoning pathways&lt;/strong&gt;, and &lt;strong&gt;systematically constrain hallucination-prone behaviors&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M3-235B#core-highlights"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Core Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üèÜ &lt;strong&gt;Surpasses GPT-5.2&lt;/strong&gt;: Outperforms OpenAI's latest model across HealthBench, HealthBench-Hard, hallucination evaluation, and BCOSCE, establishing a new SOTA in medical AI&lt;/li&gt; &lt;li&gt;ü©∫ &lt;strong&gt;High-Fidelity Clinical Inquiry&lt;/strong&gt;: The only model to rank first across all three BCOSCE dimensions‚ÄîClinical Inquiry, Laboratory Testing, and Diagnosis&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Low Hallucination, High Reliability&lt;/strong&gt;: Achieves substantially lower hallucination rates than GPT-5.2 through Fact-Aware RL, even without external tools&lt;/li&gt; &lt;li&gt;‚ö° &lt;strong&gt;Efficient Deployment&lt;/strong&gt;: W4 quantization reduces memory to 26% of original; Gated Eagle3 speculative decoding achieves 96% speedup&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M3-235B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbjbrf/baichuanincbaichuanm3235b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbjbrf/baichuanincbaichuanm3235b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T05:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbuchh</id>
    <title>LFM 2.5 1.2b IS FAST</title>
    <updated>2026-01-13T15:31:07+00:00</updated>
    <author>
      <name>/u/TheyCallMeDozer</name>
      <uri>https://old.reddit.com/user/TheyCallMeDozer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbuchh/lfm_25_12b_is_fast/"&gt; &lt;img alt="LFM 2.5 1.2b IS FAST" src="https://b.thumbs.redditmedia.com/UgqYOVKNVG-AdPhC2nJKkn0KO3SliU2SKC1C0QIA0tg.jpg" title="LFM 2.5 1.2b IS FAST" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So recently seen the 1.4gb model by Liquid and decided to give it ago, that size could run on a pi, maybe not fast but its small enough. For context, I ran this on my desktop in LMStudio on a 5090, 192gb and gave it a question of &amp;quot;What Can you Do&amp;quot; here was the output: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5y7lb7a0w4dg1.png?width=964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8684757df67f09ee88b27e83a7cd45aa7426ea6d"&gt;https://preview.redd.it/5y7lb7a0w4dg1.png?width=964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8684757df67f09ee88b27e83a7cd45aa7426ea6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Output was 578.01 tok/s for 389 tokens, in 0.08s that was FAST... comaprised to other 1B and 2B models I have tried recently the max I was getting was 380's for about 0.5 of a second. &lt;/p&gt; &lt;p&gt;Of note yes I have checked becase I know people will ask, Not it is not UNCENSORED, tried the starned questions like Stealing a Car and such, its response was &amp;quot;I cannot assist with that type of information&amp;quot; which is perfectly fine, at that speed and size I could see this model being a handle little RAG model for an embeded device. &lt;/p&gt; &lt;p&gt;Anyone tried anything on it themselves yet? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyCallMeDozer"&gt; /u/TheyCallMeDozer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbuchh/lfm_25_12b_is_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbuchh/lfm_25_12b_is_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbuchh/lfm_25_12b_is_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T15:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbp52n</id>
    <title>FrogBoss 32B and FrogMini 14B from Microsoft</title>
    <updated>2026-01-13T11:40:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"&gt; &lt;img alt="FrogBoss 32B and FrogMini 14B from Microsoft" src="https://b.thumbs.redditmedia.com/gLOM6tp4n9cVVk2b-NQNYqlkyiPlEXip_bGZfpns1mI.jpg" title="FrogBoss 32B and FrogMini 14B from Microsoft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FrogBoss is a 32B-parameter coding agent specialized in fixing bugs in code. FrogBoss was obtained by fine‚Äëtuning a Qwen3‚Äë32B language model on debugging trajectories generated by Claude Sonnet 4 within the &lt;a href="https://aka.ms/bug-pilot"&gt;BugPilot framework&lt;/a&gt;. The training data combines real‚Äëworld bugs from R2E‚ÄëGym, synthetic bugs from SWE‚ÄëSmith, and novel ‚ÄúFeatAdd‚Äù bugs.&lt;/p&gt; &lt;p&gt;FrogMini is a 14B-parameter coding agent specialized in fixing bugs in code. FrogMini was obtained by fine‚Äëtuning a Qwen3‚Äë14B language model on debugging trajectories generated by Claude Sonnet 4 within the &lt;a href="https://aka.ms/bug-pilot"&gt;BugPilot framework&lt;/a&gt;. The training data combines real‚Äëworld bugs from R2E‚ÄëGym, synthetic bugs from SWE‚ÄëSmith, and novel ‚ÄúFeatAdd‚Äù bugs.&lt;/p&gt; &lt;p&gt;context length 64k&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/FrogBoss-32B-2510"&gt;https://huggingface.co/microsoft/FrogBoss-32B-2510&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/FrogMini-14B-2510"&gt;https://huggingface.co/microsoft/FrogMini-14B-2510&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1woo8ui5t3dg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=687cb5972b02c2afc6a4f83217f1ad6a24c3b81f"&gt;https://preview.redd.it/1woo8ui5t3dg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=687cb5972b02c2afc6a4f83217f1ad6a24c3b81f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T11:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc485u</id>
    <title>Building a game where you talk to NPCs using Llama 3.1-8B-q4, optimized for 6GB VRAM</title>
    <updated>2026-01-13T21:38:36+00:00</updated>
    <author>
      <name>/u/bayhan2000</name>
      <uri>https://old.reddit.com/user/bayhan2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc485u/building_a_game_where_you_talk_to_npcs_using/"&gt; &lt;img alt="Building a game where you talk to NPCs using Llama 3.1-8B-q4, optimized for 6GB VRAM" src="https://external-preview.redd.it/MjFraTE5dGdxNmRnMeC5Rm5xTPWPllbMPfh3CJD-5GSC-t7xIsMmhXMeJtjN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afad020551a48a46b2bf20cc6b182f727b7105ae" title="Building a game where you talk to NPCs using Llama 3.1-8B-q4, optimized for 6GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on an investigative indie game. The core mechanic isn't a dialogue tree. It‚Äôs a direct interface with local LLMs. My goal was to make a polished, atmospheric experience that runs entirely offline on mid-range consumer hardware.&lt;/p&gt; &lt;p&gt;The game runs a local &lt;strong&gt;Llama-3.1-8B (Q4_K_M)&lt;/strong&gt; instance. I am using tauri and llama-server with vulkan support. The UI is a custom WebGL-driven &amp;quot;OS&amp;quot; that simulates a retro-future terminal.&lt;/p&gt; &lt;p&gt;Targeting &lt;strong&gt;6GB VRAM&lt;/strong&gt; was the biggest challenge. I had to keep low context window like 2048-4096 the LLM‚Äôs KV cache.&lt;/p&gt; &lt;p&gt;In this clip, I‚Äôm testing a bribery scenario. NPC tries to bribe me with bribe action, basically function calling at the end of the prompt.&lt;/p&gt; &lt;p&gt;I have tested with RTX2060 and 4070Ti Super and it both works realtime.&lt;/p&gt; &lt;p&gt;I am planning to train a custom LoRA specifically for the game‚Äôs world and essentially eliminate any remaining hallucinations. It works surprisingly well right now, but a dedicated fine-tune will be the final step for total immersion.&lt;/p&gt; &lt;p&gt;I would like to hear your thoughts!!&lt;/p&gt; &lt;p&gt;Edit :&lt;br /&gt; I managed to get the VRAM usage down to ~5.3 GB for Llama 3.1 8B by sticking to a 4096 context window and enabling Flash Attention.&lt;/p&gt; &lt;p&gt;To handle that tight context limit, I‚Äôm using a vector DB and a RAG pipeline. It basically &amp;quot;swaps in&amp;quot; relevant lore and action tags on the fly so the AI stays smart without the prompt bloating.&lt;/p&gt; &lt;p&gt;Performance is surprisingly solid on mid-range gear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RTX 4070:&lt;/strong&gt; ~70 TPS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RTX 2060 (6GB):&lt;/strong&gt; ~15-20 TPS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I was actually skeptical about the 2060 since there‚Äôs only about 700MB of headroom left for the OS and other apps, but it hasn't been an issue at all. It runs super smooth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bayhan2000"&gt; /u/bayhan2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rrveazsgq6dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc485u/building_a_game_where_you_talk_to_npcs_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc485u/building_a_game_where_you_talk_to_npcs_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T21:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbqmon</id>
    <title>SPARKLE Announces Intel Arc Pro B60 24GB Graphics Card Series Launch on January 12, 2026 for USD $799 MSRP</title>
    <updated>2026-01-13T12:58:16+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.sparkle.com.tw/en/sparkle-news/view/93E0b95ea8A0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqmon/sparkle_announces_intel_arc_pro_b60_24gb_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqmon/sparkle_announces_intel_arc_pro_b60_24gb_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbpf8s</id>
    <title>Nemotron 3 Super release soon?</title>
    <updated>2026-01-13T11:56:40+00:00</updated>
    <author>
      <name>/u/Lorelabbestia</name>
      <uri>https://old.reddit.com/user/Lorelabbestia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found this entry in the autoconfig YAML of the TRT-LLM github repo from 3 days ago:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/model_registry/models.yaml"&gt;nvidia/NVIDIA-Nemotron-3-Super-120B-BF16-BF16KV-010726&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was just wondering if we have a release date?&lt;/p&gt; &lt;p&gt;I'm currently training nemotron 3 nano 30B to assess my current setup and was thinking to train final model on qwen's 3 next 80B, but if NVIDIA comes out with a 120B banger, I'm going for it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lorelabbestia"&gt; /u/Lorelabbestia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpf8s/nemotron_3_super_release_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpf8s/nemotron_3_super_release_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpf8s/nemotron_3_super_release_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T11:56:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc14cz</id>
    <title>Best local model / agent for coding, replacing Claude Code</title>
    <updated>2026-01-13T19:42:50+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually use Claude Code (Pro) for coding (Xcode / Swift etc). Are there any decent local agents / models which could be a replacement for it? I don't expect it to match the intelligence of Claude Code, but I quite like the terminal-based experience, and wonder if there's a system which nearly matches it. Just for when I've used up 100% of Claude plan.&lt;/p&gt; &lt;p&gt;Computer specs: MacBook Pro, M3 Pro chip, 36 GB RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc14cz/best_local_model_agent_for_coding_replacing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc14cz/best_local_model_agent_for_coding_replacing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc14cz/best_local_model_agent_for_coding_replacing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T19:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc53rf</id>
    <title>MedGemma 1.5: Next generation medical image interpretation with medical speech to text with MedASR</title>
    <updated>2026-01-13T22:11:12+00:00</updated>
    <author>
      <name>/u/CheekyBastard55</name>
      <uri>https://old.reddit.com/user/CheekyBastard55</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc53rf/medgemma_15_next_generation_medical_image/"&gt; &lt;img alt="MedGemma 1.5: Next generation medical image interpretation with medical speech to text with MedASR" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="MedGemma 1.5: Next generation medical image interpretation with medical speech to text with MedASR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyBastard55"&gt; /u/CheekyBastard55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/next-generation-medical-image-interpretation-with-medgemma-15-and-medical-speech-to-text-with-medasr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc53rf/medgemma_15_next_generation_medical_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc53rf/medgemma_15_next_generation_medical_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:11:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbz7h6</id>
    <title>Owners, not renters: Mozilla's open source AI strategy</title>
    <updated>2026-01-13T18:34:22+00:00</updated>
    <author>
      <name>/u/NelsonMinar</name>
      <uri>https://old.reddit.com/user/NelsonMinar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbz7h6/owners_not_renters_mozillas_open_source_ai/"&gt; &lt;img alt="Owners, not renters: Mozilla's open source AI strategy" src="https://external-preview.redd.it/eBhDV53Bx2pdf58HHsmIDWpPzti_SmsXMDBh7hzdnLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=272d58abf490660743722caac57b754d523d0b9f" title="Owners, not renters: Mozilla's open source AI strategy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NelsonMinar"&gt; /u/NelsonMinar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.mozilla.org/en/mozilla/mozilla-open-source-ai-strategy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbz7h6/owners_not_renters_mozillas_open_source_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbz7h6/owners_not_renters_mozillas_open_source_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T18:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc5nml</id>
    <title>Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!</title>
    <updated>2026-01-13T22:32:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt; &lt;img alt="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" src="https://external-preview.redd.it/amZxajFtZXF6NmRnMQO5kEggYbW8-0IppaPjE5mW-pGiD_HSvWQwK_psM6yd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a2ad1d1aac49f10f9a525a08d7b23d8a37a99b3" title="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I‚Äôve been listening to all your feedback on Soprano, and I‚Äôve been working nonstop over these past three weeks to incorporate everything, so I have a TON of updates for you all!&lt;/p&gt; &lt;p&gt;For those of you who haven‚Äôt heard of Soprano before, it is an on-device text-to-speech model I designed to have highly natural intonation and quality with a small model footprint. It can run up to &lt;strong&gt;20x realtime&lt;/strong&gt; on CPU, and up to &lt;strong&gt;2000x&lt;/strong&gt; on GPU. It also supports lossless streaming with &lt;strong&gt;15 ms latency&lt;/strong&gt;, an order of magnitude lower than any other TTS model. You can check out Soprano here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;&lt;strong&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-80M"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-80M&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I am releasing training code for you guys! This was by far the most requested feature to be added, and I am happy to announce that you can now train your own ultra-lightweight, ultra-realistic TTS models like the one in the video with your &lt;strong&gt;own data&lt;/strong&gt; on your &lt;strong&gt;own hardware&lt;/strong&gt; with &lt;strong&gt;Soprano-Factory&lt;/strong&gt;! Using Soprano-Factory, you can add new &lt;strong&gt;voices&lt;/strong&gt;, &lt;strong&gt;styles&lt;/strong&gt;, and &lt;strong&gt;languages&lt;/strong&gt; to Soprano. The entire repository is just 600 lines of code, making it easily customizable to suit your needs.&lt;/p&gt; &lt;p&gt;In addition to the training code, I am also releasing &lt;strong&gt;Soprano-Encoder&lt;/strong&gt;, which converts raw audio into audio tokens for training. You can find both here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Factory:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano-factory"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano-factory&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Encoder:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-Encoder"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-Encoder&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I hope you enjoy it! See you tomorrow,&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;p&gt;Disclaimer: I did not originally design Soprano with finetuning in mind. As a result, I cannot guarantee that you will see good results after training. Personally, I have my doubts that an 80M-parameter model trained on just 1000 hours of data can generalize to OOD datasets, but I have seen bigger miracles on this sub happen, so knock yourself out :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wnuwfpdqz6dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbpz5l</id>
    <title>kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required</title>
    <updated>2026-01-13T12:25:26+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"&gt; &lt;img alt="kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required" src="https://b.thumbs.redditmedia.com/1twWaeVXhu1muEmRUClJoZ9yZJXDVOmoBhoTPlp5ntc.jpg" title="kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post with demo: Pocket TTS: A high quality TTS that gives your CPU a voice: &lt;a href="https://kyutai.org/blog/2026-01-13-pocket-tts"&gt;https://kyutai.org/blog/2026-01-13-pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/kyutai-labs/pocket-tts"&gt;https://github.com/kyutai-labs/pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Model Card: &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;https://huggingface.co/kyutai/pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;arXiv:2509.06926 [cs.SD]: Continuous Audio Language Models&lt;br /&gt; Simon Rouard, Manu Orsini, Axel Roebel, Neil Zeghidour, Alexandre D√©fossez&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2509.06926"&gt;https://arxiv.org/abs/2509.06926&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From kyutai on ùïè: &lt;a href="https://x.com/kyutai_labs/status/2011047335892303875"&gt;https://x.com/kyutai_labs/status/2011047335892303875&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qbpz5l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbw325</id>
    <title>My wishes for 2026</title>
    <updated>2026-01-13T16:35:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt; &lt;img alt="My wishes for 2026" src="https://preview.redd.it/8knck5zv85dg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a8be13989bebb31b688873f7197d169cb43651e" title="My wishes for 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which do you think will happen first? And which won‚Äôt happen in 2026?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8knck5zv85dg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T16:35:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
