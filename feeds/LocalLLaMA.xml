<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-08T14:07:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1phcyh7</id>
    <title>In need for a dev (paid) koboldccp</title>
    <updated>2025-12-08T13:54:04+00:00</updated>
    <author>
      <name>/u/Worried_Sock9618</name>
      <uri>https://old.reddit.com/user/Worried_Sock9618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need a fully self-hosted, 24/7 AI chat system with these exact requirements:&lt;/p&gt; &lt;p&gt;• Normal Telegram user accounts (NOT bots) that auto-reply to incoming messages&lt;br /&gt; • Local LLM backend: KoboldCpp + GGUF model (Pygmalion/MythoMax or similar uncensored)&lt;br /&gt; • Each Telegram account has its own persona (prompt, style, memory, upsell commands)&lt;br /&gt; • Personas and accounts managed via simple JSON/YAML files – no code changes needed to add new ones&lt;br /&gt; • Human-like behaviour (typing indicator, small random delays)&lt;br /&gt; • Runs permanently on a VPS (systemd + auto-restart)&lt;br /&gt; • KoboldCpp only internally accessible (no public exposure)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worried_Sock9618"&gt; /u/Worried_Sock9618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyh7/in_need_for_a_dev_paid_koboldccp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyh7/in_need_for_a_dev_paid_koboldccp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyh7/in_need_for_a_dev_paid_koboldccp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph14do</id>
    <title>dynamic allocation of less used experts to slower memory</title>
    <updated>2025-12-08T02:43:46+00:00</updated>
    <author>
      <name>/u/zqkb</name>
      <uri>https://old.reddit.com/user/zqkb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"&gt; &lt;img alt="dynamic allocation of less used experts to slower memory" src="https://b.thumbs.redditmedia.com/TSEG2N0W9eMlIStr9sCBAh2F6vtoFNU88rMcjRGXhhQ.jpg" title="dynamic allocation of less used experts to slower memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, when Cerebras shared their REAP approach, we had a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/comment/nkjqt91/"&gt;discussion&lt;/a&gt; about offloading less frequently used experts to slower memory. Here's a quick follow-up on testing that (more details + repro steps &lt;a href="https://github.com/okuvshynov/golem?tab=readme-ov-file"&gt;on github&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Coverage of expert activation per layer for two different prompts looks like this (short prompts, 512 tokens generated)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q69bfynq2w5g1.png?width=2100&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09a63ab5f52b1ce41acfb0f217afb1d27173fa84"&gt;Qwen3-235b (6bit, 128 experts total, 8/token)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5jso00333w5g1.png?width=2100&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9eb6cda756742e903169567a6734b29d45e066f9"&gt;GLM 4.6 (4 bit, 160 experts total, 8/token)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Storing a static set of experts/layer will be suboptimal, but we can get some initial seed + implement reasonable allocation/eviction policies and run models which would not fit into fast memory otherwise. Looking at these charts, we can see that first layers and few last layers are more diverse, while the middle part is more likely to benefit from partial allocation.&lt;/p&gt; &lt;p&gt;Here's practical result of running Qwen3-235B @Q6 on M2 Ultra (192GB).&lt;/p&gt; &lt;p&gt;With warm start on some aggregated frequently used expert set, for short prompt + 512 tokens generated, we get hit rate which looks like this, depending on cache size per layer:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/he329uhi4w5g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d18b4c049466618f4abf7079b25c61994934a894"&gt;https://preview.redd.it/he329uhi4w5g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d18b4c049466618f4abf7079b25c61994934a894&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A reasonable thing to do would be to just store less-cacheable layers fully, and be more aggressive in caching the middle layers. &lt;/p&gt; &lt;p&gt;We can make some comparison with t/s for 4bit version, which fits into unified memory:&lt;/p&gt; &lt;p&gt;4bit baseline, model in unified memory:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% mlx_lm.generate --model mlx-community/Qwen3-235B-A22B-4bit-DWQ -p &amp;quot;Write 5 poems about the ocean in different styles&amp;quot; -m 512 ... ========== Prompt: 18 tokens, 48.314 tokens-per-sec Generation: 512 tokens, 28.679 tokens-per-sec Peak memory: 132.397 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;6bit with 96 (out of 128) experts:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% python scripts/generate.py -m ~/projects/llms/Qwen3-235B-A22B-Instruct-2507-6bit -c 96 -p &amp;quot;Write 5 poems about the ocean in different styles&amp;quot; -n 512 -W /tmp/qwen235-6b ... Generation: 512 tokens, 10.4 t/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;6bit with 96 (out of 128) experts + some layers loaded fully:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/generate.py -m ~/projects/llms/Qwen3-235B-A22B-Instruct-2507-6bit -c 96 -p &amp;quot;Write 5 poems about the ocean in different styles&amp;quot; -n 512 -W /tmp/qwen235-6b -f 0-40,90-93 ... Generation: 512 tokens, 14.6 t/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is more information in the repo (including longer prompts, known inefficiencies, etc), but some conclusions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;it's definitely feasible for models which are 'slightly not fitting' for personal usage, where we don't care much about multi-query throughput; &lt;/li&gt; &lt;li&gt;it should work better when secondary memory is faster (say, RAM -&amp;gt; PCIe -&amp;gt; VRAM)&lt;/li&gt; &lt;li&gt;in this experiment, we were bringing experts to fast memory/compute. On different hardware the alternative could be to just decide to keep less frequently experts on slower memory/compute, with periodic prompt-specific reallocation not on critical path. &lt;/li&gt; &lt;li&gt;we can speculatively prefetch experts a few layers in advance and amortize the cost. Current experimental implementation is suboptimal and fetching experts right when we need them, blocking the compute.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zqkb"&gt; /u/zqkb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T02:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgvhal</id>
    <title>mbzuai ifm releases Open 70b model - beats qwen-2.5</title>
    <updated>2025-12-07T22:23:17+00:00</updated>
    <author>
      <name>/u/Powerful-Sail-8826</name>
      <uri>https://old.reddit.com/user/Powerful-Sail-8826</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/LLM360/K2-V2-Instruct"&gt;https://huggingface.co/LLM360/K2-V2-Instruct&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Sail-8826"&gt; /u/Powerful-Sail-8826 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T22:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph3js7</id>
    <title>21 Days of Building a Small Language Model.</title>
    <updated>2025-12-08T04:46:45+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"&gt; &lt;img alt="21 Days of Building a Small Language Model." src="https://b.thumbs.redditmedia.com/cWFTudRm4yxp6DIriTfuZ9INDROjEcGR2pdy4_WqgRw.jpg" title="21 Days of Building a Small Language Model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Starting tomorrow, I’m beginning a new series: “21 Days of Building a Small Language Model.”&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bw2jtqnztw5g1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=264ee6545e42bbb39fb7fb9043ad66e8fd6b3c91"&gt;https://preview.redd.it/bw2jtqnztw5g1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=264ee6545e42bbb39fb7fb9043ad66e8fd6b3c91&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As we get close to the end of the year, I want to try something meaningful: help anyone who’s interested build their own small language model by the end of the year.&lt;/p&gt; &lt;p&gt;I’ll be following the structure of my book while keeping everything beginner-friendly and hands-on.&lt;/p&gt; &lt;p&gt;Just to set real expectations: Building AND understanding a small language model in 21 days is definitely challenging.&lt;br /&gt; It won’t be easy. There will be concepts that take time to sink in.&lt;br /&gt; But I’m going to do everything I can to break things down in simple language and make the journey as accessible as possible.&lt;/p&gt; &lt;p&gt;If you want to follow along, I’ll be posting updates every day at 9am PST on LinkedIn &lt;/p&gt; &lt;p&gt;Happy learning, and see you tomorrow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T04:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1phc878</id>
    <title>GLM-4.6V-Flash now available on HuggingChat</title>
    <updated>2025-12-08T13:20:54+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phc878/glm46vflash_now_available_on_huggingchat/"&gt; &lt;img alt="GLM-4.6V-Flash now available on HuggingChat" src="https://external-preview.redd.it/foZjOdhgFbynMPkk1VbM0h59F2XNCIyyRotIerkWrUs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6571774e25496428a60e8c8a009f791e448c9e7b" title="GLM-4.6V-Flash now available on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/models/zai-org/GLM-4.6V-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phc878/glm46vflash_now_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phc878/glm46vflash_now_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1phb9nr</id>
    <title>Key Insights from OpenRouter's 2025 State of AI report</title>
    <updated>2025-12-08T12:34:20+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phb9nr/key_insights_from_openrouters_2025_state_of_ai/"&gt; &lt;img alt="Key Insights from OpenRouter's 2025 State of AI report" src="https://b.thumbs.redditmedia.com/zY2yEOD1bIIE0aKojSmHJ5P-j3FpCE_KZPEjMSbDYcg.jpg" title="Key Insights from OpenRouter's 2025 State of AI report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. new landscape of open source: Chinese models rise, market moves beyond monopoly&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Although proprietary closed-source models still dominate, the market share of open-source models has steadily grown to about one-third. Notably, a significant portion of this growth comes from models developed in China, such as the DeepSeek, Qwen and Kimi, which have gained a large global user base thanks to their strong performance and rapid iteration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. AI's top use isn't productivity, it's &amp;quot;role-playing&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/87aedwx82z5g1.png?width=1612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4207a19387cd827696e3db38c15ca73ebf374eb9"&gt;https://preview.redd.it/87aedwx82z5g1.png?width=1612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4207a19387cd827696e3db38c15ca73ebf374eb9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contrary to the assumption that AI is mainly used for productivity tasks such as programming and writing, data shows that in open-source models, the largest use case is creative role-playing. Among all uses of open-source models, more than half (about 52%) fall under the role-playing category. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. the &amp;quot;cinderella effect&amp;quot;: winning users hinges on solving the problem the &amp;quot;first time&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When a newly released model successfully solves a previously unresolved high-value workload for the first time, it achieves a perfect “fit”, much like Cinderella putting on her unique glass slipper. Typically, this “perfect fit” is realized through the model’s new capabilities in agentic reasoning, such as multi-step reasoning or reliable tool use that address a previously difficult business problem. The consequence of this “fit” is a strong user lock-in effect. Once users find the “glass slipper” model that solves their core problem, they rarely switch to newer or even technically superior models that appear later.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. rise of agents: ai shifts from &amp;quot;text generator&amp;quot; to &amp;quot;task executor&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Current models not only generate text but also take concrete actions through planning, tool invocation, and handling long-form context to solve complex problems.&lt;/p&gt; &lt;p&gt;Key data evidence supporting this trend includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Proliferation of reasoning models: Models with multi-step reasoning capabilities now process more than 50% of total tokens, becoming the mainstream in the market.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Surge in context length: Over the past year, the average number of input tokens (prompts) per request has grown nearly fourfold. This asymmetric growth is primarily driven by use cases in software development and technical reasoning, indicating that users are engaging models with increasingly complex background information.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Normalization of tool invocation: An increasing number of requests now call external APIs or tools to complete tasks, with this proportion stabilizing at around 15% and continuing to grow, marking AI’s role as the “action hub” connecting the digital world.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w23h9uqn4z5g1.png?width=1326&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=020bdbbd6f8f5604a1f6a3331f2420eb89ac153e"&gt;https://preview.redd.it/w23h9uqn4z5g1.png?width=1326&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=020bdbbd6f8f5604a1f6a3331f2420eb89ac153e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. the economics of AI: price isn't the only deciding factor&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Data shows that demand for AI models is relatively “price inelastic,” meaning there is no strong correlation between model price and usage volume. When choosing a model, users consider cost, quality, reliability, and specific capabilities comprehensively, rather than simply pursuing the lowest price. Value, not price, is the core driver of choice.&lt;/p&gt; &lt;p&gt;The research categorizes models on the market into four types, clearly revealing this dynamic:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Efficient Giants&lt;/strong&gt;: Such as Google Gemini Flash, with extremely low cost and massive usage, serving as an “attractive default option for high-volume or long-context workloads.”&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Premium Leaders&lt;/strong&gt;: Such as Anthropic Claude Sonnet, which are expensive yet heavily used, indicating that users are willing to pay for “superior reasoning ability and scalable reliability.”&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Premium Specialists&lt;/strong&gt;: Such as OpenAI GPT-4, which are extremely costly and relatively less used, dedicated to “niche, high-stakes critical tasks where output quality far outweighs marginal token cost.”&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long Tail Market&lt;/strong&gt;: Includes a large number of low-cost, low-usage models that meet various niche needs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5t2jufy44z5g1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa9a6c43a00dc2f138e4416ef737d2fc63d32f5b"&gt;https://preview.redd.it/5t2jufy44z5g1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa9a6c43a00dc2f138e4416ef737d2fc63d32f5b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phb9nr/key_insights_from_openrouters_2025_state_of_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phb9nr/key_insights_from_openrouters_2025_state_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phb9nr/key_insights_from_openrouters_2025_state_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T12:34:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1phcnyt</id>
    <title>vLLM supports the new GLM-4.6V and GLM-4.6V-Flash models</title>
    <updated>2025-12-08T13:41:06+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcnyt/vllm_supports_the_new_glm46v_and_glm46vflash/"&gt; &lt;img alt="vLLM supports the new GLM-4.6V and GLM-4.6V-Flash models" src="https://preview.redd.it/m9b0x4figz5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4f2a7ee967df453ee0f8014c01eb0f2fd1a2851" title="vLLM supports the new GLM-4.6V and GLM-4.6V-Flash models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This guide describes how to run GLM-4.6V with native FP8. In the GLM-4.6V series, FP8 models have minimal accuracy loss. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM-4.6V focuses on high-quality multimodal reasoning with long context and native tool/function calling, &lt;/li&gt; &lt;li&gt;GLM-4.6V-Flash is a 9B variant tuned for lower latency and smaller-footprint deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unless you need strict reproducibility for benchmarking or similar scenarios, it is recommend to use FP8 to run at a lower cost.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM-V.html"&gt;GLM-4.6V usage guide&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9b0x4figz5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcnyt/vllm_supports_the_new_glm46v_and_glm46vflash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phcnyt/vllm_supports_the_new_glm46v_and_glm46vflash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph7yrx</id>
    <title>Built a photography workflow tool powered entirely by local vision models (Ollama + Qwen2.5-VL)</title>
    <updated>2025-12-08T09:16:01+00:00</updated>
    <author>
      <name>/u/AppropriatePublic687</name>
      <uri>https://old.reddit.com/user/AppropriatePublic687</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7yrx/built_a_photography_workflow_tool_powered/"&gt; &lt;img alt="Built a photography workflow tool powered entirely by local vision models (Ollama + Qwen2.5-VL)" src="https://external-preview.redd.it/qN5IQiQiiniOerdt9xfagCBn7W6cpvD6lVeKXz6ndNg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d762a60e3ac955362e47ce1a9d55dbb243619f7e" title="Built a photography workflow tool powered entirely by local vision models (Ollama + Qwen2.5-VL)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ph7yrx/video/34fabzwc5y5g1/player"&gt;https://reddit.com/link/1ph7yrx/video/34fabzwc5y5g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ph7yrx/video/9lvthxwc5y5g1/player"&gt;https://reddit.com/link/1ph7yrx/video/9lvthxwc5y5g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wanted to share something I've been building that puts local VLMs to practical use beyond chat.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FIXXER&lt;/strong&gt; is a Python TUI for photographers that automates the tedious parts of post-shoot workflow. The tool takes a hybrid local CV/ML/AI approach to burst grouping, quality culling, and file naming. The key constraint was &lt;em&gt;no internet required&lt;/em&gt; – everything runs locally via Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How local AI fits in:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Naming:&lt;/strong&gt; Qwen2.5vl:3b analyzes each image and generates descriptive, searchable filenames + tags. No prompting required – you press a button, it reasons over the image and outputs structured JSON.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Critique (k):&lt;/strong&gt; Highlight any photo and get a structured creative critique – composition score, lighting analysis, and an artistic suggestion. We tested Bakllava, Llava, and Phi-3-Vision. Phi-3 failed hard on structured JSON. Qwen was the only one consistent enough for production.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graceful degradation:&lt;/strong&gt; CLIP embeddings for semantic burst detection, falls back to imagehash if unavailable. BRISQUE for quality scoring, falls back to Laplacian variance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Runs comfortably on M4 MacBook Air (24gb). The vision model calls are the bottleneck, but qwen2.5vl:3b keeps things snappy.&lt;/p&gt; &lt;p&gt;The TUI has two aesthetic modes: a retro warez theme and a clean &amp;quot;Pro Mode&amp;quot; HUD. F12 toggles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/BandwagonVibes/fixxer"&gt;https://github.com/BandwagonVibes/fixxer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Screenshots / dev blog: &lt;a href="https://oaklens.art/dev"&gt;https://oaklens.art/dev&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious if anyone's running larger vision models and wants to benchmark the critique feature. My hardware tops out at 24GB unified memory, so I'd love to see what beefier setups can do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppropriatePublic687"&gt; /u/AppropriatePublic687 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7yrx/built_a_photography_workflow_tool_powered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7yrx/built_a_photography_workflow_tool_powered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7yrx/built_a_photography_workflow_tool_powered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T09:16:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgv2fi</id>
    <title>Unimpressed with Mistral Large 3 675B</title>
    <updated>2025-12-07T22:06:04+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From initial testing (coding related), this seems to be the new llama4.&lt;/p&gt; &lt;p&gt;The accusation from an ex-employee few months ago looks legit now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/suchenzang/status/1954973424486608928"&gt;https://x.com/suchenzang/status/1954973424486608928&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://36kr.com/p/3428277839465857"&gt;https://36kr.com/p/3428277839465857&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No idea whether the new Mistral Large 3 675B was indeed trained from scratch, or &amp;quot;shell-wrapped&amp;quot; on top of DSV3 (i.e. like Pangu: &lt;a href="https://github.com/HW-whistleblower/True-Story-of-Pangu"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt; ). Probably from scratch as it is much worse than DSV3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T22:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgsodd</id>
    <title>ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face</title>
    <updated>2025-12-07T20:28:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"&gt; &lt;img alt="ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face" src="https://external-preview.redd.it/KDS1GGF2jYTqD2RRTZIBI42Bz7Kwl8ZrRXizgMq0fZU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d2bc4dce3f56a2b8d2f8f355fdfb5b4072551a" title="ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Apriel-1.6-15B-Thinker&lt;/strong&gt; is an updated multimodal reasoning model in ServiceNow’s Apriel SLM series, building on &lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;&lt;strong&gt;Apriel-1.5-15B-Thinker&lt;/strong&gt;&lt;/a&gt;. With significantly improved text and image reasoning capabilities, Apriel-1.6 achieves competitive performance against models up to 10x its size. Like its predecessor, it benefits from extensive continual pretraining across both text and image domains. We further perform post-training, focusing on Supervised Finetuning (SFT) and Reinforcement Learning (RL). Apriel-1.6 obtains frontier performance without sacrificing reasoning token efficiency. The model improves or maintains task performance in comparison with Apriel-1.5-15B-Thinker, while &lt;em&gt;reducing reasoning token usage by more than 30%&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves a score of &lt;strong&gt;57&lt;/strong&gt; on the Artificial Analysis index outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient.&lt;/li&gt; &lt;li&gt;Scores &lt;strong&gt;69&lt;/strong&gt; on Tau2 Bench Telecom and &lt;strong&gt;69&lt;/strong&gt; on IFBench, which are key benchmarks for the enterprise domain.&lt;/li&gt; &lt;li&gt;At 15B parameters, the model fits on a single GPU, making it highly memory-efficient.&lt;/li&gt; &lt;li&gt;Based on community feedback on Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (&lt;code&gt;&amp;lt;tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;[BEGIN FINAL RESPONSE]&lt;/code&gt;, &lt;code&gt;&amp;lt;|end|&amp;gt;&lt;/code&gt;) for easier output parsing.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T20:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph9pg9</id>
    <title>New Jina-VLM-2.4B Reaches SOTA for Multilingual Visual Question Answering</title>
    <updated>2025-12-08T11:06:39+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph9pg9/new_jinavlm24b_reaches_sota_for_multilingual/"&gt; &lt;img alt="New Jina-VLM-2.4B Reaches SOTA for Multilingual Visual Question Answering" src="https://preview.redd.it/xsgg4t96py5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1b75b525a19b396db15af1179829e45366c5e90" title="New Jina-VLM-2.4B Reaches SOTA for Multilingual Visual Question Answering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jina-vlm is an open-source VLM built on top of &lt;strong&gt;SigLIP2 vision encoder&lt;/strong&gt; and &lt;strong&gt;Qwen3 language decoder&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Training data includes &lt;strong&gt;5M multimodal samples and 12B text tokens across 29 languages&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;This model achieves the highest average score (72.3) across eight VQA benchmarks.&lt;/p&gt; &lt;p&gt;This model also leads on multilingual multimodal understanding (MMMB: 78.8, Multilingual MMBench: 74.3).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;VQA Avg&lt;/th&gt; &lt;th align="left"&gt;MMMB&lt;/th&gt; &lt;th align="left"&gt;MM-Bench&lt;/th&gt; &lt;th align="left"&gt;RealWorld QA&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;jina-vlm&lt;/td&gt; &lt;td align="left"&gt;2.4B&lt;/td&gt; &lt;td align="left"&gt;72.3&lt;/td&gt; &lt;td align="left"&gt;78.8&lt;/td&gt; &lt;td align="left"&gt;74.3&lt;/td&gt; &lt;td align="left"&gt;68.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2-VL-2B&lt;/td&gt; &lt;td align="left"&gt;2.2B&lt;/td&gt; &lt;td align="left"&gt;66.4&lt;/td&gt; &lt;td align="left"&gt;71.3&lt;/td&gt; &lt;td align="left"&gt;69.4&lt;/td&gt; &lt;td align="left"&gt;62.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-VL-2B&lt;/td&gt; &lt;td align="left"&gt;2.2B&lt;/td&gt; &lt;td align="left"&gt;71.6&lt;/td&gt; &lt;td align="left"&gt;75.0&lt;/td&gt; &lt;td align="left"&gt;72.3&lt;/td&gt; &lt;td align="left"&gt;63.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;InternVL3-2B&lt;/td&gt; &lt;td align="left"&gt;2.2B&lt;/td&gt; &lt;td align="left"&gt;69.2&lt;/td&gt; &lt;td align="left"&gt;73.6&lt;/td&gt; &lt;td align="left"&gt;71.9&lt;/td&gt; &lt;td align="left"&gt;64.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;InternVL3.5-2B&lt;/td&gt; &lt;td align="left"&gt;2.2B&lt;/td&gt; &lt;td align="left"&gt;71.6&lt;/td&gt; &lt;td align="left"&gt;74.6&lt;/td&gt; &lt;td align="left"&gt;70.9&lt;/td&gt; &lt;td align="left"&gt;62.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Source: &lt;a href="https://huggingface.co/jinaai/jina-vlm"&gt;Hugging Face model card&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xsgg4t96py5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph9pg9/new_jinavlm24b_reaches_sota_for_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph9pg9/new_jinavlm24b_reaches_sota_for_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:06:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph95i2</id>
    <title>chatllm.cpp adds support of Ministral-3 &amp; llama.cpp WebUI</title>
    <updated>2025-12-08T10:33:00+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph95i2/chatllmcpp_adds_support_of_ministral3_llamacpp/"&gt; &lt;img alt="chatllm.cpp adds support of Ministral-3 &amp;amp; llama.cpp WebUI" src="https://preview.redd.it/9jrhugdzjy5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f33d8f70c2fae1ee03aa0b0fc655e88ed3385bd" title="chatllm.cpp adds support of Ministral-3 &amp;amp; llama.cpp WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9jrhugdzjy5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph95i2/chatllmcpp_adds_support_of_ministral3_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph95i2/chatllmcpp_adds_support_of_ministral3_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T10:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1phbhfi</id>
    <title>GLM released 4.6V including the apparent successor to Air. But I'm most interested to test the 9B Flash version</title>
    <updated>2025-12-08T12:45:22+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phbhfi/glm_released_46v_including_the_apparent_successor/"&gt; &lt;img alt="GLM released 4.6V including the apparent successor to Air. But I'm most interested to test the 9B Flash version" src="https://b.thumbs.redditmedia.com/nuEKGBrDZUDPlfJCcQZYeqThOJ80QAYxWYX9qbE_iMU.jpg" title="GLM released 4.6V including the apparent successor to Air. But I'm most interested to test the 9B Flash version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V-Flash"&gt;https://huggingface.co/zai-org/GLM-4.6V-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1191vuzn7z5g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05360b416baa64cc163305c635af3aa5bd121c8b"&gt;https://preview.redd.it/1191vuzn7z5g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05360b416baa64cc163305c635af3aa5bd121c8b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phbhfi/glm_released_46v_including_the_apparent_successor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phbhfi/glm_released_46v_including_the_apparent_successor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phbhfi/glm_released_46v_including_the_apparent_successor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T12:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph6kfj</id>
    <title>GLM-4.6 Derestricted</title>
    <updated>2025-12-08T07:42:27+00:00</updated>
    <author>
      <name>/u/Digger412</name>
      <uri>https://old.reddit.com/user/Digger412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, figured I'd post here to get some more eyes on this. I've produced and GGUF'd a norm-preserving biprojected ablation of GLM-4.6: &lt;a href="https://huggingface.co/AesSedai/GLM-4.6-Derestricted-GGUF"&gt;https://huggingface.co/AesSedai/GLM-4.6-Derestricted-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mostly been discussing this in the BeaverAI discord but it's been generally well-received by the group there. This model should be suitable for normal assistant work, but was produced with the intent of improving some of the creative writing aspects of the model. Overall the writing feels like it doesn't inherit the same level of repetitive sentence structure patterning that the base model has, but it's not a finetune so it doesn't address some of the other known GLM-4.5/4.6 issues (eg, echoing / parroting as well as &amp;quot;slop&amp;quot; word usage patterns). The change is substantial enough that it does feel like a better model to use IMO though. &lt;/p&gt; &lt;p&gt;As mentioned in the readme, I went with a fairly light abliteration targeting the middle layers of the model. It is NOT a &amp;quot;fully decensored&amp;quot; / &amp;quot;fully derestricted&amp;quot; model that will give you zero-shot-zero-system-prompt derestricted replies. A light system prompt JB or the like is necessary to help nudge it, but it will be less censored / restricted than the base model after that. Using too heavy of an abliteration config risks damaging the intelligence of the model, so I went with this comparatively lighter touch.&lt;/p&gt; &lt;p&gt;Included in the repo is a link to Jim's llm-abliteration repo with the PR I used for producing the ablated model, as well as the measurements I collected and config I used. If someone wants to produce their own quant, they can reproduce my work that way with (hopefully) minimal effort.&lt;/p&gt; &lt;p&gt;I'm working on some further improvements to the llm-abliteration process, and looking to abliterate Kimi-K2 Thinking in the near future (probably within a month). I might circle back around to some smaller models, like gemma-3-27b, and see about producing some abliterated versions of those. Will see what happens, but if you do use this GLM-4.6 Derestricted I'd be happy to hear your feedback.&lt;/p&gt; &lt;p&gt;Thanks,&lt;/p&gt; &lt;p&gt;- Aes Sedai&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digger412"&gt; /u/Digger412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph6kfj/glm46_derestricted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph6kfj/glm46_derestricted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph6kfj/glm46_derestricted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T07:42:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1phcyvk</id>
    <title>After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM</title>
    <updated>2025-12-08T13:54:31+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt; &lt;img alt="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" src="https://a.thumbs.redditmedia.com/cM8ZY8pfeiL7V-euNxiaRZSPcskPH5ahnCORrr5O-W4.jpg" title="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, it's ugly and frankly embarrassing to look at. I just finished this build last night by adding 2 additional GPUs to go from 6 to 8, where I will stop &amp;amp; call this build complete.&lt;/p&gt; &lt;p&gt;I've built many PCs over the years but this was a whole other level and at this point I'm just happy it works. It runs off daisy chained 1500W and 1000W PSUs (5 cards on the 1500W and 3 on the 1000W), and the system is fed by a 20A dedicated branch circuit.&lt;/p&gt; &lt;p&gt;Cramming the GPUs in a case without having to use long GPU riser cables was the hardest part. If I were to do this again, I'd just use long PCIE 1x cables that give me the freedom to neatly stack the cards and save myself the headache, since this is just an inference system... only time PCIE bandwidth matters is when loading models. But I went down the path of using certified PCIE 4.0 cables that range from 200-250mm, &amp;amp; as you can see, it ain't pretty. One card has to sit outside the rack bc there was simply no space for it among the chonky GPUs &amp;amp; PCIE riser spaghetti.&lt;/p&gt; &lt;p&gt;Good news is that the system has been running stable for it's entire existence as I kept adding parts &amp;amp; just learning as I go. GPU temps never exceed 70ish*C under load since the GPUs are pretty well spread out in an open case, and all in I spent about $8k, as almost every part in the system is used (only the motherboard was bought new - a supermicro supermicro h12ssl-i which was $400 at the time).&lt;br /&gt; The most I paid for a GPU was $700, the lowest was $500, which was just this week. FB Marketplace is great in my area - I had tons of options and I highly recommend local sellers over ebay.&lt;br /&gt; All I've done so far is load GLM 4.5 air Q6_K GGUF using llama.cpp, specifically these settings - &lt;code&gt;llama-server \-m /home/hisma/llama.cpp/models/GLM-4.5-Air.i1-Q6_K/GLM-4.5-Air.i1-Q6_K.gguf -c 131072 -ngl 99 -b 4096 -ub 2048 -fa --temp 0.6 --top-p 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8888&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From the screenshot, you can see it pulled off a respectable ~49 t/s.&lt;br /&gt; My next steps -&lt;/p&gt; &lt;ul&gt; &lt;li&gt;power limit all cards to ~250W (maybe lower depending on how my system responds - confident I shouldn't need to go any lower than 200W which would only be a ~20% perf hit)&lt;/li&gt; &lt;li&gt;test some AWQ models using VLLM with tensor parallelism (specifically MiniMax-M2-AWQ-4bit). &lt;ul&gt; &lt;li&gt;My whole reason for going to 8 GPUs is bc TP requires either 2, 4 or 8 cards. So 8 cards was always my goal to get the most out of this system&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Once I find a solid set of models, start doing some agentic coding with roocode &amp;amp; let this thing rip&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With PC hardware prices going insane lately, I feel lucky to have this thing, even with the janky ass build. It was a good learning experience &amp;amp; certainly would do some things different w/ the lessons I learned, but I forsee future enshittification of cloud models as the big corpos pivot to pleasing shareholders over burning cash, and in the 1 year I've had this system local models have continued to improve and trade blows with frontier models while using less memory, I'm sure the trend will continue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phcyvk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:54:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph9p98</id>
    <title>Jan v0.7.5: Jan Browser MCP extension, file attachment, Flatpak support</title>
    <updated>2025-12-08T11:06:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph9p98/jan_v075_jan_browser_mcp_extension_file/"&gt; &lt;img alt="Jan v0.7.5: Jan Browser MCP extension, file attachment, Flatpak support" src="https://external-preview.redd.it/NXdnNzE2dGlueTVnMV4cnJTtthTMZiZkt117uzBSxdM9b0R_GvilWExkrjZE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a082df240af7dda4ee343b54d0ed3e0f59c452d6" title="Jan v0.7.5: Jan Browser MCP extension, file attachment, Flatpak support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're releasing Jan v0.7.5 with the Jan Browser MCP and a few updates many of you asked for.&lt;/p&gt; &lt;p&gt;With this release, Jan has a Chromium extension that makes browser use simpler and more stable. Install the Jan extension from the Chrome Web Store, connect it to Jan. The video above shows the quick steps. &lt;/p&gt; &lt;p&gt;You can now attach files directly in chat.&lt;/p&gt; &lt;p&gt;and yes, Flatpak support is finally here! This has been requested for months, and Linux users should have a better setup now.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan Browser MCP: &lt;a href="https://chromewebstore.google.com/detail/jan-browser-mcp/mkciifcjehgnpaigoiaakdgabbpfppal"&gt;https://chromewebstore.google.com/detail/jan-browser-mcp/mkciifcjehgnpaigoiaakdgabbpfppal&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan on Flathub: &lt;a href="https://flathub.org/en/apps/ai.jan.Jan"&gt;https://flathub.org/en/apps/ai.jan.Jan&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan GitHub: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please update your &lt;a href="https://www.jan.ai/"&gt;Jan&lt;/a&gt; or download the latest.&lt;/p&gt; &lt;p&gt;I'm Emre from the Jan - happy to answer your questions.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: Browser performance still depends on the model's MCP capabilities. In some cases, it doesn't pick the best option yet, as shown in the video... We also found a parser issue in llama.cpp that affects reliability, and we're working on it.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9gjzooqiny5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph9p98/jan_v075_jan_browser_mcp_extension_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph9p98/jan_v075_jan_browser_mcp_extension_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pha9tj</id>
    <title>GLM-4.6V Collection</title>
    <updated>2025-12-08T11:40:14+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha9tj/glm46v_collection/"&gt; &lt;img alt="GLM-4.6V Collection" src="https://preview.redd.it/md0quv12wy5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffddfa59ffb2cbd190d950399949f478b447d402" title="GLM-4.6V Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;https://huggingface.co/collections/zai-org/glm-46v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/md0quv12wy5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha9tj/glm46v_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pha9tj/glm46v_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1phbfao</id>
    <title>GLM-4.6V, the latest open-source vision language models</title>
    <updated>2025-12-08T12:42:27+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phbfao/glm46v_the_latest_opensource_vision_language/"&gt; &lt;img alt="GLM-4.6V, the latest open-source vision language models" src="https://preview.redd.it/1qizxqj46z5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=befbb4427d0d9dff091f3f16c366351798b96e46" title="GLM-4.6V, the latest open-source vision language models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.6V series model includes two versions: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM-4.6V (106B) - model designed for cloud and high-performance cluster scenarios, and&lt;/li&gt; &lt;li&gt;GLM-4.6V-Flash (9B) - lightweight model optimized for local deployment and low-latency applications. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GLM-4.6V achieves SoTA performance in visual understanding among models of similar parameter scales.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features of this model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native Function Calling capabilities for the first time.&lt;/li&gt; &lt;li&gt;Supports processing up to 128 K context tokens.&lt;/li&gt; &lt;li&gt;Designed for vision-language tasks — images + text both supported.&lt;/li&gt; &lt;li&gt;Offers improved reasoning and alignment with human preferences.&lt;/li&gt; &lt;li&gt;Suitable for complex multimodal workflows (e.g., long documents + images).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source: &lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Hugging Face Model Collection&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1qizxqj46z5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phbfao/glm46v_the_latest_opensource_vision_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phbfao/glm46v_the_latest_opensource_vision_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T12:42:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph5h2q</id>
    <title>RTX 5090 96 GB just popped up on Alibababa</title>
    <updated>2025-12-08T06:33:36+00:00</updated>
    <author>
      <name>/u/RateRoutine2268</name>
      <uri>https://old.reddit.com/user/RateRoutine2268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI Guys,&lt;br /&gt; Just found RTX 5090 96 GB on Alibaba from a verified vendor&lt;br /&gt; :&lt;a href="https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html"&gt;https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I contacted vendor and waiting for reply , anyone tried it yet?&lt;/p&gt; &lt;p&gt;EDIT : Based on supplier replies , it seems its not available yet , *sad noises*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RateRoutine2268"&gt; /u/RateRoutine2268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T06:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgza25</id>
    <title>Is this THAT bad today?</title>
    <updated>2025-12-08T01:15:06+00:00</updated>
    <author>
      <name>/u/Normal-Industry-8055</name>
      <uri>https://old.reddit.com/user/Normal-Industry-8055</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgza25/is_this_that_bad_today/"&gt; &lt;img alt="Is this THAT bad today?" src="https://preview.redd.it/3vg1imwjsv5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7a9691b1258cf67a1b8b9b9c4b102035b613ca0" title="Is this THAT bad today?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already bought it. We all know the market... This is special order so not in stock on Provantage but they estimate it should be in stock soon . With Micron leaving us, I don't see prices getting any lower for the next 6-12 mo minimum. What do you all think? For today’s market I don’t think I’m gonna see anything better. Only thing to worry about is if these sticks never get restocked ever.. which I know will happen soon. But I doubt they’re already all completely gone.&lt;/p&gt; &lt;p&gt;link for anyone interested: &lt;a href="https://www.provantage.com/crucial-technology-ct2k64g64c52cu5%7E7CIAL836.htm"&gt;https://www.provantage.com/crucial-technology-ct2k64g64c52cu5~7CIAL836.htm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Industry-8055"&gt; /u/Normal-Industry-8055 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3vg1imwjsv5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgza25/is_this_that_bad_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgza25/is_this_that_bad_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T01:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1phaaon</id>
    <title>GLM-4.6V (108B) has been released</title>
    <updated>2025-12-08T11:41:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt; &lt;img alt="GLM-4.6V (108B) has been released" src="https://b.thumbs.redditmedia.com/3UlSBmijpC7kl1f1Bcn1r5q-4r_S7Xxg1tTNLZ4W9ms.jpg" title="GLM-4.6V (108B) has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dyfhb6nhwy5g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d03177e251a72b04491b10634e66bdde1a9544c5"&gt;https://preview.redd.it/dyfhb6nhwy5g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d03177e251a72b04491b10634e66bdde1a9544c5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4.6V series model includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance cluster scenarios, and GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications. GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding among models of similar parameter scales. Crucially, we integrate native Function Calling capabilities for the first time. This effectively bridges the gap between &amp;quot;visual perception&amp;quot; and &amp;quot;executable action&amp;quot; providing a unified technical foundation for multimodal agents in real-world business scenarios.&lt;/p&gt; &lt;p&gt;Beyond achieves SoTA performance across major multimodal benchmarks at comparable model scales. GLM-4.6V introduces several key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Native Multimodal Function Calling&lt;/strong&gt; Enables native vision-driven tool use. Images, screenshots, and document pages can be passed directly as tool inputs without text conversion, while visual outputs (charts, search images, rendered pages) are interpreted and integrated into the reasoning chain. This closes the loop from perception to understanding to execution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interleaved Image-Text Content Generation&lt;/strong&gt; Supports high-quality mixed media creation from complex multimodal inputs. GLM-4.6V takes a multimodal context—spanning documents, user inputs, and tool-retrieved images—and synthesizes coherent, interleaved image-text content tailored to the task. During generation it can actively call search and retrieval tools to gather and curate additional text and visuals, producing rich, visually grounded content.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Document Understanding&lt;/strong&gt; GLM-4.6V can process up to 128K tokens of multi-document or long-document input, directly interpreting richly formatted pages as images. It understands text, layout, charts, tables, and figures jointly, enabling accurate comprehension of complex, image-heavy documents without requiring prior conversion to plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend Replication &amp;amp; Visual Editing&lt;/strong&gt; Reconstructs pixel-accurate HTML/CSS from UI screenshots and supports natural-language-driven edits. It detects layout, components, and styles visually, generates clean code, and applies iterative visual modifications through simple user instructions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V"&gt;https://huggingface.co/zai-org/GLM-4.6V&lt;/a&gt;&lt;/p&gt; &lt;p&gt;please notice that llama.cpp support for GLM 4.5V is still draft&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16600"&gt;https://github.com/ggml-org/llama.cpp/pull/16600&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pha7l1</id>
    <title>zai-org/GLM-4.6V-Flash (9B) is here</title>
    <updated>2025-12-08T11:36:39+00:00</updated>
    <author>
      <name>/u/Cute-Sprinkles4911</name>
      <uri>https://old.reddit.com/user/Cute-Sprinkles4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks incredible for your own machine. &lt;/p&gt; &lt;p&gt;GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications. GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding among models of similar parameter scales. Crucially, we integrate native Function Calling capabilities for the first time. This effectively bridges the gap between &amp;quot;visual perception&amp;quot; and &amp;quot;executable action&amp;quot; providing a unified technical foundation for multimodal agents in real-world business scenarios.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V-Flash"&gt;https://huggingface.co/zai-org/GLM-4.6V-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Sprinkles4911"&gt; /u/Cute-Sprinkles4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph7njc</id>
    <title>Vector db comparison</title>
    <updated>2025-12-08T08:55:16+00:00</updated>
    <author>
      <name>/u/Kaneki_Sana</name>
      <uri>https://old.reddit.com/user/Kaneki_Sana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"&gt; &lt;img alt="Vector db comparison" src="https://b.thumbs.redditmedia.com/EQXPxZZvz2rHdMZgPyJSqdIDNHw4hQWeTCCI6KvOvdQ.jpg" title="Vector db comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for the best vector for our RAG product, and went down a rabbit hole to compare all of them. Key findings:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;RAG systems under ~10M vectors, standard HNSW is fine.&lt;/strong&gt; Above that, you'll need to choose a different index. &lt;/p&gt; &lt;p&gt;- Large dataset + cost-sensitive&lt;em&gt;:&lt;/em&gt; &lt;strong&gt;Turbopuffer.&lt;/strong&gt; Object storage makes it cheap at scale.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;pgvector&lt;/strong&gt; is good for small scale and local experiments. Specialized vector dbs perform better at scale.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Chroma&lt;/strong&gt; - Lightweight, good for running in notebooks or small servers&lt;/p&gt; &lt;p&gt;Here's the full breakdown: &lt;a href="https://agentset.ai/blog/best-vector-db-for-rag"&gt;https://agentset.ai/blog/best-vector-db-for-rag&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kaneki_Sana"&gt; /u/Kaneki_Sana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ph7njc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T08:55:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph8wel</id>
    <title>RAM prices explained</title>
    <updated>2025-12-08T10:17:09+00:00</updated>
    <author>
      <name>/u/Lopsided_Sentence_18</name>
      <uri>https://old.reddit.com/user/Lopsided_Sentence_18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI bought up 40% of global DRAM production in raw wafers they're not even using - just stockpiling to deny competitors access. Result? Memory prices are skyrocketing. Month before chrismass.&lt;/p&gt; &lt;p&gt;Source: Moore´s law is Dead&lt;br /&gt; Link: &lt;a href="https://www.mooreslawisdead.com/post/sam-altman-s-dirty-dram-deal"&gt;Sam Altman’s Dirty DRAM Deal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Sentence_18"&gt; /u/Lopsided_Sentence_18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T10:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
