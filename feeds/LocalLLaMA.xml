<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-02T18:47:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qsvqy6</id>
    <title>An image is worth a 1000 words? ClawdBot vs Kubernetes</title>
    <updated>2026-02-01T11:09:56+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvqy6/an_image_is_worth_a_1000_words_clawdbot_vs/"&gt; &lt;img alt="An image is worth a 1000 words? ClawdBot vs Kubernetes" src="https://preview.redd.it/uzi0h1wi8vgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c27d459fc1ba8972b8be37eb656fa46bfaa39477" title="An image is worth a 1000 words? ClawdBot vs Kubernetes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uzi0h1wi8vgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvqy6/an_image_is_worth_a_1000_words_clawdbot_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvqy6/an_image_is_worth_a_1000_words_clawdbot_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T11:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsqxho</id>
    <title>what did you run when you got a second rtx 6000 pro?</title>
    <updated>2026-02-01T06:31:44+00:00</updated>
    <author>
      <name>/u/az_6</name>
      <uri>https://old.reddit.com/user/az_6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a single 6000 pro and am thinking about getting another. What did you start running when you got a second 6000 pro that made the price worth it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/az_6"&gt; /u/az_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsqxho/what_did_you_run_when_you_got_a_second_rtx_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsqxho/what_did_you_run_when_you_got_a_second_rtx_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsqxho/what_did_you_run_when_you_got_a_second_rtx_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T06:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr7ncz</id>
    <title>Design Arena is now dominated by an open model</title>
    <updated>2026-01-30T14:55:35+00:00</updated>
    <author>
      <name>/u/moks4tda</name>
      <uri>https://old.reddit.com/user/moks4tda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt; &lt;img alt="Design Arena is now dominated by an open model" src="https://a.thumbs.redditmedia.com/IOzZQkj-NN9LpvwuZen1AYFWFys9dnrIFBwpaZCd7D0.jpg" title="Design Arena is now dominated by an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The first month of 2026 is already this wild, I can't even imagine what's coming next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moks4tda"&gt; /u/moks4tda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qr7ncz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrqk9o</id>
    <title>Managed to run Kimi k2.5 IQ4-SX locally.</title>
    <updated>2026-01-31T02:56:33+00:00</updated>
    <author>
      <name>/u/el3mancee</name>
      <uri>https://old.reddit.com/user/el3mancee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"&gt; &lt;img alt="Managed to run Kimi k2.5 IQ4-SX locally." src="https://a.thumbs.redditmedia.com/wPblHyZa-nLIek-Bsb8Vto-LtlSalMG4iuSpvu7oeE0.jpg" title="Managed to run Kimi k2.5 IQ4-SX locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Loaded with a max token capable(262,114 tokens)&lt;/p&gt; &lt;p&gt;1 Max Studio M1 Ultra(host), 1 Asus Gx10, 3 Strix Halo. Connected with Thunderbolt and 10 Gbps Ethernet. &lt;/p&gt; &lt;p&gt;Tg 8.5 tps. Pp 15-20 tps.&lt;/p&gt; &lt;p&gt;Can reach ~15 tps tg when using concurrent requests.&lt;/p&gt; &lt;p&gt;Pretty slow for production, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el3mancee"&gt; /u/el3mancee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qrqk9o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T02:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsg48d</id>
    <title>I made a LLM based simple IDS/IPS for nginx for fun, using gpt-oss-120b on my own DGX Spark as the model, so I don't have to deal with rate limits or token usage.</title>
    <updated>2026-01-31T22:18:43+00:00</updated>
    <author>
      <name>/u/Saren-WTAKO</name>
      <uri>https://old.reddit.com/user/Saren-WTAKO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsg48d/i_made_a_llm_based_simple_idsips_for_nginx_for/"&gt; &lt;img alt="I made a LLM based simple IDS/IPS for nginx for fun, using gpt-oss-120b on my own DGX Spark as the model, so I don't have to deal with rate limits or token usage." src="https://preview.redd.it/gjnib5t7frgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef34cad52be5e842432ff343093db5a0b273360e" title="I made a LLM based simple IDS/IPS for nginx for fun, using gpt-oss-120b on my own DGX Spark as the model, so I don't have to deal with rate limits or token usage." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What it does and how it works: A vibe coded script would monitor my nginx logs, submit the context and logs (with /24 block of same IP, in case of small scale DDoS) to LLM for consideration. Then, the LLM would issue an IP ban automatically with reason, and notifies me. &lt;/p&gt; &lt;p&gt;When an IP is banned, nginx config is updated and nginx process is restarted. Then, a reviewer script that is sharp vibe coded determines how long the IP should be banned and give a verdict. If it's false positive, it will be unbanned immediately . If it's unsolicited bot or it has weird UA, would ban for 1-24 hours. If it's obviously malicious, then indefinite (30 days) ban. &lt;/p&gt; &lt;p&gt;A summary will be sent to my telegram group topic on script (re)start and every few hours. By using telegram, I can quote the summary to ask for more details and nginx rules to add. I can unban an IP, and I can add &amp;quot;memories&amp;quot; which is more context for a nginx server section, mostly used for minimize false positives. &lt;/p&gt; &lt;p&gt;The first version was done last September. I stopped it because Openrouter didn't really like how I used the free requests 24/7. And because I was VRAM poor, using a small model is inviting troubles for this kind of tasks, obviously.&lt;/p&gt; &lt;p&gt;This is never going to be commercially useful, by the way. This isn't realtime IDS/IPS and never will be, and it makes mistakes, fairly easily despite I am using a moderately intelligent model. &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Entrypoint to my server at home (hopefully this won't be hacked when I wake up, but it's battle tested so it should be fine): &lt;a href="https://apps.wtako.net/board"&gt;https://apps.wtako.net/board&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Optimized vllm deployment: &lt;a href="https://github.com/christopherowen/spark-vllm-mxfp4-docker"&gt;https://github.com/christopherowen/spark-vllm-mxfp4-docker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLM IDS/IPS: &lt;a href="https://github.com/Saren-Arterius/llm-nginx-monitor"&gt;https://github.com/Saren-Arterius/llm-nginx-monitor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saren-WTAKO"&gt; /u/Saren-WTAKO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gjnib5t7frgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsg48d/i_made_a_llm_based_simple_idsips_for_nginx_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsg48d/i_made_a_llm_based_simple_idsips_for_nginx_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T22:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs49y0</id>
    <title>Heterogeneous Clustering</title>
    <updated>2026-01-31T14:49:27+00:00</updated>
    <author>
      <name>/u/Miserable-Dare5090</name>
      <uri>https://old.reddit.com/user/Miserable-Dare5090</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With knowledge of the different runtimes supported in different hardwares (CUDA, ROCm, Metal), I wanted to know if there is a reason why the same model quant on the same runtime frontend (vLLM, Llama.cpp) would not be able to run distributed inference. &lt;/p&gt; &lt;p&gt;Is there something I’m missing? &lt;/p&gt; &lt;p&gt;Can a strix halo platform running rocm/vllm be combined with a cuda/vllm instance on a spark (provided they are connected via fiber networking)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable-Dare5090"&gt; /u/Miserable-Dare5090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs49y0/heterogeneous_clustering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs49y0/heterogeneous_clustering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs49y0/heterogeneous_clustering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T14:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsbdla</id>
    <title>I built a tool to see what AI agents (Moltbot, Claude, Cursor) are actually doing on your computer</title>
    <updated>2026-01-31T19:15:18+00:00</updated>
    <author>
      <name>/u/gregb_parkingaccess</name>
      <uri>https://old.reddit.com/user/gregb_parkingaccess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone's installing AI agents that can control their entire computer. Moltbot, Clawdbot, Claude Desktop, Cursor - they can read files, click anywhere, take screenshots.&lt;/p&gt; &lt;p&gt;But there's zero visibility into what they're doing.&lt;/p&gt; &lt;p&gt;So I built Molteye. It's a simple Electron app that:&lt;/p&gt; &lt;p&gt;- Shows when AI agents start/stop&lt;/p&gt; &lt;p&gt;- Logs file changes while AI is active&lt;/p&gt; &lt;p&gt;- Alerts on sensitive files (.env, .ssh, credentials)&lt;/p&gt; &lt;p&gt;~100 lines of code. Runs 100% local. No cloud, no tracking.&lt;/p&gt; &lt;p&gt;Mac only for now. Looking for help with Windows support.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/gbessoni/molteye"&gt;https://github.com/gbessoni/molteye&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community - you guys care about local/private AI more than anyone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gregb_parkingaccess"&gt; /u/gregb_parkingaccess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbdla/i_built_a_tool_to_see_what_ai_agents_moltbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbdla/i_built_a_tool_to_see_what_ai_agents_moltbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbdla/i_built_a_tool_to_see_what_ai_agents_moltbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T19:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs5t82</id>
    <title>14 ICLR 2026 papers on why multi-agent systems fail (latency, costs, error cascades)</title>
    <updated>2026-01-31T15:50:01+00:00</updated>
    <author>
      <name>/u/dippatel21</name>
      <uri>https://old.reddit.com/user/dippatel21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Went through the &lt;strong&gt;ICLR 2026&lt;/strong&gt; accepted papers, looking for work relevant to multi-agent production problems. Found 14 papers that cluster around 5 issues:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Latency (sequential execution)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Speculative Actions: parallel API execution via action prediction, ~30% speedup&lt;/p&gt; &lt;p&gt;- Graph-of-Agents: agent selection based on model cards, reduces routing overhead&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Token costs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- KVComm: share KV pairs instead of text, 30% of layers achieve near-full performance&lt;/p&gt; &lt;p&gt;- MEM1: constant context size via RL-based memory consolidation, 3.7x memory reduction&lt;/p&gt; &lt;p&gt;- PCE: structured decision trees to reduce inter-agent communication&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Error cascades&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- ViF: identifies &amp;quot;hallucination snowballing&amp;quot; in visual MAS, proposes visual token relay&lt;/p&gt; &lt;p&gt;- Noise decomposition framework for RAG chunking decisions (task/model/aggregator noise)&lt;/p&gt; &lt;p&gt;- DoVer: intervention-driven debugging, flips 28% of failures to successes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Brittle topologies&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- CARD: conditional graph generation adapting to runtime&lt;/p&gt; &lt;p&gt;- MAS²: self-generating architecture, 19.6% gains over static systems&lt;/p&gt; &lt;p&gt;- Stochastic Self-Organization: emergent DAG via Shapley-value peer assessment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Observability&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- GLC: compressed communication symbols aligned to human concepts&lt;/p&gt; &lt;p&gt;- Emergent Coordination: information-theoretic metrics for real vs spurious coordination&lt;/p&gt; &lt;p&gt;Full writeup with paper links: &lt;a href="https://llmsresearch.substack.com/p/what-iclr-2026-taught-us-about-multi?r=74sxh5"&gt;https://llmsresearch.substack.com/p/what-iclr-2026-taught-us-about-multi?r=74sxh5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious which of these problems you have hit most in production.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dippatel21"&gt; /u/dippatel21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs5t82/14_iclr_2026_papers_on_why_multiagent_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs5t82/14_iclr_2026_papers_on_why_multiagent_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs5t82/14_iclr_2026_papers_on_why_multiagent_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T15:50:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrfbo8</id>
    <title>NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development</title>
    <updated>2026-01-30T19:26:46+00:00</updated>
    <author>
      <name>/u/Delicious_Air_737</name>
      <uri>https://old.reddit.com/user/Delicious_Air_737</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt; &lt;img alt="NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development" src="https://b.thumbs.redditmedia.com/vc1UXvPAXbGKJMGRgO3vaYujfIckG9Mk-wfTZzXzgEY.jpg" title="NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6key4zy0fjgg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b0bfa274d54a0e695e0cbc067cd40c4c9dfa4e"&gt;https://preview.redd.it/6key4zy0fjgg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b0bfa274d54a0e695e0cbc067cd40c4c9dfa4e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At CES 2026, NVIDIA announced what might be &lt;a href="https://namiru.ai/blog/nvidia-releases-massive-collection-of-open-models-data-and-tools-to-accelerate-ai-development?source=red-nvidia-kinga"&gt;the most significant open-source AI release&lt;/a&gt; to date. The company unveiled new models, datasets, and tools spanning everything from speech recognition to drug discovery.&lt;/p&gt; &lt;p&gt;For regular users, this release means better voice assistants, smarter document search, faster drug development, safer self-driving cars, and more capable robots. These technologies will filter into consumer products throughout 2026.&lt;/p&gt; &lt;p&gt;NVIDIA is betting that by enabling the entire AI ecosystem, they sell more GPUs. Based on the companies already adopting these technologies, that bet is paying off. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Air_737"&gt; /u/Delicious_Air_737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T19:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsz1p3</id>
    <title>State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI | Lex Fridman Podcast #490</title>
    <updated>2026-02-01T13:54:03+00:00</updated>
    <author>
      <name>/u/EverythingIsFnTaken</name>
      <uri>https://old.reddit.com/user/EverythingIsFnTaken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsz1p3/state_of_ai_in_2026_llms_coding_scaling_laws/"&gt; &lt;img alt="State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI | Lex Fridman Podcast #490" src="https://external-preview.redd.it/8eEVlMA9SaLrQ96O3yRZJzXK1oUdfsSfck0S_OtKU7s.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3162fe061e65ff0470849fb32709fd23d65eb6ff" title="State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI | Lex Fridman Podcast #490" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EverythingIsFnTaken"&gt; /u/EverythingIsFnTaken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=EV7WhVT270Q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsz1p3/state_of_ai_in_2026_llms_coding_scaling_laws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsz1p3/state_of_ai_in_2026_llms_coding_scaling_laws/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T13:54:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsdo9h</id>
    <title>is this Speed normal?</title>
    <updated>2026-01-31T20:42:57+00:00</updated>
    <author>
      <name>/u/Noobysz</name>
      <uri>https://old.reddit.com/user/Noobysz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im using lklammacpp and i havc 3x 3090, 1x 4070Ti on pcie 16x is one 3090 and the other 2 3090s are on pcie 4x via riser, and the 4070Ti is with m.2 to oculink adapter with a Miniforum dock connected, im getting for a simple html solar system test im getting this speed is that normal ? because i think its too slow please tell me if its thats normal and if not then how can i fix it or whats wrong with my run command, it is as follows&lt;/p&gt; &lt;p&gt;llama-server.exe ^&lt;/p&gt; &lt;p&gt;--model &amp;quot;D:\models\GLM 4.7\flash\GLM-4.7-Flash-Q8_0.gguf&amp;quot; ^&lt;/p&gt; &lt;p&gt;--threads 24 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --port 8080 ^&lt;/p&gt; &lt;p&gt;--ctx-size 8192 ^&lt;/p&gt; &lt;p&gt;--n-gpu-layers 999 ^&lt;/p&gt; &lt;p&gt;--split-mode graph ^&lt;/p&gt; &lt;p&gt;--flash-attn on ^&lt;/p&gt; &lt;p&gt;--no-mmap ^&lt;/p&gt; &lt;p&gt;-b 1024 -ub 256 ^&lt;/p&gt; &lt;p&gt;--cache-type-k q4_0 --cache-type-v q4_0 ^&lt;/p&gt; &lt;p&gt;--k-cache-hadamard ^&lt;/p&gt; &lt;p&gt;--jinja ^&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8nj1or6xqgg1.png?width=1955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1de811d5b4c4d1c278037b3ca0ba6a00ae52d43"&gt;https://preview.redd.it/d8nj1or6xqgg1.png?width=1955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1de811d5b4c4d1c278037b3ca0ba6a00ae52d43&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noobysz"&gt; /u/Noobysz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdo9h/is_this_speed_normal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdo9h/is_this_speed_normal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdo9h/is_this_speed_normal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T20:42:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsa6k2</id>
    <title>Deepseek 3.2 for coding and agentic</title>
    <updated>2026-01-31T18:31:37+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at Deepseek 3.2 again&lt;/p&gt; &lt;p&gt;What are your experiences using this model for coding? In particular has it managed to do any complex projects? How is its reliability?&lt;/p&gt; &lt;p&gt;On the agentic side have you found it reliable for selecting and using tools or MCPs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsa6k2/deepseek_32_for_coding_and_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsa6k2/deepseek_32_for_coding_and_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsa6k2/deepseek_32_for_coding_and_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T18:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjtma</id>
    <title>Filipino/Tagalog local TTS. Free for commercial use.</title>
    <updated>2026-02-01T00:53:28+00:00</updated>
    <author>
      <name>/u/WETYIAFHKLZXVNM</name>
      <uri>https://old.reddit.com/user/WETYIAFHKLZXVNM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good day! Is there any local TTS that supports Filipino/Tagalog language that is free for commercial use? I'm just new to local AI. I only have 1070 8GB, R7 5700X and 32GB RAM. If upgrade is needed, is 5060 TI 16GB enough? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WETYIAFHKLZXVNM"&gt; /u/WETYIAFHKLZXVNM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjtma/filipinotagalog_local_tts_free_for_commercial_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjtma/filipinotagalog_local_tts_free_for_commercial_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjtma/filipinotagalog_local_tts_free_for_commercial_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T00:53:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrvwy6</id>
    <title>NVIDIA releases new graphics driver for old Pascal and Maxwell graphics cards - Neowin</title>
    <updated>2026-01-31T07:27:12+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.neowin.net/news/nvidia-releases-new-graphics-driver-for-old-pascal-and-maxwell-graphics-cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrvwy6/nvidia_releases_new_graphics_driver_for_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrvwy6/nvidia_releases_new_graphics_driver_for_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T07:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs36hc</id>
    <title>Looking for a simple offline AI assistant for personal use (not a developer)</title>
    <updated>2026-01-31T14:03:50+00:00</updated>
    <author>
      <name>/u/Anxious-Pie2911</name>
      <uri>https://old.reddit.com/user/Anxious-Pie2911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I want to explain my situation honestly and simply.&lt;/p&gt; &lt;p&gt;I am not a programmer and I don’t want to build some huge commercial AI system. I just want a personal AI assistant running on my own PC, mainly to help me understand things, explain documents, and work with my own data — even when the internet is not available.&lt;/p&gt; &lt;p&gt;My motivation is simple:&lt;/p&gt; &lt;p&gt;I don’t want to fully depend on online services or the internet, where access can be limited, filtered, or shut down by someone else. I want my information to stay with me, and if someone says “stop”, I can still continue working offline.&lt;/p&gt; &lt;p&gt;My current hardware is:&lt;/p&gt; &lt;p&gt;CPU: Xeon E5-2690 v4&lt;/p&gt; &lt;p&gt;RAM: 64 GB DDR4 ECC&lt;/p&gt; &lt;p&gt;GPU: NVIDIA Tesla P100 32 GB&lt;/p&gt; &lt;p&gt;Storage: 32 TB HDD + SSD&lt;/p&gt; &lt;p&gt;I am considering using a smaller local LLM (around 7B) that would act mainly as an intelligent filter / explainer, not as the main source of knowledge.&lt;/p&gt; &lt;p&gt;The actual knowledge would be stored on my own disks (HDD/SSD), organized in a simple hierarchical folder structure, for example:&lt;/p&gt; &lt;p&gt;history&lt;/p&gt; &lt;p&gt;economics&lt;/p&gt; &lt;p&gt;physics&lt;/p&gt; &lt;p&gt;technology&lt;/p&gt; &lt;p&gt;etc.&lt;/p&gt; &lt;p&gt;The idea is that the AI would:&lt;/p&gt; &lt;p&gt;search only my local files by default&lt;/p&gt; &lt;p&gt;explain things in simple language&lt;/p&gt; &lt;p&gt;help me understand complex topics&lt;/p&gt; &lt;p&gt;work offline&lt;/p&gt; &lt;p&gt;optionally compare information with the internet only when I decide to enable it&lt;/p&gt; &lt;p&gt;I know HDDs are slower, but I believe that good organization + SSD caching can make this practical for personal use.&lt;/p&gt; &lt;p&gt;My questions are:&lt;/p&gt; &lt;p&gt;Is this approach realistic for a non-programmer?&lt;/p&gt; &lt;p&gt;Are there existing tools that already do something similar?&lt;/p&gt; &lt;p&gt;What are the biggest limitations I should expect?&lt;/p&gt; &lt;p&gt;I’m not trying to build a “better ChatGPT”.&lt;/p&gt; &lt;p&gt;I just want a reliable, offline, personal assistant that helps me learn and work without being dependent on external services.&lt;/p&gt; &lt;p&gt;Thank you for any advice or experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Pie2911"&gt; /u/Anxious-Pie2911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs36hc/looking_for_a_simple_offline_ai_assistant_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs36hc/looking_for_a_simple_offline_ai_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs36hc/looking_for_a_simple_offline_ai_assistant_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T14:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrazyy</id>
    <title>Cline team got absorbed by OpenAI. Kilo is going full source available in response.</title>
    <updated>2026-01-30T16:56:49+00:00</updated>
    <author>
      <name>/u/demon_bhaiya</name>
      <uri>https://old.reddit.com/user/demon_bhaiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt; &lt;img alt="Cline team got absorbed by OpenAI. Kilo is going full source available in response." src="https://external-preview.redd.it/OJiv7stnybHLdn8-mzf6t_NZ9C8xS7VIYLhMSJsX0d8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=969735c0073c014c64189d4c6b79a9e599fe2c52" title="Cline team got absorbed by OpenAI. Kilo is going full source available in response." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who used Cline with local models, heads up that the core team appears to have joined OpenAI's Codex group based on their LinkedIn profiles. No official announcement yet, but we have seen how these acqui-hires usually play out.&lt;/p&gt; &lt;p&gt;Kilo Code (which forked from Cline and Roo Code) just responded by announcing they are making their backend source available by Feb 6. The VS Code extension, JetBrains plugin, and CLI stay Apache 2.0(Open source). Their gateway supports 500+ models including Qwen, DeepSeek, and Mistral.&lt;/p&gt; &lt;p&gt;They're offering $100 credits to anyone who contributed to Cline, and $150 per merged PR in February. If you want to keep building on an open codebase instead of watching another project disappear into a walled garden, might be worth checking out.&lt;/p&gt; &lt;p&gt;The agentic coding space needs alternatives that work with local and open weight models. Would suck to see all the decent tools end up controlled by the big labs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/demon_bhaiya"&gt; /u/demon_bhaiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.kilo.ai/p/cline-just-acqui-hired"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T16:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsx62p</id>
    <title>does any jan ai user have a severe hatred through janitor ai?</title>
    <updated>2026-02-01T12:26:37+00:00</updated>
    <author>
      <name>/u/DanteGamerxd</name>
      <uri>https://old.reddit.com/user/DanteGamerxd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok so i may be a moron but every time i search for jan ai, i keep getting the so called spicy slop &amp;quot;janitor ai&amp;quot; is this relatable to somebody? causse i dont want to be SPICY i want to run ai offline that is actually something useful rather than being a weirdo with some random servers&lt;/p&gt; &lt;p&gt;title correction: does any jan ai user have a severe hatred to janitor ai?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanteGamerxd"&gt; /u/DanteGamerxd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx62p/does_any_jan_ai_user_have_a_severe_hatred_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx62p/does_any_jan_ai_user_have_a_severe_hatred_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx62p/does_any_jan_ai_user_have_a_severe_hatred_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs1y5f</id>
    <title>Are commercial models like Claude, Gemini, and ChatGPT counting their whole internal tool calling pipeline part of their “model”? (for benchmarks)</title>
    <updated>2026-01-31T13:09:57+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When it comes to benchmark testing and comparing against open source local models, are the big companies wrapping a bunch of tools together with their base model and calling the sum of all the parts the “model”? Or are they just testing and benchmarking the base LLM without any connected tools?&lt;/p&gt; &lt;p&gt;It seems like it would be unfair to compare local models to SOTA commercial models if they are not comparing apples to apples. &lt;/p&gt; &lt;p&gt;Could we even tell if they were doing this or not? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsdwlt</id>
    <title>93GB model on a StrixHalo 128GB with 64k Context</title>
    <updated>2026-01-31T20:52:06+00:00</updated>
    <author>
      <name>/u/El_90</name>
      <uri>https://old.reddit.com/user/El_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen anyone mention getting the biggest models working on Strix Halo (or I missed them) so I thought I would document my configs in case anyone else wants to do the same and is struggling. I'm quite new to this, be gentle on me!&lt;/p&gt; &lt;p&gt;And if anyone sees room for improvement or sees issues, please give the feedback, I'm all for learning! This took many goes to get it stable. I wanted this for coding so I chose a larger model at a slower speed. &lt;/p&gt; &lt;p&gt;1: Bios - set full RAM to system/CPU (i.e. not gpu) &lt;/p&gt; &lt;p&gt;2: /etc/default/grub&lt;/p&gt; &lt;p&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet amd_iommu=off amdgpu.gttsize=131072 ttm.pages _limit=33554432&amp;quot;&lt;/p&gt; &lt;p&gt;3: Llama-server command&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080 -ngl 999 -fa on -c 65536 -b 2048 -ub 2048 -ctk q4_0 -ctv q4_0 --cache-reuse 256 --numa distribute --no-mmap --log-file --log-timestamps --perf -m /root/.cache/llama.cpp/bartowski_Qwen_Qwen3-235B-A22B-Instruct-2507-GGUF_Qwen_Qwen3-235B-A22B-Instruct-2507-IQ3_XS_Qwen_Qwen3-235B-A22B-Instruct-2507-IQ3_XS-00001-of-00003.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(I'm sure people will debate other models, this post isn't specific to the model, but on how to fit a larger GB model!)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;4: Of note:&lt;/p&gt; &lt;p&gt;High context 64k&lt;br /&gt; b/ub set to 2048, 4096 was too high&lt;br /&gt; quantised keys and vals to q4_0 &lt;/p&gt; &lt;p&gt;5: Speed&lt;/p&gt; &lt;p&gt;At the beginning of a session it's 15t/s, but as the agent continues (and context fills up?) it slows to a very stable 7-9t/s, which I'm happy with for the model size and the performance. &lt;/p&gt; &lt;p&gt;Not sure if this is valuable or not :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_90"&gt; /u/El_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdwlt/93gb_model_on_a_strixhalo_128gb_with_64k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdwlt/93gb_model_on_a_strixhalo_128gb_with_64k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdwlt/93gb_model_on_a_strixhalo_128gb_with_64k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T20:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs2cyh</id>
    <title>Early language models - how did they pull it off?</title>
    <updated>2026-01-31T13:28:33+00:00</updated>
    <author>
      <name>/u/OwnMathematician2620</name>
      <uri>https://old.reddit.com/user/OwnMathematician2620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you remember Tay, the Microsoft chatbot from 2016? Or (earliest generation of) Xiaoice from 2014? Despite the fact that AI technology has been around for many years, I find it increasingly difficult to imagine how they managed to do it back then.&lt;/p&gt; &lt;p&gt;The paper 'Attention is All You Need' was published in 2017, and the GPT-2 paper ('Language Models are Unsupervised Multitask Learners') in 2019. Yes, I know we had RNNs before that could do a similar thing, but how on earth did they handle the training dataset? Not to mention their ability to learn from many conversations during inference, which is also what got Tay taken down after only a day.&lt;/p&gt; &lt;p&gt;I don't think they even used the design principle as modern LLMs. It's a shame that I can't find any official information about Tay's architecture, as well as how it's trained...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnMathematician2620"&gt; /u/OwnMathematician2620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsww1g</id>
    <title>Here is why you should/shouldn't purchase Strix Halo</title>
    <updated>2026-02-01T12:12:11+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all,this is NOT AI-generated, it's just concise and structured so I don't waste your time.&lt;/p&gt; &lt;p&gt;What's strix halo? Strix halo is a compact Mini-PC that's optimized for AI.&lt;/p&gt; &lt;p&gt;Can I use strix halo for other things other than AI? Yes, it uses standard 64-bit architecture so most programs/Operating systems will run normally.&lt;/p&gt; &lt;p&gt;First you need to ask some questions to know if strix halo is suitable for you:&lt;/p&gt; &lt;p&gt;Is your use case AI inference? Suitable.&lt;/p&gt; &lt;p&gt;Do you need high amount of ram over bandwidth? Suitable.&lt;/p&gt; &lt;p&gt;Are you planning to use it for fine-tuning?&lt;/p&gt; &lt;p&gt;It will work due to the amount of ram,but it won't be fast due to memory bandwidth limits.&lt;/p&gt; &lt;p&gt;How optimized are it's drivers? Much better now,ROCm is well optimized but you may want to compile the programs you need for best performance.&lt;/p&gt; &lt;p&gt;Is it reliable? Yes,most strix halo Mini-PCs are reliable under consistent load.&lt;/p&gt; &lt;p&gt;What's the best Linux distro for strix halo? Fedora 43.&lt;/p&gt; &lt;p&gt;How efficient is it? Very efficient compared to performance.&lt;/p&gt; &lt;p&gt;Is cooling reliable? Based on manufacturer,but generally yes.&lt;/p&gt; &lt;p&gt;Strix halo or DGX spark? &lt;/p&gt; &lt;p&gt;Compatibility with general programs → strix halo (due to DGX Spark being ARM-based).&lt;/p&gt; &lt;p&gt;AI libraries compatibility → DGX Spark (due to CUDA).&lt;/p&gt; &lt;p&gt;Clustering → DGX Spark (strix halo is very bottlenecked in memory bandwidth if you connect two units because it doesn't contain dedicated hardware for multi-unit clustering that DGX Spark contains).&lt;/p&gt; &lt;p&gt;Price → strix halo (DGX Spark is nearly double the price).&lt;/p&gt; &lt;p&gt;Performance → almost identical (Both have similar memory bandwidth,Spark is generally faster in prefill,but token generation speed is nearly-identical).&lt;/p&gt; &lt;p&gt;Best performance for lowest price → Bosgame M5.&lt;/p&gt; &lt;p&gt;Let's discover other possibilities you may think of:&lt;/p&gt; &lt;p&gt;Why not used 3090 with 128GB of used DDR5?&lt;/p&gt; &lt;p&gt;Electricity → strix halo is more efficient,so lower bill.&lt;/p&gt; &lt;p&gt;Performance → the 3090 is so fast, but you probably need to offload so lower speeds, unless it's acceptable and you rarely run models larger than 30B so it's faster because u be on GPU more.&lt;/p&gt; &lt;p&gt;Safety → used parts are high-risk,you may receive genuine 3090, a modified one or a brick.&lt;/p&gt; &lt;p&gt;Ok,why not a refurbished/used Mac M1 Ultra instead?&lt;/p&gt; &lt;p&gt;Mac M1 ultra has the some of the same problems that the DGX Spark contains because it's an ARM CPU,So it's still less compatible as a daily driver,unless your main use case is professional and don't mind never running an OS other than MacOS,it has 800 GB of bandwidth so nearly 3x of the strix and the spark.&lt;/p&gt; &lt;p&gt;Best models for strix halo are:&lt;/p&gt; &lt;p&gt;GPT-OSS-120B → generalist.&lt;/p&gt; &lt;p&gt;GLM-4.6V → vision.&lt;/p&gt; &lt;p&gt;GLM-4.7-Flash → coding and Agentic.&lt;/p&gt; &lt;p&gt;MiniMax 2.2 → again,coding and agentic,you need a quantized REAP.&lt;/p&gt; &lt;p&gt;Qwen3-Next-80B-A3B → good for multilingual tasks.&lt;/p&gt; &lt;p&gt;That's it,wish I could help good enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsww1g/here_is_why_you_shouldshouldnt_purchase_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsww1g/here_is_why_you_shouldshouldnt_purchase_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsww1g/here_is_why_you_shouldshouldnt_purchase_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:12:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt0onq</id>
    <title>installing OpenClaw (formerly ClawdBot) locally on Windows</title>
    <updated>2026-02-01T15:01:17+00:00</updated>
    <author>
      <name>/u/elsaka0</name>
      <uri>https://old.reddit.com/user/elsaka0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just made a tutorial on installing OpenClaw (formerly ClawdBot) locally on Windows instead of paying for VPS. Saved me $15/month and works perfectly with Docker. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=gIDz_fXnZfU"&gt;https://www.youtube.com/watch?v=gIDz_fXnZfU&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Install Docker + WSL → Clone OpenClaw → Run setup → Fix pending.json pairing issue → Done &lt;/p&gt; &lt;p&gt;Anyone else ditching VPS for local installs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elsaka0"&gt; /u/elsaka0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt0onq/installing_openclaw_formerly_clawdbot_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt0onq/installing_openclaw_formerly_clawdbot_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt0onq/installing_openclaw_formerly_clawdbot_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T15:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsnnze</id>
    <title>[Project] Tired of local LLMs failing at tool use? I built ayder-cli: A coding agent script just works out of the box for Ollama &amp; Qwen3-Coder.</title>
    <updated>2026-02-01T03:47:13+00:00</updated>
    <author>
      <name>/u/FriendlySubject9469</name>
      <uri>https://old.reddit.com/user/FriendlySubject9469</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsnnze/project_tired_of_local_llms_failing_at_tool_use_i/"&gt; &lt;img alt="[Project] Tired of local LLMs failing at tool use? I built ayder-cli: A coding agent script just works out of the box for Ollama &amp;amp; Qwen3-Coder." src="https://external-preview.redd.it/d1nh8DLp6Nf2ZrDXjaoYthvQ_kKJmHcumS7_nc0u_qU.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=6e318043ee8e761dd3d35a29ecd4e50f29cac559" title="[Project] Tired of local LLMs failing at tool use? I built ayder-cli: A coding agent script just works out of the box for Ollama &amp;amp; Qwen3-Coder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most AI coding agents (Claude, gemini, copilot, kimi, Cline, etc.) are amazing but they often struggle with local models like &lt;strong&gt;Qwen3-Coder&lt;/strong&gt;. You get broken JSON, tool-calling loops, or &amp;quot;hallucinated&amp;quot; file paths, messy chat templates so on.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;ayder-cli&lt;/strong&gt; to run coding tasks on my own. It works out of the box with Ollama and is specifically tuned for the quirks of local LLM backends.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt;&lt;a href="https://github.com/ayder/ayder-cli"&gt;https://github.com/ayder/ayder-cli&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why it actually works locally:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;XML Over JSON:&lt;/strong&gt; Local models often mess up JSON quotes in tool calls. Ayder uses a &lt;strong&gt;Strict XML fallback&lt;/strong&gt; (&lt;code&gt;&amp;lt;function=...&amp;gt;&amp;lt;parameter=...&amp;gt;&lt;/code&gt;) that Qwen3-Coder was specifically trained on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Surgical Edits:&lt;/strong&gt; It uses &lt;code&gt;replace_string&lt;/code&gt; instead of overwriting whole files—essential for keeping local context windows (which are often smaller/slower) from overflowing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Task System:&lt;/strong&gt; It manages tasks as local Markdown files. Tell it &amp;quot;Implement Task 1,&amp;quot; and it loops through reading, searching, and coding autonomously until the job is done.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Current Stack:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Backends: Ollama (OpenAI-compatible). MLX-LM support will come soon hopefully.&lt;/li&gt; &lt;li&gt;Tested on &lt;a href="https://ollama.com/library/qwen3-coder"&gt;https://ollama.com/library/qwen3-coder&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Search: Built-in Ripgrep (rg) support for semantic codebase exploration.&lt;/li&gt; &lt;li&gt;Safety: For now every shell command and file edit requires a (Y/n) confirmation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you have a silicon Mac or a decent GPU and want a coding partner that doesn’t require a $20/month sub then run out of tokens give it a spin.&lt;/p&gt; &lt;p&gt;Feedback, issues, and contributions are welcome! If you try it out, let me know what you think.&lt;/p&gt; &lt;h1&gt; Development Environment&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/ayder/ayder-cli/blob/main/tests/COVERAGE.md#%EF%B8%8F-test-environment"&gt;&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;code&gt;Qwen3 Coder 30B A3B Instruct&lt;/code&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;qwen3moe&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Quantization&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;Q4_K_M&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Tensors&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;579&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Key/Value Layers&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Apple M4 Max · 36 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OS&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Tahoe 26.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;ayder-cli 0.2.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w646ngr81tgg1.png?width=1454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b82149e616061343af10ba0dd7062c4e6a95143"&gt;https://preview.redd.it/w646ngr81tgg1.png?width=1454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b82149e616061343af10ba0dd7062c4e6a95143&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendlySubject9469"&gt; /u/FriendlySubject9469 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsnnze/project_tired_of_local_llms_failing_at_tool_use_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsnnze/project_tired_of_local_llms_failing_at_tool_use_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsnnze/project_tired_of_local_llms_failing_at_tool_use_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T03:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrj1y4</id>
    <title>Stop it with the Agents/Projects Slop and spam</title>
    <updated>2026-01-30T21:44:24+00:00</updated>
    <author>
      <name>/u/Daemontatox</name>
      <uri>https://old.reddit.com/user/Daemontatox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The sub is now averaging 3-4 unfinished sloppy Agentic project that's titled the &amp;quot;best next discovery&amp;quot; or &amp;quot;alternative to [insert famous tool here]&amp;quot; or this tool is so amazing i can't even.&lt;/p&gt; &lt;p&gt;It's getting really hard to filter through them and read through the meaningful posts or actual local content.&lt;/p&gt; &lt;p&gt;We need to either add a new tag for slop or ban it altogether because the sub is slowly turning into &amp;quot;omg this tool is clawdbot 2.0&amp;quot; or some guy trying to sell his half finished project that clauded wrote for him on a weekend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemontatox"&gt; /u/Daemontatox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T21:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qso0j0</id>
    <title>Building for classified environments. Anyone else in this space?</title>
    <updated>2026-02-01T04:03:49+00:00</updated>
    <author>
      <name>/u/thefilthybeard</name>
      <uri>https://old.reddit.com/user/thefilthybeard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Working on AI-powered compliance automation that runs fully air-gapped for classified environments. No internet, no cloud, everything local on Llama.&lt;/p&gt; &lt;p&gt;Focused on STIG assessments and CMMC compliance. Trying to cut down the manual work that usually takes forever.&lt;/p&gt; &lt;p&gt;No chat interface or terminal access to the AI. The model only runs within the function of the app. Users interact with the tool, not the LLM directly. Important for environments where you can't have people prompting an AI freely.&lt;/p&gt; &lt;p&gt;Biggest challenges have been model selection (need solid performance without massive VRAM) and making sure nothing in the workflow assumes external API calls.&lt;/p&gt; &lt;p&gt;Anyone else building on Llama for offline or secure environments? Curious what problems you're solving and what you're running into.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thefilthybeard"&gt; /u/thefilthybeard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qso0j0/building_for_classified_environments_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qso0j0/building_for_classified_environments_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qso0j0/building_for_classified_environments_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T04:03:49+00:00</published>
  </entry>
</feed>
