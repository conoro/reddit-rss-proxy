<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-23T23:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1noqifv</id>
    <title>Why can’t we cancel the coding plan subscription on z.ai yet?</title>
    <updated>2025-09-23T19:14:43+00:00</updated>
    <author>
      <name>/u/thestreamcode</name>
      <uri>https://old.reddit.com/user/thestreamcode</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noqifv/why_cant_we_cancel_the_coding_plan_subscription/"&gt; &lt;img alt="Why can’t we cancel the coding plan subscription on z.ai yet?" src="https://b.thumbs.redditmedia.com/bXfs49RJuP7Kl4EVTrsiUL3mkBcw6aQ1z_Vb1xHwcdA.jpg" title="Why can’t we cancel the coding plan subscription on z.ai yet?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/el9n16f8ryqf1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f79c14146e1ca925865e4e5a724e1dfc798a53eb"&gt;https://preview.redd.it/el9n16f8ryqf1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f79c14146e1ca925865e4e5a724e1dfc798a53eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Scam? 😨&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thestreamcode"&gt; /u/thestreamcode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noqifv/why_cant_we_cancel_the_coding_plan_subscription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noqifv/why_cant_we_cancel_the_coding_plan_subscription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noqifv/why_cant_we_cancel_the_coding_plan_subscription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T19:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1novksp</id>
    <title>Qwen3VL-235B-A22B beats GPT5 and Claude-Opus 4.1</title>
    <updated>2025-09-23T22:33:38+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1novksp/qwen3vl235ba22b_beats_gpt5_and_claudeopus_41/"&gt; &lt;img alt="Qwen3VL-235B-A22B beats GPT5 and Claude-Opus 4.1" src="https://b.thumbs.redditmedia.com/Ag48nmd85eb3hIvonYG512T7qw9g5T54xsGJPhEJiQc.jpg" title="Qwen3VL-235B-A22B beats GPT5 and Claude-Opus 4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pweywhk1rzqf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20d2f6614f353eeffc5a678971d7ad8aeac98bf3"&gt;https://preview.redd.it/pweywhk1rzqf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20d2f6614f353eeffc5a678971d7ad8aeac98bf3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The open source models are leading now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1novksp/qwen3vl235ba22b_beats_gpt5_and_claudeopus_41/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1novksp/qwen3vl235ba22b_beats_gpt5_and_claudeopus_41/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1novksp/qwen3vl235ba22b_beats_gpt5_and_claudeopus_41/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T22:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nopqf9</id>
    <title>Alpie-Core: A 4-Bit Quantized Reasoning Model that Outperforms Full-Precision Models</title>
    <updated>2025-09-23T18:45:27+00:00</updated>
    <author>
      <name>/u/BlockLight2207</name>
      <uri>https://old.reddit.com/user/BlockLight2207</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopqf9/alpiecore_a_4bit_quantized_reasoning_model_that/"&gt; &lt;img alt="Alpie-Core: A 4-Bit Quantized Reasoning Model that Outperforms Full-Precision Models" src="https://external-preview.redd.it/qIMPpq1YvjlfOIpsK6C_3aIsgmMZaE_mrS8dZNWoz4k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09912e16bd4a468440b11a9f444ba3224e891706" title="Alpie-Core: A 4-Bit Quantized Reasoning Model that Outperforms Full-Precision Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m part of the team at 169Pi, and I wanted to share something we’ve been building for the past few months.&lt;/p&gt; &lt;p&gt;We just released &lt;strong&gt;Alpie Core, a 32B parameter, 4-bit quantized reasoning model.&lt;/strong&gt; It’s one of the first large-scale 4-bit reasoning models from India (and globally). Our goal wasn’t to chase trillion-parameter scaling, but instead to prove that efficiency + reasoning can coexist.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;~75% lower VRAM usage vs FP16 → runs on much more accessible hardware&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Strong performance + lower carbon + cost footprint&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Released under Apache 2.0 license (fully open to contributions)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (4-bit):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- GSM8K: 92.8%&lt;/strong&gt; (mathematical reasoning)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- SciQ: 98%&lt;/strong&gt; (scientific reasoning)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- SWE-Bench Verified: 57.8%&lt;/strong&gt; (software engineering, leading score)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- BBH: 85.1%&lt;/strong&gt; (outperforming GPT-4o, Claude 3.5, Qwen2.5)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- AIME: 47.3%&lt;/strong&gt; (strong performance on advanced mathematics)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- Humanity’s Last Exam(HLE):&lt;/strong&gt; (matching Claude 4, beating Deepseek V3, Llama 4 Maverick)&lt;/p&gt; &lt;p&gt;The model is live now on Hugging Face: &lt;a href="https://huggingface.co/169Pi/Alpie-Core"&gt;https://huggingface.co/169Pi/Alpie-Core&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also released 6 high-quality curated datasets on HF (~2B tokens) across STEM, Indic reasoning, law, psychology, coding, and advanced math to support reproducibility &amp;amp; community research.&lt;/p&gt; &lt;p&gt;We’ll also have an API &amp;amp; Playground dropping very soon, and our AI platform Alpie goes live this week, so you can try it in real workflows.&lt;/p&gt; &lt;p&gt;We’d love feedback, contributions, and even critiques from this community, the idea is to build in the open and hopefully create something useful for researchers, devs, and organisations worldwide. &lt;/p&gt; &lt;p&gt;Happy to answer any questions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nopqf9/video/15smx16jmyqf1/player"&gt;https://reddit.com/link/1nopqf9/video/15smx16jmyqf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlockLight2207"&gt; /u/BlockLight2207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopqf9/alpiecore_a_4bit_quantized_reasoning_model_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopqf9/alpiecore_a_4bit_quantized_reasoning_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nopqf9/alpiecore_a_4bit_quantized_reasoning_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T18:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1no765m</id>
    <title>how is qwen shipping so hard</title>
    <updated>2025-09-23T03:41:27+00:00</updated>
    <author>
      <name>/u/Background-Pepper-38</name>
      <uri>https://old.reddit.com/user/Background-Pepper-38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt; &lt;img alt="how is qwen shipping so hard" src="https://b.thumbs.redditmedia.com/Ix-BRKnwE48eZZgCfEQXo4d7D9ctp0BS15z0d4MC0UE.jpg" title="how is qwen shipping so hard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d0jhab945uqf1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d657e419c81e1261e3204d12b2f4c3a658aaa428"&gt;https://preview.redd.it/d0jhab945uqf1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d657e419c81e1261e3204d12b2f4c3a658aaa428&lt;/a&gt;&lt;/p&gt; &lt;p&gt;yes, how is qwen shipping so hard&lt;br /&gt; but too many variants exist that I can't decide which one to use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Background-Pepper-38"&gt; /u/Background-Pepper-38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T03:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nokxsj</id>
    <title>DeepStudio - Google AI Studio's App Builder at home (for static html/css/js apps and sites)</title>
    <updated>2025-09-23T15:45:48+00:00</updated>
    <author>
      <name>/u/Perfect_Twist713</name>
      <uri>https://old.reddit.com/user/Perfect_Twist713</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"&gt; &lt;img alt="DeepStudio - Google AI Studio's App Builder at home (for static html/css/js apps and sites)" src="https://external-preview.redd.it/yDvyf-zbJDe3LDBNM7frIodnUlArIlsW27VvFHZ7mM8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecdb4d9c5d62513c3d9d4e891cdad0e6ed24d7aa" title="DeepStudio - Google AI Studio's App Builder at home (for static html/css/js apps and sites)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4xudsfesnxqf1.png?width=3083&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dd0f4bf93f6ebc44e767adb2b13f61e4dc314c8"&gt;DeepStudio - the main workspace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Howdy!&lt;/p&gt; &lt;p&gt;I've been tinkering on &lt;strong&gt;DeepStudio&lt;/strong&gt; for a while and I think it's finally good and clean enough to share. &lt;/p&gt; &lt;p&gt;A &lt;a href="https://huggingface.co/spaces/enzostvs/deepsite"&gt;DeepSite v2&lt;/a&gt; fork where I first added support for more providers and model listing, then multi-file support, taking that much further with a Virtual File System (file storage in IndexedDB), adding agentic capabilities for the code changes, conversation/session history, checkpoints and saves, then adding sh/bash commands in the VFS for the agent to use (reducing the need for dozens of tool definitions to just 2), support for non-tool models via JSON parsing, responsive UX/UI and so much more that I can't even remember. &lt;/p&gt; &lt;p&gt;In the end I ended up with what is basically &lt;strong&gt;Google AI Studio's App Builder&lt;/strong&gt; at home.&lt;/p&gt; &lt;p&gt;Major part of the motivation for the project has also been the fact that I quite enjoy Google AI Studio's App builder for testing out ideas whether at home or out, but I always have a nagging feeling that there's going to be a day when they slap a 5k/mo price tag on it and then I'll be back to being a frustrated peasant.&lt;/p&gt; &lt;p&gt;Work with &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;LM Studio&lt;/strong&gt; as well, but I've been testing mostly with OpenRouter (note it reports 4x higher costs than actual). Some models that work well: gpt-oss-120b, Qwen3 series, GLM-4.5, Kimi K2. The closed source SOTA models obviously work great too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you're using OpenRouter or any other remote provider then be sure to set up limits&lt;/strong&gt;. Although there is a stop functionality for stopping further tool calls/processing, it's entirely possible something goes wrong and I'd be plenty miffed if someone spent their lifesavings on a html5 snake game.&lt;/p&gt; &lt;p&gt;If you make something cool with DeepStudio I'd appreciate it a lot if you could share it with me and please consider that this is a &lt;strong&gt;solo project&lt;/strong&gt; that I've been doing on the side, so please be patient if fixes take a bit of time to arrive. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF Demo&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/otst/deepstudio"&gt;https://huggingface.co/spaces/otst/deepstudio&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Git / Source code&lt;/strong&gt;: &lt;a href="https://github.com/o-stahl/deepstudio"&gt;https://github.com/o-stahl/deepstudio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Twist713"&gt; /u/Perfect_Twist713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T15:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nopjmx</id>
    <title>Magistral-Small Results in My Personal LLM Benchmark</title>
    <updated>2025-09-23T18:38:26+00:00</updated>
    <author>
      <name>/u/Different_File6723</name>
      <uri>https://old.reddit.com/user/Different_File6723</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopjmx/magistralsmall_results_in_my_personal_llm/"&gt; &lt;img alt="Magistral-Small Results in My Personal LLM Benchmark" src="https://b.thumbs.redditmedia.com/YpFNxDULzHxKiQRZnTgA_TI-uwHfVrsIZG4mpzBrYXs.jpg" title="Magistral-Small Results in My Personal LLM Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;A few days ago, I posted a thread discussing how surprised I was by the result of Magistral-small in a small personal benchmark I use to evaluate some LLMs I test. Due to the positive reception of the post, I've decided to create a couple of graphs showing some results.&lt;/p&gt; &lt;h1&gt;What does it consist of?&lt;/h1&gt; &lt;p&gt;The benchmark is based on a well-known TV show in Spain called &amp;quot;Pasapalabra.&amp;quot; The show works as follows: an alphabet is presented in a circular format (rosco), and a question starting with the first letter of the alphabet—in this case, &amp;quot;A&amp;quot;—is asked about any topic. The user must answer correctly to score points or pass to the next word. If they answer incorrectly, they are penalized; if correct, they score points. The thing is, a football (soccer) YouTube channel I follow created several challenges emulating this TV show, but with a solely football-themed focus. The questions are generally historical in nature, such as player dates, obscure team names, stadium references, or obscure rules, among others.&lt;/p&gt; &lt;p&gt;In this case, I have 104 questions, corresponding to 4 rounds (roscos) of 26 letters each. I provided all the LLMs with the option that if they were unsure of the answer or had serious doubts, they could pass to the next word instead of risking an incorrect response.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;I've created two graphs, one of which shows the hit rate, pass rate, and failure rate for each LLM. The second one shows a scoring system where the LLM earns 3 points for each correct answer, 1 point for passing, and loses 1 point for each incorrect answer. All models are in thinking mode except Kimi K2, which obviously lacks this mode, yet curiously delivers some of the best results. The LLMs with over 200 billion parameters all achieved high scores, but Magistral still surprises me, as although it failed more questions than these larger models, when combining hit and pass rates, it performs quite comparably. It's also worth noting that in 70% of the instances where Magistral passed on a word, upon reviewing its thought process, I realized it actually knew the answer but deviated at the last moment—perhaps with better prompt tuning, the results could be even better. GLM-4.5 Air also performs reasonably well, while Qwen-30B-A3B gives a worse result, and Qwen-4B performs even more poorly. Additionally, Magistral is a dense model, which I believe may also contribute to its precision.&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm a novice in all of this, so I welcome suggestions and criticism.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit: I'm adding a few more details I initially overlooked. I'm using the 3-bit quantized version of Magistral from Unsloth, while for the other LLMs I used the web versions (except for Qwen 30B and 4B, which I ran with 6-bit quantization). I've also been really impressed by one thing about Magistral: it used very few tokens on average for reasoning—the thought process was very well structured, whereas in most other LLMs, the number of tokens used to think through each question was simply absurd.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ttlbpf2lyqf1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f71c941a3edbff06009432725c4375106a64520f"&gt;https://preview.redd.it/3ttlbpf2lyqf1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f71c941a3edbff06009432725c4375106a64520f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1ydhhof2lyqf1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7331e34a2d56f023f815f5f865e2a3a0b9afeb37"&gt;https://preview.redd.it/1ydhhof2lyqf1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7331e34a2d56f023f815f5f865e2a3a0b9afeb37&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_File6723"&gt; /u/Different_File6723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopjmx/magistralsmall_results_in_my_personal_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopjmx/magistralsmall_results_in_my_personal_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nopjmx/magistralsmall_results_in_my_personal_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T18:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nomi16</id>
    <title>I built an open-source Writing Assistant inspired by Apple Intelligence, called ProseFlow.</title>
    <updated>2025-09-23T16:44:01+00:00</updated>
    <author>
      <name>/u/LSXPRIME</name>
      <uri>https://old.reddit.com/user/LSXPRIME</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomi16/i_built_an_opensource_writing_assistant_inspired/"&gt; &lt;img alt="I built an open-source Writing Assistant inspired by Apple Intelligence, called ProseFlow." src="https://external-preview.redd.it/YWUxbmU3Z3ZweHFmMQ5U_qROBXjFN3SoDz3kTm8LCTfeK5cYjjrj33SnLUqP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66e01a2875482ec6037455e8006611566e499fa2" title="I built an open-source Writing Assistant inspired by Apple Intelligence, called ProseFlow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good evening,&lt;/p&gt; &lt;p&gt;As someone who barely communicates with others, I really find it hard to write to talk to others, and while AI makes it easier, still, selecting the right words—is it correct or not—is this the best way to deliver information? Ah, while AI helps, but keeping copy-paste and refining my inputs is just frustrating. I was tired of the clunky workflow of copy-pasting text into a separate UI. I wanted my models to feel integrated into my OS. So, I built ProseFlow.&lt;/p&gt; &lt;p&gt;ProseFlow is a system-level utility that lets you apply AI actions to selected text anywhere. You highlight text in your browser, IDE, or document editor, press a hotkey, and a menu of your custom actions appears.&lt;/p&gt; &lt;p&gt;The core workflow is simple: 1. &lt;strong&gt;Select text&lt;/strong&gt; in any application. 2. &lt;strong&gt;Press a global hotkey&lt;/strong&gt; (e.g., &lt;code&gt;Ctrl+J&lt;/code&gt;). 3. A floating, searchable menu of your custom AI &lt;strong&gt;Actions&lt;/strong&gt; (Proofread, Summarize, Refactor Code) appears. 4. Select an action, and it transforms your text instantly.&lt;/p&gt; &lt;p&gt;The key features are: * &lt;strong&gt;Deep Customization:&lt;/strong&gt; You can create unlimited actions, each with its own system prompt, to tailor the model's behavior for specific tasks. * &lt;strong&gt;Iterative Refinement:&lt;/strong&gt; For complex tasks, the result opens in a window where you can conversationally refine it (e.g., &amp;quot;make it shorter,&amp;quot; &amp;quot;add bullet points&amp;quot;). * &lt;strong&gt;Smart Paste:&lt;/strong&gt; Assign a second hotkey to your most-used action for one-press text transformation. * &lt;strong&gt;Context-Aware Actions:&lt;/strong&gt; You can make actions (like code refactoring) only appear when you're in specific apps (like VS Code). * &lt;strong&gt;Official Models &amp;amp; Dataset:&lt;/strong&gt; I fine-tuned &lt;strong&gt;&lt;a href="https://huggingface.co/LSXPrime/ProseFlow-v1-1.5B-Instruct"&gt;ProseFlow-v1-1.5B-Instruct&lt;/a&gt;&lt;/strong&gt; specifically for this action-based format. It's trained on an open-source dataset I created, &lt;strong&gt;&lt;a href="https://huggingface.co/datasets/LSXPrime/ProseFlow-Actions-v1"&gt;ProseFlow-Actions-v1&lt;/a&gt;&lt;/strong&gt;, to ensure high-quality, structured output. Both are available for one-click download in the app. * &lt;strong&gt;Live Hardware Monitoring:&lt;/strong&gt; The dashboard includes real-time VRAM, RAM, CPU, and GPU monitoring so you can see exactly what your models are doing.&lt;/p&gt; &lt;p&gt;This project is free, open-source (AGPLv3), and ready for you to try. I'm looking for feedback on performance with different hardware and models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Download &amp;amp; Website:&lt;/strong&gt; &lt;a href="https://lsxprime.github.io/proseflow-web"&gt;https://lsxprime.github.io/proseflow-web&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;GitHub Repository:&lt;/strong&gt; &lt;a href="https://github.com/LSXPrime/ProseFlow"&gt;https://github.com/LSXPrime/ProseFlow&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;p&gt;macOS still untested; I would be thankful if any Mac user can confirm its functionality or report with the logs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LSXPRIME"&gt; /u/LSXPRIME &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9dqh3tfvpxqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomi16/i_built_an_opensource_writing_assistant_inspired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nomi16/i_built_an_opensource_writing_assistant_inspired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nogrv2</id>
    <title>Computer literally warms my room by 5 degrees Celsius during sustained generations</title>
    <updated>2025-09-23T13:01:27+00:00</updated>
    <author>
      <name>/u/nad_lab</name>
      <uri>https://old.reddit.com/user/nad_lab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t know how to even go about fixing this other than opening a window but for a workflow I have gpt-oss 20 b running for hours and my room acc heats up, I usually love mechanical and technological heat like 3d printing heat or heat when I play video games / pcvr BUT THIS, these ai workloads literally feel like a warm updraft from my computer, any thoughts on what to do? Anything helps on the software side to help not be so hot, yes I can and do open a window, and I live in Canada so I’m very very excited to not pay a heating bill this month cuz of this RTX 5060 ti 16 gb ram with a 3950x, cuz istg rn in the summer/fall my room avgs 30 deg c&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nad_lab"&gt; /u/nad_lab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T13:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nonbug</id>
    <title>MediaTek claims 1.58-bit BitNet support with Dimensity 9500 SoC</title>
    <updated>2025-09-23T17:14:49+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Integrating the ninth-generation MediaTek NPU 990 with Generative AI Engine 2.0 doubles compute power and introduces BitNet 1.58-bit large model processing, reducing power consumption by up to 33%. Doubling its integer and floating-point computing capabilities, users benefit from 100% faster 3 billion parameter LLM output, 128K token long text processing, and the industry’s first 4k ultra-high-definition image generation; all while slashing power consumption at peak performance by 56%.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Anyone any idea which model(s) they could have tested this on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.mediatek.com/press-room/mediatek-dimensity-9500-unleashes-best-in-class-performance-ai-experiences-and-power-efficiency-for-the-next-generation-of-mobile-devices"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nonbug/mediatek_claims_158bit_bitnet_support_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nonbug/mediatek_claims_158bit_bitnet_support_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T17:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nouiqj</id>
    <title>Qwen3-Omni thinking model running on local H100 (major leap over 2.5)</title>
    <updated>2025-09-23T21:49:43+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"&gt; &lt;img alt="Qwen3-Omni thinking model running on local H100 (major leap over 2.5)" src="https://external-preview.redd.it/ZG5qNW92cXRoenFmMQidY-VedNK5oWhNvWMcKBJGzqCaGjB2dyVwW_xfHksA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49e20215e9b1e3d03e6f379d9005a12d480bacc6" title="Qwen3-Omni thinking model running on local H100 (major leap over 2.5)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just gave the new Qwen3-Omni (thinking model) a run on my local H100.&lt;/p&gt; &lt;p&gt;Running FP8 dynamic quant with a 32k context size, enough room for 11x concurrency without issue. Latency is higher (which is expected) since thinking is enabled and it's streaming reasoning tokens.&lt;/p&gt; &lt;p&gt;But the output is sharp, and it's clearly smarter than Qwen 2.5 with better reasoning, memory, and real-world awareness.&lt;/p&gt; &lt;p&gt;It consistently understands what I’m saying, and even picked up when I was “singing” (just made some boop boop sounds lol).&lt;/p&gt; &lt;p&gt;Tool calling works too, which is huge. More on that + load testing soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hsp0mvqthzqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T21:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nonsvg</id>
    <title>Xet powers 5M models and datasets on Hugging Face</title>
    <updated>2025-09-23T17:32:52+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nonsvg/xet_powers_5m_models_and_datasets_on_hugging_face/"&gt; &lt;img alt="Xet powers 5M models and datasets on Hugging Face" src="https://preview.redd.it/8nzs9ffk9yqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=900d49ab7de15060ca933d082069e2b24385301e" title="Xet powers 5M models and datasets on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8nzs9ffk9yqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nonsvg/xet_powers_5m_models_and_datasets_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nonsvg/xet_powers_5m_models_and_datasets_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T17:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1noefxl</id>
    <title>Parkiet: Fine-tuning Dia for any language</title>
    <updated>2025-09-23T11:09:00+00:00</updated>
    <author>
      <name>/u/pevers</name>
      <uri>https://old.reddit.com/user/pevers</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"&gt; &lt;img alt="Parkiet: Fine-tuning Dia for any language" src="https://preview.redd.it/r8293025cwqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=755e9d237792b112ecde2c2f108d85f0258fc59a" title="Parkiet: Fine-tuning Dia for any language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;A lot of the open-source TTS models are released for English or Chinese and lack support for other languages. I was curious to see if I could train a state-of-the-art text-to-speech (TTS) model for Dutch by using Google's free TPU Research credits. I open-sourced the weights, and documented the whole journey, from Torch model conversion, data preparation, JAX training code and inference pipeline here &lt;a href="https://github.com/pevers/parkiet"&gt;https://github.com/pevers/parkiet&lt;/a&gt; . Hopefully it can serve as a guide for others that are curious to train these models for other languages (without burning through all the credits trying to fix the pipeline). &lt;/p&gt; &lt;p&gt;Spoiler: the results are great! I believe they are *close* to samples generated with ElevenLabs. I spent about $300, mainly on GCS egress. Sample comparison can be found here &lt;a href="https://peterevers.nl/posts/2025/09/parkiet/"&gt;https://peterevers.nl/posts/2025/09/parkiet/&lt;/a&gt; .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pevers"&gt; /u/pevers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r8293025cwqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T11:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nohcgs</id>
    <title>How can we run Qwen3-omni-30b-a3b?</title>
    <updated>2025-09-23T13:26:04+00:00</updated>
    <author>
      <name>/u/PermanentLiminality</name>
      <uri>https://old.reddit.com/user/PermanentLiminality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This looks awesome, but I can't run it. At least not yet and I sure want to run it. &lt;/p&gt; &lt;p&gt;It looks like it needs to be run with straight python transformer. I could be wrong, but none of the usual suspects like vllm, llama.cpp, etc support the multimodal nature of the model. Can we expect support in any of these?&lt;/p&gt; &lt;p&gt;Given the above, will there be quants? I figured there would at least be some placeholders on HFm but I didn't see any when I just looked. The native 16 bit format is 70GB and my best system will maybe just barely fit that in combined VRAM and system RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PermanentLiminality"&gt; /u/PermanentLiminality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T13:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1noikw2</id>
    <title>Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board</title>
    <updated>2025-09-23T14:15:42+00:00</updated>
    <author>
      <name>/u/Ok-Actuary-4527</name>
      <uri>https://old.reddit.com/user/Ok-Actuary-4527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"&gt; &lt;img alt="Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board" src="https://a.thumbs.redditmedia.com/pcm1xAY4zY-tgo1Qcr29HPiU2wVkxx0BAdZ2I0j9uf8.jpg" title="Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are some curiosities and questions here about the modded 4090 48GB cards. For my local AI test environment, I need a setup with a larger VRAM pool to run some tests, so I got my hands on a dual-card rig with these. I've run some initial benchmarks and wanted to share the data.&lt;/p&gt; &lt;p&gt;The results are as expected, and I think it's a good idea to have these modded 4090 48GB cards.&lt;/p&gt; &lt;h1&gt;Test 1: Single Card GGUF Speed (GPUStack llama-box/llama.cpp)&lt;/h1&gt; &lt;p&gt;Just a simple, raw generation speed test on a single card to see how they compare head-to-head.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-32B (GGUF, Q4_K_M)&lt;/li&gt; &lt;li&gt;Backend: llama-box (llama-box in GPUStack)&lt;/li&gt; &lt;li&gt;Test: Single short prompt request generation via GPUStack UI's compare feature.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modded 4090 48GB: 38.86 t/s&lt;/li&gt; &lt;li&gt;Standard 4090 24GB (ASUS TUF): 39.45 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observation: The standard 24GB card was slightly faster. Not by much, but consistently.&lt;/p&gt; &lt;h1&gt;Test 2: Single Card vLLM Speed&lt;/h1&gt; &lt;p&gt;The same test but with a smaller model on vLLM to see if the pattern held.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-8B (FP16)&lt;/li&gt; &lt;li&gt;Backend: vLLM v0.10.2 in GPUStack (custom backend)&lt;/li&gt; &lt;li&gt;Test: Single short request generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modded 4090 48GB: 55.87 t/s&lt;/li&gt; &lt;li&gt;Standard 4090 24GB: 57.27 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observation: Same story. The 24GB card is again marginally faster in a simple, single-stream inference task. The extra VRAM doesn't translate to more speed for a single request, which is expected, and there might be a tiny performance penalty for the modded memory.&lt;/p&gt; &lt;h1&gt;Test 3: Multi-GPU Stress Test (2x 48GB vs 4x 24GB)&lt;/h1&gt; &lt;p&gt;This is where I compared my dual 48GB rig against a cloud machine with four standard 4090s. Both setups have 96GB of total VRAM running the same large model under a heavy concurrent load.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-32B (FP16)&lt;/li&gt; &lt;li&gt;Backend: vLLM v0.10.2 in GPUStack (custom backend)&lt;/li&gt; &lt;li&gt;Tool: evalscope (100 concurrent users, 400 total requests)&lt;/li&gt; &lt;li&gt;Setup A (Local): 2x Modded 4090 48GB (TP=2) on an ASUS ProArt Z790&lt;/li&gt; &lt;li&gt;Setup B (Cloud): 4x Standard 4090 24GB (TP=4) on a server-grade board&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results (Cloud 4x24GB was significantly better):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;2x 4090 48GB (Our Rig)&lt;/th&gt; &lt;th align="left"&gt;4x 4090 24GB (Cloud)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Output Throughput (tok/s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1054.1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1262.95&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. Latency (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;105.46&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;86.99&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. TTFT (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.4179&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.3947&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. Time Per Output Token (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.0844&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.0690&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis: The 4-card setup on the server was clearly superior across all metrics—almost 20% higher throughput and significantly lower latency. My initial guess was the motherboard's PCIe topology (PCIE 5.0 x16 PHB on my Z790 vs. a better link on the server, which is also PCIE).&lt;/p&gt; &lt;p&gt;To confirm this, I ran nccl-test to measure the effective inter-GPU bandwidth. The results were clear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local 2x48GB Rig:&lt;/strong&gt; Avg bus bandwidth was &lt;strong&gt;~3.0 GB/s&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud 4x24GB Rig:&lt;/strong&gt; Avg bus bandwidth was &lt;strong&gt;~3.3 GB/s&lt;/strong&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That ~10% higher bus bandwidth on the server board seems to be the key difference, allowing it to overcome the extra communication overhead of a larger tensor parallel group (TP=4 vs TP=2) and deliver much better performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Actuary-4527"&gt; /u/Ok-Actuary-4527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1noikw2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T14:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1noe09l</id>
    <title>2 new open source models from Qwen today</title>
    <updated>2025-09-23T10:44:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt; &lt;img alt="2 new open source models from Qwen today" src="https://preview.redd.it/goah9v2r8wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07a67dbd5f99e7851c1f27295952913b340ead4d" title="2 new open source models from Qwen today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/goah9v2r8wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1noru3p</id>
    <title>GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA</title>
    <updated>2025-09-23T20:05:10+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"&gt; &lt;img alt="GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA" src="https://external-preview.redd.it/U0uchtAJMRqNemoxp-a8VWVQpNPPATSQ8bLLGYLEcHQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2df1ad2c345943a90fdd8666f38e455de3a4819c" title="GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Over 112 GB high-bandwidth memory for large-scale AI workloads&lt;/li&gt; &lt;li&gt;First Chinese GPU with hardware ray tracing support&lt;/li&gt; &lt;li&gt;vGPU design architecture with hardware virtualization&lt;/li&gt; &lt;li&gt;Supports DirectX 12, Vulkan 1.2, OpenGL 4.6, and up to six 8K displays&lt;/li&gt; &lt;li&gt;Domestic design based on OpenCore RISC-V CPU and full set of IP&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/innosilicon-unveils-fenghua-3-gpu-with-directx12-support-and-hardware-ray-tracing"&gt;https://videocardz.com/newz/innosilicon-unveils-fenghua-3-gpu-with-directx12-support-and-hardware-ray-tracing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai"&gt;https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;&lt;a href="https://www.techpowerup.com/341268/innosilicons-fenghua-no-3-gpu-launches-with-112gb-hbm-memory-and-claims-to-support-cuda"&gt;Claims to Support CUDA&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kxpifn9e0zqf1.jpg?width=168&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebccecdf4a52af907db2b392b698eb7557d75e74"&gt;https://preview.redd.it/kxpifn9e0zqf1.jpg?width=168&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebccecdf4a52af907db2b392b698eb7557d75e74&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1not4zb</id>
    <title>Qwen3-VL-235B-A22B available on HF</title>
    <updated>2025-09-23T20:55:13+00:00</updated>
    <author>
      <name>/u/AlbeHxT9</name>
      <uri>https://old.reddit.com/user/AlbeHxT9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlbeHxT9"&gt; /u/AlbeHxT9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4zb/qwen3vl235ba22b_available_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4zb/qwen3vl235ba22b_available_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1not4zb/qwen3vl235ba22b_available_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:55:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1novzr4</id>
    <title>Qwen Devs and Release Teams right now.</title>
    <updated>2025-09-23T22:51:31+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1novzr4/qwen_devs_and_release_teams_right_now/"&gt; &lt;img alt="Qwen Devs and Release Teams right now." src="https://preview.redd.it/g6rdscgkuzqf1.gif?width=320&amp;amp;crop=smart&amp;amp;s=dc07fa2490e2521f5f3215fe223b8637193bbe3d" title="Qwen Devs and Release Teams right now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Friggin’ Legends!!! Hope they get some well-deserved time off, but not too much time cause we need those Llama.cpp PRs worked LOL 🤣 Seriously tho, thanks for all the amazing new models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g6rdscgkuzqf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1novzr4/qwen_devs_and_release_teams_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1novzr4/qwen_devs_and_release_teams_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T22:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nomrj7</id>
    <title>Leaderboards &amp; Benchmarks</title>
    <updated>2025-09-23T16:53:47+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"&gt; &lt;img alt="Leaderboards &amp;amp; Benchmarks" src="https://preview.redd.it/n79ymm450yqf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=366296e43fd0844292650a0fe0b1176903e5bd77" title="Leaderboards &amp;amp; Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many Leaderboards are not up to date, recent models are missing. Don't know what happened to GPU Poor LLM Arena? I check Livebench, Dubesor, EQ-Bench, oobabooga often. Like these boards because these come with more Small &amp;amp; Medium size models(Typical boards usually stop with 30B at bottom &amp;amp; only few small models). For my laptop config(8GB VRAM &amp;amp; 32GB RAM), I need models 1-35B models. Dubesor's benchmark comes with Quant size too which is convenient &amp;amp; nice.&lt;/p&gt; &lt;p&gt;It's really heavy &amp;amp; consistent work to keep things up to date so big kudos to all leaderboards. What leaderboards do you check usually?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Forgot to add oobabooga&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n79ymm450yqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1not4up</id>
    <title>Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct</title>
    <updated>2025-09-23T20:55:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt; &lt;img alt="Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct" src="https://external-preview.redd.it/buohQYfptNXWK_RjSglwO9z3swviJ-ly59KfrJBuCDs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7b84536da0880f9dbcad1c25d46560ac5263a67" title="Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; &lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; &lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‑enhanced Thinking editions for flexible, on‑demand deployment.&lt;/p&gt; &lt;h1&gt;Key Enhancements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs—recognizes elements, understands functions, invokes tools, completes tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates &lt;a href="http://Draw.io/HTML/CSS/JS"&gt;Draw.io/HTML/CSS/JS&lt;/a&gt; from images/videos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math—causal analysis and logical, evidence-based answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to “recognize everything”—celebrities, anime, products, landmarks, flora/fauna, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text–vision fusion for lossless, unified comprehension.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yx8zvj9z9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3dd9ee51e1ed2ee5b561d613720f88406ce22a14"&gt;https://preview.redd.it/yx8zvj9z9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3dd9ee51e1ed2ee5b561d613720f88406ce22a14&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k2lvsypz9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dff5af92d79eb12f9b7ac684373c589fa924d18"&gt;https://preview.redd.it/k2lvsypz9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dff5af92d79eb12f9b7ac684373c589fa924d18&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nolz9e</id>
    <title>Qwen3Guard - a Qwen Collection</title>
    <updated>2025-09-23T16:24:35+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"&gt; &lt;img alt="Qwen3Guard - a Qwen Collection" src="https://external-preview.redd.it/SybQlpd57ri5DOffonwxQ3RJbORPPReSb_vD77lSWek.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa8f4c7470cc8c0c033a57dba7ee804d9d9e1d36" title="Qwen3Guard - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3guard-68d2729abbfae4716f3343a1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nopcry</id>
    <title>Huawei Plans Three-Year Campaign to Overtake Nvidia in AI Chips</title>
    <updated>2025-09-23T18:31:10+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopcry/huawei_plans_threeyear_campaign_to_overtake/"&gt; &lt;img alt="Huawei Plans Three-Year Campaign to Overtake Nvidia in AI Chips" src="https://external-preview.redd.it/RG_Drphb5Z3LeMOBYSjdaBWtUZI-aLmpgWBBRijj4mk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d14901df55c903a2634eb230416e5ea6c1bf81c" title="Huawei Plans Three-Year Campaign to Overtake Nvidia in AI Chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://finance.yahoo.com/news/huawei-plans-three-campaign-overtake-052622404.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopcry/huawei_plans_threeyear_campaign_to_overtake/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nopcry/huawei_plans_threeyear_campaign_to_overtake/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T18:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nosdxy</id>
    <title>Qwen3-VL: Sharper Vision, Deeper Thought, Broader Action</title>
    <updated>2025-09-23T20:26:06+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nosdxy/qwen3vl_sharper_vision_deeper_thought_broader/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nosdxy/qwen3vl_sharper_vision_deeper_thought_broader/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nodc6q</id>
    <title>How are they shipping so fast 💀</title>
    <updated>2025-09-23T10:04:43+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt; &lt;img alt="How are they shipping so fast 💀" src="https://preview.redd.it/8higdv9r1wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6b83f7127ef234c48c0416953381b9c3c6004a7" title="How are they shipping so fast 💀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well good for us &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8higdv9r1wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nor65d</id>
    <title>Qwen 3 max released</title>
    <updated>2025-09-23T19:40:02+00:00</updated>
    <author>
      <name>/u/clem844</name>
      <uri>https://old.reddit.com/user/clem844</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Following the release of the Qwen3-2507 series, we are thrilled to introduce Qwen3-Max — our largest and most capable model to date. The preview version of Qwen3-Max-Instruct currently ranks third on the Text Arena leaderboard, surpassing GPT-5-Chat. The official release further enhances performance in coding and agent capabilities, achieving state-of-the-art results across a comprehensive suite of benchmarks — including knowledge, reasoning, coding, instruction following, human preference alignment, agent tasks, and multilingual understanding. We invite you to try Qwen3-Max-Instruct via its API on Alibaba Cloud or explore it directly on Qwen Chat. Meanwhile, Qwen3-Max-Thinking — still under active training — is already demonstrating remarkable potential. When augmented with tool usage and scaled test-time compute, the Thinking variant has achieved 100% on challenging reasoning benchmarks such as AIME 25 and HMMT. We look forward to releasing it publicly in the near future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem844"&gt; /u/clem844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T19:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
