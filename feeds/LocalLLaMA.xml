<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-03T10:38:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q1ura1</id>
    <title>[IQuestLab/IQuest-Coder-V1] SWE-bench score is compromised because environment setup was wrong</title>
    <updated>2026-01-02T09:57:34+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR is that they didn't clean the repo (.git/ folder), model just reward hacked its way to look up future commits with fixes. Credit goes to everyone in this thread for solving this: &lt;a href="https://xcancel.com/xeophon/status/2006969664346501589"&gt;https://xcancel.com/xeophon/status/2006969664346501589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(given that IQuestLab published their SWE-Bench Verified trajectory data, I want to be charitable and assume genuine oversight rather than &amp;quot;benchmaxxing&amp;quot;, probably an easy to miss thing if you are new to benchmarking)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T09:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2et07</id>
    <title>I got frustrated dealing with massive responses from many MCPs and threw something together over the last couple days... it might help you too. Or not!</title>
    <updated>2026-01-02T23:53:31+00:00</updated>
    <author>
      <name>/u/steezy13312</name>
      <uri>https://old.reddit.com/user/steezy13312</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;/r/LocalLlama&lt;/a&gt;, I spent the last couple of days working on a little personal project and figured I‚Äôd share. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/samteezy/mcp-context-proxy/"&gt;https://github.com/samteezy/mcp-context-proxy/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Background: As a relatively low-investment homelabber, I'm always battling context size and chasing optimal prompt processing/token generation speeds.&lt;/p&gt; &lt;p&gt;I don‚Äôt mean to pick on this one in particular, but a MCP that really got me frustrated was an otherwise very &lt;a href="https://github.com/sirkirby/unifi-network-mcp"&gt;well built MCP&lt;/a&gt; that allows you to extract data from your UniFi network devices. I was working with it to build documentation of my home network, and I was finding it was giving me response payloads from the UniFi API that had a ton of extra data which started just filling up my context and taking &lt;em&gt;forever&lt;/em&gt; for gpt-oss-120b to process. I don't blame the author - this is just a fundamental failing in current MCP implementation; MCPs are meant to help give instruction but there's no special solution to optimizing number of tokens returned (there's no free lunch).&lt;/p&gt; &lt;p&gt;I love small models like those from Qwen and Liquid AI, and I have &lt;code&gt;llama-swap&lt;/code&gt; configured to always have a small task model in the background for tools like Karakeep and Open WebUI to use... so what if I could use this for basically compressing any MCP response?&lt;/p&gt; &lt;p&gt;So I decided to turn Claude Code onto the problem and create a little tool that we have here. It is an MCP which acts a transparent proxy, oriented towards the home lab/context poor user with the following features/benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Transparently presents MCP tools to the client, but allows you to preprocess the MCP's response before sending it back to the client LLM (ideally you use a locally hosted LLM, but could also make remote callouts to the cloud to a super inexpensive or free API via something like OpenRouter)&lt;/li&gt; &lt;li&gt;Uses a simple in-memory cache for caching responses for identical requests&lt;/li&gt; &lt;li&gt;Allows disabling individual tools or overwriting the upstream tool descriptions to better control context size and tool selection accuracy when launching an agent&lt;/li&gt; &lt;li&gt;Adds capability to intercept outgoing tool calls and incoming MCP responses for things like PII masking or prompt injection (future)&lt;/li&gt; &lt;li&gt;One proxy for managing multiple MCPs; great for if you're playing with multiple AI tools/coding assistants and hate having to reconfigure MCPs for each one&lt;/li&gt; &lt;li&gt;Very configurable options to override behavior globally or different tools via a single JSON file, plus a UI for management and visibility&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been testing with a high-quant Qwen3-0.6b and LFM2-1.2b and it's doing very well for me. For example, I have it use web search and fetch for URLs and instead of having the larger model process the entire pages, the tiny model reads the page up to 10x faster, and just gives the large model the answers it needs, also keeping context lower. It's made using tools like search and fetch worthwhile. YMMV.&lt;/p&gt; &lt;p&gt;It is not:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Being monetized or going to make you a ton of money&lt;/li&gt; &lt;li&gt;Guaranteed to work in a high-stress environment (not that it's emotionally sensitive, just that I don't know where its performance limits are)&lt;/li&gt; &lt;li&gt;Completely revolutionary&lt;/li&gt; &lt;li&gt;Going to solve all of MCP flaws and failings&lt;/li&gt; &lt;li&gt;Going to make your ex take you back&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And yes, it is vibe coded... so of course take it with a grain of salt, but I use these tools professionally and understand how to use AI as a coding assistant rather than an expert. Don't like that? Fork it and have AI inspect it yourself. Or write your own. Or do whatever, &lt;a href="https://tenor.com/view/youre-not-my-supervisor-youre-not-my-boss-gif-12971403"&gt;I'm not your supervisor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm planning on adding in optional prompt injection review (curious about some of IBM's and others' models out there more to understand how they work) and seeing how well I can get the masking side working. I haven't tested that a ton yet. I'm also playing around with the idea of adding an optional override for the client LLM to bypass content summarization, but I feel like that risks defeating the purpose.&lt;/p&gt; &lt;p&gt;Hope this helps you get more value out of the hardware and setup you currently have.&lt;/p&gt; &lt;p&gt;Note, I'm not super familiar with publishing npm packages and open source projects, so I might not be doing versioning or other things by-the-book... open to any constructive criticism on how you see things set up and structured so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/steezy13312"&gt; /u/steezy13312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2et07/i_got_frustrated_dealing_with_massive_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2et07/i_got_frustrated_dealing_with_massive_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2et07/i_got_frustrated_dealing_with_massive_responses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T23:53:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2qnix</id>
    <title>Let your grandmother run LLama models on her own device</title>
    <updated>2026-01-03T09:43:52+00:00</updated>
    <author>
      <name>/u/Maleficent-Acadia736</name>
      <uri>https://old.reddit.com/user/Maleficent-Acadia736</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qnix/let_your_grandmother_run_llama_models_on_her_own/"&gt; &lt;img alt="Let your grandmother run LLama models on her own device" src="https://b.thumbs.redditmedia.com/nQHyQUo5dukmqN2y7DYFbOb-NvI7xQsCSvpetfCqwFQ.jpg" title="Let your grandmother run LLama models on her own device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 6 months of work I published &lt;a href="https://pocketbrain.app"&gt;Brain Pocket&lt;/a&gt; - the easiest way on earth to run LLMs on earth - no backend required.&lt;/p&gt; &lt;p&gt;You just open the website, choose the model you want to run from the list of open-source models, download it once with 1 click, and then use it wherever you like - Everest, on airplane, Mars or Jupyter. It requires 0 technical knowledge, no installations and &lt;strong&gt;even your grandmother can run it&lt;/strong&gt; - send her so she can understand the benefits of open-source AI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before down-voting, note this is open-source, free, and there is no mean to &amp;quot;pay&amp;quot; -&lt;/strong&gt; &lt;strong&gt;and I spent 6 months building it&lt;/strong&gt;. Let me know what you think it sucks and I will fix it or follow-up&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hfz6lluu3bg1.png?width=880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc2a95007aa5104875548f8c85a9e3f2453a1e9c"&gt;https://preview.redd.it/6hfz6lluu3bg1.png?width=880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc2a95007aa5104875548f8c85a9e3f2453a1e9c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Acadia736"&gt; /u/Maleficent-Acadia736 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qnix/let_your_grandmother_run_llama_models_on_her_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qnix/let_your_grandmother_run_llama_models_on_her_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qnix/let_your_grandmother_run_llama_models_on_her_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T09:43:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2afpr</id>
    <title>üç≥ Cook High Quality Custom GGUF Dynamic Quants ‚Äî right from your web browser</title>
    <updated>2026-01-02T20:59:07+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just published a web front-end that wraps the GGUF Tool Suite's &lt;code&gt;quant_assign.py&lt;/code&gt; so you can produce high-quality dynamic GGUF quants without touching the command line. Everything is integrated in the browser: upload or pick calibration/deg CSVs, tune advanced options in a friendly UI, and export a &lt;code&gt;.recipe&lt;/code&gt; tuned to your hardware in seconds.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this exists&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Making GGUF quantization accessible: no more wrestling with terminals, dependency hell or manual piping. If you want precise, automated, system-tuned GGUF dynamic quant production ‚Äî but prefer a web-first experience ‚Äî this is for you.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üî• Cook High Quality Custom GGUF Dynamic Quants in 3 Steps&lt;/h3&gt; &lt;p&gt;&lt;em&gt;‚ú® Target exact VRAM/RAM sizes. Mix quant types. Done in minutes!&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;üç≥ &lt;strong&gt;Step 1 ‚Äî Generate a GGUF recipe&lt;/strong&gt;: open &lt;code&gt;quant_assign.html&lt;/code&gt; and let the UI size a recipe for your hardware.&lt;br /&gt; &lt;a href="https://gguf.thireus.com/quant_assign.html"&gt;https://gguf.thireus.com/quant_assign.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;Step 2 ‚Äî Download GGUF files&lt;/strong&gt;: feed the recipe into &lt;code&gt;quant_downloader.html&lt;/code&gt; and grab the GGUFs.&lt;br /&gt; &lt;a href="https://gguf.thireus.com/quant_downloader.html"&gt;https://gguf.thireus.com/quant_downloader.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üöÄ &lt;strong&gt;Step 3 ‚Äî Run anywhere&lt;/strong&gt;: use &lt;code&gt;llama.cpp&lt;/code&gt;, &lt;code&gt;ik_llama.cpp&lt;/code&gt;, or any GGUF-compatible runtime.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;A few notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GLM-4.7 calibration data is coming soon ‚Äî subscribe to this issue for updates: &lt;a href="https://github.com/Thireus/GGUF-Tool-Suite/issues/50"&gt;https://github.com/Thireus/GGUF-Tool-Suite/issues/50&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2afpr/cook_high_quality_custom_gguf_dynamic_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2afpr/cook_high_quality_custom_gguf_dynamic_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2afpr/cook_high_quality_custom_gguf_dynamic_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T20:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1p5q5</id>
    <title>Getting ready to train in Intel arc</title>
    <updated>2026-01-02T04:33:19+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"&gt; &lt;img alt="Getting ready to train in Intel arc" src="https://a.thumbs.redditmedia.com/iosPXZzRPv_Wd4_Y3UYcEyKFQKwEaHWvSBwjyGk3uo8.jpg" title="Getting ready to train in Intel arc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just waiting on pcie risers can't wait to start training on Intel arc I'm not sure in anyone else is attempting the same thing yet so I though I would share &lt;/p&gt; &lt;p&gt;PS. I am not causing a GPU shortage pls dont comment about this I am not open ai or google believe me there would have been signs on my other posts gamers would say sh*t like this so before u comment please educate yourselves&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q1p5q5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T04:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1uyf6</id>
    <title>New Models from South Korea's Sovereign AI Foundation Model Project</title>
    <updated>2026-01-02T10:09:28+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen posts with individual models here and there, but not together in one post. Also I'm including some English articles I found about the project.&lt;/p&gt; &lt;p&gt;It's bit old news, but the South Korean government funded the Sovereign AI Foundation Model Project, and the five selected teams released their initial models and presented on December 30, 2025.&lt;/p&gt; &lt;p&gt;Below are the repos I was able to track down on Huggingface, but please let me know if I missed or included wrong repo.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Naver Cloud: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;HyperCLOVAX-SEED-Omni-8B &lt;/a&gt;, &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Upstage: &lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;Solar-Open-102B-A12B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SK Telecom: &lt;a href="https://huggingface.co/skt/A.X-K1"&gt;A.X-K1-519B-A33B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LG AI Research: &lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B"&gt;K-EXAONE-236B-A23B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;NC AI: &lt;a href="https://huggingface.co/NC-AI-consortium-VAETKI/VAETKI"&gt;VAETKI-112B-A10B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;South Koreas current president ran with AI as one of his prominent campaign themes, and the government &lt;a href="https://www.chosun.com/english/market-money-en/2025/12/11/25QXCJ2Q4NCINOC5EXP6SR3QYQ/"&gt;pledged to invest&lt;/a&gt; 30T KRW (20.8B USD) in the AI sector over five years , roughly 0.23% of GDP per year, as part of National Growth Fund.&lt;/p&gt; &lt;p&gt;It looks like MSIT is backing the project with funding, GPUs, and datasets. Teams will be evaluated and eliminated through 2026 and into mid 2027 until two finalists.&lt;/p&gt; &lt;p&gt;Also it said all 5 teams &amp;quot;presented robust open-source policies so that foundation models they develop and release can also be used commercially by other companies, thereby contributing in many ways to expansion of the domestic AI ecosystem, to the acceleration of diverse AI services, and to improved public access to AI.&amp;quot;&lt;/p&gt; &lt;p&gt;You can read more about the project below:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.msit.go.kr/eng/bbs/view.do?bbsSeqNo=42&amp;amp;mId=4&amp;amp;nttSeqNo=1152&amp;amp;sCode=eng"&gt;https://www.msit.go.kr/eng/bbs/view.do?bbsSeqNo=42&amp;amp;mId=4&amp;amp;nttSeqNo=1152&amp;amp;sCode=eng&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.upi.com/Top_News/World-News/2025/12/30/ai-model-national-project/7441767133090/"&gt;https://www.upi.com/Top_News/World-News/2025/12/30/ai-model-national-project/7441767133090/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.koreatimes.co.kr/business/tech-science/20251230/consortia-unveil-models-for-national-ai-project"&gt;https://www.koreatimes.co.kr/business/tech-science/20251230/consortia-unveil-models-for-national-ai-project&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T10:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2eau3</id>
    <title>My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0</title>
    <updated>2026-01-02T23:32:12+00:00</updated>
    <author>
      <name>/u/DigiJoe79</name>
      <uri>https://old.reddit.com/user/DigiJoe79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"&gt; &lt;img alt="My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0" src="https://a.thumbs.redditmedia.com/TNdBuFemSJ4jRJ66xw8XJAlZ-H0HvTXr_8DHB2mjLq8.jpg" title="My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;About three weeks ago I shared my passion project here - an app to create audiobooks from text using local TTS engines like XTTS and Chatterbox. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The response was amazing and motivated me to keep going. Special shoutout to &lt;a href="https://github.com/codesterribly"&gt;https://github.com/codesterribly&lt;/a&gt; who pushed me to tackle Docker support - you were right, it was worth it!&lt;/p&gt; &lt;p&gt;So here's my slightly-late New Year's gift to the community: v1.1.0 üéÅ&lt;/p&gt; &lt;p&gt;What's New?&lt;/p&gt; &lt;p&gt;Docker-First Architecture&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No more Python environment hell! Engines come as prebuilt Docker images&lt;/li&gt; &lt;li&gt;One-click installation from the online catalog&lt;/li&gt; &lt;li&gt;Works on Windows, Linux, and partially with macOS (Apple Silicon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Remote GPU Offloading&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Got a beefy GPU server in your closet? Run VibeVoice 7B there via SSH&lt;/li&gt; &lt;li&gt;Your laptop stays cool while the server does the heavy lifting&lt;/li&gt; &lt;li&gt;Built-in SSH key wizard - no manual config needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;New TTS Engine: VibeVoice&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Microsoft's long-form multi-speaker TTS&lt;/li&gt; &lt;li&gt;Great for podcasts and dialogues&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick Start&lt;/p&gt; &lt;h1&gt;Pull the backend&lt;/h1&gt; &lt;p&gt;docker pull ghcr.io/digijoe79/audiobook-maker/backend:latest&lt;/p&gt; &lt;h1&gt;Run it&lt;/h1&gt; &lt;p&gt;&lt;code&gt;docker run -d --name audiobook-maker-backend \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-p 8765:8765 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--add-host=host.docker.internal:host-gateway \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-e DOCKER_ENGINE_HOST=host.docker.internal \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v /var/run/docker.sock:/var/run/docker.sock \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v audiobook-data-path:/app/data \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v audiobook-media-path:/app/media \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ghcr.io/digijoe79/audiobook-maker/backend:latest&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then grab the desktop app, connect, and install engines from the catalog. That's it!&lt;/p&gt; &lt;p&gt;Links&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker"&gt;https://github.com/DigiJoe79/audiobook-maker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker/releases"&gt;https://github.com/DigiJoe79/audiobook-maker/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker/tree/main/docs/samples"&gt;https://github.com/DigiJoe79/audiobook-maker/tree/main/docs/samples&lt;/a&gt; (Moby Dick previews)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's Next?&lt;/p&gt; &lt;p&gt;Already thinking about v1.2.0 - better batch processing, more for Apple Silicon. Open to suggestions!&lt;/p&gt; &lt;p&gt;Thanks again for all the feedback on the original post. This community is awesome. üôè&lt;/p&gt; &lt;p&gt;Happy (belated) New Year, and happy listening!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigiJoe79"&gt; /u/DigiJoe79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2eau3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T23:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2pqdd</id>
    <title>LLMeQueue: let me queue LLM requests from my GPU - local or over the internet</title>
    <updated>2026-01-03T08:46:47+00:00</updated>
    <author>
      <name>/u/PromptAndHope</name>
      <uri>https://old.reddit.com/user/PromptAndHope</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am working on a PoC project where I need to generate a fairly large number of embeddings and chat completions during development. Since I have an NVIDIA GPU (5060Ti) available locally, I was thinking about setting up a lightweight public server that only receives requests, while a locally running worker connects to it, processes the requests using the GPU, and sends the results back to the server.&lt;/p&gt; &lt;p&gt;You can find the code here: &lt;a href="https://github.com/gszecsenyi/LLMeQueue"&gt;https://github.com/gszecsenyi/LLMeQueue&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The worker is capable of handling both embedding generation and chat completions concurrently in OpenAI API format. By default, the model used is &lt;code&gt;llama3.2:3b&lt;/code&gt;, but a different model can be specified per request, as long as it is available in the worker‚Äôs Ollama container or local Ollama installation. All inference and processing are handled by Ollama running on the worker.&lt;/p&gt; &lt;p&gt;The original idea was that I could also process the requests myself - essentially a &amp;quot;let me queue&amp;quot; approach - which is where the name &lt;strong&gt;LLMeQueue&lt;/strong&gt; comes from.&lt;/p&gt; &lt;p&gt;Any feedback or ideas are welcome, and I would especially appreciate it if you could star the GitHub repository.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PromptAndHope"&gt; /u/PromptAndHope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pqdd/llmequeue_let_me_queue_llm_requests_from_my_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pqdd/llmequeue_let_me_queue_llm_requests_from_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pqdd/llmequeue_let_me_queue_llm_requests_from_my_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jazd</id>
    <title>Part 4 (Finale): Building LLMs from Scratch ‚Äì Evaluation &amp; Deployment [Follow-up to Parts 1, thru 3]</title>
    <updated>2026-01-03T03:10:03+00:00</updated>
    <author>
      <name>/u/amitbahree</name>
      <uri>https://old.reddit.com/user/amitbahree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy new year! I‚Äôm excited to share &lt;strong&gt;Part 4&lt;/strong&gt; (and the final part) of my series on building an LLM from scratch.&lt;/p&gt; &lt;p&gt;This installment covers the ‚Äúokay, but does it &lt;em&gt;work&lt;/em&gt;?‚Äù phase: evaluation, testing, and deployment - taking the trained models from Part 3 and turning them into something you can validate, iterate on, and actually share/use (including publishing to HF).&lt;/p&gt; &lt;p&gt;What you‚Äôll find inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A practical evaluation framework (quick vs comprehensive) for historical language models (not just perplexity).&lt;/li&gt; &lt;li&gt;Tests and validation patterns: historical accuracy checks, linguistic checks, temporal consistency, and basic performance sanity checks.&lt;/li&gt; &lt;li&gt;Deployment paths: &lt;ul&gt; &lt;li&gt;local inference from PyTorch checkpoints&lt;/li&gt; &lt;li&gt;Hugging Face Hub publishing + model cards&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;CI-ish smoke checks you can run on CPU to catch obvious regressions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why it matters?&lt;br /&gt; Training is only half the battle. Without evaluation + tests + a repeatable publishing workflow, you can easily end up with a model that ‚Äútrains fine‚Äù but is unreliable, inconsistent, or impossible for others to reproduce/use. This post focuses on making the last mile boring (in the best way).&lt;/p&gt; &lt;p&gt;Resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîó Blog post (Part 4) - &lt;a href="https://blog.desigeek.com/post/2026/01/building-llm-from-scratch-part4-evaluation-deployment/"&gt;Evaluations and Deployment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó GitHub repo: &lt;a href="https://github.com/bahree/helloLondon"&gt;https://github.com/bahree/helloLondon&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Hugging Face: &lt;a href="https://huggingface.co/bahree"&gt;https://huggingface.co/bahree&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In case you are interested in the previous parts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîó Part 3 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oluay3/part_3_building_llms_from_scratch_model/"&gt;Model Architecture &amp;amp; GPU Training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Part 2 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o562l3/part_2_building_llms_from_scratch_data_collection/"&gt;Data Collection &amp;amp; Custom Tokenizers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Part 1 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1npzstw/a_step_by_step_guide_on_how_to_build_a_llm_from/"&gt;Quick Start &amp;amp; Overview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó LinkedIn &lt;a href="https://www.linkedin.com/posts/amitbahree_building-llms-from-scratch-part-4-evaluation-activity-7413050136974700544-0OwB/"&gt;post&lt;/a&gt; (if that is your thing).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitbahree"&gt; /u/amitbahree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21zql</id>
    <title>Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing</title>
    <updated>2026-01-02T15:47:29+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt; &lt;img alt="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing" src="https://external-preview.redd.it/hL9IlSGjDxXkUJI7Ss74xpCqhzcKCtv4f-p8hzWkQ_U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=704790efb261d2a51abf0f339a4ace74d00ec32c" title="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This isn't new, but somehow I missed it, and figure many in this community might also not be aware of this.&lt;/p&gt; &lt;p&gt;The TLDR, as the title says: Supermicro is stopping standalone motherboard sales and now selling only entire servers. As if things weren't already bad enough...&lt;/p&gt; &lt;p&gt;I had noticed an uptick in used board prices on ebay, local ads, and tech forums but didn't have an explanation for it. This explains why.&lt;/p&gt; &lt;p&gt;While most discussions in this community center around consumer boards, workstation and server boards offer so many more features and functionality, and used to be much cheaper than their desktop counterparts.&lt;/p&gt; &lt;p&gt;Supermicro was arguably the largest supplier of such boards, and with them stopping motherboard sales, all workstation and server boards in standard industry form-factor (EATX, ATX, MATX, IT, and SSE variants) will have a sharp drop in availability in the foreseeable future.&lt;/p&gt; &lt;p&gt;Add to that the sharp increase in RAM prices, and you can see why many businesses will be hesitant to move to newer DDR5 server platforms and instead choose to stock to DDR4 platforms to reuse their existing memory. I suspect many will consolidate their existing DDR4 based Xeon and early Epyc (Naples) to Epyc Milan servers using existing market supply of servers and boards.&lt;/p&gt; &lt;p&gt;We're barely in 2026, but it's looking like this year will squeeze us, consumer, even more than 2025 has.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.spectrumsourcing.com/spectrum-news-feed/industry-update-supermicro-policy-on-standalone-motherboards-sales-discontinued"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1w1qj</id>
    <title>Most optimal vram/performance per price and advice for Shenzhen GPU market</title>
    <updated>2026-01-02T11:14:30+00:00</updated>
    <author>
      <name>/u/notafakename10</name>
      <uri>https://old.reddit.com/user/notafakename10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt; &lt;img alt="Most optimal vram/performance per price and advice for Shenzhen GPU market" src="https://preview.redd.it/4nfcarq96xag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34f4fbc2fba6317c3d5435a92332540815eb3714" title="Most optimal vram/performance per price and advice for Shenzhen GPU market" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm in Shanghai at the moment and heading to Shenzhen soon - I‚Äôve got around $1500-3000 USD to get the most optimal setup possible. The people I am with are great at negotiating (natives, speak the language) I just need to figure out what I want‚Ä¶ &lt;/p&gt; &lt;p&gt;I main use local models I would want at least 48gb vram, ideally closer to 96gb an at least some grunt for the odd PyTorch model training run. I‚Äôm open to modded cards (one of my current front runners is 4x 3080 20gb cards) open to both AMD and domestic / enterprise cards. &lt;/p&gt; &lt;p&gt;Prices are best estimates from deep seek - could be wildly wrong, anyone had experience navigating the GPU markets? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notafakename10"&gt; /u/notafakename10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4nfcarq96xag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T11:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2lm5a</id>
    <title>Chinny ‚Äî the unlimited, on-device voice cloner ‚Äî just dropped on iOS! (macOS version pending review üëÄ)</title>
    <updated>2026-01-03T04:59:02+00:00</updated>
    <author>
      <name>/u/Tingxiaojue</name>
      <uri>https://old.reddit.com/user/Tingxiaojue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"&gt; &lt;img alt="Chinny ‚Äî the unlimited, on-device voice cloner ‚Äî just dropped on iOS! (macOS version pending review üëÄ)" src="https://external-preview.redd.it/iNDM-52klhSKPCGjZoP0ODll_caIe6IsMg4zOI4TGkk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cee819517aa5be7acd89d5a70276ad6aa1bdf99" title="Chinny ‚Äî the unlimited, on-device voice cloner ‚Äî just dropped on iOS! (macOS version pending review üëÄ)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chinny is an on-device voice cloning app for iOS and macOS, powered by a SoTA AI voice-cloning model (Chatterbox). It runs fully offline with no information leaving your device. &lt;strong&gt;No ads. No registration. No permission required. No network connectivity.&lt;/strong&gt; &lt;strong&gt;No hidden fees. No usage restrictions. Free forever.&lt;/strong&gt; Use it to have a familiar voice read bedtime stories, record personal audiobooks, add voiceovers for videos, generate podcast narration, create game or film temp lines, or provide accessible read-aloud for long articles‚Äîall privately on your device.&lt;/p&gt; &lt;p&gt;You can try the iOS and Mac version at &lt;a href="https://apps.apple.com/us/app/chinny-offline-voice-cloner/id6753816417"&gt;https://apps.apple.com/us/app/chinny-offline-voice-cloner/id6753816417&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Require 3 GB RAM for inference, 3.41 GB space because all models are packed inside the app.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; (1) (You can run a quick test from menu-&amp;gt;multi spkear. If you hit generate and it shows &lt;strong&gt;&amp;quot;Exception during initlization std::bad_alloc&amp;quot;&lt;/strong&gt;, this suggests your iPhone doesn't have enough memory) (2) If it &lt;strong&gt;crashed&lt;/strong&gt;, it is more likely because your phone doesn't have enough memory. You can try with another phone, like iPhone 16 Pro or iPhone 17 Pro.&lt;/p&gt; &lt;p&gt;If you want to clone your voice, prepare a clean voice sample of at least 10 seconds in mp3, wav, or m4a format.&lt;/p&gt; &lt;p&gt;PS: I've anonymized the voice source data to comply with App Store policies&lt;/p&gt; &lt;p&gt;All I need is feedback and reviews on App store!&lt;/p&gt; &lt;p&gt;Happy new year and best wishes to you and your family :).&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q2lm5a/video/4daiccsff2bg1/player"&gt;Chinny: offline voice Cloner&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tingxiaojue"&gt; /u/Tingxiaojue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T04:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2qo2u</id>
    <title>WhisperNote ‚Äî a simple local Whisper-based transcription app (Windows)</title>
    <updated>2026-01-03T09:44:46+00:00</updated>
    <author>
      <name>/u/_fortexe</name>
      <uri>https://old.reddit.com/user/_fortexe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"&gt; &lt;img alt="WhisperNote ‚Äî a simple local Whisper-based transcription app (Windows)" src="https://preview.redd.it/wcbalo8cu3bg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0328ac2605fcb9e788498a1ee7e1ae8c3f63b484" title="WhisperNote ‚Äî a simple local Whisper-based transcription app (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a small personal project called WhisperNote. It‚Äôs a simple Windows desktop app for local audio transcription using OpenAI Whisper.&lt;/p&gt; &lt;p&gt;The main goal was not to build ‚Äúthe best‚Äù tool, but a clean and straightforward one: press record or drop an audio file ‚Äî get text.&lt;/p&gt; &lt;p&gt;All processing happens locally on your machine. No cloud, no accounts. It‚Äôs intentionally minimal and focused on doing one thing well. Models are downloaded once, then everything runs offline.&lt;/p&gt; &lt;p&gt;I‚Äôm sharing it here in case someone values simplicity and local-first tools as much as I do. If it‚Äôs useful to you ‚Äî that‚Äôs great. Note: the Windows build is ~4 GB because it bundles Python, PyTorch with CUDA, and FFmpeg for a fully offline, out-of-the-box experience.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/LokiSkardina/WhisperNote"&gt;https://github.com/LokiSkardina/WhisperNote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_fortexe"&gt; /u/_fortexe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcbalo8cu3bg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T09:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21wqw</id>
    <title>A deep dive in DeepSeek's mHC: They improved things everyone else thought didn‚Äôt need improving</title>
    <updated>2026-01-02T15:44:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Context&lt;/h1&gt; &lt;p&gt;Since ResNet (2015), the Residual Connection (x_{l+1} = x_l + F(x_l)) has been the untouchable backbone of deep learning (from CNN to Transformer, from BERT to GPT). It solves the vanishing gradient problem by providing an &amp;quot;identity mapping&amp;quot; fast lane. For 10 years, almost no one questioned it.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;However, this standard design forces a rigid 1:1 ratio between the input and the new computation, preventing the model from dynamically adjusting how much it relies on past layers versus new information.&lt;/p&gt; &lt;h1&gt;The Innovation&lt;/h1&gt; &lt;p&gt;ByteDace tried to break this rule with &amp;quot;Hyper-Connections&amp;quot; (HC), allowing the model to learn the connection weights instead of using a fixed ratio.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The potential:&lt;/strong&gt; Faster convergence and better performance due to flexible information routing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The issue:&lt;/strong&gt; It was incredibly unstable. Without constraints, signals were amplified by &lt;strong&gt;3000x&lt;/strong&gt; in deep networks, leading to exploding gradients.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Solution: Manifold-Constrained Hyper-Connections (mHC)&lt;/h1&gt; &lt;p&gt;In their new paper, DeepSeek solved the instability by constraining the learnable matrices to be &amp;quot;Double Stochastic&amp;quot; (all elements ‚âß 0, rows/cols sum to 1).&lt;/p&gt; &lt;p&gt;Mathematically, this forces the operation to act as a weighted average (convex combination). It guarantees that signals are never amplified beyond control, regardless of network depth.&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Max gain magnitude dropped from &lt;strong&gt;3000 to 1.6&lt;/strong&gt; (3 orders of magnitude improvement).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; mHC beats both the standard baseline and the unstable HC on benchmarks like GSM8K and DROP.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; Only adds ~6% to training time due to heavy optimization (kernel fusion).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f"&gt;https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As hinted in the attached tweet, we are seeing a fascinating split in the AI world. While the industry frenzy focuses on commercialization and AI Agents‚Äîexemplified by Meta spending $2 Billion to acquire Manus‚Äîlabs like DeepSeek and Moonshot (Kimi) are playing a different game.&lt;/p&gt; &lt;p&gt;Despite resource constraints, they are digging into the deepest levels of macro-architecture and optimization. They have the audacity to question what we took for granted: &lt;strong&gt;Residual Connections&lt;/strong&gt; (challenged by DeepSeek's mHC) and &lt;strong&gt;AdamW&lt;/strong&gt; (challenged by Kimi's Muon). Just because these have been the standard for 10 years doesn't mean they are the optimal solution.&lt;/p&gt; &lt;p&gt;Crucially, instead of locking these secrets behind closed doors for commercial dominance, they are &lt;strong&gt;open-sourcing&lt;/strong&gt; these findings for the advancement of humanity. This spirit of relentless self-doubt and fundamental reinvention is exactly how we evolve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2p2wa</id>
    <title>nanbeige4 is an incredible model for running locally</title>
    <updated>2026-01-03T08:06:54+00:00</updated>
    <author>
      <name>/u/Revolutionalredstone</name>
      <uri>https://old.reddit.com/user/Revolutionalredstone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like a deepseek moment might have slipped a few people by&lt;/p&gt; &lt;p&gt;nanbeige (weird name- apparently chosen to be bland/uninteresting)&lt;/p&gt; &lt;p&gt;..It's very interesting! basically 3 invalidating most 30B models.&lt;/p&gt; &lt;p&gt;(you can find it up ridiculously high on this chart: for a 3B model)&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm stoked to have intelligence like this at home, but I'd love to know how to push this into super fast interference territory! (I've heard about diffusion based conversion etc and am super keen!)&lt;/p&gt; &lt;p&gt;Has anyone else seen something newer (this is a few weeks old now)? Seems like various charts show this one to be an outlier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionalredstone"&gt; /u/Revolutionalredstone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2ppkb</id>
    <title>MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration</title>
    <updated>2026-01-03T08:45:29+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt; &lt;img alt="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" src="https://external-preview.redd.it/AAFIDX32Yo3yBOTXBXkwbpmtKeh886wBWSOhkOds4Pc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc2cf3d74a648b569c2bd55f6d299c6f5f27ea9" title="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2onpg</id>
    <title>Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)</title>
    <updated>2026-01-03T07:42:14+00:00</updated>
    <author>
      <name>/u/atif_dev</name>
      <uri>https://old.reddit.com/user/atif_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt; &lt;img alt="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" src="https://b.thumbs.redditmedia.com/27iIaYMlD6cCAk7xURWf9QOU6i1_KjiShnklurpG_mo.jpg" title="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a personal project called ATOM ‚Äî a fully local AI assistant designed more like an operating system for intelligence than a chatbot.&lt;/p&gt; &lt;p&gt;Everything runs locally. No cloud inference.&lt;/p&gt; &lt;p&gt;Key components: - Local LLM via LM Studio (currently Qwen3-VL-4B, vision + tool calling) - Tool orchestration (system info, web search via self-hosted SearXNG, file/PDF generation, Home Assistant, robotics) - Long-term memory with ChromaDB - Async memory saving via a smaller ‚Äújudge‚Äù model Semantic retrieval + periodic RAG-style injection - Dedicated local embedding server (OpenAI-style API) - Real hardware control (robotic arm, sensors) - JSON logging + test harness for reproducible scenarios&lt;/p&gt; &lt;p&gt;On the UI side, I built a React + React Three Fiber interface using Firebase Studio that visualizes tool usage as orbiting ‚Äúplanets‚Äù around a central core. It‚Äôs mostly for observability and debugging, but it turned out pretty fun.&lt;/p&gt; &lt;p&gt;Constraints: Hardware is limited (GTX 1650), so performance tradeoffs were necessary System is experimental and some components are still evolving&lt;/p&gt; &lt;p&gt;This is not a product, just a personal engineering project exploring: - long-term memory consolidation - tool-centric reasoning - fully local personal AI systems&lt;/p&gt; &lt;p&gt;Would appreciate feedback, especially from others running local setups or experimenting with memory/tool architectures.&lt;/p&gt; &lt;p&gt;GitHub (backend): &lt;a href="https://github.com/AtifUsmani/A.T.O.M"&gt;https://github.com/AtifUsmani/A.T.O.M&lt;/a&gt; UI repo: &lt;a href="https://github.com/AtifUsmani/ATOM-UI"&gt;https://github.com/AtifUsmani/ATOM-UI&lt;/a&gt; Demo videos linked in the README.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atif_dev"&gt; /u/atif_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2onpg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2dcje</id>
    <title>ASUS officially announces price hikes from January 5, right before CES 2026</title>
    <updated>2026-01-02T22:53:19+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt; &lt;img alt="ASUS officially announces price hikes from January 5, right before CES 2026" src="https://external-preview.redd.it/e0KouFF887lm3A9dV6mq_44cYZmQvCh1I3h_7LTZz8c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1b6efd1e001ac6907d2df9de1ac4165e4b086a" title="ASUS officially announces price hikes from January 5, right before CES 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/asus-officially-announces-price-hikes-from-january-5-right-before-ces-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T22:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2p5dh</id>
    <title>Glm4.7 + CC not bad</title>
    <updated>2026-01-03T08:10:55+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I genuinely think it's pretty good this time - GLM4.7 + CC is actually somewhat close to 4.5 Sonnet, or more accurately I'd say it's on par with 4 Sonnet. I'm subscribed to the middle-tier plan.&lt;/p&gt; &lt;p&gt;I tested it with a project that has a Python backend and TypeScript frontend, asking it to add a feature that involved both backend and frontend work. It handled everything smoothly, and the MCP calls all went through without getting stuck (which used to be a problem before).&lt;/p&gt; &lt;p&gt;Of course, to be completely honest, there's still a massive gap between this and 4.5 Opus - Opus is on a completely insane level&lt;/p&gt; &lt;p&gt;So I'm still keeping my $10/month GitHub Copilot subscription. For the really tough problems, I'll use 4.5 Opus, but for regular stuff, GLM4.7 + CC basically handles everything. GLM4.7 costs me $100/month now, plus the $10 for Copilot - that's less than around $13 per month total(bigmodel.cn coding plan), which feels pretty good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jwsn</id>
    <title>How is Cloud Inference so cheap</title>
    <updated>2026-01-03T03:37:13+00:00</updated>
    <author>
      <name>/u/VolkoTheWorst</name>
      <uri>https://old.reddit.com/user/VolkoTheWorst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do cloud inference companies like DeepInfra, Together, Chutes, Novita etc manage to be in profit regarding to the price of the GPUs/electricity and the fact that I guess it's difficult to have always someone to serve ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VolkoTheWorst"&gt; /u/VolkoTheWorst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q25070</id>
    <title>LeCun Says Llama 4 results "were fudged a little bit"</title>
    <updated>2026-01-02T17:38:01+00:00</updated>
    <author>
      <name>/u/MrPecunius</name>
      <uri>https://old.reddit.com/user/MrPecunius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:&lt;/p&gt; &lt;p&gt;&lt;a href="https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation"&gt;'Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation &lt;/a&gt;&lt;/p&gt; &lt;p&gt;This bit jumped out at me:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Zuckerberg subsequently &amp;quot;sidelined the entire GenAI organisation,&amp;quot; according to LeCun. &amp;quot;A lot of people have left, a lot of people who haven't yet left will leave.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPecunius"&gt; /u/MrPecunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T17:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2o033</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 20GB VRAM and 24GB RAM</title>
    <updated>2026-01-03T07:04:18+00:00</updated>
    <author>
      <name>/u/Death_12_35_taken</name>
      <uri>https://old.reddit.com/user/Death_12_35_taken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for something that can stay in character and be fast but also creative. I am looking for models that i can run locally and at decent speed. Just need something that is smart and uncensored. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Death_12_35_taken"&gt; /u/Death_12_35_taken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2pons</id>
    <title>GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)</title>
    <updated>2026-01-03T08:43:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt; &lt;img alt="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" src="https://external-preview.redd.it/RT6xZIQ5U8h3GMBsKzEeqHJyXy63I2_XP8TVKTT_Hvg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c7ca25d885be26a9f257d4e17e2b038061773a" title="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
