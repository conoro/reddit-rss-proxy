<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-30T14:25:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pzgtjk</id>
    <title>What is a good model for assisting with patching source code?</title>
    <updated>2025-12-30T12:24:52+00:00</updated>
    <author>
      <name>/u/signalclown</name>
      <uri>https://old.reddit.com/user/signalclown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently experimenting with an LLM service specifically with patching software. My workflow is like this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I tell it I want to patch a popular opensource program to add a new feature, and ask it to give me keywords to search for throughout the source.&lt;/li&gt; &lt;li&gt;It gives me a list of keywords, and then I give it the output of `grep -n -r` and ask if anything looks interesting.&lt;/li&gt; &lt;li&gt;It mentions a file, and then I show it the entire content if the file size is reasonable, or I specifically show the function of interest.&lt;/li&gt; &lt;li&gt;I report back build errors and it makes changes and we try some things until we get it to work.&lt;/li&gt; &lt;li&gt;Once it works, I review everything, highlight codesmells and find better ways to do it, refactor, and see ultimately what makes most sense.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There is some trial and error involved but so far I have been able to patch many programs to customize applications specifically for me. Now I am looking to have this same capability in a self-hosted LLM for privacy reasons.&lt;/p&gt; &lt;p&gt;I'm not so sure how the context size and memory requirement is determined. I mean, if I can show it about 800-1000 LoC in one message, that's more than enough. However, it should not simply forget every thing after a just a few messages.&lt;/p&gt; &lt;p&gt;Many people say LLMs are not that great for programming, but I find it to be exceptionally good in this narrow scope of what I'm using it in - in the sense that there is code already written in the same project that it can use as a reference to simply make small changes. To implement a feature, it usually takes me 3-4 hours of manual copy/pasting and lots of trial and error, and the final result is possibly less than 100 LoC but I still find it impressive that I was able to get something new to work in a project whose codebase I previously had no idea about.&lt;/p&gt; &lt;p&gt;What kind of model do I even need to replicate something like this, and how would it compare with the code-generation capabilities of something like ChatGPT or others? Is a 5090 or a modded 4090 good enough for something like this? If so, what is a good model for this? I'm looking to put together a new build specifically to host an LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/signalclown"&gt; /u/signalclown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgtjk/what_is_a_good_model_for_assisting_with_patching/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgtjk/what_is_a_good_model_for_assisting_with_patching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgtjk/what_is_a_good_model_for_assisting_with_patching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzh0w2</id>
    <title>Lemonade NPU/iGPU hybrid mode benchmarks?</title>
    <updated>2025-12-30T12:35:36+00:00</updated>
    <author>
      <name>/u/dabiggmoe2</name>
      <uri>https://old.reddit.com/user/dabiggmoe2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I was reading that Lemonade server on Windows has NPU/iGPU hybrid inference mode that supposedly improves the prompt processing speed on Strix Halo but I couldn't find any benchmarks online. &lt;/p&gt; &lt;p&gt;Does anyone have benchmarks for prompt processing using iGPU vs NPU/iGPU on Strix Halo using Lemonade server on Windows? &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabiggmoe2"&gt; /u/dabiggmoe2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzh0w2/lemonade_npuigpu_hybrid_mode_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzh0w2/lemonade_npuigpu_hybrid_mode_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzh0w2/lemonade_npuigpu_hybrid_mode_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzi339</id>
    <title>Building "Derin" - An Embodied AI project for Jetson AGX Thor (94K lines, looking for feedback)</title>
    <updated>2025-12-30T13:26:22+00:00</updated>
    <author>
      <name>/u/Logarhitma</name>
      <uri>https://old.reddit.com/user/Logarhitma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been developing an embodied AI system designed for edge deployment on NVIDIA Jetson AGX Thor.&lt;/p&gt; &lt;p&gt;What I'm building:&lt;/p&gt; &lt;p&gt;Consciousness-inspired decision making&lt;/p&gt; &lt;p&gt;- Not just prompt-response, but continuous awareness&lt;/p&gt; &lt;p&gt;- Autonomous goal setting and execution&lt;/p&gt; &lt;p&gt;Real-time perception&lt;/p&gt; &lt;p&gt;- Designed for 30ms visual processing loop&lt;/p&gt; &lt;p&gt;- Continuous environmental awareness&lt;/p&gt; &lt;p&gt;Physical embodiment (in progress)&lt;/p&gt; &lt;p&gt;- Robotic arm integration with visual feedback&lt;/p&gt; &lt;p&gt;- Learning from demonstration&lt;/p&gt; &lt;p&gt;100% Edge deployment&lt;/p&gt; &lt;p&gt;- Multi-model LLM architecture&lt;/p&gt; &lt;p&gt;- No cloud dependency&lt;/p&gt; &lt;p&gt;Current status: Architecture complete, waiting for Thor hardware to test.&lt;/p&gt; &lt;p&gt;Looking for feedback on the approach. Is embodied AI the right direction after the &amp;quot;LLM scaling wall&amp;quot; discussions&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/YusufAliToklu/Derin.git"&gt;https://github.com/YusufAliToklu/Derin.git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Logarhitma"&gt; /u/Logarhitma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi339/building_derin_an_embodied_ai_project_for_jetson/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi339/building_derin_an_embodied_ai_project_for_jetson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi339/building_derin_an_embodied_ai_project_for_jetson/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T13:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzi6an</id>
    <title>Llama 3.2 3B fMRI - Findings Update!</title>
    <updated>2025-12-30T13:30:27+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry, no fancy pictures today :(&lt;/p&gt; &lt;p&gt;I tried hard ablation (zeroing) of the target dimension and saw no measurable effect on model output.&lt;/p&gt; &lt;p&gt;However, targeted perturbation of the same dimension reliably modulates behavior. This strongly suggests the signal is part of a distributed mechanism rather than a standalone causal unit.&lt;/p&gt; &lt;p&gt;I‚Äôm now pivoting to tracing correlated activity across dimensions (circuit-level analysis). Next step is measuring temporal co-activation with the target dim across tokens, focusing on correlation rather than magnitude, to map the surrounding circuit (‚Äúconstellation‚Äù) that moves together.&lt;/p&gt; &lt;p&gt;Turns out the cave goes deeper. Time to spelunk.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi6an/llama_32_3b_fmri_findings_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi6an/llama_32_3b_fmri_findings_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi6an/llama_32_3b_fmri_findings_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T13:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrn9v</id>
    <title>I Finished a Fully Local Agentic RAG Tutorial</title>
    <updated>2025-12-29T17:03:44+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôve just finished a &lt;strong&gt;complete Agentic RAG tutorial + repository&lt;/strong&gt; that shows how to build a fully local, end-to-end system.&lt;/p&gt; &lt;p&gt;No APIs, no cloud, no hidden costs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üí° What‚Äôs inside&lt;/h3&gt; &lt;p&gt;The tutorial covers the full pipeline, including the parts most examples skip:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF ‚Üí Markdown ingestion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hierarchical chunking (parent / child)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hybrid retrieval (dense + sparse)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Vector store with &lt;strong&gt;Qdrant&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Query rewriting + &lt;strong&gt;human-in-the-loop&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Context summarization&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-agent map-reduce&lt;/strong&gt; with &lt;strong&gt;LangGraph&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Local inference with &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Simple &lt;strong&gt;Gradio&lt;/strong&gt; UI&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üéØ Who it‚Äôs for&lt;/h3&gt; &lt;p&gt;If you want to &lt;strong&gt;understand Agentic RAG by building it&lt;/strong&gt;, not just reading theory, this might help.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üîó Repo&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzgmvr</id>
    <title>OEM vs Retail PNY 6000 Pro</title>
    <updated>2025-12-30T12:15:25+00:00</updated>
    <author>
      <name>/u/NaiRogers</name>
      <uri>https://old.reddit.com/user/NaiRogers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had experience with the differences between the OEM and retail versions of the PNY VCNRTXPRO6000 (SB vs PB)? They seem to have the same warranty at least from the vendors I checked. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaiRogers"&gt; /u/NaiRogers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgmvr/oem_vs_retail_pny_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgmvr/oem_vs_retail_pny_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgmvr/oem_vs_retail_pny_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz4x0v</id>
    <title>RAG Paper 25.12.24</title>
    <updated>2025-12-30T01:45:44+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.21280v1"&gt;SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20916v1"&gt;MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20884v1"&gt;The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T01:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyyp59</id>
    <title>What is the best way to allocated $15k right now for local LLMs?</title>
    <updated>2025-12-29T21:26:38+00:00</updated>
    <author>
      <name>/u/LargelyInnocuous</name>
      <uri>https://old.reddit.com/user/LargelyInnocuous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best bang for $15k right now? Would like to be able to run DeepSeek, Kimi K2 and GLM 4.5+. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LargelyInnocuous"&gt; /u/LargelyInnocuous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T21:26:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz9x3u</id>
    <title>One answer to "what do you use local LLMs for?": a hyper-personalized multimodal event crawler</title>
    <updated>2025-12-30T05:42:32+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see the &amp;quot;what do you use local LLMs for?&amp;quot; question come up every month, so here's one example: a multimodal agent that crawls local websites to find events happening around me.&lt;/p&gt; &lt;h1&gt;Why local instead of API?&lt;/h1&gt; &lt;p&gt;People ask me this a lot. Cloud providers are cheap, until you're generating millions of tokens. I'm crawling dozens of event sources, processing images, deduplicating across sites. That adds up fast.&lt;/p&gt; &lt;p&gt;Local is also faster for my use case. Claude and GPT grind to a halt during peak loads. &lt;a href="https://www.ovidiudan.com/2025/12/25/dual-rtx-pro-6000-llm-guide.html"&gt;My home server&lt;/a&gt; gives me consistent throughput whenever I need it.&lt;/p&gt; &lt;h1&gt;The setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Dual RTX Pro 6000 (96GB VRAM each)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V"&gt;GLM-4.6V&lt;/a&gt; (106B parameter multimodal model) running on vLLM&lt;/li&gt; &lt;li&gt;The crawler, backend, and mobile app were all vibe coded with Claude Opus&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What GLM-4.6V actually does&lt;/h1&gt; &lt;p&gt;The crawler uses the model for five tasks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Extracting info from event flyers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where multimodal models shine. &lt;a href="https://whidbeycamanoislands.com/event/the-dead-guise-new-years-eve/"&gt;Here's an event&lt;/a&gt; where the text description doesn't mention the price, but the flyer image does. The LLM reads the flyer and extracts &amp;quot;$25&amp;quot; into a structured field.&lt;/p&gt; &lt;p&gt;OCR can read text from an image, but it can't understand that &amp;quot;$25&amp;quot; on a psychedelic Grateful Dead flyer is the ticket price and not a date or an address. That requires a model that actually understands what it's looking at.&lt;/p&gt; &lt;p&gt;The model also extracts venue names, performer lineups, age restrictions, and registration requirements from a combination of the raw HTML and the accompanying image.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Rewriting messy descriptions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Scraped event descriptions are a mess: HTML artifacts, escaped characters, inconsistent formatting. The LLM rewrites these into clean paragraphs while preserving the essential info.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Link classification&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rather than fragile regex to find ticket links, the LLM analyzes all links on a page and identifies the primary registration URL (not the &amp;quot;Buy Tickets&amp;quot; link for a different event in the sidebar).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Cross-source deduplication&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The same event appears on multiple websites. The LLM compares new events against existing ones and determines if it's a duplicate. It understands that &amp;quot;NYE Party at The Clyde&amp;quot; and &amp;quot;New Year's Eve Celebration - Clyde Theatre&amp;quot; are the same event.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Multi-event extraction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some sources publish newsletter images containing multiple events. The LLM extracts each event separately from a single composite image.&lt;/p&gt; &lt;h1&gt;The point&lt;/h1&gt; &lt;p&gt;A few years ago, some of this would have been practically impossible. Not just expensive or slow, but actually impossible. Multimodal understanding of unstructured visual data wasn't something you could just spin up.&lt;/p&gt; &lt;p&gt;Now I can throw together a custom tool over a weekend that does exactly what I need. Tools built for an audience of one, running on hardware I control.&lt;/p&gt; &lt;p&gt;Full writeup with more details on the Firebase backend and Flutter app: &lt;a href="https://www.ovidiudan.com/2025/12/30/age-customized-software.html"&gt;The age of hyper-personalized software&lt;/a&gt; (I am not selling or promoting anything, I do this for fun.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T05:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzd0j7</id>
    <title>Has anyone built a RAG on WikiLeaks?</title>
    <updated>2025-12-30T08:41:14+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because that would be a useful application. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz67hm</id>
    <title>5 new korean models will be released in 2 hours</title>
    <updated>2025-12-30T02:42:37+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura"&gt;https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Naver, LG, SK, NC, Upstage&lt;/p&gt; &lt;p&gt;All 5 models will be released in 2 to 3 hours. Follow with the YouTube link&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pze13o</id>
    <title>Exploring a 1.58-bit / ternary LLM core inspired by BitNet (CUDA attention, GTX 1050 tests)</title>
    <updated>2025-12-30T09:44:36+00:00</updated>
    <author>
      <name>/u/HuseyinKama</name>
      <uri>https://old.reddit.com/user/HuseyinKama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with extreme low-bit LLM inference inspired by the BitNet 1.58-bit paper,&lt;/p&gt; &lt;p&gt;and wanted to share a research-style project I‚Äôve been working on over the last few weeks.&lt;/p&gt; &lt;p&gt;This is NOT a production-ready model, but rather an exploration of how far ternary / sparse logic&lt;/p&gt; &lt;p&gt;can be pushed on consumer GPUs.&lt;/p&gt; &lt;p&gt;What this project explores:&lt;/p&gt; &lt;p&gt;- A custom LLM core using ternary weights {-1, 0, +1}&lt;/p&gt; &lt;p&gt;- Trainable via Straight-Through Estimator (STE)&lt;/p&gt; &lt;p&gt;- Custom CUDA attention kernel (thresholded / shifted-ReLU instead of softmax)&lt;/p&gt; &lt;p&gt;- Designed for local inference (tested on GTX 1050)&lt;/p&gt; &lt;p&gt;Core ideas:&lt;/p&gt; &lt;p&gt;- Replace FP16-heavy matmul layers with ternary linear layers&lt;/p&gt; &lt;p&gt;- Abs-mean scaling (BitNet-style quantization)&lt;/p&gt; &lt;p&gt;- Focus on reducing interference via sparsity rather than magnitude precision&lt;/p&gt; &lt;p&gt;- Attention without softmax to reduce compute and improve stability in low-bit regimes&lt;/p&gt; &lt;p&gt;Current results:&lt;/p&gt; &lt;p&gt;- End-to-end training works&lt;/p&gt; &lt;p&gt;- Overfitting tests succeed (Python training ‚Üí CUDA inference consistency)&lt;/p&gt; &lt;p&gt;- Character-level Shakespeare training produces coherent output&lt;/p&gt; &lt;p&gt;- Memory footprint is significantly reduced compared to FP16 baselines&lt;/p&gt; &lt;p&gt;Limitations / open problems:&lt;/p&gt; &lt;p&gt;- Not competitive with large FP16/INT8 models (expected)&lt;/p&gt; &lt;p&gt;- Sensitive to threshold and temperature tuning&lt;/p&gt; &lt;p&gt;- No advanced optimizations like FlashAttention&lt;/p&gt; &lt;p&gt;- Very much a research prototype&lt;/p&gt; &lt;p&gt;I‚Äôm mainly sharing this to get feedback from people who:&lt;/p&gt; &lt;p&gt;- Have worked with BitNet / ternary networks&lt;/p&gt; &lt;p&gt;- Experiment with custom CUDA kernels&lt;/p&gt; &lt;p&gt;- Care about local / low-power LLM inference&lt;/p&gt; &lt;p&gt;Code and CUDA kernels are available here for anyone curious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QKV-Core/Trion"&gt;https://github.com/QKV-Core/Trion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer technical questions or discuss design tradeoffs.&lt;/p&gt; &lt;p&gt;EDIT / Clarification:&lt;/p&gt; &lt;p&gt;This is not a commercial project, not a startup pitch, and not a benchmark claim.&lt;/p&gt; &lt;p&gt;I‚Äôm sharing this as an experimental research / engineering exploration inspired by the BitNet 1.58-bit paper.&lt;/p&gt; &lt;p&gt;The goal is to understand how far ternary + sparse computation can go on consumer hardware.&lt;/p&gt; &lt;p&gt;No paid product, no token, no API, no funding.&lt;/p&gt; &lt;p&gt;Just code, CUDA kernels, and learning.&lt;/p&gt; &lt;p&gt;Feedback and criticism are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HuseyinKama"&gt; /u/HuseyinKama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T09:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzaz73</id>
    <title>[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the "Thinking Process" (Hidden States).</title>
    <updated>2025-12-30T06:40:32+00:00</updated>
    <author>
      <name>/u/JB_King1919</name>
      <uri>https://old.reddit.com/user/JB_King1919</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt; &lt;img alt="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." src="https://b.thumbs.redditmedia.com/Bpxf3FjdwheVRpbXrcXJxisJg0Hd6sGLSEAe6P2x7Fg.jpg" title="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm a PhD student in &lt;strong&gt;Electromagnetics&lt;/strong&gt;. In my daily work, I deal with fields, waves, and trajectories. When I started playing with Local LLMs, I felt something was missing: we usually look at the &lt;em&gt;output&lt;/em&gt; text or the &lt;em&gt;loss curves&lt;/em&gt;, but we rarely see &lt;strong&gt;how&lt;/strong&gt; the model gets from A to B.&lt;/p&gt; &lt;p&gt;To an RF engineer, reasoning isn't just a probability distribution‚Äîit's a &lt;strong&gt;dynamic flow&lt;/strong&gt; through a high-dimensional space.&lt;/p&gt; &lt;p&gt;So, I built a lightweight Python toolkit to extract hidden states layer-by-layer and visualize them as continuous &lt;strong&gt;2D/3D trajectories&lt;/strong&gt;. I wanted to see if &amp;quot;thoughts&amp;quot; have a geometric shape.&lt;/p&gt; &lt;p&gt;The results were surprisingly consistent. I‚Äôm sharing the tool so you can run it on your own models (Llama, Qwen, Mistral, etc.).&lt;/p&gt; &lt;h1&gt;1. The &amp;quot;Confidence Funnel&amp;quot; (Convergence)&lt;/h1&gt; &lt;p&gt;I found that if you feed the model slightly different prompts about the same concept (e.g., &amp;quot;Define Justice&amp;quot;, &amp;quot;What is Fairness&amp;quot;), the internal states start far apart but &lt;strong&gt;physically collapse&lt;/strong&gt; into a single &amp;quot;attractor basin&amp;quot; as the layers get deeper.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c"&gt;https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; You can use this to test &lt;strong&gt;Prompt Stability&lt;/strong&gt;. If the funnel is tight, the model is sure. If it sprays out at the end, the model is confused or hallucinating.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Llama-3 vs. Qwen-2.5: Different &amp;quot;Thinking Styles&amp;quot;&lt;/h1&gt; &lt;p&gt;This was the coolest find. When I ran the same prompts through different architectures, the &amp;quot;shape&amp;quot; of their thinking was totally different.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8"&gt;https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama-3 (Left):&lt;/strong&gt; Seems to &amp;quot;decide&amp;quot; on the semantics very early (Layers 5-10). The trajectory is direct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen-2.5 (Right):&lt;/strong&gt; Keeps the trajectory expanded (in superposition?) until the very last layers (Layer 20+). It seems to &amp;quot;hold&amp;quot; the ambiguity much longer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; This might give us a geometric way to profile model behaviors beyond just benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Visualizing &amp;quot;Refusal&amp;quot; (The Safety Spike)&lt;/h1&gt; &lt;p&gt;I was curious what RLHF looks like geometrically. I visualized the trajectory when the model refuses a jailbreak versus when it follows a safe instruction.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca"&gt;https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hard Refusal(Red):&lt;/strong&gt; Looks like a particle hitting a brick wall‚Äîa sharp, high-curvature spike.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Soft Steering(Green):&lt;/strong&gt; Looks like a smooth turn. And an obvious &amp;quot;U-turn&amp;quot; at the end of its trajectory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; A visual &amp;quot;Geiger Counter&amp;quot; for safety tuning. You can see if your system prompt is creating a hard wall or a soft guide.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì• The Toolkit&lt;/h1&gt; &lt;p&gt;I packaged this into a Python library with example scripts. It works with local HuggingFace weights (no API needed).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/JBKing514/map_llm_toolkit"&gt;LLM Toolkit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß† The Theory (Optional)&lt;/h1&gt; &lt;p&gt;I‚Äôm not an AI researcher, but I wrote up some notes on the &lt;strong&gt;manifold dynamics&lt;/strong&gt; perspective behind this tool (treating inference as a Langevin flow). If you are interested in the math/physics intuition behind these visualizations or need more info about my experiment setup, I put up a page and my notes here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Project Page &amp;amp; Math:&lt;/strong&gt; &lt;a href="https://jbking514.github.io/map_blog/"&gt;Project GitHub Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Foundational Notes:&lt;/strong&gt; &lt;a href="https://zenodo.org/records/17900444"&gt;Manifold Alignment Protocol (MAP)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what &lt;strong&gt;Mistral&lt;/strong&gt; or &lt;strong&gt;Gemma&lt;/strong&gt; trajectories look like if anyone runs this. Let me know what you find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JB_King1919"&gt; /u/JB_King1919 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T06:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7mxr</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:49:11+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if this is real, but the author provides a fascinating story behind its acquisition. I would like for it to be real!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bartowski GGUFs: &lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcdu1</id>
    <title>LG K EXAONE 236b</title>
    <updated>2025-12-30T08:02:08+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt; &lt;img alt="LG K EXAONE 236b" src="https://preview.redd.it/1wirc918taag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fcab270f79f71dba1d330db0ee8e85422de763b" title="LG K EXAONE 236b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will be released in few days &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wirc918taag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzggbf</id>
    <title>Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide</title>
    <updated>2025-12-30T12:05:52+00:00</updated>
    <author>
      <name>/u/at0mi</name>
      <uri>https://old.reddit.com/user/at0mi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"&gt; &lt;img alt="Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide" src="https://preview.redd.it/2eimvrgo0cag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a97232a34737346670be6cb9292a1cdde03aa47a" title="Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! If you're passionate about squeezing every last bit of performance out of older hardware for local large language models, I've got something exciting to share. I managed to get GLM-4.7 ‚Äì that's the massive 355B parameter Mixture of Experts model ‚Äì running in Q8_0 quantization on a seriously vintage setup: a 2015 Lenovo System x3950 X6 with eight Xeon E7-8880 v3 CPUs (no GPU in sight, just pure CPU inference). After a bunch of trial and error, I'm hitting around 5-6 tokens per second, which is pretty respectable for such an ancient beast. The Q8 quantization delivers extremely high quality outputs, preserving nearly all the model's intelligence with minimal degradation ‚Äì it's practically indistinguishable from full precision for most tasks.&lt;/p&gt; &lt;p&gt;The key was optimizing everything from BIOS settings (like enabling hyper-threading and tweaking power management) to NUMA node distribution for better memory access, and experimenting with different llama.cpp forks to handle the MoE architecture efficiently. I also dove into Linux kernel tweaks, like adjusting CPU governors and hugepages, to minimize latency. Keep in mind, this setup draws about 1300W AC under full load, so it's power-hungry but worth it for local runs. Benchmarks show solid performance for generation tasks, though it's not blazing fast ‚Äì perfect for homelab enthusiasts or those without access to modern GPUs.&lt;/p&gt; &lt;p&gt;I documented the entire process chronologically in this blog post, including step-by-step setup, code snippets, potential pitfalls, and full performance metrics: &lt;a href="https://postl.ai/2025/12/29/glm47on3950x6/?referrer=grok.com"&gt;https://postl.ai/2025/12/29/glm47on3950x6/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else tried pushing big MoE models like this on CPU-only rigs? What optimizations worked for you, or what models are you running on similar hardware? Let's discuss!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/at0mi"&gt; /u/at0mi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2eimvrgo0cag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcj1q</id>
    <title>Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy</title>
    <updated>2025-12-30T08:11:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt; &lt;img alt="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" src="https://b.thumbs.redditmedia.com/d_jApTNkEXlNvJcoJA6qryuDnUo0ni-DFWBY6RTdAfg.jpg" title="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/collections/tencent/hy-mt15"&gt;https://huggingface.co/collections/tencent/hy-mt15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights: üîπ 1.8B On-Device Power: Optimized for consumer hardware with a 1GB memory footprint. Using on-policy distillation to align with larger models, it delivers 0.18s latency (50 tokens), outperforming mainstream commercial APIs. üîπ 7B SOTA Performance: An upgraded version of our WMT25 champion, surpassing mid-sized open-source models and rivaling the 90th percentile of closed-source giants like Gemini-3.0-Pro. üîπ 33+ Languages &amp;amp; Dialects: High-fidelity translation across 33 languages and 5 Chinese dialects. üîπ Production-Ready: Native support for custom terminology, long-dialogue context, and maintaining document formatting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzcj1q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzhcqu</id>
    <title>Any guesses?</title>
    <updated>2025-12-30T12:52:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt; &lt;img alt="Any guesses?" src="https://preview.redd.it/xqvj95zv8cag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30044fbca7ba499223943c95d7d236600fdbb10e" title="Any guesses?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xqvj95zv8cag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzfuqg</id>
    <title>Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi</title>
    <updated>2025-12-30T11:33:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt; &lt;img alt="Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi" src="https://b.thumbs.redditmedia.com/6XCL91-lLSt5Lf0kxdky6Te-XoW5kEkEa9zxhgmAQGg.jpg" title="Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the recent &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt;discussion&lt;/a&gt; here regarding MiniMax engineer's tweet about why they decided &lt;em&gt;against&lt;/em&gt; using int4 QAT for the MiniMax M2.1 model.&lt;/p&gt; &lt;p&gt;Interestingly, at the time of the K2 Thinking release, a Kimi infra engineer posted a deep dive on Zhihu explaining why native int4 QAT was actually crucial for them. I‚Äôve summarized the key takeaways below to offer a different perspective on the 'to quant or not to quant' debate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Kimi found int4 QAT is essential for &lt;strong&gt;MoE latency&lt;/strong&gt;, &lt;strong&gt;long-context stability&lt;/strong&gt;, and &lt;strong&gt;speeding up the RL training loop&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Decoding is Memory-Bound (Latency Focus)&lt;/h1&gt; &lt;p&gt;Unlike the MiniMax case, Kimi found that for their specific MoE architecture (which is highly sparse), the decoding phase is almost exclusively memory-bound. By using W4A16 (4-bit weights, 16-bit activations), they reduced memory usage significantly. This allowed the model to fit on fewer GPUs, which reduced inter-device communication overhead, a major factor in lowering end-to-end latency for users.&lt;/p&gt; &lt;h1&gt;PTQ Failed at &amp;quot;Thinking&amp;quot; Lengths&lt;/h1&gt; &lt;p&gt;The team initially tried standard Post-Training Quantization (PTQ). While it worked for short responses, it fell apart for the long chain-of-thought &amp;quot;thinking&amp;quot; process. As generation length increased, quantization errors accumulated, leading to degradation. Furthermore, PTQ struggled with sparse experts; if an expert wasn't hit frequently during the calibration step with the calibration dataset, it essentially &amp;quot;forgot&amp;quot; knowledge. QAT (Quantization Aware Training) was necessary to make the model &amp;quot;lossless&amp;quot; compared to the BF16 baseline.&lt;/p&gt; &lt;h1&gt;A less discussed benefit: Faster RL Training&lt;/h1&gt; &lt;p&gt;This is the point that often gets overlooked: Int4 QAT wasn't just for inference serving, it accelerated the training process itself. In Reinforcement Learning, the model spends a massive amount of time in the &amp;quot;rollout&amp;quot; phase (generating text). By using the Int4 model for these rollouts, they reduced the total time for an RL iteration by 10-20%. It also reduced the discrepancy between the training forward pass and the inference engine.&lt;/p&gt; &lt;h1&gt;Why Int4 and not FP4?&lt;/h1&gt; &lt;p&gt;They chose standard Int4 over newer formats like FP4 to maintain compatibility with existing hardware (non-Blackwell GPUs) and to utilize mature, highly efficient kernels like Marlin.&lt;/p&gt; &lt;p&gt;In summary, I believe there isn't a one-size-fits-all answer regarding quantization. It depends heavily on the model's parameters and specific architecture. It is a matter of trade-offs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dzmceu5zybag1.png?width=1362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0ba8f78c6e5ade3463a1c62fba1d338a1c01ce9"&gt; AI translation, there may be some translation errors.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T11:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzg32r</id>
    <title>Solar 100B claimed that it counts better than GPT today</title>
    <updated>2025-12-30T11:46:20+00:00</updated>
    <author>
      <name>/u/Icy_Company_6216</name>
      <uri>https://old.reddit.com/user/Icy_Company_6216</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"&gt; &lt;img alt="Solar 100B claimed that it counts better than GPT today" src="https://preview.redd.it/kxyfw9z2xbag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7132ef64860af64198c058381a4037b3b7c69818" title="Solar 100B claimed that it counts better than GPT today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Company_6216"&gt; /u/Icy_Company_6216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kxyfw9z2xbag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T11:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz68fz</id>
    <title>Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.</title>
    <updated>2025-12-30T02:43:48+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt; &lt;img alt="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." src="https://preview.redd.it/ocq43c2a79ag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e9f7477bee69f806ab9bab82c73557ea1345393" title="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ocq43c2a79ag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcrtb</id>
    <title>Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model</title>
    <updated>2025-12-30T08:26:06+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt; &lt;img alt="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" src="https://preview.redd.it/yq8uriwhxaag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d990cf5383783b3e2aa22351ddeb29ebac5eb2b2" title="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to open-source Tencent HY-Motion 1.0, a billion-parameter text-to-motion model built on the Diffusion Transformer (DiT) architecture and flow matching. Tencent HY-Motion 1.0 empowers developers and individual creators alike by transforming natural language into high-fidelity, fluid, and diverse 3D character animations, delivering exceptional instruction-following capabilities across a broad range of categories. The generated 3D animation assets can be seamlessly integrated into typical 3D animation pipelines.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;üîπBillion-Scale DiT: Successfully scaled flow-matching DiT to 1B+ parameters, setting a new ceiling for instruction-following capability and generated motion quality.&lt;/p&gt; &lt;p&gt;üîπFull-Stage Training Strategy: The industry‚Äôs first motion generation model featuring a complete Pre-training ‚Üí SFT ‚Üí RL loop to optimize physical plausibility and semantic accuracy.&lt;/p&gt; &lt;p&gt;üîπComprehensive Category Coverage: Features 200+ motion categories across 6 major classes‚Äîthe most comprehensive in the industry, curated via a meticulous data pipeline.&lt;/p&gt; &lt;p&gt;üåêProject Page: &lt;a href="https://hunyuan.tencent.com/motion"&gt;https://hunyuan.tencent.com/motion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóGithub: &lt;a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0"&gt;https://github.com/Tencent-Hunyuan/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§óHugging Face: &lt;a href="https://huggingface.co/tencent/HY-Motion-1.0"&gt;https://huggingface.co/tencent/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑTechnical report: &lt;a href="https://arxiv.org/pdf/2512.23464"&gt;https://arxiv.org/pdf/2512.23464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yq8uriwhxaag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7bmv</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:34:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt; &lt;img alt="Llama-3.3-8B-Instruct" src="https://external-preview.redd.it/F-RvVhAB2x8ac9OzOxDw905YUEWDIOQBeDMa2ZyMwo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a109a6917bc66847cb36c61990f58523049b666" title="Llama-3.3-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;allura-forge&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct#llama-33-8b-instruct"&gt;&lt;/a&gt;&lt;strong&gt;Llama 3.3 8B Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yes, this is official, and yes, this is, to my knowledge, a real version of Llama 3.3 8B. (I think, anyways)&lt;/p&gt; &lt;p&gt;Facebook has a &lt;a href="https://llama.developer.meta.com"&gt;Llama API&lt;/a&gt; available that allows for inference of the other Llama models (L3.3 70B, L4 Scout and Maverick), but &lt;em&gt;also&lt;/em&gt; includes a special, new (according to the original press release) &amp;quot;Llama 3.3 8B&amp;quot; that didn't exist anywhere else and was stuck behind the Facebook API!&lt;/p&gt; &lt;p&gt;However. The Llama API supports finetuning L3.3... &lt;em&gt;and downloading the final model in HF format.&lt;/em&gt; Problem solved, right?&lt;/p&gt; &lt;p&gt;Wellllllllllllllll. Not really. The finetuning API was hidden behind layers of support tickets. I tried when the original API dropped in April, and was just told &amp;quot;We'll think about it and send you any updates&amp;quot; (there never were any updates).&lt;/p&gt; &lt;p&gt;Flash forward to December, on a whim I decide to look at the API again. And... by god... the finetuning tab was there. I could click on it and start a job (please ignore that I have no idea how it works, and in fact the finetuning tab actually disappeared after the first time I clicked on it, though I could still manually go to the page).&lt;/p&gt; &lt;p&gt;Apparently, this was not very well tested, as there were a good few bugs, the UI was janky, and the download model function did not actually work due to CORS (I had to manually curl things to get the CDN link).&lt;/p&gt; &lt;p&gt;But... by god... the zip file downloaded, and I had my slightly finetuned model.&lt;/p&gt; &lt;p&gt;To my shock and delight, however, they also provide the adapter that they merged into the model. That means I can &lt;em&gt;subtract&lt;/em&gt; that adapter and get the original model. And... here we are!&lt;/p&gt; &lt;p&gt;(actually, it should be ‚Äúnew model,‚Äù but I used ‚Äúother‚Äù to avoid triggering people)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
