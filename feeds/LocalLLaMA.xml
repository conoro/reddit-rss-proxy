<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-24T10:25:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p5bnlr</id>
    <title>which GPU upgrade for real-time speech to text using v3 turbo?</title>
    <updated>2025-11-24T08:23:26+00:00</updated>
    <author>
      <name>/u/go-getters</name>
      <uri>https://old.reddit.com/user/go-getters</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using rtx3060ti 8gb. will upgrading help to reduce the latency of real-time transcription? which GPU is the sweet spot and how much improvement will I see? &lt;/p&gt; &lt;p&gt;I tried using Parakeet 3 before and it's amazingly fast, but the accuracy is nowhere as good as v3 turbo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/go-getters"&gt; /u/go-getters &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5bnlr/which_gpu_upgrade_for_realtime_speech_to_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5bnlr/which_gpu_upgrade_for_realtime_speech_to_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5bnlr/which_gpu_upgrade_for_realtime_speech_to_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T08:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5092p</id>
    <title>what do we think of Tenstorrent Blackhole p150a's capabilities as we move into 2026?</title>
    <updated>2025-11-23T22:39:59+00:00</updated>
    <author>
      <name>/u/starkruzr</name>
      <uri>https://old.reddit.com/user/starkruzr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://tenstorrent.com/hardware/blackhole"&gt;https://tenstorrent.com/hardware/blackhole&lt;/a&gt;&lt;/p&gt; &lt;p&gt;spoke to a couple of their folks at some length at Supercomputing last week and 32GB &amp;quot;VRAM&amp;quot; (not exactly, but still) plus the strong connectivity capabilities for ganging cards together for training seems interesting, plus it's less than half as expensive as a 5090. with advancements in software over the last six-ish months, I'm curious how it's benching today vs. other options from Nvidia. about 4 months ago I think it was doing about half the performance of a 5090 at tg.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starkruzr"&gt; /u/starkruzr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5092p/what_do_we_think_of_tenstorrent_blackhole_p150as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5092p/what_do_we_think_of_tenstorrent_blackhole_p150as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5092p/what_do_we_think_of_tenstorrent_blackhole_p150as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T22:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4r0ol</id>
    <title>I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo</title>
    <updated>2025-11-23T16:30:05+00:00</updated>
    <author>
      <name>/u/Glass-Ant-6041</name>
      <uri>https://old.reddit.com/user/Glass-Ant-6041</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"&gt; &lt;img alt="I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo" src="https://external-preview.redd.it/bHBhb2J3bDJhMTNnMe_d2tHulvaS54ITJ5YIpl3vKGq8IwT_QpQcAhaljqVu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44cfc0fb49b1efd73232aa38ace208b1d2c96406" title="I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glass-Ant-6041"&gt; /u/Glass-Ant-6041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wfgc0yl2a13g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T16:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4wlej</id>
    <title>Ai2's Olmo 3 now on OpenRouter üëÄ</title>
    <updated>2025-11-23T20:10:17+00:00</updated>
    <author>
      <name>/u/ghostderp</name>
      <uri>https://old.reddit.com/user/ghostderp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wlej/ai2s_olmo_3_now_on_openrouter/"&gt; &lt;img alt="Ai2's Olmo 3 now on OpenRouter üëÄ" src="https://external-preview.redd.it/ET-WVr4mXHD1An075CPHuJQ0Di6PZSiO2KfryrF_HGA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=792f34f5ea050c95ee3f1033fc94a23dcaca3d68" title="Ai2's Olmo 3 now on OpenRouter üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Parasail added Ai2's Olmo 3 to OpenRouter‚ÄîThink (32B and 7B) and Instruct (7B). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghostderp"&gt; /u/ghostderp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://openrouter.ai/allenai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wlej/ai2s_olmo_3_now_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wlej/ai2s_olmo_3_now_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5d093</id>
    <title>I tried to separate "Thinking" from "Speaking" in LLMs (PoC)</title>
    <updated>2025-11-24T09:51:12+00:00</updated>
    <author>
      <name>/u/t-_-ji</name>
      <uri>https://old.reddit.com/user/t-_-ji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back in april, I made a video about experimenting to see if a small model can plan its answer entirely in abstract vector space before generating a single word.&lt;/p&gt; &lt;p&gt;The idea is to decouple the &amp;quot;reasoning&amp;quot; from the &amp;quot;token generation&amp;quot; to make it more efficient. I wrote an experiment, the math behind it, and the specific failure cases (it struggles with long stories) in a whitepaper style post.&lt;/p&gt; &lt;p&gt;I‚Äôd love to get some feedback on the paper structure and the concept itself.&lt;/p&gt; &lt;p&gt;Does the methodology and scalability analysis section seem sound to you?&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://gallahat.substack.com/p/proof-of-concept-decoupling-semantic"&gt;https://gallahat.substack.com/p/proof-of-concept-decoupling-semantic&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/t-_-ji"&gt; /u/t-_-ji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d093/i_tried_to_separate_thinking_from_speaking_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d093/i_tried_to_separate_thinking_from_speaking_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d093/i_tried_to_separate_thinking_from_speaking_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T09:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5d288</id>
    <title>Best LLM for mobile? Gemma vs Qwen</title>
    <updated>2025-11-24T09:54:49+00:00</updated>
    <author>
      <name>/u/tonyc1118</name>
      <uri>https://old.reddit.com/user/tonyc1118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying to pick a model for my app to run an LLM on mobile. &lt;/p&gt; &lt;p&gt;So I looked at the performance of Gemma gen 1-3, 1-2B, and Qwen gen 1-3, 0.5B-2B.&lt;/p&gt; &lt;p&gt;An interesting observation is that Gemma had a lead in generation 1, but in the past two years, Qwen has caught up. Now Qwen 3 outperforms Gemma 3. &lt;/p&gt; &lt;p&gt;This also seems to mirror the open-source competition between Google/US and Alibaba/China.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;MMLU&lt;/th&gt; &lt;th align="left"&gt;GSM8K&lt;/th&gt; &lt;th align="left"&gt;MATH&lt;/th&gt; &lt;th align="left"&gt;HumanEval&lt;/th&gt; &lt;th align="left"&gt;MBPP&lt;/th&gt; &lt;th align="left"&gt;BBH&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 1 PT 2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.0B&lt;/td&gt; &lt;td align="left"&gt;42.3&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;td align="left"&gt;11.8&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;29.2&lt;/td&gt; &lt;td align="left"&gt;35.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 2 PT 2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.0B&lt;/td&gt; &lt;td align="left"&gt;51.3&lt;/td&gt; &lt;td align="left"&gt;23.9&lt;/td&gt; &lt;td align="left"&gt;15.0&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;td align="left"&gt;29.6&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 3 IT 1B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.0B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.7 (MMLU-Pro)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;62.8&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;48.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.5&lt;/td&gt; &lt;td align="left"&gt;35.2&lt;/td&gt; &lt;td align="left"&gt;39.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 1.5 ‚Äì 0.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.5B&lt;/td&gt; &lt;td align="left"&gt;39.2&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;3.1&lt;/td&gt; &lt;td align="left"&gt;12.2&lt;/td&gt; &lt;td align="left"&gt;6.8&lt;/td&gt; &lt;td align="left"&gt;18.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 1.5 ‚Äì 1.8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.8B&lt;/td&gt; &lt;td align="left"&gt;46.8&lt;/td&gt; &lt;td align="left"&gt;38.4&lt;/td&gt; &lt;td align="left"&gt;10.1&lt;/td&gt; &lt;td align="left"&gt;20.1&lt;/td&gt; &lt;td align="left"&gt;18.0&lt;/td&gt; &lt;td align="left"&gt;24.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 2 ‚Äì 0.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.5B&lt;/td&gt; &lt;td align="left"&gt;45.4&lt;/td&gt; &lt;td align="left"&gt;36.5&lt;/td&gt; &lt;td align="left"&gt;10.7&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;28.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 2 ‚Äì 1.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.5B&lt;/td&gt; &lt;td align="left"&gt;56.5&lt;/td&gt; &lt;td align="left"&gt;58.5&lt;/td&gt; &lt;td align="left"&gt;21.7&lt;/td&gt; &lt;td align="left"&gt;31.1&lt;/td&gt; &lt;td align="left"&gt;37.4&lt;/td&gt; &lt;td align="left"&gt;37.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 2.5 ‚Äì 0.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.5B&lt;/td&gt; &lt;td align="left"&gt;47.5&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;19.5&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;29.8&lt;/td&gt; &lt;td align="left"&gt;20.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 3 ‚Äì 0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.6B&lt;/td&gt; &lt;td align="left"&gt;52.8&lt;/td&gt; &lt;td align="left"&gt;59.6&lt;/td&gt; &lt;td align="left"&gt;32.4&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;36.6&lt;/td&gt; &lt;td align="left"&gt;41.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 3 ‚Äì 1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.7B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;62.6&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;43.5&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;55.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.5&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;References:&lt;/p&gt; &lt;p&gt;- Gemma 1: &lt;a href="https://ai.google.dev/gemma/docs/core/model_card"&gt;https://ai.google.dev/gemma/docs/core/model_card&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Gemma 2: &lt;a href="https://ai.google.dev/gemma/docs/core/model_card_2"&gt;https://ai.google.dev/gemma/docs/core/model_card_2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Gemma 3: &lt;a href="https://ai.google.dev/gemma/docs/core/model_card_3"&gt;https://ai.google.dev/gemma/docs/core/model_card_3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Qwen 1.5: &lt;a href="https://qwen.ai/blog?id=qwen1.5"&gt;https://qwen.ai/blog?id=qwen1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Qwen 2: &lt;a href="https://huggingface.co/Qwen/Qwen2-1.5B"&gt;https://huggingface.co/Qwen/Qwen2-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Qwen 3: &lt;a href="https://arxiv.org/pdf/2505.09388"&gt;https://arxiv.org/pdf/2505.09388&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyc1118"&gt; /u/tonyc1118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d288/best_llm_for_mobile_gemma_vs_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d288/best_llm_for_mobile_gemma_vs_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d288/best_llm_for_mobile_gemma_vs_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T09:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ftd5</id>
    <title>Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT.</title>
    <updated>2025-11-23T06:36:53+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt; &lt;img alt="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." src="https://preview.redd.it/94nizo5acy2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a7cc2846512c02a0c4119a338567cbcbcc8574" title="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94nizo5acy2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T06:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p58g1e</id>
    <title>I created a GUI for local Speech-to-Text Transcription (OpenWhisper)</title>
    <updated>2025-11-24T05:09:34+00:00</updated>
    <author>
      <name>/u/MyFest</name>
      <uri>https://old.reddit.com/user/MyFest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p58g1e/i_created_a_gui_for_local_speechtotext/"&gt; &lt;img alt="I created a GUI for local Speech-to-Text Transcription (OpenWhisper)" src="https://external-preview.redd.it/lZVsndI7Xt6RmzC1xqsS3_szqkasDlqEt-MB4RSysT0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6d2bdc53e8785f300cc41a22a2c343be89ea4b4" title="I created a GUI for local Speech-to-Text Transcription (OpenWhisper)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of paying $10/month for SuperWhisper (which kept making transcription errors anyway), so I built my own 100% local speech-to-text app using OpenAI's Whisper. It's completely free, runs entirely on your machine with zero cloud dependencies, and actually transcribes better than SuperWhisper in my testing, especially for technical content. You can use it for live dictation to reduce typing strain, transcribe existing audio files, or quickly draft notes and blog posts. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DalasNoin/open_whisper"&gt;https://github.com/DalasNoin/open_whisper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MyFest"&gt; /u/MyFest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://simonlermen.substack.com/p/run-local-speech-to-text-transcription"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p58g1e/i_created_a_gui_for_local_speechtotext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p58g1e/i_created_a_gui_for_local_speechtotext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T05:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5d3j6</id>
    <title>I fine-tuned a model with GRPO + TRL + OpenEnv environment on Colab to play Wordle!</title>
    <updated>2025-11-24T09:57:07+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created a &lt;strong&gt;beginner-friendly notebook&lt;/strong&gt; (Colab) that walks you through training a model with &lt;strong&gt;reinforcement learning&lt;/strong&gt; using an &lt;strong&gt;OpenEnv environment&lt;/strong&gt; to play &lt;strong&gt;Wordle&lt;/strong&gt; üéÆ&lt;/p&gt; &lt;p&gt;The model is trained with &lt;strong&gt;TRL&lt;/strong&gt;, which now supports RL environments directly from &lt;strong&gt;OpenEnv&lt;/strong&gt;.&lt;br /&gt; For this example, I use the &lt;strong&gt;TextArena Wordle environment&lt;/strong&gt; and fine-tune the model with &lt;strong&gt;GRPO&lt;/strong&gt; (Group-Relative Preference Optimization).&lt;/p&gt; &lt;p&gt;Notebook on GitHub (can run on Colab):&lt;br /&gt; &lt;a href="https://github.com/huggingface/trl/blob/main/examples/notebooks/openenv_wordle_grpo.ipynb"&gt;https://github.com/huggingface/trl/blob/main/examples/notebooks/openenv_wordle_grpo.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're curious about RL, TRL, or OpenEnv, this is a great place to start.&lt;br /&gt; Happy learning! üåª&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/SergioPaniego/status/1992878809503412665/photo/1"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3j6/i_finetuned_a_model_with_grpo_trl_openenv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3j6/i_finetuned_a_model_with_grpo_trl_openenv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3j6/i_finetuned_a_model_with_grpo_trl_openenv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T09:57:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5d3uy</id>
    <title>Question...Mac Studio M2 Ultra 128GB RAM or second RTX 5090 Question | Help</title>
    <updated>2025-11-24T09:57:41+00:00</updated>
    <author>
      <name>/u/ajujox</name>
      <uri>https://old.reddit.com/user/ajujox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have a Ryzen 9 5900X with 64GB of RAM and a 5090. I do data science and have local LLMs for my daily work: Qwen 30b and Gemma 3 27b on Arch Linux. &lt;/p&gt; &lt;p&gt;I wanted to broaden my horizons and was looking at a Mac Studio M2 Ultra with 128GB of RAM to add more context and because it's a higher-quality model. But I'm wondering if I should buy a second 5090 and another PSU to handle both, but I think I'd only benefit from the extra RAM and not the extra power, plus it would generate more heat and consume more power for everyday use. I work mornings and afternoons. I tend to leave the PC on a lot. &lt;/p&gt; &lt;p&gt;I'm wondering if the M2 Ultra would be a better daily workstation and I could leave the PC for tasks with CUDA processing. I'm not sure if my budget would allow me to get an M3 Ultra (which I wouldn't be able to afford) or an M4 Max. &lt;/p&gt; &lt;p&gt;Any suggestions or similar experiences? What would you recommend for a 3k budget?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajujox"&gt; /u/ajujox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3uy/questionmac_studio_m2_ultra_128gb_ram_or_second/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3uy/questionmac_studio_m2_ultra_128gb_ram_or_second/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3uy/questionmac_studio_m2_ultra_128gb_ram_or_second/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T09:57:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5751y</id>
    <title>llama.cpp experiment with multi-turn thinking and real-time tool-result injection for instruct models</title>
    <updated>2025-11-24T04:00:56+00:00</updated>
    <author>
      <name>/u/butlan</name>
      <uri>https://old.reddit.com/user/butlan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran an experiment to see what happens when you stream tool call outputs into the model in real time. I tested with the Qwen/Qwen3-4B instruct model, should work on all non think models. With a detailed system prompt and live tool result injection, it seems the model is noticeably better at using multiple tools, and instruct models end up gaining a kind of lightweight ‚Äúvirtual thinking‚Äù ability. This improves performance on math and date-time related tasks.&lt;/p&gt; &lt;p&gt;If anyone wants to try, the tools are integrated directly into llama.cpp no extra setup required, but you need to use system prompt in the repo. &lt;/p&gt; &lt;p&gt;For testing, I only added math operations, time utilities, and a small memory component. Code mostly produced by gemini 3 there maybe logic errors but I'm not interested any further development on this :P&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cturan/llama.cpp/tree/toolinjection"&gt;code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5751y/video/2mydxgxch43g1/player"&gt;https://reddit.com/link/1p5751y/video/2mydxgxch43g1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/butlan"&gt; /u/butlan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5751y/llamacpp_experiment_with_multiturn_thinking_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5751y/llamacpp_experiment_with_multiturn_thinking_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5751y/llamacpp_experiment_with_multiturn_thinking_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T04:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p56jaa</id>
    <title>Introducing GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization | "GeoVista is a new 7B open-source agentic model that achieves SOTA performance in geolocalization by integrating visual tools and web search into an RL loop."</title>
    <updated>2025-11-24T03:30:47+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56jaa/introducing_geovista_webaugmented_agentic_visual/"&gt; &lt;img alt="Introducing GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization | &amp;quot;GeoVista is a new 7B open-source agentic model that achieves SOTA performance in geolocalization by integrating visual tools and web search into an RL loop.&amp;quot;" src="https://external-preview.redd.it/N2NhdWhncXpqNDNnMVwDDWZ-Avt25k7RB5p1ZYzJHn1VGkAqRgW2HjfpTSih.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2240fcbd44d6932da70db19d0fa076c13b359989" title="Introducing GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization | &amp;quot;GeoVista is a new 7B open-source agentic model that achieves SOTA performance in geolocalization by integrating visual tools and web search into an RL loop.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Abstract:&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocation task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. &lt;/p&gt; &lt;p&gt;Since existing geolocation benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocation ability of agentic models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information.&lt;/strong&gt; We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocation performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experimental results show that GeoVista surpasses other open-source agentic models on the geolocation task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Paper: &lt;a href="https://arxiv.org/pdf/2511.15705"&gt;https://arxiv.org/pdf/2511.15705&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the GitHub: &lt;a href="https://github.com/ekonwang/GeoVista"&gt;https://github.com/ekonwang/GeoVista&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;p&gt;Link to the HuggingFace: &lt;a href="https://huggingface.co/papers/2511.15705"&gt;https://huggingface.co/papers/2511.15705&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Project Page: &lt;a href="https://ekonwang.github.io/geo-vista/"&gt;https://ekonwang.github.io/geo-vista/&lt;/a&gt;&lt;/h4&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4hzcgppzj43g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56jaa/introducing_geovista_webaugmented_agentic_visual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p56jaa/introducing_geovista_webaugmented_agentic_visual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T03:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p58cai</id>
    <title>Recommend Coding model</title>
    <updated>2025-11-24T05:03:59+00:00</updated>
    <author>
      <name>/u/Small_Car6505</name>
      <uri>https://old.reddit.com/user/Small_Car6505</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Ryzen 7800x3D, 64Gb ram with RTX 5090 which model should I try. At the moment I have tried with llama.cpp with Qwen3-coder-30B-A3B-instruct-Bf16. Any other model is better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small_Car6505"&gt; /u/Small_Car6505 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p58cai/recommend_coding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p58cai/recommend_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p58cai/recommend_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T05:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4s7nt</id>
    <title>Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!</title>
    <updated>2025-11-23T17:17:31+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" src="https://external-preview.redd.it/QHcTMS_GK1SpPCsYVSA_d521aSr77tuQOduExaTV8io.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=260671c6b499b9df6965e93453782a12c31d98d9" title="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I have your attention, I'd like to ask: Does anyone here honestly bother with models below 12B? Like 8B, 4B, or 2B? I feel like I might have neglected smaller model sizes for far too long.&lt;/p&gt; &lt;p&gt;Also: &amp;quot;Air 4.6 in two weeks!&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Snowpiercer v4 is part of the Gen 4.0 series I'm working on that puts more focus on character adherence. YMMV. You might want to check out Gen 3.5/3.0 if Gen 4.0 isn't doing it for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TheDrummer/directory"&gt;https://huggingface.co/spaces/TheDrummer/directory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ln6s</id>
    <title>Computer Manufacturer threw my $ 20000 rig down the stairs and now says everything is fine</title>
    <updated>2025-11-23T12:34:18+00:00</updated>
    <author>
      <name>/u/phwlarxoc</name>
      <uri>https://old.reddit.com/user/phwlarxoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought a custom built Threadripper Pro water-cooled dual RTX 4090 workstation from a builder and had it updated a couple of times with new hardware so that finally it became a rig worth about $20000.&lt;/p&gt; &lt;p&gt;Upon picking up the machine last week from the builder after another upgrade I asked staff that we check together the upgrade before paying and confirming the order fulfilled.&lt;/p&gt; &lt;p&gt;They lifted the machine (still in its box and secured with two styrofoam blocks), on a table, but the heavy box (30kg) slipped from their hands, the box fell on the floor and from there down a staircase where it cartwheeled several times until it stopped at the end of the stairs.&lt;/p&gt; &lt;p&gt;They sent a mail saying they checked the machine and everything is fine.&lt;/p&gt; &lt;p&gt;Who wouldn't expect otherwise.&lt;/p&gt; &lt;p&gt;Can anyone comment on possible damages such an incident can have on the electronics, PCIe Slots, GPUs, watercooling, mainboard etc, ‚Äî also on what damages might have occurred that are not immediately evident, but could e.g. impact signal quality and therefore speed? Would you accept back such a machine?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phwlarxoc"&gt; /u/phwlarxoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T12:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4xscg</id>
    <title>Can an expert chime in and explain what is holding Vulkan back from becoming the standard API for ML?</title>
    <updated>2025-11-23T20:59:05+00:00</updated>
    <author>
      <name>/u/A_Chungus</name>
      <uri>https://old.reddit.com/user/A_Chungus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm just getting into GPGPU programming, and my knowledge is limited. I‚Äôve only written a handful of code and mostly just read examples. I‚Äôm trying to understand whether there are any major downsides or roadblocks to writing or contributing to AI/ML frameworks using Vulkan, or whether I should just stick to CUDA or others.&lt;/p&gt; &lt;p&gt;My understanding is that Vulkan is primarily a graphics-focused API, while CUDA, ROCm, and SYCL are more compute-oriented. However, Vulkan has recently been shown to match or even beat CUDA in performance in projects like llama.cpp. With features like &lt;a href="https://www.vulkan.org/user/pages/09.events/vulkanised-2025/T47-Jeff-Bolz-NVIDIA.pdf"&gt;Vulkan Cooperative Vectors&lt;/a&gt;, it seems it possible to squeeze the most performance out of the hardware and only limited by architecture tuning. The only times I see Vulkan lose to CUDA are in a few specific workloads on Linux or when the model exceeds VRAM. In those cases, Vulkan tends to fail or crash, while CUDA still finishes generation, although very slowly.&lt;/p&gt; &lt;p&gt;Since Vulkan can already reach this level of performance and is improving quickly, it seems like a serious contender to challenge CUDA‚Äôs moat and to offer true cross-vendor, cross-platform support unlike the rest. Even if Vulkan never fully matches CUDA‚Äôs performance in every framework, I can still see it becoming the default backend for many applications. For example, Electron dominates desktop development despite its sub-par performance because it makes cross-platform development so easy.&lt;/p&gt; &lt;p&gt;Setting aside companies‚Äô reluctance to invest in Vulkan as part of their AI/ML ecosystems in order to protect their proprietary platforms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are vendors actively doing anything to limit its capabilities?&lt;/li&gt; &lt;li&gt;Could we see more frameworks like &lt;a href="https://docs.pytorch.org/tutorials/unstable/vulkan_workflow.html"&gt;PyTorch&lt;/a&gt; adopting it and eventually making Vulkan a go-to cross-vendor solution?&lt;/li&gt; &lt;li&gt;If more contributions were made to Vulkan ecosystem, could it eventually reach the ecosystem that of CUDA has with libraries and tooling, or will Vulkan always be limited as a permanent ‚Äúsecond source‚Äù backend?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even with the current downsides, I don't think they‚Äôre significant enough to prevent Vulkan from gaining wider adoption in the AI/ML space. Could I be wrong here?&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;I guess what I'm really asking is if there are any CUDA/Vulkan devs that can provide some input on where they think Vulkan is lacking other than what I mentioned and if it its doable eventually to be feature parity with CUDA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A_Chungus"&gt; /u/A_Chungus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jyrv</id>
    <title>No way kimi gonna release new model !!</title>
    <updated>2025-11-23T10:57:51+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt; &lt;img alt="No way kimi gonna release new model !!" src="https://preview.redd.it/1ezldlbumz2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ad6c7d5d0f6a6e2b160c885a08a82d80d71ef81" title="No way kimi gonna release new model !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ezldlbumz2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5afo7</id>
    <title>My chatbot went rogue again‚Ä¶ I think it hates me lol</title>
    <updated>2025-11-24T07:05:17+00:00</updated>
    <author>
      <name>/u/Aggravating_Log9704</name>
      <uri>https://old.reddit.com/user/Aggravating_Log9704</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to fine-tune a bot for customer support but if users nudge it even slightly, it starts rambling conspiracy theories or making up company policies we never created.&lt;/p&gt; &lt;p&gt;I swear it behaves until one guy on the team tries something weird, then bam chaos.&lt;/p&gt; &lt;p&gt;How are y‚Äôall keeping your bots from acting like little internet feral gremlins&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggravating_Log9704"&gt; /u/Aggravating_Log9704 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5afo7/my_chatbot_went_rogue_again_i_think_it_hates_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5afo7/my_chatbot_went_rogue_again_i_think_it_hates_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5afo7/my_chatbot_went_rogue_again_i_think_it_hates_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T07:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p552bw</id>
    <title>[Update] Epstein Files dataset stays open and ungated on Hugging Face</title>
    <updated>2025-11-24T02:20:19+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you to everyone who provided feedback on our previous post. We agree with your comments - public data should stay public.&lt;/p&gt; &lt;p&gt;As for maintaining the data, we kindly request you to go through this &lt;a href="https://huggingface.co/blog/tensonaut/the-epstein-files"&gt;data usage article&lt;/a&gt; and contribute as volunteer in any way you can. Every small contribution is valuable - priority wise adding additional data from official sources while performing data integrity is of utmost importance&lt;/p&gt; &lt;p&gt;We're creating a central hub for all the investigative tools being built on this dataset. We already have 5 projects from this sub. If you are working on any tool to help journalists to search through the documents efficiently or share findings you've made, we request you to submit a PR &lt;a href="https://github.com/EF20K/Projects"&gt;here&lt;/a&gt; so we can update our documentation and have a central index of all the tools that journalists can use.&lt;/p&gt; &lt;p&gt;Thank you again to everyone who provided feedback and support. This dataset exists because of your feedbacks and suggestions, and we look forward to continuing to build this resource with this sub &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T02:20:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4t5ix</id>
    <title>I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!</title>
    <updated>2025-11-23T17:54:06+00:00</updated>
    <author>
      <name>/u/Inv1si</name>
      <uri>https://old.reddit.com/user/Inv1si</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt; &lt;img alt="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" src="https://external-preview.redd.it/YWZjazBjYjBwMTNnMfl_KE3bRLUmxUrgo6sq7iH5IJtc0qUYB-cQv58tKBaC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59e7a57c308ef96e98b98c0b7dd565ffc1a5aa0c" title="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inv1si"&gt; /u/Inv1si &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9r0ixbb0p13g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p56v22</id>
    <title>It been 2 years but why llama 3.1 8B still a popular choice to fine tune?</title>
    <updated>2025-11-24T03:47:07+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the model is so old now but new fine tuned model with this llama 3.1 8B as base still come out, do you think this trend will shift to olmo3 7B as a newer and more open ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T03:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p55dbq</id>
    <title>What‚Äôs the best High Parameter (100B+) Local LLM for NSFW RP?</title>
    <updated>2025-11-24T02:34:53+00:00</updated>
    <author>
      <name>/u/LyutsiferSafin</name>
      <uri>https://old.reddit.com/user/LyutsiferSafin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have about 400 GB GPU memory, what would be the best NSFW RP model I can try locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LyutsiferSafin"&gt; /u/LyutsiferSafin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T02:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5by1a</id>
    <title>Qwen3-Next support in llama.cpp almost ready!</title>
    <updated>2025-11-24T08:41:57+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"&gt; &lt;img alt="Qwen3-Next support in llama.cpp almost ready!" src="https://external-preview.redd.it/HHVAlPQ4eWe2-qdcom6wml40H6sWoSOnzo2PuJ4icH8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86d583e47273dca1af24d1e45d57a0f1544aa04c" title="Qwen3-Next support in llama.cpp almost ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/15940#issuecomment-3567006967"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T08:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
