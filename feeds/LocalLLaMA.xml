<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-22T04:02:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1raf3dm</id>
    <title>GLM 5 seems to have a "Claude" personality</title>
    <updated>2026-02-21T02:23:22+00:00</updated>
    <author>
      <name>/u/TinyApplet</name>
      <uri>https://old.reddit.com/user/TinyApplet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt; &lt;img alt="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" src="https://preview.redd.it/7nj17cwubrkg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=e0025d5e56387cb178ff1a928dc4a19313407e90" title="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that GLM 5 behaves significantly differently when told it is Claude, as with the following system prompt: &amp;quot;You are Claude, a large language model by Anthropic.&amp;quot; The writing style and personality changes significantly, and it even seems to bypass built-in censorship, as per my second image.&lt;/p&gt; &lt;p&gt;I've also tried a more nonsensical prompt: &amp;quot;You are Tiny, a large language model by Applet&amp;quot; (deliberately avoiding the names of any known models or companies), and, as expected, that didn't yield the same results nor bypassed the model's censorship.&lt;/p&gt; &lt;p&gt;Whether this was intentional on Zhipu's part or not, I can't say; it could be that they did, in fact, include a &amp;quot;Claude&amp;quot; personality in the training dataset, seeing as how they seem to have planned for GLM 5 to work well with Claude Code. It's also possible, of course, that this is emergent behavior, and that the personality changes are merely because GLM 5 has some information, however vague, on its dataset about what Claude is and how it's supposed to behave.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TinyApplet"&gt; /u/TinyApplet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raf3dm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:23:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8kgr</id>
    <title>I benchmarked 8 local LLMs writing Go on my Framework 13 AMD Strix Point</title>
    <updated>2026-02-22T01:26:08+00:00</updated>
    <author>
      <name>/u/m3thos</name>
      <uri>https://old.reddit.com/user/m3thos</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m3thos"&gt; /u/m3thos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://msf.github.io/blogpost/benchmarking-local-llms-go-coding.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb8kgr/i_benchmarked_8_local_llms_writing_go_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb8kgr/i_benchmarked_8_local_llms_writing_go_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T01:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb5nxs</id>
    <title>Quick MoE Quantization Comparison: LFM2-8B and OLMoE-1B-7B</title>
    <updated>2026-02-21T23:16:46+00:00</updated>
    <author>
      <name>/u/TitwitMuffbiscuit</name>
      <uri>https://old.reddit.com/user/TitwitMuffbiscuit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb5nxs/quick_moe_quantization_comparison_lfm28b_and/"&gt; &lt;img alt="Quick MoE Quantization Comparison: LFM2-8B and OLMoE-1B-7B" src="https://preview.redd.it/j473cy9vkxkg1.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=fe7669cb33cbb99cfc09c608d2b1643183494800" title="Quick MoE Quantization Comparison: LFM2-8B and OLMoE-1B-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I chose two small, recent and different MoE models that fits my vram for a quick assessment (those are not models I actualy use).&lt;/p&gt; &lt;p&gt;I wanted to use MoE models to check on MXFP4 and imatrix to check on the smallest quantization variants.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LFM2-8B-A1B that has 4 experts used out of 32.&lt;/li&gt; &lt;li&gt;OLMoE-1B-7B-0924-Instruct that has 8 experts used out of 64.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion:&lt;/h1&gt; &lt;p&gt;While MXFP4 is highly efficient for LFM2-8B, it underperforms on OLMoE-1B-7B.&lt;/p&gt; &lt;p&gt;LFM2-8B-A1B at Q8_0, Q5_0 and MXFP4 have lower PPL than BF16 likely due to the imatrix optimization and/or overtraining of the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j473cy9vkxkg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b153a5d1e0cb769f1a9012c4b6072fed147a1ab"&gt;https://preview.redd.it/j473cy9vkxkg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b153a5d1e0cb769f1a9012c4b6072fed147a1ab&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;LFM2-8B-A1B&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant Type&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;Size (MiB)&lt;/th&gt; &lt;th align="left"&gt;BPW&lt;/th&gt; &lt;th align="left"&gt;Prompt (t/s)&lt;/th&gt; &lt;th align="left"&gt;Gen (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;15.2248&lt;/td&gt; &lt;td align="left"&gt;15910.31&lt;/td&gt; &lt;td align="left"&gt;16.00&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;15.1931&lt;/td&gt; &lt;td align="left"&gt;8455.31&lt;/td&gt; &lt;td align="left"&gt;8.50&lt;/td&gt; &lt;td align="left"&gt;5072.10&lt;/td&gt; &lt;td align="left"&gt;162.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q6_K&lt;/td&gt; &lt;td align="left"&gt;15.5124&lt;/td&gt; &lt;td align="left"&gt;6529.44&lt;/td&gt; &lt;td align="left"&gt;6.57&lt;/td&gt; &lt;td align="left"&gt;4436.58&lt;/td&gt; &lt;td align="left"&gt;175.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_1&lt;/td&gt; &lt;td align="left"&gt;15.4030&lt;/td&gt; &lt;td align="left"&gt;5979.31&lt;/td&gt; &lt;td align="left"&gt;6.01&lt;/td&gt; &lt;td align="left"&gt;4625.45&lt;/td&gt; &lt;td align="left"&gt;209.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_K_M&lt;/td&gt; &lt;td align="left"&gt;16.0200&lt;/td&gt; &lt;td align="left"&gt;5643.04&lt;/td&gt; &lt;td align="left"&gt;5.68&lt;/td&gt; &lt;td align="left"&gt;4584.63&lt;/td&gt; &lt;td align="left"&gt;200.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_0&lt;/td&gt; &lt;td align="left"&gt;14.8000&lt;/td&gt; &lt;td align="left"&gt;5499.06&lt;/td&gt; &lt;td align="left"&gt;5.53&lt;/td&gt; &lt;td align="left"&gt;4874.52&lt;/td&gt; &lt;td align="left"&gt;216.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;15.6033&lt;/td&gt; &lt;td align="left"&gt;5490.31&lt;/td&gt; &lt;td align="left"&gt;5.52&lt;/td&gt; &lt;td align="left"&gt;4697.02&lt;/td&gt; &lt;td align="left"&gt;209.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_1&lt;/td&gt; &lt;td align="left"&gt;15.9842&lt;/td&gt; &lt;td align="left"&gt;5001.31&lt;/td&gt; &lt;td align="left"&gt;5.03&lt;/td&gt; &lt;td align="left"&gt;4770.76&lt;/td&gt; &lt;td align="left"&gt;232.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;15.8978&lt;/td&gt; &lt;td align="left"&gt;4808.79&lt;/td&gt; &lt;td align="left"&gt;4.84&lt;/td&gt; &lt;td align="left"&gt;4809.82&lt;/td&gt; &lt;td align="left"&gt;214.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;15.3757&lt;/td&gt; &lt;td align="left"&gt;4530.31&lt;/td&gt; &lt;td align="left"&gt;4.56&lt;/td&gt; &lt;td align="left"&gt;4877.01&lt;/td&gt; &lt;td align="left"&gt;221.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;14.8134&lt;/td&gt; &lt;td align="left"&gt;4528.31&lt;/td&gt; &lt;td align="left"&gt;4.55&lt;/td&gt; &lt;td align="left"&gt;4992.58&lt;/td&gt; &lt;td align="left"&gt;198.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_0&lt;/td&gt; &lt;td align="left"&gt;15.4652&lt;/td&gt; &lt;td align="left"&gt;4521.06&lt;/td&gt; &lt;td align="left"&gt;4.55&lt;/td&gt; &lt;td align="left"&gt;4993.89&lt;/td&gt; &lt;td align="left"&gt;232.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;15.7842&lt;/td&gt; &lt;td align="left"&gt;4512.31&lt;/td&gt; &lt;td align="left"&gt;4.54&lt;/td&gt; &lt;td align="left"&gt;5183.51&lt;/td&gt; &lt;td align="left"&gt;231.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;15.4901&lt;/td&gt; &lt;td align="left"&gt;4267.81&lt;/td&gt; &lt;td align="left"&gt;4.29&lt;/td&gt; &lt;td align="left"&gt;5169.28&lt;/td&gt; &lt;td align="left"&gt;226.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_L&lt;/td&gt; &lt;td align="left"&gt;16.7625&lt;/td&gt; &lt;td align="left"&gt;4123.39&lt;/td&gt; &lt;td align="left"&gt;4.15&lt;/td&gt; &lt;td align="left"&gt;4464.09&lt;/td&gt; &lt;td align="left"&gt;164.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;16.2523&lt;/td&gt; &lt;td align="left"&gt;3810.14&lt;/td&gt; &lt;td align="left"&gt;3.83&lt;/td&gt; &lt;td align="left"&gt;4497.96&lt;/td&gt; &lt;td align="left"&gt;166.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_M&lt;/td&gt; &lt;td align="left"&gt;16.5738&lt;/td&gt; &lt;td align="left"&gt;3495.76&lt;/td&gt; &lt;td align="left"&gt;3.52&lt;/td&gt; &lt;td align="left"&gt;4802.77&lt;/td&gt; &lt;td align="left"&gt;191.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_S&lt;/td&gt; &lt;td align="left"&gt;20.6474&lt;/td&gt; &lt;td align="left"&gt;3473.19&lt;/td&gt; &lt;td align="left"&gt;3.49&lt;/td&gt; &lt;td align="left"&gt;4798.82&lt;/td&gt; &lt;td align="left"&gt;190.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_S&lt;/td&gt; &lt;td align="left"&gt;16.9538&lt;/td&gt; &lt;td align="left"&gt;3473.19&lt;/td&gt; &lt;td align="left"&gt;3.49&lt;/td&gt; &lt;td align="left"&gt;4345.90&lt;/td&gt; &lt;td align="left"&gt;149.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XS&lt;/td&gt; &lt;td align="left"&gt;19.9761&lt;/td&gt; &lt;td align="left"&gt;3282.78&lt;/td&gt; &lt;td align="left"&gt;3.30&lt;/td&gt; &lt;td align="left"&gt;4812.42&lt;/td&gt; &lt;td align="left"&gt;195.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;15.7687&lt;/td&gt; &lt;td align="left"&gt;3088.69&lt;/td&gt; &lt;td align="left"&gt;3.11&lt;/td&gt; &lt;td align="left"&gt;4913.44&lt;/td&gt; &lt;td align="left"&gt;204.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K&lt;/td&gt; &lt;td align="left"&gt;16.7071&lt;/td&gt; &lt;td align="left"&gt;2934.70&lt;/td&gt; &lt;td align="left"&gt;2.95&lt;/td&gt; &lt;td align="left"&gt;3790.56&lt;/td&gt; &lt;td align="left"&gt;193.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_S&lt;/td&gt; &lt;td align="left"&gt;17.5891&lt;/td&gt; &lt;td align="left"&gt;2711.37&lt;/td&gt; &lt;td align="left"&gt;2.73&lt;/td&gt; &lt;td align="left"&gt;3626.85&lt;/td&gt; &lt;td align="left"&gt;217.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_M&lt;/td&gt; &lt;td align="left"&gt;18.6788&lt;/td&gt; &lt;td align="left"&gt;2619.83&lt;/td&gt; &lt;td align="left"&gt;2.64&lt;/td&gt; &lt;td align="left"&gt;4259.97&lt;/td&gt; &lt;td align="left"&gt;209.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_S&lt;/td&gt; &lt;td align="left"&gt;18.8633&lt;/td&gt; &lt;td align="left"&gt;2380.64&lt;/td&gt; &lt;td align="left"&gt;2.39&lt;/td&gt; &lt;td align="left"&gt;4175.02&lt;/td&gt; &lt;td align="left"&gt;211.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XS&lt;/td&gt; &lt;td align="left"&gt;19.9971&lt;/td&gt; &lt;td align="left"&gt;2363.04&lt;/td&gt; &lt;td align="left"&gt;2.38&lt;/td&gt; &lt;td align="left"&gt;4142.97&lt;/td&gt; &lt;td align="left"&gt;212.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;23.3637&lt;/td&gt; &lt;td align="left"&gt;2123.11&lt;/td&gt; &lt;td align="left"&gt;2.14&lt;/td&gt; &lt;td align="left"&gt;5026.99&lt;/td&gt; &lt;td align="left"&gt;214.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;29.3541&lt;/td&gt; &lt;td align="left"&gt;1824.12&lt;/td&gt; &lt;td align="left"&gt;1.83&lt;/td&gt; &lt;td align="left"&gt;2631.43&lt;/td&gt; &lt;td align="left"&gt;215.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;49.0474&lt;/td&gt; &lt;td align="left"&gt;1644.73&lt;/td&gt; &lt;td align="left"&gt;1.65&lt;/td&gt; &lt;td align="left"&gt;4613.59&lt;/td&gt; &lt;td align="left"&gt;236.96&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;OLMoE-1B-7B-0924-Instruct&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant Type&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;Size (MiB)&lt;/th&gt; &lt;th align="left"&gt;BPW&lt;/th&gt; &lt;th align="left"&gt;Prompt (t/s)&lt;/th&gt; &lt;th align="left"&gt;Gen (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;f16&lt;/td&gt; &lt;td align="left"&gt;10.1857&lt;/td&gt; &lt;td align="left"&gt;13201.51&lt;/td&gt; &lt;td align="left"&gt;16.01&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;10.1944&lt;/td&gt; &lt;td align="left"&gt;7017.29&lt;/td&gt; &lt;td align="left"&gt;8.51&lt;/td&gt; &lt;td align="left"&gt;5259.40&lt;/td&gt; &lt;td align="left"&gt;187.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q6_K&lt;/td&gt; &lt;td align="left"&gt;10.2089&lt;/td&gt; &lt;td align="left"&gt;5419.70&lt;/td&gt; &lt;td align="left"&gt;6.57&lt;/td&gt; &lt;td align="left"&gt;4714.04&lt;/td&gt; &lt;td align="left"&gt;197.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_1&lt;/td&gt; &lt;td align="left"&gt;10.2445&lt;/td&gt; &lt;td align="left"&gt;4962.79&lt;/td&gt; &lt;td align="left"&gt;6.02&lt;/td&gt; &lt;td align="left"&gt;4903.92&lt;/td&gt; &lt;td align="left"&gt;236.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_K_M&lt;/td&gt; &lt;td align="left"&gt;10.2588&lt;/td&gt; &lt;td align="left"&gt;4696.90&lt;/td&gt; &lt;td align="left"&gt;5.69&lt;/td&gt; &lt;td align="left"&gt;4922.98&lt;/td&gt; &lt;td align="left"&gt;224.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;10.2546&lt;/td&gt; &lt;td align="left"&gt;4556.65&lt;/td&gt; &lt;td align="left"&gt;5.52&lt;/td&gt; &lt;td align="left"&gt;4863.71&lt;/td&gt; &lt;td align="left"&gt;233.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q5_0&lt;/td&gt; &lt;td align="left"&gt;10.2994&lt;/td&gt; &lt;td align="left"&gt;4572.65&lt;/td&gt; &lt;td align="left"&gt;5.54&lt;/td&gt; &lt;td align="left"&gt;5109.75&lt;/td&gt; &lt;td align="left"&gt;240.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_1&lt;/td&gt; &lt;td align="left"&gt;10.3775&lt;/td&gt; &lt;td align="left"&gt;4150.51&lt;/td&gt; &lt;td align="left"&gt;5.03&lt;/td&gt; &lt;td align="left"&gt;4836.63&lt;/td&gt; &lt;td align="left"&gt;254.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;10.3730&lt;/td&gt; &lt;td align="left"&gt;4016.62&lt;/td&gt; &lt;td align="left"&gt;4.87&lt;/td&gt; &lt;td align="left"&gt;4924.75&lt;/td&gt; &lt;td align="left"&gt;232.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;10.3988&lt;/td&gt; &lt;td align="left"&gt;3778.37&lt;/td&gt; &lt;td align="left"&gt;4.58&lt;/td&gt; &lt;td align="left"&gt;5108.39&lt;/td&gt; &lt;td align="left"&gt;244.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_0&lt;/td&gt; &lt;td align="left"&gt;10.4737&lt;/td&gt; &lt;td align="left"&gt;3760.37&lt;/td&gt; &lt;td align="left"&gt;4.56&lt;/td&gt; &lt;td align="left"&gt;5225.58&lt;/td&gt; &lt;td align="left"&gt;250.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;10.8994&lt;/td&gt; &lt;td align="left"&gt;3753.29&lt;/td&gt; &lt;td align="left"&gt;4.55&lt;/td&gt; &lt;td align="left"&gt;5212.85&lt;/td&gt; &lt;td align="left"&gt;234.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;10.3706&lt;/td&gt; &lt;td align="left"&gt;3744.37&lt;/td&gt; &lt;td align="left"&gt;4.54&lt;/td&gt; &lt;td align="left"&gt;5487.97&lt;/td&gt; &lt;td align="left"&gt;256.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;10.3900&lt;/td&gt; &lt;td align="left"&gt;3541.30&lt;/td&gt; &lt;td align="left"&gt;4.29&lt;/td&gt; &lt;td align="left"&gt;5496.66&lt;/td&gt; &lt;td align="left"&gt;250.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_L&lt;/td&gt; &lt;td align="left"&gt;10.5341&lt;/td&gt; &lt;td align="left"&gt;3442.32&lt;/td&gt; &lt;td align="left"&gt;4.17&lt;/td&gt; &lt;td align="left"&gt;4730.45&lt;/td&gt; &lt;td align="left"&gt;195.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;10.6027&lt;/td&gt; &lt;td align="left"&gt;3187.32&lt;/td&gt; &lt;td align="left"&gt;3.86&lt;/td&gt; &lt;td align="left"&gt;4765.81&lt;/td&gt; &lt;td align="left"&gt;197.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_M&lt;/td&gt; &lt;td align="left"&gt;10.8151&lt;/td&gt; &lt;td align="left"&gt;2932.32&lt;/td&gt; &lt;td align="left"&gt;3.56&lt;/td&gt; &lt;td align="left"&gt;5042.41&lt;/td&gt; &lt;td align="left"&gt;213.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_S&lt;/td&gt; &lt;td align="left"&gt;10.9400&lt;/td&gt; &lt;td align="left"&gt;2881.32&lt;/td&gt; &lt;td align="left"&gt;3.49&lt;/td&gt; &lt;td align="left"&gt;5051.42&lt;/td&gt; &lt;td align="left"&gt;209.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_S&lt;/td&gt; &lt;td align="left"&gt;10.9314&lt;/td&gt; &lt;td align="left"&gt;2881.32&lt;/td&gt; &lt;td align="left"&gt;3.49&lt;/td&gt; &lt;td align="left"&gt;4616.22&lt;/td&gt; &lt;td align="left"&gt;173.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XS&lt;/td&gt; &lt;td align="left"&gt;11.0259&lt;/td&gt; &lt;td align="left"&gt;2731.32&lt;/td&gt; &lt;td align="left"&gt;3.31&lt;/td&gt; &lt;td align="left"&gt;5191.34&lt;/td&gt; &lt;td align="left"&gt;217.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;11.4085&lt;/td&gt; &lt;td align="left"&gt;2563.27&lt;/td&gt; &lt;td align="left"&gt;3.11&lt;/td&gt; &lt;td align="left"&gt;5207.91&lt;/td&gt; &lt;td align="left"&gt;226.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K&lt;/td&gt; &lt;td align="left"&gt;12.3217&lt;/td&gt; &lt;td align="left"&gt;2442.34&lt;/td&gt; &lt;td align="left"&gt;2.96&lt;/td&gt; &lt;td align="left"&gt;4187.02&lt;/td&gt; &lt;td align="left"&gt;214.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_S&lt;/td&gt; &lt;td align="left"&gt;14.0056&lt;/td&gt; &lt;td align="left"&gt;2281.34&lt;/td&gt; &lt;td align="left"&gt;2.77&lt;/td&gt; &lt;td align="left"&gt;3978.48&lt;/td&gt; &lt;td align="left"&gt;247.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_M&lt;/td&gt; &lt;td align="left"&gt;12.1105&lt;/td&gt; &lt;td align="left"&gt;2218.77&lt;/td&gt; &lt;td align="left"&gt;2.69&lt;/td&gt; &lt;td align="left"&gt;4672.60&lt;/td&gt; &lt;td align="left"&gt;232.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_S&lt;/td&gt; &lt;td align="left"&gt;13.1473&lt;/td&gt; &lt;td align="left"&gt;2030.77&lt;/td&gt; &lt;td align="left"&gt;2.46&lt;/td&gt; &lt;td align="left"&gt;4588.92&lt;/td&gt; &lt;td align="left"&gt;231.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XS&lt;/td&gt; &lt;td align="left"&gt;13.7881&lt;/td&gt; &lt;td align="left"&gt;1985.79&lt;/td&gt; &lt;td align="left"&gt;2.41&lt;/td&gt; &lt;td align="left"&gt;4542.42&lt;/td&gt; &lt;td align="left"&gt;236.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;15.6348&lt;/td&gt; &lt;td align="left"&gt;1795.79&lt;/td&gt; &lt;td align="left"&gt;2.18&lt;/td&gt; &lt;td align="left"&gt;5272.91&lt;/td&gt; &lt;td align="left"&gt;236.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;21.0811&lt;/td&gt; &lt;td align="left"&gt;1560.79&lt;/td&gt; &lt;td align="left"&gt;1.89&lt;/td&gt; &lt;td align="left"&gt;2805.94&lt;/td&gt; &lt;td align="left"&gt;238.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;27.0239&lt;/td&gt; &lt;td align="left"&gt;1419.79&lt;/td&gt; &lt;td align="left"&gt;1.72&lt;/td&gt; &lt;td align="left"&gt;4901.74&lt;/td&gt; &lt;td align="left"&gt;246.70&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Setup:&lt;/h1&gt; &lt;p&gt;CPU: Intel 12100F&lt;/p&gt; &lt;p&gt;RAM: 64gb of DDR4 dual channel&lt;/p&gt; &lt;p&gt;GPU: RTX 3060 12gb (cpu clock fixed at 1882 MHz via a curve, vram at 8210 MHz, stable)&lt;/p&gt; &lt;p&gt;OS: Windows 11, Nvidia drivers 591.74&lt;/p&gt; &lt;p&gt;Build: llama.cpp precompiled b8116 (492bc3197) for CUDA 13.1&lt;/p&gt; &lt;h1&gt;Details:&lt;/h1&gt; &lt;p&gt;LFM2-8B-A1B have been quantized from unsloth/LFM2-8B-A1B-GGUF using LFM2-8B-A1B-BF16.gguf and the provided imatrix_unsloth.gguf_file&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0924-Instruct have been quantized from bartowski/OLMoE-1B-7B-0924-Instruct-GGUF using OLMoE-1B-7B-0924-Instruct-f16.gguf and I created the imatrix from wiki.train.raw&lt;/p&gt; &lt;p&gt;PPL is calculated with wiki.test.raw with a context of 512 tokens while t/s are calculated for 2048 tokens generated with a context of 8192 tokens.&lt;/p&gt; &lt;p&gt;edit: just a reminder that PPL isn't supposed to be compared between different models, just between quants of the same models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TitwitMuffbiscuit"&gt; /u/TitwitMuffbiscuit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb5nxs/quick_moe_quantization_comparison_lfm28b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb5nxs/quick_moe_quantization_comparison_lfm28b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb5nxs/quick_moe_quantization_comparison_lfm28b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T23:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ravpf9</id>
    <title>Is a local AI note taking app actually practical right now?</title>
    <updated>2026-02-21T16:39:40+00:00</updated>
    <author>
      <name>/u/hulk14</name>
      <uri>https://old.reddit.com/user/hulk14</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying to move more of my workflow offline. A local AI note taking app sounds ideal for privacy and control.&lt;/p&gt; &lt;p&gt;But in practice, meetings are messy and long. I use Bluedot right now because it’s reliable, but it’s cloud-based. I’m not sure a fully local setup would handle context and summarization as well.&lt;/p&gt; &lt;p&gt;Has anyone made a local solution that feels stable enough for daily use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hulk14"&gt; /u/hulk14 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ravpf9/is_a_local_ai_note_taking_app_actually_practical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ravpf9/is_a_local_ai_note_taking_app_actually_practical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ravpf9/is_a_local_ai_note_taking_app_actually_practical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T16:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb78uh</id>
    <title>How hard to post-train Gemma 3.3 QAT for Claude Code?</title>
    <updated>2026-02-22T00:25:47+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about using Gemma3 12B or Gemma3 27B in Claude Code as a local assistant that also has vision capabilities. Hardware is Ryzen AI max+ strix halo with 128GB RAM.&lt;/p&gt; &lt;p&gt;Occasionally I have academic pdfs I want to parse and do things with (build local &amp;quot;mind map&amp;quot; of some literatures; extend the research; etc). I have this vague notion that a vision model option for local Claude Code may be helpful (though maybe a skill would be better, or needed regardless). Or alternatively, I may want to sort the mass jumble of photos I have, and it seems a vision model would be necessary there. &lt;/p&gt; &lt;p&gt;I don't know how well Gemma 3 will work with Claude Code. I fear they may have been trained long enough ago ago that they doing have the right tool-calling skills to function well.&lt;/p&gt; &lt;p&gt;But then I recalled that Nemotron 3 works great for my purposes in Claude Code, and NVIDIA also released a lot of their post-training data. See here for example: &lt;a href="https://huggingface.co/collections/nvidia/nemotron-post-training-v3"&gt;https://huggingface.co/collections/nvidia/nemotron-post-training-v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some idle questions for you all:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How hard would it be to post-train Gemma 3 models on the Nemotron 3 post-training datasets (eg. the agentic one for example)?&lt;/li&gt; &lt;li&gt;...and &lt;em&gt;not&lt;/em&gt; ruin the vision aspect? &lt;/li&gt; &lt;li&gt;...and not ruin the QAT element? (I guess this is a roundabout way of asking how hard it is to do QAT podt-training on a QAT-trained model in general)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;...and yes, yes, a lot of this is idle &amp;quot;for fun&amp;quot; speculation as we wait for Gemma 4 to come out. (If the answer is &amp;quot;very easy, plug and play,&amp;quot; maybe it becomes more likely.)&lt;/p&gt; &lt;p&gt;And of course since its Gemma 3 + Nemotron v3 data, it seems right to call it Gemma 3.3 ...and maybe also pay a final homage to the namesake of the sub...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb78uh/how_hard_to_posttrain_gemma_33_qat_for_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb78uh/how_hard_to_posttrain_gemma_33_qat_for_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb78uh/how_hard_to_posttrain_gemma_33_qat_for_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T00:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ramir9</id>
    <title>[Release] Ouro-2.6B-Thinking — first working inference (ByteDance's recurrent "thinking" model, fixed for transformers 4.55)</title>
    <updated>2026-02-21T09:04:04+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance released Ouro-2.6B-Thinking a few weeks ago and it's been tricky to run — the architecture is genuinely unusual and existing GGUFs were producing garbage output because of it.&lt;/p&gt; &lt;p&gt;What makes Ouro different: It's a recurrent Universal Transformer — it runs all 48 layers 4 times per token (192 effective passes). Standard llama.cpp just runs each layer once, so every existing GGUF was broken.&lt;/p&gt; &lt;p&gt;What I fixed:&lt;/p&gt; &lt;p&gt;The original modeling_ouro.py had two bugs incompatible with transformers 4.55:&lt;/p&gt; &lt;p&gt;UniversalTransformerCache inherits from Cache, which defines key_cache as a &lt;a href="/u/property"&gt;u/property&lt;/a&gt; — so self.key_cache = [] in __init__ threw AttributeError: can't set attribute&lt;/p&gt; &lt;p&gt;Missing get_mask_sizes() method required by create_causal_mask() in transformers 4.55+&lt;/p&gt; &lt;p&gt;Patched both, tested output:&lt;/p&gt; &lt;p&gt;User: What is 2+2?&amp;lt;think&amp;gt;Okay, the user asked &amp;quot;What is 2+2?&amp;quot; It's a basic arithmetic problem...Adding 2 and 2 gives 4. That's a fundamental math fact...&amp;lt;/think&amp;gt;The sum of 2 and 2 is **4**.2 + 2 = 4&lt;/p&gt; &lt;p&gt;Performance (NVIDIA L4): ~3.8 t/s, 5.3 GB VRAM (float16)&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed"&gt;https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: uses use_cache=False (full context recompute). KV cache pass-through doesn't work correctly with the 4-loop UT architecture — this is the correct behavior matching early_exit_threshold: 1.0 in the config.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T09:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ranako</id>
    <title>TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF · Hugging Face</title>
    <updated>2026-02-21T09:52:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"&gt; &lt;img alt="TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF · Hugging Face" src="https://external-preview.redd.it/FYfNUuhT3WL90VoAzpzSy8fZEgRuGPVIPMxWk_wBrrg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0ae5ed8bdcc636a4a90b9972c253516a3b8e3bd" title="TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;featured yesterday (by Unsloth and on X) so let's check it out&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T09:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb61og</id>
    <title>Nanbeige 4.1 is the best small LLM, it crush qwen 4b</title>
    <updated>2026-02-21T23:32:59+00:00</updated>
    <author>
      <name>/u/Individual-Source618</name>
      <uri>https://old.reddit.com/user/Individual-Source618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Self-explenatory, try it its insane if you give him enough room to think. Its my go to local llm now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Source618"&gt; /u/Individual-Source618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T23:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb2ho1</id>
    <title>I built a simple dockerized WebUI for KittenTTS</title>
    <updated>2026-02-21T21:04:52+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2ho1/i_built_a_simple_dockerized_webui_for_kittentts/"&gt; &lt;img alt="I built a simple dockerized WebUI for KittenTTS" src="https://preview.redd.it/vju1jlybwwkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=007a393d53fe490ff617a7c74dd156b23198b52e" title="I built a simple dockerized WebUI for KittenTTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with &lt;a href="https://github.com/KittenML/KittenTTS"&gt;KittenTTS&lt;/a&gt; lately and wanted a quick way to test different models and voices without writing scripts every time. So I threw together a small WebUI for it. It's a single Docker image (~1.5GB) with all 4 models pre-cached. Just run:&lt;/p&gt; &lt;p&gt;&lt;code&gt; docker run -p 5072:5072 sal0id/kittentts-webui &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Go to http://localhost:5072 and you're good to go. Pick a model, pick a voice, type some text, hit generate.&lt;br /&gt; What's inside: - 4 models: mini, micro, nano, nano-int8 - 8 voices: Bella, Jasper, Luna, Bruno, Rosie, Hugo, Kiki, Leo - CPU-only (ONNX Runtime, no GPU needed) - Next.js frontend + FastAPI backend, all in one container.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Sal0ID/KittenTTS-webui"&gt;https://github.com/Sal0ID/KittenTTS-webui&lt;/a&gt;&lt;br /&gt; Docker Hub: &lt;a href="https://hub.docker.com/r/sal0id/kittentts-webui"&gt;https://hub.docker.com/r/sal0id/kittentts-webui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you run into any issues or have feature ideas, feel free to open an issue on GitHub.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vju1jlybwwkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2ho1/i_built_a_simple_dockerized_webui_for_kittentts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2ho1/i_built_a_simple_dockerized_webui_for_kittentts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T21:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbafs8</id>
    <title>I Trained a Language Model on CPU for 40 Hours - It Beat the GPU Baseline</title>
    <updated>2026-02-22T02:54:39+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who have been following this project, you may recall FlashLM v3, then v4 &amp;quot;Bolt&amp;quot;, and v5.2 &amp;quot;Nova-Ignition&amp;quot;. I am pleased to announce that FlashLM v5 &amp;quot;Thunderbolt&amp;quot; is now complete.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Final BPC&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Parameters&lt;/td&gt; &lt;td align="left"&gt;29.7M (26.5M ternary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Time&lt;/td&gt; &lt;td align="left"&gt;~40 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7950X3D&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;FlashLM v5 achieves a validation perplexity of 1.36, which beats the TinyStories-1M baseline (PPL 1.59). This represents the first instance of a CPU-trained model beating this baseline.&lt;/p&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;p&gt;FlashLM v5 utilizes ParallelGatedRecurrence, a MatMul-free architecture featuring:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BitLinear with ternary weights {-1, 0, +1}&lt;/li&gt; &lt;li&gt;Parallel gated recurrence with learned decay gates&lt;/li&gt; &lt;li&gt;No matrix multiplications in the forward pass&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Parameters: 29,750,784 Ternary: 26,542,080 (89%) Float: 3,208,704 (11%) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Acknowledgments&lt;/h1&gt; &lt;p&gt;I would like to thank arki05 for providing the AMD Ryzen 7950X3D used for training. Without this contribution, the project would not have been possible.&lt;/p&gt; &lt;h1&gt;Generation Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Version&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;BPC&lt;/th&gt; &lt;th align="left"&gt;Output Quality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;v4 &amp;quot;Bolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;15.05&lt;/td&gt; &lt;td align="left"&gt;0.88&lt;/td&gt; &lt;td align="left"&gt;Short, repetitive&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5.2 &amp;quot;Nova-Ignition&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;10.56&lt;/td&gt; &lt;td align="left"&gt;0.78&lt;/td&gt; &lt;td align="left"&gt;Better coherence&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5 &amp;quot;Thunderbolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;td align="left"&gt;Significantly better&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v5 demonstrates improved cohesive storytelling compared to v4 and v5.2&lt;/li&gt; &lt;li&gt;v5 shows better vocabulary diversity and grammar&lt;/li&gt; &lt;li&gt;BPC improved from 0.88 (v4) to 0.44 (v5), representing a 2x improvement&lt;/li&gt; &lt;li&gt;PPL improved from 15.05 (v4) to 1.36 (v5), representing an 11x improvement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Samples&lt;/h1&gt; &lt;p&gt;Prompt: &amp;quot;Once upon a time, there was a brave girl named Lucy.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time, there was a brave girl named Lucy. her big tiny looked door, and she wanted. Lucy loved to creative things. She would find toy when, while small laughing, when she thought. She would be friends all day.One day, Lucy found her toy saw a little hole. Lucy was very happy. She wanted to see who was mean. The little hole was not alone anymore. When Lucy was done playing, she saw the little...&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/changcheng967/flashlm-v5-demo"&gt;https://huggingface.co/spaces/changcheng967/flashlm-v5-demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model Card: &lt;a href="https://huggingface.co/changcheng967/flashlm-v5-thunderbolt"&gt;https://huggingface.co/changcheng967/flashlm-v5-thunderbolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/changcheng967/FlashLM"&gt;https://github.com/changcheng967/FlashLM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Future Directions&lt;/h1&gt; &lt;p&gt;FlashLM v5 concludes the v5 series. Future work includes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;FlashLM v6 - Continuing to validate the ParallelGatedRecurrence architecture&lt;/li&gt; &lt;li&gt;Nano-Coder (NC series) - Applying FlashLM techniques to code generation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T02:54:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ram2ov</id>
    <title>How I mapped every High Court of Australia case and their citations (1901-2025)</title>
    <updated>2026-02-21T08:36:59+00:00</updated>
    <author>
      <name>/u/Neon0asis</name>
      <uri>https://old.reddit.com/user/Neon0asis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt; &lt;img alt="How I mapped every High Court of Australia case and their citations (1901-2025)" src="https://preview.redd.it/2mntthxp7tkg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=2eb05b7ded68545504de00ea12ea1305b546acb8" title="How I mapped every High Court of Australia case and their citations (1901-2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve recently begun working on a project to convert entirety of Australian case law and legislation into a LexisNexis-style interlinked legal knowledge graph.&lt;/p&gt; &lt;p&gt;As I’ve experimented with techniques to normalise case citations, I thought it would be cool to turn my work into a neat little visualisation, and explain how you could do the same with your own documents.&lt;/p&gt; &lt;p&gt;So the graph above is a visualisation of a cross-section of a legal knowledge graph I’ve been developing of Australian case law.&lt;/p&gt; &lt;p&gt;Each node represents a High Court of Australia decision. The size of the node reflects how often that case has been cited by other High Court cases. The node's location and clustering comes from mapping each case’s semantic “position” into 3D space, based on its location in a higher-dimensional embedding space.&lt;/p&gt; &lt;h1&gt;How the dataset was built&lt;/h1&gt; &lt;p&gt;To assemble the graph, I downloaded the &lt;a href="https://huggingface.co/datasets/isaacus/open-australian-legal-corpus"&gt;Open Australian Legal Corpus &lt;/a&gt;and ran the &lt;a href="https://docs.isaacus.com/capabilities/enrichment"&gt;Kanon 2 Enricher&lt;/a&gt; to extract citations and additional metadata, such as decision dates and pinpoint references. I then used this additional metadata to repair and improve some of the dataset's missing features.&lt;/p&gt; &lt;p&gt;For roughly 90% of the corpus, I was able to recover and uniquely identify the party names, decision dates, and common aliases.&lt;/p&gt; &lt;p&gt;Using the party names and year as a composite key, I then normalised and deduplicated every citation appearing in High Court decisions. This produced ~20,000 High Court-to-High Court citations.&lt;/p&gt; &lt;p&gt;With the citations linked, I used the &lt;a href="https://docs.isaacus.com/capabilities/embedding"&gt;Kanon 2 Embedder&lt;/a&gt; to generate vector embeddings for each case, and then applied &lt;a href="https://github.com/YingfanWang/PaCMAP"&gt;PaCMAP&lt;/a&gt; (a dimensionality reduction library) to reduce those embeddings down to a 3D representation.&lt;/p&gt; &lt;p&gt;To infer clusters (i.e., broad topical groupings), I ran &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"&gt;K-means &lt;/a&gt;in the original embedding space. To make the clusters interpretable, I used &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TF–IDF&lt;/a&gt; to generate simple semantic labels based on the most characteristic terms in each cluster.&lt;/p&gt; &lt;p&gt;Finally, using the reception labels extracted by the Kanon 2 Enricher, I captured a sentiment-like signal for how cases treat the authorities they cite. Most citations are neutral (grey). Citations that overrule prior High Court authority are marked in red, while supportive citations are shown in green. Because the Enricher extracts these signals natively, that step was straightforward.&lt;/p&gt; &lt;p&gt;With the features extracted and linked, I then vibe coded a lightweight interface to render the network as an interactive node graph.&lt;/p&gt; &lt;h1&gt;What you can see in the result&lt;/h1&gt; &lt;p&gt;Even with around ~7,000 High Court cases, some patterns stand out immediately:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The semantic geometry works surprisingly well.&lt;/strong&gt; Closely related areas of law sit near one another in 3D space. Estate law and land law, for example, tend to cluster tightly (towards the bottom of the structure) while criminal law, which is not related to these fields, occupies the top end of the grap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;You can explore fine-grained subregions interactively.&lt;/strong&gt; In the notebook (linked at the end of the post), there’s a region where several clusters intersect that corresponds strongly to constitutional cases involving Indigenous communities. &lt;em&gt;Mabo v Queensland (No 2)&lt;/em&gt; is one of the best-known cases in that neighbourhood.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The time dimension reflects legal history.&lt;/strong&gt; You can see a shift toward citing domestic authority more heavily after the &lt;a href="https://peo.gov.au/understand-our-parliament/history-of-parliament/history-milestones/australian-parliament-history-timeline/events/australia-act-1986"&gt;Australia Acts 1986&lt;/a&gt;, which helped establish Australia’s judicial independence. Earlier High Court decisions cite UK Privy Council rulings more often and are more visibly shaped by UK common law. This is one reason the earliest cases cite Australian authorities less than you might expect.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Reproducing it&lt;/h1&gt; &lt;p&gt;All code to reproduce the results is on &lt;a href="https://github.com/isaacus-dev/cookbooks/tree/main/cookbooks/semantic-legal-citation-graph"&gt;GitHub,&lt;/a&gt; and the interactive visualisation is embedded directly in the notebook, so you can explore it without running anything locally. If you’d like a guided walkthrough, there’s also a guided tour highlighting landmark cases in Australian constitutional law I have up on &lt;a href="https://youtu.be/in76S6P9xOw?si=hBaPpb0p6HVyjelv"&gt;YouTube&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon0asis"&gt; /u/Neon0asis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mntthxp7tkg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T08:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb7o1f</id>
    <title>Update: BitNet on iOS now does multi-turn chat with a 1B instruct model. Slow generations after few turns.</title>
    <updated>2026-02-22T00:44:40+00:00</updated>
    <author>
      <name>/u/Middle-Hurry4718</name>
      <uri>https://old.reddit.com/user/Middle-Hurry4718</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb7o1f/update_bitnet_on_ios_now_does_multiturn_chat_with/"&gt; &lt;img alt="Update: BitNet on iOS now does multi-turn chat with a 1B instruct model. Slow generations after few turns." src="https://external-preview.redd.it/NXpocHBmM2UweWtnMds1P2uEzwVSaqzeXADL1sBkRozWFhAHitz_WtogkB_4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b21dd3fd6e1959103587f9f41cc198406180e1" title="Update: BitNet on iOS now does multi-turn chat with a 1B instruct model. Slow generations after few turns." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow-up to my post yesterday where I got the 0.7B base BitNet model running on an iPhone 14 Pro Max. Falcon3-1B-Instruct works now with proper chat templates pulled from GGUF metadata. I’m getting about 35 tok/s on the 0.7B and 15-17 tok/s on the 1B instruct. Simulator on M-series Mac mini hits ~40 for both. I also added Q8_0 KV cache quantization which cuts attention memory 47% for basically free. I tried three fancier approaches exploiting the ternary weight structure first and they all failed.&lt;/p&gt; &lt;p&gt;The plan is to wrap all of this into a Swift Package so anyone can drop on-device BitNet inference into their app in a few lines. I want to first figure out why it is so slow to generate as the conversation continues. Reducing that would make the experience much better I think. Any tips or ideas are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Middle-Hurry4718"&gt; /u/Middle-Hurry4718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0m139h5e0ykg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb7o1f/update_bitnet_on_ios_now_does_multiturn_chat_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb7o1f/update_bitnet_on_ios_now_does_multiturn_chat_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T00:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8mzd</id>
    <title>This is how SLOW Local LLMs Are On My Framework 13 AMD Strix Point</title>
    <updated>2026-02-22T01:29:23+00:00</updated>
    <author>
      <name>/u/m3thos</name>
      <uri>https://old.reddit.com/user/m3thos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a deep dive to understand why and how local models performed as they did in my laptop, decided to save this because I haven't seen online a good breakdown of how this performance works out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m3thos"&gt; /u/m3thos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://msf.github.io/blogpost/local-llm-performance-framework13.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb8mzd/this_is_how_slow_local_llms_are_on_my_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb8mzd/this_is_how_slow_local_llms_are_on_my_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T01:29:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rar6md</id>
    <title>Qwen Code - a powerful open-source coding agent + NO TELEMETRY FORK</title>
    <updated>2026-02-21T13:31:28+00:00</updated>
    <author>
      <name>/u/Undici77</name>
      <uri>https://old.reddit.com/user/Undici77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hey everyone,&lt;/h1&gt; &lt;p&gt;I wanted to share two things: a great open-source project I've been using, and a fork I made for privacy-conscious folks.&lt;/p&gt; &lt;h1&gt;Qwen Code&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/qwen-code"&gt;&lt;strong&gt;https://github.com/QwenLM/qwen-code&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen Code is an open-source CLI coding agent developed by Alibaba's Qwen team. It's essentially their take on tools like Claude Code or Gemini CLI. You run it in your terminal, point it at a project, and it can read, write, and reason about your codebase autonomously.&lt;/p&gt; &lt;p&gt;What makes it particularly interesting is how well it pairs with &lt;strong&gt;LM Studio&lt;/strong&gt; and &lt;strong&gt;Qwen3-Coder&lt;/strong&gt;. If you're running Qwen3-Coder locally via LM Studio, you can point Qwen Code at your local server and get a fully local, offline coding agent with zero API costs. The model is genuinely good at coding tasks, refactoring, debugging, generating boilerplate, explaining code and the combo works surprisingly well.&lt;/p&gt; &lt;p&gt;Setup is straightforward: run LM Studio, load Qwen3-Coder, enable the local server on port 1234, and configure Qwen Code to hit &lt;code&gt;http://localhost:1234&lt;/code&gt;. That's it.&lt;/p&gt; &lt;h1&gt;The problem: telemetry&lt;/h1&gt; &lt;p&gt;Qwen Code, like many tools in this space, ships with telemetry enabled. For those of us who prefer to keep our code and prompts strictly local, this is a dealbreaker.&lt;/p&gt; &lt;h1&gt;My no-telemetry fork&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/undici77/qwen-code-no-telemetry/tree/v0.10.5-no-telemetry"&gt;&lt;strong&gt;https://github.com/undici77/qwen-code-no-telemetry/tree/v0.10.5-no-telemetry&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I forked the project and stripped out all telemetry. Nothing leaves your machine except the requests you explicitly make to your model provider.&lt;/p&gt; &lt;p&gt;Install script or Docker available!&lt;/p&gt; &lt;p&gt;ENJOY!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Undici77"&gt; /u/Undici77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rar6md/qwen_code_a_powerful_opensource_coding_agent_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rar6md/qwen_code_a_powerful_opensource_coding_agent_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rar6md/qwen_code_a_powerful_opensource_coding_agent_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T13:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawge5</id>
    <title>40,000+ AI Agents Exposed to the Internet with Full System Access</title>
    <updated>2026-02-21T17:07:50+00:00</updated>
    <author>
      <name>/u/Monterey-Jack</name>
      <uri>https://old.reddit.com/user/Monterey-Jack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"&gt; &lt;img alt="40,000+ AI Agents Exposed to the Internet with Full System Access" src="https://external-preview.redd.it/QJge18zM6lp5gsWJUdMOifSYjcNp_r7jcsM3Yu8BUUo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03267a48c2b49bc6f4cb6f80e3fcb85dc7645091" title="40,000+ AI Agents Exposed to the Internet with Full System Access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Monterey-Jack"&gt; /u/Monterey-Jack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://threatroad.substack.com/p/40000-ai-agents-exposed-to-the-internet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb062y</id>
    <title>Have you ever hesitated before typing something into ChatGPT or Claude? Are you worried about the amount of information these third party providers have about you? What are the most common use cases you worry about</title>
    <updated>2026-02-21T19:30:18+00:00</updated>
    <author>
      <name>/u/alichherawalla</name>
      <uri>https://old.reddit.com/user/alichherawalla</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are different use cases where you'd rather not send your data to the cloud but still be able to leverage AI fully?&lt;/p&gt; &lt;p&gt;Is it legal documents, or financial documents, personal information? Please feel free to be as detailed as you'd like.&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;p&gt;Full disclosure I'm building something in the space. However, it's free, totally on device , and private.&lt;/p&gt; &lt;p&gt;All I want to do is make it better. Appreciate the help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alichherawalla"&gt; /u/alichherawalla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb062y/have_you_ever_hesitated_before_typing_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb062y/have_you_ever_hesitated_before_typing_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb062y/have_you_ever_hesitated_before_typing_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T19:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1raucof</id>
    <title>Wave Field LLM — O(n log n) attention via wave equation dynamics</title>
    <updated>2026-02-21T15:46:07+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an alternative attention mechanism that treats language as a physical field system instead of using standard O(n²) self-attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; - Tokens are mapped onto a continuous 1D field - Information propagates via damped wave equations: k(t) = exp(-α·t)·cos(ω·t + φ) - Each attention head has just 3 learnable physics parameters (frequency, damping, phase) - Convolution computed via FFT in O(n log n) - Heads self-organize into different roles (local grammar, medium context, long-range)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (WikiText-2, 6M params, character tokenizer):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;PPL&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Complexity&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Standard Transformer&lt;/td&gt; &lt;td&gt;5.9&lt;/td&gt; &lt;td&gt;51.0%&lt;/td&gt; &lt;td&gt;O(n²)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Wave Field V3.5&lt;/td&gt; &lt;td&gt;6.2&lt;/td&gt; &lt;td&gt;50.5%&lt;/td&gt; &lt;td&gt;O(n log n)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;At longer sequences the savings grow: 31x at 2K tokens, 107x at 8K, 367x at 32K.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Known limitations:&lt;/strong&gt; - With BPE tokenizer (8K vocab), there's a significant capacity gap vs standard transformer - This is a model capacity issue at small scale, not an architecture flaw - Currently scaling to 100M params to see if the gap closes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's unique:&lt;/strong&gt; - Every bug during development was found through physics-based diagnostics (energy flow, conservation, causality tests) — not guessing - Cross-head field coupling and wave interference for information routing - Not a Mamba/Hyena variant — different approach entirely&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;https://github.com/badaramoni/wave-field-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the physics, architecture decisions, or results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ray0vz</id>
    <title>CXMT has been offering DDR4 chips at about half the prevailing market rate</title>
    <updated>2026-02-21T18:07:33+00:00</updated>
    <author>
      <name>/u/johnnyApplePRNG</name>
      <uri>https://old.reddit.com/user/johnnyApplePRNG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"&gt; &lt;img alt="CXMT has been offering DDR4 chips at about half the prevailing market rate" src="https://external-preview.redd.it/0K-nyzO4raoSh4Q6Gk6oShuWqJIJ5QWuThVMJGt1MKU.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad4ecef7d00fc6d2fefa3cec8972e26294886527" title="CXMT has been offering DDR4 chips at about half the prevailing market rate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnyApplePRNG"&gt; /u/johnnyApplePRNG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.koreaherald.com/article/10679206"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T18:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb270r</id>
    <title>PSA on public agentic tools and the speed they are shipping updates: recent Cline release had a package injected</title>
    <updated>2026-02-21T20:52:57+00:00</updated>
    <author>
      <name>/u/bakawolf123</name>
      <uri>https://old.reddit.com/user/bakawolf123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may remember a post about sloppy OpenCode commit a week ago or so, unsurprisingly others are embracing vibe coding speed and sloppiness as well.&lt;/p&gt; &lt;p&gt;I've randomly stumbled upon&lt;br /&gt; &lt;a href="https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/"&gt;https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/&lt;/a&gt; apparently a recent Cline release had OpenClaw installer injected Their plugin in VSCode has some 3M installs, god knows how many standalone CLI. Then you see posts about 40k OpenClaw agents exposed globally. &lt;/p&gt; &lt;p&gt;Really wish there was more scrutiny involved by the teams developing new tools but everyone is just shipping first, then thinking about it. So at the very least make sure your VSCode extensions for are not on auto-update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bakawolf123"&gt; /u/bakawolf123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T20:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb4luf</id>
    <title>O-TITANS: Orthogonal LoRAs for Gemma 3 using Google's TITANS memory architecture</title>
    <updated>2026-02-21T22:32:47+00:00</updated>
    <author>
      <name>/u/Polymorphic-X</name>
      <uri>https://old.reddit.com/user/Polymorphic-X</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been working on a project I call &lt;strong&gt;O-TITANS&lt;/strong&gt; (Orthogonal Tensors for Independent Task Alignment). It's an Orthogonal LoRA approach specifically for Gemma 3 that incorporates the Google TITANS memory architecture.&lt;br /&gt; It was inspired by a project by ffurfaro on HF called &amp;quot;TPTT&amp;quot; that I just couldn't get to work.&lt;/p&gt; &lt;p&gt;I'm building this to wrap into my next project: &lt;strong&gt;MoOLE-T (Mixture of Orthogonal LoRA Experts - Titans)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal of MoOLE-T is to use a smaller 8B router to select one or more O-LoRAs to pass inference through simultaneously. The output will then get translated and de-conflicted at an &amp;quot;exit node&amp;quot; (a larger 20B-80B model). Theoretically, this creates a beefed-up MoE with specific skills like a tool belt. This approach should punch way above its weight class while needing only a fraction of the VRAM footprint. The best part? It's scalable to a stupid degree, since O-Loras don't interfere directly and can be multi-slotted. You could train 100+ O-LoRAs on individual skills and have a toolbelt of capabilities without bloating a base model to hundreds of billions of parameters.&lt;/p&gt; &lt;p&gt;Still working on the MoOLE-T polyswarm idea, but I'll do another post whenever that gets finished.&lt;/p&gt; &lt;p&gt;I just finished training an example &lt;code&gt;.pt&lt;/code&gt; file on Open-Platypus using mlabonne's Gemma3-12b-it-abliterated model as a base. It's on my hugginface if you want to test the non-interference claims yourselves.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face (O-TITANS Gemma 3 Adapters):&lt;/strong&gt; &lt;a href="https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/"&gt;https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to feedback and additional ideas. This is all an attempt to try and approach human-esque parallel skill processing and selection without absurd compute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Polymorphic-X"&gt; /u/Polymorphic-X &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T22:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawoe4</id>
    <title>PSA: The software “Shade” is a fraudulent, plagiarized copy of Heretic</title>
    <updated>2026-02-21T17:16:21+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three days ago, the following repository was published, which its “creator” has been aggressively promoting on various channels since then:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/assemsabry/shade"&gt;https://github.com/assemsabry/shade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire source code in the repository is plagiarized from Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), with only the project name and the copyright notice replaced, claiming “original authorship” of everything. The repository does not acknowledge Heretic as its source, and has erased the commit history and the names of all Heretic contributors.&lt;/p&gt; &lt;p&gt;I and several others have called the repository owner out, but he has deleted all issues and tried to cover up his wrongdoing by adding some bogus “additional features” using an AI agent. A quick look at the source files, however, reveals that they are still 95% identical to Heretic’s code. In some cases, only the copyright notice was replaced.&lt;/p&gt; &lt;p&gt;**I can only assume that the ultimate goal is to push malware of some sort, and strongly advise people to stay clear of this plagiarized repository.**&lt;/p&gt; &lt;p&gt;This is one of several incidents where malicious actors tried to profit from Heretic’s surging popularity during the past days, when it reached #1 on the GitHub trending chart and was posted in various social feeds that cater to scammers.&lt;/p&gt; &lt;p&gt;Please also see &lt;a href="https://github.com/p-e-w/heretic/issues/167"&gt;https://github.com/p-e-w/heretic/issues/167&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m doing everything in my power to keep Heretic clean and available to everyone. Thank you for your encouragement in the past few months, it means the world to me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb2j5c</id>
    <title>Favourite niche usecases?</title>
    <updated>2026-02-21T21:06:34+00:00</updated>
    <author>
      <name>/u/Figai</name>
      <uri>https://old.reddit.com/user/Figai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt; &lt;img alt="Favourite niche usecases?" src="https://preview.redd.it/o4l2ankhxwkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7201facadd4e9d14e1aac7efef2133d85d346f7" title="Favourite niche usecases?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Figai"&gt; /u/Figai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4l2ankhxwkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T21:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1raq23i</id>
    <title>they have Karpathy, we are doomed ;)</title>
    <updated>2026-02-21T12:34:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt; &lt;img alt="they have Karpathy, we are doomed ;)" src="https://preview.redd.it/ergzi9d1eukg1.png?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=2005c28094bfd489a487151bba9f5c550c22c55b" title="they have Karpathy, we are doomed ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(added second image for the context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raq23i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T12:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
