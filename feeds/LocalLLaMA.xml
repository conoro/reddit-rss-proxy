<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-04T07:49:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pd1yqc</id>
    <title>Hot take: We’re overselling 'semantic search' in RAG.</title>
    <updated>2025-12-03T11:42:02+00:00</updated>
    <author>
      <name>/u/Raisin_False</name>
      <uri>https://old.reddit.com/user/Raisin_False</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building some RAG stuff and 'semantic search' feels way more magical in marketing than in reality.&lt;/p&gt; &lt;p&gt;Embeddings are great &lt;strong&gt;fuzzy matchers in meaning space&lt;/strong&gt; - they shine on paraphrases, synonyms, 'something like this' queries. But whenever I need sharper behavior (logic, constraints, dates, 'papers using X on Y after 2019'), plain bi-encoder vector search starts to fall over unless I add extra machinery.&lt;/p&gt; &lt;p&gt;In practice my setups end up looking more like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;BM25 or dense (or hybrid) &lt;/li&gt; &lt;li&gt;Reranker and/or LLM query rewrite &lt;/li&gt; &lt;li&gt;LLM reasoning also maybe graphs/filters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At that point, calling just the first stage 'semantic search' feels a bit misleading, cause it's more like 'dense/vector retrieval' plus a bunch of stuff on top that actually does the reasoning.&lt;/p&gt; &lt;p&gt;So i have 2 questions for you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is 'semantic search' a fair name for plain vector similarity, or do you avoid that term?&lt;/li&gt; &lt;li&gt;How far did you get with just embeddings before needing reranking / query rewriting / graphs / filters?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raisin_False"&gt; /u/Raisin_False &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdhuyy</id>
    <title>Why doesn't deepseek release a smaller air model? Because they are focused at research?</title>
    <updated>2025-12-03T21:58:57+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why doesn't deepseek release a smaller air model like a 120b A10b MoE model or a 32b dense model? It seems like they are mainly focused in research and doesn't frequently release small models unlike GLM and qwen &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqvet</id>
    <title>Question for LLM engineers: is there value in a tool that tests prompts at scale and rewrites them until they behave correctly?</title>
    <updated>2025-12-04T04:40:49+00:00</updated>
    <author>
      <name>/u/BulkyAd7044</name>
      <uri>https://old.reddit.com/user/BulkyAd7044</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want feedback from people who work with LLMs on a regular basis.&lt;/p&gt; &lt;p&gt;A lot of prompt development still feels like guesswork. Teams write a few examples, test in a playground, or keep spreadsheets. When a prompt or model changes, it is hard to know what quietly broke. Running batches of tests across multiple providers often requires custom scripts and rate limit workarounds.&lt;/p&gt; &lt;p&gt;Claude or GPT can generate a couple examples, but they do not create diverse synthetic test suites and they do not run evaluations at scale. Most developers end up tweaking prompts by hand until they feel right, even though the behavior may not be validated.&lt;/p&gt; &lt;p&gt;I am exploring whether a tool focused on synthetic test generation and multi-model evaluation would be useful. The idea is to help developers arrive at a prompt that is actually tested and predictable, not something tuned by manual trial and error. The system would generate around 100 realistic and edge-case inputs, evaluate them across models, and then automatically rewrite and refine the prompt until it performs well on the full test set.&lt;/p&gt; &lt;p&gt;Ideas I am considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate ~100 realistic and edge-case inputs for a prompt&lt;/li&gt; &lt;li&gt;Run those tests across GPT, Claude, Gemini, etc&lt;/li&gt; &lt;li&gt;Show where outputs diverge&lt;/li&gt; &lt;li&gt;Automatically refine the prompt based on the failures&lt;/li&gt; &lt;li&gt;Give developers more confidence that the final prompt is stable and ready to ship&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is not a product pitch. I just want to understand the pain points.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would a tool that generates tests and automatically improves your prompt until it performs well be useful&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BulkyAd7044"&gt; /u/BulkyAd7044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqvet/question_for_llm_engineers_is_there_value_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqvet/question_for_llm_engineers_is_there_value_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqvet/question_for_llm_engineers_is_there_value_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:40:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlhyn</id>
    <title>Local ai for music?</title>
    <updated>2025-12-04T00:28:51+00:00</updated>
    <author>
      <name>/u/Rique_Belt</name>
      <uri>https://old.reddit.com/user/Rique_Belt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is the Qwen-image-editing and those fancy new VLM that can be run on local hardware without requiring crazy specs. But are there such thing for music input and output? Recently, there was two situations on which I wanted to extract only the piano melody from a song and find similar songs that starts with an specific rhythm like Supernaut from Black Sabbath which sounded really familiar. The first situation I know that there is ai for that for I have used but in that case was for vocals, but the second case I am not sure since it would require a world-knowledge and special training for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rique_Belt"&gt; /u/Rique_Belt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlhyn/local_ai_for_music/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlhyn/local_ai_for_music/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlhyn/local_ai_for_music/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdmc7w</id>
    <title>1 week update on ForgeIndex, my directory for local AI tools</title>
    <updated>2025-12-04T01:06:21+00:00</updated>
    <author>
      <name>/u/Equivalent-Ad-9798</name>
      <uri>https://old.reddit.com/user/Equivalent-Ad-9798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone — last week I shared ForgeIndex.ai here, a lightweight directory I’m building to make it easier to discover open-source local AI tools in one place.&lt;/p&gt; &lt;p&gt;Quick 1-week update: - The index has grown from around 30 projects to 60+ - I added hardware + OS + software requirements for most tools - Improved categories/tags for easier filtering - Fixed UI issues based on feedback - Added more demos + GitHub links - Working on a roadmap for smarter filtering (GPU-friendly, CPU-only, mobile capable, etc.)&lt;/p&gt; &lt;p&gt;The goal is to make ForgeIndex the simplest way to explore local AI tools without digging through GitHub, Reddit threads, Discords, YouTube (like me lol) or newsletters.&lt;/p&gt; &lt;p&gt;If you know projects I should add, or features you’d like to see (search filters, categories, compatibility flags etc.), let me know. Still really early, but steadily improving.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://forgeindex.ai"&gt;https://forgeindex.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or get feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Ad-9798"&gt; /u/Equivalent-Ad-9798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdmc7w/1_week_update_on_forgeindex_my_directory_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdmc7w/1_week_update_on_forgeindex_my_directory_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdmc7w/1_week_update_on_forgeindex_my_directory_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T01:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdl3q0</id>
    <title>Parameters to run Deepseek R1 671b Q4</title>
    <updated>2025-12-04T00:11:46+00:00</updated>
    <author>
      <name>/u/I_like_fragrances</name>
      <uri>https://old.reddit.com/user/I_like_fragrances</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdl3q0/parameters_to_run_deepseek_r1_671b_q4/"&gt; &lt;img alt="Parameters to run Deepseek R1 671b Q4" src="https://b.thumbs.redditmedia.com/dj_ndZdqEls0WR74zJtsmcO_odY9JVi164RGExix-GE.jpg" title="Parameters to run Deepseek R1 671b Q4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to run Deepseek R1 671b Q4, I need to offload some to RAM but every config I try it fails to load. How can I get it to load on LMStudio? I attached images of my hardware and the model parameter config options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_like_fragrances"&gt; /u/I_like_fragrances &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pdl3q0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdl3q0/parameters_to_run_deepseek_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdl3q0/parameters_to_run_deepseek_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqlkp</id>
    <title>Built a Python SDK for tool calling with Ollama (also has a TUI)</title>
    <updated>2025-12-04T04:26:39+00:00</updated>
    <author>
      <name>/u/jrummy16</name>
      <uri>https://old.reddit.com/user/jrummy16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqlkp/built_a_python_sdk_for_tool_calling_with_ollama/"&gt; &lt;img alt="Built a Python SDK for tool calling with Ollama (also has a TUI)" src="https://external-preview.redd.it/H_Xk72f2p-5BMENjkachQUexRhOMhSdq6pGnN1HVmBM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0ed292d5b1727c8c0a87f5cad09195ad91ce81d" title="Built a Python SDK for tool calling with Ollama (also has a TUI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been running Ollama locally and got tired of writing the same boilerplate for tool calling every time. Built Consoul so I can just do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from consoul import Consoul console = Consoul(provider=&amp;quot;ollama&amp;quot;, model=&amp;quot;llama3.2&amp;quot;) console.chat(&amp;quot;refactor this&amp;quot;, tools=True) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It handles the tool calling loop, file editing, code search, etc. Also works with Claude/GPT if you want to compare responses.&lt;/p&gt; &lt;p&gt;The TUI is actually nice too (Textual-based):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install 'consoul[tui]' consoul tui &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Added HuggingFace tokenizers for Ollama because calling the API to count tokens was painfully slow. Now it's instant.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/goatbytes/consoul"&gt;https://github.com/goatbytes/consoul&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT licensed. Been using it daily, curious what breaks for others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jrummy16"&gt; /u/jrummy16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/goatbytes/consoul"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqlkp/built_a_python_sdk_for_tool_calling_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqlkp/built_a_python_sdk_for_tool_calling_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:26:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9tgj</id>
    <title>A Technical Tour of the DeepSeek Models from V3 to V3.2</title>
    <updated>2025-12-03T17:03:17+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt; &lt;img alt="A Technical Tour of the DeepSeek Models from V3 to V3.2" src="https://external-preview.redd.it/Oy9W7OYOeVO8Z6Sl3EWWZR-9AbREkAwoyEei1XJ7yeY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a5effd7f132f71b2efdd47cc12daa448023c0bf" title="A Technical Tour of the DeepSeek Models from V3 to V3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/technical-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T17:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd5yxy</id>
    <title>My experiences with the new Ministral 3 14B Reasoning 2512 Q8</title>
    <updated>2025-12-03T14:41:03+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt; &lt;img alt="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" src="https://b.thumbs.redditmedia.com/YQNWxn03P5a0Q35GBj3cSIS0Oa0a8pdRn0Pkkl0sUGM.jpg" title="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;45 minutes and 33K tokens of thinking about making html tetris (1 line prompt):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819"&gt;https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tool calling breaks all the time:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184"&gt;https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also at some point it stopped using the [think] tags altogether and just started thinking out loud. I'll leave it running for a couple of hours and see if it eventually manages to build the HTML Tetris.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T14:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdtj6c</id>
    <title>entered a memory competition with my local llama setup, results were weird</title>
    <updated>2025-12-04T07:07:11+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this long term memory competition thing on twitter a few weeks back and decided to enter with my local setup. Llama 3.1 8B Instruct + some memory hacks i've been working on.&lt;/p&gt; &lt;p&gt;Competition had 3 main tasks:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Long-term dialogue (50+ turns, reference stuff from turn 5 at turn 45)&lt;/li&gt; &lt;li&gt;Multi-person conversation tracking (track who said what when) &lt;/li&gt; &lt;li&gt;Causal reasoning (if X happened because of Y, remember the connection)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My approach was pretty basic. Used transformers library, monkey patched the generate() function to not reset past_key_values between conversation turns. Added some janky importance scoring - basically tracked which tokens got high attention scores and tried to keep those when hitting memory limits. Nothing fancy, just hacked together over a weekend.&lt;/p&gt; &lt;p&gt;Results were all over the place:&lt;/p&gt; &lt;p&gt;Task 1 (long conversations): 72.3% - not bad Task 2 (multi person): 43.8% - terrible Task 3 (causal reasoning): 81.7% - surprisingly good&lt;/p&gt; &lt;p&gt;The weird part is task 3. My system somehow got causal connections way better than conversation tracking. No clue why that worked.&lt;/p&gt; &lt;p&gt;Looking at other entries, most people did RAG stuff. Vector DBs, embeddings, retrieval, you know. Standard approach. My KV cache thing was kinda different.&lt;/p&gt; &lt;p&gt;Top scorer got 92.3% overall using some open source memory system. Way better than my 65.9% average but their approach was completely different from mine. From the leaderboard description, they used hybrid retrieval with multiple databases instead of just KV cache hacks. Found the repo later: github.com/EverMind-AI/EverMemOS. Seemed like a proper memory framework with MongoDB, Elasticsearch, and vector databases vs my simple KV cache approach.&lt;/p&gt; &lt;p&gt;Couple things i figured out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache stuff works but eats memory like crazy (hit 22.8GB on my 3090 for the 50+ turn conversations, had to restart multiple times)&lt;/li&gt; &lt;li&gt;importance scoring is key, otherwise you run out of space fast&lt;/li&gt; &lt;li&gt;multi person chats are a nightmare, way harder than i expected. spent most time debugging this&lt;/li&gt; &lt;li&gt;causal reasoning was surprisingly ok, not sure why. maybe got lucky?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Might look into other approaches. My hack was fun but obviously not great lol. The winning approach looked more serious but setup seemed complicated from what i could see. Maybe worth checking out if i have time.&lt;/p&gt; &lt;p&gt;Competition was actually useful tho. Made me test things properly instead of just &amp;quot;eh seems to work&amp;quot;. Realized my approach had way more issues than i thought.&lt;/p&gt; &lt;p&gt;Anyone else tried these memory challenge things? Curious what approaches worked for you. Mine was obviously not great but learned a lot about the limitations of simple KV cache approaches.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdrist</id>
    <title>Epyc setup + 1/2 Pro 6000 that can run Qwen coder 480b at 20 tps?</title>
    <updated>2025-12-04T05:13:58+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the old rig that I have been using to experiment with the Pro 6000 - I am finally going to replace it with a comparable SOTA setup (minus the GPU). I would like a working setup that could achieve 20 tps with my favourite model. If that is unrealistic, 10+ tps could work too. I already know 5 tps is fairly achievable (but not useful)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrist/epyc_setup_12_pro_6000_that_can_run_qwen_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrist/epyc_setup_12_pro_6000_that_can_run_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrist/epyc_setup_12_pro_6000_that_can_run_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T05:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdswk2</id>
    <title>Spec-Kit with Ministral 3 14b</title>
    <updated>2025-12-04T06:30:23+00:00</updated>
    <author>
      <name>/u/International_Quail8</name>
      <uri>https://old.reddit.com/user/International_Quail8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had been fighting with a few models to get spec-kit working locally. gpt-oss-20b, qwen3-coder-30b and qwen3-next all failed me! Used lmstudio for local inference and qwen code as the codegen cli. &lt;/p&gt; &lt;p&gt;I gave the new ministral 3 14b reasoning model a shot and was very impressed that it was able to follow the spec-kit process, work with the templates and generate my feature as spec’d! It also did it with reasonably good speed. Not perfect, but got through an entire complex feature from start to finish where the other models failed. Mistral did it again! Was a huge fan of Mixtral 8x7B from “back in the day”&lt;/p&gt; &lt;p&gt;Nice one Mistral AI!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/International_Quail8"&gt; /u/International_Quail8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T06:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3xyp</id>
    <title>Why don't Google and Openai release their old models?</title>
    <updated>2025-12-03T13:19:51+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPt 4 and gemini 2 pro are dated, they should release it... Are they afraid of releasing their data and architecture? They released gemma and gpt oss already. Gemini 2 has a large context window, but the quality degrades when it gets large though and it is replicable.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdi8o7</id>
    <title>DeepSeek-OCR – Apple Metal Performance Shaders (MPS) &amp; CPU Support</title>
    <updated>2025-12-03T22:13:40+00:00</updated>
    <author>
      <name>/u/Dogacel</name>
      <uri>https://old.reddit.com/user/Dogacel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt; &lt;img alt="DeepSeek-OCR – Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" src="https://external-preview.redd.it/9aiVAPD5fnnzElgE74nQoRx4aoPflI7CwyCPycl2BLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f593757a501d49b05111d714ec689f7acf643d1" title="DeepSeek-OCR – Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently updated DeepSeek-OCR to support Apple Metal (MPS) and CPU acceleration. I wanted to share this in case anyone else has been looking to run it efficiently on macOS.&lt;/p&gt; &lt;p&gt;To make it easier to use, I also forked an existing desktop client and applied the patch. You can check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Dogacel/deepseek-ocr-client-macos"&gt;https://github.com/Dogacel/deepseek-ocr-client-macos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogacel"&gt; /u/Dogacel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Dogacel/DeepSeek-OCR-Metal-MPS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2wjt</id>
    <title>DeepSeek V3.2 Technical Report</title>
    <updated>2025-12-03T12:31:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt; &lt;img alt="DeepSeek V3.2 Technical Report" src="https://preview.redd.it/q3rjrhs0gz4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2e078ce099142771b5d3999cbb9670fbfc18d8" title="DeepSeek V3.2 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a brief summary of &lt;strong&gt;key breakthroughs of DeepSeek V3.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. DeepSeek Sparse Attention (DSA)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new efficient attention mechanism that dramatically reduces computational complexity while preserving performance in long-context scenarios. &lt;/p&gt; &lt;p&gt;It uses a lightning indexer with fine-grained top-k token selection to achieve sparse but effective attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Scalable and Stable Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Implements a heavily scaled post-training RL pipeline, with compute exceeding 10% of pretraining cost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Large-Scale Agentic Task Synthesis Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Provides a novel pipeline that programmatically generates large numbers of tool-use environments (1,800+ environments, 85,000+ complex prompts). &lt;/p&gt; &lt;p&gt;This boosts generalization, tool-use ability, and instruction-following in interactive settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Unified Reasoning + Agentic RL Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Merges reasoning, tool-use, and human-alignment RL into a single stage rather than multi-stage pipelines. &lt;/p&gt; &lt;p&gt;This avoids catastrophic forgetting and improves cross-domain performance simultaneously.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A high-compute variant trained with relaxed length penalties and enhanced mathematical-reasoning rewards. &lt;/p&gt; &lt;p&gt;This model even surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.02556"&gt;Arxiv paper &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3rjrhs0gz4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdk4zx</id>
    <title>[Resource] 20,000+ Pages of U.S. House Oversight Epstein Estate Docs (OCR'd &amp; Cleaned for RAG/Analysis)</title>
    <updated>2025-12-03T23:30:44+00:00</updated>
    <author>
      <name>/u/Ok-District-1330</name>
      <uri>https://old.reddit.com/user/Ok-District-1330</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve reuploaded the recent release of 20,000+ pages of documents regarding the Epstein Estate from the U.S. House Oversight Committee. The goal is to make these scattered government files accessible for journalists and researchers using open-source tools.&lt;/p&gt; &lt;p&gt;Note, this was originally shared here, by another user who's account has now been deleted, then uploaded to huggingface. The original huggingface repo has since been removed, as well as the original uploaders account. Credit for the original dataset goes to him/her. This is simply a clone, hosted on Github and my huggingface account, with a gradio app I built for interacting/searching it.&lt;/p&gt; &lt;p&gt;The original release contained mixed file formats and nested folders. This dataset converts images/PDFs to text (via Tesseract OCR) and standardizes them into a single CSV format.&lt;/p&gt; &lt;p&gt;Searchable App: A Gradio browser to search the corpus without downloading the full set.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/theelderemo/epstein-files"&gt;Hugging Face Gradio App and Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/theelderemo/Epstein-files"&gt;Github Mirror&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-District-1330"&gt; /u/Ok-District-1330 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T23:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdin3b</id>
    <title>The Best Open Weights Coding Models of 2025</title>
    <updated>2025-12-03T22:29:44+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt; &lt;img alt="The Best Open Weights Coding Models of 2025" src="https://external-preview.redd.it/wxWktbYwfvy3j0Y1BhifZGPwnHUnY7JGZEyykx4N_HM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48a8af816e33acba1ab83ce8086f21346740c37d" title="The Best Open Weights Coding Models of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm back with uncontaminated evals for DeepSeek-V3.2, Kimi K2 Thinking, and MiniMax M2. (We caught GLM 4.6 last time around.) &lt;/p&gt; &lt;p&gt;If you just want the numbers, you can find them for the finalists &lt;a href="https://brokk.ai/power-ranking?models=dsv3.2%2Cglm4.6-fp8%2Ck2-thinking%2Cm2"&gt;here&lt;/a&gt; and for everyone else &lt;a href="https://brokk.ai/power-ranking?dataset=openround"&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.brokk.ai/the-best-open-weights-coding-models-of-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlfu3</id>
    <title>Frozen networks show usable early-layer intent: 1370× fewer FLOPs and 10× faster inference (code + weights)9</title>
    <updated>2025-12-04T00:26:15+00:00</updated>
    <author>
      <name>/u/anima-core</name>
      <uri>https://old.reddit.com/user/anima-core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with whether a frozen network’s early activations contain enough “semantic intent” to skip most of the compute.&lt;/p&gt; &lt;p&gt;I used a standard ResNet-18 trained on CIFAR-10 (87.89 percent accuracy), pulled a single 64-dimensional vector from an early layer, and trained a tiny decoder on top of it.&lt;/p&gt; &lt;p&gt;Results on the same hardware: • 72.57 percent accuracy from that early-layer vector • ~10× faster real latency • 1370× fewer FLOPs • No pruning, distillation, quantization, early exit tricks, or sparsity • The full model stayed completely frozen&lt;/p&gt; &lt;p&gt;This means 99.93 percent of the original network’s compute was not required to recover 82.6 percent of its performance.&lt;/p&gt; &lt;p&gt;Code + one-click run script: &lt;a href="https://github.com/Anima-Core/an1-meaning-engine"&gt;https://github.com/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF demo + pretrained weights: &lt;a href="https://huggingface.co/Anima-Core/an1-meaning-engine"&gt;https://huggingface.co/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Runs end to end on almost any GPU or CPU in a few minutes.&lt;/p&gt; &lt;p&gt;Dedicated to my late father, Asad Shamim, whose loss opened the path that led me here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anima-core"&gt; /u/anima-core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdm268</id>
    <title>How Attention Got So Efficient [GQA/MLA/DSA]</title>
    <updated>2025-12-04T00:53:59+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt; &lt;img alt="How Attention Got So Efficient [GQA/MLA/DSA]" src="https://external-preview.redd.it/4QixmEzxJtTr5ZgAjR4FoJjK4qVPLU4zAuNo-fsPzgM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f8badcadc6197f760be212560f8188dc2793fa6" title="How Attention Got So Efficient [GQA/MLA/DSA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone trying to understand why Deepseek 3.2 DSA is a milestone in terms of solving long context, I really recommend this video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Y-o545eYjXM?si=pt-SxR5anfLNSN8j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdfk0o</id>
    <title>Hermes 4.3 - 36B Model released</title>
    <updated>2025-12-03T20:30:22+00:00</updated>
    <author>
      <name>/u/crazeum</name>
      <uri>https://old.reddit.com/user/crazeum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt; &lt;img alt="Hermes 4.3 - 36B Model released" src="https://external-preview.redd.it/thAQxjbw3fpc9fgR1nrJDb-3cDeZ9f7TtJWveW5lCQ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49b96ff1b32dfa841362b8c2a0d4449fdd83b1f0" title="Hermes 4.3 - 36B Model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes uncensored line models with apache 2 license. Post trained from Seed-OSS-36B-Base on their psyche network. The cool bit is they also trained it centralized and the distributed psyche trained version outperformed the centrally trained one.&lt;/p&gt; &lt;p&gt;GGUF links: &lt;a href="https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF"&gt;https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazeum"&gt; /u/crazeum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdcytv</id>
    <title>Micron Announces Exit from Crucial Consumer Business</title>
    <updated>2025-12-03T18:54:47+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technically speaking, we're screwed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqxbw</id>
    <title>Chinese CXMT unveils DDR5-8000 RAM</title>
    <updated>2025-12-04T04:43:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory"&gt;https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chinese RAM might be the way to buck the trend of rising prices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdh0sm</id>
    <title>8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich</title>
    <updated>2025-12-03T21:26:43+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt; &lt;img alt="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" src="https://external-preview.redd.it/ZzlmajZ6b3gzMjVnMYyMXOA9G9iEfbHd4uR1YsqLbApEsnv66h0V49mXIA5l.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f3aaa8ebe40ec05d81117c0df8ce6a792a1bd" title="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o8n25oox325g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
