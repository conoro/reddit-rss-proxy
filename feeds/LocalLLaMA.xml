<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-12T05:24:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1otxs37</id>
    <title>Our sub got a shout-out from the Corridor Crew</title>
    <updated>2025-11-11T02:33:11+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt; &lt;img alt="Our sub got a shout-out from the Corridor Crew" src="https://external-preview.redd.it/MG5qbWE3OXZoajBnMfJFc8SM8imSZJpbD6BkmsMZ2u1jbLaP-XMJEPc_yiXX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=269b48a90a4a2de8bb79dd262a86c54e29a97ed9" title="Our sub got a shout-out from the Corridor Crew" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From their recent video &lt;a href="https://youtu.be/6hI9T4jnrSI?si=h7An0736C93hs7YO"&gt;AI Experts Debunk The Latest SLOP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/10yfbe8vhj0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouuko3</id>
    <title>Kimi K2 Thinking Q4_K_XL Running on Strix Halo</title>
    <updated>2025-11-12T03:32:50+00:00</updated>
    <author>
      <name>/u/ga239577</name>
      <uri>https://old.reddit.com/user/ga239577</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got it to run on the ZBook Ultra G1a ... it's very slow, obviously way too slow for most use cases. However, if you provide well crafted prompts and are willing to wait hours or overnight, there could still be some use cases. Such as trying to fix code other local LLMs are failing at - you could wait overnight for something like that ... or private financial questions etc. Basically anything you don't need right away, prefer to keep on local and are willing to wait for.&lt;/p&gt; &lt;p&gt;prompt eval time = 74194.96 ms / 19 tokens ( 3905.00 ms per token, 0.26 tokens per second)&lt;br /&gt; eval time = 1825109.87 ms / 629 tokens ( 2901.61 ms per token, 0.34 tokens per second)&lt;br /&gt; total time = 1899304.83 ms / 648 tokens&lt;/p&gt; &lt;p&gt;Here was my llama-server start up command.&lt;/p&gt; &lt;p&gt;llama-server -m &amp;quot;Kimi-K2-Thinking-UD-Q4_K_XL-00001-of-00014.gguf&amp;quot; -c 4096 -ngl 62 --override-tensor &amp;quot;([0-9]+).ffn_.*_exps.=CPU&amp;quot; -ub 4096 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --cache-type-k q4_0 --cache-type-v q4_0 --port 8080&lt;/p&gt; &lt;p&gt;Have tried loading with a bigger context window (8192) but it outputs gibberish. It will run with the below command as well, and results were basically the same. Offloading to disk is slow ... but it works.&lt;/p&gt; &lt;p&gt;llama-server -m &amp;quot;./Kimi-K2-Thinking-UD-Q4_K_XL-00001-of-00014.gguf&amp;quot; -c 4096 -ngl 3 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --cache-type-k q4_0 --cache-type-v q4_0 --port 8080&lt;/p&gt; &lt;p&gt;If anyone has any ideas to speed this up, let me know. I'm going to try merging the shards to see whether that helps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ga239577"&gt; /u/ga239577 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouuko3/kimi_k2_thinking_q4_k_xl_running_on_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouuko3/kimi_k2_thinking_q4_k_xl_running_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouuko3/kimi_k2_thinking_q4_k_xl_running_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T03:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou14ry</id>
    <title>baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case..</title>
    <updated>2025-11-11T05:21:46+00:00</updated>
    <author>
      <name>/u/PaceZealousideal6091</name>
      <uri>https://old.reddit.com/user/PaceZealousideal6091</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"&gt; &lt;img alt="baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case.." src="https://external-preview.redd.it/81GI5f2SH41ji6Aiuro1sKkxz-x19lfHg7ZgCRL6MOI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4217ff485db0b42df0be643ea3fe3e8636aa4480" title="baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems Baidu has released the &amp;quot;thinking&amp;quot; variant if their vl model silently. The earlier model was supposedly hybrid, supporting both &amp;quot;thinking&amp;quot; and &amp;quot;non-thinking&amp;quot;. The model card says that they have introduced something called &amp;quot;thinking with images&amp;quot; without explaining what it is. They have one put a small hardly visible graph comparing it with gemini 2.5 pro and gpt-5 high in various benchmarks . If you squint your eye enough, then you'll see they claim using the graph that this model keeps up or beat them good in many of the benchmarks. Surely benchmaxxed. Its too good to believe. Has anyone tried it? The previous ernie versions have been decent. It might be worth testing it. Does anyone have any idea how is this &amp;quot;thinking&amp;quot; variant different?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceZealousideal6091"&gt; /u/PaceZealousideal6091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouh46d</id>
    <title>What happened with Kimi Linear?</title>
    <updated>2025-11-11T18:23:30+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been out for a bit, is it any good? It looks like Llama.cpp support is currently lacking&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh46d/what_happened_with_kimi_linear/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh46d/what_happened_with_kimi_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh46d/what_happened_with_kimi_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T18:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1otveug</id>
    <title>A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K</title>
    <updated>2025-11-11T00:44:49+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt; &lt;img alt="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" src="https://external-preview.redd.it/j6x6Pm9GXcBDejuI8fZ_JaGjEF5FKmyowYdHbKM_k34.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f2207a39f85b0be48a03566c4c904bcc528405b" title="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/342779/olares-to-launch-a-personal-ai-device-bringing-cloud-level-performance-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T00:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouqwv4</id>
    <title>üöÄLLM Overthinking? DTS makes LLM think shorter and answer smarter</title>
    <updated>2025-11-12T00:44:35+00:00</updated>
    <author>
      <name>/u/Dear_Treat3688</name>
      <uri>https://old.reddit.com/user/Dear_Treat3688</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqwv4/llm_overthinking_dts_makes_llm_think_shorter_and/"&gt; &lt;img alt="üöÄLLM Overthinking? DTS makes LLM think shorter and answer smarter" src="https://b.thumbs.redditmedia.com/QdhK-E_BhboNs6NKXINOAgNolfe41OlhvbqPonuRIQU.jpg" title="üöÄLLM Overthinking? DTS makes LLM think shorter and answer smarter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Large Reasoning Models (LRMs) have achieved remarkable breakthroughs on reasoning benchmarks. However, they often fall into a paradox: the longer they reason, the &lt;em&gt;less&lt;/em&gt; accurate they become. To solve this problem, we propose &lt;strong&gt;DTS (Decoding Tree Sketching)&lt;/strong&gt;, a &lt;strong&gt;plug-and-play&lt;/strong&gt; framework to enhance LRM reasoning accuracy and efficiency. &lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;How it works:&lt;/strong&gt;&lt;br /&gt; The variance in generated output is predominantly determined by high-uncertainty (high-entropy) tokens. DTS selectively branches at high-entropy tokens, forming a sparse decoding tree to approximate the decoding CoT space. By early-stopping on the first complete CoT path, DTS leads to the &lt;strong&gt;shortest and most accurate&lt;/strong&gt; CoT trajectory.&lt;/p&gt; &lt;p&gt;üìà &lt;strong&gt;Results on AIME 2024 / 2025:&lt;/strong&gt;&lt;br /&gt; ‚úÖ Accuracy ‚Üë up to 8%&lt;br /&gt; ‚úÖ Average reasoning length ‚Üì ~23%&lt;br /&gt; ‚úÖ Repetition rate ‚Üì up to 20%&lt;br /&gt; ‚Äî all achieved purely through a &lt;strong&gt;plug-and-play&lt;/strong&gt; decoding framework.&lt;/p&gt; &lt;p&gt;Try our code and Colab Demo&lt;/p&gt; &lt;p&gt;üìÑ Paper:&lt;a href="https://arxiv.org/pdf/2511.00640?utm_source=chatgpt.com"&gt; &lt;/a&gt;&lt;a href="https://arxiv.org/pdf/2511.00640"&gt;https://arxiv.org/pdf/2511.00640&lt;/a&gt;&lt;/p&gt; &lt;p&gt; üíª Code:&lt;a href="https://github.com/ZichengXu/Decoding-Tree-Sketching"&gt; https://github.com/ZichengXu/Decoding-Tree-Sketching&lt;/a&gt;&lt;/p&gt; &lt;p&gt; üß© Colab Demo (free single GPU):&lt;a href="https://colab.research.google.com/github/ZichengXu/Decoding-Tree-Sketching/blob/main/notebooks/example_DeepSeek_R1_Distill_Qwen_1_5B.ipynb#scrollTo=oTrZL0i3UstX"&gt; &lt;/a&gt;&lt;a href="https://colab.research.google.com/github/ZichengXu/Decoding-Tree-Sketching/blob/main/notebooks/example_DeepSeek_R1_Distill_Qwen_1_5B.ipynb"&gt;https://colab.research.google.com/github/ZichengXu/Decoding-Tree-Sketching/blob/main/notebooks/example_DeepSeek_R1_Distill_Qwen_1_5B.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ts77lncq4q0g1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1044938eb0863ba9cc330d66fe985a0e4066b9da"&gt;https://preview.redd.it/ts77lncq4q0g1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1044938eb0863ba9cc330d66fe985a0e4066b9da&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ak22qgur4q0g1.jpg?width=2152&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=641f179bff8bc838acfe6457d98902da2b4a7e9c"&gt;https://preview.redd.it/ak22qgur4q0g1.jpg?width=2152&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=641f179bff8bc838acfe6457d98902da2b4a7e9c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wtyva5r05q0g1.png?width=5894&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab4b011cf5fc3f91187b62f35bc1b76c6612aa7"&gt;https://preview.redd.it/wtyva5r05q0g1.png?width=5894&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab4b011cf5fc3f91187b62f35bc1b76c6612aa7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0qd2rsc15q0g1.png?width=6196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c219bf4bd85f82216ce9c6c80ac0d99d4f56607d"&gt;https://preview.redd.it/0qd2rsc15q0g1.png?width=6196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c219bf4bd85f82216ce9c6c80ac0d99d4f56607d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear_Treat3688"&gt; /u/Dear_Treat3688 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqwv4/llm_overthinking_dts_makes_llm_think_shorter_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqwv4/llm_overthinking_dts_makes_llm_think_shorter_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqwv4/llm_overthinking_dts_makes_llm_think_shorter_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T00:44:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou8t7z</id>
    <title>Kimi K2 Thinking is a Better Agentic AI than I thought</title>
    <updated>2025-11-11T13:00:09+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ou8t7z/video/9dtnlbhhlm0g1/player"&gt;https://reddit.com/link/1ou8t7z/video/9dtnlbhhlm0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;just ran a quick eval on a deep agent built for customer support. It‚Äòs on par with GPT-5 in agentic capabilities.&lt;br /&gt; It's a bigger deal than I thought!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T13:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouh5c1</id>
    <title>I built a tool that maps and visualizes backend codebases</title>
    <updated>2025-11-11T18:24:37+00:00</updated>
    <author>
      <name>/u/Weary-Commercial-922</name>
      <uri>https://old.reddit.com/user/Weary-Commercial-922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"&gt; &lt;img alt="I built a tool that maps and visualizes backend codebases" src="https://b.thumbs.redditmedia.com/C7Mq5RMriMimKlbkiuPoIWaRGKqTCSrZx9l_q4mDzbs.jpg" title="I built a tool that maps and visualizes backend codebases" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For some weeks, I‚Äôve been trying to solve the problem of how to make LLMs actually understand a codebase architecture. Most coding tools can generate good code, but they don‚Äôt usually get how systems fit together.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6n870x947o0g1.png?width=2556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a4625070306da3d852acaa8a48a6e4e428299a1"&gt;https://preview.redd.it/6n870x947o0g1.png?width=2556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a4625070306da3d852acaa8a48a6e4e428299a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I started working on a solution, a tool that parses backend codebases (FastAPI, Django, Node, etc.) into a semantic graph. It maps every endpoint, service, and method as nodes, and connects them through their relationships, requests, dependencies, or data flows. From there, it can visualize backend like a living system. Then I found out this might be useful for engineers instead of LLMs, as a way to rapidly understand a codebase.&lt;/p&gt; &lt;p&gt;The architecture side looks a bit like an interactive diagramming tool, but everything is generated automatically from real code. You can ask it things like &lt;em&gt;‚ÄúShow me everything that depends on the auth router‚Äù&lt;/em&gt; or &lt;em&gt;‚ÄúExplain how does the parsing works?‚Äù&lt;/em&gt; and it will generate a node map of the focalized query.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pff5x7uc7o0g1.png?width=2512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a689dd64f90616daa6ba138c80c920b70ca3f589"&gt;https://preview.redd.it/pff5x7uc7o0g1.png?width=2512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a689dd64f90616daa6ba138c80c920b70ca3f589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mwk5rzce7o0g1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0091b437372ef4e02e2b1772ef801ce21cddf7d7"&gt;https://preview.redd.it/mwk5rzce7o0g1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0091b437372ef4e02e2b1772ef801ce21cddf7d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm also working in a PR review engine that uses the graph to detect when a change might affect another service (e.g., modifying a shared database method). And because it understands system context, it can connect through MCP to AI tools like Claude or Cursor, in an effort to make them ‚Äúarchitecture-aware.‚Äù&lt;/p&gt; &lt;p&gt;I‚Äôm mostly curious to hear if others have tried solving similar problems, or if you believe this is a problem at all, especially around codebase understanding, feature planning, or context-aware AI tooling.&lt;/p&gt; &lt;p&gt;Built with FastAPI, Tree Sitter, Supabase, Pinecone, and a React/Next.js frontend.&lt;/p&gt; &lt;p&gt;Would love to get feedback or ideas on what you‚Äôd want a system like this to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Commercial-922"&gt; /u/Weary-Commercial-922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T18:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oujse4</id>
    <title>Anyone tried Ling/Ring Flash 2.0?</title>
    <updated>2025-11-11T20:00:50+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF support landed about a month ago and both models seem to be of reasonable size with nice benchmark scores.&lt;/p&gt; &lt;p&gt;Has anyone tested these models? In particular how does Ring-Flash-2.0 compare against GLM 4.5 Air and GPT-OSS-120B? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oujse4/anyone_tried_lingring_flash_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oujse4/anyone_tried_lingring_flash_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oujse4/anyone_tried_lingring_flash_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T20:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1oueq51</id>
    <title>Agentic RAG: from Zero to Hero</title>
    <updated>2025-11-11T16:56:38+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;After spending several months building agents and experimenting with RAG systems, I decided to publish a GitHub repository to help those who are approaching agents and RAG for the first time.&lt;/p&gt; &lt;p&gt;I created an &lt;strong&gt;agentic RAG&lt;/strong&gt; with an educational purpose, aiming to provide a clear and practical reference. When I started, I struggled to find a single, structured place where all the key concepts were explained. I had to gather information from many different sources‚Äîand that‚Äôs exactly why I wanted to build something more accessible and beginner-friendly.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üìö What you‚Äôll learn in this repository&lt;/h2&gt; &lt;p&gt;An end-to-end walkthrough of the essential building blocks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PDF ‚Üí Markdown conversion&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hierarchical chunking&lt;/strong&gt; (parent/child structure)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid embeddings&lt;/strong&gt; (dense + sparse)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector storage&lt;/strong&gt; of chunks using &lt;em&gt;Qdrant&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallel multi-query handling&lt;/strong&gt; ‚Äî ability to generate and evaluate multiple queries simultaneously&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query rewriting&lt;/strong&gt; ‚Äî automatically rephrases unclear or incomplete queries before retrieval&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human-in-the-loop&lt;/strong&gt; to clarify ambiguous user queries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context management&lt;/strong&gt; across multiple messages using summarization&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;fully working agentic RAG&lt;/strong&gt; using LangGraph that retrieves, evaluates, corrects, and generates answers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple chatbot&lt;/strong&gt; using Gradio library &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;I hope this repository can be helpful to anyone starting their journey. &lt;/p&gt; &lt;p&gt;Thanks to everyone who takes a look and finds it useful! GitHub: &lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueq51/agentic_rag_from_zero_to_hero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueq51/agentic_rag_from_zero_to_hero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oueq51/agentic_rag_from_zero_to_hero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T16:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ous6zt</id>
    <title>Selective (smart) MoE experts offloading to CPU?</title>
    <updated>2025-11-12T01:42:14+00:00</updated>
    <author>
      <name>/u/greentheonly</name>
      <uri>https://old.reddit.com/user/greentheonly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing recent REAP models where existing MoE models were processed somehow and less frequent experts pruned out decreasing the model size made me wonder why the same thing is not applied to in general to the actual loading:&lt;/p&gt; &lt;p&gt;Basically the idea is to either run some sort of a benchmark/testrun and see which experts are the most frequent and prioritize loading those to VRAM, that should result in much higher generation speed since we are more likely to work off of fast VRAM vs slower cpu RAM. It should also be possible to do &amp;quot;autotune&amp;quot; sort of thing where over time the statistics for the current workload is gathered and the experts are reshuffled - more frequently used ones migrate to VRAM and less frequently used ones sink to CPU RAM.&lt;/p&gt; &lt;p&gt;Since I don't think I am the only one that could come up with this, there must be some underlying reason why it's not done? Some cursory search found &lt;a href="https://arxiv.org/html/2508.18983v1"&gt;https://arxiv.org/html/2508.18983v1&lt;/a&gt; this paper that seems tangentially related, but they load frequent experts to CPU RAM and leave the less frequent in storage which I guess could be the extra level of optimization too: i.e. have 3 tiers: 1. VRAM for most frequent 2. RAM for less frequent 3. the &amp;quot;mmap-mapped&amp;quot; that were not actually loaded (I know people nowadays recommend --no-mmap in llama.cpp because it indiscriminately keeps weights just mapped, so (at least some first runs?) are very slow as we have to fetch them from storage.&lt;/p&gt; &lt;p&gt;That way even the pruned ones (in the REAP models) you can keep in the much cheaper place.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/greentheonly"&gt; /u/greentheonly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ous6zt/selective_smart_moe_experts_offloading_to_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ous6zt/selective_smart_moe_experts_offloading_to_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ous6zt/selective_smart_moe_experts_offloading_to_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T01:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oup3zw</id>
    <title>Kimi K2 thinking, GLM 4.6 and Minimax M2 - the new era of opensource models?</title>
    <updated>2025-11-11T23:27:00+00:00</updated>
    <author>
      <name>/u/Bob5k</name>
      <uri>https://old.reddit.com/user/Bob5k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, a few weeks ago we had glm 4.6 - pretty damn good model for coding and agentic tasks. Capable as hell, being able to replace my sonnet4 (and sonnet4.5 later) on my usual day work for my clients.&lt;/p&gt; &lt;p&gt;After that - recently - minimax released m2 - quite damn good model aswell - and it's also FAST. Way faster than GLM via coding plan. Good to tackle coding tasks aswell, good to go on working on longer / bigger things aswell. I'm impressed.&lt;/p&gt; &lt;p&gt;Now we have kimi k2 thinking - another pretty damn good model. For coding itself probably a tad bit better than those 2 above. Takes longer to generate code, but quality is better (overall) - not a super significant difference, but it's very, very capable thing.&lt;/p&gt; &lt;p&gt;And now - all those are opensource. But also all those have their relevant coding plans making those available for vast majority of population (however glm still leads being the cheapest and more generous than other 2 basically - on the 20$ tier - those are all available there and pretty generous limits).&lt;/p&gt; &lt;p&gt;I wondered what are your thoughts on those models and thier relevant pricing / coding plans and so on. I want to know what the community thinks to include those thoughts in my guide - aimed at vibecoders, but considering this community quite dedicated to understanding LLMs itself rather than 'coding' community I think the value of insights on user ends is totally here.&lt;br /&gt; Enlighten me - as I have my own opinion, but also want to know yours (and check my profile if you want to read the guide :D)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bob5k"&gt; /u/Bob5k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oup3zw/kimi_k2_thinking_glm_46_and_minimax_m2_the_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oup3zw/kimi_k2_thinking_glm_46_and_minimax_m2_the_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oup3zw/kimi_k2_thinking_glm_46_and_minimax_m2_the_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T23:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1outtda</id>
    <title>Workstation in east TN (4x4090, 7950x3d)</title>
    <updated>2025-11-12T02:56:44+00:00</updated>
    <author>
      <name>/u/Adorable_Walrus5278</name>
      <uri>https://old.reddit.com/user/Adorable_Walrus5278</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1outtda/workstation_in_east_tn_4x4090_7950x3d/"&gt; &lt;img alt="Workstation in east TN (4x4090, 7950x3d)" src="https://b.thumbs.redditmedia.com/vjSGT14v0Nj7Xc92XRn8N_BgGauQDlldYFewKlvBj8s.jpg" title="Workstation in east TN (4x4090, 7950x3d)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone looking for a workstation? I'll probably have to part it out otherwise. (downsizing to a couple sparks)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adorable_Walrus5278"&gt; /u/Adorable_Walrus5278 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1outtda"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1outtda/workstation_in_east_tn_4x4090_7950x3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1outtda/workstation_in_east_tn_4x4090_7950x3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T02:56:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouqsoh</id>
    <title>Should I sell my 3090?</title>
    <updated>2025-11-12T00:39:18+00:00</updated>
    <author>
      <name>/u/Apart_Paramedic_7767</name>
      <uri>https://old.reddit.com/user/Apart_Paramedic_7767</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm going through some rough times financially right now. &lt;/p&gt; &lt;p&gt;Originally I wanted something that could run models for privacy but considering how far behind models that can fit in 24gb of VRAM are, I don‚Äôt see the point in keeping it.&lt;/p&gt; &lt;p&gt;I‚Äôm sad to let it go, but do you think there‚Äôs value in keeping it until some sort of breakthrough happens? Maybe in a few years it can run something on par with GPT-5 or will that never happen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart_Paramedic_7767"&gt; /u/Apart_Paramedic_7767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqsoh/should_i_sell_my_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqsoh/should_i_sell_my_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqsoh/should_i_sell_my_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T00:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouk53u</id>
    <title>Local, multi-model AI that runs on a toaster. One-click setup, 2GB GPU enough</title>
    <updated>2025-11-11T20:13:43+00:00</updated>
    <author>
      <name>/u/VivianIto</name>
      <uri>https://old.reddit.com/user/VivianIto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a desktop program that runs multiple AI models in parallel on hardware most people would consider e-waste. Built from the ground up to be lightweight.&lt;/p&gt; &lt;p&gt;The device only uses a 2GB GPU. If there's a gaming laptop or a mid-tier PC from the last 5-7 years lying around, this will probably run on it.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;&amp;gt; Runs 100% offline. No internet needed after the first model download.&lt;/p&gt; &lt;p&gt;&amp;gt; One-click installer for Windows/Mac/Linux auto-detects the OS and handles setup. (The release is a pre-compiled binary. You only need Rust installed if you're building from source.)&lt;/p&gt; &lt;p&gt;&amp;gt; Three small, fast models (Gemma2:2b, TinyLlama, DistilBERT) collaborate on each response. They make up for their small size with teamwork.&lt;/p&gt; &lt;p&gt;&amp;gt; Includes a smart, persistent memory system. Remembers past chats without ballooning in size.&lt;/p&gt; &lt;p&gt;Real-time metrics show the models working together live.&lt;/p&gt; &lt;p&gt;No cloud, no API keys, no subscriptions. The installers are on the releases page. Lets you run three models at once locally.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/ryanj97g/Project_VI"&gt;https://github.com/ryanj97g/Project_VI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VivianIto"&gt; /u/VivianIto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouk53u/local_multimodel_ai_that_runs_on_a_toaster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouk53u/local_multimodel_ai_that_runs_on_a_toaster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouk53u/local_multimodel_ai_that_runs_on_a_toaster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T20:13:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1j3e</id>
    <title>Seems like the new K2 benchmarks are not too representative of real-world performance</title>
    <updated>2025-11-11T05:45:03+00:00</updated>
    <author>
      <name>/u/cobalt1137</name>
      <uri>https://old.reddit.com/user/cobalt1137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"&gt; &lt;img alt="Seems like the new K2 benchmarks are not too representative of real-world performance" src="https://preview.redd.it/awzjyvo3gk0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7f2dd0a4c362653c960b794e0943c0b3784b17b" title="Seems like the new K2 benchmarks are not too representative of real-world performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobalt1137"&gt; /u/cobalt1137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awzjyvo3gk0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1emx</id>
    <title>We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp; coding benchmarks</title>
    <updated>2025-11-11T05:37:41+00:00</updated>
    <author>
      <name>/u/innocent2powerful</name>
      <uri>https://old.reddit.com/user/innocent2powerful</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"&gt; &lt;img alt="We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp;amp; coding benchmarks" src="https://preview.redd.it/fnpk5t7kbk0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c57d729e6fb3ff57f9b7d7ba1d9d6be31f27588" title="We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp;amp; coding benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;We put a lot of care into making sure the &lt;strong&gt;training data is fully decontaminated&lt;/strong&gt; ‚Äî every stage (SFT and RL) went through strict filtering to avoid any overlap with evaluation benchmarks.&lt;/li&gt; &lt;li&gt;It achieves state-of-the-art performance among small (&amp;lt;4B) models, both in competitive math and competitive coding tasks. Even &lt;strong&gt;surpass the DeepSeek R1 0120 in competitive math benchmarks&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It‚Äôs not designed as a general chatbot&lt;/strong&gt; (though it can handle basic conversation and factual QA). Our main goal was to &lt;strong&gt;prove that small models can achieve strong reasoning&lt;/strong&gt; ability, and we‚Äôve put a lot of work and iteration into achieving that, starting from a base like Qwen2.5-Math-1.5B (which originally had weak math and almost no coding ability) to reach this point.&lt;/li&gt; &lt;li&gt;We‚Äôd love for the community to &lt;strong&gt;test it on your own competitive math/coding benchmarks&lt;/strong&gt; and share results or feedback here. Any insights will help us keep improving.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;HuggingFace Paper: &lt;a href="https://huggingface.co/papers/2511.06221"&gt;paper&lt;/a&gt;&lt;br /&gt; X Post: &lt;a href="https://x.com/WeiboLLM/status/1988109435902832896?s=20"&gt;X&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;Download Model&lt;/a&gt; Ôºàset resp_len=40k, temp=0.6 / 1.0, top_p=0.95, top_k=-1 for better performance.Ôºâ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/innocent2powerful"&gt; /u/innocent2powerful &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fnpk5t7kbk0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouqbyo</id>
    <title>Local conversational model with STT TTS</title>
    <updated>2025-11-12T00:18:39+00:00</updated>
    <author>
      <name>/u/DuncanEyedaho</name>
      <uri>https://old.reddit.com/user/DuncanEyedaho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqbyo/local_conversational_model_with_stt_tts/"&gt; &lt;img alt="Local conversational model with STT TTS" src="https://external-preview.redd.it/eGU2ZzkydnJ5cDBnMX7vvGggqTNZOst5uXqXZt7URDd0IOwrN4Cxg9i1Tmfm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1efb5bd97fb338d0dfdc60d946215b0f1ccc6f7c" title="Local conversational model with STT TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to make an animatronic cohost to hang out with me and my workshop and basically roast me. It was really interesting how simple things like injecting relevant memories into the system prompt (or vision captioning) really messed with its core identity; very subtle tweaks repeatedly turned it into &amp;quot;a helpful AI assistant,&amp;quot; but I eventually got the personality to be pretty consistent with a medium context size and decent episodic memory. &lt;/p&gt; &lt;p&gt;Details: faster-whisper base model fine-tuned on my voice, Piper TTS tiny model find tuned on my passable impression of Skeletor, win11 ollama running llama 3.2 3B q4, custom pre-processing and prompt creation using pgvector, captioning with BLIP (v1), facial recognition that Claude basically wrote/ trained for me in a jiffy, and other assorted servos and relays.&lt;/p&gt; &lt;p&gt;There is a 0.5 second pause detection before sending off the latest STT payload. &lt;/p&gt; &lt;p&gt;Everything is running on an RTX 3060, and I can use a context size of 8000 tokens without difficulty, I may push it further but I had to slam it down because there's so much other stuff running on the card.&lt;/p&gt; &lt;p&gt;I'm getting back into the new version of Reddit, hope this is entertaining to somebody.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DuncanEyedaho"&gt; /u/DuncanEyedaho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hngyx3yryp0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqbyo/local_conversational_model_with_stt_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqbyo/local_conversational_model_with_stt_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T00:18:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oubrbc</id>
    <title>Meta chief AI scientist Yann LeCun plans to exit to launch startup, FT reports</title>
    <updated>2025-11-11T15:05:23+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/technology/meta-chief-ai-scientist-yann-lecun-plans-exit-launch-startup-ft-reports-2025-11-11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oubrbc/meta_chief_ai_scientist_yann_lecun_plans_to_exit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oubrbc/meta_chief_ai_scientist_yann_lecun_plans_to_exit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T15:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouq7oe</id>
    <title>I've just ordered an RTX 6000 Pro. What are the best models to use in its 96GB for inference and OCR processing of documents?</title>
    <updated>2025-11-12T00:13:24+00:00</updated>
    <author>
      <name>/u/AlwaysLateToThaParty</name>
      <uri>https://old.reddit.com/user/AlwaysLateToThaParty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, just trying to find out what people think are the best LLM's these days for inference and OCR document processing? So what model and quant works? I need it because a lot of the inference and documentation is confidential (medical and legal). More than one person will use the device via configuring a web front-end. Your suggestions would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlwaysLateToThaParty"&gt; /u/AlwaysLateToThaParty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouq7oe/ive_just_ordered_an_rtx_6000_pro_what_are_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouq7oe/ive_just_ordered_an_rtx_6000_pro_what_are_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouq7oe/ive_just_ordered_an_rtx_6000_pro_what_are_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T00:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oueiuj</id>
    <title>Half-trillion parameter model on a machine with 128 GB RAM + 24 GB VRAM</title>
    <updated>2025-11-11T16:49:09+00:00</updated>
    <author>
      <name>/u/pulse77</name>
      <uri>https://old.reddit.com/user/pulse77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;just wanted to share that I‚Äôve successfully run &lt;strong&gt;Qwen3-Coder-480B&lt;/strong&gt; on &lt;strong&gt;llama.cpp&lt;/strong&gt; using the following setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel i9-13900KS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 128 GB (DDR5 4800 MT/s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4090 (24 GB VRAM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm using the &lt;strong&gt;4-bit and 3-bit Unsloth quantizations&lt;/strong&gt; from Hugging Face: &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;UD-Q3_K_XL:&lt;/strong&gt; ~2.0 tokens/sec (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UD-Q4_K_XL:&lt;/strong&gt; ~1.0 token/sec (generation)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Command lines used (llama.cpp):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--threads 32 --jinja --flash-attn on \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-type-k q8_0 --cache-type-v q8_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model &amp;lt;YOUR-MODEL-DIR&amp;gt;/Qwen3-Coder-480B-A35B-Instruct-UD-Q3_K_XL-00001-of-00005.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--ctx-size 131072 --n-cpu-moe 9999 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--threads 32 --jinja --flash-attn on \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-type-k q8_0 --cache-type-v q8_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model &amp;lt;YOUR-MODEL-DIR&amp;gt;/Qwen3-Coder-480B-A35B-Instruct-UD-Q4_K_XL-00001-of-00006.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--ctx-size 131072 --n-cpu-moe 9999 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; The &lt;em&gt;--no-warmup&lt;/em&gt; flag is &lt;strong&gt;required&lt;/strong&gt; - without it, the process will terminate before you can start chatting.&lt;/p&gt; &lt;p&gt;In short: yes, it‚Äôs possible to run a &lt;strong&gt;half-trillion parameter model&lt;/strong&gt; on a machine with &lt;strong&gt;128 GB RAM + 24 GB VRAM&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pulse77"&gt; /u/pulse77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueiuj/halftrillion_parameter_model_on_a_machine_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueiuj/halftrillion_parameter_model_on_a_machine_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oueiuj/halftrillion_parameter_model_on_a_machine_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T16:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouazho</id>
    <title>Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)</title>
    <updated>2025-11-11T14:34:43+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"&gt; &lt;img alt="Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)" src="https://external-preview.redd.it/Z2ZjbDkzdmowbjBnMUB18FDMNIKrOWZMaI6GCxWf_t_2BvSabc90NvjIF-MD.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57cf924fd9176e70ff73462191aaf19ef1b60b55" title="Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face, (apache 2.0): &lt;a href="https://huggingface.co/datasets/builddotai/Egocentric-10K"&gt;https://huggingface.co/datasets/builddotai/Egocentric-10K&lt;/a&gt;&lt;br /&gt; Eddy Xu on ùïè: &lt;a href="https://x.com/eddybuild/status/1987951619804414416"&gt;https://x.com/eddybuild/status/1987951619804414416&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nlsslzuj0n0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T14:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouogiq</id>
    <title>Most used models and performance on M3u 512 gb</title>
    <updated>2025-11-11T23:00:24+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/"&gt; &lt;img alt="Most used models and performance on M3u 512 gb" src="https://preview.redd.it/s0jrlz569p0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5951ff139e6d111ec9e08e507274871f5fdc1a6" title="Most used models and performance on M3u 512 gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bored, thought this screenshot was cute, might delete later.&lt;/p&gt; &lt;p&gt;Overall GLM 4.6 is queen right now.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: Kimi K2 thinking&lt;br /&gt; &lt;strong&gt;Use case&lt;/strong&gt;: idk it's just cool having a huge model running local. I guess I will use it for brainstorming stuff, medical stuff, other questionable activities like academic writing. PP speed/context size is too limited for a lot of agentic workflows but it's a modest step above other open source models for pure smarts&lt;br /&gt; &lt;strong&gt;PP speed:&lt;/strong&gt; Q3 GGUF 19 t/s (26k context) faster with lower context;&lt;br /&gt; &lt;strong&gt;Token gen&lt;/strong&gt; speed: 3ish to 20 t/s depending on context size&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; GLM 4.6&lt;br /&gt; &lt;strong&gt;Use Case:&lt;/strong&gt; vibe coding (slow but actually can create working software semi-autonomously with Cline); creative writing; expository/professional writing; general quality-sensitive use&lt;br /&gt; &lt;strong&gt;PP Speed:&lt;/strong&gt; 4 bit MLX 50-70 t/s at large context sizes (greater than 40k)&lt;br /&gt; &lt;strong&gt;Token Gen speed:&lt;/strong&gt; generally 10-20&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Minimax-m2&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; Document review, finance, math. Like a smarter OSS 120.&lt;br /&gt; &lt;strong&gt;PP Speed&lt;/strong&gt;: MLX 4 bit 3-400 at modest sizes (10k ish)&lt;br /&gt; &lt;strong&gt;Token gen speed:&lt;/strong&gt; 40-50 at modest sizes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: GPT-OSS-120&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; Agentic searching, large document ingesting; general medium-quality, fast use&lt;br /&gt; &lt;strong&gt;PP speed:&lt;/strong&gt; 4 bit MLX near 1000 at modest context sizes. But context caching doesn't work, so has to reprocess every turn.&lt;br /&gt; &lt;strong&gt;Token gen speed:&lt;/strong&gt; about 80 at medium context sizes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model: Hermes 405b&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; When you want stuff to have that early 2024 vibe... not really good at anything except maybe low context roleplay/creative writing. Not the trivia king people seem to think.&lt;br /&gt; &lt;strong&gt;PP Speed:&lt;/strong&gt; mlx 4 bit: Low... maybe 25 t/s?&lt;br /&gt; &lt;strong&gt;Token gen Speed:&lt;/strong&gt; Super low... 3-5 t/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model: Deepseek 3.1:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; Used to be for roleplay, long context high quality slow work. Might be obsoleted by glm 4.6... not sure it can do anything better&lt;br /&gt; &lt;strong&gt;PP Speed:&lt;/strong&gt; Q3 GGUF: 50 t/s&lt;br /&gt; &lt;strong&gt;Token gen speed:&lt;/strong&gt; 3-20 depending on context size&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s0jrlz569p0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T23:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ousy0e</id>
    <title>Repeat after me.</title>
    <updated>2025-11-12T02:16:54+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs okay to be getting 45 tokens per second on an AMD card that costs 4 times less than an Nvidia card with same VRAM. Again, it‚Äôs okay. &lt;/p&gt; &lt;p&gt;They‚Äôll get better and better. And if you want 120 toks per second or 160 toks per second, go for it. Pay the premium. But don‚Äôt shove it up people‚Äôs asses. &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ousy0e/repeat_after_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ousy0e/repeat_after_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ousy0e/repeat_after_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T02:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ougamx</id>
    <title>gpt-oss-120b on Cerebras</title>
    <updated>2025-11-11T17:53:30+00:00</updated>
    <author>
      <name>/u/Corporate_Drone31</name>
      <uri>https://old.reddit.com/user/Corporate_Drone31</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"&gt; &lt;img alt="gpt-oss-120b on Cerebras" src="https://preview.redd.it/qkygjyoz1o0g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0028679c0cac64d9ce3f55e2a3aad86019108bc" title="gpt-oss-120b on Cerebras" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss-120b reasoning CoT on Cerebras be like &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corporate_Drone31"&gt; /u/Corporate_Drone31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qkygjyoz1o0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
