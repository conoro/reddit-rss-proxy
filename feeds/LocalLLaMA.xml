<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-04T18:49:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nxnszs</id>
    <title>Where do you think we'll be at for home inference in 2 years?</title>
    <updated>2025-10-04T07:41:51+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I suppose we'll never see any big price reduction jumps? Especially with inflation rising globally?&lt;/p&gt; &lt;p&gt;I'd love to be able to have a home SOTA tier model for under $15k. Like GLM 4.6, etc. But wouldn't we all?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T07:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxyfwr</id>
    <title>What are the best models for legal work in Oct 2025?</title>
    <updated>2025-10-04T16:26:12+00:00</updated>
    <author>
      <name>/u/yuch85</name>
      <uri>https://old.reddit.com/user/yuch85</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: I've been experimenting with models from the 20b-120b range recently and I found that if you can reliably get past the censorship issues, the gpt-oss models do seem to be the best for (English language) legal work. Would be great to hear some thoughts. &lt;/p&gt; &lt;p&gt;By &amp;quot;legal work' I mean - instruction following in focused tasks like contract drafting - RAG tasks - producing work not covered by RAG which requires good world knowledge (better inherent &amp;quot;legal knowledge&amp;quot;)&lt;/p&gt; &lt;p&gt;For document processing itself (eg raptor summaries, tagging, triplet extraction, clause extraction) there are plenty of good 4b models like qwen3-4b, IBM granite models etc which are more than up to the task&lt;/p&gt; &lt;p&gt;For everything else these are my observations - loosely, I used perplexity to draft a drafting prompt to amend a contract in a certain way and provide commentary. &lt;/p&gt; &lt;p&gt;Then I (1) tried to get the model to draft that same prompt and (2) use the perplexity drafted prompt to review a few clauses of the contract.&lt;/p&gt; &lt;p&gt;-Qwen3 (30b MOE, 32b): Everyone is going on about how amazing these models are. I think the recent instruct models are very fast, but I don't think they give the best quality for legal work or instruction following. They generally show poorer legal knowledge and miss out on subtler drafting points. When they do catch the points, the commentary sometimes wasn't clear why the amendments were being made. &lt;/p&gt; &lt;p&gt;-Gemma3-27b: This seems to have better latent legal knowledge, but again trips up slightly when instruction following in drafting.&lt;/p&gt; &lt;p&gt;-Llama3.3-70b (4 bit) and distills like Cogito: I find that despite being slighty dated by now, llama3.3-70b still holds up very well in terms of accuracy of its latent legal knowledge and instruction following when clause drafting. I had high hopes for the Cogito distilled variant but performance was very similar and not too different from the base 70b.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Magistral 24b: I find this is slightly lousier than Gemma3 - I'm not sure if it's the greater focus on European languages that makes it lose nuance on English texts. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GLM 4.5-Air (tried 4bit and 8bit): although it's 115b model, it had surprisngly slightly lousier performance than llama3-70b in both latent legal knowledge and instruction following (clause drafting). The 8bit quant I would say is on par with llama3-70b (4 bit). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GPT-OSS-20B and GPT-OSS-120B: Saving the best (and perhaps more controversial) for last - I would say that both models are really good at both their knowledge and instruction following - provided you can get past the censorship. The first time I asked a legal sounding question it clammed up. I changed the prompt to reassure it that it was only assisting a qualified attorney who would check its work and that seemed to work though. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, their redrafts are very on point and adhere to the instructions pretty well. I asked the GPT-OSS-120B model to draft the drafting prompt, and it provided something that was pretty comprehensive in terms of the legal knowledge. I was also surprised at how performant it was despite having to offload to CPU (I have a 48GB GPU) - giving me a very usable 25 tps. &lt;/p&gt; &lt;p&gt;Honorable mention: Granite4-30b. It just doesn't have the breadth of legal knowledge of llama3-70b, and instruction following was surprisingly not as good even though I expected it perform better. I would say it's actually slightly inferior to the Qwen3-30b-a3b.&lt;/p&gt; &lt;p&gt;Does anyone else have any good recommendations in this range? 70b is the sweet spot for me but with some offloading I can go up to around 120b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yuch85"&gt; /u/yuch85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxyfwr/what_are_the_best_models_for_legal_work_in_oct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxyfwr/what_are_the_best_models_for_legal_work_in_oct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxyfwr/what_are_the_best_models_for_legal_work_in_oct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T16:26:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqxtl</id>
    <title>Anyone running llm on their 16GB android phone?</title>
    <updated>2025-10-04T10:58:27+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My 8gb dual channel phone is dying, so I would like buy a 16gb quad channel android phone to run llm.&lt;/p&gt; &lt;p&gt;I am interested in running gemma3-12b-qat-q4_0 on it. &lt;/p&gt; &lt;p&gt;If you have one, can you run it for me on pocketpal or chatterUI and report the performance (t/s for both prompt processing and inference)? Please also report your phone model such that I can link GPU GFLOPS and memory bandwidth to the performance.&lt;/p&gt; &lt;p&gt;Thanks a lot in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T10:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjh4c</id>
    <title>GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy.</title>
    <updated>2025-10-04T03:30:30+00:00</updated>
    <author>
      <name>/u/Aiochedolor</name>
      <uri>https://old.reddit.com/user/Aiochedolor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt; &lt;img alt="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." src="https://external-preview.redd.it/yP0CnjxBFJCXTVacHixSvy4H_F7MTnOAVtKcV29Lggk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c413349140863192c8413b0f7b8e7f32ec48822c" title="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aiochedolor"&gt; /u/Aiochedolor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxyw0a</id>
    <title>My janky way of getting 2 GPUs into my rig</title>
    <updated>2025-10-04T16:43:54+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxyw0a/my_janky_way_of_getting_2_gpus_into_my_rig/"&gt; &lt;img alt="My janky way of getting 2 GPUs into my rig" src="https://b.thumbs.redditmedia.com/jkdZ5UrqJbSObpbKcvIkQyUQH6EX2nyYW9z8Y3R7rkQ.jpg" title="My janky way of getting 2 GPUs into my rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had forgotten I had a second power supply from when I upgraded my rig, and realized that I had a second GPU that I had upgraded from. RX 6800 16GB. so I bought a tool to make it possible to use both power supplies, and it’s working fine in LM Studio. Now to try it in Ollama. And if I have to, vLLM is next &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxyw0a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxyw0a/my_janky_way_of_getting_2_gpus_into_my_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxyw0a/my_janky_way_of_getting_2_gpus_into_my_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T16:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxr4gu</id>
    <title>Smartest model to run on 5090?</title>
    <updated>2025-10-04T11:08:10+00:00</updated>
    <author>
      <name>/u/eCityPlannerWannaBe</name>
      <uri>https://old.reddit.com/user/eCityPlannerWannaBe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the largest model I should run on 5090 for reasoning? E.g. GLM 4.6 - which version is ideal for one 5090?&lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eCityPlannerWannaBe"&gt; /u/eCityPlannerWannaBe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T11:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjhnj</id>
    <title>Behold, the jankiest setup ever</title>
    <updated>2025-10-04T03:31:15+00:00</updated>
    <author>
      <name>/u/T-VIRUS999</name>
      <uri>https://old.reddit.com/user/T-VIRUS999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt; &lt;img alt="Behold, the jankiest setup ever" src="https://b.thumbs.redditmedia.com/twOOoKU5XbRq6uFRGfXj_XqIEzieTWVvhWE3zg-T_qA.jpg" title="Behold, the jankiest setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I plan to get an open test bench, after I get my second P40 in a week or two (which will fit nicely on the other side of that fan) &lt;/p&gt; &lt;p&gt;Performance is as shown, Qwen 3 32B Q4 5.9T/sec&lt;/p&gt; &lt;p&gt;The fan is one of those stupidly powerful Delta electronics server fans that pushes out like 250cfm, so I needed to add a PWM controller to slow it down, and it wouldn't run without that giant capacitor, and it's powered by a Li-ion battery instead of the PSU (for now) &lt;/p&gt; &lt;p&gt;It's not stable at all, the whole system BSODs if a program tries to query the GPU while something else is using it (such as if I try to run GPUZ while LM Studio is running), but if only 1 thing touches the GPU at a time, it works &lt;/p&gt; &lt;p&gt;It has a Ryzen 5 5500GT, 16GB of DDR4, a 1000w PSU, a 512GB SSD, and 1 Nvidia P40 (soon to be 2) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T-VIRUS999"&gt; /u/T-VIRUS999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxjhnj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxtuiy</id>
    <title>Is MLX in itself somehow making the models a little bit different / more "stupid"?</title>
    <updated>2025-10-04T13:21:20+00:00</updated>
    <author>
      <name>/u/CBW1255</name>
      <uri>https://old.reddit.com/user/CBW1255</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an MBP M4 128GB RAM.&lt;/p&gt; &lt;p&gt;I run LLMs using LMStudio.&lt;br /&gt; I (nearly) always let LMStudio decide on the temp and other params.&lt;/p&gt; &lt;p&gt;I simply load models and use the chat interface or use them directly from code via the local API.&lt;/p&gt; &lt;p&gt;As a Mac user, I tend to go for the MLX versions of models since they are generally faster than GGUF for Macs.&lt;br /&gt; However, I find myself, now and then, testing the GGUF equivalent of the same model and it's slower but very often presents better solutions and is &amp;quot;more exact&amp;quot;.&lt;/p&gt; &lt;p&gt;I'm writing this to see if anyone else is having the same experience?&lt;/p&gt; &lt;p&gt;Please note that there's no &amp;quot;proof&amp;quot; or anything remotely scientific behind this question. It's just my feeling and I wanted to check if some of you who use MLX have witnessed something simliar.&lt;/p&gt; &lt;p&gt;In fact, it could very well be that I'm expected to do / tweak something that I'm not currently doing. Feel free to bring forward suggestions on what I might be doing wrong. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CBW1255"&gt; /u/CBW1255 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxtuiy/is_mlx_in_itself_somehow_making_the_models_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxtuiy/is_mlx_in_itself_somehow_making_the_models_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxtuiy/is_mlx_in_itself_somehow_making_the_models_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T13:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxvlm8</id>
    <title>Comparison between Qwen-Image, HunyuanImage 2.1, HunyuanImage 3.0</title>
    <updated>2025-10-04T14:33:52+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"&gt; &lt;img alt="Comparison between Qwen-Image, HunyuanImage 2.1, HunyuanImage 3.0" src="https://b.thumbs.redditmedia.com/vScmLJCz-SW6HqFktT-PwqhUvVAzHRTvQosO2HoTFOU.jpg" title="Comparison between Qwen-Image, HunyuanImage 2.1, HunyuanImage 3.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Couple of days ago i asked about the difference between the archticture in HunyuanImage 2.1 and HunyuanImage 3.0 and which is better and as you may have geussed nobody helped me. so, i decided to compare between the three myself and this is the results i got.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1w6bgzguu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2f963da35cfb954942e83f650689ada0964261"&gt;https://preview.redd.it/1w6bgzguu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2f963da35cfb954942e83f650689ada0964261&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tq2boe8xu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a15d14c86c89e7989698937e2145cee8aef97770"&gt;https://preview.redd.it/tq2boe8xu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a15d14c86c89e7989698937e2145cee8aef97770&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ud9zf60v3tf1.png?width=1313&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e40288150bb9aaa070d9c85cee386a25eedaf266"&gt;https://preview.redd.it/3ud9zf60v3tf1.png?width=1313&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e40288150bb9aaa070d9c85cee386a25eedaf266&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7sk97114v3tf1.png?width=1507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49870261ef6119681213b414f41243cae2bf567b"&gt;https://preview.redd.it/7sk97114v3tf1.png?width=1507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49870261ef6119681213b414f41243cae2bf567b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6e1vr068v3tf1.png?width=1544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cfbd2e84d636a685c070a3408a88d48e9b744e5"&gt;https://preview.redd.it/6e1vr068v3tf1.png?width=1544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cfbd2e84d636a685c070a3408a88d48e9b744e5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on my assessment i would rank them like this:&lt;br /&gt; 1. &lt;strong&gt;HunyuanImage 3.0&lt;/strong&gt;&lt;br /&gt; 2. Qwen-Image,&lt;br /&gt; 3. HunyuanImage 2.1&lt;/p&gt; &lt;p&gt;Hope someone finds this use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqabe</id>
    <title>Awesome Local LLM Speech-to-Speech Models &amp; Frameworks</title>
    <updated>2025-10-04T10:19:27+00:00</updated>
    <author>
      <name>/u/tleyden</name>
      <uri>https://old.reddit.com/user/tleyden</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"&gt; &lt;img alt="Awesome Local LLM Speech-to-Speech Models &amp;amp; Frameworks" src="https://external-preview.redd.it/sv2a4YrAVR9g08yOa0AOrBrIqErmKKSQAYIjNTCx_eI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1770300a09702157f67115321b5768d01848a3a" title="Awesome Local LLM Speech-to-Speech Models &amp;amp; Frameworks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did some digging into speech-to-speech models/frameworks for a project recently and ended up with a pretty comprehensive list. Figured I'd drop it here in case it helps anyone else avoid going down the same rabbit hole. &lt;/p&gt; &lt;p&gt;What made the cut:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has &lt;strong&gt;LLM integration&lt;/strong&gt; (built-in or via modules)&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;full speech-to-speech&lt;/strong&gt; pipeline, not just STT or TTS alone&lt;/li&gt; &lt;li&gt;Works &lt;strong&gt;locally/self-hosted&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Had to trim quite a bit to keep this readable, but the full list with more details is on GitHub at &lt;a href="https://github.com/tleyden/awesome-llm-speech-to-speech"&gt;tleyden/awesome-llm-speech-to-speech&lt;/a&gt;. PRs welcome if you spot anything wrong or missing! &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Project&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;LLM + Tool Calling&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Platforms&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/kyutai-labs/unmute"&gt;&lt;strong&gt;Unmute.sh&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Works with any local LLM · Tool calling not yet but planned&lt;/td&gt; &lt;td align="left"&gt;Linux only&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/fixie-ai/ultravox"&gt;&lt;strong&gt;Ultravox (Fixie)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;Hybrid (audio-native LLM + ASR + TTS)&lt;/td&gt; &lt;td align="left"&gt;Uses Llama/Mistral/Gemma · Full tool-calling via backend LLM&lt;/td&gt; &lt;td align="left"&gt;Windows / Linux&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/KoljaB/RealtimeVoiceChat"&gt;&lt;strong&gt;RealtimeVoiceChat&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Pluggable LLM (local or remote) · Likely supports tool calling&lt;/td&gt; &lt;td align="left"&gt;Linux recommended&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/Lex-au/Vocalis"&gt;&lt;strong&gt;Vocalis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Apache-2&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Fine-tuned LLaMA-3-8B-Instruct · Tool calling via backend LLM&lt;/td&gt; &lt;td align="left"&gt;macOS / Windows / Linux (runs on Apple Silicon)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models?ref=producthunt"&gt;&lt;strong&gt;LFM2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;End-to-End&lt;/td&gt; &lt;td align="left"&gt;Built-in LLM (E2E) · Native tool calling&lt;/td&gt; &lt;td align="left"&gt;Windows / Linux&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/gpt-omni/mini-omni2"&gt;&lt;strong&gt;Mini-omni2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;End-to-End&lt;/td&gt; &lt;td align="left"&gt;Built-in Qwen2 LLM · Tool calling TBD&lt;/td&gt; &lt;td align="left"&gt;Cross-platform&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/pipecat-ai/pipecat"&gt;&lt;strong&gt;Pipecat&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Pluggable LLM, ASR, TTS · Explicit tool-calling support&lt;/td&gt; &lt;td align="left"&gt;Windows / macOS / Linux / iOS / Android&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“Cascading” = modular ASR → LLM → TTS&lt;/li&gt; &lt;li&gt;“E2E” = end-to-end LLM that directly maps speech-to-speech&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tleyden"&gt; /u/tleyden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tleyden/awesome-llm-speech-to-speech"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T10:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxi82t</id>
    <title>Why do private companies release open source models?</title>
    <updated>2025-10-04T02:26:58+00:00</updated>
    <author>
      <name>/u/desudesu15</name>
      <uri>https://old.reddit.com/user/desudesu15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love open source models. I feel they are an alternative for general knowledge, and since I started in this world, I stopped paying for subscriptions and started running models locally.&lt;/p&gt; &lt;p&gt;However, I don't understand the business model of companies like OpenAI launching an open source model. &lt;/p&gt; &lt;p&gt;How do they make money by launching an open source model? &lt;/p&gt; &lt;p&gt;Isn't it counterproductive to their subscription model?&lt;/p&gt; &lt;p&gt;Thank you, and forgive my ignorance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desudesu15"&gt; /u/desudesu15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxv3ke</id>
    <title>It's alive!</title>
    <updated>2025-10-04T14:14:11+00:00</updated>
    <author>
      <name>/u/Illustrious-Dot-6888</name>
      <uri>https://old.reddit.com/user/Illustrious-Dot-6888</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"&gt; &lt;img alt="It's alive!" src="https://a.thumbs.redditmedia.com/pFXC-TeVXMXtFoQu77PDa1p-WrHJPId5U0wx7KqN1D8.jpg" title="It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The H in Granite 4.0-h stands for hilarious!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7y03utomr3tf1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3aaaf875911e3a123e0651758a1d1a077225178e"&gt;https://preview.redd.it/7y03utomr3tf1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3aaaf875911e3a123e0651758a1d1a077225178e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Dot-6888"&gt; /u/Illustrious-Dot-6888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjzbn</id>
    <title>Distributed Inference over wifi with 8x 3090 egpus performance</title>
    <updated>2025-10-04T03:57:59+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I smoked some really good weed recently and decided it was a good idea to buy more 3090s.&lt;/p&gt; &lt;p&gt;Naturally I didn't want to use a real build with server parts, put 8 3090s in one build on home depot racks? No thanks I'm lazy.&lt;/p&gt; &lt;p&gt;I got 4 3090 egpus from a guy on facebook. He's cool, sold them to me for 650 each with the egpu. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD"&gt;https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD&lt;/a&gt; &amp;lt;--- these are the EGPUs&lt;/p&gt; &lt;p&gt;Then I got 4 other random 3090s of different brands and put them in 3 spare Pcs I have lying around.&lt;/p&gt; &lt;p&gt;Node #1&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z390 Prime&lt;/li&gt; &lt;li&gt;9900K&lt;/li&gt; &lt;li&gt;64gb of DDR4&lt;/li&gt; &lt;li&gt;3090 (duh)&lt;/li&gt; &lt;li&gt;850W.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MSI Unify ITX z690&lt;/li&gt; &lt;li&gt;12400K&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;3090 (duh) &lt;/li&gt; &lt;li&gt;650W&lt;/li&gt; &lt;li&gt;2X 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #3 (Host)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z790 Maximus Hero&lt;/li&gt; &lt;li&gt;13700k&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;1200W PSU&lt;/li&gt; &lt;li&gt;2x 3090s &lt;/li&gt; &lt;li&gt;2x 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I ran all of it over VLLM with Ray to distribute the load. It's connected over Wifi, I got a good router so speed is about only 10% slower than ethernet from across the house. For now it's all pipeline parallel until the parts arrive then I'll do a 2 node system with 4 gpu each.&lt;/p&gt; &lt;p&gt;&lt;a href="https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/"&gt;https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/&lt;/a&gt; &amp;lt;--- my router(s).&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;At 128k context limit running GLM 4.5 Air AWQ 8 bit (that's Q8 for you gguf folks)&lt;/p&gt; &lt;p&gt;I get 5500 tokens/s prompt processing and 24 tokens a second for a 50k~ ish token prompt. &lt;/p&gt; &lt;p&gt;It works great over Roo.&lt;/p&gt; &lt;p&gt;Ray has a very annoying overhead cost so just assume that each system has like 1gb less vram. Running all my node in headless helps alot too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwx1rx</id>
    <title>The most important AI paper of the decade. No debate</title>
    <updated>2025-10-03T11:55:32+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt; &lt;img alt="The most important AI paper of the decade. No debate" src="https://preview.redd.it/d2rcvb6nyvsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0051a67e9886e507e2b0a35679f4d469050fda91" title="The most important AI paper of the decade. No debate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2rcvb6nyvsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ny022j</id>
    <title>Open source text-to-image Hunyuan 3.0 by Tencent is now #1 in LMArena, Beating proprietary models like Nano Banana and SeeDream 4 for the first time</title>
    <updated>2025-10-04T17:29:59+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ny022j/open_source_texttoimage_hunyuan_30_by_tencent_is/"&gt; &lt;img alt="Open source text-to-image Hunyuan 3.0 by Tencent is now #1 in LMArena, Beating proprietary models like Nano Banana and SeeDream 4 for the first time" src="https://preview.redd.it/whxcmf68r4tf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e57540eaf50e0cf82f21f3faa90ba8b1008ee6c7" title="Open source text-to-image Hunyuan 3.0 by Tencent is now #1 in LMArena, Beating proprietary models like Nano Banana and SeeDream 4 for the first time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/whxcmf68r4tf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ny022j/open_source_texttoimage_hunyuan_30_by_tencent_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ny022j/open_source_texttoimage_hunyuan_30_by_tencent_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T17:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxv7x6</id>
    <title>Performance of GLM 4.6 Q3_K_S on 6x MI50</title>
    <updated>2025-10-04T14:18:58+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt; &lt;img alt="Performance of GLM 4.6 Q3_K_S on 6x MI50" src="https://external-preview.redd.it/JsfroVEhCwDAyGkCRZ2-n87DtLempLcI758DM9LAUaU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=679e856cb0c9baa56ff4650ed899284d38a0b924" title="Performance of GLM 4.6 Q3_K_S on 6x MI50" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last night I downloaded the latest GLM 4.6 GGUFs from &lt;a href="https://huggingface.co/unsloth/GLM-4.6-GGUF"&gt;unsloth/GLM-4.6-GGUF · Hugging Face&lt;/a&gt;. I chose Q3_K_S since it was the best size allowing for full context on six AMD Instinct MI50 32gb (192gb). I also took the opportunity to download and rebuild the latest llama.cpp. &lt;strong&gt;I was pleasantly surprised by the 38% lift in text generation and over 200% increase in prompt processing over the previous build.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My questions for the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Would a Vulkan build outperform the current rocm-6.3.4 build?&lt;/li&gt; &lt;li&gt;Is my performance optimal given the hardware?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;/llama.cpp.rocm.20050902$ git rev-parse HEAD 3de008208b9b8a33f49f979097a99b4d59e6e521 srv params_from_: Chat format: Content-only slot launch_slot_: id 0 | task 2449 | processing task slot update_slots: id 0 | task 2449 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 2204 slot update_slots: id 0 | task 2449 | kv cache rm [4, end) slot update_slots: id 0 | task 2449 | prompt processing progress, n_past = 2052, n_tokens = 2048, progress = 0.929220 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot update_slots: id 0 | task 2449 | kv cache rm [2052, end) slot update_slots: id 0 | task 2449 | prompt processing progress, n_past = 2204, n_tokens = 152, progress = 0.998185 slot update_slots: id 0 | task 2449 | prompt done, n_past = 2204, n_tokens = 152 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot release: id 0 | task 2449 | stop processing: n_past = 2629, truncated = 0 slot print_timing: id 0 | task 2449 | prompt eval time = 111295.11 ms / 2200 tokens ( 50.59 ms per token, 19.77 tokens per second) eval time = 62451.95 ms / 426 tokens ( 146.60 ms per token, 6.82 tokens per second) total time = 173747.06 ms / 2626 tokens slot launch_slot_: id 0 | task 2451 | processing task slot update_slots: id 0 | task 2451 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 2280 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 2451 | kv cache rm [7, end) slot update_slots: id 0 | task 2451 | prompt processing progress, n_past = 2055, n_tokens = 2048, progress = 0.898246 slot update_slots: id 0 | task 2451 | kv cache rm [2055, end) slot update_slots: id 0 | task 2451 | prompt processing progress, n_past = 2280, n_tokens = 225, progress = 0.996930 slot update_slots: id 0 | task 2451 | prompt done, n_past = 2280, n_tokens = 225 slot release: id 0 | task 2451 | stop processing: n_past = 2869, truncated = 0 slot print_timing: id 0 | task 2451 | prompt eval time = 117166.76 ms / 2273 tokens ( 51.55 ms per token, 19.40 tokens per second) eval time = 88855.45 ms / 590 tokens ( 150.60 ms per token, 6.64 tokens per second) total time = 206022.21 ms / 2863 tokens slot launch_slot_: id 0 | task 2513 | processing task slot update_slots: id 0 | task 2513 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 2165 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 2513 | kv cache rm [8, end) slot update_slots: id 0 | task 2513 | prompt processing progress, n_past = 2056, n_tokens = 2048, progress = 0.945958 slot update_slots: id 0 | task 2513 | kv cache rm [2056, end) slot update_slots: id 0 | task 2513 | prompt processing progress, n_past = 2165, n_tokens = 109, progress = 0.996305 slot update_slots: id 0 | task 2513 | prompt done, n_past = 2165, n_tokens = 109 slot release: id 0 | task 2513 | stop processing: n_past = 2446, truncated = 0 slot print_timing: id 0 | task 2513 | prompt eval time = 109925.11 ms / 2157 tokens ( 50.96 ms per token, 19.62 tokens per second) eval time = 40961.53 ms / 282 tokens ( 145.25 ms per token, 6.88 tokens per second) total time = 150886.64 ms / 2439 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;-------------------------------------&lt;/p&gt; &lt;pre&gt;&lt;code&gt;/llama.cpp.rocm.20251004$ git rev-parse HEAD 898acba6816ad23b6a9491347d30e7570bffadfd srv params_from_: Chat format: Content-only slot get_availabl: id 0 | task -1 | selected slot by LRU, t_last = -1 slot launch_slot_: id 0 | task 0 | processing task slot update_slots: id 0 | task 0 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 38 slot update_slots: id 0 | task 0 | n_past = 0, memory_seq_rm [0, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 38, n_tokens = 38, progress = 1.000000 slot update_slots: id 0 | task 0 | prompt done, n_past = 38, n_tokens = 38 slot release: id 0 | task 0 | stop processing: n_past = 2851, truncated = 0 slot print_timing: id 0 | task 0 | prompt eval time = 4300.19 ms / 38 tokens ( 113.16 ms per token, 8.84 tokens per second) eval time = 323842.83 ms / 2814 tokens ( 115.08 ms per token, 8.69 tokens per second) total time = 328143.02 ms / 2852 tokens srv update_slots: all slots are idle srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot get_availabl: id 0 | task 0 | selected slot by LRU, t_last = 2724371263681 slot launch_slot_: id 0 | task 2815 | processing task slot update_slots: id 0 | task 2815 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 1734 slot update_slots: id 0 | task 2815 | n_past = 4, memory_seq_rm [4, end) slot update_slots: id 0 | task 2815 | prompt processing progress, n_past = 1734, n_tokens = 1730, progress = 0.997693 slot update_slots: id 0 | task 2815 | prompt done, n_past = 1734, n_tokens = 1730 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot release: id 0 | task 2815 | stop processing: n_past = 2331, truncated = 0 slot print_timing: id 0 | task 2815 | prompt eval time = 27189.85 ms / 1730 tokens ( 15.72 ms per token, 63.63 tokens per second) eval time = 70550.21 ms / 598 tokens ( 117.98 ms per token, 8.48 tokens per second) total time = 97740.06 ms / 2328 tokens slot get_availabl: id 0 | task 2815 | selected slot by LRU, t_last = 2724469122645 slot launch_slot_: id 0 | task 3096 | processing task slot update_slots: id 0 | task 3096 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 1810 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 3096 | n_past = 7, memory_seq_rm [7, end) slot update_slots: id 0 | task 3096 | prompt processing progress, n_past = 1810, n_tokens = 1803, progress = 0.996133 slot update_slots: id 0 | task 3096 | prompt done, n_past = 1810, n_tokens = 1803 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot release: id 0 | task 3096 | stop processing: n_past = 2434, truncated = 0 slot print_timing: id 0 | task 3096 | prompt eval time = 27702.48 ms / 1803 tokens ( 15.36 ms per token, 65.08 tokens per second) eval time = 74080.73 ms / 625 tokens ( 118.53 ms per token, 8.44 tokens per second) total time = 101783.21 ms / 2428 tokens slot get_availabl: id 0 | task 3096 | selected slot by LRU, t_last = 2724570907348 slot launch_slot_: id 0 | task 3416 | processing task slot update_slots: id 0 | task 3416 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 1695 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 3416 | n_past = 8, memory_seq_rm [8, end) slot update_slots: id 0 | task 3416 | prompt processing progress, n_past = 1695, n_tokens = 1687, progress = 0.995280 slot update_slots: id 0 | task 3416 | prompt done, n_past = 1695, n_tokens = 1687 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;-------------------------------------&lt;/p&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp.rocm.20251004/build/bin/llama-server --model ~/models/GLM-4.6-Q3_K_S-00001-of-00004.gguf --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 94 --temp 0.6 --ctx-size 131072 --device ROCm0,ROCm1,ROCm2,ROCm3,ROCm4,ROCm5 --tensor-split 9,8,8,8,9,8 --host 0.0.0.0 --jinja --alias GLM-4.6 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hyiod0epr3tf1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0e2313e04763913efe8f0c15436c59981e3e0af"&gt;https://preview.redd.it/hyiod0epr3tf1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0e2313e04763913efe8f0c15436c59981e3e0af&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxrssl</id>
    <title>This is pretty cool</title>
    <updated>2025-10-04T11:45:15+00:00</updated>
    <author>
      <name>/u/wowsers7</name>
      <uri>https://old.reddit.com/user/wowsers7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"&gt; &lt;img alt="This is pretty cool" src="https://external-preview.redd.it/fWjpQVd5VjUiyz85oEUZ3MBMlQycdcPTlMPYbRWoE6A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f573aa728fb79af617ec9e24df900618595c6abb" title="This is pretty cool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less"&gt;https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/huawei-csl/SINQ/blob/main/README.md"&gt;https://github.com/huawei-csl/SINQ/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wowsers7"&gt; /u/wowsers7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ/blob/main/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T11:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxs8tr</id>
    <title>GLM 4.6 Makes Incredible Front End Design with 2 prompts</title>
    <updated>2025-10-04T12:08:05+00:00</updated>
    <author>
      <name>/u/dev_is_active</name>
      <uri>https://old.reddit.com/user/dev_is_active</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"&gt; &lt;img alt="GLM 4.6 Makes Incredible Front End Design with 2 prompts" src="https://external-preview.redd.it/HRx3NTzzZMOIdtM0oRmpT2rIW9OnDaS9AE7D0C1FPSc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d08204e2ea136e84ee75e08ffa737f0e7653aea" title="GLM 4.6 Makes Incredible Front End Design with 2 prompts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been playing with GLM 4.6, I've also implemented it inside Claud Code, and I'll be doing a new video on how to set up GLM 4.6 in Cloud Code, but I really wanted to show everybody how great z ai is with front end design.&lt;/p&gt; &lt;p&gt;In this video I take a screenshot of a website and I do one simple prompt and it kicks out a good design and then I ask it to enhance it, and then it turns it into an incredible design, you can watch it here&lt;/p&gt; &lt;p&gt;Would love to know what you think and if any of you are using GLM in Claude Code yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dev_is_active"&gt; /u/dev_is_active &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/AvHsytH-K84"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:08:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxhfcq</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking are here</title>
    <updated>2025-10-04T01:46:34+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" src="https://external-preview.redd.it/NiG8S3h1NHl5GO5rtLVQY3YBzVNQNV4xvZR0hFbk_vE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88770649ad1f1c425c3a22e1502363d18f9727dc" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7"&gt;https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxx9r9</id>
    <title>My mildly janky setup</title>
    <updated>2025-10-04T15:39:59+00:00</updated>
    <author>
      <name>/u/sergeysi</name>
      <uri>https://old.reddit.com/user/sergeysi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxx9r9/my_mildly_janky_setup/"&gt; &lt;img alt="My mildly janky setup" src="https://b.thumbs.redditmedia.com/oZOof3eQ0QdiEkmnB50wkQnbUPXB2Rhd5m9R6iwW_dI.jpg" title="My mildly janky setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sergeysi"&gt; /u/sergeysi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxx9r9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxx9r9/my_mildly_janky_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxx9r9/my_mildly_janky_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T15:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxshw2</id>
    <title>IBM granite 4.0-h-tiny leads the way for extra small MoEs</title>
    <updated>2025-10-04T12:20:23+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"&gt; &lt;img alt="IBM granite 4.0-h-tiny leads the way for extra small MoEs" src="https://preview.redd.it/nlkf3btz73tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1368a9b2fc469f876a31fef05020119815deb818" title="IBM granite 4.0-h-tiny leads the way for extra small MoEs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope the trend for those MoEs carries on. Normies with laverage laptops will soon be able to use decent models with little ressources. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nlkf3btz73tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxztlx</id>
    <title>gpt-oss 120B is running at 20t/s with $500 AMD M780 iGPU mini PC and 96GB DDR5 RAM</title>
    <updated>2025-10-04T17:20:52+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone here is talking about how great AMD Ryzen AI MAX+ 395 128GB is. But mini PCs with those specs cost almost $2k. I agree the specs are amazing but the price is way high for most local LLM users. I wondered if there was any alternative. My primary purpose was to run gpt-oss 120B at readable speeds.&lt;/p&gt; &lt;p&gt;I searched for mini PCs that supported removable DDR5 sticks and had PCIE4.0 slots for future external GPU upgrades. I focused on AMD CPU/iGPU based setups since Intel specs were not as performant as AMD ones. The iGPU that came before AI MAX 395 (8060S iGPU) was AMD Radeon 890M (still RDNA3.5). Mini PCs with 890M iGPU were still expensive. The cheapest I could find was Minisforum EliteMini AI370 (32GB RAM with 1TB SSD) for $600. Otherwise, these AI 370 based mini PCs are still going for around $1000. However, that was still expensive since I would need to purchase more RAM to run gpt-oss 120B.&lt;/p&gt; &lt;p&gt;Next, I looked at previous generation of AMD iGPUs which are based on RDNA3. I found out AMD Radeon 780M iGPU based mini PC start from $300 for barebone setup (no RAM and no SSD). 780M iGPU based mini PCs are 2x times cheaper and is only 20% behind 890M performance metrics. This was perfect! I checked many online forums if there was ROCm support for 780M. Even though there is no official support for 780M, I found out there were multiple repositories that added ROCm support for 780M (gfx1103) (e.g. arch linux - &lt;a href="https://aur.archlinux.org/packages/rocwmma-gfx1103"&gt;https://aur.archlinux.org/packages/rocwmma-gfx1103&lt;/a&gt; ; Windows - &lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU"&gt;https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU&lt;/a&gt; ; and Ubuntu - &lt;a href="https://github.com/lamikr/rocm_sdk_builder"&gt;https://github.com/lamikr/rocm_sdk_builder&lt;/a&gt; ). Then I bought MINISFORUM UM870 Slim Mini PC barebone for $300 and 2x48GB Crucial DDR5 5600Mhz for $200. I already had 2TB SSD, so I paid $500 in total for this setup.&lt;/p&gt; &lt;p&gt;There was no guidelines on how to install ROCm or allocate most of the RAM for iGPU for 780M. So, I did the research and this is how I did it.&lt;/p&gt; &lt;p&gt;ROCm. The default ROCm 6.4.4 official installation does not work. rocm-smi does not show the iGPU. I installed 6.4.1 and it recognized the iGPU but still gfx1103 tensiles were missing. Overriding HSA_OVERRIDE_GFX_VERSION=11.0.0 did not work. Last working version that recognized this iGPU was ROCm 6.1 based on some posts. But I stopped trying here. Potentially, I could compile and build ROCM SDK Builder 6.1.2 (from lamikr's repo above) but I did not want to spend 4 hours for that.&lt;/p&gt; &lt;p&gt;Then I found out there is a repo called lemonade that ships llama cpp with rocm as release builds. Here: &lt;a href="https://github.com/aigdat/llamacpp-rocm/releases/latest"&gt;https://github.com/aigdat/llamacpp-rocm/releases/latest&lt;/a&gt; . I downloaded gfx110x version e.g. &lt;a href="http://llama-b1068-ubuntu-rocm-gfx110X-x64.zip"&gt;llama-b1068-ubuntu-rocm-gfx110X-x64.zip&lt;/a&gt; . Extracted it. Ran llama-bench with llama2-7b Q4_0 to check its speed and it was working! I was getting 20t/s for it. Not bad! But still I could load gpt-oss 120B. Ubuntu was crashing when I tried to load that model.&lt;/p&gt; &lt;p&gt;Then I searched for iGPU memory allocation. I found this amazing article about iGPU memory allocation (it is called GTT memory): &lt;a href="https://strixhalo-homelab.d7.wtf/AI/AI-Capabilities-Overview#memory-limits"&gt;https://strixhalo-homelab.d7.wtf/AI/AI-Capabilities-Overview#memory-limits&lt;/a&gt; . In short, we create a conf file in modprobe.d folder.&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nano /etc/modprobe.d/amdgpu_llm_optimized.conf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;then add the following lines:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;options amdgpu gttsize=89000 ## 89GB allocated to GTT options ttm pages_limit=23330816 options ttm page_pool_size=23330816 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In grub, we need to also add edit the line that starts with GRUB_CMDLINE_LINUX_DEFAULT (add to the end if it already has some text):&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nano /etc/default/grub&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet splash amd_iommu=off transparent_hugepage=always numa_balancing=disable amdttm.pages_limit=23330816 amdttm.page_pool_size=23330816&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then update grub with above changes.&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo update-grub&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Reboot the mini PC.&lt;/p&gt; &lt;p&gt;Also, minimize the VRAM size from the bios settings to 1GB or 512MB.&lt;/p&gt; &lt;p&gt;You can check the GTT size with this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo dmesg | egrep &amp;quot;amdgpu: .*memory&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You should see something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ 3.4] amdgpu 0000:c4:00.0: amdgpu: amdgpu: 1024M of VRAM memory ready [ 3.4] amdgpu 0000:c4:00.0: amdgpu: amdgpu: 89000M of GTT memory ready. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;lemonade compiled llama cpp with ROCm was giving me 18t/s TG and 270t/s PP for gpt-oss 120B in short context (pp512, tg128) but in long context TG suffered (8k context) and I was getting 6t/s. So, I continued with vulkan.&lt;/p&gt; &lt;p&gt;I installed RADV vulkan.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install vulkan-tools libvulkan-dev mesa-vulkan-drivers &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Downloaded the latest release build from llama cpp for vulkan in ubuntu: &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And finally, I was getting great numbers that aligned with dual DDR5 5600Mhz speeds (~80GB/s).&lt;/p&gt; &lt;p&gt;Enough talking. Here are some metrics. &lt;/p&gt; &lt;p&gt;ROCM with gpt-oss 120B mxfp4&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ml-ai@ai-mini-pc:/media/ml-ai/wd_2tb/llama-b1066-ubuntu-rocm-gfx110X-x64$ HSA_OVERRIDE_GFX_VERSION=11.0.0 ./llama-bench -m /media/ml-ai/wd_2tb/llm_models/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf -mmp 0 -fa 1 &amp;amp;&amp;amp; HSA_OVERRIDE_GFX_VERSION=11.0.0 ./llama-bench -m /media/ml-ai/wd_2tb/llm_models/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf -mmp 0 -fa 1 -d 8192 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | pp512 | 269.28 ± 1.59 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | tg128 | 18.75 ± 0.01 | build: 703f9e3 (1) ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | pp512 @ d8192 | 169.47 ± 0.70 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | tg128 @ d8192 | 6.76 ± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;VULKAN (RADV only) all with Flash attention enabled&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# qwen3moe 30B.A3B Q4_1 # llama cpp build: 128d522c (6686) # command used: ml-ai@ai-mini-pc:/media/ml-ai/wd_2tb/minipc/llama-b6686-bin-ubuntu-vulkan-x64$ ./build/bin/llama-bench -m /media/ml-ai/wd_2tb/llm_models/Qwen3-30B-A3B-Q4_1.gguf -mmp 0 -fa 1 &amp;amp;&amp;amp; ./build/bin/llama-bench -m /media/ml-ai/wd_2tb/llm_models/Qwen3-30B-A3B-Q4_1.gguf -mmp 0 -d 8192 -fa 1 | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | pp512 | 243.33 ± 0.92 | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | tg128 | 32.61 ± 0.07 | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | pp512 @ d8192 | 105.00 ± 0.14 | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | tg128 @ d8192 | 22.29 ± 0.08 | # gpt-oss-20b-GGUF | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | pp512 | 355.13 ± 2.79 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | tg128 | 28.08 ± 0.09 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | pp512 @ d8192 | 234.17 ± 0.34 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | tg128 @ d8192 | 24.86 ± 0.07 | # gpt-oss-120b-GGUF | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | pp512 | 137.60 ± 0.70 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | tg128 | 20.43 ± 0.01 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | pp512 @ d8192 | 106.22 ± 0.24 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | tg128 @ d8192 | 18.09 ± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I know ROCm support is not great but vulkan is better at text generation for most models (even though it is 2x slower for prompt processing than ROCm).&lt;/p&gt; &lt;p&gt;Mini PCs with 780M are great value and enables us to run large MoE models at acceptable speeds. Overall, this mini PC is more than enough for my daily LLM usage (mostly asking math/CS related questions, coding and brainstorming).&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T17:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxxeu1</id>
    <title>Why are AI labs in China not focused on creating new search engines?</title>
    <updated>2025-10-04T15:45:31+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxxeu1/why_are_ai_labs_in_china_not_focused_on_creating/"&gt; &lt;img alt="Why are AI labs in China not focused on creating new search engines?" src="https://preview.redd.it/4glawt4k84tf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b93923ea011b8fdf60fa2c2f9929d68ed20ad498" title="Why are AI labs in China not focused on creating new search engines?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4glawt4k84tf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxxeu1/why_are_ai_labs_in_china_not_focused_on_creating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxxeu1/why_are_ai_labs_in_china_not_focused_on_creating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T15:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxzx6t</id>
    <title>GPT-1 Thinking 2.6m coming soon</title>
    <updated>2025-10-04T17:24:39+00:00</updated>
    <author>
      <name>/u/Creative-Ad-2112</name>
      <uri>https://old.reddit.com/user/Creative-Ad-2112</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"&gt; &lt;img alt="GPT-1 Thinking 2.6m coming soon" src="https://preview.redd.it/2ln0mw87m4tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=409621b267ee5b6f05466d87ab6e4ace420dcc56" title="GPT-1 Thinking 2.6m coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took my GPT-1 from past posts, gave it roughly 600 million tokens from Nemotron's pretraining dataset.&lt;br /&gt; Then finetuned it on my own private thinking dataset, used sonnet 4.5 to help me come up with it, essentially to help the model 'see' the pattern of the thinking rather than just random q/a with thinking in it.&lt;br /&gt; Used a custom tokenizer trained from scratch.&lt;/p&gt; &lt;p&gt;As for the image attached, I did it for the lolz, hope you laughed but i did test it out on some of those metrics (simple ones lol). As for the model generation, here it is lolol.&lt;/p&gt; &lt;p&gt;[PROMPT] &amp;lt;|user|&amp;gt; What are two fun facts about octopuses? &amp;lt;|think|&amp;gt; &lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; What are two fun fact##s about o##ct##op##uses? &amp;lt;|think|&amp;gt; 1. The user is asking for a log##ical question. 2. The user is asking about a user is asking for the simple and is asking for a specific, and a 'F##P##I'##. 3. I should explain the answer. 2. I should explain a small process: 3. I should have a great step is to read. 4. I should provide a specific questions: a simple, but it's specific, the user is to be able to ask. &amp;lt;|assistant|&amp;gt; 1) They have three hea##rts and blue, copper-##based blood. 2) They can insta##ntly camou##flage by changing col##or and tex##ture, and their suck##ers can “tas##te” what they touch. &amp;lt;|eot|&amp;gt; &lt;/p&gt; &lt;p&gt;--- [PROMPT] &amp;lt;|user|&amp;gt; Write a haiku about rain. &amp;lt;|think|&amp;gt;&lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; W##rite a ha##ik##u about rain. &amp;lt;|think|&amp;gt; 1. The user wants of list a specific sentence. 2. I should include the user for the ##n common. I should explain the simple, a poem, and a simple, and understand##ing. &amp;lt;|eot|&amp;gt; &amp;lt;|assistant|&amp;gt; Rain whis##pers on roofs Pud##dles bloom in soft lamp##light Earth brea##thes, rin##sed anew. &amp;lt;|eot|&amp;gt; &lt;/p&gt; &lt;p&gt;--- [PROMPT] &amp;lt;|user|&amp;gt; Explain photosynthesis in one sentence. &amp;lt;|think|&amp;gt;&lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; Expl##ain photos##ynt##hesis in one sentence. &amp;lt;|think|&amp;gt; 1. The user is the user is asking about a simple for a explanation. The user is the question. 2. The user is ask##ing a great question is a clear, it's a specific example. 4. I will explain the best way for the answer, 'a##m##aking is asking for the sound and ask about the order. 2. The user is asking for a simple, for the common question. 3. I should explain the words: ##g##ood &amp;lt;|assistant|&amp;gt; Photo##syn##thesis is the pro##cess by which plants algae, and some bac##teria use sun##light to con##vert a water and car##bon dio##xide into a glu##cose, relea##sing a oxy##gen. &amp;lt;|eot|&amp;gt;&lt;/p&gt; &lt;p&gt;As you can see its pretty good for a 2 mil parameter. Now you might be wondering that something is up, what's the catch? Well, obviously I didn't use GPT-1, I used their original implementation, converted it to pytorch, and then added differential attention, along with sparse attention.&lt;br /&gt; But that is still not enough, which is why I introduce two variants of diff_attn. &lt;/p&gt; &lt;p&gt;[model] params=2,494,574&lt;br /&gt; [model] layer_types=['dense', 'diff_sparse', 'sparse', 'diff_dense', 'sparse', 'diff_sparse', 'dense', 'sparse', 'diff_dense', 'sparse', 'diff_sparse', 'dense', 'sparse', 'diff_sparse', 'diff_dense', 'dense'] &lt;/p&gt; &lt;p&gt;I have found this to be effective. I kept the GPT-1 like core, gave it moe (but didn't use moe in this model run btw), then I introduced it to these two diff attn and intertwined it with the others. &lt;/p&gt; &lt;p&gt;So is it GPT-1? Nope, it's GPT-1 like (for clarification), abs positioning and pre-lm instead of the modern day post-lm + RoPE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Ad-2112"&gt; /u/Creative-Ad-2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2ln0mw87m4tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T17:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxw08b</id>
    <title>Quite accurate</title>
    <updated>2025-10-04T14:50:10+00:00</updated>
    <author>
      <name>/u/Komarov_d</name>
      <uri>https://old.reddit.com/user/Komarov_d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"&gt; &lt;img alt="Quite accurate" src="https://preview.redd.it/9lms1idpy3tf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7e75e0118f3c3580e1e97fa7a4bda42ad4520a7" title="Quite accurate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Komarov_d"&gt; /u/Komarov_d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9lms1idpy3tf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
