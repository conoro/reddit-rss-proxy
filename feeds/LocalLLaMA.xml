<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-10T18:42:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oswo5v</id>
    <title>Qwen3-VL Now EXL3 Supported</title>
    <updated>2025-11-09T22:21:49+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt; &lt;img alt="Qwen3-VL Now EXL3 Supported" src="https://b.thumbs.redditmedia.com/SuI8cNmsMD7QQjM85nzVN4go04so8nc4q9t6e8mY17c.jpg" title="Qwen3-VL Now EXL3 Supported" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚ö†Ô∏è Requires &lt;a href="https://github.com/turboderp-org/exllamav3"&gt;ExLlamaV3 v0.0.13&lt;/a&gt; (or higher)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/turboderp/Qwen3-VL-8B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-8B-Instruct-exl3&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/Qwen3-VL-30B-A3B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-30B-A3B-Instruct-exl3&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/Qwen3-VL-32B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-32B-Instruct-exl3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0kso6yh3gb0g1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ed40413e383c31a0f466f26dbce1a01df4f6cb6"&gt;CatBench results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions? Ask here or in the&lt;a href="https://discord.gg/VbR8wQxf"&gt; exllama discord&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T22:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ostdcn</id>
    <title>Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore</title>
    <updated>2025-11-09T20:10:26+00:00</updated>
    <author>
      <name>/u/Previous_Nature_5319</name>
      <uri>https://old.reddit.com/user/Previous_Nature_5319</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"&gt; &lt;img alt="Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore" src="https://preview.redd.it/90im3um0fa0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1880277abb2e16deb196c79509aa47dbb7d349ae" title="Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache"&gt;https://github.com/airnsk/proxycache&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What this service is&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#what-this-service-is"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This service is a smart proxy in front of llama.cpp that makes long‚Äëcontext chat and IDE workflows much faster by managing llama.cpp slots, reusing cached context, and restoring saved caches from disk when needed. It speaks an OpenAI‚Äëcompatible Chat Completions API, so existing clients can connect without changes, including both streaming (SSE) and non‚Äëstream responses depending on request settings.&lt;/p&gt; &lt;h1&gt;Why it‚Äôs needed&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#why-its-needed"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp provides ‚Äúslots,‚Äù each holding a conversation‚Äôs KV cache so repeated requests with the same or very similar prefix can skip recomputing the whole prompt and continue from the first mismatching token, which dramatically cuts latency for large prompts. In real teams the number of users can easily exceed the number of available slots (e.g., 20 developers but only 4 slots), so naive routing causes random slot reuse and cache overwrites that waste time and GPU/CPU cycles. This proxy solves that by steering requests to the right slot, saving evicted caches to disk, and restoring them on demand, so long prompts don‚Äôt need to be recomputed from scratch each time.&lt;/p&gt; &lt;h1&gt;How requests are balanced and slots are chosen&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#how-requests-are-balanced-and-slots-are-chosen"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slots and heat: When a request lands in a slot and its cache is valid for reuse, the slot is considered ‚Äúhot,‚Äù and new requests won‚Äôt overwrite it if other options exist, preserving useful KV for future reuse.&lt;/li&gt; &lt;li&gt;Similarity matching: The proxy computes a fast, word‚Äëblock prefix similarity between the incoming conversation and existing hot slots, and only reuses a hot slot if the similarity meets a single ratio threshold (e.g., 85% of the shorter sequence), otherwise it rejects reuse to avoid polluting the hot cache with a weakly related prompt.&lt;/li&gt; &lt;li&gt;Free and cold first: If reuse is rejected, the proxy sends the request to a free slot or a cold slot (one not currently carrying a valuable hot cache), protecting high‚Äëvalue contexts from accidental overwrites under load.&lt;/li&gt; &lt;li&gt;Oldest when full: If there are no free or cold slots, the proxy picks the least‚Äërecently used slot and saves its current KV cache to disk before assigning the new request, ensuring nothing valuable is lost when the pool is exhausted.&lt;/li&gt; &lt;li&gt;Restore on demand: When a new request matches a cache that was previously saved, the proxy restores that cache into a free/cold/oldest slot and routes the request there, which takes seconds versus minutes for full prompt recomputation on long contexts, especially in IDE scenarios with 30‚Äì60k tokens.&lt;/li&gt; &lt;li&gt;Concurrency safety: Each slot is guarded with an async lock; if all are busy, the request waits for the first LRU slot to free, preventing race conditions and unintended cache overwrites during concurrent generation.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Save and restore from disk&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#save-and-restore-from-disk"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp‚Äôs HTTP server exposes slot save/restore; saving writes a cache file to the directory provided by --slot‚Äësave‚Äëpath, and restore loads by file basename (e.g., slotcache_.bin), which is exactly how this proxy persists and revives caches across requests and restarts. The proxy keeps small local .meta files describing cached prefixes for fast lookup, while llama.cpp owns the actual KV .bin files under --slot‚Äësave‚Äëpath for correctness and performance.&lt;/p&gt; &lt;h1&gt;Quick start&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#quick-start"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start llama.cpp ( &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt; ) with slots and a cache directory:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -m ./model.gguf -np 4 --slot-save-path /var/kvcache --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This enables the OpenAI‚Äëcompatible HTTP server, a pool of 4 slots, and a directory where slot KV caches are saved and restored by basename.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Run the proxy next to it:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/airnsk/proxycache.git cd proxycache python3 -m venv venv &amp;amp;&amp;amp; source venv/bin/activate &amp;amp;&amp;amp; pip install -r requirements.txt python3 proxycache.py # or: uvicorn app:app --host 0.0.0.0 --port 8081 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your clients should call the proxy‚Äôs /v1/chat/completions endpoint; the proxy will handle similarity, slot selection, save/restore, and streaming vs non‚Äëstreaming automatically.&lt;/p&gt; &lt;p&gt;If you run into issues using gpt-oss-20b with an IDE like Cline, follow these instructions: &lt;a href="https://www.reddit.com/r/CLine/comments/1mtcj2v/making_gptoss_20b_and_cline_work_together/"&gt;https://www.reddit.com/r/CLine/comments/1mtcj2v/making_gptoss_20b_and_cline_work_together/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Parameters&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#parameters"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLAMA_SERVER_URL: The llama.cpp server base URL, e.g., &lt;a href="http://127.0.0.1:8080/"&gt;http://127.0.0.1:8080&lt;/a&gt;, which must expose the OpenAI‚Äëcompatible chat completions endpoint.&lt;/li&gt; &lt;li&gt;SLOTS_COUNT: The number of server slots (should match llama.cpp -np) so the proxy can track and plan reuse/restore correctly under load.&lt;/li&gt; &lt;li&gt;SIMILARITY_MIN_RATIO: One similarity threshold (e.g., 0.85) controlling both active reuse and disk restore; if a match is below this ratio, the proxy will prefer a free/cold slot or restore instead of overwriting a hot slot.&lt;/li&gt; &lt;li&gt;MIN_PREFIX_* (chars/words/blocks): Requests below this size are treated as ‚Äúsmall‚Äù and steered to free/cold/oldest slots to avoid disturbing valuable hot caches used by large, long‚Äërunning prompts.&lt;/li&gt; &lt;li&gt;LOCAL_META_DIR and --slot-save-path: The proxy stores small .meta descriptors locally for fast candidate lookup, while llama.cpp reads/writes the real KV cache files under --slot‚Äësave‚Äëpath using basename in the HTTP API.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this boosts IDE and long‚Äëcontext productivity&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#why-this-boosts-ide-and-longcontext-productivity"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For 30‚Äì60k‚Äëtoken contexts typical in project‚Äëwide IDE assistants, recomputing a full prompt can take minutes, whereas restoring a previously cached context and continuing from the first mismatching token typically takes seconds on llama.cpp, dramatically improving iteration speed for large teams with limited slots.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Previous_Nature_5319"&gt; /u/Previous_Nature_5319 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90im3um0fa0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot2eqd</id>
    <title>[Research] 31 % perplexity drop on 8.4 M transformer model using a lightweight periodic regulator ‚Äî looking for replication on stronger GPUs</title>
    <updated>2025-11-10T02:42:30+00:00</updated>
    <author>
      <name>/u/freeky78</name>
      <uri>https://old.reddit.com/user/freeky78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I ran a controlled training experiment on an 8.4 M-parameter transformer model and observed a consistent **31 % perplexity reduction** compared to baseline after 2 000 steps.&lt;/p&gt; &lt;p&gt;üìä Full metrics &amp;amp; logs: &lt;a href="https://limewire.com/d/j7jDI#OceCXHWNhG"&gt;https://limewire.com/d/j7jDI#OceCXHWNhG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**Setup**&lt;/p&gt; &lt;p&gt;- Model: small LM (~8.4 M params)&lt;/p&gt; &lt;p&gt;- GPU: RTX 5070&lt;/p&gt; &lt;p&gt;- Optimizer: AdamW, lr = 2e-6, warmup = 200, grad-clip = 1.0&lt;/p&gt; &lt;p&gt;- Sequence = 256, batch = 8 √ó GA 4&lt;/p&gt; &lt;p&gt;- Seed = 41&lt;/p&gt; &lt;p&gt;- Modification: added a compact periodic regulator in the optimizer update (‚âà 0.07 % extra params)&lt;/p&gt; &lt;p&gt;**Result**&lt;/p&gt; &lt;p&gt;| Metric | Baseline | Regulated | Œî |&lt;/p&gt; &lt;p&gt;|---------|-----------|-----------|---|&lt;/p&gt; &lt;p&gt;| eval CE | 6.731 | 6.360 | ‚àí0.371 |&lt;/p&gt; &lt;p&gt;| eval PPL | 838.17 | **578.49 (‚àí31 %)** |&lt;/p&gt; &lt;p&gt;| stability Œ≤ | ‚Äî | 0.91 |&lt;/p&gt; &lt;p&gt;Same data, same seed, no architecture changes. &lt;/p&gt; &lt;p&gt;The effect is reproducible and stable.&lt;/p&gt; &lt;p&gt;**Why post here**&lt;/p&gt; &lt;p&gt;Looking for:&lt;/p&gt; &lt;p&gt;- community replication on larger GPUs (A100 / L40S / H100)&lt;/p&gt; &lt;p&gt;- discussion about scaling behaviour and scheduler-level interventions&lt;/p&gt; &lt;p&gt;- any pointers to similar experiments you may have seen&lt;/p&gt; &lt;p&gt;I‚Äôll share the Python scripts and configs (ready-to-run) with anyone who wants to test. &lt;/p&gt; &lt;p&gt;The full repo isn‚Äôt public yet but will follow once results are replicated.&lt;/p&gt; &lt;p&gt;Thanks for reading and for any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freeky78"&gt; /u/freeky78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot2eqd/research_31_perplexity_drop_on_84_m_transformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot2eqd/research_31_perplexity_drop_on_84_m_transformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot2eqd/research_31_perplexity_drop_on_84_m_transformer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T02:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot79n2</id>
    <title>I'm new to LLMs and just ran my first model. What LLM "wowed" you when you started out?</title>
    <updated>2025-11-10T07:10:06+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm brand new to the world of LLMs and finally took the plunge this week. I set up my first model and honestly, I'm hooked. There's something special about running this tech on my own machine and seeing it respond in real time.&lt;/p&gt; &lt;p&gt;Since I'm just starting out, I'd love to hear from this community:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What was the first LLM that truly &amp;quot;wowed&amp;quot; you?&lt;/strong&gt;&lt;br /&gt; Was it a particular model's creativity? Its speed? Its uncensored or unexpected responses? Or just the thrill of running it completely offline?&lt;/p&gt; &lt;p&gt;I'm looking for recommendations and stories to guide my next steps, and I'm sure other newcomers are too.&lt;/p&gt; &lt;p&gt;Thanks in advance, and I'm excited to join the conversation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot79n2/im_new_to_llms_and_just_ran_my_first_model_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot79n2/im_new_to_llms_and_just_ran_my_first_model_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot79n2/im_new_to_llms_and_just_ran_my_first_model_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T07:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgxs8</id>
    <title>Maxsun displays quad GPU and dual GPU workstations. Pricing TBD</title>
    <updated>2025-11-10T15:35:52+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otgxs8/maxsun_displays_quad_gpu_and_dual_gpu/"&gt; &lt;img alt="Maxsun displays quad GPU and dual GPU workstations. Pricing TBD" src="https://b.thumbs.redditmedia.com/Q8LqsDvfBBfysFwUmEk397yspQZzzx94ohTsreyqvdA.jpg" title="Maxsun displays quad GPU and dual GPU workstations. Pricing TBD" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nkalq1w98g0g1.png?width=1703&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96a2f25bd5e789bfa26524ab510eecddc6da027f"&gt;https://preview.redd.it/nkalq1w98g0g1.png?width=1703&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96a2f25bd5e789bfa26524ab510eecddc6da027f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.maxsun.com/blogs/maxsun-motherboard/maxsun-showcases-ai-solutions-at-ciie-2025"&gt;https://www.maxsun.com/blogs/maxsun-motherboard/maxsun-showcases-ai-solutions-at-ciie-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;Quad-GPU AI Workstation&lt;/strong&gt; is equipped with &lt;strong&gt;four MAXSUN Intel Arc Pro B60 Dual 48G Turbo GPUs&lt;/strong&gt; and the &lt;strong&gt;MS-WorkStation W790-112L motherboard&lt;/strong&gt;, it enables &lt;strong&gt;eight GPUs&lt;/strong&gt; to operate in parallel. With a &lt;strong&gt;Linux software stack optimized for large language models&lt;/strong&gt;, the system provides up to &lt;strong&gt;192GB of total VRAM.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;ARL-HX Mini Dual-GPU Workstation&lt;/strong&gt; is paired with &lt;strong&gt;two MAXSUN Intel Arc Pro B60 24G GPUs&lt;/strong&gt; (48GB total VRAM), supporting &lt;em&gt;Qwen3-32B&lt;/em&gt; and other demanding inference tasks.&lt;/p&gt; &lt;p&gt;Will we be able to afford?&lt;/p&gt; &lt;p&gt;Correction: title is wrong: should be 8 gpu , not quad gpu. It is quad gpu cards, each gpu card having 2 gpus on it.&lt;/p&gt; &lt;p&gt;Update: &lt;a href="https://www.youtube.com/watch?v=vZupIBqKHqM&amp;amp;t=408s"&gt;https://www.youtube.com/watch?v=vZupIBqKHqM&amp;amp;t=408s&lt;/a&gt; . Linus video estimated price for the 8 gpu version to be ~ $10K. The dual GPU system to be competitive needs to be $3K or less in my opinion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otgxs8/maxsun_displays_quad_gpu_and_dual_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otgxs8/maxsun_displays_quad_gpu_and_dual_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otgxs8/maxsun_displays_quad_gpu_and_dual_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot9346</id>
    <title>NVIDIA RTX Pro 5000 Blackwell 72 GB Price</title>
    <updated>2025-11-10T09:08:14+00:00</updated>
    <author>
      <name>/u/Low_Philosophy7906</name>
      <uri>https://old.reddit.com/user/Low_Philosophy7906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot9346/nvidia_rtx_pro_5000_blackwell_72_gb_price/"&gt; &lt;img alt="NVIDIA RTX Pro 5000 Blackwell 72 GB Price" src="https://b.thumbs.redditmedia.com/rXjsj4BIQiwNGd80wvtzqLzsHERAFcaOduHZxzuem5w.jpg" title="NVIDIA RTX Pro 5000 Blackwell 72 GB Price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found one of the first price tags in germany. Seems quite high, I expected it to be around 6000-6500‚Ç¨. I hope it will go down when other offers come up... &lt;/p&gt; &lt;p&gt;What do you think about this GPU? I think the 6000 series has better value, especially considering bandwidth and core count.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.comnet-itshop.de/eshop.php?eslink=1&amp;amp;action=article_detail&amp;amp;s_supplier_id=12&amp;amp;s_supplier_aid=12189390"&gt;https://www.comnet-itshop.de/eshop.php?eslink=1&amp;amp;action=article_detail&amp;amp;s_supplier_id=12&amp;amp;s_supplier_aid=12189390&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pk7d074qae0g1.png?width=1284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ad13e0998d176a60ae79e3141e86ccf1fa3e9b4"&gt;https://preview.redd.it/pk7d074qae0g1.png?width=1284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ad13e0998d176a60ae79e3141e86ccf1fa3e9b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Philosophy7906"&gt; /u/Low_Philosophy7906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot9346/nvidia_rtx_pro_5000_blackwell_72_gb_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot9346/nvidia_rtx_pro_5000_blackwell_72_gb_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot9346/nvidia_rtx_pro_5000_blackwell_72_gb_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T09:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot67nn</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-11-10T06:04:53+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://a.thumbs.redditmedia.com/_ehDePF5xr3odzacIrNpvwUbVHDsuf9cAhwbOLzsNz8.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/edge highlights from this week:&lt;/p&gt; &lt;p&gt;Rolling Forcing - Real-Time Streaming Video on 1 GPU&lt;br /&gt; ‚Ä¢ Generates multi-minute video interactively with joint multi-frame denoising.&lt;br /&gt; ‚Ä¢ Anchors temporal context for stability without heavy clusters.&lt;br /&gt; ‚Ä¢ &lt;a href="https://kunhao-liu.github.io/Rolling_Forcing_Webpage/"&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2509.25161"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/TencentARC/RollingForcing"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/TencentARC/RollingForcing"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/q45gljk2ed0g1/player"&gt;https://reddit.com/link/1ot67nn/video/q45gljk2ed0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Step-Audio-EditX (3B) - Text-Driven Audio Editing&lt;br /&gt; ‚Ä¢ Controls emotion, style, breaths, laughs via prompts.&lt;br /&gt; ‚Ä¢ Runs on a single GPU; open weights for local pipelines.&lt;br /&gt; ‚Ä¢ &lt;a href="https://stepaudiollm.github.io/step-audio-editx/"&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2511.03601"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/stepfun-ai/Step-Audio-EditX"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-EditX"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fsl15il8ed0g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caa10ad203ad44158a1ba8dbe7f303b0eb03cfbd"&gt;An overview of the architecture of Step-Audio-EditX.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BindWeave - Consistent Subjects, Local Pipelines&lt;br /&gt; ‚Ä¢ Subject-consistent video gen; ComfyUI support.&lt;br /&gt; ‚Ä¢ Drop-in for desktop creative stacks.&lt;br /&gt; ‚Ä¢ &lt;a href="https://lzy-dot.github.io/BindWeave/"&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/papers/2510.00438"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/bytedance/BindWeave"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/ByteDance/BindWeave"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/ay7nndyaed0g1/player"&gt;https://reddit.com/link/1ot67nn/video/ay7nndyaed0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InfinityStar (8B) - Unified Spacetime AR Gen&lt;br /&gt; ‚Ä¢ 8B model targets high-res image/video generation.&lt;br /&gt; ‚Ä¢ Fits prosumer GPUs for local experimentation.&lt;br /&gt; ‚Ä¢ &lt;a href="https://arxiv.org/abs/2511.04675"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/FoundationVision/InfinityStar"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/FoundationVision/InfinityStar"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/ouipokpbed0g1/player"&gt;https://reddit.com/link/1ot67nn/video/ouipokpbed0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OlmoEarth-v1-Large - Remote Sensing for Builders&lt;br /&gt; ‚Ä¢ Satellite model ready for on-prem analysis.&lt;br /&gt; ‚Ä¢ Strong for geospatial R&amp;amp;D without cloud lock-in.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/allenai/OlmoEarth-v1-Large"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://www.datocms-assets.com/64837/1762355216-olmoearth_v2.pdf"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://x.com/allen_ai/status/1985719070407176577"&gt;&lt;strong&gt;Announcement&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/mkbihhrced0g1/player"&gt;https://reddit.com/link/1ot67nn/video/mkbihhrced0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-32-multi-query?r=12l7fk&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;full newsletter&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1otddd4</id>
    <title>Cheapest method to selfhost Qwen 3VL Model</title>
    <updated>2025-11-10T13:12:16+00:00</updated>
    <author>
      <name>/u/PavanRocky</name>
      <uri>https://old.reddit.com/user/PavanRocky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otddd4/cheapest_method_to_selfhost_qwen_3vl_model/"&gt; &lt;img alt="Cheapest method to selfhost Qwen 3VL Model" src="https://preview.redd.it/aebhrmzyif0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf98f24c25e7253b6af66a0712d1e065b7d506d3" title="Cheapest method to selfhost Qwen 3VL Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hi everyone I need suggestions to selfhost this model with cheapest price&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PavanRocky"&gt; /u/PavanRocky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aebhrmzyif0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otddd4/cheapest_method_to_selfhost_qwen_3vl_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otddd4/cheapest_method_to_selfhost_qwen_3vl_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T13:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oti4tc</id>
    <title>Minimax M2 for App creation</title>
    <updated>2025-11-10T16:20:07+00:00</updated>
    <author>
      <name>/u/HectorLavoe33</name>
      <uri>https://old.reddit.com/user/HectorLavoe33</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, lately I have been testing Minimax for creating a simple PWA that only handles data with Supabase, Spreedsheets and Google Drive. But when I tell Minimax what I need, every time it fixes something, it breaks something else and I can spend 3 hours walking around trying to correct the same error. I paid for the more expensive PRO version because I thought it would be worth it and I could carry out my project. But the truth is that it's giving me a lot of headaches and wasting time constantly correcting it so that it then breaks another part of the app. The truth is I feel a little frustrated, I promised more. Can anyone take a project from start to finish with Minimax?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HectorLavoe33"&gt; /u/HectorLavoe33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oti4tc/minimax_m2_for_app_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oti4tc/minimax_m2_for_app_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oti4tc/minimax_m2_for_app_creation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T16:20:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1otislj</id>
    <title>After a year building an open-source AI framework, I‚Äôm starting to wonder what actually gets attention</title>
    <updated>2025-11-10T16:44:16+00:00</updated>
    <author>
      <name>/u/DocteurW</name>
      <uri>https://old.reddit.com/user/DocteurW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;It took me over a year to finally write this.&lt;br /&gt; Even now, I‚Äôm not sure it's worth it.&lt;br /&gt; But whatever, yolo.&lt;/p&gt; &lt;p&gt;I‚Äôm the creator of Yacana, a free and open source multi-agent framework.&lt;br /&gt; I‚Äôve spent more than a year working late nights on it, &lt;strong&gt;thinking that if the software was good, people would naturally show up.&lt;/strong&gt;&lt;br /&gt; Turns out‚Ä¶ not really.&lt;/p&gt; &lt;h1&gt;How it started&lt;/h1&gt; &lt;p&gt;Back when local LLMs first became usable, there was no proper tool calling.&lt;br /&gt; That made it nearly impossible to build anything useful on top of them.&lt;/p&gt; &lt;p&gt;So I started writing a framework to fix that. That‚Äôs how Yacana began. Its main goal was to let LLMs call tools automatically.&lt;br /&gt; Around the same time, LangChain released a buggy &amp;quot;function calling&amp;quot; thing for Ollama, but it still wasn‚Äôt real tool calling. You had to handle everything manually.&lt;/p&gt; &lt;p&gt;That‚Äôs why I can confidently say Yacana was the first official framework to actually make it work.&lt;/p&gt; &lt;p&gt;I dare to say &amp;quot;official&amp;quot; because roughly at the same time it got added to the Ollama Github's main page which I thought would be enough to attract some users.&lt;/p&gt; &lt;p&gt;Spoiler: it wasn‚Äôt.&lt;/p&gt; &lt;h1&gt;How it went&lt;/h1&gt; &lt;p&gt;As time passed, tool calling became standard across the board.&lt;br /&gt; Everyone started using the OpenAI-style syntax.&lt;br /&gt; Yacana followed that path too but also kept its original tool calling mechanism.&lt;/p&gt; &lt;p&gt;I added a ton of stuff since then: checkpoints, history management, state saving, VLLM support, thinking model support, streaming, structured outputs, and so on.&lt;br /&gt; And still‚Ä¶ almost no feedback.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The GitHub stars and PyPI downloads? Let‚Äôs just say they‚Äôre modest.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then came MCP, which looked like the next big standard.&lt;br /&gt; I added support for MCP tools, staying true to Yacana‚Äôs simple OOP API (unlike LangChain‚Äôs tangle of abstractions).&lt;br /&gt; Still no big change.&lt;/p&gt; &lt;h1&gt;Self-reflection time&lt;/h1&gt; &lt;p&gt;At one point, I thought maybe I just needed to advertized some more. &lt;/p&gt; &lt;p&gt;But I hesitated.&lt;br /&gt; There were already so many &amp;quot;agentic&amp;quot; frameworks popping up...&lt;br /&gt; I started wondering if I was just fooling myself.&lt;br /&gt; Was Yacana really good enough to deserve a small spotlight?&lt;br /&gt; Was I just promoting something that wasn‚Äôt as advanced as the competition?&lt;/p&gt; &lt;p&gt;Maybe.&lt;/p&gt; &lt;p&gt;And yet, I kept thinking that it deserved a bit more.&lt;br /&gt; There aren‚Äôt that many frameworks out there that are both independent (not backed by a company ~Strands~) and actually documented (sorry, LangChain).&lt;/p&gt; &lt;h1&gt;Meanwhile, in AI-land...&lt;/h1&gt; &lt;p&gt;Fast forward to today. It‚Äôs been 1 year and ~4 months.&lt;br /&gt; Yacana sits at around 60+ GitHub stars.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Meanwhile, random fake AI projects get thousands of stars.&lt;/strong&gt;&lt;br /&gt; Some of them aren‚Äôt even real, just flashy demos or vaporware.&lt;br /&gt; Sometimes I genuinely wonder if there are bots starring repos to make them look more popular.&lt;br /&gt; Like some invisible puppeteer trying to shape developers attention.&lt;/p&gt; &lt;h1&gt;A little sting&lt;/h1&gt; &lt;p&gt;Recently I was reading through LangChain‚Äôs docs and saw they had a &amp;quot;checkpoints&amp;quot; feature.&lt;br /&gt; Not gonna lie, that one stung a bit.&lt;br /&gt; It wasn‚Äôt the first time I stumbled upon a Yacana feature that had been implemented elsewhere.&lt;br /&gt; What hurts is that Yacana‚Äôs features weren‚Äôt copied from other frameworks, they were &lt;strong&gt;invented&lt;/strong&gt;.&lt;br /&gt; And seeing them appear somewhere else kind of proves that I might actually be good at what I do. But the fact that so few people seem to care about my work just reinforces the feeling that maybe I‚Äôm doing all of this for nothing.&lt;/p&gt; &lt;h1&gt;My honest take&lt;/h1&gt; &lt;p&gt;I don‚Äôt think agentic frameworks are a revolution.&lt;br /&gt; The real revolution is the LLMs themselves.&lt;br /&gt; Frameworks like Yacana (or LangChain, CrewAI, etc.) are mostly structured wrappers around POST requests to an inference server.&lt;/p&gt; &lt;p&gt;Still, Yacana has a purpose.&lt;br /&gt; It‚Äôs simple, lightweight, easy to learn, and can work with models that aren‚Äôt fine-tuned for function calling.&lt;br /&gt; It‚Äôs great for people who don't want to invest 100+ hours in Langchain. Not saying that Langchain isn't worth it, but it's not always needed depending on the problem to solve.&lt;/p&gt; &lt;h1&gt;Where things stand&lt;/h1&gt; &lt;p&gt;So why isn‚Äôt it catching on?&lt;br /&gt; I am still unsure.&lt;/p&gt; &lt;p&gt;I‚Äôve written detailed docs, made examples, and even started recording video tutorials.&lt;br /&gt; The problem doesn‚Äôt seem to be the learning curve.&lt;br /&gt; Maybe it still lacks something, like native RAG support. But after having followed the hype curve for more than a year, I‚Äôve realized there‚Äôs probably more to it than just features.&lt;/p&gt; &lt;p&gt;I‚Äôll keep updating Yacana regardless.&lt;br /&gt; I just think it deserves a (tiny) bit more visibility.&lt;br /&gt; Not because it‚Äôs revolutionary, but because it‚Äôs real.&lt;/p&gt; &lt;p&gt;And maybe that should count for something.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Github:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/rememberSoftwares/yacana"&gt;https://github.com/rememberSoftwares/yacana&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Documentation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://remembersoftwares.github.io/yacana"&gt;https://remembersoftwares.github.io/yacana&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocteurW"&gt; /u/DocteurW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otislj/after_a_year_building_an_opensource_ai_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otislj/after_a_year_building_an_opensource_ai_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otislj/after_a_year_building_an_opensource_ai_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T16:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgd3j</id>
    <title>VoxCPM Text-to-Speech running or Apple Neural Engine ANE</title>
    <updated>2025-11-10T15:13:51+00:00</updated>
    <author>
      <name>/u/0seba</name>
      <uri>https://old.reddit.com/user/0seba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otgd3j/voxcpm_texttospeech_running_or_apple_neural/"&gt; &lt;img alt="VoxCPM Text-to-Speech running or Apple Neural Engine ANE" src="https://external-preview.redd.it/dvjTO3Ntbl7DpYMVsEyG0Q8lBCe_LB-me-5HVErZU58.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d167e12953f9af749f13c3c224b850317eb67ca9" title="VoxCPM Text-to-Speech running or Apple Neural Engine ANE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I ported OpenBMB's VoxCPM to CoreML so now it mostly runs using the Apple Neural Engine ANE.&lt;/p&gt; &lt;p&gt;Here is the &lt;a href="https://github.com/0seba/VoxCPMANE"&gt;repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The models supports voice cloning and handles real time streaming speech generation on my M1 Macbook Air 8GB.&lt;/p&gt; &lt;p&gt;Hopefully someone can try it, any feedback is useful.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1otgd3j/video/f73iublf3g0g1/player"&gt;https://reddit.com/link/1otgd3j/video/f73iublf3g0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am also looking into porting more models to CoreML for NE support, so let me know what could be useful to you. Here are some characteristics to help filter out if a task or model makes sense for the NE or not.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compute heavy operations. I am looking into porting the image encoder of OCR models (like DeepsSeekOCR) and running the text generation/decoding with MLX&lt;/li&gt; &lt;li&gt;Same as above, but more generally encoder/embedding models that lean on the compute heavy and latency is not as important&lt;/li&gt; &lt;li&gt;MoEs are awful for the NE&lt;/li&gt; &lt;li&gt;4 bit quantization is a big issue, NE does not support grouping so there is too much degradation under 6 bits, 8 bits recommended to stay on the safe side.&lt;/li&gt; &lt;li&gt;NE can not access the full RAM bandwidth (120 GB/s on M3 Max, M4 Pro and M4 Max, 60 GB/s in other models, &lt;a href="https://github.com/Anemll/anemll-bench"&gt;source&lt;/a&gt;, note this is peak bandwidth and full model runs under 50 GB/s in my experience. On iPhone 15 Pro Max I get 44 GB/s peak bandwidth)&lt;/li&gt; &lt;li&gt;For the reason above avoid tasks where (big models and) latency is important, other situations where generation at reading speed is enough can be acceptable, 6 inferences per second can be performed on a 6GB model at 40 GB/s bandwidth.&lt;/li&gt; &lt;li&gt;It is highly preferable for tasks where context is bound, 0-8K tokens, CoreML computation graph is static so the attention is always performed on the full context of the computation graph you are using. It is possible to have several computations graphs with different lengths but this would require model switching and I haven't looked into the downsides if you want to do things like extend the current context if it is full.&lt;/li&gt; &lt;li&gt;Async batch generation may be a favorable scenario.&lt;/li&gt; &lt;li&gt;Running on the NE instead of the GPU means the GPU is free and it has less power consumption which could also prevent throttling.&lt;/li&gt; &lt;li&gt;I am not sure but I think it is better to lean on small-ish models. CoreML has a maximum model size of 2 GB for the NE, so to run bigger models you have to split the whole (transformer) model into groups of its consecutive blocks (also my Macbook has 8 GB so I cannot test anything bigger).&lt;/li&gt; &lt;li&gt;CoreML has a big first compilation time for a new model (specially for the Neural Engine) but on subsequent model loads it is cached and it is much faster.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to help if you have any more questions or have any issues with the package.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0seba"&gt; /u/0seba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otgd3j/voxcpm_texttospeech_running_or_apple_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otgd3j/voxcpm_texttospeech_running_or_apple_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otgd3j/voxcpm_texttospeech_running_or_apple_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1otb5vw</id>
    <title>Ultra-fast robotic TTS</title>
    <updated>2025-11-10T11:19:27+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a TTS engine where speed/low resources (no GPU) along with clarity are important.&lt;/p&gt; &lt;p&gt;It doesn't need to sound human and I imagine it to be closer to espeak-ng than Kokoro-82.&lt;/p&gt; &lt;p&gt;The problem with espeak-ng itself is that it is robotic to the point of not being easy to understand.&lt;/p&gt; &lt;p&gt;What options are there that lie between espeak-ng and Kokoro-82 on the same quality/speed curves?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otb5vw/ultrafast_robotic_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otb5vw/ultrafast_robotic_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otb5vw/ultrafast_robotic_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T11:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1othqbc</id>
    <title>Minimax now offers Coding Plans, but is it worth it?</title>
    <updated>2025-11-10T16:05:27+00:00</updated>
    <author>
      <name>/u/baykarmehmet</name>
      <uri>https://old.reddit.com/user/baykarmehmet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1othqbc/minimax_now_offers_coding_plans_but_is_it_worth_it/"&gt; &lt;img alt="Minimax now offers Coding Plans, but is it worth it?" src="https://b.thumbs.redditmedia.com/-9xikgDl92qUN8nRCSqqorPnqrZja9Zt8QCFp2xFmGo.jpg" title="Minimax now offers Coding Plans, but is it worth it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a GLM Coding Plan subscription, and so far I‚Äôve had a pretty good experience with GLM-4.6 in Claude Code. I paid $180, and it gives me ~600 prompts every 5 hours. Here, the plan costs $20 more and offers 300 prompts every 5 hours, which is about half. What do you guys think? Is it better to stick with GLM, or is it worth trying Minimax M2? I‚Äôm not sure if a yearly plan would include better models during the term‚Äîmaybe I pay for a year and wait 6‚Äì8 months to see a new model from Minimax.&lt;/p&gt; &lt;p&gt;Let me know your thoughts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3zotexgkcg0g1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fb1e7532e7e626119b7778a342ff32940a962a5"&gt;https://preview.redd.it/3zotexgkcg0g1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fb1e7532e7e626119b7778a342ff32940a962a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oc6dk9kndg0g1.png?width=2784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee37aeff1a01846b940e62324bcb7257f0e657e5"&gt;https://preview.redd.it/oc6dk9kndg0g1.png?width=2784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee37aeff1a01846b940e62324bcb7257f0e657e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baykarmehmet"&gt; /u/baykarmehmet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1othqbc/minimax_now_offers_coding_plans_but_is_it_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1othqbc/minimax_now_offers_coding_plans_but_is_it_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1othqbc/minimax_now_offers_coding_plans_but_is_it_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T16:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1otk3bv</id>
    <title>Name your favorite OSS Agent tool(s)!</title>
    <updated>2025-11-10T17:31:07+00:00</updated>
    <author>
      <name>/u/Badger-Purple</name>
      <uri>https://old.reddit.com/user/Badger-Purple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm not talking about roo or cline. &lt;/p&gt; &lt;p&gt;I mean things like Flow Agent, Mem Agent, training agents, etc. Python or JS based agentic workflow systems that deserve a look. &lt;/p&gt; &lt;p&gt;Anyone have suggestions? &lt;/p&gt; &lt;p&gt;I‚Äôm aware of the agent building tools out there, but I stay away from Claude Code. I want systems I can run, set as an MCP server or otherwise, and when called from another LLM they spin up the model you selected to do their hyperspecialized task, be it deep research, visual recognition, audio transcription, etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badger-Purple"&gt; /u/Badger-Purple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otk3bv/name_your_favorite_oss_agent_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otk3bv/name_your_favorite_oss_agent_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otk3bv/name_your_favorite_oss_agent_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T17:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1osnnfn</id>
    <title>How to build an AI computer (version 2.0)</title>
    <updated>2025-11-09T16:28:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt; &lt;img alt="How to build an AI computer (version 2.0)" src="https://preview.redd.it/03t3yj51d90g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d467717022f60865bdfcbb8d96bb265e2bdb541" title="How to build an AI computer (version 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/03t3yj51d90g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T16:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1otihl1</id>
    <title>Open-dLLM: Open Diffusion Large Language Models</title>
    <updated>2025-11-10T16:33:06+00:00</updated>
    <author>
      <name>/u/pengzhangzhi</name>
      <uri>https://old.reddit.com/user/pengzhangzhi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"&gt; &lt;img alt="Open-dLLM: Open Diffusion Large Language Models" src="https://external-preview.redd.it/eHlpNXJmc3BpZzBnMbC2Q-rs9CfDNgw85akHP4ZCgTS81bEyqZb3k8CkqU2r.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fedc17befcb0ad76c15f3434bb58981727894c5" title="Open-dLLM: Open Diffusion Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the most open release of a diffusion-based large language model to date ‚Äî&lt;br /&gt; including &lt;strong&gt;pretraining, evaluation, inference, and checkpoints&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/pengzhangzhi/Open-dLLM"&gt;https://github.com/pengzhangzhi/Open-dLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://oval-shell-31c.notion.site/Open-dLLM-Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a"&gt;https://oval-shell-31c.notion.site/Open-dLLM-Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pengzhangzhi"&gt; /u/pengzhangzhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qb62efspig0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T16:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot682o</id>
    <title>Montana Becomes First State to Enshrine ‚ÄòRight to Compute‚Äô Into Law - Montana Newsroom</title>
    <updated>2025-11-10T06:05:33+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot682o/montana_becomes_first_state_to_enshrine_right_to/"&gt; &lt;img alt="Montana Becomes First State to Enshrine ‚ÄòRight to Compute‚Äô Into Law - Montana Newsroom" src="https://external-preview.redd.it/mQmftbFg8dXc1pZL5UOJLL9AO6IH64kyLn5ax_JS4QM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2bc023e5f328155ef589e67ab05e5434c741142" title="Montana Becomes First State to Enshrine ‚ÄòRight to Compute‚Äô Into Law - Montana Newsroom" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Montana has made history as the first state in the U.S. to legally protect its citizens‚Äô right to access and use computational tools and artificial intelligence technologies. Governor Greg Gianforte signed Senate Bill 212, officially known as the Montana Right to Compute Act (MRTCA), into law.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The groundbreaking legislation affirms Montanans‚Äô fundamental right to own and operate computational resources ‚Äî including hardware, software, and AI tools ‚Äî under the state‚Äôs constitutional protections for property and free expression. Supporters of the bill say it represents a major step in securing digital freedoms in an increasingly AI-driven world.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;‚ÄúMontana is once again leading the way in defending individual liberty,‚Äù said Senator Daniel Zolnikov, the bill‚Äôs sponsor and a longtime advocate for digital privacy. ‚ÄúWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;While the law allows state regulation of computation in the interest of public health and safety, it sets a high bar: any restrictions must be demonstrably necessary and narrowly tailored to serve a compelling interest. Legal experts note that this is one of the most protective standards available under Montana law.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Hopefully this leads to more states following / similar federal legislation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot682o/montana_becomes_first_state_to_enshrine_right_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot682o/montana_becomes_first_state_to_enshrine_right_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot80p0</id>
    <title>Is it too early for local LLMs?</title>
    <updated>2025-11-10T07:58:46+00:00</updated>
    <author>
      <name>/u/Substantial_Mode_167</name>
      <uri>https://old.reddit.com/user/Substantial_Mode_167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been thinking for a while about setting up a local environment for running an LLM. Since I was already planning to build a gaming PC, I saw it as a good opportunity to tweak the setup so I could also use AI tools locally, I use them quite a lot.&lt;/p&gt; &lt;p&gt;But after looking into the market, it really feels like it‚Äôs still too early. Everything is overpriced, full of compromises, or the few uncompromising options cost an absurd amount. It just doesn‚Äôt seem worth it yet. I feel like we‚Äôll need to wait another couple of years before running an LLM locally becomes truly viable for most people.&lt;/p&gt; &lt;p&gt;Of course, it depends on your use case and budget, but I think only a few can realistically justify or get a real return on such an investment right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Mode_167"&gt; /u/Substantial_Mode_167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot80p0/is_it_too_early_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot80p0/is_it_too_early_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot80p0/is_it_too_early_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T07:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1otj99f</id>
    <title>LinkedIn now tells you when you're looking at an AI-generated image, if you haven't noticed.</title>
    <updated>2025-11-10T17:01:12+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otj99f/linkedin_now_tells_you_when_youre_looking_at_an/"&gt; &lt;img alt="LinkedIn now tells you when you're looking at an AI-generated image, if you haven't noticed." src="https://preview.redd.it/bl396lgsng0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d06e52cf2855e25cb75bab6e7f8d9e9a70cccd3" title="LinkedIn now tells you when you're looking at an AI-generated image, if you haven't noticed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the 1st image shows, the C2PA label is used.&lt;/p&gt; &lt;p&gt;Here's what's interesting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The feature only applies to image platforms who join the C2PA.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now there's only:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ChatGPT/DALL-E 3 images&lt;/li&gt; &lt;li&gt;Adobe Firefly images&lt;/li&gt; &lt;li&gt;Leica Camera images&lt;/li&gt; &lt;li&gt;BBC news images&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 2nd image, generated by &lt;a href="https://www.netmind.ai/modelsLibrary/nano-banana"&gt;Google's Nano Banana&lt;/a&gt;, does not have the label.&lt;/p&gt; &lt;p&gt;What's even more interesting?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It's easy to bypass this new rule.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;You just need to upload the screenshot of the AI-generated pic, as we did with the 3rd image, a screenshot of the 1st one.&lt;/p&gt; &lt;p&gt;Do you think more AI image platforms, like Google, will join C2PA?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bl396lgsng0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otj99f/linkedin_now_tells_you_when_youre_looking_at_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otj99f/linkedin_now_tells_you_when_youre_looking_at_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T17:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1osydym</id>
    <title>BERTs that chat: turn any BERT into a chatbot with dLLM</title>
    <updated>2025-11-09T23:34:15+00:00</updated>
    <author>
      <name>/u/Individual-Ninja-141</name>
      <uri>https://old.reddit.com/user/Individual-Ninja-141</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osydym/berts_that_chat_turn_any_bert_into_a_chatbot_with/"&gt; &lt;img alt="BERTs that chat: turn any BERT into a chatbot with dLLM" src="https://external-preview.redd.it/aXQyaXFqbnhjYjBnMcUkZmGot1jxvd3JGKHlRvmTnIHsjUGTXFsDE1YCFtzY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ca452c52e70528594ede3b08fdf40c34e19a371" title="BERTs that chat: turn any BERT into a chatbot with dLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;https://github.com/ZHZisZZ/dllm&lt;/a&gt;&lt;br /&gt; Report: &lt;a href="https://api.wandb.ai/links/asap-zzhou/101h5xvg"&gt;https://api.wandb.ai/links/asap-zzhou/101h5xvg&lt;/a&gt;&lt;br /&gt; Checkpoints: &lt;a href="https://huggingface.co/collections/dllm-collection/bert-chat"&gt;https://huggingface.co/collections/dllm-collection/bert-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;: I couldn‚Äôt find a good ‚ÄúHello World‚Äù tutorial for training &lt;strong&gt;diffusion language models&lt;/strong&gt;, a class of bidirectional language models capable of parallel token generation in arbitrary order, instead of left-to-right autoregression. So I tried finetuning a tiny BERT to make it &lt;strong&gt;talk with discrete diffusion&lt;/strong&gt;‚Äîand it turned out more fun than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: With a small amount of open-source instruction data, a standard BERT can gain conversational ability. Specifically, a finetuned &lt;a href="https://huggingface.co/answerdotai/ModernBERT-large"&gt;ModernBERT-large&lt;/a&gt;, with a similar number of parameters, performs close to &lt;a href="https://huggingface.co/Qwen/Qwen1.5-0.5B"&gt;Qwen1.5-0.5B&lt;/a&gt;. All training and evaluation code, along with detailed results and comparisons, is available in our &lt;a href="https://api.wandb.ai/links/asap-zzhou/101h5xvg"&gt;W&amp;amp;B report&lt;/a&gt; and our &lt;a href="https://github.com/ZHZisZZ/dllm/tree/main/examples/bert"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ZHZisZZ/dllm"&gt;&lt;strong&gt;dLLM&lt;/strong&gt;&lt;/a&gt;: The BERT chat series is &lt;em&gt;trained, evaluated and visualized&lt;/em&gt; with &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;dLLM&lt;/a&gt; ‚Äî a unified library for training and evaluating diffusion language models. It brings transparency, reproducibility, and simplicity to the entire pipeline, &lt;strong&gt;serving as an all-in-one, tutorial-style resource.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Ninja-141"&gt; /u/Individual-Ninja-141 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/47030knxcb0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osydym/berts_that_chat_turn_any_bert_into_a_chatbot_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osydym/berts_that_chat_turn_any_bert_into_a_chatbot_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T23:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1otdr19</id>
    <title>What is the best hardware under 10k to run local big models with over 200b parameters?</title>
    <updated>2025-11-10T13:28:39+00:00</updated>
    <author>
      <name>/u/nadiemeparaestavez</name>
      <uri>https://old.reddit.com/user/nadiemeparaestavez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm looking to build an AI rig that can run these big models for coding purposes, but also as a hobby.&lt;/p&gt; &lt;p&gt;I have been playing around with a 3090 I had for gaming, but I'm interested in running bigger models. So far my options seem:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Upgrade motherboard/psu/case and get another 3090/4090, total 42gb vram, 128gb ram, and a server-cpu to support more channels.&lt;/li&gt; &lt;li&gt;Buy a mac studio with m3 ultra.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would a mixed ram/vram setup like 1 be slower than the m3 when running 230b models? What about models like minimax m2 which use MoE? Would those run much faster on the gpu+ram approach?&lt;/li&gt; &lt;li&gt;Is there any other sensible option to get huge amounts of ram/vram and enough performance for inference on 1 user without going over 10k?&lt;/li&gt; &lt;li&gt;Would it be worth it to go for a mix of 1 3090 and 1 5090? Or would the 5090 just be bottle necked waiting for the 3090?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm in no rush, I'm starting to save up to buy something in a few months, but I want to understand what direction should I go for. If something like option 1 was the best idea I might upgrade little by little from my current setup.&lt;/p&gt; &lt;p&gt;Short term I will use this to refactor codebases, coding features, etc. I don't mind if it runs slow, but I need to be able to run thinking/high quality models that can follow long processes (like splitting big tasks into smaller ones, and following procedures). But long term I just want to learn and experiment, so anything that can actually run big models would be good enough, even if slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nadiemeparaestavez"&gt; /u/nadiemeparaestavez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otdr19/what_is_the_best_hardware_under_10k_to_run_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otdr19/what_is_the_best_hardware_under_10k_to_run_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otdr19/what_is_the_best_hardware_under_10k_to_run_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T13:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1otl8q8</id>
    <title>Omnilingual ASR: Advancing Automatic Speech Recognition for 1,600+ Languages</title>
    <updated>2025-11-10T18:12:13+00:00</updated>
    <author>
      <name>/u/jean-</name>
      <uri>https://old.reddit.com/user/jean-</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jean-"&gt; /u/jean- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otl8q8/omnilingual_asr_advancing_automatic_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otl8q8/omnilingual_asr_advancing_automatic_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T18:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot6k56</id>
    <title>Kimi infra team: Quantization is not a compromise, it's the next paradigm</title>
    <updated>2025-11-10T06:25:55+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After K2-Thinking's release, many developers have been curious about its native INT4 quantization format.&lt;/p&gt; &lt;p&gt;Shaowei Liu, &lt;strong&gt;infra engineer&lt;/strong&gt; at &lt;a href="/u/Kimi-Moonshot"&gt;u/Kimi-Moonshot&lt;/a&gt; shares an insider's view on why this choice matters, and why quantization today isn't just about sacrificing precision for speed.&lt;/p&gt; &lt;h1&gt;Key idea&lt;/h1&gt; &lt;p&gt;In the context of LLMs, &lt;strong&gt;quantization is no longer a trade-off&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;With the evolution of param-scaling and test-time-scaling, native low-bit quantization will become a standard paradigm for large model training.&lt;/p&gt; &lt;h1&gt;Why Low-bit Quantization Matters&lt;/h1&gt; &lt;p&gt;In modern LLM inference, there are two distinct optimization goals:&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;High throughput&lt;/strong&gt; (cost-oriented): maximize GPU utilization via large batch sizes.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Low latency&lt;/strong&gt; (user-oriented): minimize per-query response time.&lt;/p&gt; &lt;p&gt;For Kimi-K2's MoE structure (with &lt;strong&gt;1/48 sparsity&lt;/strong&gt;), &lt;strong&gt;decoding is memory-bound&lt;/strong&gt; ‚Äî the smaller the model weights, the faster the compute.&lt;/p&gt; &lt;p&gt;FP8 weights (‚âà1 TB) already hit the limit of what a single high-speed interconnect GPU node can handle.&lt;/p&gt; &lt;p&gt;By switching to W4A16, latency drops sharply while maintaining quality ‚Äî a perfect fit for low-latency inference.&lt;/p&gt; &lt;h1&gt;Why QAT over PTQ&lt;/h1&gt; &lt;p&gt;Post-training quantization (PTQ) worked well for shorter generations, but &lt;strong&gt;failed in longer reasoning chains&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;‚Ä¢ Error accumulation during long decoding degraded precision.&lt;/p&gt; &lt;p&gt;‚Ä¢ Dependence on calibration data caused &amp;quot;expert distortion&amp;quot; in sparse MoE layers.&lt;/p&gt; &lt;p&gt;Thus, K2-Thinking adopted QAT for &lt;strong&gt;minimal loss&lt;/strong&gt; and &lt;strong&gt;more stable long-context reasoning&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;K2-Thinking uses a &lt;strong&gt;weight-only QAT&lt;/strong&gt; with &lt;strong&gt;fake quantization + STE (straight-through estimator)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The pipeline was fully integrated in just days ‚Äî from QAT training ‚Üí INT4 inference ‚Üí RL rollout ‚Äî enabling near lossless results without extra tokens or retraining.&lt;/p&gt; &lt;h1&gt;INT4's hidden advantage in RL&lt;/h1&gt; &lt;p&gt;Few people mention this: &lt;strong&gt;native INT4&lt;/strong&gt; doesn't just speed up inference ‚Äî it &lt;strong&gt;accelerates RL training&lt;/strong&gt; itself.&lt;/p&gt; &lt;p&gt;Because RL rollouts often suffer from &amp;quot;long-tail&amp;quot; inefficiency, INT4's low-latency profile makes those stages much faster.&lt;/p&gt; &lt;p&gt;In practice, each RL iteration runs &lt;strong&gt;10-20% faster end-to-end.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Moreover, quantized RL brings stability: smaller representational space reduces accumulation error, improving learning robustness.&lt;/p&gt; &lt;h1&gt;Why INT4, not MXFP4&lt;/h1&gt; &lt;p&gt;Kimi chose INT4 over &amp;quot;fancier&amp;quot; MXFP4/NVFP4 to better support &lt;strong&gt;non-Blackwell GPUs&lt;/strong&gt;, with strong existing kernel support (e.g., Marlin).&lt;/p&gt; &lt;p&gt;At a quant scale of 1√ó32, INT4 matches FP4 formats in expressiveness while being more hardware-adaptable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3lxv</id>
    <title>I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck</title>
    <updated>2025-11-10T03:42:05+00:00</updated>
    <author>
      <name>/u/Hungry_Elk_3276</name>
      <uri>https://old.reddit.com/user/Hungry_Elk_3276</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt; &lt;img alt="I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck" src="https://preview.redd.it/ezjtolwnoc0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9f058ee27bdab9923ee3d40ab306fea5558c71" title="I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; While InfiniBand is cool, 10 Gbps Thunderbolt is sufficient for llama.cpp.&lt;/p&gt; &lt;p&gt;Recently I got really fascinated by clustering with Strix Halo to get a potential 200 GB of VRAM without significant costs. I'm currently using a 4x4090 solution for research, but it's very loud and power-hungry (plus it doesn't make much sense for normal 1-2 user inference‚Äîthis machine is primarily used for batch generation for research purposes). I wanted to look for a low-power but efficient way to inference ~230B models at Q4. And here we go.&lt;/p&gt; &lt;p&gt;I always had this question of how exactly networking would affect the performance. So I got two modded Mellanox ConnectX-5 Ex 100 Gig NICs which I had some experience with on NCCL. These cards are very cool with reasonable prices and are quite capable. However, due to the Strix Halo platform limitation, I only got a PCIe 4.0 x4 link. But I was still able to get around 6700 MB/s or roughly 55 Gbps networking between the nodes, which is far better than using IP over Thunderbolt (10 Gbps).&lt;/p&gt; &lt;p&gt;I tried using vLLM first and quickly found out that RCCL is not supported on Strix Halo. :( Then I tried using llama.cpp RPC mode with the &lt;code&gt;-c&lt;/code&gt; flag to enable caching, and here are the results I got:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test Type&lt;/th&gt; &lt;th align="left"&gt;Single Machine w/o rpc&lt;/th&gt; &lt;th align="left"&gt;2.5 Gbps&lt;/th&gt; &lt;th align="left"&gt;10 Gbps (TB)&lt;/th&gt; &lt;th align="left"&gt;50 Gbps&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;653.74&lt;/td&gt; &lt;td align="left"&gt;603.00&lt;/td&gt; &lt;td align="left"&gt;654.03&lt;/td&gt; &lt;td align="left"&gt;663.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;49.73&lt;/td&gt; &lt;td align="left"&gt;30.98&lt;/td&gt; &lt;td align="left"&gt;36.44&lt;/td&gt; &lt;td align="left"&gt;35.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;47.54&lt;/td&gt; &lt;td align="left"&gt;29.13&lt;/td&gt; &lt;td align="left"&gt;35.07&lt;/td&gt; &lt;td align="left"&gt;34.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;601.75&lt;/td&gt; &lt;td align="left"&gt;554.17&lt;/td&gt; &lt;td align="left"&gt;599.76&lt;/td&gt; &lt;td align="left"&gt;611.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;45.81&lt;/td&gt; &lt;td align="left"&gt;27.78&lt;/td&gt; &lt;td align="left"&gt;33.88&lt;/td&gt; &lt;td align="left"&gt;32.67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;44.90&lt;/td&gt; &lt;td align="left"&gt;27.14&lt;/td&gt; &lt;td align="left"&gt;31.33&lt;/td&gt; &lt;td align="left"&gt;32.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;519.40&lt;/td&gt; &lt;td align="left"&gt;485.93&lt;/td&gt; &lt;td align="left"&gt;528.52&lt;/td&gt; &lt;td align="left"&gt;537.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.84&lt;/td&gt; &lt;td align="left"&gt;25.34&lt;/td&gt; &lt;td align="left"&gt;31.22&lt;/td&gt; &lt;td align="left"&gt;30.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.33&lt;/td&gt; &lt;td align="left"&gt;25.01&lt;/td&gt; &lt;td align="left"&gt;30.66&lt;/td&gt; &lt;td align="left"&gt;30.11&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, the Thunderbolt connection almost matches the 50 Gbps MLX5 on token generation. Compared to the non-RPC single node inference, the performance difference is still quite substantial‚Äîwith about a 15 token/s difference‚Äîbut as the context lengthens, the text generation difference somehow gets smaller and smaller. Another strange thing is that somehow the prompt processing is better on RPC over 50 Gbps, even better than the single machine. That's very interesting to see.&lt;/p&gt; &lt;p&gt;During inference, I observed that the network was never used at more than maybe ~100 Mbps or 10 MB/s most of the time, suggesting the gain might not come from bandwidth‚Äîmaybe latency? But I don't have a way to prove what exactly is affecting the performance gain from 2.5 Gbps to 10 Gbps IP over Thunderbolt.&lt;/p&gt; &lt;p&gt;Here is the llama-bench command I'm using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m ./gpt-oss-120b-mxfp4-00001-of-00003.gguf -d 0,512,2048 -n 128,512 -o md --rpc &amp;lt;IP:PORT&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So the result is pretty clear: you don't need a fancy IB card to gain usable results on llama.cpp with Strix Halo. At least until RCCL supports Strix Halo, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hungry_Elk_3276"&gt; /u/Hungry_Elk_3276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ezjtolwnoc0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T03:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot95gj</id>
    <title>Qwen3-VL's perceptiveness is incredible.</title>
    <updated>2025-11-10T09:12:28+00:00</updated>
    <author>
      <name>/u/Trypocopris</name>
      <uri>https://old.reddit.com/user/Trypocopris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.imgur.com/liqVUJd.jpeg"&gt;I took a 4k image and scattered around 6 medium-length words.&lt;/a&gt; &lt;/p&gt; &lt;p&gt;With &lt;code&gt;Qwen3-VL-8B-Instruct-GGUF&lt;/code&gt; and a temperature of &lt;code&gt;0&lt;/code&gt;, an image token count of &lt;code&gt;2300&lt;/code&gt; (seems to be the sweet spot), and the prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Provide transcriptions and bounding boxes for the words in the image. Use JSON format.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is the output:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[ {&amp;quot;bbox_2d&amp;quot;: [160, 867, 181, 879], &amp;quot;text_content&amp;quot;: &amp;quot;steam&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [146, 515, 168, 527], &amp;quot;text_content&amp;quot;: &amp;quot;queen&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [565, 731, 589, 743], &amp;quot;text_content&amp;quot;: &amp;quot;satisfied&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [760, 615, 784, 627], &amp;quot;text_content&amp;quot;: &amp;quot;feather&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [335, 368, 364, 379], &amp;quot;text_content&amp;quot;: &amp;quot;mention&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [515, 381, 538, 392], &amp;quot;text_content&amp;quot;: &amp;quot;cabinet&amp;quot;} ]&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Flawless. No notes. &lt;a href="https://i.imgur.com/5bejqK9.jpeg"&gt;It even got the bounding boxes correct.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How do other models compare?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini 2.5 pro: Hallucinates an answer.&lt;/li&gt; &lt;li&gt;Claude Opus 4: Correctly identifies 3/6 words.&lt;/li&gt; &lt;li&gt;ChatGPT 5: After 5 minutes (!!) of thinking, it finds all 6 words. The bounding boxes are wrong.&lt;/li&gt; &lt;li&gt;DeepSeekOCR: Produces garbage (possible PEBCAK)&lt;/li&gt; &lt;li&gt;PaddleOCR-VL-0.9B: Finds 3 words, hallucinates 2. Doesn't output bounding boxes.&lt;/li&gt; &lt;li&gt;GLM-4.5V: Also perfect results.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Very impressive that such as small model can get such good results, especially considering it's not tuned for OCR.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trypocopris"&gt; /u/Trypocopris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T09:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly. &lt;/p&gt; &lt;p&gt;Our participants today: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
