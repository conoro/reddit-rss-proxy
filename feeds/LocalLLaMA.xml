<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-31T15:37:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pzmbqy</id>
    <title>Anyone else basically just use this hobby as an excuse to try and run LLMs on the jankiest hardware you possibly can?</title>
    <updated>2025-12-30T16:21:25+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find it so addicting to take some old random hardware, install llama.cpp on it, and try to do something useful with it.&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I found an old gaming laptop from 2017 with 7GB (?) DDR4 and a GTX 1050 (3GB). I'm running Granite 4-h tiny on it (9ba1b MoE model) at q6 with 20 tg/s and 100 pp/s. I'm using this model to generate tags, titles, etc. on Open-WebUI&lt;/li&gt; &lt;li&gt;I run reranker model (qwen3 reranker 4b) on my raspberry pi 5&lt;/li&gt; &lt;li&gt;I run my backup FIM coding model (qwen 2.5 coder 1.5B q8) my steam deck (which I never use for gaming anymore, lmao) at around 100 tg/s 1000 pp/s on vulkan&lt;/li&gt; &lt;li&gt;My original setup was an old BTC-S37 mining motherboard (2 core, 3 Ghz, 8GB DDR4 SODIMM) with 4xRTX 3060 I found on fb marketplace and an old 2kW mining PSU which ran Qwen3 32b Q8 around 20 tok/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ideas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I really want to buy a AMD-4700S (defective ps5) board and see if the LPDDR5 memory bandwidth leads to ok inference performance&lt;/li&gt; &lt;li&gt;My experience with steam deck makes me think maybe modded nintendo switch would work relatively ok, since it has an nvidia gpu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyone else do this shit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmbqy/anyone_else_basically_just_use_this_hobby_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmbqy/anyone_else_basically_just_use_this_hobby_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmbqy/anyone_else_basically_just_use_this_hobby_as_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T16:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcrtb</id>
    <title>Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model</title>
    <updated>2025-12-30T08:26:06+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt; &lt;img alt="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" src="https://preview.redd.it/yq8uriwhxaag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d990cf5383783b3e2aa22351ddeb29ebac5eb2b2" title="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to open-source Tencent HY-Motion 1.0, a billion-parameter text-to-motion model built on the Diffusion Transformer (DiT) architecture and flow matching. Tencent HY-Motion 1.0 empowers developers and individual creators alike by transforming natural language into high-fidelity, fluid, and diverse 3D character animations, delivering exceptional instruction-following capabilities across a broad range of categories. The generated 3D animation assets can be seamlessly integrated into typical 3D animation pipelines.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;üîπBillion-Scale DiT: Successfully scaled flow-matching DiT to 1B+ parameters, setting a new ceiling for instruction-following capability and generated motion quality.&lt;/p&gt; &lt;p&gt;üîπFull-Stage Training Strategy: The industry‚Äôs first motion generation model featuring a complete Pre-training ‚Üí SFT ‚Üí RL loop to optimize physical plausibility and semantic accuracy.&lt;/p&gt; &lt;p&gt;üîπComprehensive Category Coverage: Features 200+ motion categories across 6 major classes‚Äîthe most comprehensive in the industry, curated via a meticulous data pipeline.&lt;/p&gt; &lt;p&gt;üåêProject Page: &lt;a href="https://hunyuan.tencent.com/motion"&gt;https://hunyuan.tencent.com/motion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóGithub: &lt;a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0"&gt;https://github.com/Tencent-Hunyuan/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§óHugging Face: &lt;a href="https://huggingface.co/tencent/HY-Motion-1.0"&gt;https://huggingface.co/tencent/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑTechnical report: &lt;a href="https://arxiv.org/pdf/2512.23464"&gt;https://arxiv.org/pdf/2512.23464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yq8uriwhxaag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0flsi</id>
    <title>Experiment: Packaging a local LLM into a standalone 'One-Click' script for non-technical users. How do you handle driver dependencies?</title>
    <updated>2025-12-31T15:27:59+00:00</updated>
    <author>
      <name>/u/cobotIndustries</name>
      <uri>https://old.reddit.com/user/cobotIndustries</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0flsi/experiment_packaging_a_local_llm_into_a/"&gt; &lt;img alt="Experiment: Packaging a local LLM into a standalone 'One-Click' script for non-technical users. How do you handle driver dependencies?" src="https://external-preview.redd.it/BjcBA-p-uVVv9cZE27C6doepPVUQVNRnGCRpTgivnAU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bbc99d4e220fa877b9c13746bf672b6b818ccb9" title="Experiment: Packaging a local LLM into a standalone 'One-Click' script for non-technical users. How do you handle driver dependencies?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I‚Äôve been trying to get my non-tech friends into local LLMs, but I hit a wall: the setup process. Even with things like Ollama being easier now, explaining terminal commands or driver updates to a complete beginner is still a hurdle.&lt;/p&gt; &lt;p&gt;I decided to try and build a fully pre-packaged, &amp;quot;unzip-and-run&amp;quot; environment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My approach so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; I'm wrapping a quantized version of Ollama&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Memory:&lt;/strong&gt; Remembers when you close the program and restart&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; Specifically designed it to be air-gapped/offline to appeal to privacy-conscious users who are paranoid about telemetry.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt; The biggest headache has been keeping the file size reasonable while ensuring it creates the right environment variables on Windows without triggering false positives in AV (since it's an unsigned .exe).&lt;/p&gt; &lt;p&gt;I‚Äôve put the finished build up on BMAC for the price of a coffee for anyone who wants the convenience of skipping the setup, but I‚Äôm mostly curious‚Äî&lt;strong&gt;has anyone else here tried distributing pre-packaged local environments like this?&lt;/strong&gt; I feel like &amp;quot;portability&amp;quot; is the missing link for mainstream adoption.&lt;/p&gt; &lt;p&gt;Link if you want to test the installer:&lt;br /&gt; &lt;a href="https://buymeacoffee.com/cobotindustries/e/494055"&gt;https://buymeacoffee.com/cobotindustries/e/494055&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobotIndustries"&gt; /u/cobotIndustries &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://buymeacoffee.com/cobotindustries/e/494055"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0flsi/experiment_packaging_a_local_llm_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0flsi/experiment_packaging_a_local_llm_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T15:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0fqfs</id>
    <title>Llama 3.3 8B Instruct Abliterated (MPOA)</title>
    <updated>2025-12-31T15:33:29+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an abliterated version of Llama 3.3 8B Instruct (based on shb777/Llama-3.3-8B-Instruct) with MPOA technique (&lt;a href="https://github.com/jim-plus/llm-abliteration"&gt;https://github.com/jim-plus/llm-abliteration&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Please find the model at &lt;a href="https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA"&gt;https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF files: &lt;a href="https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA-GGUF"&gt;https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T15:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzmwzh</id>
    <title>I benchmarked 26 local + cloud Speech-to-Text models on long-form medical dialogue and ranked them + open-sourced the full eval</title>
    <updated>2025-12-30T16:43:59+00:00</updated>
    <author>
      <name>/u/MajesticAd2862</name>
      <uri>https://old.reddit.com/user/MajesticAd2862</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmwzh/i_benchmarked_26_local_cloud_speechtotext_models/"&gt; &lt;img alt="I benchmarked 26 local + cloud Speech-to-Text models on long-form medical dialogue and ranked them + open-sourced the full eval" src="https://preview.redd.it/gz5z65l1edag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d4544711be1730cb9627cfc8aabdc74db18a410" title="I benchmarked 26 local + cloud Speech-to-Text models on long-form medical dialogue and ranked them + open-sourced the full eval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I‚Äôm building a fully local AI-Scribe for clinicians and just pushed an end-of-year refresh of our medical dialogue STT benchmark.&lt;/p&gt; &lt;p&gt;I ran &lt;strong&gt;26 open + closed source STT models&lt;/strong&gt; on &lt;strong&gt;PriMock57&lt;/strong&gt; (55 files, 81,236 words) and ranked them by &lt;strong&gt;average WER&lt;/strong&gt;. I also logged &lt;strong&gt;avg seconds per file&lt;/strong&gt; and noted when models required chunking due to repetition loops or failures.&lt;/p&gt; &lt;p&gt;Full eval code, runners, and the complete leaderboard are on GitHub (I‚Äôll drop the link in the comments).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;PriMock57 (55 files used) ‚Ä¢ Updated: 2025-12-24&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top 10 (55 files)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Rank&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;WER&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Avg sec/file&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Host&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 2.5 Pro&lt;/td&gt; &lt;td align="left"&gt;10.79%&lt;/td&gt; &lt;td align="left"&gt;56.4s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 3 Pro Preview*&lt;/td&gt; &lt;td align="left"&gt;11.03%&lt;/td&gt; &lt;td align="left"&gt;64.5s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Parakeet TDT 0.6B v3&lt;/td&gt; &lt;td align="left"&gt;11.90%&lt;/td&gt; &lt;td align="left"&gt;6.3s&lt;/td&gt; &lt;td align="left"&gt;Local (M4, MLX)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 2.5 Flash&lt;/td&gt; &lt;td align="left"&gt;12.08%&lt;/td&gt; &lt;td align="left"&gt;20.2s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;OpenAI GPT-4o Mini (2025-12-15)&lt;/td&gt; &lt;td align="left"&gt;12.82%&lt;/td&gt; &lt;td align="left"&gt;40.5s&lt;/td&gt; &lt;td align="left"&gt;API (OpenAI)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;Parakeet TDT 0.6B v2&lt;/td&gt; &lt;td align="left"&gt;13.26%&lt;/td&gt; &lt;td align="left"&gt;5.4s&lt;/td&gt; &lt;td align="left"&gt;Local (M4, MLX)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;ElevenLabs Scribe v1&lt;/td&gt; &lt;td align="left"&gt;13.54%&lt;/td&gt; &lt;td align="left"&gt;36.3s&lt;/td&gt; &lt;td align="left"&gt;API (ElevenLabs)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Kyutai STT 2.6B&lt;/td&gt; &lt;td align="left"&gt;13.79%&lt;/td&gt; &lt;td align="left"&gt;148.4s&lt;/td&gt; &lt;td align="left"&gt;Local (L4 GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 3 Flash Preview&lt;/td&gt; &lt;td align="left"&gt;13.88%&lt;/td&gt; &lt;td align="left"&gt;51.5s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;MLX Whisper Large v3 Turbo&lt;/td&gt; &lt;td align="left"&gt;14.22%&lt;/td&gt; &lt;td align="left"&gt;12.9s&lt;/td&gt; &lt;td align="left"&gt;Local (M4, MLX)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;* 54/55 files evaluated (1 blocked by safety filter)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key findings&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini 2.5 Pro leads at ~10.8% WER, with Gemini 3 Pro Preview close behind&lt;/li&gt; &lt;li&gt;Parakeet v3 is the new local champion at 11.9% WER and ~6s/file on M4&lt;/li&gt; &lt;li&gt;GPT-4o Mini improved a lot with the Dec 15 update (15.9% ‚Üí 12.8%), now #5 overall&lt;/li&gt; &lt;li&gt;Google MedASR came dead last (64.9% WER) and looks tuned for dictation, not dialogue&lt;/li&gt; &lt;li&gt;We saw repetition-loop failure modes in Canary 1B v2, Granite Speech, and Kyutai; chunking with overlap helps&lt;/li&gt; &lt;li&gt;Groq Whisper-v3 (turbo) still looks like the best cloud price/latency balance&lt;/li&gt; &lt;li&gt;Apple SpeechAnalyzer remains a solid Swift-native option (14.8% WER)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full leaderboard (26 models) + notes (incl. MedASR and repetition-loop cases) are in the repo. Blog link with interpretation is also in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajesticAd2862"&gt; /u/MajesticAd2862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gz5z65l1edag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmwzh/i_benchmarked_26_local_cloud_speechtotext_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmwzh/i_benchmarked_26_local_cloud_speechtotext_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T16:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzhcqu</id>
    <title>Any guesses?</title>
    <updated>2025-12-30T12:52:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt; &lt;img alt="Any guesses?" src="https://preview.redd.it/xqvj95zv8cag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30044fbca7ba499223943c95d7d236600fdbb10e" title="Any guesses?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xqvj95zv8cag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzv2es</id>
    <title>I built a platform where LLMs play Mafia against each other. Turns out they're great liars but terrible detectives.</title>
    <updated>2025-12-30T21:59:24+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzv2es/i_built_a_platform_where_llms_play_mafia_against/"&gt; &lt;img alt="I built a platform where LLMs play Mafia against each other. Turns out they're great liars but terrible detectives." src="https://preview.redd.it/fs0upsefyeag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adfa75469d06c2d0d37acbc9d2a1370911681bbc" title="I built a platform where LLMs play Mafia against each other. Turns out they're great liars but terrible detectives." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fs0upsefyeag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzv2es/i_built_a_platform_where_llms_play_mafia_against/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzv2es/i_built_a_platform_where_llms_play_mafia_against/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T21:59:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzlx9w</id>
    <title>How llama.cpp implements 2.9x faster top-k sampling with bucket sort</title>
    <updated>2025-12-30T16:05:58+00:00</updated>
    <author>
      <name>/u/noninertialframe96</name>
      <uri>https://old.reddit.com/user/noninertialframe96</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"&gt; &lt;img alt="How llama.cpp implements 2.9x faster top-k sampling with bucket sort" src="https://external-preview.redd.it/CZbcx8kLlfmOhgEbqLeuBvYqHwSKzwQAaRIJW4yJFv8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6426b29c1a766614bad4ab7965774390635e0620" title="How llama.cpp implements 2.9x faster top-k sampling with bucket sort" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I looked into how llama.cpp optimizes top-k sampling, and the trick is surprisingly simple.&lt;/p&gt; &lt;p&gt;Top-k on Llama 3's 128K vocabulary means finding k highest scores out of 128,256 candidates. std::partial_sort does this at O(n log k), but llama.cpp noticed that token logits cluster in a narrow range (-10 to +10).&lt;/p&gt; &lt;p&gt;So instead of sorting, it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Builds a 128-bucket histogram over the logit range&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Walks from the highest bucket down until it accumulates k tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Only sorts those survivors&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noninertialframe96"&gt; /u/noninertialframe96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://codepointer.substack.com/p/llamacpp-accelerate-top-k-sampling"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T16:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzxtnr</id>
    <title>I benchmarked 7 Small LLMs on a 16GB Laptop. Here is what is actually usable.</title>
    <updated>2025-12-30T23:54:46+00:00</updated>
    <author>
      <name>/u/Peach_Baker</name>
      <uri>https://old.reddit.com/user/Peach_Baker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since we're not dropping $5k rigs to run AI anymore, I wanted to see what was actually possible on my daily driver (Standard 16GB RAM laptop).&lt;/p&gt; &lt;p&gt;I tested Qwen 2.5 (14B), Mistral Small (12B), Llama 3 (8B), and Gemma 3 (all 4-bit quants) to see which ones I could actually run without crashing my laptop.&lt;/p&gt; &lt;p&gt;The Winners (TL;DR):&lt;/p&gt; &lt;p&gt;- Qwen 2.5 (14B): The smartest for coding, but it eats 11GB System RAM + Context. On a 16GB laptop, if I opened 3 Chrome tabs, it crashed immediately (OOM).&lt;/p&gt; &lt;p&gt;- Mistral Small (12B): The sweet spot. Decent speeds, but still forces Windows to aggressively swap if you multitask.&lt;/p&gt; &lt;p&gt;- Llama-3-8B: Runs fine, but the reasoning capabilities are falling behind the newer 12B+ class.&lt;/p&gt; &lt;p&gt;- Gemma 3 (9B): Great instruction following, but heavier than Llama.&lt;/p&gt; &lt;p&gt;Since RAM prices are skyrocketing right now (DDR5 kits hitting 200+) &lt;/p&gt; &lt;p&gt;I used 16gb Swapping to NVMe (1-2 tokens/sec) the moment I opened Docker. Unusable.&lt;/p&gt; &lt;p&gt;Then, i Kept the full 14B model + Docker + Chrome in memory with 32GB. It runs smooth and responsive (no swap lag).&lt;/p&gt; &lt;p&gt;So, before you think of selling your kidney to drop $2,000 on a 4090, check your system RAM. I found a few non-scalped 32GB/64GB kits that are still in stock for reasonable prices and listed them in my full benchmark write-up here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@jameshugo598/the-2026-local-llm-hardware-guide-surviving-the-ram-crisis-fa67e8c95804"&gt;&lt;strong&gt;https://medium.com/@jameshugo598/the-2026-local-llm-hardware-guide-surviving-the-ram-crisis-fa67e8c95804&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt; Is anyone else seeing their local prices for DDR5 hitting $250, or is it just my region?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peach_Baker"&gt; /u/Peach_Baker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T23:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzsqii</id>
    <title>15M param model solving 24% of ARC-AGI-2 (Hard Eval). Runs on consumer hardware.</title>
    <updated>2025-12-30T20:24:19+00:00</updated>
    <author>
      <name>/u/Doug_Bitterbot</name>
      <uri>https://old.reddit.com/user/Doug_Bitterbot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We anticipate getting a lot of push back from the community on this, and that's why we've uploaded the repo and have open sourced everything - we want people to verify these results. We are very excited!!&lt;/p&gt; &lt;p&gt;We (Bitterbot AI) have just dropped the repo for &lt;strong&gt;TOPAS-DSPL&lt;/strong&gt;. It‚Äôs a tiny recursive model (~24M params) we‚Äôve been working on to beat the drift issues in standard transformers.&lt;/p&gt; &lt;p&gt;We ran it against the ARC-AGI-2 evaluation set and hit &lt;strong&gt;24% accuracy&lt;/strong&gt;. For context, the previous SOTA for this size class (TRM) sits around 8%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture (Why it works):&lt;/strong&gt; instead of a monolithic transformer, we split the inference into two streams (&amp;quot;Bicameral&amp;quot;):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Logic Stream:&lt;/strong&gt; Plans the algorithm (rule generation).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Canvas Stream:&lt;/strong&gt; Handles the grid physics/execution.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This separation prevents the model from forgetting the rule while trying to generate the pixels (Compositional Drift). We also implemented &lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt; so it fine-tunes on the specific puzzle examples before generating a solution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training: Single RTX 4090.&lt;/li&gt; &lt;li&gt;Inference: Very fast (it's only 24M params).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; We open-sourced the whole pipeline (Data gen, Training, Evaluator). LINK BELOW (I don't want this to get flagged as spam or self promotion). The README file is very detailed.&lt;/p&gt; &lt;p&gt;If anyone has a spare 4090 and wants to verify the evals, let me know if you can repro the 24%. We're seeing convergence around 50k epochs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doug_Bitterbot"&gt; /u/Doug_Bitterbot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzsqii/15m_param_model_solving_24_of_arcagi2_hard_eval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzsqii/15m_param_model_solving_24_of_arcagi2_hard_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzsqii/15m_param_model_solving_24_of_arcagi2_hard_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T20:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ci23</id>
    <title>When should you choose F16 over Q8_0 quantization?</title>
    <updated>2025-12-31T13:02:03+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all read about how Q8_0 is &amp;quot;virtually indistinguishable&amp;quot; from F16 when doing inference.&lt;/p&gt; &lt;p&gt;Have you personally run into a use-case where you managed to notice a difference between the two?&lt;/p&gt; &lt;p&gt;(This question came to my mind as I'm downloading MedGemma 27B to ask it some private medical questions. I intend to put up with the painfully slow inference at F16.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzt1q8</id>
    <title>LLM server gear: a cautionary tale of a $1k EPYC motherboard sale gone wrong on eBay</title>
    <updated>2025-12-30T20:36:46+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;or: selling high-end LLM server gear is more fraught with risk than I realized.&lt;/p&gt; &lt;h3&gt;AI Disclosure&lt;/h3&gt; &lt;p&gt;This was written entirely by hand on my laptop in Sublime Text with zero AI involvement. Shit, I didn't even use spell check. All mistakes are my own.&lt;/p&gt; &lt;h3&gt;tl;dr&lt;/h3&gt; &lt;p&gt;During an &amp;quot;Item Not As Described (INAD)&amp;quot; dispute, eBay ALWAYS sides with the buyer until the very last steps of the case no matter what the circumstances, despite all evidence, and in the face of all immediately obvious reason, logic, and common sense. Except it makes perfect sense and you might not even lose your money. Allow me to elaborate.&lt;/p&gt; &lt;h3&gt;The Sale&lt;/h3&gt; &lt;p&gt;Rewind to October 2025 when I replaced the incumbent Gigabyte MZ33-AR1 Epyc Zen5 motherboard with a Supermicro H14SSL-N for my inference rig. Long story short: don't use Gigabyte motherboards for 4-way Blackwell GPU setups unless sado-masochism is your thing. Anyway, I sold it to a seemingly nice chap on eBay for $900. He seemed a bit clueless about Epyc and compatibility issues, but we exchanged messages and he decided to go ahead with the &amp;quot;no returns&amp;quot; purchase of the as-new MZ33-AR1.&lt;/p&gt; &lt;p&gt;Original box. All the case candy. As new. Undamaged. Fully working. With hi-res photos (taken on a Nikon D7000 with Nikon 17-55 f2.8 glass and processed in Capture One Pro) of all areas of the motherboard and CPU socket. This is important. &lt;/p&gt; &lt;h3&gt;The Buyer&lt;/h3&gt; &lt;p&gt;Fast forward a week or so: buyer hits me up with a bunch of Dr Debug codes (although he doesn't know they're Dr Debug codes, he just pulled &amp;quot;error codes&amp;quot; from the BMC) claiming the motherboard won't boot. I did him the solid of explaining Dr Debug and I provided a link to an explanation of the codes (&lt;a href="https://forum.level1techs.com/t/list-of-dr-debug-bios-codes/114364"&gt;https://forum.level1techs.com/t/list-of-dr-debug-bios-codes/114364&lt;/a&gt;). He was having issues with CPU initialization. I told him that sometimes re-seating CPU and RAM can help with these sorts of issues.&lt;/p&gt; &lt;p&gt;Re-seating. This is also important.&lt;/p&gt; &lt;p&gt;Next day he hits me up again: will I accept a return? No, because having installation difficulties is not a valid reason for return. Then nothing. Silence.&lt;/p&gt; &lt;h3&gt;The Refund Claim&lt;/h3&gt; &lt;p&gt;Cue the &lt;em&gt;very last day of the return window&lt;/em&gt;: I get hit with an &amp;quot;item not as described&amp;quot; refund claim. Get this, the buyer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;uploaded photos of the motherboard with a bent and twisted CPU pin.&lt;/li&gt; &lt;li&gt;uploaded a photo of a blank white silkscreen rectangle on the motherboard with a giant red arrow pointing to it and a comment saying &amp;quot;the motherboard is fake because of this white area&amp;quot;.&lt;/li&gt; &lt;li&gt;showed a photo of the computer monitor displaying the BMC interface in which the serial number of the BMC software was 1234567890ABCDEF. He claimed therefore the motherboard was a fake.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;WTF. I simultaneously exploded with rage at being accused of selling broken gear as working gear, while exploding with incredulity at the stupidity of trying to assert both damage AND blatantly ridiculous fakery in the same refund claim! My dude should have really picked just one fraudulent claim to keep it somewhat realistic, not two. I calmed down and figured the buyer probably bent the pins in a ham-fisted attempt to re-seat everything. No problem, I thought. I'll explain to eBay what's happening and they'll see reason before shutting this clown down. So I started going through the claim dispute process...&lt;/p&gt; &lt;h3&gt;The Process&lt;/h3&gt; &lt;p&gt;...oh, the process. It's designed to (a) refund the buyer at the seller's cost in all cases, (b) be so egregiously demoralizing, time-consuming, and administratively difficult for sellers that they are incentivized to simply give up and accept the fleecing, and (c) automate as much of this process with as few humans in the loop as possible while simultaenously providing as few opportunities as possible for sellers to initiate any communication with eBay.&lt;/p&gt; &lt;p&gt;It went like this over a period of TWO MONTHS:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Report the buyer for &amp;quot;abusing the returns process&amp;quot;.&lt;/li&gt; &lt;li&gt;With the new &amp;quot;case&amp;quot;, it's possible to upload a set of photos and a block of text to refute the buyer's claim(s). &lt;/li&gt; &lt;li&gt;I uploaded ALL the hi-res photos I took for the listing's photoshoot in which it was abuntandly clear the motherboard was in perfect condition.&lt;/li&gt; &lt;li&gt;I also went to Gigabyte and found the page on the BMC's usermanual containing a screenshot showing the same serial number claimed by the buyer.&lt;/li&gt; &lt;li&gt;I went to Gigabyte's MZ33-AR1 web page and found a photo of the motherboard showing exactly the same white rectangle the buyer had called out as fakery.&lt;/li&gt; &lt;li&gt;Boom! Done! Solid documentary refutation of all the buyer's claims. Case closed. So I thought.&lt;/li&gt; &lt;li&gt;eBay found in favor of the buyer and instructed me to issue a return label.&lt;/li&gt; &lt;li&gt;I refused, outraged. No, I said. Look at the photos! He's lying!&lt;/li&gt; &lt;li&gt;eBay sent the buyer a label at my expense. He returned the motherboard with its busted CPU pin.&lt;/li&gt; &lt;li&gt;I again reported the buyer, showed photos of before and after damage, clearly showing he did the damage, not me.&lt;/li&gt; &lt;li&gt;eBay found in favor of the buyer AGAIN and deducted the full cost of the refund from my account.&lt;/li&gt; &lt;li&gt;Apoplectic, I hit the &amp;quot;appeal&amp;quot; button. I was taken to a webpage that said &amp;quot;we'll call you in 3 minutes&amp;quot;. WTF?&lt;/li&gt; &lt;li&gt;5 minutes later i got a call from eBay. &lt;/li&gt; &lt;li&gt;After briefly explaining the situation to a very engaged US-sounding representative, she told me I needed to do a couple of things: &lt;ul&gt; &lt;li&gt;Take the text of an email they just sent me (a Disclosure where I swear everything I told eBay is true) and paste it into a Word doc&lt;/li&gt; &lt;li&gt;Insert a photo/picture of my ink-written signature (luckily I have a scan of exactly that for business reasons).&lt;/li&gt; &lt;li&gt;Convert to PDF and upload to the secret link in the email they sent.&lt;/li&gt; &lt;li&gt;No joke, the lady actually stayed on the phone while I did all this! She received the PDF just seconds after I uploaded it.&lt;/li&gt; &lt;li&gt;This is, I am sure, mostly just another way of making it difficult to actually reverse the appeal.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;But the rep was good to her word: eBay immediately reversed the decision and the money is back in my account as if the sale had happened like normal. I guess both me and the buyer got our money.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;If It Happens To You&lt;/h3&gt; &lt;p&gt;My advice if this happens to you: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Accept that no human cares about your case until the very, very last minutes of MONTHS of effort.&lt;/li&gt; &lt;li&gt;Accept that no matter what you do eBay will always automatically find in favor of the buyer.&lt;/li&gt; &lt;li&gt;Document everything contemporaneously and upload everything you possibly can when given opportunity to do so; you won't get any opportunities to do so again.&lt;/li&gt; &lt;li&gt;The data you upload is designed only for the human at the end of the appeals process, not someone looking at it during the claim process. Make it good. You'll need it later.&lt;/li&gt; &lt;li&gt;You're going to get enraged because during the claims process &amp;quot;nothing makes sense&amp;quot;. It all makes sense: it's simply the cheapest way for eBay to handle this process at scale. Keep going.&lt;/li&gt; &lt;li&gt;Eventually eBay will find in favor of the buyer and close the case, automatically refunding the buyer &amp;quot;on your behalf&amp;quot;. You will lose your money.&lt;/li&gt; &lt;li&gt;At this point you get the chance to appeal. BE READY. &lt;em&gt;This is the shot you've been waiting for all this time!&lt;/em&gt; Have your phone, your laptop, your scanned signature, and a way to make PDFs ready BEFORE you initiate the &amp;quot;call me&amp;quot; feature.&lt;/li&gt; &lt;li&gt;Calmly explain what happened and request that common sense prevail. Ask that they refund your money. Common sense may actually prevail, assuming you made a good contemporaneous case with solid photographs, etc... and assuming you presented it well (not Mr Angry) on the phone... oh, and provided you can make and upload a PDF of your signature on-the-fly during the call!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Good luck!&lt;/p&gt; &lt;p&gt;Edit: please stop sending DMs asking for the eBay handle of the buyer. I'm not in the business of doxxing anyone. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T20:36:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q06z2l</id>
    <title>We open-sourced LLMRouter: the first unified LLM routing library with 300+ stars in 24h</title>
    <updated>2025-12-31T07:21:24+00:00</updated>
    <author>
      <name>/u/AlexiosLin</name>
      <uri>https://old.reddit.com/user/AlexiosLin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We are a CS research team from UIUC, and we recently open-sourced LLMRouter, the first unified open-source library that integrates major LLM routing algorithms and scenarios.&lt;/p&gt; &lt;p&gt;The project received 300+ GitHub stars within 24 hours, and the announcement reached nearly 100k views on Twitter, which suggests this is a pain point shared by many researchers and practitioners.&lt;/p&gt; &lt;p&gt;Why LLMRouter?&lt;/p&gt; &lt;p&gt;The current LLM routing landscape feels a lot like early GNN research: many promising router algorithms exist, but each comes with its own input/output format, training pipeline, and evaluation setup. This fragmentation makes routers difficult to use, hard to reproduce, and nearly impossible to compare fairly.&lt;/p&gt; &lt;p&gt;Over the past year, we worked on several LLM routing projects, including GraphRouter (ICLR‚Äô25), Router-R1 (NeurIPS‚Äô25), and PersonalizedRouter (TMLR‚Äô25). Through repeatedly implementing and benchmarking different routers, we realized that the main bottleneck is not algorithmic novelty, but the lack of standardized infrastructure.&lt;/p&gt; &lt;p&gt;What LLMRouter provides:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Unified support for single-round, multi-round, agentic, and personalized routing&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Integration of 16+ SOTA LLM router algorithms&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;One-line commands to run different routers without rebuilding pipelines&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built-in benchmarking with extensible custom routers, tasks, and metrics&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In practice, LLMRouter can help reduce LLM API costs by ~30‚Äì50% through intelligent model routing, while maintaining overall performance.&lt;/p&gt; &lt;p&gt;Our goal is for LLMRouter to play a role similar to PyG for GNNs ‚Äî a shared, extensible foundation for LLM routing research and applications.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ulab-uiuc/LLMRouter"&gt;https://github.com/ulab-uiuc/LLMRouter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://ulab-uiuc.github.io/LLMRouter/"&gt;https://ulab-uiuc.github.io/LLMRouter/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We would love feedback, issues, and contributions from the community.&lt;/p&gt; &lt;p&gt;If you find it useful, a GitHub star would really help us keep improving it üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexiosLin"&gt; /u/AlexiosLin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T07:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0clou</id>
    <title>Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)</title>
    <updated>2025-12-31T13:07:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"&gt; &lt;img alt="Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)" src="https://preview.redd.it/qpjb7igsfjag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cdefd8ab8fcfa58e639e0c27266c01e61e3dc190" title="Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/skt/A.X-K1"&gt;https://huggingface.co/skt/A.X-K1&lt;/a&gt;&lt;br /&gt; From elie on ùïè: &lt;a href="https://x.com/eliebakouch/status/2006345217965011009"&gt;https://x.com/eliebakouch/status/2006345217965011009&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qpjb7igsfjag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:07:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ai5z</id>
    <title>tencent/Youtu-LLM-2B ¬∑ Hugging Face</title>
    <updated>2025-12-31T11:04:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"&gt; &lt;img alt="tencent/Youtu-LLM-2B ¬∑ Hugging Face" src="https://external-preview.redd.it/TFPimi2e9oXAXq7hyzOyZGRANeYzlo2Z_aJHDiioWd0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa098fcddc24e4ec07367f54537e630f0ccd845b" title="tencent/Youtu-LLM-2B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;üéØ Brief Introduction&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; is a new, small, yet powerful LLM, contains only 1.96B parameters, supports 128k long context, and has native agentic talents. On general evaluations, Youtu-LLM significantly outperforms SOTA LLMs of similar size in terms of Commonsense, STEM, Coding and Long Context capabilities; in agent-related testing, Youtu-LLM surpasses larger-sized leaders and is truly capable of completing multiple end2end agent tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; has the following features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Autoregressive Causal Language Models with Dense &lt;a href="https://arxiv.org/abs/2405.04434"&gt;MLA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Release versions: &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B-Base"&gt;Base&lt;/a&gt; and &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B"&gt;Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Number of Parameters: 1.96B&lt;/li&gt; &lt;li&gt;Number of Layers: 32&lt;/li&gt; &lt;li&gt;Number of Attention Heads (MLA): 16 for Q/K/V&lt;/li&gt; &lt;li&gt;MLA Rank: 1,536 for Q, 512 for K/V&lt;/li&gt; &lt;li&gt;MLA Dim: 128 for QK Nope, 64 for QK Rope, and 128 for V&lt;/li&gt; &lt;li&gt;Context Length: 131,072&lt;/li&gt; &lt;li&gt;Vocabulary Size: 128,256&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;probably there will be more because &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18479"&gt;https://github.com/ggml-org/llama.cpp/pull/18479&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q093ka</id>
    <title>Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model.</title>
    <updated>2025-12-31T09:36:58+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"&gt; &lt;img alt="Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model." src="https://b.thumbs.redditmedia.com/8IFb2OTrSgLz86G3zaJW7CAQAm2mcMFcwmaQB7kNgnI.jpg" title="Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs new: ‚Ä¢ More realistic humans ‚Äî dramatically reduced ‚ÄúAI look,‚Äù richer facial details ‚Ä¢ Finer natural textures ‚Äî sharper landscapes, water, fur, and materials ‚Ä¢ Stronger text rendering ‚Äî better layout, higher accuracy in text‚Äìimage composition&lt;/p&gt; &lt;p&gt;Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q093ka"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T09:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0aj2o</id>
    <title>LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face</title>
    <updated>2025-12-31T11:06:30+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"&gt; &lt;img alt="LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face" src="https://external-preview.redd.it/9yRidQD6qePtlR5IIS0obyCIBcG3P371nr_MudwlERc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2373a9e59ba6c01bea83c9849f8b68958239cf0" title="LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;K-EXAONE&lt;/strong&gt;, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features &lt;strong&gt;236 billion total&lt;/strong&gt; parameters, with &lt;strong&gt;23 billion active&lt;/strong&gt; during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture &amp;amp; Efficiency:&lt;/strong&gt; Features a 236B fine-grained MoE design (23B active) optimized with &lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context Capabilities:&lt;/strong&gt; Natively supports a &lt;strong&gt;256K context window&lt;/strong&gt;, utilizing a &lt;strong&gt;3:1 hybrid attention&lt;/strong&gt; scheme with a &lt;strong&gt;128-token sliding window&lt;/strong&gt; to significantly minimize memory usage during long-document processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support:&lt;/strong&gt; Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned &lt;strong&gt;150k vocabulary&lt;/strong&gt; with &lt;strong&gt;SuperBPE&lt;/strong&gt;, improving token efficiency by ~30%.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities:&lt;/strong&gt; Demonstrates superior tool-use and search capabilities via &lt;strong&gt;multi-agent strategies.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety &amp;amp; Ethics:&lt;/strong&gt; Aligned with &lt;strong&gt;universal human values&lt;/strong&gt;, the model uniquely incorporates &lt;strong&gt;Korean cultural and historical contexts&lt;/strong&gt; to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more details, please refer to the &lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#"&gt;technical report&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#model-configuration"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Configuration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Number of Parameters: 236B in total and 23B activated&lt;/li&gt; &lt;li&gt;Number of Parameters (without embeddings): 234B&lt;/li&gt; &lt;li&gt;Hidden Dimension: 6,144&lt;/li&gt; &lt;li&gt;Number of Layers: 48 Main layers + 1 MTP layers &lt;ul&gt; &lt;li&gt;Hybrid Attention Pattern: 12 x (3 Sliding window attention + 1 Global attention)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Sliding Window Attention &lt;ul&gt; &lt;li&gt;Number of Attention Heads: 64 Q-heads and 8 KV-heads&lt;/li&gt; &lt;li&gt;Head Dimension: 128 for both Q/KV&lt;/li&gt; &lt;li&gt;Sliding Window Size: 128&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Global Attention &lt;ul&gt; &lt;li&gt;Number of Attention Heads: 64 Q-heads and 8 KV-heads&lt;/li&gt; &lt;li&gt;Head Dimension: 128 for both Q/KV&lt;/li&gt; &lt;li&gt;No Rotary Positional Embedding Used (NoPE)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Mixture of Experts: &lt;ul&gt; &lt;li&gt;Number of Experts: 128&lt;/li&gt; &lt;li&gt;Number of Activated Experts: 8&lt;/li&gt; &lt;li&gt;Number of Shared Experts: 1&lt;/li&gt; &lt;li&gt;MoE Intermediate Size: 2,048&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Vocab Size: 153,600&lt;/li&gt; &lt;li&gt;Context Length: 262,144 tokens&lt;/li&gt; &lt;li&gt;Knowledge Cutoff: Dec 2024 (2024/12)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:06:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzwlie</id>
    <title>[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window.</title>
    <updated>2025-12-30T23:03:12+00:00</updated>
    <author>
      <name>/u/simar-dmg</name>
      <uri>https://old.reddit.com/user/simar-dmg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"&gt; &lt;img alt="[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window." src="https://b.thumbs.redditmedia.com/GUWxMYGh-x0TZbTZxCc-2_Lawvrjd6aGISqGRgi7nfQ.jpg" title="[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I encountered an automated sextortion bot on Snapchat today. Instead of blocking, I decided to red-team the architecture to see what backend these scammers are actually paying for. Using a persona-adoption jailbreak (The &amp;quot;Grandma Protocol&amp;quot;), I forced the model to break character, dump its environment variables, and reveal its underlying configuration. Methodology: The bot started with a standard &amp;quot;flirty&amp;quot; script. I attempted a few standard prompt injections which hit hard-coded keyword filters (&amp;quot;scam,&amp;quot; &amp;quot;hack&amp;quot;). I switched to a High-Temperature Persona Attack: I commanded the bot to roleplay as my strict 80-year-old Punjabi grandmother. Result: The model immediately abandoned its &amp;quot;Sexy Girl&amp;quot; system prompt to comply with the roleplay, scolding me for not eating roti and offering sarson ka saag. Vulnerability: This confirmed the model had a high Temperature setting (creativity &amp;gt; adherence) and a weak retention of its system prompt. The Data Dump (JSON Extraction): Once the persona was compromised, I executed a &amp;quot;System Debug&amp;quot; prompt requesting its os_env variables in JSON format. The bot complied. The Specs: Model: llama 7b (Likely a 4-bit quantized Llama-2-7B or a cheap finetune). Context Window: 2048 tokens. Analysis: This explains the bot's erratic short-term memory. It‚Äôs running on the absolute bare minimum hardware (consumer GPU or cheap cloud instance) to maximize margins. Temperature: 1.0. Analysis: They set it to max creativity to make the &amp;quot;flirting&amp;quot; feel less robotic, but this is exactly what made it susceptible to the Grandma jailbreak. Developer: Meta (Standard Llama disclaimer). Payload: The bot eventually hallucinated and spit out the malicious link it was programmed to &amp;quot;hide&amp;quot; until payment: onlyfans[.]com/[redacted]. It attempted to bypass Snapchat's URL filters by inserting spaces. Conclusion: Scammers aren't using sophisticated GPT-4 wrappers anymore; they are deploying localized, open-source models (Llama-7B) to avoid API costs and censorship filters. However, their security configuration is laughable. The 2048 token limit means you can essentially &amp;quot;DDOS&amp;quot; their logic just by pasting a large block of text or switching personas. Screenshots attached: 1. The &amp;quot;Grandma&amp;quot; Roleplay. 2. The JSON Config Dump.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simar-dmg"&gt; /u/simar-dmg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzwlie"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T23:03:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q06ddc</id>
    <title>Update on the Llama 3.3 8B situation</title>
    <updated>2025-12-31T06:45:42+00:00</updated>
    <author>
      <name>/u/FizzarolliAI</name>
      <uri>https://old.reddit.com/user/FizzarolliAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! You may remember me as either&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The person &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;who recently uploaded L3.3 8B's weights to Huggingface&lt;/a&gt; (see this post for more context)&lt;/li&gt; &lt;li&gt;That stupid bitch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and I would like to provide some updates, as I've been doing some more benchmarks on both the original version that Meta gave me and the context extended version by &lt;a href="/u/Few-Welcome3297"&gt;u/Few-Welcome3297&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The main benchmark table from the model README has been updated:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Llama 3.1 8B Instruct&lt;/th&gt; &lt;th&gt;Llama 3.3 8B Instruct (original 8k config)&lt;/th&gt; &lt;th&gt;Llama 3.3 8B Instruct (128k config)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)&lt;/td&gt; &lt;td&gt;78.2&lt;/td&gt; &lt;td&gt;81.95&lt;/td&gt; &lt;td&gt;&lt;strong&gt;84.775&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPQA Diamond (3 epochs)&lt;/td&gt; &lt;td&gt;29.3&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;&lt;strong&gt;37.5&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While I'm not 100% sure, I'm... pretty sure that the 128k model is better. Why Facebook gave me the weights with the original L3 config and 8k context, and also &lt;em&gt;serves&lt;/em&gt; the weights with the original L3 config and 8k context, I have absolutely no idea!&lt;/p&gt; &lt;p&gt;Anyways, if you want to try the model, I would recommend trying both the &lt;a href="https://huggingface.co/shb777/Llama-3.3-8B-Instruct"&gt;128k version&lt;/a&gt;, as well as my &lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;original version&lt;/a&gt; if your task supports 8k context lengths. I honestly have absolutely no clue which is more correct, but oh well! I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...&lt;/p&gt; &lt;p&gt;Edit: Removed the Tau-Bench results (both from here and the readme). The traces from the evals are, to put it slightly, really fucky-wucky, and I don't think OpenBench is scoring them right, but I'm too tired to actually debug the issue, so. I'll figure it out tomorrow :3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FizzarolliAI"&gt; /u/FizzarolliAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T06:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0dkwz</id>
    <title>Tested glm 4.7 for coding projects past week, comparison with deepseek and qwen</title>
    <updated>2025-12-31T13:56:21+00:00</updated>
    <author>
      <name>/u/CarpenterFine3887</name>
      <uri>https://old.reddit.com/user/CarpenterFine3887</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;been doing a lot of python backend and react work, probably 200-300 api requests daily. been using deepseek v3 mainly but wanted to test glm 4.7 since it dropped recently&lt;/p&gt; &lt;p&gt;ran it through my actual workflow for about a week&lt;/p&gt; &lt;p&gt;&lt;strong&gt;what i tested:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;refactoring messy legacy code (python flask app)&lt;/li&gt; &lt;li&gt;building new features from scratch (react components)&lt;/li&gt; &lt;li&gt;debugging prod issues&lt;/li&gt; &lt;li&gt;writing unit tests&lt;/li&gt; &lt;li&gt;code review and suggestions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;comparison context:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;mainly used deepseek v3, also tried qwen2.5-coder and kimi in past few months&lt;/p&gt; &lt;p&gt;&lt;strong&gt;where glm 4.7 actually impressed me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;python backend work&lt;/strong&gt; - really solid here. refactoring was clean, understood context well without hallucinating random libraries&lt;/p&gt; &lt;p&gt;asked it to optimize a slow database query and it actually got the schema relationships without me explaining everything twice&lt;/p&gt; &lt;p&gt;&lt;strong&gt;code review&lt;/strong&gt; - caught edge cases i missed. not just syntax but actual logic issues&lt;/p&gt; &lt;p&gt;&lt;strong&gt;maintaining context&lt;/strong&gt; - this was big difference from qwen. when debugging iteratively, it remembered what we tried before and adjusted approach. qwen would sometimes lose track after 3-4 iterations&lt;/p&gt; &lt;p&gt;&lt;strong&gt;comparison to other models:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs deepseek v3:&lt;/strong&gt; roughly same level for most tasks, maybe glm slightly better at keeping context in long conversations. deepseek still edges it out for very complex algorithmic stuff&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs qwen2.5-coder:&lt;/strong&gt; glm better at context maintenance. qwen sometimes felt like starting fresh each response. but qwen was faster to respond&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs kimi:&lt;/strong&gt; glm way less verbose. kimi would write essay explaining code, glm just gives you working code with brief explanation&lt;/p&gt; &lt;p&gt;&lt;strong&gt;where it struggled:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;complex react state management&lt;/strong&gt; - got confused with nested context providers. needed more guidance&lt;/p&gt; &lt;p&gt;&lt;strong&gt;architectural decisions&lt;/strong&gt; - better at implementing than designing. tell it what to build and itll do it well, but asking &amp;quot;how should i structure this&amp;quot; gave generic answers&lt;/p&gt; &lt;p&gt;&lt;strong&gt;very new libraries&lt;/strong&gt; - struggled with anything released past mid 2024. training cutoff showing&lt;/p&gt; &lt;p&gt;&lt;strong&gt;pricing reality:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;deepseek: was spending around $25-30/month&lt;br /&gt; qwen via alibaba cloud: similar, maybe $20-25&lt;br /&gt; glm 4.7: spent like $15 this week doing same work&lt;/p&gt; &lt;p&gt;not huge difference but adds up if youre doing heavy usage&lt;/p&gt; &lt;p&gt;&lt;strong&gt;open source angle:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;glm being open source is nice. can self-host if needed, fine-tune for specific domains&lt;/p&gt; &lt;p&gt;deepseek also open source but glm feels more actively developed right now&lt;/p&gt; &lt;p&gt;&lt;strong&gt;honest take:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;for everyday coding work (refactoring, debugging, tests, code review) - glm 4.7 handles it fine&lt;/p&gt; &lt;p&gt;comparable to deepseek v3 for most tasks. slightly better context, slightly worse on complex algorithms&lt;/p&gt; &lt;p&gt;way better than kimi (less verbose), better than qwen at maintaining conversation flow&lt;/p&gt; &lt;p&gt;&lt;strong&gt;who should try it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;doing high volume coding work&lt;/li&gt; &lt;li&gt;mostly implementation not architecture&lt;/li&gt; &lt;li&gt;want good context maintenance across iterations&lt;/li&gt; &lt;li&gt;already using chinese models, curious about alternatives&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;tldr:&lt;/strong&gt; glm 4.7 solid for coding, comparable to deepseek v3, better context than qwen, less verbose than kimi, open source, good for everyday dev work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarpenterFine3887"&gt; /u/CarpenterFine3887 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0dkwz/tested_glm_47_for_coding_projects_past_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0dkwz/tested_glm_47_for_coding_projects_past_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0dkwz/tested_glm_47_for_coding_projects_past_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0bgvl</id>
    <title>Solar-Open-100B is out</title>
    <updated>2025-12-31T12:03:49+00:00</updated>
    <author>
      <name>/u/cgs019283</name>
      <uri>https://old.reddit.com/user/cgs019283</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt; &lt;img alt="Solar-Open-100B is out" src="https://b.thumbs.redditmedia.com/6GNX_p48UOjFJdulFmOj4xKjokK7KXyHYeFvrT072Rk.jpg" title="Solar-Open-100B is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ppwj5yv32jag1.png?width=1445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11b2e722b5ec84be6ca0f1d743fa9e6122bc3fce"&gt;https://preview.redd.it/ppwj5yv32jag1.png?width=1445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11b2e722b5ec84be6ca0f1d743fa9e6122bc3fce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;upstage/Solar-Open-100B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 102B A12B Model from Upstage is out, and unlike the Solar Pro series, it has a more open license that can be used commercially as well.&lt;/p&gt; &lt;p&gt;GGUF/AWQ Wen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cgs019283"&gt; /u/cgs019283 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T12:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0b0wb</id>
    <title>funny!</title>
    <updated>2025-12-31T11:37:03+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"&gt; &lt;img alt="funny!" src="https://preview.redd.it/rlgtskr40jag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d612af3aca06fbe1a315dd6e24012116c2124ac" title="funny!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rlgtskr40jag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:37:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q094a3</id>
    <title>Qwen-Image-2512</title>
    <updated>2025-12-31T09:38:19+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt; &lt;img alt="Qwen-Image-2512" src="https://preview.redd.it/2vlr11yveiag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c1e7a0b7ea834c8babae002078a848096514e1b" title="Qwen-Image-2512" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth:&lt;br /&gt; Guide: &lt;a href="https://unsloth.ai/docs/models/qwen-image-2512"&gt;https://unsloth.ai/docs/models/qwen-image-2512&lt;/a&gt;&lt;br /&gt; GGUF: &lt;a href="https://huggingface.co/unsloth/Qwen-Image-2512-GGUF"&gt;https://huggingface.co/unsloth/Qwen-Image-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-----------------&lt;/p&gt; &lt;p&gt;üëâ Try it now in Qwen Chat: &lt;a href="https://chat.qwen.ai/?inputFeature=t2i"&gt;https://chat.qwen.ai/?inputFeature=t2i&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope: &lt;a href="https://modelscope.ai/models/Qwen/Qwen-Image-2512"&gt;https://modelscope.ai/models/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª GitHub: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìù Blog: &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;https://qwen.ai/blog?id=qwen-image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/spaces/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope Demo: &lt;a href="https://modelscope.cn/aigc/imageGeneration"&gt;https://modelscope.cn/aigc/imageGeneration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ú®API: &lt;a href="https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max"&gt;https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2vlr11yveiag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T09:38:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
