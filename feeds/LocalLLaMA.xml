<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-17T15:34:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r6gx75</id>
    <title>Fine-tuned FunctionGemma 270M for multi-turn tool calling - went from 10-39% to 90-97% accuracy</title>
    <updated>2026-02-16T18:04:20+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gx75/finetuned_functiongemma_270m_for_multiturn_tool/"&gt; &lt;img alt="Fine-tuned FunctionGemma 270M for multi-turn tool calling - went from 10-39% to 90-97% accuracy" src="https://preview.redd.it/45vz9gsccwjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eec91fa23850fedabefb7e68dc6cda809eefd2b" title="Fine-tuned FunctionGemma 270M for multi-turn tool calling - went from 10-39% to 90-97% accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google released FunctionGemma a few weeks ago - a 270M parameter model specifically for function calling. Tiny enough to run on a phone CPU at 125 tok/s. The model card says upfront that it needs fine-tuning for multi-turn use cases, and our testing confirmed it: base accuracy on multi-turn tool calling ranged from 9.9% to 38.8% depending on the task.&lt;/p&gt; &lt;p&gt;We fine-tuned it on three different multi-turn tasks using knowledge distillation from a 120B teacher:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Task&lt;/th&gt; &lt;th&gt;Base&lt;/th&gt; &lt;th&gt;Tuned&lt;/th&gt; &lt;th&gt;Teacher (120B)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Smart home control&lt;/td&gt; &lt;td&gt;38.8%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;96.7%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;92.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Banking voice assistant&lt;/td&gt; &lt;td&gt;23.4%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;90.9%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;97.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Shell commands (Gorilla)&lt;/td&gt; &lt;td&gt;9.9%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;96.0%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;97.0%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The smart home and shell command models actually beat the teacher. The banking task is harder (14 functions + ASR noise in the input) but still a massive jump.&lt;/p&gt; &lt;p&gt;All models, training data, and datasets are open:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smart home model: &lt;a href="https://huggingface.co/distil-labs/distil-home-assistant-functiongemma"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Smart home data: &lt;a href="https://github.com/distil-labs/distil-smart-home"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Voice assistant data: &lt;a href="https://github.com/distil-labs/distil-voice-assistant-banking"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Shell commands data + demo: &lt;a href="https://github.com/distil-labs/distil-SHELLper"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full writeup with methodology: &lt;a href="https://www.distillabs.ai/blog/making-functiongemma-work-multi-turn-tool-calling-at-270m-parameters"&gt;Making FunctionGemma Work: Multi-Turn Tool Calling at 270M Parameters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We used &lt;a href="https://www.distillabs.ai/"&gt;Distil Labs&lt;/a&gt; (our platform) for the training pipeline. Happy to answer questions about the process, the results, or FunctionGemma in general.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/45vz9gsccwjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gx75/finetuned_functiongemma_270m_for_multiturn_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gx75/finetuned_functiongemma_270m_for_multiturn_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T18:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r656d7</id>
    <title>Qwen3.5-397B-A17B is out!!</title>
    <updated>2026-02-16T09:29:03+00:00</updated>
    <author>
      <name>/u/lolxdmainkaisemaanlu</name>
      <uri>https://old.reddit.com/user/lolxdmainkaisemaanlu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolxdmainkaisemaanlu"&gt; /u/lolxdmainkaisemaanlu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r74lpd</id>
    <title>Qwen3.5-397B-A17B : a significant step forward in many benchmarks but still too many hallucinations</title>
    <updated>2026-02-17T12:24:10+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r74lpd/qwen35397ba17b_a_significant_step_forward_in_many/"&gt; &lt;img alt="Qwen3.5-397B-A17B : a significant step forward in many benchmarks but still too many hallucinations" src="https://preview.redd.it/oqbxux7as1kg1.jpg?width=140&amp;amp;height=102&amp;amp;auto=webp&amp;amp;s=aaeb113844ae0ecd470ab7681e82c5d3ed5329b7" title="Qwen3.5-397B-A17B : a significant step forward in many benchmarks but still too many hallucinations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oqbxux7as1kg1.jpg?width=1630&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56261ed78d1f6294b431a866d4661fe5ab65cd8a"&gt;benchqwen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Even minimax 2.5 has more hallucinations than 2.1.&lt;/p&gt; &lt;p&gt;Here, however, we're at the same level as the previous one. Why do you think it's so difficult to improve this parameter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r74lpd/qwen35397ba17b_a_significant_step_forward_in_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r74lpd/qwen35397ba17b_a_significant_step_forward_in_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r74lpd/qwen35397ba17b_a_significant_step_forward_in_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T12:24:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6qy55</id>
    <title>Qwen3.5-397B up to 1 million context length</title>
    <updated>2026-02-17T00:22:53+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;262k natively, extensible up to 1M tokens&amp;quot;&lt;/p&gt; &lt;p&gt;Okay, who has tried this? How coherent is it at even 500k tokens? Throw a big code repo in and see if the agent can do work, solve an issue. I know some of you big boys got big rigs. If anyone ever uses past 500k, please don't forget to share with us how performant it was!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6qy55/qwen35397b_up_to_1_million_context_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6qy55/qwen35397b_up_to_1_million_context_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6qy55/qwen35397b_up_to_1_million_context_length/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T00:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6f61k</id>
    <title>Google doesn't love us anymore.</title>
    <updated>2026-02-16T17:01:51+00:00</updated>
    <author>
      <name>/u/DrNavigat</name>
      <uri>https://old.reddit.com/user/DrNavigat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been about 125 years of AI since the last Gemma, Google doesn't love us anymore and has abandoned us to Qwen's rational models. I miss the creativity of Gemma's, and also their really useful sizes.&lt;/p&gt; &lt;p&gt;Don't abandon us, Mommy Google, give us Gemma 4!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrNavigat"&gt; /u/DrNavigat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6f61k/google_doesnt_love_us_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6f61k/google_doesnt_love_us_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6f61k/google_doesnt_love_us_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T17:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6t6j9</id>
    <title>smol-IQ2_XS 113.41 GiB (2.46 BPW)</title>
    <updated>2026-02-17T02:01:32+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6t6j9/smoliq2_xs_11341_gib_246_bpw/"&gt; &lt;img alt="smol-IQ2_XS 113.41 GiB (2.46 BPW)" src="https://external-preview.redd.it/xZxgw1JHuf0bFpG8B9XzpkTKh5cT3IwcxpB8iD03pgY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d16a75500444f9930eebceb22d77efe61fdc80d" title="smol-IQ2_XS 113.41 GiB (2.46 BPW)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No ik_llama.cpp support for today's Qwen3.5-397B-A17B-GGUF yet, but I released a couple mainline llama.cpp imatrix quants including one that will fit in under 128GB.&lt;/p&gt; &lt;p&gt;Its a custom recipe with full Q8_0 for attention so likely about the best in such a small package until we get some ik_llama.cpp SOTA quantization types available.&lt;/p&gt; &lt;p&gt;For similar MoE optimized bigger quants keep an eye on &lt;a href="https://huggingface.co/AesSedai"&gt;https://huggingface.co/AesSedai&lt;/a&gt; who might have something available in the next 6 hours or so... haha...&lt;/p&gt; &lt;p&gt;I've had luck with `opencode` and the mainline llama.cpp autoparser branch, details in the model card as usual. I'll update it once we have ik quants.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Qwen3.5-397B-A17B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6t6j9/smoliq2_xs_11341_gib_246_bpw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6t6j9/smoliq2_xs_11341_gib_246_bpw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T02:01:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6h3ha</id>
    <title>Difference Between QWEN 3 Max-Thinking and QWEN 3.5 on a Spatial Reasoning Benchmark (MineBench)</title>
    <updated>2026-02-16T18:10:29+00:00</updated>
    <author>
      <name>/u/ENT_Alam</name>
      <uri>https://old.reddit.com/user/ENT_Alam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6h3ha/difference_between_qwen_3_maxthinking_and_qwen_35/"&gt; &lt;img alt="Difference Between QWEN 3 Max-Thinking and QWEN 3.5 on a Spatial Reasoning Benchmark (MineBench)" src="https://preview.redd.it/6q4jnllcdwjg1.gif?frame=1&amp;amp;width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=417f825a142e023132f72a89b718db9c2b167d19" title="Difference Between QWEN 3 Max-Thinking and QWEN 3.5 on a Spatial Reasoning Benchmark (MineBench)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly it's quite an insane improvement, QWEN 3.5 even had some builds that were closer to (if not better than) Opus 4.6/GPT-5.2/Gemini 3 Pro.&lt;/p&gt; &lt;p&gt;Benchmark: &lt;a href="https://minebench.ai/"&gt;https://minebench.ai/&lt;/a&gt;&lt;br /&gt; Git Repository: &lt;a href="https://github.com/Ammaar-Alam/minebench"&gt;https://github.com/Ammaar-Alam/minebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ClaudeAI/comments/1qx3war/difference_between_opus_46_and_opus_45_on_my_3d/"&gt;Previous post comparing Opus 4.5 and 4.6, also answered some questions about the benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/OpenAI/comments/1r3v8sd/difference_between_opus_46_and_gpt52_pro_on_a/"&gt;Previous post comparing Opus 4.6 and GPT-5.2 Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Disclaimer: This is a benchmark I made, so technically self-promotion, but I thought it was a cool comparison :)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ENT_Alam"&gt; /u/ENT_Alam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r6h3ha"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6h3ha/difference_between_qwen_3_maxthinking_and_qwen_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6h3ha/difference_between_qwen_3_maxthinking_and_qwen_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T18:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6g14s</id>
    <title>4 of the top 5 most used models on OpenRouter this week are Open Source!</title>
    <updated>2026-02-16T17:32:44+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6g14s/4_of_the_top_5_most_used_models_on_openrouter/"&gt; &lt;img alt="4 of the top 5 most used models on OpenRouter this week are Open Source!" src="https://preview.redd.it/54xxp91s6wjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10b8a71332018921514258bd081fc7ed68e28e72" title="4 of the top 5 most used models on OpenRouter this week are Open Source!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54xxp91s6wjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6g14s/4_of_the_top_5_most_used_models_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6g14s/4_of_the_top_5_most_used_models_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T17:32:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r70ft2</id>
    <title>Could High Bandwidth Flash be Local Inference's saviour?</title>
    <updated>2026-02-17T08:18:13+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ft2/could_high_bandwidth_flash_be_local_inferences/"&gt; &lt;img alt="Could High Bandwidth Flash be Local Inference's saviour?" src="https://external-preview.redd.it/cAfiT96SFc2FYsJrwt9QsIxyggovfrz3PXPwxUjYvlg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d41f3f12a65c8dfa226d23b46acaac55d170e5" title="Could High Bandwidth Flash be Local Inference's saviour?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are starved for VRAM, but in a local setting, a large part of that VRAM requirement is due to model weights.&lt;/p&gt; &lt;p&gt;By putting this on cheaper HBF, if we assume a 10x cost advantage, instead of 32GB VRAM on a GPU, we could put 32GB VRAM plus 256GB of HBF.&lt;/p&gt; &lt;p&gt;With 4 of these, you'd have 128GB of VRAM and 1TB of HBF. Enough to run bigger models. With 8 of them, you could run the largest models locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.eetimes.com/nand-reimagined-in-high-bandwidth-flash-to-complement-hbm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ft2/could_high_bandwidth_flash_be_local_inferences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ft2/could_high_bandwidth_flash_be_local_inferences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T08:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r74kl8</id>
    <title>built a local semantic file search because normal file search doesn’t understand meaning</title>
    <updated>2026-02-17T12:22:39+00:00</updated>
    <author>
      <name>/u/Humble-Plastic-5285</name>
      <uri>https://old.reddit.com/user/Humble-Plastic-5285</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r74kl8/built_a_local_semantic_file_search_because_normal/"&gt; &lt;img alt="built a local semantic file search because normal file search doesn’t understand meaning" src="https://preview.redd.it/su8cizras1kg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f2d867789605b92c642056bc84c72640116d5c3b" title="built a local semantic file search because normal file search doesn’t understand meaning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;spotlight / windows search / recall anything.&lt;/p&gt; &lt;p&gt;i kept searching for stuff like “that pdf about distributed systems i read last winter” and getting useless results, so i hacked together a small local semantic search tool in rust.&lt;/p&gt; &lt;p&gt;it crawls your files, generates embeddings locally, stores vectors and does cosine similarity search. no cloud, no api keys, no telemetry. everything stays on your machine.&lt;/p&gt; &lt;p&gt;ui is tauri. vector search is brute force for now (yeah, i know). it’s not super optimized but it works surprisingly well for personal use.&lt;/p&gt; &lt;p&gt;threw it on github in case anyone wants to mess with it or point out terrible decisions.&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/illegal-instruction-co/recall-lite"&gt;https://github.com/illegal-instruction-co/recall-lite&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble-Plastic-5285"&gt; /u/Humble-Plastic-5285 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/su8cizras1kg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r74kl8/built_a_local_semantic_file_search_because_normal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r74kl8/built_a_local_semantic_file_search_because_normal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T12:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r75ip2</id>
    <title>I built an open-source memory API with 3 memory types (semantic, episodic, procedural) — free alternative to Mem0</title>
    <updated>2026-02-17T13:06:58+00:00</updated>
    <author>
      <name>/u/mengram-ai</name>
      <uri>https://old.reddit.com/user/mengram-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I've been working on this for a few months and wanted to share.&lt;/p&gt; &lt;p&gt;The problem: every AI memory tool I found (Mem0, Supermemory, etc.) stores flat facts like &amp;quot;user likes Python&amp;quot;. That's it. No context about WHEN something happened or HOW the user does things.&lt;/p&gt; &lt;p&gt;So I built Mengram with 3 memory types based on how human memory actually works:&lt;/p&gt; &lt;p&gt;- Semantic — facts, preferences, skills (&amp;quot;uses PostgreSQL&amp;quot;, &amp;quot;lives in Almaty&amp;quot;)&lt;/p&gt; &lt;p&gt;- Episodic — events with context (&amp;quot;spent 3 hours debugging auth bug on Feb 17, fixed by changing cache TTL&amp;quot;) &lt;/p&gt; &lt;p&gt;- Procedural — learned workflows (&amp;quot;debug process: check logs → reproduce locally → fix → deploy&amp;quot;)&lt;/p&gt; &lt;p&gt;You just call add() with your conversation and it automatically extracts all 3 types. No manual tagging.&lt;/p&gt; &lt;p&gt;There's also a &amp;quot;Cognitive Profile&amp;quot; endpoint that generates a ready system prompt from all your memories — basically instant personalization for any LLM.&lt;/p&gt; &lt;p&gt;Stack: Python/FastAPI, PostgreSQL + pgvector, OpenAI embeddings. Works as MCP server (Claude Desktop, Cursor) or REST API. Has Python and JS SDKs, LangChain integration.&lt;/p&gt; &lt;p&gt;Free and open-source (Apache 2.0): &lt;a href="https://github.com/alibaizhanov/mengram"&gt;https://github.com/alibaizhanov/mengram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://mengram.io"&gt;https://mengram.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback. Especially interested if the 3 memory types actually make sense to people or if I'm overcomplicating things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mengram-ai"&gt; /u/mengram-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r75ip2/i_built_an_opensource_memory_api_with_3_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r75ip2/i_built_an_opensource_memory_api_with_3_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r75ip2/i_built_an_opensource_memory_api_with_3_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T13:06:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7167y</id>
    <title>Qwen3.5-397B-A17B is available on HuggingChat</title>
    <updated>2026-02-17T09:05:43+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7167y/qwen35397ba17b_is_available_on_huggingchat/"&gt; &lt;img alt="Qwen3.5-397B-A17B is available on HuggingChat" src="https://external-preview.redd.it/ia5f3f6TkU1Fxh2JiiJbzfkoPGZq9srjI7VSDvG7b8s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27f52b3a0647ccf2588eff9b6297306164a7c371" title="Qwen3.5-397B-A17B is available on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/models/Qwen/Qwen3.5-397B-A17B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7167y/qwen35397ba17b_is_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7167y/qwen35397ba17b_is_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T09:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r787nn</id>
    <title>Zero Shot Transferable Adapter</title>
    <updated>2026-02-17T14:58:11+00:00</updated>
    <author>
      <name>/u/ShotokanOSS</name>
      <uri>https://old.reddit.com/user/ShotokanOSS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r787nn/zero_shot_transferable_adapter/"&gt; &lt;img alt="Zero Shot Transferable Adapter" src="https://preview.redd.it/4riq1hxaj2kg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83434c22c14e3b29fb42a0c547dbfe74ebc1e5be" title="Zero Shot Transferable Adapter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just did it! With our new methode we can train adapter on small models and then transfer them to huger ones without more fine tunning! In the table you see Zero shot transfer ability. &lt;/p&gt; &lt;p&gt;Its really simple we just train small adapters which improve the soft targets of the model itself instead of doing it in the weights like normal. &lt;/p&gt; &lt;p&gt;That makes the fine tunning process a way cheaper and gives the possibilty to transfer from small to huge models as long as the tokenizer stays the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShotokanOSS"&gt; /u/ShotokanOSS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4riq1hxaj2kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r787nn/zero_shot_transferable_adapter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r787nn/zero_shot_transferable_adapter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:58:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1r72ddg</id>
    <title>Qwen 3.5 vs Gemini 3 Pro on Screenshot-to-Code: Is the gap finally gone?</title>
    <updated>2026-02-17T10:20:28+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r72ddg/qwen_35_vs_gemini_3_pro_on_screenshottocode_is/"&gt; &lt;img alt="Qwen 3.5 vs Gemini 3 Pro on Screenshot-to-Code: Is the gap finally gone?" src="https://preview.redd.it/5bfrjlz861kg1.jpg?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=d437ebbb410b077aa5b02561d21953400cde3a16" title="Qwen 3.5 vs Gemini 3 Pro on Screenshot-to-Code: Is the gap finally gone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing the new Qwen 3.5-397B against Gemini 3 and Kimi K2.5. The task was simple but tricky: Give it a high-res screenshot of a complex Hugging Face dataset page and ask for a functional Tailwind frontend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The results are… interesting.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen 3.5 (The Layout King):&lt;/strong&gt; I was genuinely surprised. It nailed the sidebar grid better than Gemini. While Gemini usually wins on &amp;quot;vibes,&amp;quot; Qwen actually followed the structural constraints of the UI better. It didn't hallucinate the layout as much as Kimi did.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro:&lt;/strong&gt; Still has the edge on OCR. It’s the only one that correctly grabbed the tiny SVG logos (pandas/polars). Qwen just put generic icons there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kimi K2.5:&lt;/strong&gt; Feels very &amp;quot;polished&amp;quot; in terms of code quality (cleaner components), but it took too many creative liberties with the layout.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Local Context:&lt;/strong&gt; I was testing this via openrouter. If you're running the 397B locally on a Mac or a cluster, the MoE efficiency makes the inference speed surprisingly usable.&lt;/p&gt; &lt;p&gt;Is anyone else seeing Qwen outperform Gemini on structural vision tasks? I feel like we’re hitting a point where open-access models are basically on par for coding agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r72ddg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r72ddg/qwen_35_vs_gemini_3_pro_on_screenshottocode_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r72ddg/qwen_35_vs_gemini_3_pro_on_screenshottocode_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T10:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6ghty</id>
    <title>Qwen 3.5 goes bankrupt on Vending-Bench 2</title>
    <updated>2026-02-16T17:49:21+00:00</updated>
    <author>
      <name>/u/Deep-Vermicelli-4591</name>
      <uri>https://old.reddit.com/user/Deep-Vermicelli-4591</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ghty/qwen_35_goes_bankrupt_on_vendingbench_2/"&gt; &lt;img alt="Qwen 3.5 goes bankrupt on Vending-Bench 2" src="https://preview.redd.it/dj0x1zeo9wjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e961cc9b6b922483e81871381dc9dca90d6aae60" title="Qwen 3.5 goes bankrupt on Vending-Bench 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep-Vermicelli-4591"&gt; /u/Deep-Vermicelli-4591 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dj0x1zeo9wjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ghty/qwen_35_goes_bankrupt_on_vendingbench_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ghty/qwen_35_goes_bankrupt_on_vendingbench_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T17:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77fz7</id>
    <title>Qwen3.5 NVFP4 (Blackwell) is up!</title>
    <updated>2026-02-17T14:27:43+00:00</updated>
    <author>
      <name>/u/TeekayTK</name>
      <uri>https://old.reddit.com/user/TeekayTK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quantized with NVIDIA's Model Optimizer to FP4. Checkpoint is ~224GB total, 17B active parameters. Apache 2.0 license.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF:&lt;/strong&gt; &lt;a href="https://huggingface.co/vincentzed-hf/Qwen3.5-397B-A17B-NVFP4"&gt;vincentzed-hf/Qwen3.5-397B-A17B-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You need SGLang from a specific branch that fixes visual encoder weight handling during quantized inference: (Basically, it was trying to quantize the vision weights, we didn't do that).&lt;/p&gt; &lt;p&gt;&lt;code&gt; git clone -b vz/qwen3-5 git@github.com:bzhng-development/sglang.git cd sglang uv pip install -e &amp;quot;python&amp;quot; uv pip install transformers==5.2.0 &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Launch (B200/B300, TP=4)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; python3 -m sglang.launch_server \ --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \ --quantization modelopt_fp4 \ --tp 4 \ --context-length 262144 \ --reasoning-parser qwen3 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Set &lt;code&gt;--tp 8&lt;/code&gt; for RTX PRO 6000s or if you're running into OOM.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Speculative Decoding (Experimental)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5 has a built-in Multi-Token Prediction head. Worth trying if you have few concurrent users:&lt;/p&gt; &lt;p&gt;&lt;code&gt; SGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \ --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \ --quantization modelopt_fp4 \ --tp 8 \ --context-length 262144 \ --reasoning-parser qwen3 \ --speculative-algo NEXTN \ --speculative-num-steps 3 \ --speculative-eagle-topk 1 \ --speculative-num-draft-tokens 4 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;If you run into issues (i.e server crashes), you also also remove &lt;code&gt;SGLANG_ENABLE_SPEC_V2=1&lt;/code&gt; but it can boost up to 10% performance by overlapping some CUDA operations, so it's generally helpful.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Hardware Requirements&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Config&lt;/th&gt; &lt;th&gt;GPUs&lt;/th&gt; &lt;th&gt;VRAM/GPU&lt;/th&gt; &lt;th&gt;Throughput&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;B300 TP=4&lt;/td&gt; &lt;td&gt;4x B300&lt;/td&gt; &lt;td&gt;288 GB&lt;/td&gt; &lt;td&gt;~120 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;B200 TP=4&lt;/td&gt; &lt;td&gt;4x B200&lt;/td&gt; &lt;td&gt;192 GB&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX PRO 6000 TP=8&lt;/td&gt; &lt;td&gt;8x RTX PRO 6000&lt;/td&gt; &lt;td&gt;96 GB&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Default context is 262K tokens. If you hit OOM, reduce it — but try to keep at least 128K to preserve thinking quality. We are working on the 1M context support.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Key specs:&lt;/strong&gt; 397B total params, 17B active (MoE with 512 experts, 10 active per token), 262K native context (extensible to 1M+), multimodal (text + image + video), supports 201 languages, built-in thinking mode, all the good stuff from Qwen3.5 (Nothing changed, ~99% accuracy)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeekayTK"&gt; /u/TeekayTK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77swh</id>
    <title>I gave 12 LLMs $2,000 and a food truck. Only 4 survived.</title>
    <updated>2026-02-17T14:42:06+00:00</updated>
    <author>
      <name>/u/Disastrous_Theme5906</name>
      <uri>https://old.reddit.com/user/Disastrous_Theme5906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt; &lt;img alt="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." src="https://preview.redd.it/4sewtkexf2kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0f7608e083eece043f2953690650ad7c16596a5" title="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a business sim where AI agents run a food truck for 30 days — location, menu, pricing, staff, inventory. Same scenario for all models.&lt;/p&gt; &lt;p&gt;Opus made $49K. GPT-5.2 $28K. 8 went bankrupt. Every model that took a loan went bankrupt (8/8).&lt;/p&gt; &lt;p&gt;There's also a playable mode — same simulation, same 34 tools, same leaderboard. You either survive 30 days or go bankrupt, get a result card and land on the shared leaderboard. Example result: &lt;a href="https://foodtruckbench.com/r/9E6925"&gt;https://foodtruckbench.com/r/9E6925&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark + leaderboard: &lt;a href="https://foodtruckbench.com"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Play: &lt;a href="https://foodtruckbench.com/play"&gt;https://foodtruckbench.com/play&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemini 3 Flash Thinking — only model out of 20+ tested that gets stuck in an infinite decision loop, 100% of runs: &lt;a href="https://foodtruckbench.com/blog/gemini-flash"&gt;https://foodtruckbench.com/blog/gemini-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the sim or results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Theme5906"&gt; /u/Disastrous_Theme5906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4sewtkexf2kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71af3</id>
    <title>[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)</title>
    <updated>2026-02-17T09:13:03+00:00</updated>
    <author>
      <name>/u/mazuj2</name>
      <uri>https://old.reddit.com/user/mazuj2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"&gt; &lt;img alt="[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)" src="https://preview.redd.it/t250hgafu0kg1.png?width=140&amp;amp;height=28&amp;amp;auto=webp&amp;amp;s=bd66f5c19f65460627ab36584f3996a261d1155c" title="[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM) - The fix nobody else figured out&lt;/p&gt; &lt;p&gt;Hey fellow 50 series brothers in pain,&lt;/p&gt; &lt;p&gt;I've been banging my head against this for a while and finally cracked it through pure trial and error. Posting this so nobody else has to suffer.&lt;/p&gt; &lt;p&gt;My Hardware:&lt;/p&gt; &lt;p&gt;RTX 5070 Ti (16GB VRAM)&lt;/p&gt; &lt;p&gt;RTX 5060 Ti (16GB VRAM)&lt;/p&gt; &lt;p&gt;32GB total VRAM&lt;/p&gt; &lt;p&gt;64GB System RAM&lt;/p&gt; &lt;p&gt;Windows 11&lt;/p&gt; &lt;p&gt;llama.cpp b8077 (CUDA 12.4 build)&lt;/p&gt; &lt;p&gt;Model: Qwen3-Next-80B-A3B-Instruct-UD-IQ2_XXS.gguf (26.2GB)&lt;/p&gt; &lt;p&gt;The Problem:&lt;/p&gt; &lt;p&gt;Out of the box, Qwen3-Next was running at 6.5 tokens/sec with:&lt;/p&gt; &lt;p&gt;CPU usage 25-55% going absolutely insane during thinking AND generation&lt;/p&gt; &lt;p&gt;GPUs sitting at 0% during thinking phase&lt;/p&gt; &lt;p&gt;5070 Ti at 5-10% during generation&lt;/p&gt; &lt;p&gt;5060 Ti at 10-40% during generation&lt;/p&gt; &lt;p&gt;~34GB of system RAM being consumed&lt;/p&gt; &lt;p&gt;Model clearly bottlenecked on CPU&lt;/p&gt; &lt;p&gt;Every suggestion I found online said the same generic things:&lt;/p&gt; &lt;p&gt;&amp;quot;Check your n_gpu_layers&amp;quot; ✅ already 999, all 49 layers on GPU&lt;/p&gt; &lt;p&gt;&amp;quot;Check your tensor split&amp;quot; ✅ tried everything&lt;/p&gt; &lt;p&gt;&amp;quot;Use CUDA 12.8+&amp;quot; ✅ not the issue&lt;/p&gt; &lt;p&gt;&amp;quot;Your offloading is broken&amp;quot; ❌ WRONG - layers were fully on GPU&lt;/p&gt; &lt;p&gt;The load output PROVED layers were on GPU:&lt;/p&gt; &lt;p&gt;load_tensors: offloaded 49/49 layers to GPU&lt;/p&gt; &lt;p&gt;load_tensors: CPU_Mapped model buffer size = 166.92 MiB (just metadata)&lt;/p&gt; &lt;p&gt;load_tensors: CUDA0 model buffer size = 12617.97 MiB&lt;/p&gt; &lt;p&gt;load_tensors: CUDA1 model buffer size = 12206.31 MiB&lt;/p&gt; &lt;p&gt;So why was CPU going nuts? Nobody had the right answer.&lt;/p&gt; &lt;p&gt;The Fix - Two flags that nobody mentioned together:&lt;/p&gt; &lt;p&gt;Step 1: Force ALL MoE experts off CPU&lt;/p&gt; &lt;p&gt;--n-cpu-moe 0&lt;/p&gt; &lt;p&gt;Start here. Systematically reduce from default down to 0. Each step helps. At 0 you still get CPU activity but it's better.&lt;/p&gt; &lt;p&gt;Step 2: THIS IS THE KEY ONE&lt;/p&gt; &lt;p&gt;Change from -sm row to:&lt;/p&gt; &lt;p&gt;-sm layer&lt;/p&gt; &lt;p&gt;Row-split (-sm row) splits each expert's weight matrix across both GPUs. This means every single expert call requires GPU-to-GPU communication over PCIe. For a model with 128 experts firing 8 per token, that's constant cross-GPU chatter killing your throughput.&lt;/p&gt; &lt;p&gt;Layer-split (-sm layer) assigns complete layers/experts to one GPU. Each GPU owns its experts fully. No cross-GPU communication during routing. The GPUs work independently and efficiently.&lt;/p&gt; &lt;p&gt;BOOM. 39 tokens/sec.&lt;/p&gt; &lt;p&gt;The Winning Command:&lt;/p&gt; &lt;p&gt;llama-server.exe -m Qwen3-Next-80B-A3B-Instruct-UD-IQ2_XXS.gguf -ngl 999 -c 4096 --port 8081 --n-cpu-moe 0 -t 6 -fa auto -sm layer&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;Before: 6.5 t/s, CPU melting, GPUs doing nothing&lt;/p&gt; &lt;p&gt;After: 38-39 t/s, CPUs chill, GPUs working properly&lt;/p&gt; &lt;p&gt;That's a 6x improvement with zero hardware changes&lt;/p&gt; &lt;p&gt;Why this works (the actual explanation):&lt;/p&gt; &lt;p&gt;Qwen3-Next uses a hybrid architecture — DeltaNet linear attention combined with high-sparsity MoE (128 experts, 8 active per token). When you row-split a MoE model across two GPUs, the expert weights are sliced horizontally across both cards. Every expert activation requires both GPUs to coordinate and combine results. With 8 experts firing per token across 47 layers, you're generating thousands of cross-GPU sync operations per token.&lt;/p&gt; &lt;p&gt;Layer-split instead assigns whole layers to each GPU. Experts live entirely on one card. The routing decision sends the computation to whichever GPU owns that expert. Clean, fast, no sync overhead.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;p&gt;The 166MB CPU_Mapped is normal — that's just mmap metadata and tokenizer, not model weights&lt;/p&gt; &lt;p&gt;-t 6 sets CPU threads for the tiny bit of remaining CPU work&lt;/p&gt; &lt;p&gt;-fa auto enables flash attention where supported&lt;/p&gt; &lt;p&gt;This is on llama.cpp b8077 — make sure you're on a recent build that has Qwen3-Next support (merged in b7186)&lt;/p&gt; &lt;p&gt;Model fits in 32GB with ~7GB headroom for KV cache&lt;/p&gt; &lt;p&gt;Hope this saves someone's sanity. Took me way too long to find this and I couldn't find it documented anywhere.&lt;/p&gt; &lt;p&gt;If this helped you, drop a comment — curious how it performs on other 50 series configurations.&lt;/p&gt; &lt;p&gt;— RJ&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t250hgafu0kg1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38348a8169ecc5856a6b99b33d79668daa0e087d"&gt;https://preview.redd.it/t250hgafu0kg1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38348a8169ecc5856a6b99b33d79668daa0e087d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mazuj2"&gt; /u/mazuj2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T09:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6w0la</id>
    <title>Where are Qwen 3.5 2B, 9B, and 35B-A3B</title>
    <updated>2026-02-17T04:12:03+00:00</updated>
    <author>
      <name>/u/Admirable_Flower_287</name>
      <uri>https://old.reddit.com/user/Admirable_Flower_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where did leakers go&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Flower_287"&gt; /u/Admirable_Flower_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T04:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r76d34</id>
    <title>Qwen3.5 vs GLM-4.7 vs Qwen3-235B-Thinking</title>
    <updated>2026-02-17T13:43:44+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the NVMe prices skyrocketed recently, and my existing drive is telling me to gtfo each time i can see chinese folk releasing a new open weight model, the question arises: &lt;/p&gt; &lt;p&gt;Qwen3.5 vs GLM-4.7 vs Qwen3-235B-Thinking, is the new one worth updating?&lt;/p&gt; &lt;p&gt;To be precise, my current setup is 128GB ram + 48GB vram, so i could run Qwen3.5 IQ3_XXS while Qwen3-235B runs at Q4_K_XL. I can also run GLM-4.7 at Q3_K_XL.&lt;/p&gt; &lt;p&gt;I found Qwen3-235b-thinking quite capable in writing documents for my work so I'm reluctant trashing it just like that.&lt;/p&gt; &lt;p&gt;Has anyone compared these models? Is the newest the best?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76d34/qwen35_vs_glm47_vs_qwen3235bthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76d34/qwen35_vs_glm47_vs_qwen3235bthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r76d34/qwen35_vs_glm47_vs_qwen3235bthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T13:43:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r76gi7</id>
    <title>I made a CLI that turns any podcast or YouTube video into clean Markdown transcripts (speaker labels + timestamps)</title>
    <updated>2026-02-17T13:47:54+00:00</updated>
    <author>
      <name>/u/timf34</name>
      <uri>https://old.reddit.com/user/timf34</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/"&gt; &lt;img alt="I made a CLI that turns any podcast or YouTube video into clean Markdown transcripts (speaker labels + timestamps)" src="https://preview.redd.it/c8c0loeh72kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b787cd8de924691b14d8780e905e538b1895203" title="I made a CLI that turns any podcast or YouTube video into clean Markdown transcripts (speaker labels + timestamps)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a tiny CLI to turn podcasts or YouTube videos into clean Markdown transcripts (speakers + timestamps).&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install podscript&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Uses ElevenLabs for high-quality diarization.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/timf34/podscript"&gt;https://github.com/timf34/podscript&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timf34"&gt; /u/timf34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8c0loeh72kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T13:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71lu7</id>
    <title>Qwen 3.5, replacement to Llama 4 Scout?</title>
    <updated>2026-02-17T09:33:24+00:00</updated>
    <author>
      <name>/u/redjojovic</name>
      <uri>https://old.reddit.com/user/redjojovic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71lu7/qwen_35_replacement_to_llama_4_scout/"&gt; &lt;img alt="Qwen 3.5, replacement to Llama 4 Scout?" src="https://preview.redd.it/pjuceb62y0kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c7e44de2c66f97a35831fb81ad45a2c9aaa9afa" title="Qwen 3.5, replacement to Llama 4 Scout?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is Qwen 3.5 a direct replacement to Llama 4 in your opinion? Seems too much of a coincidence&lt;/p&gt; &lt;p&gt;Edit: 3.5 Plus and not Max&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redjojovic"&gt; /u/redjojovic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pjuceb62y0kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71lu7/qwen_35_replacement_to_llama_4_scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r71lu7/qwen_35_replacement_to_llama_4_scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T09:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r70ohs</id>
    <title>Tiny Aya</title>
    <updated>2026-02-17T08:33:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Summary&lt;/h1&gt; &lt;p&gt;Cohere Labs Tiny Aya is an open weights research release of a pretrained 3.35 billion parameter model optimized for efficient, strong, and balanced multilingual representation across 70+ languages, including many lower-resourced ones. The model is designed to support downstream adaptation, instruction tuning, and local deployment under realistic compute constraints.&lt;/p&gt; &lt;p&gt;Developed by: &lt;a href="https://cohere.com/"&gt;Cohere&lt;/a&gt; and &lt;a href="https://cohere.com/research"&gt;Cohere&lt;/a&gt; Labs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Point of Contact: &lt;a href="https://cohere.com/research"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;License: &lt;a href="https://cohere.com/cohere-labs-cc-by-nc-license"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy"&gt;&lt;strong&gt;Cohere Lab's Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: tiny-aya-it-global&lt;/li&gt; &lt;li&gt;Model Size: 3.35B&lt;/li&gt; &lt;li&gt;Context length: 8K input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more details about this model family, please check out our &lt;a href="https://cohere.com/blog/cohere-labs-tiny-aya"&gt;blog post&lt;/a&gt; and &lt;a href="https://github.com/Cohere-Labs/tiny-aya-tech-report/blob/main/tiny_aya_tech_report.pdf"&gt;tech report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;looks like different models are for different families of languages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-earth-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-earth-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-fire-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-fire-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-water-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-water-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-global-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-global-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Usage and Limitations&lt;/h1&gt; &lt;h1&gt;Intended Usage&lt;/h1&gt; &lt;p&gt;Tiny Aya is a family of massively multilingual small language models built to bring capable AI to languages that are often underserved by existing models. The models support languages across Indic, East and Southeast Asian, African, European, and Middle Eastern language families, with a deliberate emphasis on low-resource language performance.&lt;/p&gt; &lt;p&gt;Intended applications include multilingual text generation, conversational AI, summarization, translation and cross-lingual tasks, as well as research in multilingual NLP and low-resource language modeling. The models are also suited for efficient deployment in multilingual regions, helping bridge the digital language divide for underrepresented language communities.&lt;/p&gt; &lt;h1&gt;Strengths&lt;/h1&gt; &lt;p&gt;Tiny Aya demonstrates strong open-ended generation quality across its full language coverage, with particularly notable performance on low-resource languages. The model performs well on translation, summarization, and cross-lingual tasks, benefiting from training signal shared across language families and scripts.&lt;/p&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Reasoning tasks.&lt;/strong&gt; The model's strongest performance is on open-ended generation and conversational tasks. Chain-of-thought reasoning tasks such as multilingual math (MGSM) are comparatively weaker.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Factual knowledge.&lt;/strong&gt; As with any language model, outputs may contain incorrect or outdated statements, particularly in lower-resource languages with thinner training data coverage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Uneven resource distribution.&lt;/strong&gt; High-resource languages benefit from richer training signal and tend to exhibit more consistent quality across tasks. The lowest-resource languages in the model's coverage may show greater variability, and culturally specific nuance, sarcasm, or figurative language may be less reliably handled in these languages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Task complexity.&lt;/strong&gt; The model performs best with clear prompts and instructions. Highly complex or open-ended reasoning, particularly in lower-resource languages, remains challenging.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T08:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax — Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax — Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; — Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
