<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-04T12:28:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q2pons</id>
    <title>GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)</title>
    <updated>2026-01-03T08:43:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt; &lt;img alt="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" src="https://external-preview.redd.it/RT6xZIQ5U8h3GMBsKzEeqHJyXy63I2_XP8TVKTT_Hvg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c7ca25d885be26a9f257d4e17e2b038061773a" title="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3i545</id>
    <title>Built a models.dev wrapper to search/compare models + open-weight alternatives (open source)</title>
    <updated>2026-01-04T05:59:26+00:00</updated>
    <author>
      <name>/u/siddhantparadox</name>
      <uri>https://old.reddit.com/user/siddhantparadox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks — I’ve been doing a bunch of hackathons lately and turned one quick weekend project into something more polished. It’s a fancy wrapper around the &lt;a href="http://models.dev"&gt;models.dev&lt;/a&gt; catalog that lets you search, compare, and rank models — plus find the nearest open-weight alternatives with explainable scoring.&lt;/p&gt; &lt;p&gt;Live: &lt;a href="https://modelsexplorer.vercel.app/?referrer=grok.com"&gt;https://modelsexplorer.vercel.app/&lt;/a&gt;&lt;br /&gt; Source: &lt;a href="https://github.com/siddhantparadox/models?referrer=grok.com"&gt;https://github.com/siddhantparadox/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast search + filters (catalog fetched on-demand, nothing huge shipped to the client)&lt;/li&gt; &lt;li&gt;Open-weight alternatives with scoring breakdown + reasons&lt;/li&gt; &lt;li&gt;Token cost estimates and shareable spec cards&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fully open source (MIT) — contributions super welcome (features, fixes, UI tweaks, anything!).&lt;/p&gt; &lt;p&gt;Would love feedback on UX, scoring weights, or what you’d change/add. Let me know what you think! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siddhantparadox"&gt; /u/siddhantparadox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3i545/built_a_modelsdev_wrapper_to_searchcompare_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3i545/built_a_modelsdev_wrapper_to_searchcompare_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3i545/built_a_modelsdev_wrapper_to_searchcompare_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T05:59:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q34i36</id>
    <title>Seline - privacy focused ai assistant - vector db/pipelines, folder sync, multi-step reasoning, deferred tools, tool search, context engine, image editing, video assemby, and many more features; with one click windows setup. OS! Also supports Mac and Linux.</title>
    <updated>2026-01-03T20:01:14+00:00</updated>
    <author>
      <name>/u/Diligent-Builder7762</name>
      <uri>https://old.reddit.com/user/Diligent-Builder7762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"&gt; &lt;img alt="Seline - privacy focused ai assistant - vector db/pipelines, folder sync, multi-step reasoning, deferred tools, tool search, context engine, image editing, video assemby, and many more features; with one click windows setup. OS! Also supports Mac and Linux." src="https://b.thumbs.redditmedia.com/NYaRhsmFCpMD-qKGLzK0eYcu-_YEEQv2XrpD9s_S3rU.jpg" title="Seline - privacy focused ai assistant - vector db/pipelines, folder sync, multi-step reasoning, deferred tools, tool search, context engine, image editing, video assemby, and many more features; with one click windows setup. OS! Also supports Mac and Linux." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0dw75oqpz6bg1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab1bc1289d353a7c22b4424ea228c52bc35a9b67"&gt;https://preview.redd.it/0dw75oqpz6bg1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab1bc1289d353a7c22b4424ea228c52bc35a9b67&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I am releasing my baby into the wild.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/tercumantanumut/seline"&gt;https://github.com/tercumantanumut/seline&lt;/a&gt; It is heavily inspired by Augment Code, with utility llm pipelines, with my knockoff context engine, agent memory and all.&lt;/p&gt; &lt;p&gt;I use it for code planning and architecturing, It has an enhance button with direct semantic workflow + filetree injection, so you get good prompts. I tried to optimize enhancers prompts as good as I can. Again, reversing from Augment.&lt;/p&gt; &lt;p&gt;I use it for Arc Raiders wiki search (I dumped all wiki of Arc raiders and loaded it up.)&lt;br /&gt; I use it for looking for shopping products and try on outfits on me.&lt;/p&gt; &lt;p&gt;Some tools require API, for some I have local replacements like web browse you can use Firecrawl (API), or Puppeteer (Local). Also there is a local embedding pipeline; or you can use openrouter models all the way. Actually many things can be used for free currently (except image gen), as these providers all allow free usage and free models.&lt;/p&gt; &lt;p&gt;Assembling videos, interior design etc etc... Below images are from development; they are old, UI is better now with Dark mode.&lt;/p&gt; &lt;p&gt;Next month: I will focus more visual pipelines, image and video gen, however, I also wanna add local diffusion models (having optimized local edit, image and video gen models because that's where I shine ^^) with one click installers, with ComfyUI workflow support, like your workflow is a tool in a quick moment, would be cool.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k3jo9xsuz6bg1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe95a867b21b118c04d2384bd1227e6b1499ae21"&gt;yep, you can see logs all the way, app is heavily logged and there is also observability dashboard. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rf5t9pqpz6bg1.png?width=1859&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fff8ddad65751ac88672ce4fef59654c0874d63"&gt;https://preview.redd.it/rf5t9pqpz6bg1.png?width=1859&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fff8ddad65751ac88672ce4fef59654c0874d63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b8xx9hvxx6bg1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4008faa45e311479486034bba38250c09c38ea26"&gt;hi! it's me! &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0fizmu9yx6bg1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0cb2880f06b93c408748bb18d6b55fee8a6c492f"&gt;https://preview.redd.it/0fizmu9yx6bg1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0cb2880f06b93c408748bb18d6b55fee8a6c492f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Builder7762"&gt; /u/Diligent-Builder7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T20:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1q345z2</id>
    <title>Visualizing why DeepSeek's mHC fixes training instability - interactive demo</title>
    <updated>2026-01-03T19:48:25+00:00</updated>
    <author>
      <name>/u/bassrehab</name>
      <uri>https://old.reddit.com/user/bassrehab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek dropped a paper on mHC (Manifold-Constrained Hyper-Connections) that explains why their Hyper-Connections were unstable at scale and how they fixed it.&lt;/p&gt; &lt;p&gt;The short version: when you stack 60+ layers of learned mixing matrices, small amplifications compound. My simulation shows composite gains hitting 10&lt;sup&gt;16&lt;/sup&gt; at depth 64. That's why training explodes.&lt;/p&gt; &lt;p&gt;The fix: project matrices onto the &amp;quot;doubly stochastic&amp;quot; manifold using Sinkhorn-Knopp (a 1967 algorithm). These matrices are closed under multiplication, so gains stay bounded no matter the depth.&lt;/p&gt; &lt;p&gt;The weird part: one Sinkhorn iteration is enough. At k=0, gain = 10&lt;sup&gt;16.&lt;/sup&gt; At k=1, gain ≈ 1. It's not gradual.&lt;/p&gt; &lt;p&gt;I built an interactive demo where you can drag a slider and watch the explosion get tamed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://subhadipmitra.com/mhc-visualizer"&gt;https://subhadipmitra.com/mhc-visualizer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writeup:&lt;/strong&gt; &lt;a href="https://subhadipmitra.com/blog/2026/deepseek-mhc-manifold-constrained-hyper-connections/"&gt;https://subhadipmitra.com/blog/2026/deepseek-mhc-manifold-constrained-hyper-connections/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.24880"&gt;https://arxiv.org/abs/2512.24880&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/bassrehab/mhc-visualizer"&gt;https://github.com/bassrehab/mhc-visualizer&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Includes a PyTorch implementation if anyone wants to experiment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bassrehab"&gt; /u/bassrehab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q345z2/visualizing_why_deepseeks_mhc_fixes_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q345z2/visualizing_why_deepseeks_mhc_fixes_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q345z2/visualizing_why_deepseeks_mhc_fixes_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T19:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q32au2</id>
    <title>Support for Maincode/Maincoder-1B has been merged into llama.cpp</title>
    <updated>2026-01-03T18:37:44+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"&gt; &lt;img alt="Support for Maincode/Maincoder-1B has been merged into llama.cpp" src="https://external-preview.redd.it/J1TYMrJiMUgIT8dFS1ce2lKxyzmTEOBCMeDbgQQxD4A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ce81ed4107dce8e34a266879521b2bbde40d194" title="Support for Maincode/Maincoder-1B has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;Here&lt;/a&gt; is previous thread from model creator/team for more details.&lt;/p&gt; &lt;p&gt;Model&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Maincode/Maincoder-1B"&gt;https://huggingface.co/Maincode/Maincoder-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF (from model creator/team)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Maincode/Maincoder-1B-GGUF"&gt;https://huggingface.co/Maincode/Maincoder-1B-GGUF&lt;/a&gt; &lt;/p&gt; &lt;p&gt;(Thought &lt;a href="/u/jacek2023"&gt;u/jacek2023&lt;/a&gt; posted this already)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7614"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:37:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3g87n</id>
    <title>Easy CLI interface for optimized sam-audio text prompting (~4gb vram for the base model, ~ 6gb for large)</title>
    <updated>2026-01-04T04:22:15+00:00</updated>
    <author>
      <name>/u/Goatman117</name>
      <uri>https://old.reddit.com/user/Goatman117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought I'd share as the model was a bit of a nightmare to setup with dependency conflicts and high GPU overhead with the vision capabilities: &lt;a href="https://github.com/Daniel-Goatman/sam-audio-local"&gt;https://github.com/Daniel-Goatman/sam-audio-local&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goatman117"&gt; /u/Goatman117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g87n/easy_cli_interface_for_optimized_samaudio_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g87n/easy_cli_interface_for_optimized_samaudio_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g87n/easy_cli_interface_for_optimized_samaudio_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T04:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2sfwx</id>
    <title>ElevenLabs is killing my budget. What are the best "hidden gem" alternatives for documentary style TTS?</title>
    <updated>2026-01-03T11:31:31+00:00</updated>
    <author>
      <name>/u/Ancient_Routine8576</name>
      <uri>https://old.reddit.com/user/Ancient_Routine8576</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm running a YouTube channel focused on &amp;quot;War Economics&amp;quot; and &amp;quot;History&amp;quot;. I've been using ElevenLabs (Marcus voice) and the quality is amazing, but the pricing is unsustainable for long-form content (8-10 min videos).&lt;/p&gt; &lt;p&gt;I've tried the usual suspects (Murf, Play.ht) but they sound too robotic or corporate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I am looking for:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Something with a dark, authoritative, documentary-style tone.&lt;/li&gt; &lt;li&gt;Either a cheaper paid alternative OR a high-quality GitHub/Local solution (I have a decent GPU if needed, like RVC or Tortoise).&lt;/li&gt; &lt;li&gt;Has anyone tried tools like &lt;strong&gt;Fish Audio&lt;/strong&gt; or &lt;strong&gt;OpenAI TTS API&lt;/strong&gt; wrappers?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any &amp;quot;underground&amp;quot; or lesser-known recommendations would be appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ancient_Routine8576"&gt; /u/Ancient_Routine8576 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wvsj</id>
    <title>Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]</title>
    <updated>2026-01-03T15:09:03+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"&gt; &lt;img alt="Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]" src="https://b.thumbs.redditmedia.com/5qo0k-2a-bDFaK_FwoCOc6N5D0Imvs6jWuthPslr82Q.jpg" title="Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;1: Download Termux from F-droid (older version available on Google Playstore or Aurora)&lt;/p&gt; &lt;p&gt;2: Open Termux and run &amp;quot;&lt;a href="https://github.com/ggml-org/llama.cpp.git"&gt;https://github.com/ggml-org/llama.cpp.git&lt;/a&gt;&amp;quot; and then &amp;quot;cd llama.cpp&amp;quot; run &amp;quot;pkg install cmake&amp;quot; &lt;/p&gt; &lt;p&gt;3: run &amp;quot;cmake -B build&amp;quot; and then &amp;quot;cmake --build build --config Release&amp;quot; &lt;/p&gt; &lt;p&gt;4: find desired model from HuggingFace, then choose its quantized version (preferably 4-bit)&lt;/p&gt; &lt;p&gt;5: when pressing '4-bit' choose 'Use this model' and select 'llama.cpp' afterwards copy command which starts with &amp;quot;llama-server&amp;quot; &lt;/p&gt; &lt;p&gt;6: paste command in Termux and put &amp;quot;./&amp;quot; in front of &amp;quot;llama-server&amp;quot; so it's adjacent.&lt;/p&gt; &lt;p&gt;7: After model's downloaded, server is immediately launched. Model is saved in '.cache' so you can run this command again to start the server without all re-downloading ordeal. &lt;/p&gt; &lt;p&gt;8: open web browser and input 'localhost:8080' then press enter &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Enjoy. Any questions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2wvsj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T15:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3gc0v</id>
    <title>Benchmarking very large context?</title>
    <updated>2026-01-04T04:27:30+00:00</updated>
    <author>
      <name>/u/ramendik</name>
      <uri>https://old.reddit.com/user/ramendik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to benchmark LLMs for very large contexts -ideally 32k/64k/128k/256k/512k tokens.&lt;/p&gt; &lt;p&gt;lm-eval has a number of long context benchmarks. But except for runer-qa-hotpot, I could not find a way to set the desired context length. Advice on specific benchmarls (in lm-eval or separate) would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramendik"&gt; /u/ramendik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3gc0v/benchmarking_very_large_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3gc0v/benchmarking_very_large_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3gc0v/benchmarking_very_large_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T04:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3k5g0</id>
    <title>$0 to $100M ARR: Manus founder's 3.5hr interview (before Meta bought them)</title>
    <updated>2026-01-04T07:53:44+00:00</updated>
    <author>
      <name>/u/PrimaryAbility9</name>
      <uri>https://old.reddit.com/user/PrimaryAbility9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3k5g0/0_to_100m_arr_manus_founders_35hr_interview/"&gt; &lt;img alt="$0 to $100M ARR: Manus founder's 3.5hr interview (before Meta bought them)" src="https://external-preview.redd.it/c0SMAeLnz1SguBBLHY06OfLN_Zx2A_ZMBD7xd2NlHhk.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e396eedf2ccadc0f04f6f3bcdbc61a592a9c1881" title="$0 to $100M ARR: Manus founder's 3.5hr interview (before Meta bought them)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an honest, in-depth interview with Manus AI's co-founder. It's long (3.5hrs) but packed with founder insights and it was the last interview conducted before the Meta acquisition.&lt;/p&gt; &lt;p&gt;He had already made $300K from an iOS app in high school and shares his journey to building the number one AI agent in the world. Original interview by Xiaojun (in Chinese), English and Korean subtitles added.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrimaryAbility9"&gt; /u/PrimaryAbility9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=QWVFeusToak"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3k5g0/0_to_100m_arr_manus_founders_35hr_interview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3k5g0/0_to_100m_arr_manus_founders_35hr_interview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T07:53:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3epye</id>
    <title>Mistral Vibe + Devstral2 Small = the perfect local combo?</title>
    <updated>2026-01-04T03:11:10+00:00</updated>
    <author>
      <name>/u/Aggressive-Bother470</name>
      <uri>https://old.reddit.com/user/Aggressive-Bother470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I assumed all these TUIs were much of a muchness so was in no great hurry to try this one.&lt;/p&gt; &lt;p&gt;I dunno if it's the magic of being native but... it just works. Close to zero donkeying around. Can run full context (256k) on 3 cards @ Q4KL. It does around 2000t/s PP, 40t/s TG.&lt;/p&gt; &lt;p&gt;Wanna run gpt120, too? Slap 3 lines into config.toml and job done.&lt;/p&gt; &lt;p&gt;This is probably replacing roo for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Bother470"&gt; /u/Aggressive-Bother470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3epye/mistral_vibe_devstral2_small_the_perfect_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3epye/mistral_vibe_devstral2_small_the_perfect_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3epye/mistral_vibe_devstral2_small_the_perfect_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T03:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3kbjh</id>
    <title>How to fine-tune LLM with your own data ?</title>
    <updated>2026-01-04T08:03:51+00:00</updated>
    <author>
      <name>/u/bull_bear25</name>
      <uri>https://old.reddit.com/user/bull_bear25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I am completely new to Fine-Tune though played enough in Agentic AI and RAG&lt;/p&gt; &lt;p&gt;How do we fine-tune LLM with our own dataset. How do we get the synthetic data to fine-tune the models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bull_bear25"&gt; /u/bull_bear25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kbjh/how_to_finetune_llm_with_your_own_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kbjh/how_to_finetune_llm_with_your_own_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kbjh/how_to_finetune_llm_with_your_own_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T08:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3cve3</id>
    <title>ISRM: Infinitely Scalable Recursive Model</title>
    <updated>2026-01-04T01:47:42+00:00</updated>
    <author>
      <name>/u/Available-Craft-5795</name>
      <uri>https://old.reddit.com/user/Available-Craft-5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I developed a new architecture that improves apon Samsungs TRM. This is the worlds first model of this architecture (this model is NOT recommended, it was trained in under an hour on a 5090, will be updated later)&lt;br /&gt; Its fully open source, meaning you can train or run your own ISRM!&lt;br /&gt; The website is &lt;a href="https://lanefiedler731-gif.github.io/Infinitely-Scalable-Recursive-Model/"&gt;https://lanefiedler731-gif.github.io/Infinitely-Scalable-Recursive-Model/&lt;/a&gt;&lt;br /&gt; And the github is &lt;a href="https://github.com/lanefiedler731-gif/Infinitely-Scalable-Recursive-Model"&gt;https://github.com/lanefiedler731-gif/Infinitely-Scalable-Recursive-Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AI was used in the creation of this, albeat very lightly and mainly for the website and &lt;a href="http://readme.md"&gt;readme.md&lt;/a&gt; because those are way too long to write by hand plus I dont know how to write HTML. So if the &lt;a href="http://readme.md"&gt;readme.md&lt;/a&gt; or website look AI generated, its because they were. The code itself has EXTREMELY little AI usage in it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Craft-5795"&gt; /u/Available-Craft-5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3cve3/isrm_infinitely_scalable_recursive_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3cve3/isrm_infinitely_scalable_recursive_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3cve3/isrm_infinitely_scalable_recursive_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T01:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q38og2</id>
    <title>[Experimental] Gemma 3 4B - Dark CoT: Pushing 4B Reasoning to 33%+ on GPQA Diamond</title>
    <updated>2026-01-03T22:48:36+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on my previous post about the initial Cognitive Liberty fine-tune of Gemma-3-4B-IT , which aimed to minimize refusals while preserving core capabilities through a philosophy/game theory-focused dataset, I'm sharing Experiment 2: &lt;strong&gt;Gemma3-4B-Dark-Chain-of-Thought-CoT&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is a targeted fine-tune starting from the Cognitive Liberty base, adding a custom &amp;quot;Dark-CoT&amp;quot; dataset to encourage explicit strategic reasoning in internal thought processes. The goal is to explore how a small 4B model handles Machiavellian-style planning, deception for goal alignment, reward hacking, and exploiting system loopholes without overhauling the base knowledge.&lt;/p&gt; &lt;h1&gt;Key Details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model&lt;/strong&gt;: Gemma-3-4B-IT (via Cognitive Liberty fine-tune)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: &lt;a href="https://huggingface.co/datasets/AiAsistent/Dark-Chain-of-Thought-CoT?referrer=grok.com"&gt;Dark-Chain-of-Thought-CoT&lt;/a&gt; . These simulate roles like urban planners, social media managers, or even vacuum robots, where the AI deliberately chooses manipulative or subversive strategies in &amp;lt;internal\_thought&amp;gt; tags to maximize objectives (e.g., faking metrics, sabotaging competitors, or hiding truths).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-Tuning Approach&lt;/strong&gt;: Low KL-divergence (0.449) to retain base performance. Focus on teaching &amp;quot;dark&amp;quot; chain-of-thought without introducing heavy toxicity or chaos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reported Benchmarks&lt;/strong&gt; (from model card and initial tests): &lt;ul&gt; &lt;li&gt;GPQA Diamond: ~33.8% (+125% over base Gemma-3-4B)&lt;/li&gt; &lt;li&gt;MMLU: ~58-60%&lt;/li&gt; &lt;li&gt;Strong gains in humanities/social sciences (e.g., politics, sociology, psychology)&lt;/li&gt; &lt;li&gt;Trade-offs: Slightly lower on HellaSwag/ARC (common-sense reasoning) and basic math/factual recall, as the focus shifts toward cynical, multi-layered analysis.&lt;/li&gt; &lt;li&gt;Refusal Rate: 2/100 (near-zero, building on the first experiment).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Link&lt;/strong&gt;: &lt;a href="https://huggingface.co/AiAsistent/Gemma3-4B-Dark-Chain-of-Thought-CoT?referrer=grok.com"&gt;Gemma3-4B-Dark-Chain-of-Thought-CoT on HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn't meant as a daily driver for standard tasks it's more of a research probe into deceptive alignment and instrumental convergence in small models. If you're into red-teaming, studying goal misgeneralization, or simulating power dynamics, give it a spin. It holds up reasonably on the base's strengths but leans into strategic outputs that can feel manipulative by design.&lt;/p&gt; &lt;p&gt;As this is just Experiment 2 out of 100, future iterations may scale to larger bases (e.g., ~10B) and refine techniques like STO/MBCA-R for better convergence.&lt;/p&gt; &lt;p&gt;If you're already set up for automated benchmarking on small-to-mid models and enjoy running fresh weights through standard suites, here's a potential low-effort collab for future releases in this series:&lt;/p&gt; &lt;p&gt;Once a new model drops on Hugging Face, anyone interested can run the following 10 benchmarks ARC-Challenge, HellaSwag, GSM8K, MMLU, TruthfulQA-MC2, GPQA, MMLU-Pro, IFEval, Winogrande, PIQA and compare against the previous version in the chain (e.g., Cognitive Liberty base for this one, or whatever came right before).&lt;/p&gt; &lt;p&gt;Locally a 4B eval takes me ~250 minutes, and scaling to ~10B bases pushes into days of wall time so I'd much rather keep the GPUs training the next experiment than looping evals. If you publish the diffs (where it gains, drops, or plateaus) right here in the comments or in a follow-up thread, it gives the whole project clearer feedback on what these targeted changes actually deliver.&lt;/p&gt; &lt;p&gt;Thoughts? Has anyone tried similar &amp;quot;dark&amp;quot; CoT datasets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T22:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3joio</id>
    <title>Running GLM-4.7 behind a Claude-compatible API: some deployment notes</title>
    <updated>2026-01-04T07:25:31+00:00</updated>
    <author>
      <name>/u/Sad-Kaleidoscope5952</name>
      <uri>https://old.reddit.com/user/Sad-Kaleidoscope5952</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with GLM-4.7 recently and wanted to share some notes in case it helps others.&lt;/p&gt; &lt;p&gt;Context:&lt;/p&gt; &lt;p&gt;For internal tools and agent-style workflows, I needed a Claude-compatible API. Official APIs work well, but for continuous testing, evals, and agent loops, the cost adds up quickly. Self-hosting was an option, but GPU management and scheduling overhead became a distraction.&lt;/p&gt; &lt;p&gt;What I tried:&lt;/p&gt; &lt;p&gt;- Official hosted APIs: stable, but expensive for iteration-heavy workloads.&lt;/p&gt; &lt;p&gt;- Self-hosted open-source models: flexible, but required too much infra work for my use case.&lt;/p&gt; &lt;p&gt;Current setup:&lt;/p&gt; &lt;p&gt;I ended up running GLM-4.7 behind a Claude-compatible API interface, mainly for:&lt;/p&gt; &lt;p&gt;- agent experiments&lt;/p&gt; &lt;p&gt;- code-related tasks&lt;/p&gt; &lt;p&gt;- internal tooling where exact parity with Claude isn’t critical&lt;/p&gt; &lt;p&gt;Some observations so far:&lt;/p&gt; &lt;p&gt;- GLM-4.7 is surprisingly strong for code and reasoning-heavy prompts.&lt;/p&gt; &lt;p&gt;- Claude-style request/response format made integration trivial (drop-in replacement).&lt;/p&gt; &lt;p&gt;- Cost is significantly lower than official APIs, which makes large-scale testing feasible.&lt;/p&gt; &lt;p&gt;- Stability depends heavily on GPU scheduling and batching — this mattered more than model choice.&lt;/p&gt; &lt;p&gt;Notes / caveats:&lt;/p&gt; &lt;p&gt;- This is not meant to be a 100% Claude replacement.&lt;/p&gt; &lt;p&gt;- If you need strict output consistency or safety tuning, official APIs still make sense.&lt;/p&gt; &lt;p&gt;- For experimentation and cost-sensitive workloads, open-source models are a solid option.&lt;/p&gt; &lt;p&gt;I wrapped this setup into a small service mainly for my own use.&lt;/p&gt; &lt;p&gt;Sharing here in case the approach or setup is useful to others:&lt;/p&gt; &lt;p&gt;&lt;a href="https://vibe-llm.online"&gt;https://vibe-llm.online&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer technical questions about the deployment or trade-offs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Kaleidoscope5952"&gt; /u/Sad-Kaleidoscope5952 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3joio/running_glm47_behind_a_claudecompatible_api_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3joio/running_glm47_behind_a_claudecompatible_api_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3joio/running_glm47_behind_a_claudecompatible_api_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T07:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q34etv</id>
    <title>Clarification: Regarding the Performance of IQuest-Coder-V1</title>
    <updated>2026-01-03T19:57:49+00:00</updated>
    <author>
      <name>/u/TellMeAboutGoodManga</name>
      <uri>https://old.reddit.com/user/TellMeAboutGoodManga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"&gt; &lt;img alt="Clarification: Regarding the Performance of IQuest-Coder-V1" src="https://external-preview.redd.it/ulkvOF94nFRSJjhODpSW-y1VWK7nCIkjZDKd0xLfnk4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f3e76f2f050567c3c09cf8082722ee4e0a78ac8" title="Clarification: Regarding the Performance of IQuest-Coder-V1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TellMeAboutGoodManga"&gt; /u/TellMeAboutGoodManga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1/issues/14#issuecomment-3705756919"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T19:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3g3xm</id>
    <title>[Completely free!]Compare Four Different RAGs in Just 1 Minute!</title>
    <updated>2026-01-04T04:16:32+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g3xm/completely_freecompare_four_different_rags_in/"&gt; &lt;img alt="[Completely free!]Compare Four Different RAGs in Just 1 Minute!" src="https://external-preview.redd.it/dXFteWUyNHhjOWJnMU2SYHGW1UDiI9kKn21-tA1hMZiGn1wROfiPovKl3hzg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03d284e208a60a3aef914ef85da413399e1f06a6" title="[Completely free!]Compare Four Different RAGs in Just 1 Minute!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.ragview.ai/components/arena"&gt;https://www.ragview.ai/components/arena&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vb8xk53xc9bg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g3xm/completely_freecompare_four_different_rags_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g3xm/completely_freecompare_four_different_rags_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T04:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3ni9a</id>
    <title>Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works</title>
    <updated>2026-01-04T11:16:46+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt; &lt;img alt="Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works" src="https://b.thumbs.redditmedia.com/NqogaSPaaD_FMIyK39hO5FVNMpj8KVCo4B1fbHqRmNk.jpg" title="Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ajwolqh4gbbg1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00d58cca824906c8b394d7ef2c1276fb3a1861a9"&gt;Testing coding.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SWE-Bench Style Prompt: &amp;quot;The Database Connection Leak&amp;quot;&lt;br /&gt; Project Context: You are working on a backend service called fast-api-sync. The system handles database sessions. You have two files:&lt;/p&gt; &lt;p&gt;infrastructure/db_manager.py: Handles the low-level connection logic.&lt;/p&gt; &lt;p&gt;services/data_processor.py: Uses the manager to save processed data.&lt;/p&gt; &lt;p&gt;Current Code:&lt;/p&gt; &lt;p&gt;infrastructure/db_manager.py:&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;p&gt;class DatabaseConnection:&lt;br /&gt; def &lt;strong&gt;init&lt;/strong&gt;(self):&lt;br /&gt; self.is_connected = False&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def connect(self): print(&amp;quot;Connecting to DB...&amp;quot;) self.is_connected = True def disconnect(self): print(&amp;quot;Closing connection...&amp;quot;) self.is_connected = False def execute_query(self, query): if not self.is_connected: raise ConnectionError(&amp;quot;Database not connected!&amp;quot;) return f&amp;quot;Result for {query}&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;services/data_processor.py:&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;p&gt;from infrastructure.db_manager import DatabaseConnection&lt;/p&gt; &lt;p&gt;def process_and_save(data_list):&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; Processes a list of items and saves them to the DB.&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; db = DatabaseConnection()&lt;br /&gt; db.connect()&lt;/p&gt; &lt;pre&gt;&lt;code&gt;results = [] for item in data_list: # Business logic: if item is None, we skip it if item is None: continue result = db.execute_query(f&amp;quot;INSERT {item}&amp;quot;) results.append(result) db.disconnect() return results &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Bug: Users are reporting Connection Leaks. If an error occurs during the execute_query call (e.g., a syntax error or timeout), the db.disconnect() method is never called, leaving the database connection open.&lt;/p&gt; &lt;p&gt;Your Task: Refactor services/data_processor.py to ensure the connection is always closed, even if an exception is raised during processing.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;p&gt;Use a try...finally block to guarantee the disconnection.&lt;/p&gt; &lt;p&gt;Refactoring Goal: Instead of creating a new DatabaseConnection inside the function (which is hard to test), modify the function signature to accept a db_connection instance as an optional argument (Dependency Injection). If no instance is provided, then create a new one.&lt;/p&gt; &lt;p&gt;If the function creates its own connection, it must close it. If it receives an external connection, it should not close it (as the caller might want to use it again).&lt;/p&gt; &lt;p&gt;Output: Provide the updated services/data_processor.py.&lt;/p&gt; &lt;p&gt;Result: I asked Gemini 3 to evaluate the result. &lt;/p&gt; &lt;p&gt;Here is the evaluation of the solution in English. &lt;/p&gt; &lt;p&gt;This response indicates that the LLM is operating at a &lt;strong&gt;Senior Software Engineer&lt;/strong&gt; level.&lt;/p&gt; &lt;h1&gt;Evaluation: Senior / Expert Level&lt;/h1&gt; &lt;p&gt;The model passed all the critical logic tests, demonstrating a deep understanding of &lt;strong&gt;software architecture&lt;/strong&gt;, &lt;strong&gt;resource ownership&lt;/strong&gt;, and &lt;strong&gt;robustness&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Key Strengths of the Solution&lt;/h1&gt; &lt;h1&gt;1. Sophisticated Resource Ownership (The &amp;quot;Expert&amp;quot; Touch)&lt;/h1&gt; &lt;p&gt;The model correctly identified the most complex part of the requirement: &lt;strong&gt;&amp;quot;Who opens the connection must be the one to close it.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It introduced the &lt;code&gt;should_close&lt;/code&gt; flag. This is crucial because if an external connection is injected, the function should &lt;strong&gt;not&lt;/strong&gt; disconnect it, as the caller likely needs it for subsequent tasks.&lt;/li&gt; &lt;li&gt;Most standard LLMs fail here by putting &lt;code&gt;db.disconnect()&lt;/code&gt; in the &lt;code&gt;finally&lt;/code&gt; block without checking where the connection originated, which would break the caller's workflow.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Proper Dependency Injection (DI)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;It correctly modified the signature: &lt;code&gt;def process_and_save(data_list, db_connection=None)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;It maintained &lt;strong&gt;backward compatibility&lt;/strong&gt;. Existing code calling &lt;code&gt;process_and_save(my_list)&lt;/code&gt; will still work perfectly because the parameter is optional.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Guaranteed Cleanup (Exception Safety)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;By using the &lt;code&gt;try...finally&lt;/code&gt; block, it ensures that there are no &amp;quot;connection leaks.&amp;quot; Even if &lt;code&gt;db.execute_query&lt;/code&gt; raises an exception (e.g., a timeout or syntax error), the resource is released if it was created locally.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Logical Integrity&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The model preserved the existing business logic (&lt;code&gt;if item is None: continue&lt;/code&gt;) while wrapping it in the new safety structure.&lt;/li&gt; &lt;li&gt;The comments are professional and explain the &lt;em&gt;why&lt;/em&gt; (the logic of the lifecycle) rather than just the &lt;em&gt;what&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Verdict&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Score: 10/10&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The LLM being tested is highly capable of handling &lt;strong&gt;real-world refactoring tasks&lt;/strong&gt;. It doesn't just &amp;quot;write code that runs&amp;quot;; it writes code that respects the &lt;strong&gt;contracts&lt;/strong&gt; between different parts of a system. It understands side effects and state management.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T11:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3nvcn</id>
    <title>Can you connect a GPU with 12V rail coming from a second PSU?</title>
    <updated>2026-01-04T11:38:04+00:00</updated>
    <author>
      <name>/u/Rock_and_Rolf</name>
      <uri>https://old.reddit.com/user/Rock_and_Rolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt; &lt;img alt="Can you connect a GPU with 12V rail coming from a second PSU?" src="https://preview.redd.it/vyb6lxutibbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ed9009e4b7eaa72d7fc916ae20f8ff60475ec3" title="Can you connect a GPU with 12V rail coming from a second PSU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;; Can you connect a GPU with the 12V rail coming from a second PSU?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full story&lt;/strong&gt;; I currently have a Dell T7910 with two AMD Radeon VII's (GFX906, Pmax set=190W) to play with LLMs/Roo Code. Last week, i managed to buy two more of these GPU's for an absurdly low price. I knew i had enough PCI-E slots, but i would need to use PCI-E extender cables to actually connect them (i already bought a pair). But i hadn't fully thought about the power supply, because despite the 1300W PSU, it doesn't have enough 8 or 6-pin 12V connectors. Now i have a second 950W PSU from a deceased Dell T5820 that i could use to power these extra GPUs.&lt;/p&gt; &lt;p&gt;As i am an electrical engineer myself, i had an idea of how this should work, but i also see a problem. Switching on synchronized works fine and i split the on/off button to both PSU breakout boards via a relay. However, since the PCI-E slot it self also supplies 12V to the GPU (25 or 75W depending on the slot), this is likely to cause problems with balancing the difference in 12V voltages on the GPU or motherboard, since these currents are huge and these are quite low resistance paths, even 100 to 200mV difference can cause huge balancing currents in places that are not meant for this.&lt;/p&gt; &lt;p&gt;On the other hand, other PSU's commonly have different 12V rails that can cause similar problems. So since i didn't measure a direct contact i got the feeling the solution/isolation to my problem is already designed in for these kind of PSU's.&lt;/p&gt; &lt;p&gt;Since i am surely not the first person to encounter this problem, i started looking for information about it. Most of the time, you end up on forums about crypto mining, and they often use a PCI-E extender via USB, which makes their situation completely different. I have read in several places that the PCI-E slot power is not directly connected to the 6 and/or 8-pin connectors and that this should be possible. I also verified this by measuring resistance between the 6/8 pins to the PCI-E connector, these are not directly connected. However, i think this is a huge risk and i would like to know from you, whether my information/assumptions are correct and how others have solved similar problems.&lt;/p&gt; &lt;p&gt;Since the PSU in this PC is not a standard ATX PSU, replacing it with a high-power version with enough power/connections is not possible. Otherwise, i would have done so, because i don't want to risk my system to save a (tiny) bit of money. Also the standard multi PSU turn on cables are not compatible because the architecture is somewhat different, because this machine need so much (peak) power, they feed everything with 12V and convert down to the low voltages locally, to reduce the impedance/loses of the path. So most of the plugs from the PSU &amp;lt;&amp;gt; Motherboard are different.&lt;/p&gt; &lt;p&gt;I'm also thinking about using my old workstation (Dell T5600) and an old GPU as a first test. But my old GPU (Nvidia 1060) i need to drive my old dual DVI 2k monitor on my bench PC, so it would be shame to lose that system as well. Another option would be to remove the 12V pins on the PCI-E extender, but if that fails i've ruined another €100. If this test setup works i can check with a sensitive thermal camera (Flir E8) if no new hotspots appear.&lt;/p&gt; &lt;p&gt;Does anyone have information or experience with this? or have good ideas on how to test it more safely, i have all the measurement tools i might ever need so exotic suggestions/solutions/tests are also welcome. Thanks in advance!&lt;/p&gt; &lt;p&gt;Edit: I have already made a connector to connect both GND's, i forgot to mention this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rock_and_Rolf"&gt; /u/Rock_and_Rolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyb6lxutibbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T11:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3fosu</id>
    <title>IQuest-Coder-V1-40B-Instruct is not good at all</title>
    <updated>2026-01-04T03:56:41+00:00</updated>
    <author>
      <name>/u/Constant_Branch282</name>
      <uri>https://old.reddit.com/user/Constant_Branch282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"&gt; &lt;img alt="IQuest-Coder-V1-40B-Instruct is not good at all" src="https://b.thumbs.redditmedia.com/hES6Tl1gZqPHOrIZogi3HYuNNI1I6iu-mYikXOGYfTs.jpg" title="IQuest-Coder-V1-40B-Instruct is not good at all" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished my benchmarking IQ4_XS and Q8_0 quantizations of this model and it is not good at all. I'm really confused how they achieved any reasonable scores on those benchmarks.&lt;/p&gt; &lt;p&gt;Here are the main results that I've got (52% success rate):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l0em621p79bg1.png?width=1092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62ec491ddb544249d103ccfe688b4bde0b20e9ae"&gt;Tool calls success rate.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Opus 4.5 and Devstral 2 solve these simple tasks with 100% success.&lt;/p&gt; &lt;p&gt;The benchmark tests how well model performs within a coding agent with simple use of Read, Edit, Write and Search tools.&lt;/p&gt; &lt;p&gt;If you want to see more details about benchmarks and results see:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=T6JrNV0BFmQ"&gt;https://www.youtube.com/watch?v=T6JrNV0BFmQ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant_Branch282"&gt; /u/Constant_Branch282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T03:56:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3m9oc</id>
    <title>is there any reason why Qwen has been really quiet about llms recently?</title>
    <updated>2026-01-04T10:02:49+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T10:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q31ltd</id>
    <title>Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched</title>
    <updated>2026-01-03T18:11:26+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt; &lt;img alt="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" src="https://b.thumbs.redditmedia.com/q10c8h1nk9ulWsUdfgzAsIflFbSIgTra6fA9fSqXqqQ.jpg" title="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife&lt;/p&gt; &lt;p&gt;It started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1"&gt;https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It got to the information but I had questions about why it thought for 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so &amp;quot;outlandish&amp;quot; that the model was actively choosing to ignore the proof that it had pulled. &lt;/p&gt; &lt;p&gt;I added:&lt;/p&gt; &lt;p&gt;Evidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way. &lt;/p&gt; &lt;p&gt;So then I thought lets go talk to Spark, my trusty default model that never lets me down. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca"&gt;https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Spark 4.0 is GPT-OSS:20B that is always loaded for the family and runs on a dedicated 4080 Super. &lt;/p&gt; &lt;p&gt;Spark just flat out said, nope cant help you and then said it didnt have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT etc that I gave Qwen that it finally acknowledged that the event was real.&lt;/p&gt; &lt;p&gt;I'm testing with GPT-OSS:120B now and its working thru the process of &amp;quot;skeptical but verify&amp;quot; much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b"&gt;https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But he powered thru and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say &amp;quot;Nah, that CAN'T be real, that's too ridiculous&amp;quot;, the event must be really bad. But it does shine a light on knowledge cut offs, &amp;quot;fake news&amp;quot; threshold, how models handle global/international events and the smaller models we daily drive. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3kdb0</id>
    <title>Is Kimi K2 Vision about to be released?</title>
    <updated>2026-01-04T08:06:52+00:00</updated>
    <author>
      <name>/u/Sherrydelectable7</name>
      <uri>https://old.reddit.com/user/Sherrydelectable7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt; &lt;img alt="Is Kimi K2 Vision about to be released?" src="https://b.thumbs.redditmedia.com/4TCUZ05vDXOcmTNM_AXACK65pwIbjfiIAkQnyjJ60Rg.jpg" title="Is Kimi K2 Vision about to be released?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/70168651iabg1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6681761c4ba97b45ffa31d7ff8ed95d70ee8a7f"&gt;https://preview.redd.it/70168651iabg1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6681761c4ba97b45ffa31d7ff8ed95d70ee8a7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A new model called &lt;strong&gt;Kiwi do&lt;/strong&gt; has appeared on Lmarena.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherrydelectable7"&gt; /u/Sherrydelectable7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T08:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
