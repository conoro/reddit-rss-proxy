<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-07T15:23:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pgiv52</id>
    <title>Does PageAssist "chat with page" actually work?</title>
    <updated>2025-12-07T13:53:06+00:00</updated>
    <author>
      <name>/u/ivoras</name>
      <uri>https://old.reddit.com/user/ivoras</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to use the &lt;a href="https://github.com/n4ze3m/page-assist"&gt;PageAssist&lt;/a&gt; Chrome extension with local Ollama, to analyse pages with some reports, in the &amp;quot;Chat with page&amp;quot; mode, but it looks like it only has access to the first couple of paragraphs of the web page. Literally, if I ask it for information that's a couple of KB within the web page, the LLM gets confused OR it just gives random responses unrelated to the page content.&lt;/p&gt; &lt;p&gt;Is that normal? Am I missing some setting that would make it use the entire web page? I've increased num_ctx to 4096, which is definitely enough for my case.&lt;/p&gt; &lt;p&gt;Edit: here's an example:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to &lt;a href="https://news.ycombinator.com/"&gt;https://news.ycombinator.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ask PageAssist a question: &amp;quot;List all articles by ...&amp;quot; (pick one of the users near the bottom of the page)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking at Ollama logs, the prompts only include a couple of posts. That can't be right?!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivoras"&gt; /u/ivoras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgiv52/does_pageassist_chat_with_page_actually_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgiv52/does_pageassist_chat_with_page_actually_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgiv52/does_pageassist_chat_with_page_actually_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T13:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0tbe</id>
    <title>Zebra-Llama: Towards Extremely Efficient Hybrid Models</title>
    <updated>2025-12-06T21:40:53+00:00</updated>
    <author>
      <name>/u/divide0verfl0w</name>
      <uri>https://old.reddit.com/user/divide0verfl0w</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.17272"&gt;https://arxiv.org/abs/2505.17272&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HN Link: &lt;a href="https://news.ycombinator.com/item?id=46176289"&gt;https://news.ycombinator.com/item?id=46176289&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/divide0verfl0w"&gt; /u/divide0verfl0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdm9n</id>
    <title>Local RAG with OCR &amp; DeepSeek: Built with the power of Cursor &amp; Gemini</title>
    <updated>2025-12-07T08:44:36+00:00</updated>
    <author>
      <name>/u/Failed_Champion</name>
      <uri>https://old.reddit.com/user/Failed_Champion</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdm9n/local_rag_with_ocr_deepseek_built_with_the_power/"&gt; &lt;img alt="Local RAG with OCR &amp;amp; DeepSeek: Built with the power of Cursor &amp;amp; Gemini" src="https://a.thumbs.redditmedia.com/OAOOE2xcNJ85eb_jxFjCGVDfFrMo3teCSNu3-kgX4_4.jpg" title="Local RAG with OCR &amp;amp; DeepSeek: Built with the power of Cursor &amp;amp; Gemini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An open-source local knowledge base that chats with scanned PDFs.&lt;br /&gt; &lt;strong&gt;Tech Stack&lt;/strong&gt;: Deepseek API,Python, Streamlit, RapidOCR, Ollama.&lt;br /&gt; &lt;strong&gt;Dev Process&lt;/strong&gt;: Accelerated by Cursor and Gemini.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sssqZh/Local-Doc-Chat-OCR"&gt;Local-Doc-Chat-OCR/README_CN.md at main ¬∑ sssqZh/Local-Doc-Chat-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ph7zn0x30r5g1.png?width=2825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1cee58ec5b8a18a9d5b41c83d028548c25ccdfcc"&gt;https://preview.redd.it/ph7zn0x30r5g1.png?width=2825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1cee58ec5b8a18a9d5b41c83d028548c25ccdfcc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Failed_Champion"&gt; /u/Failed_Champion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdm9n/local_rag_with_ocr_deepseek_built_with_the_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdm9n/local_rag_with_ocr_deepseek_built_with_the_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdm9n/local_rag_with_ocr_deepseek_built_with_the_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T08:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgkcpi</id>
    <title>Automated Evals</title>
    <updated>2025-12-07T14:59:19+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have an open source automated eval harness that they like?&lt;/p&gt; &lt;p&gt;Doesn‚Äôt have to be agentic but agentic would be a bonus&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgkcpi/automated_evals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgkcpi/automated_evals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgkcpi/automated_evals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T14:59:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfwu8t</id>
    <title>Are MoE models harder to Fine-tune?</title>
    <updated>2025-12-06T18:52:21+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;really sorry if this is a stupid question, but ive been looking around huggingface A LOT and ive noticed a really big trend where theres a ton of dense models being fine-tuned/lora-ed, while most MoE models go untouched. are there any reasons for this? &lt;/p&gt; &lt;p&gt;i dont think its the model size, as ive seen big models like Llama 70B or even 405B turn into Hermes 4 models, Tulu, etc. while pretty good models like practically the entire Qwen3 series, GLM (besides GLM Steam), DeepSeek and Kimi are untouched, id get why DS and Kimi are untouched... but, seriously, Qwen3?? so far ive seen an ArliAI finetune only. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgh2gy</id>
    <title>Needing ArXiv endorsement (cs.LG)</title>
    <updated>2025-12-07T12:20:32+00:00</updated>
    <author>
      <name>/u/National_Control4101</name>
      <uri>https://old.reddit.com/user/National_Control4101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for an arXiv endorser for cs.LG (or cs.AI).&lt;/p&gt; &lt;p&gt;My optimizer release post just hit 33k views and 134 upvotes here - so clearly there‚Äôs interest. Now I need to get the paper on arXiv.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/christophergardner-star/Crux1"&gt;https://github.com/christophergardner-star/Crux1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PyPI: pip install cruxy Beats AdamW, verified to 14B.&lt;/p&gt; &lt;p&gt;Happy to share the draft paper privately. Just need someone published in cs.LG/cs.AI to vouch it‚Äôs legit.&lt;/p&gt; &lt;p&gt;I also have a second paper ready - EPTO-Dirac, a completely different approach. Where Cruxy uses control theory, EPTO uses thermodynamics&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/auth/endorse?x=B4N6T6"&gt;https://arxiv.org/auth/endorse?x=B4N6T6&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks in advance Cruxy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National_Control4101"&gt; /u/National_Control4101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgh2gy/needing_arxiv_endorsement_cslg/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgh2gy/needing_arxiv_endorsement_cslg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgh2gy/needing_arxiv_endorsement_cslg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T12:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfvt9e</id>
    <title>VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server</title>
    <updated>2025-12-06T18:10:19+00:00</updated>
    <author>
      <name>/u/marhensa</name>
      <uri>https://old.reddit.com/user/marhensa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt; &lt;img alt="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" src="https://b.thumbs.redditmedia.com/s91ewzR1qewE9gWdg8jOP6ycdK9l1T_UjLsYMD8uNoo.jpg" title="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft recently released &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;VibeVoice-Realtime-0.5B&lt;/a&gt;, a lightweight &lt;strong&gt;&lt;em&gt;expressive&lt;/em&gt;&lt;/strong&gt; TTS model.&lt;/p&gt; &lt;p&gt;I wrapped it in an OpenAI-compatible API server so it works directly with Open WebUI's TTS settings.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/marhensa/vibevoice-realtime-openai-api.git"&gt;https://github.com/marhensa/vibevoice-realtime-openai-api.git&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop-in using OpenAI-compatible &lt;code&gt;/v1/audio/speech&lt;/code&gt; endpoint&lt;/li&gt; &lt;li&gt;Runs locally with Docker or Python venv (via uv)&lt;/li&gt; &lt;li&gt;Using only ~2GB of VRAM&lt;/li&gt; &lt;li&gt;CUDA-optimized (around ~1x RTF on RTX 3060 12GB)&lt;/li&gt; &lt;li&gt;Multiple voices with OpenAI name aliases (alloy, nova, etc.)&lt;/li&gt; &lt;li&gt;All models auto-download on first run&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pfvt9e/video/7emfqdbdjm5g1/player"&gt;Video demonstration of \&amp;quot;Mike\&amp;quot; male voice. Audio üì¢ ON.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The expression and flow is better than Kokoro, imho. But Kokoro is faster.&lt;/p&gt; &lt;p&gt;But (for now) it lacks female voice model, there's just two female, and one is weirdly sounds like a male üòÖ.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6r87w5d9pm5g1.png?width=1073&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adfd10fae1523fed7f2898c38ae92816130cbf2d"&gt;vibevoice-realtime-openai-api Settings on Open WebUI: Set chunk splitting to Paragraphs.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contribution are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marhensa"&gt; /u/marhensa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg1rhf</id>
    <title>Minimax M2</title>
    <updated>2025-12-06T22:22:16+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What does the community think of Minimax M2?&lt;/p&gt; &lt;p&gt;Benches surprisingly well and the Minimax team tend to be strong at RL.&lt;/p&gt; &lt;p&gt;Any experiences with this model? Any tips or preferred use-cases?&lt;/p&gt; &lt;p&gt;Particularly interested in STEM, coding and agentic but all use-cases welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T22:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfyrwm</id>
    <title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
    <updated>2025-12-06T20:12:22+00:00</updated>
    <author>
      <name>/u/Educational-Pound269</name>
      <uri>https://old.reddit.com/user/Educational-Pound269</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt; &lt;img alt="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" src="https://external-preview.redd.it/YjQybWV5bmY1bjVnMatgKKMAYanNbnGU9s9FiIXTW5q8AYgZBBw2qwcYT6Ul.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aad98486fbb5e0f55eb92c0c79b334973e80e088" title="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They just dropped a REALTIME, infinite length video generator.&lt;/p&gt; &lt;p&gt;Based on Wan, 20 fps, with dialogue&lt;/p&gt; &lt;p&gt;The code will be open source in early December.&lt;br /&gt; &lt;a href="https://liveavatar.github.io/"&gt;https://liveavatar.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational-Pound269"&gt; /u/Educational-Pound269 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zewd3onf5n5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T20:12:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pggbbq</id>
    <title>Follow-up: Hybrid Search in Apache Solr is NOW Production-Ready (with 1024D vectors!)</title>
    <updated>2025-12-07T11:36:05+00:00</updated>
    <author>
      <name>/u/WillingnessQuick5074</name>
      <uri>https://old.reddit.com/user/WillingnessQuick5074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hey everyone,&lt;/h1&gt; &lt;p&gt;A few days back I shared my experiments with hybrid search (combining traditional lexical search with vector/semantic search). Well, I've been busy, and I'm back with some &lt;strong&gt;major upgrades&lt;/strong&gt; that I think you'll find interesting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We now have 1024-dimensional embeddings, blazing fast GPU inference, and you can generate embeddings via our free API endpoint. Plus: you can literally search with emojis now. Yes, really. üö≤ finds bicycles. üêï finds dog jewelry. Keep reading.&lt;/p&gt; &lt;h1&gt;What Changed?&lt;/h1&gt; &lt;h1&gt;1. Upgraded from 384D to 1024D Embeddings&lt;/h1&gt; &lt;p&gt;We switched from &lt;code&gt;paraphrase-multilingual-MiniLM-L12-v2&lt;/code&gt; (384 dimensions) to &lt;code&gt;BAAI/bge-m3&lt;/code&gt; (1024 dimensions).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why does this matter?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Think of dimensions like pixels in an image. A 384-pixel image is blurry. A 1024-pixel image is crisp. More dimensions = the model can capture more nuance and meaning from your text.&lt;/p&gt; &lt;p&gt;The practical result? Searches that &amp;quot;kind of worked&amp;quot; before now work &lt;strong&gt;really well&lt;/strong&gt;, especially for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Non-English languages (Romanian, German, French, etc.)&lt;/li&gt; &lt;li&gt;Domain-specific terminology&lt;/li&gt; &lt;li&gt;Conceptual/semantic queries&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Moved Embeddings to GPU&lt;/h1&gt; &lt;p&gt;Before: CPU embeddings taking 50-100ms per query. Now: GPU embeddings taking ~2-5ms per query.&lt;/p&gt; &lt;p&gt;The embedding is so fast now that even with a network round-trip from Europe to USA and back, it's &lt;strong&gt;still faster&lt;/strong&gt; than local CPU embedding was. Let that sink in.&lt;/p&gt; &lt;h1&gt;3. Optimized the Hybrid Formula&lt;/h1&gt; &lt;p&gt;After a lot of trial and error, we settled on this normalization approach:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;score = vector_score + (lexical_score / (lexical_score + k)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Where &lt;code&gt;k&lt;/code&gt; is a tuning parameter (we use k=10). This gives you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lexical score normalized to 0-1 range&lt;/li&gt; &lt;li&gt;Vector and lexical scores that play nice together&lt;/li&gt; &lt;li&gt;No division by zero issues&lt;/li&gt; &lt;li&gt;Intuitive tuning (k = the score at which you get 0.5)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Quality Filter with frange&lt;/h1&gt; &lt;p&gt;Here's a pro tip: use Solr's &lt;code&gt;frange&lt;/code&gt; to filter out garbage vector matches:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;fq={!frange l=0.3}query($vectorQuery) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This says &amp;quot;only show me documents where the vector similarity is at least 0.3&amp;quot;. Anything below that is typically noise anyway. This keeps your results clean and your users happy.&lt;/p&gt; &lt;h1&gt;Live Demos (Try These!)&lt;/h1&gt; &lt;p&gt;I've set up several demo indexes. &lt;strong&gt;Each one has a Debug button in the bottom-right corner&lt;/strong&gt; - click it to see the exact Solr query parameters and full &lt;code&gt;debugQuery&lt;/code&gt; analysis. Great for learning!&lt;/p&gt; &lt;h1&gt;üõ†Ô∏è Romanian Hardware Store (Dedeman)&lt;/h1&gt; &lt;p&gt;Search a Romanian e-commerce site with emojis:&lt;/p&gt; &lt;p&gt;&lt;a href="https://opensolr.com/search/dedeman?topbar=block&amp;amp;q=%F0%9F%9A%B2&amp;amp;in=web&amp;amp;og=yes&amp;amp;locale=&amp;amp;duration=&amp;amp;source=&amp;amp;fresh=no&amp;amp;lang="&gt;&lt;strong&gt;üö≤ ‚Üí Bicycle accessories&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;No keywords. Just an emoji. And it finds bicycle mirrors, phone holders for bikes, etc. The vector model understands that üö≤ = bicicletƒÉ = bicycle-related products.&lt;/p&gt; &lt;h1&gt;üíé English Jewelry Store (Rueb.co.uk)&lt;/h1&gt; &lt;p&gt;Sterling silver, gold, gemstones - searched semantically:&lt;/p&gt; &lt;p&gt;&lt;a href="https://opensolr.com/search/rueb?topbar=block&amp;amp;q=%F0%9F%90%95&amp;amp;in=web&amp;amp;og=yes&amp;amp;locale=&amp;amp;duration=&amp;amp;source=&amp;amp;fresh=no&amp;amp;lang="&gt;&lt;strong&gt;üêï ‚Üí Dog-themed jewelry&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://opensolr.com/search/rueb?topbar=block&amp;amp;q=%E2%AD%90%EF%B8%8F&amp;amp;in=web&amp;amp;og=yes&amp;amp;locale=&amp;amp;duration=&amp;amp;source=&amp;amp;fresh=no&amp;amp;lang="&gt;&lt;strong&gt;‚≠êÔ∏è ‚Üí Star-themed jewelry&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üß£ Luxury Cashmere Accessories (Peilishop)&lt;/h1&gt; &lt;p&gt;Hats, scarves, ponchos:&lt;/p&gt; &lt;p&gt;&lt;a href="https://opensolr.com/search/peilishop?topbar=block&amp;amp;q=winter+hat&amp;amp;in=web&amp;amp;og=yes&amp;amp;locale=&amp;amp;duration=&amp;amp;source=&amp;amp;fresh=no&amp;amp;lang="&gt;&lt;strong&gt;winter hat ‚Üí Beanies, caps, cold weather gear&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üì∞ Fresh News Index&lt;/h1&gt; &lt;p&gt;Real-time crawled news, searchable semantically:&lt;/p&gt; &lt;p&gt;&lt;a href="https://opensolr.com/search/vector?topbar=block&amp;amp;q=%F0%9F%8D%B3&amp;amp;in=web&amp;amp;og=yes&amp;amp;locale=&amp;amp;duration=&amp;amp;source=&amp;amp;fresh=no&amp;amp;lang="&gt;&lt;strong&gt;üç≥ ‚Üí Food/cooking articles&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://opensolr.com/search/vector?topbar=block&amp;amp;q=what+do+we+have+to+eat+to+boost+health%3F&amp;amp;in=web&amp;amp;og=yes&amp;amp;locale=&amp;amp;duration=&amp;amp;source=&amp;amp;fresh=no&amp;amp;lang="&gt;&lt;strong&gt;what do we have to eat to boost health? ‚Üí Nutrition articles&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This last one is pure semantic search - there's no keyword &amp;quot;boost&amp;quot; or &amp;quot;health&amp;quot; necessarily in the results, but the &lt;em&gt;meaning&lt;/em&gt; matches.&lt;/p&gt; &lt;h1&gt;Free API Endpoint for 1024D Embeddings&lt;/h1&gt; &lt;p&gt;Want to try this in your own Solr setup? We're exposing our embedding endpoint for free:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -X POST https://opensolr.com/api/embed \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{&amp;quot;text&amp;quot;: &amp;quot;your text here&amp;quot;}' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Returns a 1024-dimensional vector ready to index in Solr.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Schema setup:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;fieldType name=&amp;quot;knn_vector&amp;quot; class=&amp;quot;solr.DenseVectorField&amp;quot; vectorDimension=&amp;quot;1024&amp;quot; similarityFunction=&amp;quot;cosine&amp;quot;/&amp;gt; &amp;lt;field name=&amp;quot;embeddings&amp;quot; type=&amp;quot;knn_vector&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;false&amp;quot;/&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Key Learnings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Title repetition trick&lt;/strong&gt;: For smaller embedding models, repeat the title 3x in your embedding text. This focuses the model's limited capacity on the most important content. Game changer for product search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;topK isn't &amp;quot;how many results&amp;quot;&lt;/strong&gt;: It's &amp;quot;how many documents the vector search considers&amp;quot;. The rest get score=0 for the vector component. Keep it reasonable (100-500) to avoid noise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lexical search is still king for keywords&lt;/strong&gt;: Hybrid means vector helps when lexical fails (emojis, conceptual queries), and lexical helps when you need exact matches. Best of both worlds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use synonyms for domain-specific gaps&lt;/strong&gt;: Even the best embedding model doesn't know that &amp;quot;autofiletantƒÉ&amp;quot; (Romanian) = &amp;quot;drill&amp;quot;. A simple synonym file fixes what AI can't.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality &amp;gt; Quantity&lt;/strong&gt;: Better to return 10 excellent results than 100 mediocre ones. Use &lt;code&gt;frange&lt;/code&gt; and reasonable &lt;code&gt;topK&lt;/code&gt; values.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;Still exploring:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fine-tuning embedding models for specific domains&lt;/li&gt; &lt;li&gt;RRF (Reciprocal Rank Fusion) as an alternative to score-based hybrid&lt;/li&gt; &lt;li&gt;More aggressive caching strategies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions. And seriously, click that Debug button on the demos - seeing the actual Solr queries is super educational!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Running Apache Solr 9.x on&lt;/em&gt; &lt;a href="https://opensolr.com/"&gt;&lt;em&gt;OpenSolr.com&lt;/em&gt;&lt;/a&gt; &lt;em&gt;- free hosted Solr with vector search support.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WillingnessQuick5074"&gt; /u/WillingnessQuick5074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggbbq/followup_hybrid_search_in_apache_solr_is_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggbbq/followup_hybrid_search_in_apache_solr_is_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pggbbq/followup_hybrid_search_in_apache_solr_is_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T11:36:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdlub</id>
    <title>https://huggingface.co/Doradus/Hermes-4.3-36B-FP8</title>
    <updated>2025-12-07T08:43:48+00:00</updated>
    <author>
      <name>/u/doradus_novae</name>
      <uri>https://old.reddit.com/user/doradus_novae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdlub/httpshuggingfacecodoradushermes4336bfp8/"&gt; &lt;img alt="https://huggingface.co/Doradus/Hermes-4.3-36B-FP8" src="https://external-preview.redd.it/x9IMqhcTRGU_-36sxoHqV_SaWTySlveSoHOtr059NQs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53569ff7b4911321d14e4485d235db7fac7d8ca7" title="https://huggingface.co/Doradus/Hermes-4.3-36B-FP8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes Dense 36B Quantized from BF15 to FP8 with minimal accuracy loss!&lt;/p&gt; &lt;p&gt;Should fit over TP=2 24 or 32GB VRAM cards -&amp;gt; uses about 40gb instead of 73gb using FP16&lt;/p&gt; &lt;p&gt;Dockerfile for VLLM 0.12.0 - came out 3 days ago - included!&lt;/p&gt; &lt;p&gt;Enjoy, fellow LLMers!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Doradus/Hermes-4.3-36B-FP8"&gt;https://huggingface.co/Doradus/Hermes-4.3-36B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DoradusAI/Hermes-4.3-36B-FP8"&gt;https://github.com/DoradusAI/Hermes-4.3-36B-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doradus_novae"&gt; /u/doradus_novae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Doradus/Hermes-4.3-36B-FP8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdlub/httpshuggingfacecodoradushermes4336bfp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdlub/httpshuggingfacecodoradushermes4336bfp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T08:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pghcz3</id>
    <title>QWEN3 80B Audio Support</title>
    <updated>2025-12-07T12:36:54+00:00</updated>
    <author>
      <name>/u/idesireawill</name>
      <uri>https://old.reddit.com/user/idesireawill</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;When i use qwen3 80B through qwen chat, it seems i can use audio+text as an input.&lt;/p&gt; &lt;p&gt;Yet i cant seem to find many infor regarding to the audio input in model card. IS it possible? and if so how ?&lt;/p&gt; &lt;p&gt;Thank you in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/idesireawill"&gt; /u/idesireawill &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pghcz3/qwen3_80b_audio_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pghcz3/qwen3_80b_audio_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pghcz3/qwen3_80b_audio_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T12:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg6jp6</id>
    <title>I built a minimal Claude Code clone to understand how AI coding agents work under the hood</title>
    <updated>2025-12-07T02:07:09+00:00</updated>
    <author>
      <name>/u/Money-Coast-3905</name>
      <uri>https://old.reddit.com/user/Money-Coast-3905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"&gt; &lt;img alt="I built a minimal Claude Code clone to understand how AI coding agents work under the hood" src="https://preview.redd.it/7p800vqawo5g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=622a2626b9442d79ae3232b55093a1ac1669ddbc" title="I built a minimal Claude Code clone to understand how AI coding agents work under the hood" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I've been fascinated by tools like &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt; and &lt;a href="https://github.com/langchain-ai/deepagents"&gt;deepagents&lt;/a&gt; lately. While using them, I kept wondering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What does the system prompt actually look like?&lt;/li&gt; &lt;li&gt;How are tool schemas structured for the API?&lt;/li&gt; &lt;li&gt;How does the message flow work between turns?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I decided to build a minimal implementation myself to understand these internals better. It's called &lt;strong&gt;yacc&lt;/strong&gt; (Yet Another Claude Code) - a simple AI coding assistant built with pure Python + Anthropic API (no LangChain).&lt;/p&gt; &lt;h3&gt;What I learned and documented:&lt;/h3&gt; &lt;p&gt;üìù &lt;strong&gt;System Prompts&lt;/strong&gt; - How to structure instructions for planning, filesystem operations, and tool usage&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;Tool Schemas&lt;/strong&gt; - JSON schema definitions for tools like &lt;code&gt;read_file&lt;/code&gt;, &lt;code&gt;write_file&lt;/code&gt;, &lt;code&gt;edit_file&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;bash&lt;/code&gt;, etc.&lt;/p&gt; &lt;p&gt;üîÑ &lt;strong&gt;Middleware patterns&lt;/strong&gt; - Prompt caching, context summarization (when tokens exceed limits), patching dangling tool calls&lt;/p&gt; &lt;p&gt;üí¨ &lt;strong&gt;Message flow&lt;/strong&gt; - How tool_use and tool_result blocks work in the conversation&lt;/p&gt; &lt;h3&gt;Not production-ready, but...&lt;/h3&gt; &lt;p&gt;This is definitely NOT a replacement for Claude Code or deepagents. It's more of a &lt;strong&gt;learning resource&lt;/strong&gt; for anyone curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How Claude's tool calling works in practice&lt;/li&gt; &lt;li&gt;What a typical agentic system prompt contains&lt;/li&gt; &lt;li&gt;How to manage context in long-running agent sessions&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;GitHub&lt;/h3&gt; &lt;p&gt;üîó &lt;a href="https://github.com/SeungyounShin/yet-another-claude-code"&gt;https://github.com/SeungyounShin/yet-another-claude-code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is pretty readable and documented. Check out: - &lt;code&gt;src/prompts/system.py&lt;/code&gt; - System prompt structure - &lt;code&gt;src/tools/definitions.py&lt;/code&gt; - Tool schemas - &lt;code&gt;src/agent.py&lt;/code&gt; - Main orchestration loop - &lt;code&gt;src/middleware/&lt;/code&gt; - Context management&lt;/p&gt; &lt;p&gt;Hope this helps someone who's curious about the internals! Happy to answer any questions.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Inspired by &lt;a href="https://github.com/langchain-ai/deepagents"&gt;deepagents&lt;/a&gt; from LangChain team - they have a much more complete implementation if you need something production-ready.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Money-Coast-3905"&gt; /u/Money-Coast-3905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7p800vqawo5g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfx3d0</id>
    <title>We need open source hardware lithography</title>
    <updated>2025-12-06T19:02:38+00:00</updated>
    <author>
      <name>/u/bennmann</name>
      <uri>https://old.reddit.com/user/bennmann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps it's time hardware was more democratized. RISC-V is only 1 step away.&lt;/p&gt; &lt;p&gt;There are real challenges with yield at small scales, requiring a clean environment. But perhaps a small scale system could be made &amp;quot;good enough&amp;quot;, or overcome with some clever tech or small vacuum chambers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bennmann"&gt; /u/bennmann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgeda8</id>
    <title>RTX6000Pro stability issues (system spontaneous power cycling)</title>
    <updated>2025-12-07T09:32:28+00:00</updated>
    <author>
      <name>/u/Elv13</name>
      <uri>https://old.reddit.com/user/Elv13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I just upgraded from 4xP40 to 1x RTX6000Pro (NVIDIA RTX PRO 6000 Blackwell Workstation Edition Graphic Card - 96 GB GDDR7 ECC - PCIe 5.0 x16 - 512-Bit - 2x Slot - XHFL - Active - 600 W- 900-5G144-2200-000). I bought a 1200W corsair RM1200 along with it.&lt;/p&gt; &lt;p&gt;At 600W, the machine just reboots at soon as llama.cpp or ComfyUI starts. At 200w (&lt;code&gt;sudo nvidia-smi -pl 200&lt;/code&gt;), it starts, but reboot at some point. I just can't get it to finish anything. My old 800w PSU does no better when I power limit it to 150w.&lt;/p&gt; &lt;p&gt;VBios:&lt;/p&gt; &lt;p&gt;&lt;code&gt; nvidia-smi -q | grep &amp;quot;VBIOS Version&amp;quot; VBIOS Version : 98.02.81.00.07 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;(machine is a threadriper pro 3000 series with 16 core and 128Gb ram, OS is Ubuntu 24.04). All 4 power connectors are attached to different PSU 12v lanes. Even then, power limited at 200w, this is equivalent to a single P40 and I was running 4 of them.&lt;/p&gt; &lt;p&gt;Is that card a lemon or am I doing it wrong? Has anyone experienced this kind of instability. Do I need a 3rd PSU to test?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elv13"&gt; /u/Elv13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgeda8/rtx6000pro_stability_issues_system_spontaneous/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgeda8/rtx6000pro_stability_issues_system_spontaneous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgeda8/rtx6000pro_stability_issues_system_spontaneous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T09:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgjqm3</id>
    <title>Is it possible to run two seperate llama-server.exe processes that share the same layers and weights stored in DRAM?</title>
    <updated>2025-12-07T14:32:54+00:00</updated>
    <author>
      <name>/u/PairOfRussels</name>
      <uri>https://old.reddit.com/user/PairOfRussels</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think what happens currently is if I'm running two llama-server.exe processes with the same MOE LLM model (qwen3-next-80b) on two GPUs, and if I have any layers offloaded to CPU or MOE expert weightings on CPU, then it will have TWO independent sets of that data in DRAM.&lt;/p&gt; &lt;p&gt;I was wondering if anyone thinks it's possible to have both processes use the same data to save on ram usage.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PairOfRussels"&gt; /u/PairOfRussels &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjqm3/is_it_possible_to_run_two_seperate_llamaserverexe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjqm3/is_it_possible_to_run_two_seperate_llamaserverexe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjqm3/is_it_possible_to_run_two_seperate_llamaserverexe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T14:32:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8jtk</id>
    <title>SGLang Diffusion + Cache-DiT = 20-165% Faster Local Image/Video Generation</title>
    <updated>2025-12-07T03:48:50+00:00</updated>
    <author>
      <name>/u/Expert-Pineapple-740</name>
      <uri>https://old.reddit.com/user/Expert-Pineapple-740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick heads up: SGLang Diffusion now supports Cache-DiT integration, delivering 20-165% speedup for diffusion models with basically zero effort.&lt;/p&gt; &lt;p&gt;Just add some env variables and you're getting 46%+ faster inference on models like FLUX, Qwen-Image, HunyuanVideo, etc.&lt;/p&gt; &lt;p&gt;Works with torch.compile, quantization, and all the usual optimizations. Supports pretty much every major open-source DiT model.&lt;/p&gt; &lt;p&gt;Install: &lt;code&gt;uv pip install 'sglang[diffusion]' --prerelease=allow&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md"&gt;https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Pineapple-740"&gt; /u/Expert-Pineapple-740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg7f00</id>
    <title>Zen CPU Performance Uplift (Epyc &amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp</title>
    <updated>2025-12-07T02:50:50+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt; &lt;img alt="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" src="https://external-preview.redd.it/NDJTzKU3ltYG49f6LU-R2hFmqhxjjyJK3XNi_UF7GlA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d8c9b4d0929a1c550bf37e26c240c96d38c9d9c" title="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just happened to cross this and thought this seemed interesting. Here are some benchmarks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test Configuration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: AMD EPYC 9004 Series (Zen 4)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 96&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;: 4096&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool&lt;/strong&gt;: llama-bench&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp version&lt;/strong&gt;: 7134&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ZenDNN version&lt;/strong&gt;: 1.0.0&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: &lt;code&gt;ZENDNNL_MATMUL_ALGO=2&lt;/code&gt; (Blocked AOCL BLIS)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (BF16)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;341.50&lt;/td&gt; &lt;td align="left"&gt;395.58&lt;/td&gt; &lt;td align="left"&gt;1.16x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;382.52&lt;/td&gt; &lt;td align="left"&gt;561.94&lt;/td&gt; &lt;td align="left"&gt;1.47x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;423.40&lt;/td&gt; &lt;td align="left"&gt;624.61&lt;/td&gt; &lt;td align="left"&gt;1.48x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;414.12&lt;/td&gt; &lt;td align="left"&gt;637.97&lt;/td&gt; &lt;td align="left"&gt;1.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;338.50&lt;/td&gt; &lt;td align="left"&gt;622.08&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;308.53&lt;/td&gt; &lt;td align="left"&gt;534.76&lt;/td&gt; &lt;td align="left"&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.28&lt;/td&gt; &lt;td align="left"&gt;10.53&lt;/td&gt; &lt;td align="left"&gt;1.45x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (F32)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;184.44&lt;/td&gt; &lt;td align="left"&gt;293.39&lt;/td&gt; &lt;td align="left"&gt;1.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;189.69&lt;/td&gt; &lt;td align="left"&gt;384.71&lt;/td&gt; &lt;td align="left"&gt;2.03x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.74&lt;/td&gt; &lt;td align="left"&gt;431.21&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;231.49&lt;/td&gt; &lt;td align="left"&gt;451.51&lt;/td&gt; &lt;td align="left"&gt;1.95x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;220.05&lt;/td&gt; &lt;td align="left"&gt;425.65&lt;/td&gt; &lt;td align="left"&gt;1.93x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;189.75&lt;/td&gt; &lt;td align="left"&gt;396.73&lt;/td&gt; &lt;td align="left"&gt;2.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;2.69&lt;/td&gt; &lt;td align="left"&gt;7.34&lt;/td&gt; &lt;td align="left"&gt;2.73x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Merged: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17690"&gt;https://github.com/ggml-org/llama.cpp/pull/17690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, while disappointingly for Epyc and STX-H only it seems, it has been able to work on the Ryzen 7940HS, perhaps uplifts can be seen on consumer desktop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/17684"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg76jo</id>
    <title>Why local coding models are less popular than hosted coding models?</title>
    <updated>2025-12-07T02:39:17+00:00</updated>
    <author>
      <name>/u/WasteTechnology</name>
      <uri>https://old.reddit.com/user/WasteTechnology</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In theory, local coding models sound very good. You don't send your most valuable assets to another company, keep everything local and under control. However, the leading AI coding startups work with hosted models (correct me if I'm wrong). Why do you think it is so?&lt;/p&gt; &lt;p&gt;If you use one, please share your setup. Which model, which engine, which coding tool do you use?, What is your experience? Do you get productive enough with them compared to hosted options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WasteTechnology"&gt; /u/WasteTechnology &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgjpaq</id>
    <title>Built a small Whisper.cpp + Gemini meeting buddy (transcription + real-time answers)</title>
    <updated>2025-12-07T14:31:19+00:00</updated>
    <author>
      <name>/u/OuSsAmA_O2</name>
      <uri>https://old.reddit.com/user/OuSsAmA_O2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjpaq/built_a_small_whispercpp_gemini_meeting_buddy/"&gt; &lt;img alt="Built a small Whisper.cpp + Gemini meeting buddy (transcription + real-time answers)" src="https://external-preview.redd.it/cWc3ZHp1c2ZrczVnMecs0eTNAfC10oj7n_Ja-4dn7CpM06jvRZHJQjxo80Hz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d829fa32e9fdf7f77ed3af62138d00e60c08931" title="Built a small Whisper.cpp + Gemini meeting buddy (transcription + real-time answers)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I built a small app and I‚Äôm curious if anyone finds this useful or has ideas to improve it.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses &lt;code&gt;whisper.cpp&lt;/code&gt; (&lt;code&gt;whisper-stream&lt;/code&gt;) for live transcription and streams the text into a React UI.&lt;/li&gt; &lt;li&gt;Cleans the raw output (removes ANSI junk, filters tiny/noisy bits, and reduces repeated partial sentences).&lt;/li&gt; &lt;li&gt;Has an &lt;strong&gt;‚ÄúAnswer‚Äù&lt;/strong&gt; button that sends the recent transcript to Gemini and gets: &lt;ul&gt; &lt;li&gt;direct, human answers in the same language,&lt;/li&gt; &lt;li&gt;based on questions in the conversation or just any technical question it finds in the conversation.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Stack is Flask + Flask‚ÄëSocketIO on the backend (spawning &lt;code&gt;whisper-stream&lt;/code&gt; as a subprocess and pushing lines over websockets) and React + Tailwind on the frontend with two panels: left for the live transcript, right for the AI‚Äôs answers.&lt;/p&gt; &lt;p&gt;Repo if you want to look or try it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Geesama02/live-transcription-ai-helper"&gt;&lt;code&gt;https://github.com/Geesama02/live-transcription-ai-helper&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have thoughts on better ways to handle Whisper‚Äôs streaming refinements, prompt design for the Q&amp;amp;A, or UX ideas, I‚Äôd really appreciate any feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuSsAmA_O2"&gt; /u/OuSsAmA_O2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ozr6ynsfks5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjpaq/built_a_small_whispercpp_gemini_meeting_buddy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjpaq/built_a_small_whispercpp_gemini_meeting_buddy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T14:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdyxr</id>
    <title>RnJ-1-Instruct FP8 Quantization</title>
    <updated>2025-12-07T09:07:06+00:00</updated>
    <author>
      <name>/u/doradus_novae</name>
      <uri>https://old.reddit.com/user/doradus_novae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdyxr/rnj1instruct_fp8_quantization/"&gt; &lt;img alt="RnJ-1-Instruct FP8 Quantization" src="https://external-preview.redd.it/f3hfIybvzLxoPbJzEByWUBJrtbOq2n1Nf0l2zrxAhSw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d280df0cf1ec0f321434e30aa1d6a29aebed5432" title="RnJ-1-Instruct FP8 Quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FP8 quantized version of RnJ1-Instruct-8B BF16 instruction model.&lt;/p&gt; &lt;p&gt;VRAM: 16GB ‚Üí 8GB (50% reduction)&lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;p&gt;- GSM8K: 87.2%&lt;/p&gt; &lt;p&gt;- MMLU-Pro: 44.5%&lt;/p&gt; &lt;p&gt;- IFEval: 55.3%&lt;/p&gt; &lt;p&gt;Runs on RTX 3060 12GB. One-liner to try:&lt;/p&gt; &lt;p&gt;docker run --gpus '&amp;quot;device=0&amp;quot;' -p 8000:8000 vllm/vllm-openai:v0.12.0 \&lt;/p&gt; &lt;p&gt;--model Doradus/Rn&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doradus_novae"&gt; /u/doradus_novae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Doradus/RnJ-1-Instruct-FP8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdyxr/rnj1instruct_fp8_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdyxr/rnj1instruct_fp8_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T09:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pggss8</id>
    <title>I'm tired of claude limits, what's the best alternative? (cloud based or local llm)</title>
    <updated>2025-12-07T12:04:59+00:00</updated>
    <author>
      <name>/u/Dry_Explanation_7774</name>
      <uri>https://old.reddit.com/user/Dry_Explanation_7774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone I hope y'all having a great day.&lt;/p&gt; &lt;p&gt;I've been using Claude Code since they released but I'm tired of the usage limits they have even when paying subscription.&lt;/p&gt; &lt;p&gt;I'm asking here since most of you have a great knowledge on what's the best and efficient way to run AI be it online with API or running a local LLM.&lt;/p&gt; &lt;p&gt;I'm asking, what's the best way to actually run Claude at cheap rates and at the same time getting the best of it without that ridiculous usage limits? &lt;/p&gt; &lt;p&gt;Or is there any other model that gives super similar or higher results for &amp;quot;coding&amp;quot; related activities but at the same time super cheap?&lt;/p&gt; &lt;p&gt;Or any of you recommend running my own local llm? which are your recommendations about this?&lt;/p&gt; &lt;p&gt;I currently have a GTX 1650 SUPER and 16GB RAM, i know it's super funny lol, but just lyk my current specs, so u can recommend me to buy something local or just deploy a local ai into a &amp;quot;custom ai hosting&amp;quot; and use the API?&lt;/p&gt; &lt;p&gt;I know there are a lot of questions, but I think you get my idea. I wanna get started to use the &amp;quot;&amp;quot;&amp;quot;tricks&amp;quot;&amp;quot;&amp;quot; that some of you use in order to use AI with the highest performace and at lowest rate.&lt;/p&gt; &lt;p&gt;Looking forward to hear ideas, recommendations or guidance!&lt;/p&gt; &lt;p&gt;Thanks a lot in advance, and I wish y'all a wonderful day :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Explanation_7774"&gt; /u/Dry_Explanation_7774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T12:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdh8q</id>
    <title>[ Removed by Reddit ]</title>
    <updated>2025-12-07T08:35:02+00:00</updated>
    <author>
      <name>/u/NandaVegg</name>
      <uri>https://old.reddit.com/user/NandaVegg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[ Removed by Reddit on account of violating the &lt;a href="/help/contentpolicy"&gt;content policy&lt;/a&gt;. ]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NandaVegg"&gt; /u/NandaVegg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T08:35:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8ix9</id>
    <title>My little decentralized Locallama setup, 216gb VRAM</title>
    <updated>2025-12-07T03:47:31+00:00</updated>
    <author>
      <name>/u/Goldkoron</name>
      <uri>https://old.reddit.com/user/Goldkoron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt; &lt;img alt="My little decentralized Locallama setup, 216gb VRAM" src="https://preview.redd.it/o1o7ekxycp5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66f62e54e1c923ea71f1a1d46415562ffdcbc1ba" title="My little decentralized Locallama setup, 216gb VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goldkoron"&gt; /u/Goldkoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o1o7ekxycp5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
