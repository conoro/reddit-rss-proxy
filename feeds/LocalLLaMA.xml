<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-11T12:11:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q9xfsi</id>
    <title>Ways to Benchmark this Tool?</title>
    <updated>2026-01-11T11:48:35+00:00</updated>
    <author>
      <name>/u/valkarias</name>
      <uri>https://old.reddit.com/user/valkarias</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey. I've been experimenting with the idea of applying neural networks alongside LLMs. My first experiment was simple text classification on an LLM's context to &amp;quot;curate&amp;quot; it. I employ a simple decision tree as a start. We classify segments of text to three categories. DROP, INDEX, KEEP. Defined per the dataset. KEEP is defined as anything that would break context and must be preserved in the history. DROP is anything phatic, of no importance what so ever like chit chat segments in coding sessions. INDEX, is anything of reference, might be important later but not now, old/broken code versions, or could be &amp;quot;compressed&amp;quot;.&lt;/p&gt; &lt;p&gt;Now, The tool does not classify in the immediate context, initially I fucked up and built the dataset to look for the immediate &amp;quot;local&amp;quot; patterns (current immediate context). I did an re-iteration and being more careful. The tool processes in the &amp;quot;past&amp;quot;. By employing a sliding window that has the recent segments, those are untouched. This sliding window has a FIFO mechanism (First in First out). Where the oldest segment of this window gets evicted, and classified. The tree uses a feature set of text statistics, that also concern the last classified segment and the next (or the now) oldest segment in the window.&lt;/p&gt; &lt;p&gt;One bottleneck am facing is verifying this tool. Is it actually doing something or just no better than random deletion or summarization? Initially I just did tests on a set of messy long conversations and evaluated manually to see any patterns of error. However that might potentially not be ideal for uncovering edge-cases and what not.&lt;/p&gt; &lt;p&gt;Any propositions guys? On how to measure the &amp;quot;accuracy&amp;quot; of the context produced by the tool versus the actual context.&lt;/p&gt; &lt;p&gt;I held some details out, to cut on the posts' length. A decision tree is an initial. I aim to play with attention mechanisms. But the proof of concept holds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valkarias"&gt; /u/valkarias &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xfsi/ways_to_benchmark_this_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xfsi/ways_to_benchmark_this_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xfsi/ways_to_benchmark_this_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T11:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9xhbf</id>
    <title>Looking to start a NSFW Role-playing Interactive Storytelling Narrative.</title>
    <updated>2026-01-11T11:50:58+00:00</updated>
    <author>
      <name>/u/Carrot_Jesus</name>
      <uri>https://old.reddit.com/user/Carrot_Jesus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want something similar to &lt;a href="https://infiniteworlds.app/"&gt;Infinite Worlds&lt;/a&gt; where the AI creates the narrative and introduces different characters on its own. The goal is to be able to run a full story based on Game of Thrones while changing that story as my actions impact the unfolding narrative. I currently have KoboldCCP running and use SillyTavern as the UI. The model I've been using is MythoMax-l2-13b. My GPU is a AMD RX 7900 XTX with 24gb of VRAM. CPU is an AMD Ryzen 7 9800X3D. This is my first time diving into the AI space so any assistance and patience while I learn is much appreciated.&lt;/p&gt; &lt;p&gt;Edit: I don't want this to be just another AI Waifu roleplay thing. I'm wanting world building, complex relationships and decisions, and engaging storytelling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Carrot_Jesus"&gt; /u/Carrot_Jesus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xhbf/looking_to_start_a_nsfw_roleplaying_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xhbf/looking_to_start_a_nsfw_roleplaying_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xhbf/looking_to_start_a_nsfw_roleplaying_interactive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T11:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9uc8m</id>
    <title>I tried chatterbox extended for "pseudo voice conversion" with a 15 seconds target voice audio - any other apps that allow me to do that, and do it even better?</title>
    <updated>2026-01-11T08:39:23+00:00</updated>
    <author>
      <name>/u/hugo-the-second</name>
      <uri>https://old.reddit.com/user/hugo-the-second</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is &amp;quot;genuine&amp;quot; voice conversion, by training on extensive target audio, like I can do with RVC.&lt;br /&gt; Which definitely shines at keeping faithful to the prosody of the source audio, but has limitations in making the generated voice sound like the target voice.&lt;/p&gt; &lt;p&gt;And then there is this form of pseudo voice conversion, or really voice conditioning, that chatterbox extended offers, and that works with a short audio clip, instead of a voice model, like your typical tts.&lt;br /&gt; My first impressions are that it shines at making the target voice come through, is okay good with capturing the rough features like speed, pauses, intonation of the source voice, but is not good at capturing the subtleties of the source voice.&lt;/p&gt; &lt;p&gt;Would be curious if there are other, possibly more recent local apps that do that, and that are at least as good, or better, than chatterbox extended.&lt;/p&gt; &lt;p&gt;Just to avoid any confusion:&lt;br /&gt; I am not asking for tts, I am asking for vc, or more precisely, pseude vc, or voice conditioning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hugo-the-second"&gt; /u/hugo-the-second &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9uc8m/i_tried_chatterbox_extended_for_pseudo_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9uc8m/i_tried_chatterbox_extended_for_pseudo_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9uc8m/i_tried_chatterbox_extended_for_pseudo_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T08:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9j0r8</id>
    <title>[Project] Running quantized BERT in the browser via WebAssembly (Rust + Candle) for local Semantic Search</title>
    <updated>2026-01-10T23:21:19+00:00</updated>
    <author>
      <name>/u/JellyfishFar8435</name>
      <uri>https://old.reddit.com/user/JellyfishFar8435</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9j0r8/project_running_quantized_bert_in_the_browser_via/"&gt; &lt;img alt="[Project] Running quantized BERT in the browser via WebAssembly (Rust + Candle) for local Semantic Search" src="https://external-preview.redd.it/ZDNpemFtbDB2bGNnMV82M8AjMZW4dYrOSYG00DJTbyxBApD1BqJxrMXf5lMv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1ec0cf943a117df7d46f508e764d45d4a2dc8ec" title="[Project] Running quantized BERT in the browser via WebAssembly (Rust + Candle) for local Semantic Search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long time lurker, first time poster.&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on to implement &lt;strong&gt;client-side semantic search&lt;/strong&gt; without relying on Python backends or ONNX Runtime.&lt;/p&gt; &lt;p&gt;The goal was to build a tool to search through WhatsApp exports semantically (finding messages by meaning), but strictly &lt;strong&gt;local-first&lt;/strong&gt; (no data egress).&lt;/p&gt; &lt;p&gt;I implemented the entire pipeline in &lt;strong&gt;Rust&lt;/strong&gt; compiling to &lt;strong&gt;WebAssembly&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack &amp;amp; Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Inference Engine:&lt;/strong&gt; Instead of onnxruntime-web, I used &lt;a href="https://github.com/huggingface/candle"&gt;&lt;strong&gt;Candle&lt;/strong&gt;&lt;/a&gt; (Hugging Face's minimalist ML framework for Rust).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; sentence-transformers/all-MiniLM-L6-v2.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Loading the model directly in Wasm.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; Custom in-memory vector store implemented in Rust using a flattened Vec&amp;lt;f32&amp;gt; layout for cache locality during dot product calculations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why Rust/Candle over ONNX.js?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I found that managing the memory lifecycle in Rust + Wasm was cleaner than dealing with JS Garbage Collection spikes when handling large tensor arrays. Plus, candle allows dropping unnecessary kernels to keep the Wasm binary size relatively small compared to shipping the full ONNX runtime.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Initialization:&lt;/strong&gt; ~1.5s to load weights and tokenizer (cached via IndexedDB afterwards).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference:&lt;/strong&gt; Computes embeddings for short texts in &amp;lt;30ms on a standard M4 Air.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threading:&lt;/strong&gt; Offloaded the Wasm execution to a &lt;strong&gt;Web Worker&lt;/strong&gt; to prevent the main thread (React UI) from blocking during the tokenization/embedding loop.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;br /&gt; The repo is open source (MIT). The core logic is in the /core folder (Rust).&lt;br /&gt; &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/marcoshernanz/ChatVault"&gt;https://github.com/marcoshernanz/ChatVault&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt;&lt;br /&gt; You can try the WASM inference live here (works offline after load):&lt;br /&gt; &lt;a href="https://chat-vault-mh.vercel.app/"&gt;https://chat-vault-mh.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear your thoughts on using Rust for edge inference vs the traditional TF.js/ONNX route!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JellyfishFar8435"&gt; /u/JellyfishFar8435 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/by69mgl0vlcg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9j0r8/project_running_quantized_bert_in_the_browser_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9j0r8/project_running_quantized_bert_in_the_browser_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T23:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8ckz0</id>
    <title>The reason why RAM has become so expensive</title>
    <updated>2026-01-09T16:18:22+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt; &lt;img alt="The reason why RAM has become so expensive" src="https://preview.redd.it/sgbhubsomccg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57d847c1b9a2d5786b0a888b5d0d25fe5ede9e12" title="The reason why RAM has become so expensive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sgbhubsomccg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T16:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q959am</id>
    <title>Strix Halo (Bosgame M5) + 7900 XTX eGPU: Local LLM Benchmarks (Llama.cpp vs vLLM). A loose follow-up</title>
    <updated>2026-01-10T14:20:31+00:00</updated>
    <author>
      <name>/u/reujea0</name>
      <uri>https://old.reddit.com/user/reujea0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Added prompt processing metrics for 2art 2&lt;/p&gt; &lt;p&gt;This is a loose follow-up to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"&gt;previous article regarding the 7900 XTX&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I recently got my hands on a Strix Halo system, specifically the &lt;strong&gt;Bosgame M5&lt;/strong&gt;. My goal was to benchmark the Strix Halo standalone (which is a beast), and then see what effects adding a 7900 XTX via eGPU (TB3/USB4) would have on performance.&lt;/p&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Host:&lt;/strong&gt; Bosgame M5 (Strix Halo)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Fedora Server 43&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eGPU:&lt;/strong&gt; 7900 XTX (Connected via USB4/TB3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Toolboxes:&lt;/strong&gt; Huge thanks to &lt;a href="https://github.com/kyuz0"&gt;kyuz0 on GitHub&lt;/a&gt; for the &lt;a href="https://github.com/kyuz0/amd-strix-halo-toolboxes"&gt;llama.cpp toolboxes&lt;/a&gt; and &lt;a href="https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes"&gt;vLLM toolboxes&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Critical Tip for eGPU users:&lt;/strong&gt; To prevent the whole system from becoming unresponsive when activating the Thunderbolt enclosure, I had to add the following kernel parameter: &lt;code&gt;pcie_port_pm=off&lt;/code&gt; (Found this solution online, it's a lifesaver for stability).&lt;/p&gt; &lt;h1&gt;Part 1: Strix Halo Standalone (Llama.cpp)&lt;/h1&gt; &lt;p&gt;I first ran the same models used in my previous 7900 XTX post, plus some larger ones that didn't fit on the 7900 XTX alone. &lt;em&gt;Backend: ROCm&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;PP (512)&lt;/th&gt; &lt;th align="left"&gt;Gen (tg512)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B-Instruct&lt;/strong&gt; (BF16)&lt;/td&gt; &lt;td align="left"&gt;14.96 GB&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;950 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;112.27 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-3.2-24B&lt;/strong&gt; (Q5_K_XL)&lt;/td&gt; &lt;td align="left"&gt;15.63 GB&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;405 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.10 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/strong&gt; (Q3_K_M)&lt;/td&gt; &lt;td align="left"&gt;14.84 GB&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;311 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.26 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (F16)&lt;/td&gt; &lt;td align="left"&gt;12.83 GB&lt;/td&gt; &lt;td align="left"&gt;20B&lt;/td&gt; &lt;td align="left"&gt;797 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.62 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;11.27 GB&lt;/td&gt; &lt;td align="left"&gt;20B&lt;/td&gt; &lt;td align="left"&gt;766 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.69 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-VL-30B-Thinking&lt;/strong&gt; (Q4_K_XL)&lt;/td&gt; &lt;td align="left"&gt;16.49 GB&lt;/td&gt; &lt;td align="left"&gt;30B&lt;/td&gt; &lt;td align="left"&gt;1118 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.45 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;59.02 GB&lt;/td&gt; &lt;td align="left"&gt;116B&lt;/td&gt; &lt;td align="left"&gt;612 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.07 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GLM-4.6V&lt;/strong&gt; (Q4_K_M)&lt;/td&gt; &lt;td align="left"&gt;65.60 GB&lt;/td&gt; &lt;td align="left"&gt;106B&lt;/td&gt; &lt;td align="left"&gt;294 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;19.85 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt; (Q3_K_M)&lt;/td&gt; &lt;td align="left"&gt;101.76 GB&lt;/td&gt; &lt;td align="left"&gt;228B&lt;/td&gt; &lt;td align="left"&gt;210 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;26.24 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Part 2: Strix Halo (iGPU) + 7900 XTX (eGPU) Split&lt;/h1&gt; &lt;p&gt;I wanted to see if offloading to the eGPU helped. I used &lt;code&gt;llama-serve&lt;/code&gt; with a custom Python script to measure throughput. These were all done with a context of 4K.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strategy:&lt;/strong&gt; 1:1 split for small models; maximized 7900 XTX load for large models.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model (GGUF)&lt;/th&gt; &lt;th&gt;Size&lt;/th&gt; &lt;th&gt;Config&lt;/th&gt; &lt;th&gt;iGPU PP&lt;/th&gt; &lt;th&gt;Split PP&lt;/th&gt; &lt;th&gt;PP Œî&lt;/th&gt; &lt;th&gt;iGPU TG&lt;/th&gt; &lt;th&gt;Split TG&lt;/th&gt; &lt;th&gt;TG Œî&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Llama-3.1-8B-Instruct-BF16.gguf&lt;/td&gt; &lt;td&gt;16GB&lt;/td&gt; &lt;td&gt;1:1&lt;/td&gt; &lt;td&gt;2,279 t/s&lt;/td&gt; &lt;td&gt;612 t/s&lt;/td&gt; &lt;td&gt;-73%&lt;/td&gt; &lt;td&gt;12.61 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;18.82 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+49%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf&lt;/td&gt; &lt;td&gt;17GB&lt;/td&gt; &lt;td&gt;1:1&lt;/td&gt; &lt;td&gt;1,658 t/s&lt;/td&gt; &lt;td&gt;404 t/s&lt;/td&gt; &lt;td&gt;-76%&lt;/td&gt; &lt;td&gt;12.10 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;16.90 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+40%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-R1-Distill-Qwen-32B-Q3_K_M.gguf&lt;/td&gt; &lt;td&gt;16GB&lt;/td&gt; &lt;td&gt;1:1&lt;/td&gt; &lt;td&gt;10,085 t/s&lt;/td&gt; &lt;td&gt;561 t/s&lt;/td&gt; &lt;td&gt;-94%&lt;/td&gt; &lt;td&gt;12.26 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;15.45 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+26%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss-20b-F16.gguf&lt;/td&gt; &lt;td&gt;14GB&lt;/td&gt; &lt;td&gt;1:1&lt;/td&gt; &lt;td&gt;943 t/s&lt;/td&gt; &lt;td&gt;556 t/s&lt;/td&gt; &lt;td&gt;-41%&lt;/td&gt; &lt;td&gt;50.09 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;61.17 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+22%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss-20b-mxfp4.gguf&lt;/td&gt; &lt;td&gt;12GB&lt;/td&gt; &lt;td&gt;1:1&lt;/td&gt; &lt;td&gt;1,012 t/s&lt;/td&gt; &lt;td&gt;624 t/s&lt;/td&gt; &lt;td&gt;-38%&lt;/td&gt; &lt;td&gt;70.27 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;78.01 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+11%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-VL-30B-A3B-Thinking-UD-Q4_K_XL.gguf&lt;/td&gt; &lt;td&gt;18GB&lt;/td&gt; &lt;td&gt;1:1&lt;/td&gt; &lt;td&gt;1,834 t/s&lt;/td&gt; &lt;td&gt;630 t/s&lt;/td&gt; &lt;td&gt;-66%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;65.23 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;57.50 t/s&lt;/td&gt; &lt;td&gt;-12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss-120b-mxfp4.gguf&lt;/td&gt; &lt;td&gt;63GB&lt;/td&gt; &lt;td&gt;3:1&lt;/td&gt; &lt;td&gt;495 t/s&lt;/td&gt; &lt;td&gt;371 t/s&lt;/td&gt; &lt;td&gt;-25%&lt;/td&gt; &lt;td&gt;49.35 t/s&lt;/td&gt; &lt;td&gt;52.57 t/s&lt;/td&gt; &lt;td&gt;+7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss-120b-mxfp4.gguf&lt;/td&gt; &lt;td&gt;63GB&lt;/td&gt; &lt;td&gt;3:2&lt;/td&gt; &lt;td&gt;495 t/s&lt;/td&gt; &lt;td&gt;411 t/s&lt;/td&gt; &lt;td&gt;-17%&lt;/td&gt; &lt;td&gt;49.35 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;54.56 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+11%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM-4.6V-Q4_K_M.gguf&lt;/td&gt; &lt;td&gt;70GB&lt;/td&gt; &lt;td&gt;2:1&lt;/td&gt; &lt;td&gt;1,700 t/s&lt;/td&gt; &lt;td&gt;294 t/s&lt;/td&gt; &lt;td&gt;-83%&lt;/td&gt; &lt;td&gt;20.54 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;23.46 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+14%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MiniMax-M2.1-Q3_K_M.gguf&lt;/td&gt; &lt;td&gt;~60GB&lt;/td&gt; &lt;td&gt;17:5&lt;/td&gt; &lt;td&gt;1,836 t/s&lt;/td&gt; &lt;td&gt;255 t/s&lt;/td&gt; &lt;td&gt;-86%&lt;/td&gt; &lt;td&gt;26.22 t/s&lt;/td&gt; &lt;td&gt;&lt;strong&gt;27.19 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+4%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The PP values use only Run 1 data because Runs 2-3 showed 0.00s prompt times due to llama-server's internal caching, making their PP speeds unrealistically high (50,000+ t/s). The PP speed is calculated from the &lt;code&gt;timings.prompt_ms&lt;/code&gt; value in llama-server's JSON response (prompt_tokens / prompt_time_seconds), while TG speed comes from &lt;code&gt;timings.predicted_ms&lt;/code&gt; (predicted_tokens / predicted_time_seconds). TG values are averaged across all 3 runs since generation times remained consistent and weren't affected by caching.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding the eGPU is beneficial for smaller, dense models where we get a &lt;strong&gt;~50% boost&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;However, for larger models or MoEs, the &lt;strong&gt;USB4/TB3 bandwidth&lt;/strong&gt; likely becomes a bottleneck. The latency introduced by splitting the model across the interconnect kills the gains, leading to diminishing returns (+4% to +14%) or even regression (-12% on Qwen3-VL).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 3: vLLM on Strix Halo&lt;/h1&gt; &lt;p&gt;The situation with vLLM is a bit rougher. I wasn't willing to wrestle with multi-GPU configuration here, so these results are &lt;strong&gt;Strix Halo Single GPU only&lt;/strong&gt;.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Output Speed (tok/s)&lt;/th&gt; &lt;th align="left"&gt;TTFT (Mean)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25.87 t/s&lt;/td&gt; &lt;td align="left"&gt;1164 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17.34 t/s&lt;/td&gt; &lt;td align="left"&gt;633 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-24B&lt;/strong&gt; (bnb-4bit)&lt;/td&gt; &lt;td align="left"&gt;4.23 t/s&lt;/td&gt; &lt;td align="left"&gt;3751 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25.37 t/s&lt;/td&gt; &lt;td align="left"&gt;3625 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15.5 t/s&lt;/td&gt; &lt;td align="left"&gt;4458&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;vLLM support on ROCm (specifically for Strix Halo/consumer cards) seems to be lagging behind llama.cpp significantly. The generation speeds are much lower, and the Time To First Token (TTFT) is quite high.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reujea0"&gt; /u/reujea0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T14:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q90ye2</id>
    <title>Jensen Huang at CES on how open models have really revolutionized AI last year. ‚ÄúWhen AI is open, it proliferates everywhere.‚Äù</title>
    <updated>2026-01-10T10:36:58+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"&gt; &lt;img alt="Jensen Huang at CES on how open models have really revolutionized AI last year. ‚ÄúWhen AI is open, it proliferates everywhere.‚Äù" src="https://external-preview.redd.it/bGVlcWZ0bzcxaWNnMW_K1BNM1KBv7FYngB2itMlTyoA2GP6X-h0KJFWgL9Yw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35c5c46d790cb187582f60967603f3cda5bc1020" title="Jensen Huang at CES on how open models have really revolutionized AI last year. ‚ÄúWhen AI is open, it proliferates everywhere.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From NVIDIA AI on ùïè: &lt;a href="https://x.com/NVIDIAAI/status/2009731908888895516"&gt;https://x.com/NVIDIAAI/status/2009731908888895516&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/73l3tyn71icg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T10:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9hu43</id>
    <title>I built an end-to-end local LLM fine-tuning GUI for M series macs</title>
    <updated>2026-01-10T22:31:40+00:00</updated>
    <author>
      <name>/u/riman717</name>
      <uri>https://old.reddit.com/user/riman717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a tool I‚Äôve been working on to make local fine-tuning on M series Macs a bit less painful and manual. Essentially it wraps Apple‚Äôs MLX framework, so it runs native on M-series chips. The goal of this was to include the whole end-to-end local LLM workflow all within a GUI. Here are the features I put in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data Prep- You can drag and drop CSV or JSONL files to clean/format them. I also added a local PII scrubber to strip names/emails from datasets before training.&lt;/li&gt; &lt;li&gt;Fine-Tuning- UI for LoRA/QLoRA. You can tweak learning rates, epochs, rank, etc&lt;/li&gt; &lt;li&gt;Inference- Built-in chat interface to test your Fine Tuned model adapters against the base model&lt;/li&gt; &lt;li&gt;Models- One-click download for open source LLMs, or you can &amp;quot;add a model&amp;quot; if you have local model rates&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo is here if you want to check it out: &lt;a href="https://github.com/rileycleavenger/Silicon-Studio"&gt;https://github.com/rileycleavenger/Silicon-Studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to contribute or open any issues on the repo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riman717"&gt; /u/riman717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9hu43/i_built_an_endtoend_local_llm_finetuning_gui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9hu43/i_built_an_endtoend_local_llm_finetuning_gui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9hu43/i_built_an_endtoend_local_llm_finetuning_gui_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T22:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9tqy7</id>
    <title>good uncensored online LLM for general use?</title>
    <updated>2026-01-11T08:03:32+00:00</updated>
    <author>
      <name>/u/Lolis-</name>
      <uri>https://old.reddit.com/user/Lolis-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work with nsfw material regularly and most services I know of absolutely hate it. So far I have just been using grok and it works okayish but it's quite expensive, wondering if there's any good alternative. Preferably something that can handle everything chatgpt does like transcribing images, web searching, be available on all platforms etc.&lt;/p&gt; &lt;p&gt;NOT looking for &amp;quot;rp&amp;quot; centric vendors&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lolis-"&gt; /u/Lolis- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9tqy7/good_uncensored_online_llm_for_general_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9tqy7/good_uncensored_online_llm_for_general_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9tqy7/good_uncensored_online_llm_for_general_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T08:03:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9xoj7</id>
    <title>model: try to improve Qwen3 Next by ngxson ¬∑ Pull Request #18683 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-01-11T12:02:07+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xoj7/model_try_to_improve_qwen3_next_by_ngxson_pull/"&gt; &lt;img alt="model: try to improve Qwen3 Next by ngxson ¬∑ Pull Request #18683 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/l3QvKEV3Em4ksQ_HeWB_lAYpQBJm9HMiWOmkds8SLcc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=900d5988034c78eb76d04fa4b463959f4f6e1083" title="model: try to improve Qwen3 Next by ngxson ¬∑ Pull Request #18683 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;a bit faster Qwen3Next, but you have to use the new GGUF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18683"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xoj7/model_try_to_improve_qwen3_next_by_ngxson_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xoj7/model_try_to_improve_qwen3_next_by_ngxson_pull/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T12:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bnqc</id>
    <title>RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)</title>
    <updated>2026-01-10T18:33:05+00:00</updated>
    <author>
      <name>/u/3090orBust</name>
      <uri>https://old.reddit.com/user/3090orBust</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"&gt; &lt;img alt="RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)" src="https://external-preview.redd.it/o9dwYZagSuAh6lGApyCDxDEoIhgXfz9GzoXhVGYG6FI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbb419ac1006074d3e7d893a55075c0de8698965" title="RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3090orBust"&gt; /u/3090orBust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/RTX-50-Super-GPUs-may-be-delayed-indefinitely-as-Nvidia-prioritizes-AI-during-memory-shortage.1199980.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T18:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9m0uw</id>
    <title>brain-canvas: Give any local LLM a visual display (191 lines, 0 deps)</title>
    <updated>2026-01-11T01:30:17+00:00</updated>
    <author>
      <name>/u/Signal_Usual8630</name>
      <uri>https://old.reddit.com/user/Signal_Usual8630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tired of LLM output being stuck in the terminal?&lt;/p&gt; &lt;p&gt;npx brain-canvas&lt;/p&gt; &lt;p&gt;Starts a local HTML canvas that any LLM can control via POST requests. Send JSON, get interactive UI with clickable choices that flow back to your script.&lt;/p&gt; &lt;p&gt;Works with:&lt;/p&gt; &lt;p&gt;- Ollama&lt;/p&gt; &lt;p&gt;- llama.cpp&lt;/p&gt; &lt;p&gt;- Any local model&lt;/p&gt; &lt;p&gt;- Claude/GPT (if you use those too)&lt;/p&gt; &lt;p&gt;The numbers:&lt;/p&gt; &lt;p&gt;- 191 lines of code&lt;/p&gt; &lt;p&gt;- 0 dependencies&lt;/p&gt; &lt;p&gt;- 6.9 KB package&lt;/p&gt; &lt;p&gt;- 10 section types (stats, timeline, comparison, choices, etc.)&lt;/p&gt; &lt;p&gt;POST JSON like:&lt;/p&gt; &lt;p&gt;{&amp;quot;title&amp;quot;: &amp;quot;Pick one&amp;quot;, &amp;quot;sections&amp;quot;: [{&amp;quot;type&amp;quot;: &amp;quot;choices&amp;quot;, &amp;quot;items&amp;quot;: [{&amp;quot;id&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;label&amp;quot;: &amp;quot;Option A&amp;quot;}]}]}&lt;/p&gt; &lt;p&gt;GET /choice returns what the user clicked.&lt;/p&gt; &lt;p&gt;Zero config. Works on Mac/Linux/Windows.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mordechaipotash/brain-canvas"&gt;https://github.com/mordechaipotash/brain-canvas&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal_Usual8630"&gt; /u/Signal_Usual8630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9m0uw/braincanvas_give_any_local_llm_a_visual_display/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9m0uw/braincanvas_give_any_local_llm_a_visual_display/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9m0uw/braincanvas_give_any_local_llm_a_visual_display/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T01:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9nerk</id>
    <title>Made an Rick and Morty inspired Interdimensional News site with Ollama and Gemini</title>
    <updated>2026-01-11T02:32:32+00:00</updated>
    <author>
      <name>/u/WahWahWeWah</name>
      <uri>https://old.reddit.com/user/WahWahWeWah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I love Rick and Morty esp. the interdimensional cable episodes. So, I build &lt;a href="http://greenportal.news"&gt;greenportal.news&lt;/a&gt; using ollama and gemini. &lt;/p&gt; &lt;p&gt;I'm happy to double click on how the site is made. Basically, its a scraper of a lot of news content off of the internet. Then, using ollama + nemotron-3-nano I extract and score the articles. The alternate universes work the same way, with ollama expanding the prompt and creating the rules for the universe. Lastly, I make a few images in Nano Banana--which imho are the funniest part.&lt;/p&gt; &lt;p&gt;I'd like to move off Gemini to something I can run locally. Any recommendations? I'm rolling with a single 4090 over here so I'd love to keep using that.&lt;/p&gt; &lt;p&gt;Lastly, I write enterprise software so I know the UX isn't amazing. Don't be too hard on me :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WahWahWeWah"&gt; /u/WahWahWeWah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9nerk/made_an_rick_and_morty_inspired_interdimensional/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9nerk/made_an_rick_and_morty_inspired_interdimensional/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9nerk/made_an_rick_and_morty_inspired_interdimensional/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T02:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bj5j</id>
    <title>I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)</title>
    <updated>2026-01-10T18:28:17+00:00</updated>
    <author>
      <name>/u/bullmeza</name>
      <uri>https://old.reddit.com/user/bullmeza</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"&gt; &lt;img alt="I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)" src="https://preview.redd.it/yrb4rq69ekcg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=22c6cdbfa015e9f0b501163a5ab174d6aa588f3f" title="I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built Screen Vision, an &lt;strong&gt;open source website&lt;/strong&gt; that guides you through any task by screen sharing with AI.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy Focused:&lt;/strong&gt; Your screen data is &lt;strong&gt;never&lt;/strong&gt; stored or used to train models. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local LLM Support:&lt;/strong&gt; If you don't trust cloud APIs, the app has a &amp;quot;Local Mode&amp;quot; that connects to local AI models running on your own machine. Your data never leaves your computer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web-Native:&lt;/strong&gt; No desktop app or extension required. Works directly on your browser.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Instruction &amp;amp; Grounding:&lt;/strong&gt; The system uses GPT-5.2 to determine the next logical step based on your goal and current screen state. These instructions are then passed to Qwen 3VL (30B), which identifies the exact screen coordinates for the action.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Verification:&lt;/strong&gt; The app monitors your screen for changes every 200ms using a pixel-comparison loop. Once a change is detected, it compares before and after snapshots using Gemini 3 Flash to confirm the step was completed successfully before automatically moving to the next task.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Source Code:&lt;/strong&gt; &lt;a href="https://github.com/bullmeza/screen.vision"&gt;https://github.com/bullmeza/screen.vision&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://screen.vision/"&gt;https://screen.vision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm looking for feedback, please let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullmeza"&gt; /u/bullmeza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yrb4rq69ekcg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T18:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9pe4l</id>
    <title>I built a benchmark measuring the Markdown quality of LLMs</title>
    <updated>2026-01-11T04:06:40+00:00</updated>
    <author>
      <name>/u/bengt0</name>
      <uri>https://old.reddit.com/user/bengt0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9pe4l/i_built_a_benchmark_measuring_the_markdown/"&gt; &lt;img alt="I built a benchmark measuring the Markdown quality of LLMs" src="https://preview.redd.it/toz75kg5ancg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74ecf9a9ef58952a70397f5b1fc4fc2a8a51d953" title="I built a benchmark measuring the Markdown quality of LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://lintbench.ai"&gt;https://lintbench.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bengt0"&gt; /u/bengt0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/toz75kg5ancg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9pe4l/i_built_a_benchmark_measuring_the_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9pe4l/i_built_a_benchmark_measuring_the_markdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T04:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9si66</id>
    <title>Looking for a Base Model</title>
    <updated>2026-01-11T06:49:17+00:00</updated>
    <author>
      <name>/u/AutomataManifold</name>
      <uri>https://old.reddit.com/user/AutomataManifold</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was putting together a finetuning dataset for an experiment and I realized that I have lost track of which models have base models available. I can search for models with &amp;quot;base&amp;quot; in the name and find stuff like &lt;a href="https://huggingface.co/Qwen/Qwen3-8B-Base"&gt;Qwen 3 8B base&lt;/a&gt; but I'm pretty sure that there are base models I'm overlooking. Do you have a favorite base model?&lt;/p&gt; &lt;p&gt;Models I've found so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 3 base, in 1B, &lt;a href="https://huggingface.co/Qwen/Qwen3-8B-Base"&gt;8B&lt;/a&gt;, 30B, 30B-A3B etc.&lt;/li&gt; &lt;li&gt;LiquidAI's LFM2.5 (&lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Base"&gt;1.2B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;DeepSeek-V3 (&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"&gt;671B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;DeepSeek-Coder-V2 (&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Base"&gt;236B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;NVIDIA Nemotron-3-Nano (&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16"&gt;30B-A3B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;NVIDIA Nemotron 3 (&lt;a href="https://huggingface.co/nvidia/nemotron-3-8b-base-4k"&gt;8B&lt;/a&gt;4k)&lt;/li&gt; &lt;li&gt;Nanbeige4 (&lt;a href="https://huggingface.co/Nanbeige/Nanbeige4-3B-Base"&gt;3B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Falcon H1 (&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Base"&gt;7B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;ByteDance's Seed-Coder (&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Base"&gt;8B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Llama 3.1 (&lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;8B&lt;/a&gt;, etc.)&lt;/li&gt; &lt;li&gt;SmolLLM v3 (&lt;a href="https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Base"&gt;3B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Kimi K2 (&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;1T-A32B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Kirim-V1-Base (&lt;a href="https://huggingface.co/Kirim-ai/Kirim-V1-Base"&gt;12B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;MiMo-V2-Flash-Base (&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash-Base"&gt;310B-A15B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Gumini (&lt;a href="https://huggingface.co/GuminiResearch/Gumini-1B-Base"&gt;1B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Kanana-2 (&lt;a href="https://huggingface.co/kakaocorp/kanana-2-30b-a3b-base"&gt;30B-3AB&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Gemma 3 (&lt;a href="https://huggingface.co/google/gemma-3-27b-pt"&gt;27B&lt;/a&gt;, 12B, 4B, 1B)&lt;/li&gt; &lt;li&gt;ByteDance Seed OSS (&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base"&gt;36B&lt;/a&gt; &lt;em&gt;w/ syn. and&lt;/em&gt; &lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base-woSyn"&gt;woSyn&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;zai-org's GLM 4 (&lt;a href="https://huggingface.co/zai-org/GLM-4-32B-Base-0414"&gt;32B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Skywork MoE (&lt;a href="https://huggingface.co/Skywork/Skywork-MoE-Base"&gt;146B-A16B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;IBM's Granite-4.0-Micro (&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-micro-base"&gt;3B&lt;/a&gt;, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm pretty sure I'm still missing lots of base models and lots of different sizes of some of these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AutomataManifold"&gt; /u/AutomataManifold &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9si66/looking_for_a_base_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9si66/looking_for_a_base_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9si66/looking_for_a_base_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T06:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q998is</id>
    <title>Visualizing RAG, PART 2- visualizing retrieval</title>
    <updated>2026-01-10T16:59:58+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"&gt; &lt;img alt="Visualizing RAG, PART 2- visualizing retrieval" src="https://external-preview.redd.it/d2kxeWY2ZzV6amNnMV4lBvCWgLP7SJvsjxaLS786SW2_ibHBrw3ehpZ9iEQq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950c6e4a869e18d9d6e90193c9adaaa7fb875e40" title="Visualizing RAG, PART 2- visualizing retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: code is live at &lt;a href="https://github.com/CyberMagician/Project_Golem"&gt;https://github.com/CyberMagician/Project_Golem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still editing the repository but basically just download the requirements (from requirements txt), run the python ingest to build out the brain you see here in LanceDB real quick, then launch the backend server and front end visualizer.&lt;/p&gt; &lt;p&gt;Using UMAP and some additional code to visualizing the 768D vector space of EmbeddingGemma:300m down to 3D and how the RAG ‚Äúthinks‚Äù when retrieving relevant context chunks. How many nodes get activated with each query. It is a follow up from my previous post that has a lot more detail in the comments there about how it‚Äôs done. Feel free to ask questions I‚Äôll answer when I‚Äôm free&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mrkuplj5zjcg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T16:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9u07d</id>
    <title>Which is the best model under 15B</title>
    <updated>2026-01-11T08:19:06+00:00</updated>
    <author>
      <name>/u/BothYou243</name>
      <uri>https://old.reddit.com/user/BothYou243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need a llm under 15B for agentic capabilities, reasoning, maths, general knowledge,&lt;br /&gt; making for raycast local model, i dont know hich model to select,&lt;br /&gt; ministral 3 14B, gemma 3 12B, qwen 3 14B, gpt-oss: 20B&lt;/p&gt; &lt;p&gt;gpt-oss thinks a lot, and inference is not usable.&lt;br /&gt; any recommendations?&lt;/p&gt; &lt;p&gt;any other model suggestions is all I want&lt;/p&gt; &lt;p&gt;what about Apriel-1.5-15B-Thinker&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BothYou243"&gt; /u/BothYou243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9u07d/which_is_the_best_model_under_15b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9u07d/which_is_the_best_model_under_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9u07d/which_is_the_best_model_under_15b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T08:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9xn78</id>
    <title>Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026</title>
    <updated>2026-01-11T12:00:23+00:00</updated>
    <author>
      <name>/u/GoodSamaritan333</name>
      <uri>https://old.reddit.com/user/GoodSamaritan333</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"&gt; &lt;img alt="Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026" src="https://external-preview.redd.it/9NT_b7vJLOJdi5qbccIw0AbUH9Ctzy98ZNJ7UkM8Ia8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea2ae08a8c43e60ca50bc40d967d25ca9d31b13f" title="Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GoodSamaritan333"&gt; /u/GoodSamaritan333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/345000/gigabyte-announces-support-for-256gb-of-ddr5-7200-cqdimms-at-ces-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T12:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9wc12</id>
    <title>GLM-4.7 Is this the new #1 open-source coding &amp; reasoning king in early 2026? Who's tried it?</title>
    <updated>2026-01-11T10:41:33+00:00</updated>
    <author>
      <name>/u/Impressive-Olive8372</name>
      <uri>https://old.reddit.com/user/Impressive-Olive8372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With 2026 just kicking off, Zhipu AI (aka &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;, the Chinese powerhouse often called &amp;quot;China's OpenAI&amp;quot;) dropped GLM-4.7 back in late December 2025, and it's generating a ton of buzz in the open-source AI scene.This is a ~358B MoE model (with ~200K context window, MIT license) that's posting seriously impressive numbers, especially in coding, agentic tasks, and complex reasoning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;73.8% on SWE-bench Verified (+ big jump from previous version)&lt;/li&gt; &lt;li&gt;66.7% on SWE-bench Multilingual&lt;/li&gt; &lt;li&gt;41% on Terminal-Bench 2.0 (huge +16.5% improvement!)&lt;/li&gt; &lt;li&gt;42.8% on Humanity‚Äôs Last Exam (HLE) ‚Äî a 38% leap over GLM-4.6, getting close to some closed frontier models&lt;/li&gt; &lt;li&gt;84.9% on LiveCodeBench v6&lt;/li&gt; &lt;li&gt;Top-tier math: 95.7% on AIME 2025&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What really stands out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preserved Thinking ‚Äî keeps decisions and reasoning consistent across super long sessions (great for multi-day projects)&lt;/li&gt; &lt;li&gt;Interleaved Thinking ‚Äî thinks before acting and self-corrects&lt;/li&gt; &lt;li&gt;Turn-level Thinking ‚Äî you control how deep it thinks per turn&lt;/li&gt; &lt;li&gt;Insane inference speed on Cerebras (~1,000 tokens/sec for coding, up to 1,700 TPS in some cases!) with price-performance reportedly ~10x better than Claude 4.5 Sonnet&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A lot of devs are saying it's now the strongest open-source option for real coding/agent workflows ‚Äî some are even switching from Claude/GPT in production because it's way cheaper while being super close in quality (especially for long, multilingual, or tool-heavy tasks).On Cerebras it's blazing fast ‚Äî frontier-level intelligence at real-time speeds.Have you guys tried GLM-4.7 yet?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does the real-world performance stack up (beyond just benchmarks)?&lt;/li&gt; &lt;li&gt;Better than Claude 4.5 Sonnet, DeepSeek, Qwen, or Kimi for your use cases?&lt;/li&gt; &lt;li&gt;Anyone running it locally/on reasonable hardware?&lt;/li&gt; &lt;li&gt;Thoughts on the new thinking modes or Cerebras speed?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Drop your experiences, comparisons, demos, or funny fails in the comments ‚Äî would love to hear! Links to official announcements or leaderboards welcome too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Olive8372"&gt; /u/Impressive-Olive8372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wc12/glm47_is_this_the_new_1_opensource_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wc12/glm47_is_this_the_new_1_opensource_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wc12/glm47_is_this_the_new_1_opensource_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T10:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9vtgz</id>
    <title>llama.cpp MLA KV cache support for KimiLinear-48B-A3B</title>
    <updated>2026-01-11T10:10:29+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I added backend agnostic support for KimiLinear.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q586jv/comment/nxz63pt/?context=1"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1q586jv/comment/nxz63pt/?context=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I noticed that the original author didn't implement support for MLA KV cache, so I read the DeepSeekV3 MLA kv cache PR to add the support to KimiLinear.&lt;/p&gt; &lt;p&gt;This reduces 1M tokens F16 KV cache usage from 140GB to 14.875GB. So now it is possible to run super long context locally with your low VRAM card.&lt;/p&gt; &lt;p&gt;To run it please re-download the GGUF from&lt;br /&gt; &lt;a href="https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;br /&gt; and compile the code with&lt;br /&gt; git clone &lt;a href="https://github.com/ymcki/llama.cpp"&gt;https://github.com/ymcki/llama.cpp&lt;/a&gt; --branch Kimi-Linear&lt;br /&gt; cd llama.cpp&lt;br /&gt; cmake -B build -DGGML_CUDA=ON&lt;br /&gt; cmake --build build --config Release -j 6&lt;/p&gt; &lt;p&gt;At some point, KimiLinear was the best performing open weight model at contextarena. But it has since been taken out for unknown reasons.&lt;br /&gt; &lt;a href="https://contextarena.ai/"&gt;https://contextarena.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please give it a try and tell me to see if it can serve your long context needs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T10:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9io50</id>
    <title>Model: cerebras/GLM-4.7-REAP-268B-A32B incoming!</title>
    <updated>2026-01-10T23:06:33+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can't wait!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cerebras/GLM-4.7-REAP-268B-A32B"&gt;https://huggingface.co/cerebras/GLM-4.7-REAP-268B-A32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9io50/model_cerebrasglm47reap268ba32b_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9io50/model_cerebrasglm47reap268ba32b_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9io50/model_cerebrasglm47reap268ba32b_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T23:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9t9op</id>
    <title>Announcing Kreuzberg v4 (Open Source)</title>
    <updated>2026-01-11T07:34:55+00:00</updated>
    <author>
      <name>/u/Eastern-Surround7763</name>
      <uri>https://old.reddit.com/user/Eastern-Surround7763</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Peeps,&lt;/p&gt; &lt;p&gt;I'm excited to announce &lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Kreuzberg&lt;/a&gt; v4.0.0.&lt;/p&gt; &lt;h1&gt;What is Kreuzberg:&lt;/h1&gt; &lt;p&gt;Kreuzberg is a document intelligence library that extracts structured data from 56+ formats, including PDFs, Office docs, HTML, emails, images and many more. Built for RAG/LLM pipelines with OCR, semantic chunking, embeddings, and metadata extraction.&lt;/p&gt; &lt;p&gt;The new v4 is a ground-up rewrite in Rust with a bindings for 9 other languages!&lt;/p&gt; &lt;h1&gt;What changed:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Rust core&lt;/strong&gt;: Significantly faster extraction and lower memory usage. No more Python GIL bottlenecks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pandoc is gone&lt;/strong&gt;: Native Rust parsers for all formats. One less system dependency to manage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10 language bindings&lt;/strong&gt;: Python, TypeScript/Node.js, Java, Go, C#, Ruby, PHP, Elixir, Rust, and WASM for browsers. Same API, same behavior, pick your stack.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plugin system&lt;/strong&gt;: Register custom document extractors, swap OCR backends (Tesseract, EasyOCR, PaddleOCR), add post-processors for cleaning/normalization, and hook in validators for content verification.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-ready&lt;/strong&gt;: REST API, MCP server, Docker images, async-first throughout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ML pipeline features&lt;/strong&gt;: ONNX embeddings on CPU (requires ONNX Runtime 1.22.x), streaming parsers for large docs, batch processing, byte-accurate offsets for chunking.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why polyglot matters:&lt;/h1&gt; &lt;p&gt;Document processing shouldn't force your language choice. Your Python ML pipeline, Go microservice, and TypeScript frontend can all use the same extraction engine with identical results. The Rust core is the single source of truth; bindings are thin wrappers that expose idiomatic APIs for each language.&lt;/p&gt; &lt;h1&gt;Why the Rust rewrite:&lt;/h1&gt; &lt;p&gt;The Python implementation hit a ceiling, and it also prevented us from offering the library in other languages. Rust gives us predictable performance, lower memory, and a clean path to multi-language support through FFI.&lt;/p&gt; &lt;h1&gt;Is Kreuzberg Open-Source?:&lt;/h1&gt; &lt;p&gt;Yes! Kreuzberg is MIT-licensed and will stay that way.&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kreuzberg.dev/"&gt;Read the Docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://discord.gg/38pF6qGpYD"&gt;Join our Discord Server&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Surround7763"&gt; /u/Eastern-Surround7763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9t9op/announcing_kreuzberg_v4_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9t9op/announcing_kreuzberg_v4_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9t9op/announcing_kreuzberg_v4_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T07:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9se4a</id>
    <title>Anyone successfully ran LTX2 GGUF Q4 model on 8vram, 16gb Ram potato PC?</title>
    <updated>2026-01-11T06:42:42+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9se4a/anyone_successfully_ran_ltx2_gguf_q4_model_on/"&gt; &lt;img alt="Anyone successfully ran LTX2 GGUF Q4 model on 8vram, 16gb Ram potato PC?" src="https://preview.redd.it/n3bssemz1ocg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c08fa84c56e65822b8d36ba274ecbd99b40f317f" title="Anyone successfully ran LTX2 GGUF Q4 model on 8vram, 16gb Ram potato PC?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3bssemz1ocg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9se4a/anyone_successfully_ran_ltx2_gguf_q4_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9se4a/anyone_successfully_ran_ltx2_gguf_q4_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T06:42:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
