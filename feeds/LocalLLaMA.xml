<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-25T21:48:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ofztdl</id>
    <title>Good open source offline text diff tool?</title>
    <updated>2025-10-25T19:07:04+00:00</updated>
    <author>
      <name>/u/RuiRdA</name>
      <uri>https://old.reddit.com/user/RuiRdA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The more use AI the more I find myself checking what changes the model made.&lt;/p&gt; &lt;p&gt;In Roo Code there is a diff feature built in which is great, but when use a regular chat model I and defaulting to opening &lt;a href="https://www.diffchecker.com/"&gt;https://www.diffchecker.com/&lt;/a&gt; and copy and pasting the previous and new versions of what ever text I am working on to see where the AI made changes.&lt;/p&gt; &lt;p&gt;Does anyone know of any open source tool I can install on my machine and get the same features as &lt;a href="https://www.diffchecker.com/"&gt;https://www.diffchecker.com/&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;I have my question and use case is clear.&lt;br /&gt; God bless you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RuiRdA"&gt; /u/RuiRdA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofztdl/good_open_source_offline_text_diff_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofztdl/good_open_source_offline_text_diff_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofztdl/good_open_source_offline_text_diff_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T19:07:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1og07f1</id>
    <title>Model with no exterior context.</title>
    <updated>2025-10-25T19:23:25+00:00</updated>
    <author>
      <name>/u/Kidblunder1</name>
      <uri>https://old.reddit.com/user/Kidblunder1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a model (or a way to make a model) with no existing knowledge other than language, that will only use the info I give it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kidblunder1"&gt; /u/Kidblunder1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og07f1/model_with_no_exterior_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og07f1/model_with_no_exterior_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og07f1/model_with_no_exterior_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T19:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1og0suf</id>
    <title>Anyone know how two daisy chained DGX sparks have been performing yet?</title>
    <updated>2025-10-25T19:47:47+00:00</updated>
    <author>
      <name>/u/Tired__Dev</name>
      <uri>https://old.reddit.com/user/Tired__Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It'd be nice to see some videos from some YouTube creators using different models and benchmarking. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tired__Dev"&gt; /u/Tired__Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og0suf/anyone_know_how_two_daisy_chained_dgx_sparks_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og0suf/anyone_know_how_two_daisy_chained_dgx_sparks_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og0suf/anyone_know_how_two_daisy_chained_dgx_sparks_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T19:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofb7mu</id>
    <title>4B fp16 or 8B q4?</title>
    <updated>2025-10-24T22:21:45+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"&gt; &lt;img alt="4B fp16 or 8B q4?" src="https://preview.redd.it/ukn1akunw4xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd900eb86aa1c749649781d6ebd92067020e1209" title="4B fp16 or 8B q4?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, &lt;/p&gt; &lt;p&gt;For my 8GB GPU schould I go for &lt;strong&gt;fp16 but 4B&lt;/strong&gt; or &lt;strong&gt;q4 version of 8B&lt;/strong&gt;? Any model you particularly want to recommend me? Requirement: basic ChatGPT replacement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ukn1akunw4xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofqyhc</id>
    <title>Is GLM 4.5 / 4.6 really sensitive to quantisation? Or is vLLM stupifying the models?</title>
    <updated>2025-10-25T13:00:59+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Edit: I am a nincompoop&lt;/h1&gt; &lt;p&gt;It was user error. It turns out I had a mistake in my docker-compose file where the model argument was not passed correctly. vLLM defaulted to Qwen//Qwen3-0.6B which, funnily enough, gives responses of lower quality. The correct models/quants are in line with my expectations. Sorry for the inconvenience.&lt;/p&gt; &lt;h2&gt;Original post&lt;/h2&gt; &lt;p&gt;I am trying to run GLM 4.6 or GLM 4.5 Air locally with vLLM. I've tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;zai-org/GLM-4.5-Air-FP8&lt;/li&gt; &lt;li&gt;vkerkez/GLM-4.6-REAP-266B-A32B-awq-sym&lt;/li&gt; &lt;li&gt;bullpoint/GLM-4.6-AWQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;but the answers just aren't as smart or deep as the full-fat deepinfra zai-org/GLM-4.6. I'm using OpenWebUI with the same parameters and system prompt.&lt;/p&gt; &lt;p&gt;For example, with prompt &amp;quot;Explain Girsanov's theorem&amp;quot; and my (long) system prompt,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vllm zai-org/GLM-4.5-Air-FP8&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Girsanov's theorem is a cornerstone in stochastic calculus and financial mathematics, particularly in the context of risk-neutral pricing. It states that if there exists a risk-free interest rate $ r $, then the discounted stock price can be transformed into a process with the same drift but a different volatility. Specifically, if $ X_t $ is a stochastic process with drift $ \mu $ and volatility $ \sigma $, then under the risk-free rate $ r $, the discounted process $ \frac{X_t}{r} $ follows a Gaussian distribution with mean $ \mu $ and variance $ \sigma^2 $. This transformation allows for the pricing of options by adjusting the volatility to account for the risk-free rate. The theorem is pivotal in proving the no-arbitrage principle and is widely used in options pricing to ensure that the expected value of a derivative is equal to its payoff, adjusted for the risk-free rate. The proof typically involves changing the measure to account for the risk-free rate, which may involve integrating over the Brownian motion under the risk-neutral measure. This ensures that the expected value of the discounted process remains unchanged, but its distribution is adjusted to reflect the risk-free interest rate. In summary, Girsanov's theorem enables the conversion of a stochastic process into one that is risk-neutral, allowing for more accurate pricing of financial instruments. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;deepinfra GLM 4.5 Air&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Girsanov’s theorem is a result in stochastic calculus that describes how the dynamics of a Brownian motion (or, more generally, a continuous local martingale) change when we pass to an equivalent probability measure. It is the analytical foundation for changing the drift of a stochastic differential equation by moving to a new measure, the central tool for constructing risk‑neutral measures in mathematical finance. -------------------------------------------------------------------- ### 1. Setting Let \((\Omega,\mathcal F,\{\mathcal F_t\}_{t\ge 0},\mathbb P)\) be a filtered probability space satisfying the usual conditions. Assume that \(W=(W_t)_{t\ge0}\) is an \(\mathcal F_t\)-standard Brownian motion under \(\mathbb P\). Let \(\theta=(\theta_t)_{t\ge0}\) be an \(\mathcal F_t\)-adapted process such that \[ \mathbb E_{\mathbb P}\!\left[\exp\!\Bigl(\frac12\int_0^T\theta_s^2\,ds\Bigr)\right] &amp;lt;\infty \qquad\text{for all } T\ge0 . \tag{1} \] Condition (1) is known as the **Novikov condition**; it guarantees that a certain stochastic exponential is a true martingale. -------------------------------------------------------------------- ### 2. Exponential martingale and new measure Define the **stochastic exponential** \[ Z_t := \exp\!\left(-\int_0^t \theta_s\, dW_s -\frac12\int_0^t \theta_s^2\, ds\right), \qquad t\ge0 . \] Under (1), \((Z_t)_{t\ge0}\) is a strictly positive martingale with \(Z_0=1\). Using \(Z_T\) as a Radon–Nikodym derivative we introduce a new probability measure \(\mathbb Q\) on \(\mathcal F_T\) by \[ \frac{d\mathbb Q}{d\mathbb P}\bigg|_{\mathcal F_T}=Z_T . \] The family \(\{\mathbb Q\}\) obtained in this way is equivalent to \(\mathbb P\) (i.e., no null set of \(\mathbb P\) is null under \(\mathbb Q\) and vice versa). -------------------------------------------------------------------- ### 3. Statement of the theorem Under the new measure \(\mathbb Q\) the process \[ \widetilde W_t := W_t + \int_0^t \theta_s\, ds, \qquad t\ge0 , \] is an \(\mathcal F_t\)-standard Brownian motion. Equivalently, \[ W_t = \widetilde W_t - \int_0^t \theta_s\, ds , \] so that the drift of \(W\) is shifted by \(-\theta\) when viewed under \(\mathbb Q\). -------------------------------------------------------------------- ### 4. Interpretation The theorem gives a concrete recipe for turning a Brownian motion with drift into a true Brownian motion by changing probability measures. In the language of differential equations, suppose a process \(X\) satisfies \[ dX_t = \mu_t\,dt + \sigma_t\,dW_t , \] where \(\mu_t,\sigma_t\) are adapted processes. Choose \(\theta_t = \sigma_t^{-1}\mu_t\) (assuming \(\sigma_t\neq0\)). Then the new measure \(\mathbb Q\) defined above eliminates the drift term: under \(\mathbb Q\), \[ dX_t = \sigma_t\, d\widetilde W_t , \] so \(X\) behaves like a Brownian motion scaled by \(\sigma\). This is precisely the device used in financial mathematics to construct an equivalent martingale measure (“risk‑neutral measure”) under which the discounted price of a tradable asset becomes a martingale. -------------------------------------------------------------------- ### 5. Extensions * **General local martingales**: The result extends to continuous local martingales that are not necessarily Brownian, provided the Novikov condition holds for the integrand against that martingale. * **Processes with jumps**: A Girsanov theorem for semimartingales with jumps requires an additional Radon–Nikodym derivative relating the jump measures of the two processes. * **Multidimensional setting**: If \(W\) is an \(n\)-dimensional Brownian motion and \(\theta\) an \(\mathbb R^n\)-valued adapted process satisfying a Novikov-type condition, the same exponential formula changes the drift vector and preserves the covariance matrix. -------------------------------------------------------------------- ### 6. Practical checklist * Verify that the integrand \(\theta_t\) is adapted and satisfies a Novikov–type integrability condition. * Compute the stochastic exponential \(Z_t\); ensure it is a martingale (this is the key check). * Define the new measure by \(d\mathbb Q = Z_T\, d\mathbb P\). * Conclude that shifted process \(\widetilde W_t = W_t + \int_0^t \theta_s\,ds\) is a Brownian motion under \(\mathbb Q\). -------------------------------------------------------------------- **Reference** The original proof appears in Girsanov (1960), “On transforming a certain class of stochastic processes by absolutely continuous change of measure”; see also standard texts such as Øksendal (2003) “Stochastic Differential Equations”, Chapter II, or Karatzas &amp;amp; Shreve (1991) “Brownian Motion and Stochastic Calculus”, Chapter IV. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This (qualitative) difference is repeatable and I notice it for the 4.6 quants as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqyhc/is_glm_45_46_really_sensitive_to_quantisation_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqyhc/is_glm_45_46_really_sensitive_to_quantisation_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqyhc/is_glm_45_46_really_sensitive_to_quantisation_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T13:00:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1og37oa</id>
    <title>Is there a leaderboard of current open source models?</title>
    <updated>2025-10-25T21:29:33+00:00</updated>
    <author>
      <name>/u/Few-Basil-1185</name>
      <uri>https://old.reddit.com/user/Few-Basil-1185</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I appologize if this is a question only I don't know the answer to!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Basil-1185"&gt; /u/Few-Basil-1185 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og37oa/is_there_a_leaderboard_of_current_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og37oa/is_there_a_leaderboard_of_current_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og37oa/is_there_a_leaderboard_of_current_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1og3cnt</id>
    <title>Llama.cpp model conversion guide</title>
    <updated>2025-10-25T21:35:28+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt; &lt;img alt="Llama.cpp model conversion guide" src="https://external-preview.redd.it/9H8ID2ho6Hmpcg_iEkUBl0dzrALfa1J8gFxkkoi6ojc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=960cfd58e78583a22cdad3922567cd461d36ac4b" title="Llama.cpp model conversion guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the open source community always benefits by having more people do stuff, I figured I would capitalize on my experiences with a few architectures I've done and add a guide for people who, like me, would like to gain practical experience by porting a model architecture.&lt;/p&gt; &lt;p&gt;Feel free to propose any topics / clarifications and ask any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16770"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofyfuh</id>
    <title>Can someone explain this PT-MoE please?</title>
    <updated>2025-10-25T18:11:11+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofyfuh/can_someone_explain_this_ptmoe_please/"&gt; &lt;img alt="Can someone explain this PT-MoE please?" src="https://external-preview.redd.it/D9a4f_PqnwnlUmJ4j6dOW1F7gG_5ht0c45xtUL8kxDY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfc6ba6d645111497729241672f6575f4961c54f" title="Can someone explain this PT-MoE please?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand what apple mean by this Parallel Track Mixture of Experts model architecture. I do understand the MoE part but what does the PT part mean?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://machinelearning.apple.com/research/apple-foundation-models-tech-report-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofyfuh/can_someone_explain_this_ptmoe_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofyfuh/can_someone_explain_this_ptmoe_please/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T18:11:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofggny</id>
    <title>Kimi k2 image generation</title>
    <updated>2025-10-25T02:38:13+00:00</updated>
    <author>
      <name>/u/Used-Nectarine5541</name>
      <uri>https://old.reddit.com/user/Used-Nectarine5541</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"&gt; &lt;img alt="Kimi k2 image generation" src="https://preview.redd.it/vimkvz5976xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=202c4cfa6cd438c6e2e103ec6ec094a5b596d49a" title="Kimi k2 image generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am so confused because I can’t find any information on Kimi k2 image generation abilities. When I asked Kimi to generate an image it said it couldn’t. But I’m having it code a tarot reading project and it’s generating all these images…when I asked about it Kimi still said it couldn’t generate images. What’s going on and how are these images being generated??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Used-Nectarine5541"&gt; /u/Used-Nectarine5541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vimkvz5976xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T02:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oextwc</id>
    <title>GLM-4.6-Air is not forgotten!</title>
    <updated>2025-10-24T13:31:12+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt; &lt;img alt="GLM-4.6-Air is not forgotten!" src="https://preview.redd.it/z5dduynua2xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b43f43a244e84de5bb07a0bc9e4c16127860c9a4" title="GLM-4.6-Air is not forgotten!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5dduynua2xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T13:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1of8xl2</id>
    <title>Apple Foundation is dumb</title>
    <updated>2025-10-24T20:45:00+00:00</updated>
    <author>
      <name>/u/PM_ME_UR_COFFEE_CUPS</name>
      <uri>https://old.reddit.com/user/PM_ME_UR_COFFEE_CUPS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"&gt; &lt;img alt="Apple Foundation is dumb" src="https://b.thumbs.redditmedia.com/YZ_noIfhIcr5Qu2dEET4ho0GRm1U2R0wUE1Mn8YjpWQ.jpg" title="Apple Foundation is dumb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the other poster, I’ve found Apple Foundational model to disapprove of lots of content. It’s too safe. Too corporate.&lt;/p&gt; &lt;p&gt;This is the most innocuous example I could come up with. Also attached proof that it even indirectly avoids the word. Google’s model gives me accurate info. &lt;/p&gt; &lt;p&gt;(FYI in case you are not in a region that has chiggers… they are little red bugs that bite you, no relation to a word that it rhymes with at all)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_UR_COFFEE_CUPS"&gt; /u/PM_ME_UR_COFFEE_CUPS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1of8xl2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T20:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1og26pg</id>
    <title>is MacBook Pro M1 good at working with local llm inference.</title>
    <updated>2025-10-25T20:45:42+00:00</updated>
    <author>
      <name>/u/RandRanger</name>
      <uri>https://old.reddit.com/user/RandRanger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m fairly new to LLMs, so my question may be a little bit silly. I’m choosing a laptop to run small local models (around 7B–12B parameters) and I’m torn between two options: MacBook Pro (M1 Pro cpu) — 16 GB RAM, 16 GB VRAM HP Victus (13th-gen i5, RTX 4050) — 16 GB RAM, 6 GB VRAM Which one would be better for local LLM inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandRanger"&gt; /u/RandRanger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og26pg/is_macbook_pro_m1_good_at_working_with_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og26pg/is_macbook_pro_m1_good_at_working_with_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og26pg/is_macbook_pro_m1_good_at_working_with_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T20:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oftfoe</id>
    <title>Single H100: best open-source model + deep thinking setup for reasoning?</title>
    <updated>2025-10-25T14:49:40+00:00</updated>
    <author>
      <name>/u/Accomplished_Back718</name>
      <uri>https://old.reddit.com/user/Accomplished_Back718</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I have access to a single H100 and want to run an open-source LLM with a multi-agent or “deep thinking” framework for hard math problems and proof generation (hoping to get better results than using just Gemini 2.5 pro).&lt;/p&gt; &lt;p&gt;Looking for advice on the best open-source model for mathematical or logical reasoning that fits on one H100 (80GB), and the most practical way to implement a deep-think or multi-agent workflow that supports decomposition, verification, using tools...&lt;/p&gt; &lt;p&gt;Would appreciate any concrete setups, frameworks, or model recommendations from people who’ve built local reasoning or proof systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Back718"&gt; /u/Accomplished_Back718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oftfoe/single_h100_best_opensource_model_deep_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oftfoe/single_h100_best_opensource_model_deep_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oftfoe/single_h100_best_opensource_model_deep_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T14:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1og1xkz</id>
    <title>Uncensored AI for scientific research without any filters, and can do very long tasks without bias and overfitting</title>
    <updated>2025-10-25T20:34:50+00:00</updated>
    <author>
      <name>/u/PrintCreepy8982</name>
      <uri>https://old.reddit.com/user/PrintCreepy8982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Uncensored AI for scientific research without any filters, and can do very long tasks without bias and overfitting &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrintCreepy8982"&gt; /u/PrintCreepy8982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og1xkz/uncensored_ai_for_scientific_research_without_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og1xkz/uncensored_ai_for_scientific_research_without_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og1xkz/uncensored_ai_for_scientific_research_without_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T20:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofasus</id>
    <title>MiniMax M2 is 230B-A10B</title>
    <updated>2025-10-24T22:03:31+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"&gt; &lt;img alt="MiniMax M2 is 230B-A10B" src="https://preview.redd.it/f45v1dx7u4xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c77e931b8bddbffa454a9542abfb528e2be6fc4" title="MiniMax M2 is 230B-A10B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f45v1dx7u4xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1og16cs</id>
    <title>[P] SpeechAlgo: Open-Source Speech Processing Library for Audio Pipelines</title>
    <updated>2025-10-25T20:03:05+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Released SpeechAlgo - a Python library for speech processing and audio feature extraction.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Package: pip install speechalgo&lt;/li&gt; &lt;li&gt;code: &lt;a href="https://github.com/tarun7r/SpeechAlgo"&gt;https://github.com/tarun7r/SpeechAlgo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Features: • MFCC, mel-spectrograms, and delta features for ML pipelines&lt;/p&gt; &lt;p&gt;• VAD, pitch detection, and speech enhancement&lt;/p&gt; &lt;p&gt;• 20 + algorithms with clean, type-annotated code&lt;/p&gt; &lt;p&gt;• Real-time capable, modular design Perfect for preprocessing audio data, building VAD systems, and feature extraction for speech recognition models.&lt;/p&gt; &lt;p&gt;Contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og16cs/p_speechalgo_opensource_speech_processing_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og16cs/p_speechalgo_opensource_speech_processing_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og16cs/p_speechalgo_opensource_speech_processing_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T20:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1og1z29</id>
    <title>FlashPack: High-throughput tensor loading for PyTorch</title>
    <updated>2025-10-25T20:36:37+00:00</updated>
    <author>
      <name>/u/SignificantStop1971</name>
      <uri>https://old.reddit.com/user/SignificantStop1971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FlashPack — a new, high-throughput file format and loading mechanism for PyTorch that makes model checkpoint I/O blazingly fast, even on systems without access to GPU Direct Storage (GDS).&lt;/p&gt; &lt;p&gt;With FlashPack, loading any model can be 3–6× faster than with the current state-of-the-art methods like &lt;code&gt;accelerate&lt;/code&gt; or the standard &lt;code&gt;load_state_dict()&lt;/code&gt; and &lt;code&gt;to()&lt;/code&gt; flow — all wrapped in a lightweight, pure-Python package that works anywhere. &lt;a href="https://github.com/fal-ai/flashpack"&gt;https://github.com/fal-ai/flashpack&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignificantStop1971"&gt; /u/SignificantStop1971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og1z29/flashpack_highthroughput_tensor_loading_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og1z29/flashpack_highthroughput_tensor_loading_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og1z29/flashpack_highthroughput_tensor_loading_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T20:36:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1og2k8e</id>
    <title>Who is using Granite 4? What's your use case?</title>
    <updated>2025-10-25T21:01:40+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been about 3 weeks since Granite 4 was released with base and instruct versions. If you're using it, what are you using it for? What made you choose it over (or alongside) others?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofq6m7</id>
    <title>which model has the best world knowledge? Open weights and proprietary.</title>
    <updated>2025-10-25T12:23:16+00:00</updated>
    <author>
      <name>/u/z_3454_pfk</name>
      <uri>https://old.reddit.com/user/z_3454_pfk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am looking for models with great general world knowledge and application of this. Open weights are preferred (I have access to H200s, so anything below 1.8TB VRAM) but API can be used if necessary. I am finding world knowledge really sucks for open models, even Kimi which can just get things wrong.&lt;/p&gt; &lt;p&gt;For example, knowing how much medication is wasted when you draw it up from a vial, based of the type of needle (since you get something called dead space - medication that stays in the tip o the syringe and needle). A lot of this is in nursing text books, so they know the content, but when asking models about it (such as Gemini flash) they really suck when it comes to applying this knowledge.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_3454_pfk"&gt; /u/z_3454_pfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq6m7/which_model_has_the_best_world_knowledge_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq6m7/which_model_has_the_best_world_knowledge_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq6m7/which_model_has_the_best_world_knowledge_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T12:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofxt6s</id>
    <title>Optimizing gpt-oss-120B on AMD RX 6900 XT 16GB: Achieving 19 tokens/sec</title>
    <updated>2025-10-25T17:46:16+00:00</updated>
    <author>
      <name>/u/Bright_Resolution_61</name>
      <uri>https://old.reddit.com/user/Bright_Resolution_61</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;## Introduction OpenAI's gpt-oss-120B is a massive 117B parameter language model, with official recommendations calling for datacenter-grade GPUs like the H100 or MI300X (80GB VRAM). This article documents the optimization journey to run this model at practical speeds (19 tokens/sec) on a consumer AMD RX 6900 XT with only 16GB VRAM. ## Hardware Configuration ### Main Components - **GPU**: AMD Radeon RX 6900 XT 16GB VRAM - Architecture: RDNA2 (gfx1030) - Memory Bandwidth: 512 GB/s - Stream Processors: 5120 - Released: December 2020 - **CPU**: AMD Ryzen 9 7900 (12-core/24-thread) - Base Clock: 3.7 GHz - Boost Clock: 5.4 GHz - Instruction Sets: AVX, AVX2, AVX-512 capable - L3 Cache: 64MB - Architecture: Zen 4 - **Memory**: 64GB (32GB × 2) DDR5-5600MHz - Dual-channel configuration - Memory Bandwidth: 89.6 GB/s (theoretical) - CAS Latency: CL46 (typical) - **Storage**: NVMe SSD recommended (60GB model files) ### Software Environment - **OS**: Ubuntu 24.04 LTS - **ROCm**: 6.2.4 - **llama.cpp**: Latest build (ROCm backend, AVX-512 enabled) - **Drivers**: Mesa 24.x + AMDGPU kernel driver ## Why This Hardware Configuration Matters ### Ryzen 9 7900's Advantages The 12-core/24-thread design with AVX-512 support accelerates MoE layer CPU processing significantly. AVX-512 in particular provides 15-30% performance gains for matrix operations in the CPU processing path, making it ideal for handling the 28 MoE layers offloaded from GPU. ### DDR5-5600MHz Impact The gpt-oss-120B's MoE architecture processes 28 layers on CPU/RAM. DDR5's high bandwidth (89.6 GB/s) enables rapid transfer of model weight data, reducing memory bottlenecks. This is approximately 40% faster than DDR4-3200, directly improving token generation speed. ### 64GB RAM Necessity - Model weights (MoE portion): ~50-55GB - System usage: 6-8GB - KV cache: 2-4GB - **Total**: ~58-67GB 64GB is the minimum viable configuration. For longer contexts (32K+), 128GB is recommended. System was observed using only 6GB with 57GB available, but full context windows consume more. ## Initial Challenge: The Crash Wall The first attempt with default settings resulted in immediate crashes with `ggml_cuda_error` termination. ```bash # Initial attempt (failed) ./llama-server -m gpt-oss-120b.gguf --n-gpu-layers 999 # → Aborted (core dumped) ``` With only 16GB VRAM against a 120B model, this seemed impossible. However, gpt-oss-120B uses a Mixture of Experts (MoE) architecture, activating only 5.1B parameters per token. This characteristic became the key to success. ## Breakthrough 1: Environment Variables and MoE Offloading Running RX 6900 XT with ROCm requires specific environment variables: ```bash export HSA_OVERRIDE_GFX_VERSION=10.3.0 export ROCM_PATH=/opt/rocm export HIP_VISIBLE_DEVICES=0 export GPU_MAX_HEAP_SIZE=100 export GPU_MAX_ALLOC_PERCENT=95 ``` The `HSA_OVERRIDE_GFX_VERSION=10.3.0` is critical for gfx1030 (RX 6900 XT) architecture recognition. The breakthrough came with the `--n-cpu-moe` parameter, which offloads MoE layers to CPU: ```bash ./llama-server \ -m gpt-oss-120b.gguf \ --n-gpu-layers 5 \ --n-cpu-moe 36 \ --ctx-size 4096 ``` **Result**: First successful boot, but slow at **11.63 tokens/sec**. ## Breakthrough 2: Progressive GPU Layer Increase Monitoring VRAM usage with `rocm-smi`, I progressively increased GPU layers: | GPU Layers | MoE Layers (CPU) | Speed | VRAM Usage | |------------|------------------|-------|------------| | 5 layers | 36 layers | 11.6 t/s | 52% | | 20 layers | 32 layers | 15.2 t/s | 70% | | 30 layers | 29 layers | 17.8 t/s | 85% | | 38 layers | 28 layers | **19.1 t/s** | 95% | | 40 layers | 28 layers | 19.4 t/s | **99%** | | 42 layers | 27 layers | OOM | - | 38 layers proved to be the optimal balance. While 40 layers works, increasing context length causes KV cache to overflow VRAM. ## Breakthrough 3: Enabling AVX-512 The initial build had **all CPU AVX instructions disabled**: ```bash # Check configuration cat CMakeCache.txt | grep GGML_AVX # GGML_AVX:BOOL=OFF ← Problem! ``` This meant using only 10-30% of CPU capabilities. Rebuilding fixed this: ```bash cd llama.cpp rm -rf build &amp;amp;&amp;amp; mkdir build &amp;amp;&amp;amp; cd build cmake .. \ -DGGML_HIP=ON \ -DCMAKE_BUILD_TYPE=Release \ -DGGML_NATIVE=ON # ← Auto-detect optimizations cmake --build . --config Release -j$(nproc) ``` **Result**: AVX, AVX2, and AVX512 all enabled, significantly accelerating MoE layer CPU processing. ## Final Configuration The stable configuration: ```bash export HSA_OVERRIDE_GFX_VERSION=10.3.0 export ROCM_PATH=/opt/rocm export HIP_VISIBLE_DEVICES=0 export GPU_MAX_HEAP_SIZE=100 export GPU_MAX_ALLOC_PERCENT=95 ./llama-server \ -m gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-gpu-layers 38 \ --n-cpu-moe 28 \ --ctx-size 24576 \ --batch-size 2048 \ --ubatch-size 512 \ --threads 12 \ --jinja \ --host 0.0.0.0 \ --port 8080 ``` ### Parameter Explanation - `--n-gpu-layers 38`: GPU processing layers (95% VRAM utilization) - `--n-cpu-moe 28`: Number of MoE layers processed on CPU - `--ctx-size 24576`: Context length (24K tokens) - `--batch-size 2048`: Batch size (processing efficiency) - `--threads 12`: Physical core count (12 cores) ## Performance Results ``` Prompt processing: 93-291 tokens/sec (with caching) Generation speed: 19.14 tokens/sec VRAM usage: 95% CPU usage: 47% ``` ## llama.cpp vs Ollama I used llama.cpp, but the differences with Ollama are clear: **llama.cpp**: - ✅ Fine-grained tuning possible - ✅ Extract maximum hardware performance - ❌ Complex configuration **Ollama**: - ✅ One-command startup - ✅ Beginner-friendly - ❌ Auto-settings achieve ~80% performance (10-12 t/s estimated) For specialized environments like AMD, llama.cpp's flexibility was essential. ## Troubleshooting ### Flash Attention Errors ```bash # Solution: Disable Flash Attention Remove --flash-attn parameter ``` ### OOM (Out of Memory) ```bash # Solution: Reduce GPU layers by 1-2 --n-gpu-layers 38 → 36 ``` ### Extremely Slow Performance ```bash # Check AVX instructions cat build/CMakeCache.txt | grep GGML_AVX # If all OFF, rebuild with optimizations ``` ## Key Learnings ### 1. AMD ROCm Challenges - Requires manual environment variable configuration - gfx architecture overrides necessary - Flash Attention often unstable - Less mature than CUDA ecosystem ### 2. MoE Architecture Advantages - 120B model activates only 5.1B parameters - Enables running on consumer hardware - CPU offloading is practical and effective ### 3. Progressive Optimization Works - Start conservative (low GPU layers) - Monitor VRAM with rocm-smi - Increment gradually - Find stability threshold ### 4. CPU Optimization Matters - AVX-512 provides 15-30% speedup for MoE - Physical core count optimal for threading - Memory bandwidth becomes bottleneck ## Theoretical Limits Reached At 19 tokens/sec with 95% VRAM usage, we've essentially hit the hardware ceiling. Further improvements would require: 1. **More VRAM**: Reduce MoE CPU offloading 2. **Faster Memory**: DDR5 (up to 6400MHz) 3. **Better GPU**: RDNA3 (RX 7900 series) or NVIDIA ## Conclusions Successfully running gpt-oss-120B at 19 t/s on AMD RX 6900 XT 16GB demonstrates that: 1. **Cost-Effectiveness**: $300-400 used GPU runs 120B models practically 2. **Learning Value**: Deep understanding of GPU architecture and memory management 3. **Practicality**: 19 t/s suffices for code completion and chat applications The greatest lesson: **Understand hardware limits and optimize progressively**. Perfect configuration doesn't appear instantly. Using monitoring tools (rocm-smi, htop) while adjusting parameters one-by-one requires patience. The fine‑tuning of this article was performed using gpt‑oss-120B. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bright_Resolution_61"&gt; /u/Bright_Resolution_61 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofxt6s/optimizing_gptoss120b_on_amd_rx_6900_xt_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofxt6s/optimizing_gptoss120b_on_amd_rx_6900_xt_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofxt6s/optimizing_gptoss120b_on_amd_rx_6900_xt_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T17:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofyfsb</id>
    <title>I built "Gemma Web": A fully private, in-browser AI workspace that runs 100% offline via WASM. Would love your feedback!</title>
    <updated>2025-10-25T18:11:08+00:00</updated>
    <author>
      <name>/u/Abject_Comfort_5220</name>
      <uri>https://old.reddit.com/user/Abject_Comfort_5220</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on a project that I think this community will really appreciate. It’s called Gemma Web, and it's a completely serverless, private, and offline-first AI workspace that runs Google's Gemma models entirely in your browser.&lt;/p&gt; &lt;p&gt;Live Demo: &lt;a href="https://www.gemmaweb.app/"&gt;https://www.gemmaweb.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The main goal was to create a zero-latency, 100% private AI assistant where no data ever leaves your device.&lt;/p&gt; &lt;p&gt;Here are the key features:&lt;/p&gt; &lt;p&gt;100% On-Device &amp;amp; Private: It uses the MediaPipe LLM Task API to run Gemma models directly in the browser using WebAssembly. All processing happens on your machine.&lt;/p&gt; &lt;p&gt;Full Offline Functionality: The app is an installable PWA. It uses IndexedDB to cache the AI models, conversation histories, and document embeddings, so it works perfectly without an internet connection after the first load.&lt;/p&gt; &lt;p&gt;Local RAG (Retrieval-Augmented Generation): This is the part I'm most excited about. You can upload your own PDF and TXT documents. The app uses a Web Worker to process them in the background, create vector embeddings using TensorFlow.js (Universal Sentence Encoder), and save them locally. You can then have context-aware conversations with your own documents, completely privately.&lt;/p&gt; &lt;p&gt;Feature-Rich UI: I also integrated the Web Speech API for voice-to-text input and text-to-speech output, plus an incognito mode for ephemeral chats.&lt;/p&gt; &lt;p&gt;It’s built for anyone who wants the power of a modern LLM without sacrificing privacy or relying on a server connection.&lt;/p&gt; &lt;p&gt;I’m really looking to improve it, so I’d love to get your feedback!&lt;/p&gt; &lt;p&gt;What do you think of the concept?&lt;/p&gt; &lt;p&gt;Are there any features you'd like to see added?&lt;/p&gt; &lt;p&gt;Did you run into any bugs or issues?&lt;/p&gt; &lt;p&gt;Let me know your thoughts in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject_Comfort_5220"&gt; /u/Abject_Comfort_5220 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofyfsb/i_built_gemma_web_a_fully_private_inbrowser_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofyfsb/i_built_gemma_web_a_fully_private_inbrowser_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofyfsb/i_built_gemma_web_a_fully_private_inbrowser_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T18:11:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofn0nb</id>
    <title>meituan-longcat/LongCat-Video · Hugging Face</title>
    <updated>2025-10-25T09:11:57+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"&gt; &lt;img alt="meituan-longcat/LongCat-Video · Hugging Face" src="https://external-preview.redd.it/Yt-ii7zJ14OIVDQLAiu4mswoqHj6Da2dRjpAXgOm1a4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a68e500343ff77272afd8ad706e2ac971ab2d083" title="meituan-longcat/LongCat-Video · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A foundational video generation model with 13.6B parameters, delivering strong performance across Text-to-Video, Image-to-Video, and Video-Continuation generation tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T09:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1of5ywl</id>
    <title>What’s even the goddamn point?</title>
    <updated>2025-10-24T18:47:20+00:00</updated>
    <author>
      <name>/u/ChockyBlox</name>
      <uri>https://old.reddit.com/user/ChockyBlox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt; &lt;img alt="What’s even the goddamn point?" src="https://preview.redd.it/9fjtexb9v3xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d398d419e5a3d539c3f2c82b07408ef22f90899" title="What’s even the goddamn point?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To be fair I will probably never use this model for any real use cases, but these corporations do need to go a little easy on the restrictions and be less paranoid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChockyBlox"&gt; /u/ChockyBlox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9fjtexb9v3xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T18:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofrbcy</id>
    <title>VSORA Launches Europe’s Most Powerful AI Inference Chip</title>
    <updated>2025-10-25T13:17:00+00:00</updated>
    <author>
      <name>/u/RG54415</name>
      <uri>https://old.reddit.com/user/RG54415</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"&gt; &lt;img alt="VSORA Launches Europe’s Most Powerful AI Inference Chip" src="https://external-preview.redd.it/yZMJwzqfo-1KodiPMX_oqs-WZlqWdJqQO_vA61_E2Yg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9bde9afeea2ecc09d17d3b9a496407c3e11f9c3" title="VSORA Launches Europe’s Most Powerful AI Inference Chip" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of its features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully programmable&lt;/li&gt; &lt;li&gt;Algorithm agnostic&lt;/li&gt; &lt;li&gt;Host processor agnostic&lt;/li&gt; &lt;li&gt;RISC-V cores to offload host &amp;amp; run AI completely on-chip&lt;/li&gt; &lt;li&gt;Tensorcore (dense) &lt;ul&gt; &lt;li&gt;fp8: 3200 Tflops &lt;/li&gt; &lt;li&gt;fp16: 800 Tflops&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;General Purpose &lt;ul&gt; &lt;li&gt;fp8/int8: 100 Tflops&lt;/li&gt; &lt;li&gt;fp16/int16: 50 Tflops&lt;/li&gt; &lt;li&gt;fp32/int32: 25 Tflops&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Capacity HBM: 288GB&lt;/li&gt; &lt;li&gt;Throughput HBM: 8 TB/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Seems like a big win for local AI models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RG54415"&gt; /u/RG54415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://finance.yahoo.com/news/vsora-launches-europe-most-powerful-121700744.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T13:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofu15a</id>
    <title>I rebuilt DeepSeek’s OCR model in Rust so anyone can run it locally (no Python!)</title>
    <updated>2025-10-25T15:13:52+00:00</updated>
    <author>
      <name>/u/Outrageous-Voice</name>
      <uri>https://old.reddit.com/user/Outrageous-Voice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! After wrestling with the original DeepSeek-OCR release (Python + Transformers, tons of dependencies, zero UX), I decided to port the whole inference stack to Rust. The repo is deepseek-ocr.rs (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt;) and it ships both a&lt;/p&gt; &lt;p&gt;CLI and an OpenAI-compatible server so you can drop it straight into existing clients like Open WebUI.&lt;/p&gt; &lt;h1&gt;Why bother?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt; No Python, no conda—just a single Rust binary.&lt;/li&gt; &lt;li&gt; Works offline and keeps documents private.&lt;/li&gt; &lt;li&gt; Fully OpenAI-compatible, so existing SDKs/ChatGPT-style UIs “just work”.&lt;/li&gt; &lt;li&gt; Apple Silicon support with optional Metal acceleration (FP16).&lt;/li&gt; &lt;li&gt; Built-in Hugging Face downloader: config/tokenizer/weights (≈6.3 GB) fetch automatically; needs about 13 GB RAM to run.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What’s inside the Rust port?&lt;/h1&gt; &lt;p&gt;- Candle-based reimplementation of the language model (DeepSeek-V2) with KV caches + optional FlashAttention.&lt;/p&gt; &lt;p&gt;- Full SAM + CLIP vision pipeline, image tiling, projector, and tokenizer alignment identical to the PyTorch release.&lt;/p&gt; &lt;p&gt;- Rocket server that exposes /v1/responses and /v1/chat/completions (OpenAI-compatible streaming included).&lt;/p&gt; &lt;p&gt;- Single-turn prompt compaction so OCR doesn’t get poisoned by multi-turn history.&lt;/p&gt; &lt;p&gt;- Debug hooks to compare intermediate tensors against the official model (parity is already very close).&lt;/p&gt; &lt;h1&gt;Getting started&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;You can download prebuilt archives (macOS with Metal, Windows) from the latest successful run of the repo’s GitHub Actions “build-binaries (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml)%E2%80%9D"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml)”&lt;/a&gt; workflow—no local build required.&lt;/li&gt; &lt;li&gt;Prefer compiling? git clone &lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt; → cargo fetch&lt;/li&gt; &lt;li&gt; CLI: cargo run -p deepseek-ocr-cli -- --prompt &amp;quot;&amp;lt;image&amp;gt;...&amp;quot; --image mydoc.png&lt;/li&gt; &lt;li&gt;Server: cargo run -p deepseek-ocr-server -- --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --port 8000&lt;/li&gt; &lt;li&gt;On macOS, add --features metal plus --device metal --dtype f16 for GPU acceleration.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Use cases&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Batch document conversion (receipts → markdown, contracts → summaries, etc.).&lt;/li&gt; &lt;li&gt;Plugging into Open WebUI (looks/feels like ChatGPT but runs YOUR OCR model).&lt;/li&gt; &lt;li&gt;&lt;p&gt;Building document QA bots that need faithful extraction.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you try it, I’d love to hear your feedback—feature requests, edge cases, performance reports, all welcome. And if it saves you from Python dependency hell, toss the repo a ⭐️.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Voice"&gt; /u/Outrageous-Voice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T15:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
