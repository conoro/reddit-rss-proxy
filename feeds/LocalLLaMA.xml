<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-08T16:39:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o1cti5</id>
    <title>what's the best live translation app for voice right now?</title>
    <updated>2025-10-08T14:56:57+00:00</updated>
    <author>
      <name>/u/fisch0920</name>
      <uri>https://old.reddit.com/user/fisch0920</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;eg:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;can use from my phone&lt;/li&gt; &lt;li&gt;easy to use&lt;/li&gt; &lt;li&gt;set 2 languages, one person speaks and when they're done, the app speaks the translation, then switch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;i don't need it to be real-time, just good quality, and i don't want to use custom hardware / models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fisch0920"&gt; /u/fisch0920 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1cti5/whats_the_best_live_translation_app_for_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1cti5/whats_the_best_live_translation_app_for_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1cti5/whats_the_best_live_translation_app_for_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T14:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ypna</id>
    <title>Fixing Apriel-1.5‑15B‑Thinker in Open WebUI: clean final answer + native "Thinking" panel - shareable filter</title>
    <updated>2025-10-08T02:30:22+00:00</updated>
    <author>
      <name>/u/_Bartek</name>
      <uri>https://old.reddit.com/user/_Bartek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ypna/fixing_apriel1515bthinker_in_open_webui_clean/"&gt; &lt;img alt="Fixing Apriel-1.5‑15B‑Thinker in Open WebUI: clean final answer + native &amp;quot;Thinking&amp;quot; panel - shareable filter" src="https://external-preview.redd.it/cWo3NjhvM2J1c3RmMW3zfa4MsPKiPRciHsaAQJ1NJDXy-jgFZs4wNTV6PLeF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9b1f41112dd7e02b30df6eaeb98a39b1fdc8864" title="Fixing Apriel-1.5‑15B‑Thinker in Open WebUI: clean final answer + native &amp;quot;Thinking&amp;quot; panel - shareable filter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;if you’ve tried Apriel‑1.5‑15B‑Thinker in Open WebUI, you probably noticed it prints a big “Here are my reasoning steps:” section before the final answer, which is wrapped in:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[BEGIN FINAL RESPONSE] ...final text... [END FINAL RESPONSE] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is by design (it’s in the model’s chat template), but in Open WebUI it clutters the chat and sometimes even leaves a trailing &amp;quot;[END FINAL RESPONSE&amp;quot; when stop sequences cut the stream mid‑marker.&lt;/p&gt; &lt;p&gt;I put together a small Open WebUI Filter that makes Apriel play nicely with the UI:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;shows a native Thinking panel (&amp;lt;think&amp;gt;…&amp;lt;/think&amp;gt;) for the pre‑final phase,&lt;/li&gt; &lt;li&gt;streams only the final content between [BEGIN…] and [END…],&lt;/li&gt; &lt;li&gt;and avoids the partial [END FINAL RESPONSE artifact.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve published it in the Open WebUI Functions directory: Apriel‑1.5‑15B‑Thinker - Final+Think.&lt;/p&gt; &lt;p&gt;Get it here: &lt;a href="https://openwebui.com/f/supczinskib/apriel_1_5_15b_thinker_final_think"&gt;https://openwebui.com/f/supczinskib/apriel_1_5_15b_thinker_final_think&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope this helps anyone running Apriel in OWUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Bartek"&gt; /u/_Bartek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/29rydn3bustf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ypna/fixing_apriel1515bthinker_in_open_webui_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ypna/fixing_apriel1515bthinker_in_open_webui_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T02:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1e0hq</id>
    <title>Required Reading for Llama.cpp Flags?</title>
    <updated>2025-10-08T15:40:27+00:00</updated>
    <author>
      <name>/u/Infamous_Jaguar_2151</name>
      <uri>https://old.reddit.com/user/Infamous_Jaguar_2151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to level up on llama.cpp and keep running into fragmented docs. There are a lot of startup flags (and some env-var twins), and some tuning seems hardware-specific (EPYC CPUs, multi-GPU splits, Flash-Attention, NUMA, etc.).&lt;/p&gt; &lt;p&gt;Two asks:&lt;/p&gt; &lt;p&gt;1). Best resources that actually explain the flags. Links welcome to any of these, with a note on why you like them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Official docs/pages (CLI/server/tools), manpages, source files that define the args, curated guides, blog posts, or wikis.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Hardware-specific writeups (EPYC/NUMA, CUDA vs HIP vs Metal vs Vulkan, multi-GPU split strategies).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;“Gotchas” posts (batch vs ubatch, RoPE scaling, KV-cache formats, mlock/mmap, etc.).&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;2). After it’s running: which settings are flags vs per-request?&lt;/p&gt; &lt;p&gt;For generation parameters and server behavior (e.g., temperature, top_p/top_k/min_p, repetition penalty, Mirostat, grammar/JSON schema, draft/speculative decoding, context/rope scaling, concurrency limits), which of these:&lt;/p&gt; &lt;p&gt;A. must be set at startup via flags or env vars,&lt;/p&gt; &lt;p&gt;B. can be changed per request (e.g., HTTP/JSON to llama-server), and&lt;/p&gt; &lt;p&gt;C. can be changed interactively without a restart (if at all)?&lt;/p&gt; &lt;p&gt;If you share your own working presets, please include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware (CPU model + cores/NUMA, GPU model/VRAM, RAM).&lt;/li&gt; &lt;li&gt;Backend (CUDA/HIP/Metal/Vulkan/OpenCL/SYCL) and your build options.&lt;/li&gt; &lt;li&gt;llama.cpp version/commit and the model + context length.&lt;/li&gt; &lt;li&gt;Your key flags (threads, batch/ubatch, n-gpu-layers, split-mode, rope-scaling, cache types, mlock/mmap, etc.) and why you chose them.&lt;/li&gt; &lt;li&gt;Before/after tokens/sec or latency numbers if you have them.&lt;/li&gt; &lt;li&gt;A link to any reference you leaned on.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infamous_Jaguar_2151"&gt; /u/Infamous_Jaguar_2151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e0hq/required_reading_for_llamacpp_flags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e0hq/required_reading_for_llamacpp_flags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e0hq/required_reading_for_llamacpp_flags/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o19sjw</id>
    <title>Can't decide between AI Max 395 and dedicated GPU setup for technical writing</title>
    <updated>2025-10-08T12:59:10+00:00</updated>
    <author>
      <name>/u/techreclaimer</name>
      <uri>https://old.reddit.com/user/techreclaimer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've read a lot about this topic already on this sub, but I still can't decide. Maybe you guys help me decide based on my use case. I work in a security domain with sensitive documents, which means cloud LLMs are out of the questions. So ideally a local setup would allow me to drop a 50-100 page document as context into a reasonable performing LLM and query it for different topics. I'm not interested in any video/image/speech generation. Which is why ended up taking a look at the new AI 395 mini PCs, although I see complaints that the prompt processing and t/s is not even close to a dedicated GPU setup, which is quite important to me. Ideally I would build myself a setup where the document is constantly queried based on what I am currently writing in a separate document. However, I assume that a reasonably performing model will already be enough to fill a 3090/4090/5090 without even considering context, so one probably won't be enough... Any help would be greatly appreciated, because I keep going back and forth on this for 2 weeks now :D My budget can go up to 4-5k.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techreclaimer"&gt; /u/techreclaimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19sjw/cant_decide_between_ai_max_395_and_dedicated_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19sjw/cant_decide_between_ai_max_395_and_dedicated_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o19sjw/cant_decide_between_ai_max_395_and_dedicated_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0puzj</id>
    <title>Samsung Paper Reveals a Recursive Technique that Beats Gemini 2.5 Pro on ARC-AGI with 0.01% of the Parameters!</title>
    <updated>2025-10-07T20:13:48+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.04871"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0puzj/samsung_paper_reveals_a_recursive_technique_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0puzj/samsung_paper_reveals_a_recursive_technique_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T20:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o150k1</id>
    <title>Just finished a fun open source project, a full stack system that fetches RSS feeds, uses an AI agent pipeline to write new articles, and automatically serves them through a Next.js site all done locally with Ollama and ChromaDB.</title>
    <updated>2025-10-08T08:36:57+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a project called AutoBlog that runs entirely on my local computer and uses a fully agentic setup to generate new blog posts grounded in my own data. It can ingest any files I choose, text documents, PDFs, or notes, and store them as embeddings in a local ChromaDB vector database. This database acts as the system’s knowledge base. Every piece of text I add becomes part of its contextual memory, so when the model generates new writing, it is informed by that material instead of relying on an external API or remote data source.&lt;/p&gt; &lt;p&gt;The core of the system is a group of coordinated agents that interact through a retrieval and generation loop. A researcher agent retrieves relevant context from the vector database, a writer agent synthesizes that information into a coherent draft, and an editor agent refines the result into a final piece of writing. All inference is done locally through Ollama, so each agent’s reasoning and communication happen within the boundaries of my own machine.&lt;/p&gt; &lt;p&gt;The system can also ingest external information through RSS feeds. These feeds are listed in a YAML configuration file, and the fetcher component parses and embeds their contents into the same vector store. This allows the model to combine current information from the web with my personal archive of documents, creating a grounded context for generation.&lt;/p&gt; &lt;p&gt;When the agents finish a cycle, they output a markdown file with frontmatter including title, date, tags, and a short description. A Next.js frontend automatically turns these files into a working blog. Each post reflects a blend of retrieved knowledge, reasoning across sources, and stylistic refinement from the multi-agent pipeline.&lt;/p&gt; &lt;p&gt;Everything about AutoBlog happens locally: retrieval, inference, vector storage, and rendering. It is built as a self-contained ecosystem that can think and write using whatever knowledge I choose to feed it. By grounding generation in my own material and letting specialized agents collaborate to research, write, and edit, it becomes an autonomous but controlled writer that evolves based on the data I provide.&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/kliewerdaniel/autoblog01"&gt;https://github.com/kliewerdaniel/autoblog01&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o150k1/just_finished_a_fun_open_source_project_a_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o150k1/just_finished_a_fun_open_source_project_a_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o150k1/just_finished_a_fun_open_source_project_a_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T08:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o13rfk</id>
    <title>Is Gemini 2.5 Pro still the best LLM for OCR and data extraction?</title>
    <updated>2025-10-08T07:14:02+00:00</updated>
    <author>
      <name>/u/kitgary</name>
      <uri>https://old.reddit.com/user/kitgary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My usecase is to extract data and format to JSON structured data from over a million image receipts, I am researching the best way to do it, it's not simple paper receipts, they are app photos taken directly by phone camera. so traditional OCR has a lot of noise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kitgary"&gt; /u/kitgary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13rfk/is_gemini_25_pro_still_the_best_llm_for_ocr_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13rfk/is_gemini_25_pro_still_the_best_llm_for_ocr_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o13rfk/is_gemini_25_pro_still_the_best_llm_for_ocr_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T07:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0st2o</id>
    <title>BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2 is possibly just a copy of Qwen's regular Qwen3-Coder-30B-A3B-Instruct</title>
    <updated>2025-10-07T22:05:06+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was brought up in &lt;a href="https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/discussions/1"&gt;https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/discussions/1&lt;/a&gt; and please note the &lt;em&gt;possibly&lt;/em&gt; I use in my language since unverified claims like this can be pretty damning.&lt;/p&gt; &lt;p&gt;Not sure if it's true or not, but one user seems to be convinced by their tests that the models are identical. Maybe someone smarter than me can look into this and verify this&lt;/p&gt; &lt;p&gt;EDIT - Yup. I think at this point it's pretty conclusive that this guy doesnt know what he's doing and vibe coded his way here. The models all have identical weights to the parent models. All of his distils.&lt;/p&gt; &lt;p&gt;Also, let's pay respects to anon user (not so anon if you just visit the thread to see who it is) from the discussion thread that claimed he was very picky and that we could trust him that the model was better:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/BasedBase"&gt;u/BasedBase&lt;/a&gt; feel free to add me to the list of satisfied customers lol. Your 480B coder distill in the small 30B package is something else and you guys can trust me I am VERY picky when it comes to output quality. I have no mercy for bad quality models and this one is certainly an improvement over the regular 30B coder. I've tested both thoroughly.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T22:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1eb9d</id>
    <title>Best OnDevice LocalLLM models for mobile against Nano Banana?</title>
    <updated>2025-10-08T15:51:15+00:00</updated>
    <author>
      <name>/u/Prashant_4200</name>
      <uri>https://old.reddit.com/user/Prashant_4200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title say is their any on device model available which can allow to play with images like fill the colour, replace text or the other basic task which we expect from basic photo (or similar to Samsung AI). &lt;/p&gt; &lt;p&gt;I need a model (it better if Uncensored) for my mobile application where with in the application i can able to perform basic image operation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant_4200"&gt; /u/Prashant_4200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eb9d/best_ondevice_localllm_models_for_mobile_against/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eb9d/best_ondevice_localllm_models_for_mobile_against/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eb9d/best_ondevice_localllm_models_for_mobile_against/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0vjxr</id>
    <title>There isn’t a single AI Agent on the market that can give you a day of work</title>
    <updated>2025-10-08T00:02:04+00:00</updated>
    <author>
      <name>/u/Working-Magician-823</name>
      <uri>https://old.reddit.com/user/Working-Magician-823</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use AI Agents all day, and some of them can do very good work, but, none of them can complete a large task by themselves without human intervention. None of them can spend a full day of work, even if you give detailed requirements.&lt;/p&gt; &lt;p&gt;If AI Agents can’t do a full software without a human yet, it is unlikely they are ready to be fully adopted by any business.&lt;/p&gt; &lt;p&gt;Smarter AI is coming for sure, just not what we have today&lt;/p&gt; &lt;p&gt;And a PHD level human, or bachelor’s degree, can complete a product, but I keep hearing AI is PHD level, well!!! It is smart but unable to do the full work that is not PHD..ish&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working-Magician-823"&gt; /u/Working-Magician-823 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T00:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o18p7i</id>
    <title>Replacing Google Translate with LLM translation app on smartphone?</title>
    <updated>2025-10-08T12:11:15+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I chat with a thai person semi-often on my phone, in thai. I use Google Translate to convert between Thai and English. &lt;/p&gt; &lt;p&gt;According to them, the thai text I send them is confusing and sometimes completely incomprehensible. And on my own end, often the english translation of what they send me is very unclear.&lt;/p&gt; &lt;p&gt;Today I tried something new: I tried an LLM on OpenRouter for the translation. They immediately understood, and said my messages were leaps and bounds better than Google Translate.&lt;/p&gt; &lt;p&gt;So now I'm looking to replace Google Translate on my Android phone. I'd really like the convenience of a dedicated translation and don't want to be using a browser and typing &amp;quot;Translate the following to/from $lang: my message here&amp;quot; and having to edit the prompt every time.&lt;/p&gt; &lt;p&gt;However, surprisingly I can't find any LLM translation apps on the Play Store or on F-Droid or in this sub's search. Through more googling I finally managed to find one chinese app, but it only supports OpenAI.&lt;/p&gt; &lt;p&gt;Surely I'm not the first person to hit a wall with Google Translate and wants LLM translation on their phone? Are there any obscure apps for this on Github which I can't find?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:11:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o18yl8</id>
    <title>Building a BPE Tokenizer from scratch - optimizations &amp; experiments</title>
    <updated>2025-10-08T12:23:04+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt; &lt;img alt="Building a BPE Tokenizer from scratch - optimizations &amp;amp; experiments" src="https://external-preview.redd.it/NeXI5jnwEOwXIs7_mVe04Ef6iuVmmMHfc9xat36QzrU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1608f9c8291d391bb636cc82c05cdc14a3ead6b" title="Building a BPE Tokenizer from scratch - optimizations &amp;amp; experiments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like I did in the past with my GPT-2 reimplementation, this time I followed Andrej Karpathy's “Let's build the GPT Tokenizer&amp;quot; video tutorial and implemented a BPE tokenizer from scratch. :-)&lt;/p&gt; &lt;p&gt;I went several steps further by identifying and optimizing major bottlenecks in both training and inference, implementing a Rust version for fast encoding, training custom tokenizers on large datasets, and evaluating their impact on GPT-2 pre-training.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kwaqe610svtf1.png?width=2670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef0332d4cfa57dd26ec66730bc7280b4f12d812a"&gt;BPE implementation from scratch summary&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My optimizations and experiments include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Improving training speed: 50x faster&lt;/strong&gt; (117s → 2.4s for 20 merges)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Making inference faster: 3.7x faster&lt;/strong&gt; with Rust implementation (21.3s → 5.3s)&lt;/li&gt; &lt;li&gt;Training custom 16K tokenizers on &lt;strong&gt;TinyStoriesV2 (~2.6GB)&lt;/strong&gt; and &lt;strong&gt;FineWeb (~3.3GB)&lt;/strong&gt; datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-training GPT-2 using custom tokenizers&lt;/strong&gt; and comparing their performance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To be honest, I found understanding tokenizer implementation and optimizing it a lot more confusing and harder than GPT-2 implementation (personal experience!) 😅.&lt;/p&gt; &lt;p&gt;In this implementation, I learned a lot about code profiling and optimizing code for both memory and speed. The Rust vibe-coding was fun and surprisingly successful!&lt;/p&gt; &lt;p&gt;Like always, I've documented everything—the code, optimizations, training runs, experiments, and notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/tree/main/bpe"&gt;https://github.com/garg-aayush/building-from-scratch/tree/main/bpe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/lecture_notes.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/lecture_notes.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Readme&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/Readme.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/Readme.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Commit-by-commit development&lt;/strong&gt;: Each optimization and experiment is a separate commit for easy understanding&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o13oyn</id>
    <title>30B models at full-size, or 120B models at Q4?</title>
    <updated>2025-10-08T07:09:35+00:00</updated>
    <author>
      <name>/u/arimoto02</name>
      <uri>https://old.reddit.com/user/arimoto02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a set up with an NVIDIA A100 80GB. Should i run 30B-ish models (like qwen32b) at full size or 120B-ish models (GLM4.5 air?) at Q4?&lt;/p&gt; &lt;p&gt;Also, is there any comprehensive comparison for model degradation with respect to their size/quantize level?&lt;/p&gt; &lt;p&gt;Thank you all!&lt;/p&gt; &lt;p&gt;Edit: Really sorry guys, i somehow remember that there's a qwen3 120b moe (lol). Fixed the post to qwen3 8b vs qwen3 32b.&lt;/p&gt; &lt;p&gt;Edit2: i realized the qwen3 8b vs 32b doesnt really fits with the A100 settings, so i changed it to arbitrary models &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arimoto02"&gt; /u/arimoto02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T07:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1eac0</id>
    <title>What models do you find yourself actually using, and what for?</title>
    <updated>2025-10-08T15:50:21+00:00</updated>
    <author>
      <name>/u/sine120</name>
      <uri>https://old.reddit.com/user/sine120</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got into Local LLMs, went down the rabbit hole, thrashed about trying to get my 9070XT to work in Ollama, gave up, and have been having fun in LM Studio since with models like Qwen3 4B/ 30B, gpt-oss-20B.&lt;/p&gt; &lt;p&gt;I wanted to gauge what people actually use instead of just going off benchmarks. What models are you running/ which ones are your favorites? What kind of hardware do you have? What kind of speeds do you see? What do you actually use your local LLMs for?&lt;/p&gt; &lt;p&gt;So far I'm liking gpt-oss and Qwen3 for the speed and usability in my 16GB of VRAM, but wondering if I should consider others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sine120"&gt; /u/sine120 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:50:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o11udk</id>
    <title>is GTX 3090 24GB GDDR6 good for local coding?</title>
    <updated>2025-10-08T05:16:36+00:00</updated>
    <author>
      <name>/u/TruthTellerTom</name>
      <uri>https://old.reddit.com/user/TruthTellerTom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Codex-CLI API costs are getting expensive quick. Found a local used 24 GB RTX 3090 at around 500 bucks. Would this be a good investment? and what local coding LLM would you guys recommend with it?&lt;/p&gt; &lt;p&gt;Desktop Specs:&lt;br /&gt; i7 12700 (12th Gen), 32GB RAM, windows 11 x64&lt;/p&gt; &lt;p&gt;ENV.&lt;/p&gt; &lt;p&gt;Web Applications with PHP, MySQL, jQuery.&lt;br /&gt; Mainly Boostrap 5 (or latest) for style/theme/ready-to-us components&lt;br /&gt; Solo Dev. I keep things simple, and focus on functions. 99% Functional programming.&lt;br /&gt; I dont use frameworks like laravel, i have my own Js and php lib and helpers for most stuff.&lt;/p&gt; &lt;p&gt;would appreciate some expert advise&lt;br /&gt; Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruthTellerTom"&gt; /u/TruthTellerTom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T05:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o184q6</id>
    <title>clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue.</title>
    <updated>2025-10-08T11:43:33+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"&gt; &lt;img alt="clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue." src="https://preview.redd.it/capdedupkvtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1638aaab704ffb77c8b04fc7e98e53289423588a" title="clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Clement Delangue (clem) on 𝕏: &lt;a href="https://x.com/ClementDelangue/status/1975615257923231969"&gt;https://x.com/ClementDelangue/status/1975615257923231969&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/capdedupkvtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T11:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o16584</id>
    <title>[2510.05688] vAttention: Verified Sparse Attention</title>
    <updated>2025-10-08T09:51:31+00:00</updated>
    <author>
      <name>/u/Elven77AI</name>
      <uri>https://old.reddit.com/user/Elven77AI</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elven77AI"&gt; /u/Elven77AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.05688"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o16584/251005688_vattention_verified_sparse_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o16584/251005688_vattention_verified_sparse_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T09:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ifyr</id>
    <title>Glm 4.6 air is coming</title>
    <updated>2025-10-07T15:46:04+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt; &lt;img alt="Glm 4.6 air is coming" src="https://preview.redd.it/nmwtp72fnptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78e29ea88c42c50216e45dc228bec7e885394f0c" title="Glm 4.6 air is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmwtp72fnptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0php3</id>
    <title>Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser.</title>
    <updated>2025-10-07T20:00:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt; &lt;img alt="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." src="https://external-preview.redd.it/bXZwenNmemR3cXRmMQIkfIP27ngHfIf2o9FEvt2htapLOK3sF-ey3U1M3aWC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=681db9c824b6d01bd93193fb35668e28576d4f98" title="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM recently released Granite Docling, a 258M parameter VLM engineered for efficient document conversion. So, I decided to build a demo which showcases the model running entirely in your browser with WebGPU acceleration. Since the model runs locally, no data is sent to a server (perfect for private and sensitive documents).&lt;/p&gt; &lt;p&gt;As always, the demo is available and open source on Hugging Face: &lt;a href="https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU"&gt;https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/33mh4fzdwqtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T20:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0zted</id>
    <title>LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B</title>
    <updated>2025-10-08T03:25:57+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt; &lt;img alt="LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B" src="https://external-preview.redd.it/4KqP_OSthEkz01ewHYfZ8d2q4c416HGrMsoPMLQI0Ig.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c66b4130900a73ea5487c4cac5cb3f5e8fd5eb" title="LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LFM2 is a new generation of hybrid models developed by &lt;a href="https://www.liquid.ai/blog/lfm2-8b-a1b-an-efficient-on-device-mixture-of-experts"&gt;Liquid AI&lt;/a&gt;, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oaielly54ttf1.jpg?width=1953&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2013bd08578cf610969ab162642e20a42264eac1"&gt;https://preview.redd.it/oaielly54ttf1.jpg?width=1953&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2013bd08578cf610969ab162642e20a42264eac1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The weights of their first MoE based on LFM2, with 8.3B total parameters and 1.5B active parameters.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LFM2-8B-A1B is the best on-device MoE in terms of both &lt;strong&gt;quality&lt;/strong&gt; (comparable to 3-4B dense models) and &lt;strong&gt;speed&lt;/strong&gt; (faster than Qwen3-1.7B).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code and knowledge&lt;/strong&gt; capabilities are significantly improved compared to LFM2-2.6B.&lt;/li&gt; &lt;li&gt;Quantized variants fit comfortably on high-end &lt;strong&gt;phones, tablets, and laptops&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Find more information about LFM2-8B-A1B in their &lt;a href="https://www.liquid.ai/blog/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;https://huggingface.co/LiquidAI/LFM2-8B-A1B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T03:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1dqiy</id>
    <title>Stop flexing Pass@N — show Pass-all-N</title>
    <updated>2025-10-08T15:30:27+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"&gt; &lt;img alt="Stop flexing Pass@N — show Pass-all-N" src="https://preview.redd.it/20a6i107owtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24db5289fda31d349651dd815a145c7f672d3cf6" title="Stop flexing Pass@N — show Pass-all-N" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a claim, and I’m curious what you think. I think model report should also report &lt;strong&gt;Pass-all-N&lt;/strong&gt; for tasks where they use Pass@N (like SWE tasks). Pass@N and mean resolved rate look nice, but they hide instability. Pass-all-N is simple: what share of tasks the model solves in &lt;strong&gt;EVERY&lt;/strong&gt; one of N runs. If it passes 4/5 times, it doesn’t count. For real use I want an agent that solves the task every time, not “sometimes with lucky seed.”&lt;/p&gt; &lt;p&gt;I checked this on &lt;a href="https://swe-rebench.com/"&gt;SWE-rebench&lt;/a&gt; (5 runs per model, August set) and Pass-all-5 is clearly lower than the mean resolved rate for all models. The gap size is different across models too — some are more stable, some are very flaky. That’s exactly the signal I want to see.&lt;/p&gt; &lt;p&gt;I’m not saying to drop &lt;a href="mailto:Pass@N"&gt;Pass@N&lt;/a&gt;. Keep it — but also report &lt;strong&gt;Pass-all-N&lt;/strong&gt; so we can compare reliability, not just the best-case average. Most releases already run multiple seeds to get Pass@N anyway, so it’s basically free to add Pass-all-N from the same runs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/20a6i107owtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o19ouy</id>
    <title>Can't get my local setups running smoothly, any options for uncensored generation?</title>
    <updated>2025-10-08T12:54:49+00:00</updated>
    <author>
      <name>/u/zemocrise</name>
      <uri>https://old.reddit.com/user/zemocrise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been trying to get a local environment up and running for uncensored outputs, but honestly, it’s been a pain. Constant issues with dependencies, VRAM limits, crashes, and juggling different models. I have run out of cash and am thinking of trying something new for now. &lt;/p&gt; &lt;p&gt;Is anyone here aware of any powerful online or hybrid alternatives that are fully uncensored? Would love recommendations before my finances improve to get a better local setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zemocrise"&gt; /u/zemocrise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19ouy/cant_get_my_local_setups_running_smoothly_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19ouy/cant_get_my_local_setups_running_smoothly_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o19ouy/cant_get_my_local_setups_running_smoothly_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1drs6</id>
    <title>Ling-1T</title>
    <updated>2025-10-08T15:31:37+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"&gt; &lt;img alt="Ling-1T" src="https://external-preview.redd.it/GF0ej-9rt3AXeKHvcKd5G-UgA8tEbZGSIvNEsQkOwA0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd86786f2d1ca8025b03b2b577396fa4bc316d8" title="Ling-1T" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ling-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with ≈ 50 billion active parameters per token. Built on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.&lt;/p&gt; &lt;p&gt;Pre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training. This curriculum greatly enhances the model’s efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarks—balancing accuracy and efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o19wvg</id>
    <title>LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot</title>
    <updated>2025-10-08T13:03:46+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"&gt; &lt;img alt="LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot" src="https://preview.redd.it/bql3o49zyvtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c9af7b8db7f6c91135a9b78ef4113719931ea7f" title="LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve updated our Task Completion Benchmarks, and this time Gemini 2.5 Flash (latest version) came out on top for overall task completion, scoring highest across context reasoning, SQL, agents, and normalization.&lt;/p&gt; &lt;p&gt;Our TaskBench evaluates how well language models can actually finish a variety of real-world tasks, reporting the percentage of tasks completed successfully using a consistent methodology for all models.&lt;/p&gt; &lt;p&gt;See the full rankings and details: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear how others are seeing Gemini Flash's latest version perform vs other models, any surprises or different results in your projects?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bql3o49zyvtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T13:03:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1ac09</id>
    <title>AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!</title>
    <updated>2025-10-08T13:20:50+00:00</updated>
    <author>
      <name>/u/zennaxxarion</name>
      <uri>https://old.reddit.com/user/zennaxxarion</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"&gt; &lt;img alt="AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!" src="https://b.thumbs.redditmedia.com/HzZ8IknU9-KdwN0J-_2TQJ93jzutAFq4cAeVawprNKM.jpg" title="AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for AI21, creator of the Jamba model family.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We’re super excited to announce the launch of our brand new model, Jamba 3B!&lt;/p&gt; &lt;p&gt;Jamba 3B is the swiss army knife of models, designed to be ready on the go.&lt;/p&gt; &lt;p&gt;You can run it on your iPhone, Android, Mac or PC for smart replies, conversational assistants, model routing, fine-tuning and much more.&lt;/p&gt; &lt;p&gt;We believe we’ve rewritten what tiny models can do. &lt;/p&gt; &lt;p&gt;Jamba 3B keeps up near 40 t/s even with giant context windows, while others crawl once they pass 128K. &lt;/p&gt; &lt;p&gt;Even though it’s smaller at 3B parameters, it matches or beats Qwen 3 4B and Gemma 3 4B in model intelligence.&lt;/p&gt; &lt;p&gt;We performed benchmarking using the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mac M3 36GB&lt;/li&gt; &lt;li&gt;iPhone 16 Pro&lt;/li&gt; &lt;li&gt;Galaxy S25&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here are our key findings:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Faster and steadier at scale:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keeps producing ~40 tokens per second on Mac even past 32k context&lt;/li&gt; &lt;li&gt;Still cranks out ~33 t/s at 128k while Qwen 3 4B drops to &amp;lt;1 t/s and Llama 3.2 3B goes down to ~5 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Best long context efficiency:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From 1k to 128k context, latency barely moves (43 to 33 t/s). Every rival model loses 70% speed beyond 32k&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;High intelligence per token ratio:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scored 0.31 combined intelligence index at ~40 t/s, above Gemma 3 4B (0.20) and Phi-4 Mini (0.22)&lt;/li&gt; &lt;li&gt;Qwen 3 4B ranks slightly higher in raw score (0.35) but runs 3x slower&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Outpaces IBM Granite 4 Micro:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Produces 5x more tokens per second at 256K on Mac M3 (36 GB) with reasoning intact&lt;/li&gt; &lt;li&gt;First 3B parameter model to stay coherent past 60K tokens. Achieves an effective context window ≈ 200k on desktop and mobile without nonsense outputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hardware footprint:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The 4-bit quantized version of Jamba 3B requires the following to run on llama.cpp at context length of 32k: &lt;/p&gt; &lt;p&gt;Model Weights: 1.84 GiB&lt;/p&gt; &lt;p&gt;Total Active Memory: ~2.2 GiB&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Blog:&lt;/strong&gt; &lt;a href="https://www.ai21.com/blog/introducing-jamba-reasoning-3b/"&gt;https://www.ai21.com/blog/introducing-jamba-reasoning-3b/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Huggingface:&lt;/strong&gt; &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zennaxxarion"&gt; /u/zennaxxarion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o1ac09"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T13:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
