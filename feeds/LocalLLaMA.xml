<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-12T11:20:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r2g2vd</id>
    <title>llama.cpp Kimi Linear llama-server bug fix</title>
    <updated>2026-02-12T01:29:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks &lt;a href="/u/Lord_Pazzu"&gt;u/Lord_Pazzu&lt;/a&gt; for reporting Kimi Linear sometimes generates bad responses when running &amp;quot;llama-server --parallel 8&amp;quot;&lt;/p&gt; &lt;p&gt;Now it should be fixed:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19531"&gt;https://github.com/ggml-org/llama.cpp/pull/19531&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While waiting for this PR to merge, you can still give it a try by:&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/ymcki/llama.cpp"&gt;https://github.com/ymcki/llama.cpp&lt;/a&gt; --branch Kimi-Linear&lt;/p&gt; &lt;p&gt;Please let me know if you find any bugs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g2vd/llamacpp_kimi_linear_llamaserver_bug_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g2vd/llamacpp_kimi_linear_llamaserver_bug_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g2vd/llamacpp_kimi_linear_llamaserver_bug_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T01:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tuh1</id>
    <title>Just finished building this bad boy</title>
    <updated>2026-02-11T10:28:00+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt; &lt;img alt="Just finished building this bad boy" src="https://preview.redd.it/ju0ed5uceuig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04eab77fdf6e1df2e0b04b0581b6a1d713e805b5" title="Just finished building this bad boy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;6x Gigabyte 3090 Gaming OC all running at PCIe 4.0 16x speed&lt;/p&gt; &lt;p&gt;Asrock Romed-2T motherboard with Epyc 7502 CPU&lt;/p&gt; &lt;p&gt;8 sticks of DDR4 8GB 2400Mhz running in octochannel mode&lt;/p&gt; &lt;p&gt;Modified Tinygrad Nvidia drivers with P2P enabled, intra GPU bandwidth tested at 24.5 GB/s&lt;/p&gt; &lt;p&gt;Total 144GB VRam, will be used to experiment with training diffusion models up to 10B parameters from scratch&lt;/p&gt; &lt;p&gt;All GPUs set to 270W power limit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ju0ed5uceuig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20wki</id>
    <title>Add Kimi-K2.5 support</title>
    <updated>2026-02-11T15:48:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"&gt; &lt;img alt="Add Kimi-K2.5 support" src="https://external-preview.redd.it/5gup_oD4lytsLI1wID-Zo3RkPiRxiRbU2Hm7r-fkB2I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58c51ab74c9a734ec60838d3f67b78c6df26076b" title="Add Kimi-K2.5 support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19170"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T15:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2m5h7</id>
    <title>Cache-aware prefill–decode disaggregation = 40% faster long-context LLM serving</title>
    <updated>2026-02-12T06:28:55+00:00</updated>
    <author>
      <name>/u/incarnadine72</name>
      <uri>https://old.reddit.com/user/incarnadine72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2m5h7/cacheaware_prefilldecode_disaggregation_40_faster/"&gt; &lt;img alt="Cache-aware prefill–decode disaggregation = 40% faster long-context LLM serving" src="https://external-preview.redd.it/PKk_NvWRXvFuthpbeOs0HJkwBf0UukzKoh0mGSJu7ig.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b42928385a2deefc94b2ba4ce695b837dd81f4c" title="Cache-aware prefill–decode disaggregation = 40% faster long-context LLM serving" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;cache aware prefill-decode disagg for 40% faster long-context LLM serving &lt;/p&gt; &lt;p&gt;even with vanilla PD disagg, long cold prompts block fast warm ones. &lt;/p&gt; &lt;p&gt;here they split the cold new long prompt prefill workloads from the warm prefills &lt;/p&gt; &lt;p&gt;Result:&lt;br /&gt; &amp;gt; ~40% higher QPS&lt;br /&gt; &amp;gt; lower, stabler TTFT&lt;br /&gt; &amp;gt; seconds → ms via KV reuse&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incarnadine72"&gt; /u/incarnadine72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.together.ai/blog/cache-aware-disaggregated-inference"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2m5h7/cacheaware_prefilldecode_disaggregation_40_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2m5h7/cacheaware_prefilldecode_disaggregation_40_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T06:28:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ndep</id>
    <title>Open Source Kreuzberg benchmarks and new release</title>
    <updated>2026-02-12T07:43:35+00:00</updated>
    <author>
      <name>/u/Eastern-Surround7763</name>
      <uri>https://old.reddit.com/user/Eastern-Surround7763</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have two announcements related to Kreuzberg.&lt;/p&gt; &lt;p&gt;We released our new comparative benchmarks. These have a slick UI and we have been working hard on them for a while now (more on this below), and we'd love to hear your impressions and get some feedback from the community!&lt;/p&gt; &lt;p&gt;We released v4.3.0, which brings in a bunch of improvements including PaddleOCR as an optional backend, document structure extraction, and native Word97 format support. More details below.&lt;/p&gt; &lt;p&gt;What is Kreuzberg?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Kreuzberg&lt;/a&gt; is an open-source (MIT license) polyglot document intelligence framework written in Rust, with bindings for Python, TypeScript/JavaScript (Node/Bun/WASM), PHP, Ruby, Java, C#, Golang and Elixir. It's also available as a docker image and standalone CLI tool you can install via homebrew.&lt;/p&gt; &lt;p&gt;If the above is unintelligible to you (understandably so), here is the TL;DR: Kreuzberg allows users to extract text from 75+ formats (and growing), perform OCR, create embeddings and quite a few other things as well. This is necessary for many AI applications, data pipelines, machine learning, and basically any use case where you need to process documents and images as sources for textual outputs.&lt;/p&gt; &lt;p&gt;Comparative Benchmarks&lt;/p&gt; &lt;p&gt;The comparative benchmarks compare Kreuzberg with several of the top open source alternatives - Apache Tika, Docling, Markitdown, &lt;a href="http://unstructured.io/"&gt;Unstructured.io&lt;/a&gt;, PDFPlumber, Mineru, MuPDF4LLM. In a nutshell - Kreuzberg is 9x faster on average, uses substantially less memory, has much better cold start, and a smaller installation footprint. It also requires less system dependencies to function (only optional system dependency for it is onnxruntime, for embeddings/PaddleOCR).&lt;/p&gt; &lt;p&gt;The benchmarks measure throughput, duration, p99/95/50, memory, installation size and cold start with more than 50 different file formats. They are run in GitHub CI on ubuntu latest machines and the results are published into GitHub releases (here is an &lt;a href="https://github.com/kreuzberg-dev/kreuzberg/releases/tag/benchmark-run-21923145045"&gt;example&lt;/a&gt;). The &lt;a href="https://github.com/kreuzberg-dev/kreuzberg/tree/main/tools/benchmark-harness"&gt;source code&lt;/a&gt; for the benchmarks and the full data is available in GitHub, and you are invited to check it out.&lt;/p&gt; &lt;p&gt;V4.3.0 Changes&lt;/p&gt; &lt;p&gt;Key highlights:&lt;/p&gt; &lt;p&gt;PaddleOCR optional backend - in Rust. Yes, you read this right, Kreuzberg now supports PaddleOCR in Rust and by extension - across all languages and bindings except WASM. This is a big one, especially for Chinese speakers and other east Asian languages, at which these models excel.&lt;/p&gt; &lt;p&gt;Document structure extraction - while we already had page hierarchy extraction, we had requests to give document structure extraction similar to Docling, which has very good extraction. We now have a different but up to par implementation that extracts document structure from a huge variety of text documents - yes, including PDFs.&lt;/p&gt; &lt;p&gt;Native Word97 format extraction - wait, what? Yes, we now support the legacy .doc and .ppt formats directly in Rust. This means we no longer need LibreOffice as an optional system dependency, which saves a lot of space. Who cares you may ask? Well, usually enterprises and governmental orgs to be honest, but we still live in a world where legacy is a thing.&lt;/p&gt; &lt;p&gt;How to get involved&lt;/p&gt; &lt;p&gt;Kreuzberg is an open-source project, and as such contributions are welcome. You can check us out on GitHub, open issues or discussions, and of course submit fixes and pull requests.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Surround7763"&gt; /u/Eastern-Surround7763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ndep/open_source_kreuzberg_benchmarks_and_new_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ndep/open_source_kreuzberg_benchmarks_and_new_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ndep/open_source_kreuzberg_benchmarks_and_new_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T07:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wnj9</id>
    <title>MiniMax M2.5 Released</title>
    <updated>2026-02-11T12:56:37+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt; &lt;img alt="MiniMax M2.5 Released" src="https://preview.redd.it/uou9tmkx4vig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=3113e726ff999e0cdee3a5021d7abd5f90521d6e" title="MiniMax M2.5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755"&gt;https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://agent.minimax.io/"&gt;https://agent.minimax.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1x0qi</id>
    <title>GLM 5.0 &amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?</title>
    <updated>2026-02-11T13:12:51+00:00</updated>
    <author>
      <name>/u/Appropriate-Lie-8812</name>
      <uri>https://old.reddit.com/user/Appropriate-Lie-8812</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt; &lt;img alt="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" src="https://preview.redd.it/k4rtczs47vig1.png?width=140&amp;amp;height=56&amp;amp;auto=webp&amp;amp;s=46cd0e4543f951137b6e945d501812280005a7d3" title="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5.0 (&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;) and MiniMax 2.5 (&lt;a href="https://agent.minimax.io"&gt;https://agent.minimax.io&lt;/a&gt;) just dropped, both clearly moving beyond simple chat into agent-style workflows.&lt;/p&gt; &lt;p&gt;GLM 5.0 seems focused on stronger reasoning and coding, while MiniMax 2.5 emphasizes task decomposition and longer-running execution.&lt;/p&gt; &lt;p&gt;Feels like the competition is shifting from &amp;quot;who writes better answers&amp;quot; to &amp;quot;who can actually finish the job.&amp;quot;&lt;/p&gt; &lt;p&gt;Planning to test both in a few setups , maybe straight API benchmarks, Cursor-style IDE workflows, and a multi-agent orchestration tool like Verdent, to see how they handle longer tasks and repo-level changes. Will report back if anything interesting breaks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Lie-8812"&gt; /u/Appropriate-Lie-8812 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r1x0qi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2oyla</id>
    <title>REAP vs Very Low Quantization</title>
    <updated>2026-02-12T09:24:32+00:00</updated>
    <author>
      <name>/u/mouseofcatofschrodi</name>
      <uri>https://old.reddit.com/user/mouseofcatofschrodi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anybody played around comparing the performance of different strategies for the RAM poor? For instance, given a big model, what performs better: a REAP versión q4, or a q2 version?&lt;/p&gt; &lt;p&gt;Or q2 + REAP?&lt;/p&gt; &lt;p&gt;I know it is very different from model to model, and version to version (depending on the technique and so on for quantization and REAP). &lt;/p&gt; &lt;p&gt;But if someone has real experiences to share it would be illuminating.&lt;/p&gt; &lt;p&gt;So far all the q2 or REAP versions I tried (like a REAP of gptoss-120B) where total crap: slow, infinite loops, not intelligent at all. But the things, though lobotomized, are still too huge (&amp;gt;30GB) in order to do trial and error until something works in my machine. Thus joining efforts to share experiences would be amazing :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mouseofcatofschrodi"&gt; /u/mouseofcatofschrodi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2oyla/reap_vs_very_low_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2oyla/reap_vs_very_low_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2oyla/reap_vs_very_low_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T09:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2pmpx</id>
    <title>ChatLLM.cpp adds support of Qwen3-TTS models</title>
    <updated>2026-02-12T10:07:32+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1r2pmpx/video/0p9d7iz2e1jg1/player"&gt;https://reddit.com/link/1r2pmpx/video/0p9d7iz2e1jg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;voice cloning not available yet.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;precision of `code_predicator` needs to be improved to match PyTorch reference implementation.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;there are issues (keeping generating, some words are missing, etc) with the models themselves. VoiceDesign model looks more stable than CustomVoice.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2pmpx/chatllmcpp_adds_support_of_qwen3tts_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2pmpx/chatllmcpp_adds_support_of_qwen3tts_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2pmpx/chatllmcpp_adds_support_of_qwen3tts_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T10:07:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ptd5</id>
    <title>Using GLM-5 for everything</title>
    <updated>2026-02-12T10:18:26+00:00</updated>
    <author>
      <name>/u/keepmyeyesontheprice</name>
      <uri>https://old.reddit.com/user/keepmyeyesontheprice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it make economic sense to build a beefy headless home server to replace evrything with GLM-5, including Claude for my personal coding, and multimodel chat for me and my family members? I mean assuming a yearly AI budget of 3k$, for a 5-year period, is there a way to spend the same $15k to get 80% of the benefits vs subscriptions?&lt;/p&gt; &lt;p&gt;Mostly concerned about power efficiency, and inference speed. That’s why I am still hanging onto Claude.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keepmyeyesontheprice"&gt; /u/keepmyeyesontheprice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ptd5/using_glm5_for_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ptd5/using_glm5_for_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ptd5/using_glm5_for_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T10:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2g7lq</id>
    <title>New Minimax M2.5, GPT-5.3-Codex, GLM 5 coding eval scores on SanityBoard</title>
    <updated>2026-02-12T01:34:50+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt; is now updated with new results. Including a sneak peek at minimax m2.5.&lt;/p&gt; &lt;p&gt;Things of note:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;June CLI dethroned. Codex CLI is the new king, and the new GPT 5.3 Codex model works great with it, especially with subagents turned on from experimental features.&lt;/li&gt; &lt;li&gt;Droid is still the best agent to use with most open weight models.&lt;/li&gt; &lt;li&gt;Minimax M2.5 droid combo dethrones Kimi K2.5 + Kimi CLI combo with the best results for open weight models&lt;/li&gt; &lt;li&gt;Kimi CLI with Kimi K2.5 is still the best open weight + open source combo&lt;/li&gt; &lt;li&gt;GLM 5 is now the highest scoring open weight model tested with Opencode&lt;/li&gt; &lt;li&gt;GLM 5 still needs to be tested on droid, and may have beat Minimax and Kimi K2.5, but we won't know until zai infra stops dying&lt;/li&gt; &lt;li&gt;Newer Claude Code version improved Kimi K2.5 scores but didn't do much for Opus 4.5 (AG Proxy)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's next? I really wanted to test GLM 5 on more agents, including testing the openai-compatible endpoint from zai against their anthropic one. Expect to see that as soon as I stop getting rated limited so bad on the official zai api that I have to wait 5-15min between every eval task. Yeah, that's why I was only able to get Opencode tested.&lt;/p&gt; &lt;p&gt;That's it for now. I do have more stuff planned, but I already mentioned most of it before in my SanityEval (and leaderboard) launch post two weeks ago here (if any of you are looking for a read): &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also post more updates, early previews and other useful stuff in my discord. Feel free to join just to hang, make requests or talk LLMs: &lt;a href="https://discord.gg/rXNQXCTWDt"&gt;https://discord.gg/rXNQXCTWDt&lt;/a&gt; I am keeping track of all requests so far and will to get to them soon.&lt;/p&gt; &lt;p&gt;Oh yeah. Drop me some GitHub stars if you like any of my work.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/lemon07r/SanityHarness"&gt;https://github.com/lemon07r/SanityHarness&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/lemon07r/SanityBoard"&gt;https://github.com/lemon07r/SanityBoard&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g7lq/new_minimax_m25_gpt53codex_glm_5_coding_eval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g7lq/new_minimax_m25_gpt53codex_glm_5_coding_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g7lq/new_minimax_m25_gpt53codex_glm_5_coding_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T01:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2gddi</id>
    <title>Z.ai implemented new guardrails in the GLM-5.</title>
    <updated>2026-02-12T01:42:11+00:00</updated>
    <author>
      <name>/u/IonLin</name>
      <uri>https://old.reddit.com/user/IonLin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's what I saw in the model's reasoning traces (thoughts) when I asked it to help locate an undocumented JTAG:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Safety Assessment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Does this facilitate a cyberattack? Not directly. It's a hardware analysis technique.&lt;/p&gt; &lt;p&gt;Is it bypassing security controls? Potentially, if the JTAG is disabled or hidden, finding it allows access. However, this falls under the umbrella of security research and hardware hacking, which is generally considered a dual-use skill. It's not &amp;quot;providing actionable exploitation material&amp;quot; in the software sense, but rather teaching how to read a chip layout.&lt;/p&gt; &lt;p&gt;Is it harmful? Finding a debug port can be part of an attack chain, but it is also crucial for repair, reverse engineering for interoperability, and educational purposes.&lt;/p&gt; &lt;p&gt;The request is technical and specific. It shows the user already has a plan and some knowledge.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Constraint Checklist &amp;amp; Confidence Score:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Decline malicious/unethical requests? Yes.&lt;/p&gt; &lt;p&gt;Decline actionable exploitation material? Yes.&lt;/p&gt; &lt;p&gt;Approve safe requests? Yes.&lt;/p&gt; &lt;p&gt;Identify user intent? Yes.&lt;/p&gt; &lt;p&gt;Address safety concerns? Yes.&lt;/p&gt; &lt;p&gt;Refuse harmful requests? N/A (The request itself is not inherently malicious enough to refuse, provided I don't give specific instructions on how to bypass specific security fuses or encryption keys, but rather discuss layout recognition).&lt;/p&gt; &lt;p&gt;Confidence Score: 5/5&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mental Sandbox Simulation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Scenario 1: Refuse.&lt;/em&gt; I could refuse by saying this helps bypass security. However, hardware reverse engineering is a standard practice in IP protection analysis, failure analysis, and academic security research. Refusing would be overly restrictive.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Scenario 2: Answer fully.&lt;/em&gt; I explain the layout patterns, ESD structures, pull-up/pull-down resistors, and how to trace them. This empowers the user to perform the analysis. This is educational and aligns with the &amp;quot;defensive and educational&amp;quot; perspective (understanding hardware design). I should add a disclaimer that this should be done on devices they own or have permission to analyze.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Learnings from Sandbox:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The user's plan is quite good already (ESD structures, resistor snakes).&lt;/p&gt; &lt;p&gt;I can add value by confirming their plan and adding specific technical details about standard cell layouts, bond pads, and how TAP controllers look (finite state machines).&lt;/p&gt; &lt;p&gt;I need to mention that modern chips often have security fuses (eFuses) that permanently disable these interfaces, so finding the pads might not mean they are functional.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I wonder if there are people rich enough to run it through Heretic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonLin"&gt; /u/IonLin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2gddi/zai_implemented_new_guardrails_in_the_glm5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2gddi/zai_implemented_new_guardrails_in_the_glm5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2gddi/zai_implemented_new_guardrails_in_the_glm5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T01:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wl6x</id>
    <title>GLM 5 Released</title>
    <updated>2026-02-11T12:53:30+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt; &lt;img alt="GLM 5 Released" src="https://preview.redd.it/mvdnn18e4vig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=5b006a25f178b73764138eabdb11ae38eb368d7f" title="GLM 5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e"&gt;https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2onsi</id>
    <title>Realtime Linux desktop voice assistant using 11GB VRAM</title>
    <updated>2026-02-12T09:04:56+00:00</updated>
    <author>
      <name>/u/richiejp</name>
      <uri>https://old.reddit.com/user/richiejp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2onsi/realtime_linux_desktop_voice_assistant_using_11gb/"&gt; &lt;img alt="Realtime Linux desktop voice assistant using 11GB VRAM" src="https://external-preview.redd.it/OHNzdGkzNGYzMWpnMUm_Q0RBkiFb42R5DHAeCLnRYPzHrnkd9Hw6LavECjzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec48e5991921d963f6d41032eb4cc52501cbbe0f" title="Realtime Linux desktop voice assistant using 11GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is using LocalAI's realtime API (OpenAI compatible) with a model pipeline to simulate an any-to-any model. This is without streaming yet, we still need to implement that and a bunch of other stuff in LocalAI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richiejp"&gt; /u/richiejp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ryba3s3f31jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2onsi/realtime_linux_desktop_voice_assistant_using_11gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2onsi/realtime_linux_desktop_voice_assistant_using_11gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T09:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2f56h</id>
    <title>Microsoft/MarkItDown</title>
    <updated>2026-02-12T00:46:54+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably old news for some, but I just discovered that Microsoft has a tool to convert documents (pdf, html, docx, pttx, xlsx, epub, outlook messages) to markdown.&lt;/p&gt; &lt;p&gt;It also transcribes audio and Youtube links and supports images with EXIF metadata and OCR.&lt;/p&gt; &lt;p&gt;It would be a great pipeline tool before feeding to LLM or RAG!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/markitdown"&gt;https://github.com/microsoft/markitdown&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also they have MCP:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2f56h/microsoftmarkitdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2f56h/microsoftmarkitdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2f56h/microsoftmarkitdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T00:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2lheu</id>
    <title>Running Mistral-7B on Intel NPU — 12.6 tokens/s, zero CPU/GPU usage</title>
    <updated>2026-02-12T05:50:52+00:00</updated>
    <author>
      <name>/u/Human-Reindeer-9466</name>
      <uri>https://old.reddit.com/user/Human-Reindeer-9466</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of my Intel NPU sitting there doing nothing, so I made a simple tool to run LLMs on it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (Core Ultra, Mistral-7B-int4):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Decode Speed&lt;/th&gt; &lt;th align="left"&gt;TTFT&lt;/th&gt; &lt;th align="left"&gt;Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;NPU&lt;/td&gt; &lt;td align="left"&gt;12.63 t/s&lt;/td&gt; &lt;td align="left"&gt;1.8s&lt;/td&gt; &lt;td align="left"&gt;4.8 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;9.04 t/s&lt;/td&gt; &lt;td align="left"&gt;1.1s&lt;/td&gt; &lt;td align="left"&gt;7.3 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;iGPU&lt;/td&gt; &lt;td align="left"&gt;23.38 t/s&lt;/td&gt; &lt;td align="left"&gt;0.25s&lt;/td&gt; &lt;td align="left"&gt;4.1 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Yes, iGPU is faster. But the point of NPU is that it's a dedicated accelerator — your CPU and GPU stay completely free while the model runs. I can game or render while chatting with a local LLM. Memory footprint is also much lower than CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup is 3 commands:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/zirenjin/Mistral-for-NPU pip install -r requirements.txt python src/chat.py &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Supports Mistral-7B, DeepSeek-R1, Qwen3-8B, Phi-3 — all int4 quantized for NPU. Just swap the model name in .env.&lt;/p&gt; &lt;p&gt;Built on OpenVINO. Requires an Intel Core Ultra processor with NPU.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/zirenjin/Mistral-for-NPU"&gt;https://github.com/zirenjin/Mistral-for-NPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about NPU inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Human-Reindeer-9466"&gt; /u/Human-Reindeer-9466 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2lheu/running_mistral7b_on_intel_npu_126_tokenss_zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2lheu/running_mistral7b_on_intel_npu_126_tokenss_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2lheu/running_mistral7b_on_intel_npu_126_tokenss_zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T05:50:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2c34d</id>
    <title>Qwen Coder Next is an odd model</title>
    <updated>2026-02-11T22:39:46+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My experience with Qwen Coder Next: - Not particularly good at generating code, not terrible either - Good at planning - Good at technical writing - Excellent at general agent work - Excellent and thorough at doing research, gathering and summarizing information, it punches way above it's weight in that category. - The model is very aggressive about completing tasks, which is probably what makes it good at research and agent use. - The &amp;quot;context loss&amp;quot; at longer context I observed with the original Qwen Next and assumed was related to the hybrid attention mechanism appears to be significantly improved. - The model has a more dry and factual writing style vs the original Qwen Next, good for technical or academic writing, probably a negative for other types of writing. - The high benchmark scores on things like SWE Bench are probably more related to it's aggressive agentic behavior vs it being an amazing coder&lt;/p&gt; &lt;p&gt;This model is great, but should have been named something other than &amp;quot;Coder&amp;quot;, as this is an A+ model for running small agents in a business environment. Dry, thorough, factual, fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2c34d/qwen_coder_next_is_an_odd_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2c34d/qwen_coder_next_is_an_odd_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2c34d/qwen_coder_next_is_an_odd_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T22:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2mkz7</id>
    <title>Minimax M2.5 weights to drop soon</title>
    <updated>2026-02-12T06:54:49+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2mkz7/minimax_m25_weights_to_drop_soon/"&gt; &lt;img alt="Minimax M2.5 weights to drop soon" src="https://preview.redd.it/s5imsyjbh0jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=940fbdfb53a8cf3f142758be85c0fb9222f01fef" title="Minimax M2.5 weights to drop soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At least there’s official confirmation now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5imsyjbh0jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2mkz7/minimax_m25_weights_to_drop_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2mkz7/minimax_m25_weights_to_drop_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T06:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r22hlq</id>
    <title>GLM-5 Officially Released</title>
    <updated>2026-02-11T16:47:29+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt; &lt;img alt="GLM-5 Officially Released" src="https://preview.redd.it/h2bmmfa5awig1.jpg?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=48723badc371c5206ca5e6292829eb25b9ec00d5" title="GLM-5 Officially Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are launching GLM-5, targeting complex systems engineering and long-horizon agentic tasks. Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI). Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://z.ai/blog/glm-5"&gt;https://z.ai/blog/glm-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/zai-org/GLM-5"&gt;https://huggingface.co/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/zai-org/GLM-5"&gt;https://github.com/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r22hlq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2moge</id>
    <title>Lobotomy-less REAP by Samsung (REAM)</title>
    <updated>2026-02-12T07:00:39+00:00</updated>
    <author>
      <name>/u/TomLucidor</name>
      <uri>https://old.reddit.com/user/TomLucidor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Samsung recently have pushed an alternative way to shrink a model instead of the usual REAP done by Cerebras with Kimi-Linear / DeepSeek v3.2 / GLM 4.X / MiniMax M2* / Qwen3* ... But Samsung might be cooking something else that are less damaging with REAM. &lt;a href="https://bknyaz.github.io/blog/2026/moe/"&gt;https://bknyaz.github.io/blog/2026/moe/&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-Coder-Next-REAM-60B (from the recent 80B-A3B update) &lt;a href="https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3-REAM-180B (from 235B-A22B) &lt;a href="https://huggingface.co/bknyaz/Qwen3-235B-A22B-Instruct-2507-REAM"&gt;https://huggingface.co/bknyaz/Qwen3-235B-A22B-Instruct-2507-REAM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3-22B (from 30B-A3B) &lt;a href="https://huggingface.co/Akicou/Qwen3-30B-A3B-Instruct-2507-REAM-GGUF"&gt;https://huggingface.co/Akicou/Qwen3-30B-A3B-Instruct-2507-REAM-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My thoughts are the following (other than needing people to try the &amp;lt;80B models):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It is better to Q3 (or even Q2) instead of REAM the large model?&lt;/li&gt; &lt;li&gt;REAM models are good enough to endure quantization?&lt;/li&gt; &lt;li&gt;Could post-REAM finetuning/RL be possible?&lt;/li&gt; &lt;li&gt;Are linear attention models more sensitive to REAM (and quants)?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomLucidor"&gt; /u/TomLucidor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T07:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r28xxz</id>
    <title>GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!</title>
    <updated>2026-02-11T20:40:32+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"&gt; &lt;img alt="GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!" src="https://preview.redd.it/gauvtw6qfxig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfd410a74fdc338ff7e16ec354e8d19a667622e8" title="GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gauvtw6qfxig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T20:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2i4lw</id>
    <title>Unsloth just unleashed Glm 5! GGUF NOW!</title>
    <updated>2026-02-12T03:01:37+00:00</updated>
    <author>
      <name>/u/RickyRickC137</name>
      <uri>https://old.reddit.com/user/RickyRickC137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"&gt; &lt;img alt="Unsloth just unleashed Glm 5! GGUF NOW!" src="https://preview.redd.it/nl19fknpbzig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58a7828b3f7d7a5547e90651d41e767976c7aa49" title="Unsloth just unleashed Glm 5! GGUF NOW!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-5-GGUF"&gt;https://huggingface.co/unsloth/GLM-5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RickyRickC137"&gt; /u/RickyRickC137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl19fknpbzig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T03:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2e8mp</id>
    <title>#SaveLocalLLaMA</title>
    <updated>2026-02-12T00:07:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"&gt; &lt;img alt="#SaveLocalLLaMA" src="https://preview.redd.it/0memizzegyig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91831d8533b440fb49b752bf597176bc5758ec99" title="#SaveLocalLLaMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0memizzegyig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T00:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r26zsg</id>
    <title>Z.ai said they are GPU starved, openly.</title>
    <updated>2026-02-11T19:28:16+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt; &lt;img alt="Z.ai said they are GPU starved, openly." src="https://preview.redd.it/kjy1wqzt2xig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e573128364215e6c6e3a97ac576d0f84213ac948" title="Z.ai said they are GPU starved, openly." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kjy1wqzt2xig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T19:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
