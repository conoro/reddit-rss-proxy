<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-19T19:05:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oar5du</id>
    <title>I made a multi-provider AI coding agent</title>
    <updated>2025-10-19T14:33:16+00:00</updated>
    <author>
      <name>/u/West-Bottle9609</name>
      <uri>https://old.reddit.com/user/West-Bottle9609</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been building Binharic, an open-source AI coding assistant that runs in the terminal. It's entirely written in TypeScript and uses the AI SDK from Vercel for its agentic logic, including tool use and workflow management.&lt;/p&gt; &lt;p&gt;It supports models from OpenAI, Google, Anthropic, and local ones through Ollama. It has a built-in keyword-based RAG pipeline and can use external tools via the MCP. Many things about the agent are customizable, including its personality. The default persona is a Tech-Priest (from Warhammer 40k), but this can be changed.&lt;/p&gt; &lt;p&gt;Project's GitHub repo: &lt;a href="https://github.com/CogitatorTech/binharic-cli"&gt;https://github.com/CogitatorTech/binharic-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Bottle9609"&gt; /u/West-Bottle9609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar842</id>
    <title>PC hardware questions - RAM/FCLK frequency, PCIx4 wiring</title>
    <updated>2025-10-19T14:36:28+00:00</updated>
    <author>
      <name>/u/Ertata</name>
      <uri>https://old.reddit.com/user/Ertata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run an LLM locally for no great reason, it's being more of a hobby. Completely new to it. Have a couple of technical questions&lt;/p&gt; &lt;p&gt;To start with I am going to try CPU inference with Ryzen 9700x, in that case should I bother OCing memory from 6000 to 6400 MT/s and FCLK from 2000 to 2133, or it will give less increase in speed than the numbers suggest in which case I probably will not bother stressing my system&lt;/p&gt; &lt;p&gt;Second - I have 1080 (non-Ti) and looking to get a used 3090. I know the fact that bottom PCIe is wired x4 does not matter a great deal, but does it matter it is wired to chipset and not CPU directly if I were to use both cards at the same time ot it's largely the same if I am not looking to do inference all day every day? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ertata"&gt; /u/Ertata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar842/pc_hardware_questions_ramfclk_frequency_pcix4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar842/pc_hardware_questions_ramfclk_frequency_pcix4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar842/pc_hardware_questions_ramfclk_frequency_pcix4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaxowb</id>
    <title>Best Ollama model for coding?</title>
    <updated>2025-10-19T18:50:43+00:00</updated>
    <author>
      <name>/u/Winter_Proposal_6310</name>
      <uri>https://old.reddit.com/user/Winter_Proposal_6310</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With 16GB of VRAM and 32GB of RAM, and an RTX 4070 SUPER, I need to perform large coding tasks in Python, as well as create BAT files. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Winter_Proposal_6310"&gt; /u/Winter_Proposal_6310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaxowb/best_ollama_model_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaxowb/best_ollama_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaxowb/best_ollama_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T18:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaafyq</id>
    <title>3 3090's, room for one more?</title>
    <updated>2025-10-18T23:16:49+00:00</updated>
    <author>
      <name>/u/BusinessBookkeeper63</name>
      <uri>https://old.reddit.com/user/BusinessBookkeeper63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt; &lt;img alt="3 3090's, room for one more?" src="https://preview.redd.it/4wabpxvjcyvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abcb88de7be3c5396eac5805691a1be8d3309133" title="3 3090's, room for one more?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I am currently running 3 3090's and was thinking of adding one more. But as you can see, my case Thermaltake CTE750 Air has some free space, but not sure if it can fit another 3090.&lt;/p&gt; &lt;p&gt;I know, I know, I should have had a server rack but I was looking for a Local AI + relatively decent looking case, so this is what I landed on. The CTE 750 is big enough for 3 3090's, but not sure if I should be doing 4 given temps inside a closed case is probably going to rise quick. The third 3090 needs a custom mount and sits on the side of the case in this picture, but it rests on the intake fans and I have screwed the standing with 3 screws. I have no idea, where I could fit the 4th.&lt;/p&gt; &lt;p&gt;Any suggestions on how I could do 4 3090;s in this case or if anyone has done this before?&lt;/p&gt; &lt;p&gt;Also looking for suggestions on my cooling. Currently it has intake from bottom, front, back and sides and outtake on top only. This is somewhat based on the CTE design, but open to other suggestions. Another option, is to eventually do water cooling to save on some space and keep things cooler, but that's a project kept for December.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BusinessBookkeeper63"&gt; /u/BusinessBookkeeper63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4wabpxvjcyvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T23:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa29de</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.2.0</title>
    <updated>2025-10-18T17:47:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt; &lt;img alt="Drummer's Cydonia and Magidonia 24B v4.2.0" src="https://external-preview.redd.it/texRxv_iJ0Ni14pBUMNg-YEbpRERebh0ufaJ753mjSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab199da434abf76101631f6569f8bb80838d47f9" title="Drummer's Cydonia and Magidonia 24B v4.2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magidonia is Cydonia using Magistral 2509 base.&lt;/p&gt; &lt;p&gt;Magidonia variant: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cydonia (Small 3.2) variant: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4.2.0 is an upgrade from 4.1 in regards to creativity. Enjoy!&lt;/p&gt; &lt;p&gt;Does anyone have a base to recommend for finetuning? Waiting for GLM Air 4.6 to come out :^)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;By the way, Huggingface has restricted storage in my account and I'm having a harder time doing my open-source work for the community. I'll be all out of space after a few days of work thanks to their storage restriction. &lt;/p&gt; &lt;p&gt;I tried contacting them via [&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;) but they told me to make my case to [&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;) . I haven't received a response from &lt;em&gt;that&lt;/em&gt; team yet. Other employees I've reached out to recommended that I pay around $200 / mo to get the storage I need, I think.&lt;/p&gt; &lt;p&gt;At this point I believe they're not interested in giving me an exception. I got bundled up with those who upload 1T models, I guess? I'm not sure what to do next, but I might have to start deleting models. Let me know if you guys have any ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oat6fh</id>
    <title>Best Current Model for Programming?</title>
    <updated>2025-10-19T15:55:20+00:00</updated>
    <author>
      <name>/u/MurazakiUsagi</name>
      <uri>https://old.reddit.com/user/MurazakiUsagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title says it all. I'm looking to work with Rust, C/C++, Python and Assembly. &lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MurazakiUsagi"&gt; /u/MurazakiUsagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T15:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa8klx</id>
    <title>Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models</title>
    <updated>2025-10-18T21:54:34+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt; &lt;img alt="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" src="https://external-preview.redd.it/ZnUwMzR3NXV5eHZmMTNcUS_6HPKChleMDpJ0qQU2p3V675TK2MrxaOnhSll8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=568275d60bed24ec215f8d24a426e506eab3f5d7" title="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jak8lx5uyxvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T21:54:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaumqx</id>
    <title>Energy Based Adapter Help</title>
    <updated>2025-10-19T16:52:24+00:00</updated>
    <author>
      <name>/u/arcco96</name>
      <uri>https://old.reddit.com/user/arcco96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to develop an energy based adapter which behaves like an energy based transformer. My primary goal is to provide any model uncertainty estimates (on a finetuned dataset). Unfortunately, the current code suffers degenerate generations and exhibits a lot of repeating words and patterns. &lt;/p&gt; &lt;p&gt;Any thoughts on why this is occurring and how to fix it? I think this could be a very useful technique if it works. &lt;/p&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1irCZ02XqTqQjQuE07FBjue6YYWmLsqbi?usp=sharing"&gt;https://colab.research.google.com/drive/1irCZ02XqTqQjQuE07FBjue6YYWmLsqbi?usp=sharing&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcco96"&gt; /u/arcco96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaumqx/energy_based_adapter_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaumqx/energy_based_adapter_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaumqx/energy_based_adapter_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T16:52:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar69q</id>
    <title>Any resource to understand LLM fine tuning/inference at a medium level to learn about temperature, quanitzation, loss functions, gpu setup?</title>
    <updated>2025-10-19T14:34:15+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any resource you found helpful to learn LLM fine tuning at a medium level so. i can start tinkering by knowing what's happening behind the scenes? Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xiza</id>
    <title>dgx, it's useless , High latency</title>
    <updated>2025-10-18T14:38:06+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt; &lt;img alt="dgx, it's useless , High latency" src="https://preview.redd.it/wwroq3nbtvvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d419416ab7812d4f7c564531795007c015a4c85f" title="dgx, it's useless , High latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ahmad posted a tweet where DGX latency is high : &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19"&gt;https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwroq3nbtvvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oafumx</id>
    <title>Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device</title>
    <updated>2025-10-19T03:51:17+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt; &lt;img alt="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" src="https://external-preview.redd.it/1REAPLhpZknr_BaLbzQgHufo9VWmTuOWut1-PIVgTuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3c98ea4dc27d76e781b86ebeda6b0c583cc503d" title="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T03:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavy6f</id>
    <title>Reverse Engineering and Tracing internal thoughts of LLM</title>
    <updated>2025-10-19T17:43:57+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks I did following experiments to understand inner working of LLM&lt;br /&gt; Index of experiments I did in this article (I used LLama 3 1B)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Token Prediction Trace&lt;/li&gt; &lt;li&gt;Attribution Analysis&lt;/li&gt; &lt;li&gt;Layer Emergence (knowledge tracing)&lt;/li&gt; &lt;li&gt;Weight Matrix Analyis (How knowledge encoded in weights)&lt;/li&gt; &lt;li&gt;Dimension Tokens Analysis (which Dimension stored encoded token for “paris”)&lt;/li&gt; &lt;li&gt;Prediction Chain (How does each dimension contribute to final output)&lt;/li&gt; &lt;li&gt;Token→Neuron Map (Which neurons encode token)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://medium.com/@harishhacker3010/reverse-engineering-and-tracing-internal-thoughts-of-llm-3017b5f72008"&gt;https://medium.com/@harishhacker3010/reverse-engineering-and-tracing-internal-thoughts-of-llm-3017b5f72008&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oapgtj</id>
    <title>I want to have a local llm server for my house - just focused on coding assistant - what would be a reasonable spec for that?</title>
    <updated>2025-10-19T13:19:49+00:00</updated>
    <author>
      <name>/u/gameguy56</name>
      <uri>https://old.reddit.com/user/gameguy56</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't need and am not interested in video/image generation - just want something to work with me on coding stuff. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gameguy56"&gt; /u/gameguy56 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T13:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oauxgg</id>
    <title>lazylms - TUI for LM Studio</title>
    <updated>2025-10-19T17:04:04+00:00</updated>
    <author>
      <name>/u/Rugs007</name>
      <uri>https://old.reddit.com/user/Rugs007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"&gt; &lt;img alt="lazylms - TUI for LM Studio" src="https://b.thumbs.redditmedia.com/HpahzCgyFc5ZYpuDsGsueGlJc11EMlLdwC_J2jGKOvc.jpg" title="lazylms - TUI for LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I made a TUI for using LM Studio by staying in the terminal. This is a hobby side project, MIT licensed and uses the CLI and REST API. Feel free to give it a try. This is inspired by lazygit and lazydocker. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Rugz007/lazylms"&gt;https://github.com/Rugz007/lazylms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rugs007"&gt; /u/Rugs007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oauxgg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oatip1</id>
    <title>If the bubble really pops how can that affect local AI models?</title>
    <updated>2025-10-19T16:08:41+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If all this AI bubble talk really comes to an popa after all, how might this affect the development of more local AI models? From what I've seen MoE models still outperforms most models easily, but creating models is still expensive as shit, rather for the planet than their pocket, donation exists anyways.&lt;/p&gt; &lt;p&gt;But the servers these models use to be trained consumes a shitton of load, and I could imagine most big company servers not allowing AI to be trained on their servers anymore considering the massive amounts of models being released every week. Do you think AI would immediately freeze in advancement upon a bubble pop making us have to wait more 80 years for an actual AGI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T16:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa98jf</id>
    <title>Made a website to track 348 benchmarks across 188 models.</title>
    <updated>2025-10-18T22:22:42+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt; &lt;img alt="Made a website to track 348 benchmarks across 188 models." src="https://preview.redd.it/omjxzqi82yvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df31289ac6e45db697b70c3a9add3e087585f736" title="Made a website to track 348 benchmarks across 188 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been building a website from a while ago in which we track the benchmark results from the official papers / model cards that the labs publish. &lt;/p&gt; &lt;p&gt;I thought it would be interesting to compile everything in one place to fill in the gaps on each model release.&lt;br /&gt; All the data is open in Github and all scores have references to the original posts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-stats.com/benchmarks"&gt;https://llm-stats.com/benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to provide candid feedback. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**We don't think this is the best approach yet**. We're now building a way to replicate the results from the most interesting and useful benchmarks, but we understand that most of them haven't been created yet.&lt;/p&gt; &lt;p&gt;Current benchmarks are too simple and are not testing real capabilities. We're looking to build interesting, real world, independent benchmarks with held out data, but that can be easy to reproduce and extend.&lt;/p&gt; &lt;p&gt;Another thing we're currently doing is benchmarking across different inference providers to monitor and detect changes in quality of their service.&lt;/p&gt; &lt;p&gt;We're currently giving out up to $1k to people that want to explore ideas about new benchmarks / environments. Dm me for more information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/omjxzqi82yvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oav4hi</id>
    <title>Two new Google models, "lithiumflow" and "orionmist", have been added to LMArena. This is Google's naming scheme and "orion" has been used internally with Gemini 3 codenames, so these are likely Gemini 3 models</title>
    <updated>2025-10-19T17:11:45+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist"&gt;https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavxt8</id>
    <title>I built a 1B CAD generator model</title>
    <updated>2025-10-19T17:43:33+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt; &lt;img alt="I built a 1B CAD generator model" src="https://external-preview.redd.it/ZGFhNmE0bzJ2M3dmMdhv6U5XLy0vFYTB3BWLA3H-O3YDxkmUtGbojZ8LN3lz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a21d6d0c153a39bacb389fe42d52137134b86925" title="I built a 1B CAD generator model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a weekend, I decided to build a small language model to generate me 3d files. No reason except for pure curiosity. Here's what I did:&lt;/p&gt; &lt;p&gt;- Gather dataset on OpenSCAD: This turns out to be quite bad because people's code quality is low &amp;amp; in-consistent.&lt;/p&gt; &lt;p&gt;- Generate synthetic data (prompt -&amp;gt; openscad): This was the most wasteful per dollar part. I spent 150$+ on Claude API (70% are on reasoning token). Ended up using Gemma3-12b running in 48 hours continuously.&lt;/p&gt; &lt;p&gt;- Finetune Gemma3-270M, 1B &amp;amp; 4B: 270M lacks fundamental code &amp;amp; object understanding and failed badly. 1B is a good balance between render-ability rate &amp;amp; speed.&lt;/p&gt; &lt;p&gt;Overall, I spent 150$ on Claude (totally wasted) &amp;amp; 25$ on GPU. Both given as credits and grants.&lt;/p&gt; &lt;p&gt;I also made a CLI app if you wanna try on Mac, Linux or Raspberry Pi 4/5: &lt;a href="https://github.com/ThomasVuNguyen/MakeMe"&gt;https://github.com/ThomasVuNguyen/MakeMe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models, dataset &amp;amp; code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasVuNguyen/K"&gt;https://github.com/ThomasVuNguyen/K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b"&gt;https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pn0yo3o2v3wf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oav3r1</id>
    <title>Quantized some MoE models with MXFP4</title>
    <updated>2025-10-19T17:10:57+00:00</updated>
    <author>
      <name>/u/noctrex</name>
      <uri>https://old.reddit.com/user/noctrex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So as I was sitting and trying out some MXFP4_MOE quants from &lt;a href="https://huggingface.co/Face314"&gt;Face314&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/sm54"&gt;sm54&lt;/a&gt;, I can say that liked them very much.&lt;/p&gt; &lt;p&gt;So I thought why not quantize some more this weekend.&lt;/p&gt; &lt;p&gt;Well, here they are:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex"&gt;https://huggingface.co/noctrex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any suggestions or critique welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noctrex"&gt; /u/noctrex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakrdm</id>
    <title>Drop your underrated models you run LOCALLY</title>
    <updated>2025-10-19T08:52:22+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preferably within the 0.2b -32b range, or MoEs up to 140b&lt;/p&gt; &lt;p&gt;I’m on a LLM downloading spree, and wanna fill up a 2tb SSD with them. &lt;/p&gt; &lt;p&gt;Can be any use case. Just make sure to mention the use case too &lt;/p&gt; &lt;p&gt;Thank you ✌️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahpmx</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:38:21+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt; &lt;img alt="When you have little money but want to run big models" src="https://a.thumbs.redditmedia.com/vj3j4Vjr082yd6wYwhFLXQpEt2Zp3s7yg7spOglJoq8.jpg" title="When you have little money but want to run big models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live in India. Everything is expensive. Importers want hefty margin. Government want hefty tax. Rtx 6000 96gb which is possible to get for 7-8k usd in USA is impossible to find even for 11 lakhs(12-13k usd) in India. So we have a couple of friends 1) Juggad 2) Olx ( indian craigslists) 3) Other similar p2p sites like fb marketplace.&lt;/p&gt; &lt;p&gt;Let me show you what I built. 1) Dell T7910 - it has 7 pci slots. I can only get 5 to work. Found it on fb mp with 256 gb ddr4 2) 5 * 3090 from olx 3) 5 pci raisers amazon. These are hard to find for cheap. 4) 1300 watt additional power supply &lt;/p&gt; &lt;p&gt;There are only 4*3090 in this build 5th slot I am using for nvme extension.&lt;/p&gt; &lt;p&gt;Total cost for this build of 96gb vram is around 3.25 lakhs. ( Around 4.6k usd) This post is just for reference for those who are in a similar boat. Please understand there is a lot of difference between planning and execution. Keep +1 lakhs in hand for things that can go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oamo0k</id>
    <title>Gemma 4</title>
    <updated>2025-10-19T10:53:39+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People are very very excited about the release of gemini 3.0 including me, but im more excited in the gemma family of models since they are based on gemini models and on top of that are open-sourced. And simce Gemini 3.0 is groundbreaking (apparently, like the pelican svg, robot svg, xbox svg, os etc tests), I am very curious about how will the gemma 4 models perform. And also, gemma 4 is going to be a big leap compared to gemma 3 coz It was based on gemini 2.0, not 2.5. So we are getting 2 genarational leaps!&lt;/p&gt; &lt;p&gt;When it will be released??&lt;/p&gt; &lt;p&gt;Gemma 1 was based on gemini 1 and was released ~1-2 months after gemini&lt;/p&gt; &lt;p&gt;Gemma 2 was based on gemini 1.5 and was released ~4 months after gemini 1.5&lt;/p&gt; &lt;p&gt;Gemma 3 was based on gemini 2 and was released ~1-2 months after gemini 2.0&lt;/p&gt; &lt;p&gt;So Gemma 4 might be released ~1-2 months after gemini 3??? Maybe???&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T10:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready 🎉</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready 🎉" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;/p&gt; &lt;p&gt;1: &lt;a href="https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV"&gt;https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV&lt;/a&gt;&lt;br /&gt; 2: &lt;a href="https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi"&gt;https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi&lt;/a&gt;&lt;br /&gt; 3: &lt;a href="https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1"&gt;https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
