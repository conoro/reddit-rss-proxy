<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-31T05:35:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n44xxi</id>
    <title>OpenWebUI lets you auto expand reasoning now!</title>
    <updated>2025-08-30T15:47:40+00:00</updated>
    <author>
      <name>/u/slpreme</name>
      <uri>https://old.reddit.com/user/slpreme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"&gt; &lt;img alt="OpenWebUI lets you auto expand reasoning now!" src="https://preview.redd.it/ms5d125ng6mf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb790039d8346bafae55cda40cbc7c0b88f33de8" title="OpenWebUI lets you auto expand reasoning now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not sure when they added this, but it was a pet peeve of mine so I wanted to share this is how you can turn on show reasoning content automatically. It's just in Settings &amp;gt; Interface &amp;gt; Always Expand Details. I'm guessing that also expands some other things but I don't use any tools so I don't know which.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slpreme"&gt; /u/slpreme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ms5d125ng6mf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3sdka</id>
    <title>Can 2 RTX 6000 Pros (2X98GB vram) rival Sonnet 4 or Opus 4?</title>
    <updated>2025-08-30T04:20:36+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Id rather pay $300 a month to own my hardware than pay $200 a month to rent. Anyone out there that has tried what can be achieved with 2 RTX 6000 pros?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T04:20:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4j473</id>
    <title>Recommended VLM that can run on a RTX 5070 Ti?</title>
    <updated>2025-08-31T02:16:47+00:00</updated>
    <author>
      <name>/u/sheeshable619</name>
      <uri>https://old.reddit.com/user/sheeshable619</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I am having my capstone project in my university and I will be utilizing VLMs. However, I'm still confuse on what VLM should I use for my project. May I ask for recommended VLMs that can run on a AMD Ryzen 7 7800X3D / RTX 5070 Ti / 32 GB RAM PC. My project will be focusing on risk monitoring wherein it will predict probable risk from the actions made by the person detected in the video/image.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sheeshable619"&gt; /u/sheeshable619 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4j473/recommended_vlm_that_can_run_on_a_rtx_5070_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4j473/recommended_vlm_that_can_run_on_a_rtx_5070_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4j473/recommended_vlm_that_can_run_on_a_rtx_5070_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T02:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3b13b</id>
    <title>Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)</title>
    <updated>2025-08-29T15:47:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt; &lt;img alt="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" src="https://external-preview.redd.it/ZWZwemw0NXNiemxmMRIjC8ICuXshETDKyWbElsvvahdP8-tMtjXY4bwDOY1n.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f385cf95be0a591edf241b94d0612947ca571c1" title="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to models:&lt;br /&gt; - FastVLM: &lt;a href="https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e"&gt;https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e&lt;/a&gt;&lt;br /&gt; - MobileCLIP2: &lt;a href="https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47"&gt;https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (+ source code): &lt;a href="https://huggingface.co/spaces/apple/fastvlm-webgpu"&gt;https://huggingface.co/spaces/apple/fastvlm-webgpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ayma955sbzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T15:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3u7qf</id>
    <title>How‚Äôs your experience with the GPT OSS models? Which tasks do you find them good at‚Äîwriting, coding, or something else</title>
    <updated>2025-08-30T06:06:58+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T06:06:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4ibrf</id>
    <title>What‚Äôs up with Notebooks?</title>
    <updated>2025-08-31T01:36:22+00:00</updated>
    <author>
      <name>/u/FrozenBuffalo25</name>
      <uri>https://old.reddit.com/user/FrozenBuffalo25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice that Unsloth and some other frameworks seem to rely a lot on Jupyter ‚ÄúNotebooks‚Äù for running training. Why would they prefer this over just a docker image, a script with a Conda environment, or even a CLI tool built with the right dependencies? &lt;/p&gt; &lt;p&gt;And why would I want a ‚ÄúNotebook‚Äù, instead of executing the scripts the same way I run vLLM and llama.cpp and everything else on my computer?&lt;/p&gt; &lt;p&gt;Just curious.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrozenBuffalo25"&gt; /u/FrozenBuffalo25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4ibrf/whats_up_with_notebooks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4ibrf/whats_up_with_notebooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4ibrf/whats_up_with_notebooks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T01:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4j8bw</id>
    <title>Are we about to get a new Qwen model?</title>
    <updated>2025-08-31T02:22:47+00:00</updated>
    <author>
      <name>/u/BaronVonDoggo</name>
      <uri>https://old.reddit.com/user/BaronVonDoggo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4j8bw/are_we_about_to_get_a_new_qwen_model/"&gt; &lt;img alt="Are we about to get a new Qwen model?" src="https://b.thumbs.redditmedia.com/a_vLLs0W42G8RnP4RQISMTalQLJ_qqLLEeucNMJuEIs.jpg" title="Are we about to get a new Qwen model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qg4enxi8m9mf1.png?width=2301&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23cb0319fbbc798c07029681a40abd132ce2be2a"&gt;https://preview.redd.it/qg4enxi8m9mf1.png?width=2301&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23cb0319fbbc798c07029681a40abd132ce2be2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have not seen qwen-max-2025-08-15 anywhere. Is this new? Are we about to get a new Qwen model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaronVonDoggo"&gt; /u/BaronVonDoggo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4j8bw/are_we_about_to_get_a_new_qwen_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4j8bw/are_we_about_to_get_a_new_qwen_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4j8bw/are_we_about_to_get_a_new_qwen_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T02:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4g4lq</id>
    <title>Question: will inference engines such as sglang and vllm support 2bit (or 3,5,6 etc)?</title>
    <updated>2025-08-30T23:47:12+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Question: will inference engines such as sglang and vllm support 2bit? Or 1.93bpw, 3.., 5.., 6..bpw etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4g4lq/question_will_inference_engines_such_as_sglang/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4g4lq/question_will_inference_engines_such_as_sglang/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4g4lq/question_will_inference_engines_such_as_sglang/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T23:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n451ka</id>
    <title>GPT OSS Fine-tuning QAT</title>
    <updated>2025-08-30T15:51:53+00:00</updated>
    <author>
      <name>/u/Short_Struggle7803</name>
      <uri>https://old.reddit.com/user/Short_Struggle7803</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read more about our (Nvidia) end to end example on GPT OSS fine tuning QAT + SGlang deployment üëâ &lt;a href="https://lmsys.org/blog/2025-08-28-gpt-oss-qat/"&gt;https://lmsys.org/blog/2025-08-28-gpt-oss-qat/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine-tuning QAT helps keep the original MXFP4 quantization of GPT OSS while adapting to downstream task.&lt;/p&gt; &lt;p&gt;We have some example results (and comparisons to Nvidia‚Äôs NVFP4 format) here :&lt;/p&gt; &lt;p&gt;&lt;a href="https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/"&gt;https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do checkout üôÉ!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short_Struggle7803"&gt; /u/Short_Struggle7803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4matr</id>
    <title>how to use intel npu for lm studio</title>
    <updated>2025-08-31T05:12:05+00:00</updated>
    <author>
      <name>/u/TopSecre-t</name>
      <uri>https://old.reddit.com/user/TopSecre-t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"&gt; &lt;img alt="how to use intel npu for lm studio" src="https://a.thumbs.redditmedia.com/oF_VLz5o0T7sHHvIj_JdH0QlI_y3btKgoFY02MsEXn0.jpg" title="how to use intel npu for lm studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have intel core ultra 7 255hx and rtx 5060 with 8gb vram&lt;/p&gt; &lt;p&gt;the intel cpu has an integrated npu and it have 18gb shared memory also the gpu tab shows total 26gb memory with its 8gb vram&lt;/p&gt; &lt;p&gt;but how do i use the 18gb shared memory? when i ask something on my local ai on lm studio it only uses the gpu vram of 8gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TopSecre-t"&gt; /u/TopSecre-t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n4matr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n407lv</id>
    <title>Training a 11M language model for Raspberry Pi Pico - progress</title>
    <updated>2025-08-30T12:19:39+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I have been training a Language Model for the Raspberry Pi Pico (fast &amp;amp; smart). My research showed me that 11M parameters is the sweet spot (about 2-3 token/s) to start.&lt;/p&gt; &lt;p&gt;What I use: &lt;/p&gt; &lt;p&gt;- For training: PicoLM&lt;/p&gt; &lt;p&gt;- GPU: 5090 for Prime Intellect (from grants), H100 on &lt;a href="http://Ori.co"&gt;Ori.co&lt;/a&gt; (free credits), AWS GPU (free credits)&lt;/p&gt; &lt;p&gt;Progress:&lt;/p&gt; &lt;p&gt;- I have been training it on 5M rows of Dolma + Olmo 7B tokenizers&lt;/p&gt; &lt;p&gt;Checkpoints:&lt;/p&gt; &lt;p&gt;17k:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Prompt 7: &amp;quot;In the year 2050,&amp;quot; **Response**: ``` we have been doing that it was just for the same time, he said, and I have a lot of my father, and it was my own. We had a very much of a little as I wanted to do it. I'm not sure, but the people just like to me and I've never been a lot of my blog, so that I am not the most likely was that she's not to be my face. I wanted to get to me, ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;75k:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Prompt 1: &amp;quot;Hello, how are you?&amp;quot; **Response**: ``` I've been my own own name. I think I've got a couple of reasons. I think I was a lot of my mind and my own way I did not know my work. I got the way I took a little time with my first time. I am very sure I do. I like a good, I‚Äôm sure I did my car but I do have to be on my job. I've been with my friend and ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All benchmark results: &lt;a href="https://github.com/ThomasVuNguyen/Starmind-Zero/tree/main/benchmarks/results"&gt;https://github.com/ThomasVuNguyen/Starmind-Zero/tree/main/benchmarks/results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All datasets &amp;amp; code are available on my github:&lt;br /&gt; - Github: &lt;a href="https://github.com/ThomasVuNguyen/Starmind-Zero"&gt;https://github.com/ThomasVuNguyen/Starmind-Zero&lt;/a&gt;&lt;br /&gt; - Huggingface: &lt;a href="https://huggingface.co/ThomasTheMaker"&gt;https://huggingface.co/ThomasTheMaker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note:&lt;br /&gt; - I am not a ML scientist. Purely an AI startup founder with too much energy to just do normal engineering and be happy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T12:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3xxm5</id>
    <title>üåüIntroducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinkingüåü</title>
    <updated>2025-08-30T10:08:44+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3- &lt;strong&gt;Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to &amp;quot;think in rap lyrics&amp;quot; or &amp;quot;use bullet points to organize thoughts&amp;quot; and it will literally reason that way before giving you an answer.&lt;/p&gt; &lt;p&gt;You can check out the model on HuggingFace: &lt;a href="https://huggingface.co/AGI-0/Art-0-8B"&gt;https://huggingface.co/AGI-0/Art-0-8B&lt;/a&gt; (please leave a like in the repo if you like this model)&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card &lt;a href="https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab"&gt;https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T10:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4m30g</id>
    <title>ISO: VS Code + LocalLlama help</title>
    <updated>2025-08-31T04:59:22+00:00</updated>
    <author>
      <name>/u/1EvilSexyGenius</name>
      <uri>https://old.reddit.com/user/1EvilSexyGenius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"&gt; &lt;img alt="ISO: VS Code + LocalLlama help" src="https://preview.redd.it/de87bjkbeamf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fccda791b6be5274998a0a9f3d601c60b70c374" title="ISO: VS Code + LocalLlama help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I connect vs code's latest default AI chat system to use my locally running llamacpp server that's currently running in OpenAI API mode? &lt;/p&gt; &lt;p&gt;Is this even possible? ü§î&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1EvilSexyGenius"&gt; /u/1EvilSexyGenius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/de87bjkbeamf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T04:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4329n</id>
    <title>How do you people run GLM 4.5 locally ?</title>
    <updated>2025-08-30T14:30:45+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context i have a dual rtx 3090 rig with 128gb of ddr5 ram and no matter what i try i get around 6 tokens per second...&lt;br /&gt; On CPU only inference i get between 5 and 6 tokens while on partial GPU offload i get between 5.5 and 6.8 tokens.&lt;br /&gt; I tried 2 different versions the one from unsloth Q4_K_S (&lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF&lt;/a&gt;) and the one from LovedHeart MXFP4 (&lt;a href="https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1%5C_M"&gt;https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1\_M&lt;/a&gt;)&lt;br /&gt; The one from unsloth is 1 token per second slower but still no story change.&lt;br /&gt; I changed literally all settings from lmstudio, even managed to get it to load with the full 131k context but still nowhere near the speed other users get on a single 3090 with offloading.&lt;br /&gt; I tried installing vllm but i get too much errors and i gave up.&lt;br /&gt; Is there another program i should try ? Have i chose the wrong models ?&lt;br /&gt; It's really frustrating and it's taking me too much hours to solve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T14:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3ldon</id>
    <title>Qwen3-coder is mind blowing on local hardware (tutorial linked)</title>
    <updated>2025-08-29T22:35:27+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt; &lt;img alt="Qwen3-coder is mind blowing on local hardware (tutorial linked)" src="https://external-preview.redd.it/MHAyYm12N3NjMW1mMWyTIaaq8py0BbLEXek7RrX8ohVlR1FrRoAdOlxuqQ67.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d298923517a79cfa7fc3e04c1533fbc4c70a8f3b" title="Qwen3-coder is mind blowing on local hardware (tutorial linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello!&lt;/p&gt; &lt;p&gt;I'm honestly blown away by how far local models have gotten in the past 1-2 months. Six months ago, local models were completely useless in Cline, which tbf is pretty heavyweight in terms of context and tool-calling demands. And then a few months ago I found one of the qwen models to actually be somewhat usable, but not for any real coding.&lt;/p&gt; &lt;p&gt;However, qwen3-coder-30B is really impressive. 256k context and is actually able to complete tool calls and diff edits reliably in Cline. I'm using the 4-bit quantized version on my 36GB RAM Mac.&lt;/p&gt; &lt;p&gt;My machine does turn into a bit of a jet engine after a while, but the performance is genuinely useful. My setup is LM Studio + Qwen3 Coder 30B + Cline (VS Code extension). There are some critical config details that can break it (like disabling KV cache quantization in LM Studio), but once dialed in, it just works.&lt;/p&gt; &lt;p&gt;This feels like the first time local models have crossed the threshold from &amp;quot;interesting experiment&amp;quot; to &amp;quot;actually useful coding tool.&amp;quot; I wrote a full technical walkthrough and setup guide: &lt;a href="https://cline.bot/blog/local-models"&gt;https://cline.bot/blog/local-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/75bfhw7sc1mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T22:35:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n40ngf</id>
    <title>What is the slowest Token/sec you can live with?</title>
    <updated>2025-08-30T12:41:26+00:00</updated>
    <author>
      <name>/u/OrganicApricot77</name>
      <uri>https://old.reddit.com/user/OrganicApricot77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me:&lt;/p&gt; &lt;p&gt;5tok/s is the slowest I‚Äôll accept&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicApricot77"&gt; /u/OrganicApricot77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T12:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n45lx2</id>
    <title>GLM-4.5V model for Computer Use</title>
    <updated>2025-08-30T16:15:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt; &lt;img alt="GLM-4.5V model for Computer Use" src="https://external-preview.redd.it/MjZwOTdqc3psNm1mMZtBXPQuBBghVYkEG23VKH2rdUK_y7uZuqgwTRJo1CZN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f81993da62e75eef832974ce71abbca064f96f" title="GLM-4.5V model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either locally via Hugging Face or Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eicije20m6mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n48lcx</id>
    <title>10,000 $ Budget for a rig that will run ai (24/7)</title>
    <updated>2025-08-30T18:18:07+00:00</updated>
    <author>
      <name>/u/Holiday_Leg8427</name>
      <uri>https://old.reddit.com/user/Holiday_Leg8427</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I want to make myself at home a set-up that can run AI 24/7, i need it mainly to replace the general use of llms ( chatgpt, gemini etc...), was thinking and saw lots of posts and info aboout getting an macbook studio with maximum ram capacity, is that the best way?&lt;br /&gt; Thank you for your responses (in advance)!&lt;/p&gt; &lt;p&gt;Edit: Guys, i dont think i need a llm anymore, Imma just ask on reddit everything I need, and get result from you guys, thank for all the help and tips, and btw I have some sort of &amp;quot;credit&amp;quot; (and i can write it off basically entirely on my company) for a high end pc/pc parts, thats why I wanted to invest into something that can be as usefull as possible, and i work with many legal/medical teams in europe where gdpr is king&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Leg8427"&gt; /u/Holiday_Leg8427 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4hkar</id>
    <title>I tried almost every tts model on my ryzen 7 5000 series 16gb ram rtx 3060 laptop 6-8GB Vram</title>
    <updated>2025-08-31T00:58:19+00:00</updated>
    <author>
      <name>/u/This_is_difficult_0</name>
      <uri>https://old.reddit.com/user/This_is_difficult_0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I needed a decent voiceover for my projects and I am using minimax audio currently. It's really good. But I wanted to try some local tts and i tested around 5 of them and here are my results.&lt;/p&gt; &lt;h1&gt;1. Kokoro - 0.4-0.5 x realtime .Fast and decent quality&lt;/h1&gt; &lt;p&gt;Fast koko api one from github has lower quality than the hexgrad. &lt;/p&gt; &lt;h1&gt;2. Xtts v2 - 0.8-1x realtime tried it from pinokio ai : speed is little slower than kokoro but decent quality with cloning capabilities.&lt;/h1&gt; &lt;h1&gt;3. Chatterbox - if you have lower end gpu and cpu like me don't try ! It's slow (1.5-2x realtime but high quality , expressive. I found out when i wanted it to be neutral or soothing, it still exaggerated or sounded too robotic but not in the middle.&lt;/h1&gt; &lt;p&gt;Installation was painful for me. Took me whole day figuring out to improve the speed. I tried different forks from github and messed up file pathsüòÇ Extremely slow , so not useful for me.&lt;/p&gt; &lt;h1&gt;4. Zonos : 2-3x realtime Slow... Uninstalled it pretty soon&lt;/h1&gt; &lt;h1&gt;5. Orpheus : yet to try. Looks expressive but mismatch the rythm frequently in the huggin space. Maybe it will be slow on my laptop too&lt;/h1&gt; &lt;h1&gt;6. F5tts : around 1x realtime tried it from pinokio ai. Looks decent but somewhere in the middle, it hallucinates , so not useful&lt;/h1&gt; &lt;p&gt;I am thinking of trying Piper next. I want a lightweight better than kokoro. For now , i would rank kokoro as the best option for my pc. Do you have any good suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_is_difficult_0"&gt; /u/This_is_difficult_0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T00:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4lh7s</id>
    <title>GPT-OSS-120B on Single RTX 6000 PRO</title>
    <updated>2025-08-31T04:24:33+00:00</updated>
    <author>
      <name>/u/Ill_Recipe7620</name>
      <uri>https://old.reddit.com/user/Ill_Recipe7620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 120 token/second with a single RTX 6000 PRO. It took 33 seconds to crank out the following answer after thinking for just 3 seconds.&lt;/p&gt; &lt;p&gt;If people aren't impressed with this model then they cannot be impressed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &amp;quot;compare the heat transfer coefficient of liquid water at a high pressure to steam at these conditions and IGNORE the boiling part only convection, search the web if you ned any information&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Response:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heat‚Äëtransfer‚Äëcoefficient ( h ) comparison ‚Äì liquid water vs. steam at the same high pressure&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(boiling/phase‚Äëchange is deliberately excluded ‚Äì only convective transport is considered)&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;1. What ‚Äúthe same conditions‚Äù really means&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantity&lt;/th&gt; &lt;th align="left"&gt;Typical high‚Äëpressure case&lt;/th&gt; &lt;th align="left"&gt;Why it matters for h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Pressure&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5‚Äì20 MPa (‚âà50‚Äì200 bar) ‚Äì common in super‚Äëcritical boilers, geothermal wells, high‚Äëpressure reactors&lt;/td&gt; &lt;td align="left"&gt;Raises density of both phases, compresses the gas, and moves steam toward the liquid side of the phase diagram.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Bulk temperature&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;350‚Äì550 ¬∞C (well above the normal boiling point of water)&lt;/td&gt; &lt;td align="left"&gt;Determines fluid properties (¬µ, k, œÅ, Cp, Pr) that appear in the dimensionless groups governing convection.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Flow regime&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Forced convection in a pipe or channel, fully developed turbulent flow (Re &amp;gt; 10 000) ‚Äì the regime that gives the largest, most useful h values.&lt;/td&gt; &lt;td align="left"&gt;The Nusselt number correlations (Dittus‚ÄëBoelter, Gnielinski, Sieder‚ÄëTate‚Ä¶) are based on turbulent forced flow.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geometry&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Typically a circular tube, D = 10‚Äì30 mm; length long enough for fully‚Äëdeveloped flow.&lt;/td&gt; &lt;td align="left"&gt;Geometry enters the Reynolds number and the hydraulic diameter used in correlations.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mass‚Äëflow rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Chosen so that the &lt;strong&gt;mean velocity&lt;/strong&gt; &lt;em&gt;U&lt;/em&gt; is the same for both fluids (e.g., 2 m s‚Åª¬π). This isolates the effect of the fluid properties.&lt;/td&gt; &lt;td align="left"&gt;With the same U, the only differences in h arise from œÅ, ¬µ, k, Cp, Pr.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;2. Governing dimensionless groups&lt;/h1&gt; &lt;p&gt;For turbulent forced convection in a smooth pipe the Nusselt number is usually expressed as&lt;/p&gt; &lt;p&gt;where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Re&lt;/em&gt;=&lt;em&gt;ŒºœÅUD&lt;/em&gt;‚Äã ‚Äì Reynolds number (inertia/viscous)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Pr&lt;/em&gt;=&lt;em&gt;kcp&lt;/em&gt;‚Äã&lt;em&gt;Œº&lt;/em&gt;‚Äã ‚Äì Prandtl number (momentum/thermal diffusivity)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Nu&lt;/em&gt;=&lt;em&gt;khD&lt;/em&gt;‚Äã ‚Äì Nusselt number (convective/ conductive)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The constants &lt;em&gt;C&lt;/em&gt;,&lt;em&gt;m&lt;/em&gt;,&lt;em&gt;n&lt;/em&gt; depend on the correlation (Dittus‚ÄëBoelter, Gnielinski, etc.).&lt;br /&gt; The heat‚Äëtransfer coefficient follows directly:&lt;/p&gt; &lt;p&gt;Thus &lt;strong&gt;differences in h&lt;/strong&gt; arise from the three property groups (&lt;em&gt;œÅ&lt;/em&gt;,&lt;em&gt;Œº&lt;/em&gt;,&lt;em&gt;k&lt;/em&gt;,&lt;em&gt;cp&lt;/em&gt;‚Äã) that appear in &lt;em&gt;Re&lt;/em&gt;, &lt;em&gt;Pr&lt;/em&gt; and the explicit factor &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;3. Thermophysical properties at high pressure&lt;/h1&gt; &lt;p&gt;Below is a representative data set for &lt;strong&gt;10 MPa (‚âà100 bar)&lt;/strong&gt; at &lt;strong&gt;500 ¬∞C&lt;/strong&gt; (the pressure is the same for both fluids; the temperature is above the critical temperature of water, 374 ¬∞C, so both fluids are in the &lt;em&gt;supercritical&lt;/em&gt; regime, but we still distinguish ‚Äúliquid‚Äëlike‚Äù (dense) and ‚Äústeam‚Äëlike‚Äù (less dense) states).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Property&lt;/th&gt; &lt;th align="left"&gt;Liquid‚Äëlike water (œÅ ‚âà 650 kg m‚Åª¬≥)&lt;/th&gt; &lt;th align="left"&gt;Steam‚Äëlike water (œÅ ‚âà 120 kg m‚Åª¬≥)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density, œÅ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;650 kg m‚Åª¬≥&lt;/td&gt; &lt;td align="left"&gt;120 kg m‚Åª¬≥&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Dynamic viscosity, ¬µ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.4 √ó 10‚Åª‚Å¥ Pa s&lt;/td&gt; &lt;td align="left"&gt;1.6 √ó 10‚Åª‚Å¥ Pa s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity, k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.70 W m‚Åª¬π K‚Åª¬π&lt;/td&gt; &lt;td align="left"&gt;0.45 W m‚Åª¬π K‚Åª¬π&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Specific heat, c‚Çö&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.1 kJ kg‚Åª¬π K‚Åª¬π&lt;/td&gt; &lt;td align="left"&gt;2.4 kJ kg‚Åª¬π K‚Åª¬π&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number, Pr = c‚Çö¬µ/k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;‚âà 7.3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;‚âà 0.85&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;em&gt;Sources&lt;/em&gt;: NIST REFPROP 10.0, IAPWS‚Äë95 formulation, extrapolated to 10 MPa and 500 ¬∞C.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the numbers tell us&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Density&lt;/strong&gt; ‚Äì liquid‚Äëlike water is ~5 √ó denser, giving a Reynolds number ~5 √ó larger for the same velocity and pipe diameter.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Viscosity&lt;/strong&gt; ‚Äì steam is slightly less viscous, which &lt;em&gt;increases&lt;/em&gt; its Reynolds number a little, but the dominant factor is density.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thermal conductivity&lt;/strong&gt; ‚Äì liquid water conducts heat ‚âà 55 % better.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prandtl number&lt;/strong&gt; ‚Äì liquid water has a &lt;strong&gt;high Pr&lt;/strong&gt; (‚âà 7) while steam has a &lt;strong&gt;low Pr&lt;/strong&gt; (‚âà 0.85). A high Pr means the thermal boundary layer is thinner than the velocity boundary layer, boosting h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Quantitative h‚Äëcomparison (forced turbulent flow)&lt;/h1&gt; &lt;h1&gt;4.1. Chosen flow parameters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Pipe diameter &lt;em&gt;D&lt;/em&gt;=0.020m (20 mm)&lt;/li&gt; &lt;li&gt;Mean velocity &lt;em&gt;U&lt;/em&gt;=2.0m s‚àí1 ‚Äì typical for high‚Äëpressure steam turbines or heat‚Äëexchanger tubes.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4.2. Reynolds numbers&lt;/h1&gt; &lt;p&gt;Both are &lt;strong&gt;turbulent&lt;/strong&gt; (Re &amp;gt; 4 000).&lt;/p&gt; &lt;h1&gt;4.3. Nusselt numbers (Gnielinski correlation ‚Äì valid for 0.1&amp;lt;Pr&amp;lt;2000 and 3000&amp;lt;Re&amp;lt;5√ó106)&lt;/h1&gt; &lt;p&gt;Carrying out the calculation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Fluid&lt;/th&gt; &lt;th align="left"&gt;Friction factor &lt;em&gt;f&lt;/em&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;em&gt;Nu&lt;/em&gt; (Gnielinski)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Liquid‚Äëlike water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;‚âà0.014&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;liq‚Äã‚âà1.0√ó103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Steam‚Äëlike water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;‚âà0.028&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;steam‚Äã‚âà3.1√ó102&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;4.4. Heat‚Äëtransfer coefficient&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, &lt;em&gt;under the same pressure, temperature, geometry and bulk velocity, liquid water transfers heat by convection roughly five times more efficiently than steam&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;5. Why the liquid is so much better (physics insight)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;Liquid water (high‚ÄëP)&lt;/th&gt; &lt;th align="left"&gt;Steam (high‚ÄëP)&lt;/th&gt; &lt;th align="left"&gt;Effect on h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density (œÅ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5√ó larger ‚Üí larger momentum ‚Üí higher Re ‚Üí thinner velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Low œÅ ‚Üí smaller Re ‚Üí thicker velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Higher Re ‚Üí higher Nu&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity (k)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;55 % larger ‚Üí more conductive path across the thermal BL&lt;/td&gt; &lt;td align="left"&gt;Smaller k ‚Üí poorer conduction&lt;/td&gt; &lt;td align="left"&gt;Directly multiplies Nu in h = Nu¬∑k/D&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number (Pr)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚âà 7 (thermal diffusion slower than momentum) ‚Üí thermal BL &lt;strong&gt;much thinner&lt;/strong&gt; than velocity BL&lt;/td&gt; &lt;td align="left"&gt;‚âà 0.85 (thermal diffusion faster) ‚Üí thermal BL comparable to or thicker than velocity BL&lt;/td&gt; &lt;td align="left"&gt;Thin thermal BL ‚Üí larger temperature gradient at wall ‚Üí larger h&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Viscosity (¬µ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Slightly higher, but the density effect dominates the Reynolds number&lt;/td&gt; &lt;td align="left"&gt;Lower ¬µ but low œÅ keeps Re low&lt;/td&gt; &lt;td align="left"&gt;Net: liquid still gets larger Re&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Compressibility&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Near‚Äëincompressible; flow properties change little with pressure&lt;/td&gt; &lt;td align="left"&gt;Gas‚Äëlike compressibility reduces effective velocity and thickens BL&lt;/td&gt; &lt;td align="left"&gt;Keeps liquid‚Äôs convective performance high&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;6. Influence of pressure itself&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Pressure change&lt;/th&gt; &lt;th align="left"&gt;Liquid water&lt;/th&gt; &lt;th align="left"&gt;Steam&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Increasing pressure&lt;/strong&gt; (while staying sub‚Äëcritical for the liquid)&lt;/td&gt; &lt;td align="left"&gt;œÅ ‚Üë, ¬µ ‚Üì (slightly) ‚Üí &lt;strong&gt;Re ‚Üë&lt;/strong&gt;; k ‚Üë modestly; Pr ‚Üë ‚Üí &lt;strong&gt;h rises&lt;/strong&gt; (often 10‚Äë30 % per 100 bar).&lt;/td&gt; &lt;td align="left"&gt;œÅ ‚Üë dramatically (‚âà 5‚Äì10√ó as you approach the critical point), ¬µ ‚Üì, k ‚Üë, Pr ‚Üë from ~0.7 toward 1‚Äë2. When steam becomes &lt;em&gt;dense&lt;/em&gt; (near‚Äëcritical), its Re and Pr increase enough that &lt;strong&gt;h can climb 2‚Äì4√ó&lt;/strong&gt;. However, even at 20 MPa the steam‚Äôs Pr never reaches the ~7 of liquid water, so &lt;em&gt;liquid still beats steam&lt;/em&gt; unless you cross into the &lt;em&gt;supercritical fluid&lt;/em&gt; regime where the distinction blurs.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Very near the critical point&lt;/strong&gt; (‚âà 22 MPa, 374 ¬∞C)&lt;/td&gt; &lt;td align="left"&gt;Water is a &lt;em&gt;supercritical fluid&lt;/em&gt; with properties intermediate between liquid and gas; h can be &lt;strong&gt;10‚Å¥‚Äì2√ó10‚Å¥ W m‚Åª¬≤ K‚Åª¬π&lt;/strong&gt; ‚Äì comparable to high‚Äëpressure liquid.&lt;/td&gt; &lt;td align="left"&gt;Same supercritical fluid ‚Äì the ‚Äústeam‚Äù label loses meaning; h values become similar to the liquid side.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Take‚Äëaway:&lt;/strong&gt; Pressure improves h for both phases, but because liquid water already starts with a much higher baseline, the &lt;em&gt;ratio&lt;/em&gt; &lt;em&gt;h&lt;/em&gt;liq‚Äã/&lt;em&gt;h&lt;/em&gt;steam‚Äã stays in the &lt;strong&gt;3‚Äì6 range&lt;/strong&gt; for most engineering‚Äërelevant pressures (5‚Äì20 MPa). Only when you go extremely close to the critical point does the gap shrink dramatically.&lt;/p&gt; &lt;h1&gt;7. Practical implications&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Application&lt;/th&gt; &lt;th align="left"&gt;Typical operating pressure &amp;amp; temperature&lt;/th&gt; &lt;th align="left"&gt;Which phase gives the larger convective h?&lt;/th&gt; &lt;th align="left"&gt;Design note&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;High‚Äëpressure water‚Äëto‚Äësteam heat exchangers (e.g., nuclear steam generators)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15 MPa, 300 ¬∞C liquid ‚Üí 15 MPa, 350 ¬∞C steam&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Liquid side&lt;/em&gt; (primary) has &lt;em&gt;h&lt;/em&gt; 4‚Äì6√ó larger than the steam side.&lt;/td&gt; &lt;td align="left"&gt;The steam side is usually the limiting resistance; designers boost its turbulence (twisted tapes, ribbed tubes) to compensate.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical CO‚ÇÇ heat recovery (analogy)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.5 MPa, 500 ¬∞C (fluid is supercritical)&lt;/td&gt; &lt;td align="left"&gt;The fluid behaves like a dense gas; h is &lt;strong&gt;lower&lt;/strong&gt; than a comparable liquid but &lt;strong&gt;higher&lt;/strong&gt; than low‚Äëpressure CO‚ÇÇ gas.&lt;/td&gt; &lt;td align="left"&gt;Use high‚ÄëRe flow (large velocities, small hydraulic diameters) to raise h.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geothermal wells (high‚ÄëP hot water, 10 MPa, 250 ¬∞C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fluid is liquid‚Äëlike&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Liquid&lt;/strong&gt; h ‚âà 2‚Äì3 √ó 10‚Å¥ W m‚Åª¬≤ K‚Åª¬π, far exceeding what the same well would deliver if the fluid flashed to steam.&lt;/td&gt; &lt;td align="left"&gt;Maintaining liquid conditions (prevent flashing) maximises convective heat extraction.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical water oxidation reactors (SCWO, &amp;gt; 25 MPa, &amp;gt; 400 ¬∞C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Water is supercritical (no distinct liquid/steam)&lt;/td&gt; &lt;td align="left"&gt;h values of &lt;strong&gt;10‚Å¥‚Äì2√ó10‚Å¥ W m‚Åª¬≤ K‚Åª¬π&lt;/strong&gt;; similar to high‚Äëpressure liquid water.&lt;/td&gt; &lt;td align="left"&gt;The high h is a key advantage for rapid heat removal in exothermic oxidation.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;8. Summary (concise bullet list)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Liquid water at high pressure&lt;/strong&gt; has &lt;strong&gt;much larger density&lt;/strong&gt; and &lt;strong&gt;higher Prandtl number&lt;/strong&gt; than steam at the same pressure/temperature.&lt;/li&gt; &lt;li&gt;These property differences raise the Reynolds number and shrink the thermal boundary layer, leading to &lt;strong&gt;higher Nusselt numbers&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Quantitatively, for a 20 mm tube, 2 m s‚Åª¬π flow, 10 MPa and 500 ¬∞C:‚Üí &lt;strong&gt;Liquid water convects roughly 5 √ó faster.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;h_liquid ‚âà 3‚Äì4 √ó 10‚Å¥ W m‚Åª¬≤ K‚Åª¬π&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;h_steam ‚âà 6‚Äì8 √ó 10¬≥ W m‚Åª¬≤ K‚Åª¬π&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Raising pressure improves h for &lt;strong&gt;both&lt;/strong&gt; fluids, but the &lt;strong&gt;ratio stays between 3 and 6&lt;/strong&gt; over the usual engineering range (5‚Äì20 MPa).&lt;/li&gt; &lt;li&gt;Near the critical point the distinction blurs; steam becomes a dense supercritical fluid and its h rises dramatically, approaching that of the liquid side.&lt;/li&gt; &lt;li&gt;In design practice the &lt;strong&gt;steam side is usually the thermal bottleneck&lt;/strong&gt; in high‚Äëpressure heat exchangers; engineers therefore add turbulence promoters, increase velocity, or operate at higher pressure to mitigate the lower h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Bottom line&lt;/h1&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Recipe7620"&gt; /u/Ill_Recipe7620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T04:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46mk9</id>
    <title>LongCat-Flash-Chat is here, yet another Chinese open weight model</title>
    <updated>2025-08-30T16:56:47+00:00</updated>
    <author>
      <name>/u/MindlessScrambler</name>
      <uri>https://old.reddit.com/user/MindlessScrambler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt; &lt;img alt="LongCat-Flash-Chat is here, yet another Chinese open weight model" src="https://external-preview.redd.it/SJG1eRbm3SwVjlU4Orqm4a5_PTL6rFKUpPzL-4SiNJI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d1f89904849c371c282657b5befc8d11c2c3998" title="LongCat-Flash-Chat is here, yet another Chinese open weight model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF: &lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat"&gt;https://huggingface.co/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/meituan-longcat/LongCat-Flash-Chat"&gt;https://github.com/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Web: &lt;a href="https://longcat.ai"&gt;https://longcat.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f"&gt;https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MindlessScrambler"&gt; /u/MindlessScrambler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n49hcg</id>
    <title>New AMD unified memory product - 512 bit bus = ~512GB/s memory bandwidth</title>
    <updated>2025-08-30T18:54:22+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent AMD leak hints at a new 512 bit memory bus for their unified memory systems. If so, a successor to the AI max would likely have 2x the memory bandwidth.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship"&gt;https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4dsym</id>
    <title>128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow.</title>
    <updated>2025-08-30T21:59:06+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt; &lt;img alt="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." src="https://preview.redd.it/ld3rckf8b8mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad5697c140ab4e2a23aa8df66f12cc106ee3b653" title="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ld3rckf8b8mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T21:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4garp</id>
    <title>Creating the brain behind dumb models</title>
    <updated>2025-08-30T23:55:53+00:00</updated>
    <author>
      <name>/u/ChristopherLyon</name>
      <uri>https://old.reddit.com/user/ChristopherLyon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt; &lt;img alt="Creating the brain behind dumb models" src="https://external-preview.redd.it/MWFtYzE2NDZ3OG1mMdkNi0cONUhOlI53Ev_Op4Ah4qsUiMoYY-mZ8OWM03HP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d27b6fcdb4c16b3d3a61e3f5d15d95327d5ebe8" title="Creating the brain behind dumb models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been fascinated by model intelligence enhancement and trying to deploy super tiny models like gemma3:270m in niche domains with high levels of success... &lt;/p&gt; &lt;p&gt;My latest implementation is a &amp;quot;community nested&amp;quot; relational graph knowledgebase pipeline that gives both top down context on knowledge sub-domains, but also a traditional bottom-up search (essentially regular semantic embedding cosine similarity) with a traversal mechanism to grab context from nodes that are not semantically similar but still referentially linked. Turns out there is a LOT of context that does not get picked up through regular embedding based RAG.&lt;/p&gt; &lt;p&gt;I created a quick front-end with nextjs and threejs to visualize how my knowledge base hangs together, and to quickly identify if I had a high level of overall coherence (i.e. number of isolated/disconnected clusters) and to get a better feeling for what context the LLM loads into memory for any given user query in real time (I'm a visual learner)&lt;/p&gt; &lt;p&gt;The KB you can see in the video is from a single 160 page PDF on Industrial Design, taking you anywhere from notable people, material science to manufacturing techniques. I was pleasantly surprised to see that the node for &amp;quot;ergonomics&amp;quot; was by far the most linked and overall strongly referenced in the corpus - essentially linking the &amp;quot;human factor&amp;quot; to some significant contribution to great product design.&lt;/p&gt; &lt;p&gt;If anyone hasn't gotten into graph based retrieval augmented generation I found the best resource and starter to be from Microsoft: &lt;a href="https://github.com/microsoft/graphrag"&gt;https://github.com/microsoft/graphrag&lt;/a&gt;&lt;/p&gt; &lt;p&gt;^ pip install graphrag and use the init and index commands to create your first graph in minutes.&lt;/p&gt; &lt;p&gt;Anyone else been in my shoes and already know what the NEXT step will be? Let me know.&lt;/p&gt; &lt;p&gt;It's 2 am so a quick video shot on my mobile is all I have right now, but I can't sleep thinking about this so thought I'd post what I have. I need to work some more on it and add the local LLM interface for querying the KB through the front end, but I don't mind open sourcing it if anyone is interested. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChristopherLyon"&gt; /u/ChristopherLyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bwyft136w8mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T23:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46ify</id>
    <title>Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)</title>
    <updated>2025-08-30T16:52:00+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt; &lt;img alt="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" src="https://preview.redd.it/1wl79kpjs6mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967356c2a208d26c7d657a1b535e795c0332f304" title="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wl79kpjs6mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI ‚Äî The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM ‚Äì 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
