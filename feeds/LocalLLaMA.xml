<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-03T18:41:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1omhby8</id>
    <title>Qwen 3 max thinking released.</title>
    <updated>2025-11-02T13:31:06+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try it &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1onh326</id>
    <title>Help on budget build with 8x 6700XT</title>
    <updated>2025-11-03T16:54:35+00:00</updated>
    <author>
      <name>/u/leobaillard</name>
      <uri>https://old.reddit.com/user/leobaillard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;It's my first post here. I have 8x RX 6700XT cards and I would like to use them in a budget (as budget as possible ^^) build for local AI inference for my company. I'd like to experiment with multiple models to see what we could do with such a rig.&lt;/p&gt; &lt;p&gt;I'm looking for advice on what type of hardware/software solutions would be best suited to make use of these cards and their vRAM.&lt;/p&gt; &lt;p&gt;I'm looking to run primarily coding models but if I can, maybe also a second, more general, model.&lt;/p&gt; &lt;p&gt;I currently have ordered an X99 board (4 usable PCI-E slots), an E5-2695 v3 and ~64GB of DDR4 3200 (if I can snag the sticks second hand), and looking to try to run 4 cards on it with each card running at 8x if possible and see what that gets me. I have read here that this approach would be better than trying with a dual-CPU board and more PCI-E slots so maybe 2 machines in tandem (a second, matching one with the other 4 cards)?&lt;/p&gt; &lt;p&gt;Thanks for your advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leobaillard"&gt; /u/leobaillard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onh326/help_on_budget_build_with_8x_6700xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onh326/help_on_budget_build_with_8x_6700xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onh326/help_on_budget_build_with_8x_6700xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T16:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1onhwt9</id>
    <title>Best model for low ram devices</title>
    <updated>2025-11-03T17:24:36+00:00</updated>
    <author>
      <name>/u/Green-Addition-8856</name>
      <uri>https://old.reddit.com/user/Green-Addition-8856</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My device has overall 16 GBs of RAM combined between CPU and GPU and I searched for multiple models that can fit in that range but I am still unsure,I think GPT-OSS-20B is good as I am not in need for advanced coding but I need moderate Agentic capabilities mainly for web search/image extraction I think I may use Unsloth version which only requeries 14 of combined RAM As I am running Ubuntu-based distro and system itself does not use more than like 5 percent of device resources,I am still not sure which quant should be used all of them are the same size,I am new to local AI so i am not sure which program to use or which model,any help would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Green-Addition-8856"&gt; /u/Green-Addition-8856 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhwt9/best_model_for_low_ram_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhwt9/best_model_for_low_ram_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onhwt9/best_model_for_low_ram_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T17:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8hnd</id>
    <title>Has anyone successfully used a local LLM for creative writing world-building?</title>
    <updated>2025-11-03T10:42:37+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beyond chat and coding, I'm trying to use a local model as a creative partner for building a fantasy novel's world - generating lore, character backstories, and consistent location descriptions.&lt;/p&gt; &lt;p&gt;Has anyone had real success with this? What was your process? Did you fine-tine on a specific corpus, or are you using clever prompting with a base model? What models have worked best for you for maintaining long-term consistency?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8hnd/has_anyone_successfully_used_a_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8hnd/has_anyone_successfully_used_a_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8hnd/has_anyone_successfully_used_a_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oncd4a</id>
    <title>Running Qwen 1.5B Fully On-Device on Jetson Orin Nano - No Cloud, Under 10W Power</title>
    <updated>2025-11-03T13:54:49+00:00</updated>
    <author>
      <name>/u/Founder_GenAIProtos</name>
      <uri>https://old.reddit.com/user/Founder_GenAIProtos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been exploring whatâ€™s truly possible with &lt;strong&gt;Edge AI&lt;/strong&gt;, and the results have been impressive. Managed to run &lt;strong&gt;Qwen 1.5B entirely on the Jetson Orin Nano&lt;/strong&gt; - with no cloud, no latency, and no data leaving the device.&lt;/p&gt; &lt;p&gt;Performance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;30 tokens/sec generation speed&lt;/li&gt; &lt;li&gt;Zero cloud dependency&lt;/li&gt; &lt;li&gt;No API costs&lt;/li&gt; &lt;li&gt;Runs under 10W of power&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Impressive to see this level of LLM performance on a compact device. Curious if others have tested Qwen models or Jetson setups for local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Founder_GenAIProtos"&gt; /u/Founder_GenAIProtos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oncd4a/running_qwen_15b_fully_ondevice_on_jetson_orin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oncd4a/running_qwen_15b_fully_ondevice_on_jetson_orin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oncd4a/running_qwen_15b_fully_ondevice_on_jetson_orin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T13:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1omxvbd</id>
    <title>Voice to LLM to Voice all in browser</title>
    <updated>2025-11-03T00:39:57+00:00</updated>
    <author>
      <name>/u/nullandkale</name>
      <uri>https://old.reddit.com/user/nullandkale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omxvbd/voice_to_llm_to_voice_all_in_browser/"&gt; &lt;img alt="Voice to LLM to Voice all in browser" src="https://external-preview.redd.it/andkMWhmc2N1eHlmMZnqQb6QAT_Zpu-mJr_VTzB2ofJ6yzR8C6yKW4qE1kiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1a85a2dceb12d87a26e238e1ea97deadf5798f9" title="Voice to LLM to Voice all in browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I slapped together Whisper.js, Llama 3.2 3B with Transformers.js, and Kokoro.js into a fully GPU accelerated p5.js sketch. It works well in Chrome on my desktop (chrome on my phone crashes trying to load the llm, but it should work). Because it's p5.js it's relatively easy to edit the scripts in real time in the browser. I should warn I'm a c++ dev not a JavaScript dev so alot of this code is LLM assisted. The only hard part was getting the tts to work. I would love to have some sort of voice cloning model or something where the voices are more configurable from the start. &lt;/p&gt; &lt;p&gt;&lt;a href="https://editor.p5js.org/NullandKale/full/ePLlRtzQ7"&gt;https://editor.p5js.org/NullandKale/full/ePLlRtzQ7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullandkale"&gt; /u/nullandkale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n6srrwrcuxyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omxvbd/voice_to_llm_to_voice_all_in_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omxvbd/voice_to_llm_to_voice_all_in_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T00:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1onfmr5</id>
    <title>Tool to generate datasets for finetuning local model</title>
    <updated>2025-11-03T16:01:31+00:00</updated>
    <author>
      <name>/u/Big_Tangelo_3697</name>
      <uri>https://old.reddit.com/user/Big_Tangelo_3697</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have asus tuf laptop with gpu rtx 5070 8gb. I wanted to create custom dataset for model fine tuning by using local based model on vllm. Which is the most preferred tool to generate q&amp;amp;a datasets etc. please guide&lt;/p&gt; &lt;p&gt;And the best approach also. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Tangelo_3697"&gt; /u/Big_Tangelo_3697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfmr5/tool_to_generate_datasets_for_finetuning_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfmr5/tool_to_generate_datasets_for_finetuning_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onfmr5/tool_to_generate_datasets_for_finetuning_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T16:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1onjm1u</id>
    <title>I made a simple tool to get deterministic, instant responses from my LLM setup</title>
    <updated>2025-11-03T18:24:18+00:00</updated>
    <author>
      <name>/u/MarkZealousideal7572</name>
      <uri>https://old.reddit.com/user/MarkZealousideal7572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on a project to solve a problem I'm sure many of you have seen: you get fantastic, fast responses from your local models, but if you ask the &lt;em&gt;exact same question&lt;/em&gt; in a slightly different way, the model has to run the full inference again.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Query 1: &amp;quot;how do I cancel my order&amp;quot;&lt;/code&gt; â†’ &lt;strong&gt;Full Generation (e.g., 5 seconds)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;Query 2: &amp;quot;I want to cancel an order&amp;quot;&lt;/code&gt; â†’ &lt;strong&gt;Full Generation (e.g., 5 seconds)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;Query 3: &amp;quot;what's the cancellation process&amp;quot;&lt;/code&gt; â†’ &lt;strong&gt;Full Generation (e.g., 5 seconds)&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This felt like a waste of resources, especially for common/repetitive queries in my apps (like for customer support or RAG).&lt;/p&gt; &lt;p&gt;So, I built &lt;code&gt;constraint-cache&lt;/code&gt;, a simple Python pattern that sits &lt;em&gt;in front&lt;/em&gt; of the LLM.&lt;/p&gt; &lt;p&gt;It's not semantic search. It's a deterministic normalization algorithm. It turns similar queries into a single, identical cache key.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;quot;how do I cancel my order&amp;quot;&lt;/code&gt; â†’ &lt;code&gt;normalize&lt;/code&gt; â†’ &lt;code&gt;&amp;quot;cancel_order&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;quot;I want to cancel an order&amp;quot;&lt;/code&gt; â†’ &lt;code&gt;normalize&lt;/code&gt; â†’ &lt;code&gt;&amp;quot;cancel_order&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;quot;what's the cancellation process&amp;quot;&lt;/code&gt; â†’ &lt;code&gt;normalize&lt;/code&gt; â†’ &lt;code&gt;&amp;quot;cancel_order&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The result:&lt;/strong&gt; The first query hits the LLM, but the next two are instant &lt;strong&gt;&amp;lt;1ms cache hits&lt;/strong&gt; from Redis.&lt;/p&gt; &lt;p&gt;For those of us building agentic workflows or UIs on top of local models, this has two huge benefits:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Massive Speed Up:&lt;/strong&gt; Your app feels &lt;em&gt;instantaneous&lt;/em&gt; for 90% of common user questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Deterministic:&lt;/strong&gt; You get the &lt;em&gt;exact&lt;/em&gt; same, perfect answer every time for that &amp;quot;intent,&amp;quot; which is great for testing and reliability. No more slightly different phrasing or hallucinations on solved problems.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I tested this on a 27,000-query customer support dataset and it got a &lt;strong&gt;99.9% cache hit rate&lt;/strong&gt; after the initial intents were cached.&lt;/p&gt; &lt;p&gt;It's all open-source, uses standard Redis, and is just a few lines of Python to implement. It's a perfect L1 cache to use before you even decide to hit your model.&lt;/p&gt; &lt;p&gt;Would love for you all to check it out, break it, and give me feedback.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/BitUnwiseOperator/constraint-cache"&gt;&lt;code&gt;https://github.com/BitUnwiseOperator/constraint-cache&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarkZealousideal7572"&gt; /u/MarkZealousideal7572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onjm1u/i_made_a_simple_tool_to_get_deterministic_instant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onjm1u/i_made_a_simple_tool_to_get_deterministic_instant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onjm1u/i_made_a_simple_tool_to_get_deterministic_instant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T18:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1onbqtv</id>
    <title>gemma-3-27b-it vs qwen3-32B (non-thinking)</title>
    <updated>2025-11-03T13:28:27+00:00</updated>
    <author>
      <name>/u/RepulsiveMousse3992</name>
      <uri>https://old.reddit.com/user/RepulsiveMousse3992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my experience, for general reasoning tasks (code, parsing data, following instructions, answering tricky questions), qwen3-32b seems strictly superior to gemma-3-27b, *if allowed to use thinking*.&lt;/p&gt; &lt;p&gt;But if you disable thinking for qwen3-32b how do they compare? Anyone got any experience with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RepulsiveMousse3992"&gt; /u/RepulsiveMousse3992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onbqtv/gemma327bit_vs_qwen332b_nonthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onbqtv/gemma327bit_vs_qwen332b_nonthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onbqtv/gemma327bit_vs_qwen332b_nonthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T13:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8ws6</id>
    <title>chatllm.cpp supports Ouro now</title>
    <updated>2025-11-03T11:07:56+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8ws6/chatllmcpp_supports_ouro_now/"&gt; &lt;img alt="chatllm.cpp supports Ouro now" src="https://b.thumbs.redditmedia.com/1Ar8UkXsE27N3kcOzmv_kqGXGGLMUWOIkRfwi6XF7LU.jpg" title="chatllm.cpp supports Ouro now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jl157x8ba0zf1.png?width=1002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b758004c74f8d5a8a56ecbc5562c838a0ef1839a"&gt;https://preview.redd.it/jl157x8ba0zf1.png?width=1002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b758004c74f8d5a8a56ecbc5562c838a0ef1839a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/foldl/chatllm.cpp"&gt;https://github.com/foldl/chatllm.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Customizable with additional options (&lt;code&gt;--set ...&lt;/code&gt;)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;total_ut_steps&lt;/code&gt;: default 4&lt;/li&gt; &lt;li&gt;&lt;code&gt;exit_threshold&lt;/code&gt;: default 1.0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; IMO, &amp;quot;early exit&amp;quot; will not skip future steps actually. (it will cause significant performance degradation)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ouro&lt;/strong&gt; is a parameter Looped Language Model (LoopLM) that achieves exceptional parameter efficiency through iterative shared-weight computation.&lt;/p&gt; &lt;p&gt;Discussions about Ouro:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8ws6/chatllmcpp_supports_ouro_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8ws6/chatllmcpp_supports_ouro_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8ws6/chatllmcpp_supports_ouro_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T11:07:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ompw8z</id>
    <title>Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' ðŸ”¬</title>
    <updated>2025-11-02T19:08:52+00:00</updated>
    <author>
      <name>/u/ComputeVoid</name>
      <uri>https://old.reddit.com/user/ComputeVoid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt; &lt;img alt="Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' ðŸ”¬" src="https://b.thumbs.redditmedia.com/vpnIZbZZCCRST1wmFgw4z_-ra4mD9_QOiBZ9JWlIHpw.jpg" title="Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' ðŸ”¬" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent a lot of time learning how language models work, but images obviously aren't language â€“ so how is it possible for AI to understand an image? I studied Gemma 3 to learn about how modern vision language models work.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The core finding:&lt;/strong&gt; Vision language models are just language models that learned to &amp;quot;speak image&amp;quot;. Images get encoded as tokens in linguistic space, and then the language model processes them identically to text.&lt;/p&gt; &lt;p&gt;So, if visual information gets translated into linguistic space, can we interpret the image tokens by mapping them to vocabulary space? I built an unembedding technique to answer that question and analyze what semantic information is encoded in the image tokens.&lt;/p&gt; &lt;h1&gt;Background: How VLMs Work&lt;/h1&gt; &lt;p&gt;Here's a diagram I created for my video that I think is helpful:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rk2m9rsk5wyf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1149132fb1b0148c3684a54a14bcb3a7f84cb8ae"&gt;https://preview.redd.it/rk2m9rsk5wyf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1149132fb1b0148c3684a54a14bcb3a7f84cb8ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, there are two pieces: the vision tower + a standard language model. The vision tower is quite literally bolted on to a normal language model.&lt;/p&gt; &lt;p&gt;For Gemma 3 specifically, the data flow is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Preprocessing: Convert image â†’ 3 Ã— 896 Ã— 896 pixels&lt;/li&gt; &lt;li&gt;Vision transformer: Process pixels â†’ 4,096 image tokens&lt;/li&gt; &lt;li&gt;Multimodal projector: Compress 4,096 tokens â†’ 256 tokens (semantically meaningful in language model's d_model space)&lt;/li&gt; &lt;li&gt;Language model: Image tokens and text tokens processed identically&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The brilliance is the multimodal projector â€“ it translates visual information into linguistic space.&lt;/p&gt; &lt;h1&gt;Method: Unembedding Image Tokens&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Validation:&lt;/strong&gt; First, I validated the technique with text tokens. By taking a token embedding and passing it directly through the language head (bypassing the transformer layers), I could recover the original token with 100% accuracy. This proves that unembedding works for linguistic tokens.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applying to images:&lt;/strong&gt; The same technique can be applied to image tokens:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Image â†’ Vision Tower â†’ Multimodal Projector â†’ 256 image tokens â†’ Unembed each token &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is greedy unembedding â€“ finding the nearest vocabulary token to any embedding vector. Since this is a nearest neighbor approach, it's lossy. The reality is that image tokens live in linguistic space but don't necessarily map exactly to a single vocabulary token. An image token can exist between different vocabulary words in the embedding space.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Token Type&lt;/th&gt; &lt;th align="left"&gt;Embedding Space Behavior&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Text tokens&lt;/td&gt; &lt;td align="left"&gt;Map 1:1 to a place in embedding space â€“ each token in the vocabulary has exactly 1 vector representation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Image tokens&lt;/td&gt; &lt;td align="left"&gt;Have vector representations that seem to exist &lt;em&gt;between&lt;/em&gt; text tokens&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;What I Found&lt;/h1&gt; &lt;p&gt;Here's what the unembedding revealed for different image types (see the linked notebook for more):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Purple square (monocolor):&lt;/strong&gt; The model correctly identifies the dominant color&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l2c7hko55wyf1.png?width=470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ffdc04268e03edea1c1ec69bb18ac3b2fbc703e"&gt;https://preview.redd.it/l2c7hko55wyf1.png?width=470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ffdc04268e03edea1c1ec69bb18ac3b2fbc703e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mountain scene (sunrise over mountains):&lt;/strong&gt; Rich semantic encoding: proper nouns, landscape features, time of day&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eeq8zw075wyf1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=454981867d6106775ab90668ba28f022b257d722"&gt;https://preview.redd.it/eeq8zw075wyf1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=454981867d6106775ab90668ba28f022b257d722&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;The &amp;quot; the&amp;quot; phenomenon:&lt;/em&gt; Across all image types, a large percentage of tokens map to &amp;quot; the&amp;quot;. Since &amp;quot; the&amp;quot; is usually the most common token in training data, it likely occupies a central location in embedding space. This might reveal either that not all image tokens are informative, or it might expose a limitation of greedy unembedding: when image tokens don't map cleanly to a single vocabulary word, the nearest neighbor defaults to the most &amp;quot;central&amp;quot; token â€“ there may be information encoded that greedy nearest-neighbor decoding can't reveal.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Semantic emergence:&lt;/em&gt; Even with the &amp;quot;the&amp;quot; dominance, semantically meaningful tokens do emerge â€“ colors, landscape features, proper nouns. The language model's understanding of images is messy, but there's signal in the noise.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implications &amp;amp; Open Questions&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Implication: The 256-Token Bottleneck: Feature, Not Flaw?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The multimodal projector compresses 4,096 visual patches down to 256 tokens. At first, this seemed like a clear limitation â€“ you're losing information in that compression. There is only so much that can be encoded in 256 tokens, right?&lt;/p&gt; &lt;p&gt;There has been some buzz recently about the DeepSeek-OCR paper and how image tokens can be used as a form of compression. This got me thinking about the 256-token budget differently.&lt;/p&gt; &lt;p&gt;Remember that image tokens exist &lt;em&gt;between&lt;/em&gt; text tokens in embedding space. A text token maps 1:1 to exactly one vocabulary word. But an image token isn't constrained to discrete vocabulary positions â€“ it can exist anywhere in the continuous embedding space between multiple words. This means a single image token can simultaneously encode aspects of multiple concepts.&lt;/p&gt; &lt;p&gt;In other words, &lt;em&gt;image tokens have higher information density than text tokens.&lt;/em&gt; Each of the 256 image tokens can encode more nuanced information than a discrete text token could.&lt;/p&gt; &lt;p&gt;This reframes the 256-token &amp;quot;bottleneck&amp;quot; â€“ maybe it's not a limitation but an efficient compression that can be exploited.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open Question: Positional Encoding: Distributed or Discrete?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Someone asked me recently how positional information in an image gets encoded in the vision tokens. I don't have a good answer, but I think it's a really interesting question. Positional information is obviously encoded somewhere, but where? Is it very distributed across the 256? Or are there specific token positions that effectively act as positional experts? How is information encoded across the 256 token budget?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;1 giant pool&lt;/em&gt; (each token plays a small role in constructing what appears as an aggregate meaning when looking at all 256)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;OR&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;256 smaller pools&lt;/em&gt; (each token is more of a specialist, i.e., the 0th position vision token serves a different function than the 255th)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My gut tells me the 1 giant pool idea seems more likely to me. But, as I've learned with VLMs, the reality is probably somewhere in the middle, and quite messy and hard to study! But I bet there is some cool stuff to discover with more sophisticated techniques.&lt;/p&gt; &lt;h1&gt;Want to Explore More?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://youtu.be/NpWP-hOq6II?si=Qun_EsWq7LLQ4ugw"&gt;&lt;strong&gt;&amp;quot;Dissecting Vision Language Models: How AI Sees&amp;quot;&lt;/strong&gt;&lt;/a&gt; â€“ My 20-min video walkthrough going deeper into VLM architecture and the unembedding technique&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jacob-danner/dissecting-vlm/blob/main/dissecting_vlm.ipynb"&gt;&lt;strong&gt;GitHub repo with notebook&lt;/strong&gt;&lt;/a&gt; â€“ Clone the repo and try unembedding your own images to see what the model &amp;quot;sees&amp;quot; in linguistic space&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=mLgyZ5GauhM&amp;amp;t=6s"&gt;&lt;strong&gt;Teaching AI to See: A Technical Deep-Dive on Vision Language Models with Will Hardman of Veratai&lt;/strong&gt;&lt;/a&gt; â€“ &lt;em&gt;Cognitive Revolution&lt;/em&gt; podcast episode that's an excellent comprehensive map of the VLM landscape&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think vision language models are super fascinating, especially on the mechanistic interpretability side trying to understand what those image tokens actually represent. Let me know what you discover!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComputeVoid"&gt; /u/ComputeVoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T19:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oniabv</id>
    <title>We trained SLM-powered assistants for personal expenses summaries that you can run locally via Ollama.</title>
    <updated>2025-11-03T17:37:58+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oniabv/we_trained_slmpowered_assistants_for_personal/"&gt; &lt;img alt="We trained SLM-powered assistants for personal expenses summaries that you can run locally via Ollama." src="https://preview.redd.it/nqq5x0okv2zf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b84878a61657075f8f396781278a770e7b5de03e" title="We trained SLM-powered assistants for personal expenses summaries that you can run locally via Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We trained SLM assistants for personal expenses summaries - two Llama 3.2 models (1B and 3B parameters) that you can run &lt;em&gt;locally&lt;/em&gt; via Ollama! SLMs which are not finetuned perform poorly on function calling - on our demo task, the 3B model called the correct tool only in 24% cases. By comparison, GPT-OSS was correct 88% of the time. Our knowledge distillation and fine-tuning setup bridges this performance gap between SLMs and LLMs. Details in &lt;a href="https://github.com/distil-labs/Distil-expenses/edit/main/README.md"&gt;https://github.com/distil-labs/Distil-expenses/edit/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;1. Installation&lt;/h3&gt; &lt;p&gt;First, install &lt;a href="https://ollama.com"&gt;Ollama&lt;/a&gt;, following the instructions on their website.&lt;/p&gt; &lt;p&gt;Then set up the virtual environment: &lt;code&gt; python -m venv .venv . .venv/bin/activate pip install huggingface_hub pandas openai &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Available models hosted on huggingface: - &lt;a href="https://huggingface.co/distil-labs/Distil-expenses-Llama-3.2-3B-Instruct"&gt;distil-labs/Distil-expenses-Llama-3.2-3B-Instruct&lt;/a&gt; - &lt;a href="https://huggingface.co/distil-labs/Distil-expenses-Llama-3.2-1B-Instruct"&gt;distil-labs/Distil-expenses-Llama-3.2-1B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally, download the models from huggingface and build them locally: ``` hf download distil-labs/Distil-expenses-Llama-3.2-3B-Instruct --local-dir distil-model&lt;/p&gt; &lt;p&gt;cd distil-model ollama create expense_llama3.2 -f Modelfile ```&lt;/p&gt; &lt;h3&gt;2. Examples&lt;/h3&gt; &lt;p&gt;Sum: ``` What was my total spending on dining in January 2024?&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-01-01 to 2024-01-31 you spent 24.5 total on dining.&lt;/h2&gt; &lt;p&gt;Give me my total expenses from 5th February to 11th March 2024&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-02-05 to 2024-03-11 you spent 348.28 total.&lt;/h2&gt; &lt;p&gt;&lt;code&gt; Count: &lt;/code&gt; How many times did I go shopping over $100 in 2024?&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-01-01 to 2024-12-31 you spent 8 times over 100 on shopping.&lt;/h2&gt; &lt;p&gt;Count all my shopping under $100 in the first half of 2024&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-01-01 to 2024-06-30 you spent 6 times under 100 on shopping.&lt;/h2&gt; &lt;p&gt;```&lt;/p&gt; &lt;h3&gt;3. Fine-tuning setup&lt;/h3&gt; &lt;p&gt;The tuned models were trained using knowledge distillation, leveraging the teacher model GPT-OSS 120B. We used 24 train examples and complemented them with 2500 synthetic examples.&lt;/p&gt; &lt;p&gt;We compare the teacher model and both student models on 25 held-out test examples:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Correct (25)&lt;/th&gt; &lt;th&gt;Tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-OSS&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;0.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 3B (tuned)&lt;/td&gt; &lt;td&gt;21&lt;/td&gt; &lt;td&gt;0.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 1B (tuned)&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;0.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 3B (base)&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 1B (base)&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The training config file and train/test data splits are available under &lt;code&gt;data/&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;FAQ&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Q: Why don't we just use Llama3.X yB for this??&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We focus on small models (&amp;lt; 8B parameters), and these make errors when used out of the box (see 5.)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Q: The model does not work as expected&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: The tool calling on our platform is in active development! &lt;a href="https://www.linkedin.com/company/distil-labs/"&gt;Follow us on LinkedIn&lt;/a&gt; for updates, or &lt;a href="https://join.slack.com/t/distil-labs-community/shared_invite/zt-36zqj87le-i3quWUn2bjErRq22xoE58g"&gt;join our community&lt;/a&gt;. You can also try to rephrase your query.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Q: I want to use tool calling for my use-case&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Visit our &lt;a href="https://www.distillabs.ai"&gt;website&lt;/a&gt; and reach out to us, we offer custom solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nqq5x0okv2zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oniabv/we_trained_slmpowered_assistants_for_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oniabv/we_trained_slmpowered_assistants_for_personal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T17:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1on4h8q</id>
    <title>AMD AI Pro R9700 is great for inference with Vulkan!</title>
    <updated>2025-11-03T06:21:14+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got my hands on an AMD AI Pro R9700, its awesome for inference. I am running Qwen3-30b-a3b-Thinking-2507 and with vulkan on the default radv driver its giving me about 173 t/s gen and about 1929 t/s for prompt processing.&lt;/p&gt; &lt;p&gt;&lt;code&gt;âžœ bin ./llama-bench --model ~/models/Qwen3-30B-A3B-Thinking-2507-Q4_K_M.gguf&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded RPC backend from /home/naved/apps/llama-b6920-bin-ubuntu-vulkan-x64/build/bin/libggml-rpc.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;WARNING: radv is not a conformant Vulkan implementation, testing use only.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: Found 2 Vulkan devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1201) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: 1 = AMD Radeon Graphics (RADV RAPHAEL_MENDOCINO) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded Vulkan backend from /home/naved/apps/llama-b6920-bin-ubuntu-vulkan-x64/build/bin/libggml-vulkan.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded CPU backend from /home/naved/apps/llama-b6920-bin-ubuntu-vulkan-x64/build/bin/libggml-cpu-icelake.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 17.28 GiB | 30.53 B | Vulkan | 99 | pp512 | 1929.96 Â± 213.95 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 17.28 GiB | 30.53 B | Vulkan | 99 | tg128 | 173.03 Â± 0.79 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;build: d38d9f087 (6920)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Really great value for running local models for $1299! The great thing is I still have plenty of vram remaining for filling up the context.&lt;/p&gt; &lt;p&gt;Still playing around with others, and I have yet to see the performance on a dense model, but for now this looks great, and I am trying to see if I can use this model as a coding model for building something I am working on. &lt;/p&gt; &lt;p&gt;Looking forward to ideas/feedback to see if i can get even more performance out of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4h8q/amd_ai_pro_r9700_is_great_for_inference_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4h8q/amd_ai_pro_r9700_is_great_for_inference_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on4h8q/amd_ai_pro_r9700_is_great_for_inference_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T06:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1omr9rc</id>
    <title>Qwen3 VL 30b a3b is pure love</title>
    <updated>2025-11-02T20:02:36+00:00</updated>
    <author>
      <name>/u/Njee_</name>
      <uri>https://old.reddit.com/user/Njee_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its been a bit since that model is available as GGUF and can be used with llama.cpp. A quick test using OpenWebUI showed its pretty fast on a 3060 12G with the Experts on the CPU. &lt;/p&gt; &lt;p&gt;It takes only about 3.5 sec to process high quality phone images and generates responses with 30 t/s. While taking only 8 gb of VRAM.&lt;/p&gt; &lt;p&gt;Im using Unsloths q8 with mmproj-F32 file. &lt;/p&gt; &lt;p&gt;The model is so good that i actually continued to work on a project that i have left off for a couple of months, as i couldnt get models from OpenRouter to work reliably, as well as Googles Models via their API. Well those models reliably extracted the data that i needed, but somehow i did not manage to get good boxes or single point coordinates from them. &lt;/p&gt; &lt;p&gt;And what am I supposed to say? Qwen3 VL 30b a3b simply nails it. The whole thing works exactly the way I imagined it. I got really inspired to get back to this project and get it finally finished. As my programming skills are kinda meh, i turned on the vibecoding machine and played around. But now i can proudly present my new tool to create inventory lists from images. &lt;/p&gt; &lt;p&gt;Probably nothing special for many of you, but its the only useful thing I have done with AI so far. Therefore im really happy. &lt;/p&gt; &lt;p&gt;Enjoy this demo, where i setup a project, define the data that i need from the images and that is important for my inventory. Then take a couple of images from object front and back and then review the extracted data, check if its correct and then feed it into the inventory table. The Video is 2.5x sped up. &lt;/p&gt; &lt;p&gt;will share the project as a easily deployable docker container once i got the codebase a little bit tidied up, shouldnt be too much of work. &lt;/p&gt; &lt;p&gt;Some stats: The full precision mmproj and q8 of the LLM need about 7 seconds to encode 2 images (on the 3060). So it takes 7 seconds to understand the front and the back of my object.&lt;/p&gt; &lt;p&gt;It then needs 10 seconds to output json with the extracted data and the coordinates for 4 table columns. 4 columns of the table = 300 tokens. At 30t/s it takes 10 seconds. &lt;/p&gt; &lt;p&gt;In total this is less than 20 seconds per container. And i am really looking forward to build up some nice inventory lists from whatever i need listed. &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1omr9rc/video/wm6ts19kgwyf1/player"&gt;2.5x sped up. &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Njee_"&gt; /u/Njee_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T20:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8zl6</id>
    <title>MiniMax-M2 Asteroid game - Unsloth</title>
    <updated>2025-11-03T11:12:28+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8zl6/minimaxm2_asteroid_game_unsloth/"&gt; &lt;img alt="MiniMax-M2 Asteroid game - Unsloth" src="https://b.thumbs.redditmedia.com/v_JOWMapjxQlXq1fbgVBm7vR-T-xWQ9o_bEwh-ljehc.jpg" title="MiniMax-M2 Asteroid game - Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://pastebin.com/c2rAezEs"&gt;https://pastebin.com/c2rAezEs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mxhf3813y0zf1.png?width=1909&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6feb5ca56de1c506fb3c00131b32912899de4d18"&gt;MiniMax-M2 Asteroid game&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to test this model by asking it to run the Asteroid game in HTML.&lt;/p&gt; &lt;p&gt;What surprised me?&lt;/p&gt; &lt;p&gt;1) 9~10 tokens/sec on DDR4 3200 + 5070ti. Faster than GLM 4.6 q2 despite being q3.&lt;/p&gt; &lt;p&gt;2) The code didn't work on the first pass; I copied the errors from the Chrome console, and fixed them 100% on the second pass.&lt;/p&gt; &lt;p&gt;3) This is the first time I've seen audio and VFX integrated without asking anything.&lt;/p&gt; &lt;p&gt;What I love about this model is that it thinks, but very little compared to Qwen and GLM.&lt;/p&gt; &lt;p&gt;llama-server.exe --model &amp;quot;C:\gptmodel\unsloth\MiniMax-M2-GGUF\MiniMax-M2-UD-Q3_K_XL-00001-of-00003.gguf&amp;quot; --n-gpu-layers 63 --flash-attn on --tensor-split 99,0 --cpu-moe --ctx-size 32768 --threads 16 --parallel 1 --host &lt;a href="http://127.0.0.1"&gt;127.0.0.1&lt;/a&gt; --port 8080 --top-p 0.95 --top-k 40 --ubatch-size 512 --seed 3407 --no-mmap &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8zl6/minimaxm2_asteroid_game_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8zl6/minimaxm2_asteroid_game_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8zl6/minimaxm2_asteroid_game_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T11:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1onc0hn</id>
    <title>My cheapest &amp; most consistent approach for AI 3D models so far - MiniMax-M2</title>
    <updated>2025-11-03T13:39:51+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onc0hn/my_cheapest_most_consistent_approach_for_ai_3d/"&gt; &lt;img alt="My cheapest &amp;amp; most consistent approach for AI 3D models so far - MiniMax-M2" src="https://preview.redd.it/fwg2juf4o1zf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ddf149fb8f9ba5ba949bc1b468af863cecadb62" title="My cheapest &amp;amp; most consistent approach for AI 3D models so far - MiniMax-M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with MiniMax2 locally for 3D asset generation and wanted to share some early results. I'm finding it surprisingly effective for agentic coding tasks (like tool calling). Especially like the balance of speed/cost &amp;amp; consistent quality compared to the larger models I've tried.&lt;/p&gt; &lt;p&gt;This is a &amp;quot;Jack O' Lantern&amp;quot; I generated with a prompt to an &lt;a href="https://native-blend-app.vercel.app/"&gt;agent&lt;/a&gt; using MiniMax2, and I've been able to add basic lighting and carving details pretty reliably with the pipeline.&lt;/p&gt; &lt;p&gt;Curious if anyone else here is using local LLMs for creative tasks, or what techniques you're finding for efficient generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fwg2juf4o1zf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onc0hn/my_cheapest_most_consistent_approach_for_ai_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onc0hn/my_cheapest_most_consistent_approach_for_ai_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T13:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1on43yj</id>
    <title>Is anyone else noticing fewer updates on LMArena lately? The last updates are weeks apart</title>
    <updated>2025-11-03T05:58:41+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on43yj/is_anyone_else_noticing_fewer_updates_on_lmarena/"&gt; &lt;img alt="Is anyone else noticing fewer updates on LMArena lately? The last updates are weeks apart" src="https://preview.redd.it/dual1zrnezyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87e6e9115f77cba0a5350009294a8d29a1b43720" title="Is anyone else noticing fewer updates on LMArena lately? The last updates are weeks apart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dual1zrnezyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on43yj/is_anyone_else_noticing_fewer_updates_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on43yj/is_anyone_else_noticing_fewer_updates_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T05:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1on423j</id>
    <title>RTX Pro 6000 Blackwell gets 19.3 tok/sec on 72B AWQ 8bit</title>
    <updated>2025-11-03T05:55:33+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just FYI, if you're looking to get a Pro 6000 Blackwell to be able to run ~70B dense models... long story short it's not a good idea.&lt;/p&gt; &lt;p&gt;Details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Workstation Edition&lt;/li&gt; &lt;li&gt;No power limit (600W)&lt;/li&gt; &lt;li&gt;vLLM 0.11.0&lt;/li&gt; &lt;li&gt;CUDA 12.8.0&lt;/li&gt; &lt;li&gt;Model: cpatonn/KAT-Dev-72B-Exp-AWQ-8bit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve models/KAT-Dev-72B-Q8 --enable-prefix-caching --served-model-name KAT-Dev-72B-Q8 --gpu-memory-utilization 0.95 --chat-template models/KAT-Dev-72B-Q8/chat_template.jinja --max-model-len 32000 --enable-auto-tool-choice --tool-call-parser qwen3_coder --tool-parser-plugin models/KAT-Dev-72B-Q8/qwen3coder_tool_parser.py --trust-remote-code --host 0.0.0.0 --port 8181 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For short &amp;quot;Hello&amp;quot; prompts I'm getting around 19 tok/sec TG, which is quite slow considering it's already fully offloaded... haven't bothered to check longer contexts.&lt;/p&gt; &lt;p&gt;P.S. on the flip side, GLM 4.5 Air @ UD-Q5_K_XL nets you 100+ tok/sec with full offload and 64k context :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on423j/rtx_pro_6000_blackwell_gets_193_toksec_on_72b_awq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on423j/rtx_pro_6000_blackwell_gets_193_toksec_on_72b_awq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on423j/rtx_pro_6000_blackwell_gets_193_toksec_on_72b_awq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T05:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1omst7q</id>
    <title>Polish is the most effective language for prompting AI, study reveals</title>
    <updated>2025-11-02T21:02:40+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"&gt; &lt;img alt="Polish is the most effective language for prompting AI, study reveals" src="https://external-preview.redd.it/HLkT8hEFTM_i4ECT9hWztFsoDf9RouYG6ZWJzAhQOUY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e82f2cea2a7637a951d648c1b316bc8d9248d9c" title="Polish is the most effective language for prompting AI, study reveals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.euronews.com/next/2025/11/01/polish-to-be-the-most-effective-language-for-prompting-ai-new-study-reveals"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T21:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1onfjk6</id>
    <title>multi-model coding agents hitting 76% on swe-bench. could we replicate this with local models?</title>
    <updated>2025-11-03T15:58:23+00:00</updated>
    <author>
      <name>/u/rwhitman05</name>
      <uri>https://old.reddit.com/user/rwhitman05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw some benchmark results where a coding agent hit 76.1% on swe-bench verified using multi-model approach&lt;/p&gt; &lt;p&gt;the interesting part: different models for different tasks. one for navigation, one for coding, one for review. plus auto-verification loop&lt;/p&gt; &lt;p&gt;got me thinking - could we build something similar with local models? or are we not there yet?&lt;/p&gt; &lt;p&gt;different models have different strengths right. some are better at &amp;quot;find this function across 50k lines&amp;quot; vs &amp;quot;write this specific function&amp;quot;&lt;/p&gt; &lt;p&gt;like if youre fixing a bug that touches multiple files, one model finds all references, another writes the fix, then checks for side effects. makes sense to use specialized models instead of one doing everything&lt;/p&gt; &lt;p&gt;auto-verification is interesting. writes code, runs tests, fails, fixes bug, runs tests again. repeat until pass. basically automates the debug cycle&lt;/p&gt; &lt;p&gt;so could this work locally? thinking qwen2.5-coder for coding, deepseek for navigation, maybe another for review. orchestration with langchain or custom code. verification is just pytest/eslint running automatically&lt;/p&gt; &lt;p&gt;main challenges would be context management across models, when to switch models, keeping them in sync. not sure how hard that is&lt;/p&gt; &lt;p&gt;that benchmark used thinking tokens which helped (+0.7% improvement to 76.1%)&lt;/p&gt; &lt;p&gt;wondering if local models could get to 60-70% with similar architecture. would still be super useful. plus you get privacy and no api costs&lt;/p&gt; &lt;p&gt;has anyone tried multi-model orchestration locally? what models would you use? qwen? deepseek? llama? how would you handle orchestration?&lt;/p&gt; &lt;p&gt;saw some commercial tools doing this now (verdent got that 76% score, aider with different models, cursor's multi-model thing) but wondering if we can build it ourselves with local models&lt;/p&gt; &lt;p&gt;or is this just not feasible yet. would love to hear from anyone whos experimented with this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rwhitman05"&gt; /u/rwhitman05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T15:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8kye</id>
    <title>MiniMax LLM head confirms: new model M2.1 coming soon</title>
    <updated>2025-11-03T10:48:13+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt; &lt;img alt="MiniMax LLM head confirms: new model M2.1 coming soon" src="https://b.thumbs.redditmedia.com/zeE60rAYP1oJJCxWUZL0idGJf1QomfF834XO5Og7MvQ.jpg" title="MiniMax LLM head confirms: new model M2.1 coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pengyu Zhao, head of MiniMax LLM, said that to achieve the vision of &amp;quot;Intelligence with Everyone,&amp;quot; the company will continue open-sourcing its models to promote the ongoing development of the AI community. As part of the plan, he confirmed that the new model M2.1 will be released soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4tscghepu0zf1.jpg?width=1293&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f9636c4ecf40f3f278afca1a3391a3178bb32f88"&gt;https://preview.redd.it/4tscghepu0zf1.jpg?width=1293&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f9636c4ecf40f3f278afca1a3391a3178bb32f88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In social media interactions, when asked about the launch date of the subscription plan, Pengyu Zhao replied &amp;quot;very soon,&amp;quot; specifying it would be within one to two weeks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1omyytq</id>
    <title>Reporter: â€œPOLISH: THE SUPREME LANGUAGE OF AI.â€</title>
    <updated>2025-11-03T01:31:02+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt; &lt;img alt="Reporter: â€œPOLISH: THE SUPREME LANGUAGE OF AI.â€" src="https://preview.redd.it/jlwd6xkh3yyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ba1b50f272c2870a74364026d750bd194a9f243" title="Reporter: â€œPOLISH: THE SUPREME LANGUAGE OF AI.â€" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please read the paper before making any comments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2503.01996"&gt;https://arxiv.org/pdf/2503.01996&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jlwd6xkh3yyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T01:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1onaops</id>
    <title>âš¡ï¸ Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench</title>
    <updated>2025-11-03T12:41:41+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"&gt; &lt;img alt="âš¡ï¸ Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench" src="https://b.thumbs.redditmedia.com/jSQzd0HL5GuJP640MvEdr9aB6Jcc16z589SDLzlVOhs.jpg" title="âš¡ï¸ Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ðŸ‘‹ Trekking along the forefront of applied AI is rocky territory, but it is the best place to be! My RL trained multi-agent-coding model Orca-Agent-v0.1 reached a 160% higher relative score than its base model on Stanford's TerminalBench. Which is cool! The trek across RL was at times painful, and at other times slightly less painful ðŸ˜… I've open sourced everything.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I trained a 14B orchestrator model to better coordinate explorer &amp;amp; coder subagents (subagents are tool calls for orchestrator)&lt;/li&gt; &lt;li&gt;Scaled to 32x H100s that were pushed to their limits across 4 bare-metal nodes&lt;/li&gt; &lt;li&gt;Scaled to 256 Docker environments rolling out simultaneously, automatically distributed across the cluster&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-14B jumped from &lt;strong&gt;7% â†’ 18.25%&lt;/strong&gt; on TerminalBench after training&lt;/li&gt; &lt;li&gt;Model now within striking distance of Qwen3-Coder-480B (19.7%)&lt;/li&gt; &lt;li&gt;Training was stable with smooth entropy decrease and healthy gradient norms&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key learnings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Intelligently crafted&amp;quot; reward functions pale in performance to simple unit tests. Keep it simple!&lt;/li&gt; &lt;li&gt;RL is not a quick fix for improving agent performance. It is still very much in the early research phase, and in most cases prompt engineering with the latest SOTA is likely the way to go.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training approach:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reward design and biggest learning: Kept it simple - **just unit tests**. Every &amp;quot;smart&amp;quot; reward signal I tried to craft led to policy collapse ðŸ˜…&lt;/p&gt; &lt;p&gt;Curriculum learning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stage-1: Tasks where base model succeeded 1-2/3 times (41 tasks)&lt;/li&gt; &lt;li&gt;Stage-2: Tasks where Stage-1 model succeeded 1-4/5 times&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Dataset: Used synthetically generated RL environments and unit tests&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have added lots more details in the repo:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;â­ï¸&lt;/strong&gt; &lt;a href="https://github.com/Danau5tin/Orca-Agent-RL"&gt;&lt;strong&gt;Orca-Agent-RL repo&lt;/strong&gt;&lt;/a&gt; - training code, model weights, datasets.&lt;/p&gt; &lt;p&gt;Huge thanks to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Taras for providing the compute and believing in open source&lt;/li&gt; &lt;li&gt;Prime Intellect team for building prime-rl and dealing with my endless questions ðŸ˜…&lt;/li&gt; &lt;li&gt;Alex Dimakis for the conversation that sparked training the orchestrator model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am sharing this because I believe agentic AI is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent TerminalBench benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1onaops"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T12:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1on628o</id>
    <title>Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation</title>
    <updated>2025-11-03T08:04:32+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt; &lt;img alt="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" src="https://b.thumbs.redditmedia.com/otiqqwWrYAPyBQSbvIHZ-yCKbfdiJZGic1vlE0jFFxk.jpg" title="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0hnvozwh10zf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab171458093a1ad5f07a0eaa42ac44e2c5ab5681"&gt;Google Official Statement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://techcrunch.com/2025/11/02/google-pulls-gemma-from-ai-studio-after-senator-blackburn-accuses-model-of-defamation/"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fortunately, we can still download the weights from HF and run them locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T08:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8qe5</id>
    <title>basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet</title>
    <updated>2025-11-03T10:57:38+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt; &lt;img alt="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" src="https://external-preview.redd.it/d240ODlsYmJ3MHpmMRIAV1OZPMFu-DibzoX2jf4rOivExvgg5eIy0W2GXihc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=279c74da96e009360fea0b2b573c2a5636ed406e" title="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models I used:&lt;/p&gt; &lt;p&gt;- RF-DETR â€“ a DETR-style real-time object detector. We fine-tuned it to detect players, jersey numbers, referees, the ball, and even shot types.&lt;/p&gt; &lt;p&gt;- SAM2 â€“ a segmentation and tracking. It re-identifies players after occlusions and keeps IDs stable through contact plays.&lt;/p&gt; &lt;p&gt;- SigLIP + UMAP + K-means â€“ vision-language embeddings plus unsupervised clustering. This separates players into teams using uniform colors and textures, without manual labels.&lt;/p&gt; &lt;p&gt;- SmolVLM2 â€“ a compact vision-language model originally trained on OCR. After fine-tuning on NBA jersey crops, it jumped from 56% to 86% accuracy.&lt;/p&gt; &lt;p&gt;- ResNet-32 â€“ a classic CNN fine-tuned for jersey number classification. It reached 93% test accuracy, outperforming the fine-tuned SmolVLM2.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- code: &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- blogpost: &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;https://blog.roboflow.com/identify-basketball-players&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- detection dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- numbers OCR dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/367omkbbw0zf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
