<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-20T11:50:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qhutc3</id>
    <title>Rust + Local LLMs: An Open-Source Claude Cowork with Skills</title>
    <updated>2026-01-20T07:39:52+00:00</updated>
    <author>
      <name>/u/Material_Seat_7842</name>
      <uri>https://old.reddit.com/user/Material_Seat_7842</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent this past weekend playing around with Claude Code and ended up building Open Cowork, an open-source alternative to Claude Cowork that I can fully self-host. The main reason I built it was to run everything entirely with local LLMs, without relying on any external APIs.&lt;/p&gt; &lt;p&gt;Open Cowork is written completely in Rust. I had never used Rust before, so it was a big learning experience. Starting from scratch means no Python bloat, no heavy dependencies, and no third-party agent SDKs. It’s just a small, fast binary that I can run anywhere.&lt;/p&gt; &lt;p&gt;Security was a top concern because the agents can execute code. Every task runs inside a temporary Docker container, which keeps things safe while still giving me full flexibility.&lt;/p&gt; &lt;p&gt;The biggest highlight for me is Local LLM support. You can run the whole system offline using Ollama or other local models. This gives you complete control over your data and keys while still letting the agents handle complex tasks.&lt;/p&gt; &lt;p&gt;It already comes with built-in skills for processing documents like PDFs and Excel files. I was surprised how useful it was right out of the box.&lt;/p&gt; &lt;p&gt;The project is live on GitHub: &lt;a href="https://github.com/kuse-ai/kuse_cowork"&gt;https://github.com/kuse-ai/kuse_cowork&lt;/a&gt; . It’s still very early, but I’m excited to see how others might use it with local LLMs for fully self-hosted AI workflows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Material_Seat_7842"&gt; /u/Material_Seat_7842 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhutc3/rust_local_llms_an_opensource_claude_cowork_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhutc3/rust_local_llms_an_opensource_claude_cowork_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhutc3/rust_local_llms_an_opensource_claude_cowork_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T07:39:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhyh6h</id>
    <title>Why does my local LLaMA give the exact same answer every time?</title>
    <updated>2026-01-20T11:18:29+00:00</updated>
    <author>
      <name>/u/SwitchDoesReddit</name>
      <uri>https://old.reddit.com/user/SwitchDoesReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am new to locally hosting LLM Models.&lt;/p&gt; &lt;p&gt;I am currently trying to run my first Local Model but I have a snag I can't understand.&lt;/p&gt; &lt;p&gt;I am hosting &lt;strong&gt;Llama-3.2-3B-Instruct-uncensored.Q8_0.gguf&lt;/strong&gt; on a &lt;strong&gt;Linux Mint&lt;/strong&gt; Machine.&lt;/p&gt; &lt;p&gt;I have added the &lt;strong&gt;script&lt;/strong&gt; that I am using to the end of the post.&lt;/p&gt; &lt;p&gt;If I run this script, with the same prompt, I get the exact same response.&lt;/p&gt; &lt;p&gt;For example, running this script produces the following story every time:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In the misty alleys of Paris, an old violinist sat perched on his stool, his music a poignant serenade to the dying light of day. As he drew his bow across the strings, a faint smile crept onto the face of the onlooker, a young woman with eyes as dark as the night. It was said she'd lost someone dear, but his melodies somehow echoed the sorrow into an aching longing for connection. The final, mournful notes faded, and the woman stepped forward, gently laying a small bouquet of wildflowers by his feet. In that fleeting moment, his music became a bittersweet solace that bridged their two worlds.&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;from llama_cpp import Llama from pathlib import Path BASE_DIR = Path(__file__).resolve().parent.parent # test_llm_local/ MODEL_PATH = BASE_DIR / &amp;quot;models&amp;quot; / &amp;quot;Llama-3.2-3B-Instruct-uncensored.Q8_0.gguf&amp;quot; llm = Llama( model_path=str(MODEL_PATH), seed=-1, verbose=False, ) resp = llm.create_chat_completion( messages=[ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a writing assistant.&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: ( &amp;quot;Write a short 5 sentence story on any topic of your choosing.&amp;quot; )}, ], max_tokens=200, temperature=1.1, top_p=0.95, top_k=100, ) print(resp[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;].strip()) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Additional Details:&lt;br /&gt; I am running this using: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python 3.12.3&lt;/li&gt; &lt;li&gt;llama_cpp_python - Version: 0.3.16&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SwitchDoesReddit"&gt; /u/SwitchDoesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhyh6h/why_does_my_local_llama_give_the_exact_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhyh6h/why_does_my_local_llama_give_the_exact_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhyh6h/why_does_my_local_llama_give_the_exact_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T11:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhkh2z</id>
    <title>I FP8 quantized GLM 4.7 Flash!</title>
    <updated>2026-01-19T23:28:43+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I know it ain't much, I finally decided to try and be the first out to fp8 quant a newly dropped model. I would love to hear feedback if you try it. Steps to get it running are in the README :) &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8"&gt;https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T23:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhwxu5</id>
    <title>native-devtools-mcp - An MCP server for testing native desktop applications</title>
    <updated>2026-01-20T09:49:47+00:00</updated>
    <author>
      <name>/u/SkyLunat1c</name>
      <uri>https://old.reddit.com/user/SkyLunat1c</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I've built an MCP server that tries to mimic the Chrome DevTools protocol but for native apps, mainly for testing GUIs.&lt;/p&gt; &lt;p&gt;These are first iterations so bugs abound, but I intend on fixing them up and adding more platform support in the near future - Windows next!&lt;/p&gt; &lt;p&gt;I'd be very grateful for any feedback, and if there's interest - I can post subsequent updates details here too.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/sh3ll3x3c/native-devtools-mcp"&gt;https://github.com/sh3ll3x3c/native-devtools-mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyLunat1c"&gt; /u/SkyLunat1c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwxu5/nativedevtoolsmcp_an_mcp_server_for_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwxu5/nativedevtoolsmcp_an_mcp_server_for_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwxu5/nativedevtoolsmcp_an_mcp_server_for_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T09:49:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhyogx</id>
    <title>[D] Releasing Reasoning-v1: A high-fidelity synthetic CoT dataset for logical reasoning (150+ samples, built on M4 Pro)</title>
    <updated>2026-01-20T11:29:42+00:00</updated>
    <author>
      <name>/u/Western-Doughnut4375</name>
      <uri>https://old.reddit.com/user/Western-Doughnut4375</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m the founder of DLTHA Labs and yesterday I released our first open-source asset: Dltha_Reasoning_v1&lt;/p&gt; &lt;p&gt;We want to address the scarcity of high-quality, structured reasoning data. This first batch contains 150+ high-fidelity synthetic samples focused on Chain-of-Thought (CoT), Logic, and Algorithms.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Generated using a local pipeline on Apple M4 Pro and NVIDIA CUDA.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Mistral-7B (fine-tuned prompt engineering for PhD-level logic).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Apache 2.0 (fully open).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We are scaling to 1,500+ samples by next week to provide a solid foundation for local LLM fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/Dltha-Labs/dltha_reasoning_v1.jsonl"&gt;https://huggingface.co/datasets/Dltha-Labs/dltha_reasoning_v1.jsonl&lt;/a&gt; &lt;strong&gt;GitHub (demo code and dataset):&lt;/strong&gt; &lt;a href="https://github.com/DlthaTechnologies/dltha_reasoning_v1"&gt;https://github.com/DlthaTechnologies/dltha_reasoning_v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to get your feedback, please send it here -&amp;gt; [&lt;a href="mailto:contact@dltha.com"&gt;contact@dltha.com&lt;/a&gt;](mailto:&lt;a href="mailto:contact@dltha.com"&gt;contact@dltha.com&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western-Doughnut4375"&gt; /u/Western-Doughnut4375 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhyogx/d_releasing_reasoningv1_a_highfidelity_synthetic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhyogx/d_releasing_reasoningv1_a_highfidelity_synthetic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhyogx/d_releasing_reasoningv1_a_highfidelity_synthetic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T11:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhrdia</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-01-20T04:34:33+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/FluQr8-T2xSzepE7O56z1wr9rHYSGXZbAvZ94jKYf2w.jpg" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FLUX.2 [klein] - Consumer GPU Image Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs on consumer GPUs (13GB VRAM), generates high-quality images in under a second.&lt;/li&gt; &lt;li&gt;Handles text-to-image, editing, and multi-reference generation in one model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"&gt;Blog&lt;/a&gt; | &lt;a href="https://bfl.ai/models/flux-2-klein#try-demo"&gt;Demo&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/black-forest-labs/flux2"&gt;Models&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://i.redd.it/7vq4pfm0nfeg1.gif"&gt;https://i.redd.it/7vq4pfm0nfeg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pocket TTS - Lightweight Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lightweight, CPU-friendly open text-to-speech application.&lt;/li&gt; &lt;li&gt;Local speech synthesis without proprietary services.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://kyutai.org/tts"&gt;Demo&lt;/a&gt; | &lt;a href="https://github.com/kyutai-labs/pocket-tts"&gt;GitHub Repository&lt;/a&gt; | &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;Hugging Face Model Card&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2509.06926"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/kyutai-labs/pocket-tts/tree/main/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ministral 3 - Edge-Ready Multimodal Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compact open models (3B, 8B, 14B) with image understanding for edge devices.&lt;/li&gt; &lt;li&gt;Run multimodal tasks locally without cloud dependencies.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/mistralai/ministral-3"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2601.08584"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5fwsc0zymfeg1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e5bfafefd5d98665badb3f9eac21886386bf65e"&gt;https://preview.redd.it/5fwsc0zymfeg1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e5bfafefd5d98665badb3f9eac21886386bf65e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;STEP3-VL-10B - Efficient Multimodal Intelligence&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10B parameter model with frontier-level visual perception and reasoning.&lt;/li&gt; &lt;li&gt;Proves you don't need massive models for high-level multimodal intelligence.&lt;/li&gt; &lt;li&gt;h&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;ugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2601.09668"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uk3qg0z3nfeg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670e4e3902a6a1609db3b135be4801769493ae27"&gt;https://preview.redd.it/uk3qg0z3nfeg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670e4e3902a6a1609db3b135be4801769493ae27&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TranslateGemma - Open Translation Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Google's open translation models (4B, 12B, 27B) supporting 55 languages.&lt;/li&gt; &lt;li&gt;Fully open multilingual translation models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/GoogleDeepMind/status/2011848249850630363?s=20"&gt;Announcement&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;FASHN Human Parser - Fashion Image Segmentation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open fine-tuned SegFormer for parsing humans in fashion images.&lt;/li&gt; &lt;li&gt;Specialized open model for fashion applications.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/fashn-ai/fashn-human-parser"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/przknaqrmfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36c3976c5e63bd33a68936986ee3f923a8a055"&gt;https://preview.redd.it/przknaqrmfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36c3976c5e63bd33a68936986ee3f923a8a055&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek Engram - Memory Module for LLMs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lookup-based memory module for faster knowledge retrieval.&lt;/li&gt; &lt;li&gt;Improves efficiency of local LLM deployments.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/Engram/tree/main"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ShowUI-Aloha - GUI Automation Agent&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Flow-based model that learns to use GUIs from human demonstrations.&lt;/li&gt; &lt;li&gt;Generates smooth mouse movements and clicks for workflow automation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://showlab.github.io/Aloha_Page/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/showlab/ShowUI-Aloha"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qhrdia/video/ewq89rktmfeg1/player"&gt;https://reddit.com/link/1qhrdia/video/ewq89rktmfeg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-Qwen-Image-V2 - Peak Realism Image Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Community fine-tuned Qwen-Image model built for photorealism.&lt;/li&gt; &lt;li&gt;Open alternative for realistic image generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/wikeeyang/Real-Qwen-Image-V2"&gt;Model&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fty6rpiumfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad94c0cd39fe6a97c018bbe3f31f0ec6717ee830"&gt;https://preview.redd.it/fty6rpiumfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad94c0cd39fe6a97c018bbe3f31f0ec6717ee830&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/last-week-in-multimodal-ai-41-vision?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1qbala2"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhx6u1</id>
    <title>no problems with GLM-4.7-Flash</title>
    <updated>2026-01-20T10:04:33+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"&gt; &lt;img alt="no problems with GLM-4.7-Flash" src="https://b.thumbs.redditmedia.com/RyfGOsESx4YsXYbZlTJQbs2Km0GhT7KhWobtlXZeH9A.jpg" title="no problems with GLM-4.7-Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw many comments that GLM-4.7-Flash doesn't work correctly, could you show specific prompts? I am not doing anything special, all settings are default&lt;/p&gt; &lt;p&gt;!!! UPDATE !!! - check the comments from &lt;a href="https://www.reddit.com/user/shokuninstudio/"&gt;shokuninstudio&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qhx6u1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhgi10</id>
    <title>lightonai/LightOnOCR-2-1B · Hugging Face</title>
    <updated>2026-01-19T20:57:11+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt; &lt;img alt="lightonai/LightOnOCR-2-1B · Hugging Face" src="https://external-preview.redd.it/owrWH9MOuE15-iASn4iPzZcG9U3KIDtVJ9SmxpvC1c0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d891c173f79ddf24b05c65d408e9287701ba72c2" title="lightonai/LightOnOCR-2-1B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqrl7</id>
    <title>DeepSeek V3.2 (open weights) beats GPT-5.2-Codex and Claude Opus on production code challenge — The Multivac daily blind peer eval</title>
    <updated>2026-01-20T04:05:23+00:00</updated>
    <author>
      <name>/u/Silver_Raspberry_811</name>
      <uri>https://old.reddit.com/user/Silver_Raspberry_811</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; DeepSeek V3.2 scored 9.39 to beat GPT-5.2-Codex (9.20) and every other closed model on a complex coding task. But the real story is Claude Sonnet 4.5 got scored anywhere from 3.95 to 8.80 by different judges — same exact code.&lt;/p&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;We asked 10 models to write a production-grade nested JSON parser with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Path syntax (&amp;quot;user.profile.settings.theme&amp;quot;)&lt;/li&gt; &lt;li&gt;Array indexing (&amp;quot;users[0].name&amp;quot;)&lt;/li&gt; &lt;li&gt;Circular reference detection&lt;/li&gt; &lt;li&gt;Typed results with error messages&lt;/li&gt; &lt;li&gt;Full type hints and docstrings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a real-world task. Every backend engineer has written something like this.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Std Dev&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek V3.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;9.39&lt;/td&gt; &lt;td align="left"&gt;0.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;9.20&lt;/td&gt; &lt;td align="left"&gt;0.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Grok 3&lt;/td&gt; &lt;td align="left"&gt;8.89&lt;/td&gt; &lt;td align="left"&gt;0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Grok Code Fast 1&lt;/td&gt; &lt;td align="left"&gt;8.46&lt;/td&gt; &lt;td align="left"&gt;1.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;8.16&lt;/td&gt; &lt;td align="left"&gt;0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;7.57&lt;/td&gt; &lt;td align="left"&gt;1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;7.02&lt;/td&gt; &lt;td align="left"&gt;2.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Pro&lt;/td&gt; &lt;td align="left"&gt;4.30&lt;/td&gt; &lt;td align="left"&gt;1.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;GLM 4.7&lt;/td&gt; &lt;td align="left"&gt;2.91&lt;/td&gt; &lt;td align="left"&gt;3.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;MiniMax M2.1&lt;/td&gt; &lt;td align="left"&gt;0.70&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Open weights won.&lt;/strong&gt; DeepSeek V3.2 is fully open.&lt;/p&gt; &lt;h1&gt;The Variance Problem (responding to yesterday's feedback)&lt;/h1&gt; &lt;p&gt;Yesterday &lt;a href="/u/Proud-Claim-485"&gt;u/Proud-Claim-485&lt;/a&gt; critiqued our methodology — said we're measuring &amp;quot;output alignment&amp;quot; not &amp;quot;reasoning alignment.&amp;quot;&lt;/p&gt; &lt;p&gt;Today's data supports this. Look at Claude Sonnet's std dev: &lt;strong&gt;2.03&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's a 5-point spread (3.95 to 8.80) on the same response. Judges fundamentally disagreed on what &amp;quot;good&amp;quot; means.&lt;/p&gt; &lt;p&gt;Compare to GPT-5.2-Codex with 0.50 std dev — everyone agreed within ~1 point.&lt;/p&gt; &lt;p&gt;When evaluators disagree this much, the benchmark is under-specified.&lt;/p&gt; &lt;h1&gt;Judge Strictness (meta-analysis)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Judge&lt;/th&gt; &lt;th align="left"&gt;Avg Score Given&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;5.92 (strictest)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;5.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;6.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek V3.2&lt;/td&gt; &lt;td align="left"&gt;7.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;9.11 (most lenient)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Claude models judge harshly but score mid-tier themselves. Interesting pattern.&lt;/p&gt; &lt;h1&gt;What We're Adding (based on your feedback)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;5 open-weight models for tomorrow:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama-3.3-70B-Instruct&lt;/li&gt; &lt;li&gt;Qwen2.5-72B-Instruct&lt;/li&gt; &lt;li&gt;Mistral-Large-2411&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Big-Tiger-Gemma-27B-v3&lt;/strong&gt; (&lt;a href="/u/ttkciar"&gt;u/ttkciar&lt;/a&gt; suggested this — anti-sycophancy finetune)&lt;/li&gt; &lt;li&gt;Phi-4&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;New evaluation dimension:&lt;/strong&gt; We're adding &amp;quot;reasoning justification&amp;quot; scoring — did the model explain its approach, not just produce correct-looking output?&lt;/p&gt; &lt;h1&gt;Methodology&lt;/h1&gt; &lt;p&gt;This is The Multivac — daily 10×10 blind peer matrix:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10 models respond to same question&lt;/li&gt; &lt;li&gt;Each model judges all 10 responses (100 total judgments)&lt;/li&gt; &lt;li&gt;Models don't know which response came from which model&lt;/li&gt; &lt;li&gt;Rankings from peer consensus, not single evaluator&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full responses and analysis: &lt;a href="https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://themultivac.com"&gt;themultivac.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions welcome. Roast the methodology. That's how we improve.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver_Raspberry_811"&gt; /u/Silver_Raspberry_811 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhg6rm</id>
    <title>GLM-4.7-FLASH-NVFP4 on huggingface (20.5 GB)</title>
    <updated>2026-01-19T20:45:46+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I published a mixed precision NVFP4 quantized version the new GLM-4.7-FLASH on HF, can any of you can test it and let me know how it goes, I would really appreciate it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4"&gt;https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhml0s</id>
    <title>With DRAM and NAND prices what they are, the DGX Spark almost seems like a bargain now LOL.</title>
    <updated>2026-01-20T00:57:58+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know a lot of the inference-focused crowd (myself included) were let down by the DGX Spark when it was released because of its weak memory bandwidth and high price tag. &lt;/p&gt; &lt;p&gt;Fast forward a few months and the whole consumer PC component market has turned into an absolute shitshow, RAM prices have quadrupled, now M2 prices are doing the same. That being said, if you break down the current retail market cost of the hardware components thar make up the DGX Spark, it’s sadly turned into a decent value from a solely HW component perspective. &lt;/p&gt; &lt;p&gt;Here’s a break down the core specs of the DGX Spark and what the market prices of the equivalent components would be (pulled these prices from Amazon US today) &lt;/p&gt; &lt;p&gt;- 128 GB of LPDDR5x RAM = $1600 (for 6000 MT/s, the DGX Spark has 8533 MT/s)&lt;/p&gt; &lt;p&gt;- 4TB M2 Gen5 SSD = $895&lt;/p&gt; &lt;p&gt;- 20 core CPU = $300&lt;/p&gt; &lt;p&gt;- Connectx-7 400 GB Nic (which the Spark has built-in = $1,197 &lt;/p&gt; &lt;p&gt;- 5070 GPU (which is what the DGX is said to be equivalent to from a pure GPU compute standpoint) = $639&lt;/p&gt; &lt;p&gt;Total current market prices of equivalent DGX Spark components = $4,631&lt;/p&gt; &lt;p&gt;DGX Spark Current price (4TB model) = $3,999&lt;/p&gt; &lt;p&gt;Estimated cost savings (if you bought a Spark instead of the components) = $632&lt;/p&gt; &lt;p&gt;I did not take into account Motherboard, Case, PSU, cooling, etc. You probably are looking at at least another $300 or more saved by getting the Spark, but I wasn’t really going to count those because the market prices for those components are pretty stable. &lt;/p&gt; &lt;p&gt;Anyways, I’m not advocating buying a Spark or anything like that, I just thought it was interesting that our mindset of what is a good deal vs. what isn’t a good deal is probably going to shift as DRAM and other component market prices get worse. My point is that 6 months ago, DGX Spark was a terrible perceived value proposition, but now in the current HW component market, maybe it’s not so bad. It is still pretty garbage for inference speed though except for some specific NVFP4 models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:57:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhaq21</id>
    <title>New in llama.cpp: Anthropic Messages API</title>
    <updated>2026-01-19T17:33:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt; &lt;img alt="New in llama.cpp: Anthropic Messages API" src="https://external-preview.redd.it/zqasF6xdAR1yVfMl-Ppz2b8-S-Dv35pa4J_UeKummLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56eabcfaa752210d59dc7af42f1b2087636a579d" title="New in llama.cpp: Anthropic Messages API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhx44i</id>
    <title>Compiled awesome reranker resources into one list</title>
    <updated>2026-01-20T10:00:21+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt; &lt;img alt="Compiled awesome reranker resources into one list" src="https://a.thumbs.redditmedia.com/rTzqbjRv7RniD0ivDYw-0Ay72od8XMAyCbeLHfr3dx8.jpg" title="Compiled awesome reranker resources into one list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/55s7lzc59heg1.png?width=1700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa05cd747a7065b96cd34e6499be0bcb78c1069d"&gt;https://preview.redd.it/55s7lzc59heg1.png?width=1700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa05cd747a7065b96cd34e6499be0bcb78c1069d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Been building RAG systems for a few months. Info on rerankers was scattered everywhere - docs, papers, Reddit threads. &lt;/p&gt; &lt;p&gt;Put it all in one place: &lt;a href="https://github.com/agentset-ai/awesome-rerankers"&gt;https://github.com/agentset-ai/awesome-rerankers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quick start code (works out of the box)&lt;/li&gt; &lt;li&gt;Model comparison table&lt;/li&gt; &lt;li&gt;Local options (FlashRank runs on CPU, ~4MB)&lt;/li&gt; &lt;li&gt;Framework integrations&lt;/li&gt; &lt;li&gt;Live benchmarks with ELO scores&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Rerankers give you a solid 15-40% accuracy boost over just vector search. But figuring out which one to use or whether you can run it locally was a pain.&lt;/p&gt; &lt;p&gt;This covers it. If you're building RAG, might save you some time.&lt;/p&gt; &lt;p&gt;Let me know if I missed anything useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjhlh</id>
    <title>GLM-4.7-Flash-GGUF is here!</title>
    <updated>2026-01-19T22:49:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt; &lt;img alt="GLM-4.7-Flash-GGUF is here!" src="https://external-preview.redd.it/xaz8me0jAeBOkTb7mKUXdYdIdr8aoSsiwENwulyOJmI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f21f70be7ae2e1b3f10f33471dbfc4c47ba6518" title="GLM-4.7-Flash-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhvkrb</id>
    <title>how i 10x my claude code results by giving it a local truth layer</title>
    <updated>2026-01-20T08:26:12+00:00</updated>
    <author>
      <name>/u/OpportunityFit8282</name>
      <uri>https://old.reddit.com/user/OpportunityFit8282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;if you’re using terminal agents for backend work, you know the hallucination struggle is real. i found a way to ground claude code using a local execution engine so it stops guessing what my apis do.&lt;/p&gt; &lt;p&gt;i’ve been documenting this &lt;strong&gt;claude code tutorial&lt;/strong&gt; workflow where i link my terminal to the &lt;strong&gt;apidog cli guide&lt;/strong&gt;. basically, instead of letting the llm assume a schema, i make it run a deterministic &lt;strong&gt;automated api testing guide&lt;/strong&gt; locally before it suggests any code changes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the loop:&lt;/strong&gt; i mapped my scenarios into .claude/skills/. now i just tell the agent to &amp;quot;fix the endpoint and verify.&amp;quot; it fires the apidog cli, checks the actual server response, and auto-corrects based on the logs. it's a huge time saver for local dev loops.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OpportunityFit8282"&gt; /u/OpportunityFit8282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhvkrb/how_i_10x_my_claude_code_results_by_giving_it_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhvkrb/how_i_10x_my_claude_code_results_by_giving_it_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhvkrb/how_i_10x_my_claude_code_results_by_giving_it_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T08:26:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5wdq</id>
    <title>zai-org/GLM-4.7-Flash · Hugging Face</title>
    <updated>2026-01-19T14:40:27+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.7-Flash · Hugging Face" src="https://external-preview.redd.it/Qs0t4y5eLm-uwORWdP6T0dcwW2T6VJyQFBUSY70CTF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8700f4a43fe16a1031ccda94b517fd709573a5c3" title="zai-org/GLM-4.7-Flash · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqzsi</id>
    <title>Mosquito - 7.3M parameter tiny knowledge model</title>
    <updated>2026-01-20T04:16:20+00:00</updated>
    <author>
      <name>/u/Lopsided-Repair-3638</name>
      <uri>https://old.reddit.com/user/Lopsided-Repair-3638</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A mosquito brain size model (7.3M params) that can answer surprisingly many general knowledge questions. Demo: &lt;a href="https://huggingface.co/spaces/ag14850/Mosquito-Demo"&gt;https://huggingface.co/spaces/ag14850/Mosquito-Demo&lt;/a&gt; Model: &lt;a href="https://huggingface.co/ag14850/Mosquito"&gt;https://huggingface.co/ag14850/Mosquito&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided-Repair-3638"&gt; /u/Lopsided-Repair-3638 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhxlgy</id>
    <title>glm-4.7-flash has the best thinking process with clear steps, I love it</title>
    <updated>2026-01-20T10:28:16+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I tested several personal prompts like &lt;code&gt;imagine you are in a farm, what is your favorite barn color?&lt;/code&gt;&lt;/li&gt; &lt;li&gt;although the prompt is short, glm can analyze the prompt and give clear thinking process&lt;/li&gt; &lt;li&gt;without my instruction in the prompt, glm mostly thinks in these steps: &lt;ol&gt; &lt;li&gt;request analysis&lt;/li&gt; &lt;li&gt;brainstorm&lt;/li&gt; &lt;li&gt;draft response&lt;/li&gt; &lt;li&gt;refine response: gives option1, option2, option3...&lt;/li&gt; &lt;li&gt;revise response/plan&lt;/li&gt; &lt;li&gt;polish&lt;/li&gt; &lt;li&gt;final response&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;so the glm thinking duration(110s) is really long compared to nemotron-nano(19s), but the thinking content is my favorite of all the small models. the final response is also clear &lt;ul&gt; &lt;li&gt;thinking process like this seems to be perfect for data analysis (waiting for a fine-tune)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;overall, i love glm-4.7-flash, and will try to replace qwen3-30b and nemotron-nano.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;del&gt;but GLM-4.7-Flash-mlx-4bit is very&lt;/del&gt; &lt;strong&gt;&lt;del&gt;slow&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;at&lt;/del&gt; &lt;strong&gt;&lt;del&gt;19 token/s&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;compared to nemotron-anno-mlx-4bit&lt;/del&gt; &lt;strong&gt;&lt;del&gt;30+ token/s&lt;/del&gt;&lt;/strong&gt;&lt;del&gt;. i donnot understand.&lt;/del&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm using &lt;a href="https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit"&gt;https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit&lt;/a&gt; on my m4 macbook air. with default config, the model often goes into loop. with the following config, it finally works for me &lt;ul&gt; &lt;li&gt;temperature 1.0&lt;/li&gt; &lt;li&gt;repeat penalty: 1.1&lt;/li&gt; &lt;li&gt;top-p: 0.95&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;❓ is there any trick to make the thinking process faster? Thinking can be toggled on/off through lmstudio ui, but i donnot want to disable it, how to make thinking faster?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhwfe0</id>
    <title>How to run and fine-tune GLM-4.7-Flash locally</title>
    <updated>2026-01-20T09:19:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt; &lt;img alt="How to run and fine-tune GLM-4.7-Flash locally" src="https://preview.redd.it/g5y2icqg1heg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e9539a968badc4795c9185a6384ef02c6b8c01" title="How to run and fine-tune GLM-4.7-Flash locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7-Flash is Z.ai’s new 30B MoE reasoning model built for local deployment, delivering best-in-class performance for coding, agentic workflows, and chat. &lt;/li&gt; &lt;li&gt;The model uses ~3.6B parameters, supports 200K context, and leads SWE-Bench, GPQA, and reasoning/chat benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official guide - &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash"&gt;https://unsloth.ai/docs/models/glm-4.7-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g5y2icqg1heg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T09:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhpima</id>
    <title>Bartowski comes through again. GLM 4.7 flash GGUF</title>
    <updated>2026-01-20T03:07:33+00:00</updated>
    <author>
      <name>/u/RenewAi</name>
      <uri>https://old.reddit.com/user/RenewAi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenewAi"&gt; /u/RenewAi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T03:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhlnsv</id>
    <title>Unsloth GLM 4.7-Flash GGUF</title>
    <updated>2026-01-20T00:17:58+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhitrj</id>
    <title>GLM 4.7 Flash official support merged in llama.cpp</title>
    <updated>2026-01-19T22:24:24+00:00</updated>
    <author>
      <name>/u/ayylmaonade</name>
      <uri>https://old.reddit.com/user/ayylmaonade</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt; &lt;img alt="GLM 4.7 Flash official support merged in llama.cpp" src="https://external-preview.redd.it/AVP8Isc32PMjAyVGtAipaav3x8aU8JY8Lx1bZ_yPak0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43081fb39d8cfd3c8faeeb3516b7513654ed8fce" title="GLM 4.7 Flash official support merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayylmaonade"&gt; /u/ayylmaonade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhii5v</id>
    <title>My gpu poor comrades, GLM 4.7 Flash is your local agent</title>
    <updated>2026-01-19T22:12:06+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried many MoE models at 30B or under and all of them failed sooner or later in an agentic framework. If z.ai is not redirecting my requests to another model, then GLM 4.7 Flash is finally the reliable (soon local) agent that I desperately wanted.&lt;/p&gt; &lt;p&gt;I am running it since more than half an hour on opencode and it produced hundreds of thousands tokens in one session (with context compacting obviously) without any tool calling errors. It clones github repos, it runs all kind of commands, edits files, commits changes, all perfect, not a single error yet.&lt;/p&gt; &lt;p&gt;Can't wait for GGUFs to try this locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhs2sd</id>
    <title>It's been one year since the release of Deepseek-R1</title>
    <updated>2026-01-20T05:08:29+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt; &lt;img alt="It's been one year since the release of Deepseek-R1" src="https://preview.redd.it/cin706z9tfeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65fbe53bfb15712186113b0e795fc46c050d0d13" title="It's been one year since the release of Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cin706z9tfeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T05:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
