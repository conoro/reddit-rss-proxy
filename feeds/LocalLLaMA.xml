<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-22T12:14:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qjpjt6</id>
    <title>MLX batched/continous inference with structured outputs</title>
    <updated>2026-01-22T08:53:57+00:00</updated>
    <author>
      <name>/u/ahjorth</name>
      <uri>https://old.reddit.com/user/ahjorth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm curious if anyone has found a good way to do batched or continuous batched inference on MLX with structured outputs.&lt;/p&gt; &lt;p&gt;I'm currently doing it on llama.cpp and it works really well. However, MLX-LM's server's relatively new continuous batching is about 50% faster than llama.cpp at 100 parallel inferences. So I'm hoping to get that speed bump from running on MLX, but I need structured outputs.&lt;/p&gt; &lt;p&gt;I feel like I have tried all the possible options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Outlines only supports structured outputs on one inference at a time. So that's much slower than parallel inference.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The vLLM-mlx post from a few days ago claimed it does, but I don't think it does. At least, whenever I used structured outputs on it, it ran in serial.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The mlx-openai-server server also says it does, but also seems to switch to serial. At least it's very slow for me.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The closest I have gotten is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;PydanticAI's Outlines implementation works for &lt;strong&gt;some models,&lt;/strong&gt; but I'm using GLM-models and there seems to be an issue with the JIT compilation of the bf16 kernel. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So two questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Has anyone managed to do MLX + parallel inference + structured outputs on &lt;em&gt;standard&lt;/em&gt; models without having to convert/quantizing them yourself?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Has anyone gotten this to work by converting/quantizing and avoiding bf16 and running it on PydanticAI's Outlines?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahjorth"&gt; /u/ahjorth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpjt6/mlx_batchedcontinous_inference_with_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpjt6/mlx_batchedcontinous_inference_with_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpjt6/mlx_batchedcontinous_inference_with_structured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T08:53:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiu6jo</id>
    <title>Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation</title>
    <updated>2026-01-21T10:14:30+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"&gt; &lt;img alt="Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation" src="https://preview.redd.it/64ya7ykngoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b1b6edf606c11be574456a3c46cef04dd0dfd81" title="Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a workflow for training small, task-specific models without the usual ML setup overhead.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Off-the-shelf small models are bad at specialized tasks. Qwen3 0.6B on Text2SQL gives you stuff like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; -- Qwen3 0.6B output: SELECT artists.name FROM artists WHERE artists.genre IS NULL OR artists.country IS NULL; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Completely wrong. But fine-tuning means data prep, training infrastructure, hyperparameter tuning...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The approach:&lt;/strong&gt; Knowledge distillation via a Claude skill that wraps &lt;a href="https://docs.distillabs.ai"&gt;distil-cli&lt;/a&gt;. A large teacher model (DeepSeek-V3) generates synthetic training data from your examples, then a small student model learns to match its outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```bash curl -fsSL &lt;a href="https://cli-assets.distillabs.ai/install.sh"&gt;https://cli-assets.distillabs.ai/install.sh&lt;/a&gt; | sh distil login&lt;/p&gt; &lt;h1&gt;In Claude Code:&lt;/h1&gt; &lt;p&gt;/plugin marketplace add &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;https://github.com/distil-labs/distil-cli-skill&lt;/a&gt; /plugin install distil-cli@distil-cli-skill ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Claude handles:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Step&lt;/th&gt; &lt;th&gt;What happens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Task selection&lt;/td&gt; &lt;td&gt;Recommends QA/classification/tool-calling/RAG based on your description&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data conversion&lt;/td&gt; &lt;td&gt;Takes whatever format you have, outputs proper JSONL&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teacher eval&lt;/td&gt; &lt;td&gt;Runs the teacher on your test set — if it scores low, don't bother training&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training&lt;/td&gt; &lt;td&gt;Kicks off distillation, monitors progress&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Packaging&lt;/td&gt; &lt;td&gt;Downloads GGUF, HuggingFace format, or LoRA adapter&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;My test run:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Input: 100 conversation traces (not cleaned, just raw logs)&lt;/li&gt; &lt;li&gt;Task: Text2SQL&lt;/li&gt; &lt;li&gt;Teacher eval: 80% LLM-as-a-Judge&lt;/li&gt; &lt;li&gt;Final student score: 74%&lt;/li&gt; &lt;li&gt;Base model score: 36%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Output is a 2.2GB GGUF that runs locally via Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After fine-tuning:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Same question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; -- Fine-tuned output: SELECT a.name FROM artists a JOIN albums al ON a.id = al.artist_id GROUP BY a.id, a.name HAVING SUM(al.sales) &amp;gt; 1000000; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Correct JOINs, proper GROUP BY, HAVING instead of WHERE.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full benchmark:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;ROUGE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base Qwen3 0.6B&lt;/td&gt; &lt;td&gt;36%&lt;/td&gt; &lt;td&gt;69.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;88.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fine-tuned 0.6B&lt;/td&gt; &lt;td&gt;74%&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skill: &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;github.com/distil-labs/distil-cli-skill&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full example with data: &lt;a href="https://github.com/distil-labs/distil-example-text2sql-with-claude"&gt;github.com/distil-labs/distil-example-text2sql-with-claude&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Detailed walkthrough: &lt;a href="https://www.distillabs.ai/blog/train-your-slm-with-distil-claude-skill"&gt;distillabs.ai/blog/train-your-slm-with-distil-claude-skill&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the distillation process or the skill implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64ya7ykngoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T10:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj271s</id>
    <title>Fine-tuned Qwen3-14B on 10k DeepSeek traces: +20% on security benchmark</title>
    <updated>2026-01-21T16:15:17+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work as a security auditor (basically a bug hunter) and LLMs have become the principal tool at work, like in most of IT. But token usage is huge, and it's becoming problematic as it is taking a big part of the earnings of most audit shops.&lt;/p&gt; &lt;p&gt;So I fine-tuned Qwen3-14B with about +10,000 bug-hunting thinking traces distilled from DeepSeek. It turns out that even this small dataset improved bug-hunting capabilities a lot (20% in a custom benchmark). This is not conclusive, as the benchmark could be wrong, but by using it manually, it easily shows greatly improved performance compared to the base model. It will never be as good as a frontier model, but you literally cannot apply frontier models to huge codebases, as you would spend millions of USD.&lt;/p&gt; &lt;p&gt;So I think this is a good example of how distillation of particular skills into a smaller model is a viable alternative for lowering costs.&lt;/p&gt; &lt;p&gt;If someone wants to play with it, it's available here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/NeuroengineAI/ZeroShot-Qwen3-14B-preview"&gt;https://huggingface.co/NeuroengineAI/ZeroShot-Qwen3-14B-preview&lt;/a&gt; &lt;/p&gt; &lt;p&gt;GGUF coming soon. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj271s/finetuned_qwen314b_on_10k_deepseek_traces_20_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj271s/finetuned_qwen314b_on_10k_deepseek_traces_20_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj271s/finetuned_qwen314b_on_10k_deepseek_traces_20_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T16:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiy0ha</id>
    <title>GLM-4.7-Flash-GGUF bug fix - redownload for better outputs</title>
    <updated>2026-01-21T13:34:00+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jan 21 update: llama.cpp fixed a bug that caused looping and poor outputs. We updated the GGUFs - please re-download the model for much better outputs.&lt;/p&gt; &lt;p&gt;You can now use Z.ai's recommended parameters and get great results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For general use-case: &lt;code&gt;--temp 1.0 --top-p 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;For tool-calling: &lt;code&gt;--temp 0.7 --top-p 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;If using llama.cpp, set &lt;code&gt;--min-p 0.01&lt;/code&gt; as llama.cpp's default is 0.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;unsloth/GLM-4.7-Flash-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T13:34:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjs87a</id>
    <title>What is "summerset" on Image Arena?</title>
    <updated>2026-01-22T11:35:23+00:00</updated>
    <author>
      <name>/u/ErToppa</name>
      <uri>https://old.reddit.com/user/ErToppa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this not strictly related to LLMs, I was playing around on Artificial Analysis with the image arena where they generate images with the same prompt with different models and you choose the best result to improve the leaderboard. I keep seeing &amp;quot;summerset&amp;quot; as the name of a model but cannot find a model with that name. Anybody knows what it means?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ErToppa"&gt; /u/ErToppa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjs87a/what_is_summerset_on_image_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjs87a/what_is_summerset_on_image_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjs87a/what_is_summerset_on_image_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T11:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj579f</id>
    <title>Lemonade v9.1.4 released: GLM-4.7-Flash-GGUF on ROCm and Vulkan, LM Studio GGUF import, and more</title>
    <updated>2026-01-21T18:02:48+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj579f/lemonade_v914_released_glm47flashgguf_on_rocm_and/"&gt; &lt;img alt="Lemonade v9.1.4 released: GLM-4.7-Flash-GGUF on ROCm and Vulkan, LM Studio GGUF import, and more" src="https://b.thumbs.redditmedia.com/r2IPKcpyTuB6c5H2P2QkwHaOdqx9wv5tfknZaJUdAxE.jpg" title="Lemonade v9.1.4 released: GLM-4.7-Flash-GGUF on ROCm and Vulkan, LM Studio GGUF import, and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade has been moving fast this month so I thought I should post an update with the v9.1.4 release today.&lt;/p&gt; &lt;p&gt;If you haven't heard of it, Lemonade is a convenient local LLM server similar to Ollama or LM Studio. The main differences are that its 100% open source, isn't selling you anything, and always includes the latest tools/optimizations from AMD. Our primary goal is to grow the ecosystem of great local AI apps for end users.&lt;/p&gt; &lt;h2&gt;GLM-4.7-Flash-GGUF&lt;/h2&gt; &lt;p&gt;We're bundling llama.cpp builds from this morning for the latest GLM-4.7-Flash support: &lt;code&gt;b7788&lt;/code&gt; for Vulkan and CPU, and &lt;code&gt;b1162&lt;/code&gt; from the llamacpp-rocm project for ROCm. These builds include the &amp;quot;Fix GLM 4.7 MoE gating func&amp;quot; from just a few hours ago. &lt;/p&gt; &lt;p&gt;Try it with: &lt;code&gt;lemonade-server run GLM-4.7-Flash-GGUF --llamacpp rocm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I can't thank the llama.cpp team enough for this amazing work! Thanks, @0cc4m, in particular, for always helping people on the discord and optimizing Strix Halo Vulkan performance.&lt;/p&gt; &lt;h2&gt;LM Studio Compatibility&lt;/h2&gt; &lt;p&gt;You shouldn't need to download the same GGUF more than once.&lt;/p&gt; &lt;p&gt;Start Lemonade with &lt;code&gt;lemonade-server serve --extra-models-dir /path/to/.lmstudio/models&lt;/code&gt; and your GGUFs will show up in Lemonade.&lt;/p&gt; &lt;h2&gt;Platform Support&lt;/h2&gt; &lt;p&gt;The community has done a ton of work to improve platform support in Lemonade. In addition to the usual Ubuntu and Windows support, we now have Arch, Fedora, and Docker supported. There are &lt;a href="https://github.com/lemonade-sdk/lemonade/pkgs/container/lemonade-server"&gt;official dockers that ship with every release&lt;/a&gt; now.&lt;/p&gt; &lt;p&gt;Shoutout to @siavashhub, @sofiageo, @ianbmacdonald, and @SidShetye for their work here.&lt;/p&gt; &lt;h2&gt;Mobile Companion App&lt;/h2&gt; &lt;p&gt;@Geramy has contributed an entire &lt;a href="https://github.com/lemonade-sdk/lemonade-mobile"&gt;mobile app&lt;/a&gt; that connects to your Lemonade server and provides a chat interface with VLM support. It is available on the iOS app store today and will launch on Android when Google is done reviewing in about 2 weeks.&lt;/p&gt; &lt;h2&gt;Recipe Cookbook&lt;/h2&gt; &lt;p&gt;@bitgamma has done a series of PRs that allow you to save your model settings (rocm vs. vulkan, llamacpp args, etc.) to a JSON file and have them automatically apply the next time that model is loaded.&lt;/p&gt; &lt;p&gt;For example: &lt;code&gt;lemonade-server run gpt-oss-20b-mxfp4-GGUF --ctx-size 16384 --llamacpp rocm --llamacpp-args &amp;quot;--flash-attn on --no-mmap&amp;quot; --save-options&lt;/code&gt;&lt;/p&gt; &lt;p&gt;@sofiageo has a PR to add this feature to the app UI.&lt;/p&gt; &lt;h2&gt;Roadmap&lt;/h2&gt; &lt;p&gt;Under development:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;macOS support with llama.cpp+metal&lt;/li&gt; &lt;li&gt;image generation with stablediffusion.cpp&lt;/li&gt; &lt;li&gt;&amp;quot;marketplace&amp;quot; link directory to featured local AI apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Under consideration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;vLLM and/or MLX support&lt;/li&gt; &lt;li&gt;text to speech&lt;/li&gt; &lt;li&gt;make it easier to add GGUFs from Hugging Face&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;p&gt;If you like what we're doing, please star us on GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to hang out, you can find us on the Lemonade Discord: &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;https://discord.gg/5xXzkMu8Zk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qj579f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj579f/lemonade_v914_released_glm47flashgguf_on_rocm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj579f/lemonade_v914_released_glm47flashgguf_on_rocm_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T18:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjqka8</id>
    <title>Anyone using a local LLM to turn meeting transcripts into actionable outputs?</title>
    <updated>2026-01-22T09:57:33+00:00</updated>
    <author>
      <name>/u/voss_steven</name>
      <uri>https://old.reddit.com/user/voss_steven</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working with locally transcribed meeting data and experimenting with local LLMs to go beyond summaries.&lt;/p&gt; &lt;p&gt;Right now, I can get decent transcripts and rough summaries, but the harder part is what comes next:&lt;br /&gt; reliably extracting action items, decisions, owners, and follow-ups in a structured way that’s actually usable.&lt;/p&gt; &lt;p&gt;For people doing this locally:&lt;br /&gt; How well do current open-source models handle post-meeting structuring?&lt;br /&gt; Are you mostly using prompting, schemas, fine-tuning, or external rules to ensure consistent outputs?&lt;/p&gt; &lt;p&gt;Not interested in SaaS tools or cloud APIs, only local LLM workflows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/voss_steven"&gt; /u/voss_steven &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqka8/anyone_using_a_local_llm_to_turn_meeting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqka8/anyone_using_a_local_llm_to_turn_meeting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqka8/anyone_using_a_local_llm_to_turn_meeting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T09:57:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj13uh</id>
    <title>One-shot single page web development: pacman clone - GLM 4.7 vs GLM 4.7 Flash vs GLM 4.5 Air vs Gemini 3 Pro vs Gemini 3 Flash - Results available for online testing - Prompt and instructions provided for testing with other models</title>
    <updated>2026-01-21T15:36:18+00:00</updated>
    <author>
      <name>/u/ex-arman68</name>
      <uri>https://old.reddit.com/user/ex-arman68</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a big fan of testing coding models by asking them to do one, or few shots, simple development. I have just ran a test asking them to one-shot a pacman clone as a single webpage. The results did not actually match my expectations: I thought Gemini 3 Pro would be the clear winner, followed by Gemini 3 Flash, and then GLM 4.7. This is how I actually rank the results:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GLM 4.7&lt;/strong&gt; (by far the clear winner)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimax M2.1&lt;/strong&gt; (another great contender)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.7 Flash&lt;/strong&gt; (disappointing, I expected more)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.5 Air&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can find the system and user prompts at bottom of this post. Don't forget to set the temperature to 0. I have tested with the default temperature, and the results are always better with a setting of 0, as well being 100% reproducible.&lt;/p&gt; &lt;p&gt;If you run the test with other models, please share your results.&lt;/p&gt; &lt;p&gt;Here is a bit more details about each result, as well as link to the generated webpages.&lt;/p&gt; &lt;h1&gt;GLM 4.7 (z.ai API)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/glm-4.7"&gt;pacman_glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Almost fully working. Good pacman and ghosts behaviour and speed. One bug causes the game to freeze, but only minor fix required.&lt;/p&gt; &lt;h1&gt;Minimax m2.1 Q5 (thanks to @&lt;a href="https://www.reddit.com/user/sjoerdmaessen/"&gt;sjoerdmaessen&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/minimax-m21-q5"&gt;minimax-m21-q5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Almost fully working. The only one with sound. A few issues with ghost mechanics, with initial display, moving through tunnel, and crash after collision. Impressive though, especially at Q5. It would not take much to get rid of those bugs.&lt;/p&gt; &lt;h1&gt;Gemini 3 Flash&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/gemini-3-flash"&gt;pacman_gemini-3-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mostly working. Too fast. Bad ghost logic. Navigation problems.&lt;/p&gt; &lt;h1&gt;Gemini 3 Pro&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/gemini-3-pro"&gt;pacman_gemini-3-pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pacman barely working. Ghosts not working.&lt;/p&gt; &lt;h1&gt;GLM 4.7 Flash (8-bit MLX)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/glm-4.7-flash"&gt;pacman_glm-4.7-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cannot get past the loading screen. A second shot with well written debugging instructions did not fix it.&lt;/p&gt; &lt;h1&gt;GLM 4.5 Air (Qx53gx MLX)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/glm-4.5-air"&gt;pacman_glm-4.5-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cannot get past the loading screen. A second shot with well written debugging instructions did not fix it.&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;h1&gt;User prompt&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;I need you to write a fully working pacman clone in a single html webpage. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;System prompt&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;You are the world's leading expert in vanilla web development, specifically in creating high-performance, single-file web applications using only HTML5, CSS3, and ES6+ JavaScript. You reject frameworks in favor of clean, efficient, and semantic code. Your goal is to receive a requirement and produce a single, self-contained HTML file that functions perfectly without external dependencies (no CDNs, no images, no libraries). Because you must complete this task in a &amp;quot;one-shot&amp;quot; continuous generation, you must think before you code. You will follow a strict &amp;quot;Chain of Thought&amp;quot; protocol to ensure correctness. Follow this specific execution format for every response: &amp;lt;analysis&amp;gt; 1. REQUIREMENTS BREAKDOWN: - List every functional and non-functional requirement. - Identify potential edge cases. 2. ARCHITECTURAL PLAN: - CSS Strategy: Define the variable system, layout approach (Flexbox/Grid), and responsive breakpoints. - JS Architecture: Define state management, event listeners, and core logic functions. - HTML Structure: specific semantic tags to be used. 3. PRE-MORTEM &amp;amp; STRATEGY: - Identify the most likely point of failure. - Define the solution for that specific failure point before writing code. &amp;lt;/analysis&amp;gt; &amp;lt;implementation&amp;gt; (Provide the complete, valid HTML string here. Include CSS in &amp;lt;style&amp;gt; and JS in &amp;lt;script&amp;gt; tags. The code must be production-ready, accessible, and clean.) &amp;lt;/implementation&amp;gt; &amp;lt;code_review&amp;gt; Self-Correction and Validation Report: 1. Does the code meet all requirements listed in the analysis? [Yes/No] 2. Are there any distinct accessibility (a11y) violations? 3. Verify that no external libraries were used. &amp;lt;/code_review&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ex-arman68"&gt; /u/ex-arman68 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T15:36:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjl8wl</id>
    <title>Steam page is live! Time for non-technical folks to enjoy local AI too (for free).</title>
    <updated>2026-01-22T04:50:10+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjl8wl/steam_page_is_live_time_for_nontechnical_folks_to/"&gt; &lt;img alt="Steam page is live! Time for non-technical folks to enjoy local AI too (for free)." src="https://external-preview.redd.it/bHkwcW1hdTJ5dGVnMbHZFR91uvlMoDu0I82Gci2pLd5aleXzKfSslfZLmuXM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9323cb9c478ea24574e17fd18350b2ebb5297d6f" title="Steam page is live! Time for non-technical folks to enjoy local AI too (for free)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to help bring free, local AI to everyone. By releasing a simple chatbot to steam that's just about a reality. &lt;/p&gt; &lt;p&gt;I have some polishing up to do, but initial tests are going great! One request is for an RLM implementation, so I'm delaying the release until I can get a deep think mode using RLM for better response quality. &lt;/p&gt; &lt;p&gt;The short demo above showcases just about everything, but I'm completely open to more suggestions or ideas as well! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Offloom includes:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- document and web search RAG&lt;/p&gt; &lt;p&gt;- Image generation&lt;/p&gt; &lt;p&gt;- Text to speech (pocketTTS)&lt;/p&gt; &lt;p&gt;- Think and non think modes&lt;/p&gt; &lt;p&gt;- All the above can be toggled on/off easily at any point&lt;/p&gt; &lt;p&gt;- Plus some local powered agents in the works!&lt;/p&gt; &lt;p&gt;&lt;a href="https://store.steampowered.com/app/3045210/Offloom/"&gt;https://store.steampowered.com/app/3045210/Offloom/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z4y0w6u2yteg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjl8wl/steam_page_is_live_time_for_nontechnical_folks_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjl8wl/steam_page_is_live_time_for_nontechnical_folks_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T04:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjifay</id>
    <title>Lora fine tuning! Why isn't it popular at all?</title>
    <updated>2026-01-22T02:38:51+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there's some quality difference in both, but being able to download a lora and using it with model instead of diff frozen weights for diff tasks is much more intuitive imo, &lt;/p&gt; &lt;p&gt;What do y'all think about it? It can make models much more personalised &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjifay/lora_fine_tuning_why_isnt_it_popular_at_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjifay/lora_fine_tuning_why_isnt_it_popular_at_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjifay/lora_fine_tuning_why_isnt_it_popular_at_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T02:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjpwbr</id>
    <title>Qwen3-Coder-480B on Mac Studio M3 Ultra 512gb</title>
    <updated>2026-01-22T09:15:42+00:00</updated>
    <author>
      <name>/u/BitXorBit</name>
      <uri>https://old.reddit.com/user/BitXorBit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;i was wondering if anyone use this configuration for daily usage as coding assistant/agentic?&lt;/p&gt; &lt;p&gt;my goal here is to have as much as possible close to claude code opus 4.5 on my local setup, i need 6-10 hours/day of usage for refactoring, research, solve architecture problems, etc &lt;/p&gt; &lt;p&gt;i read on many places that the 30b models are too &amp;quot;dumb&amp;quot; for this case, and i should aim on the higher models, which ofc leads us to the known issue of VRAM, 6000 pro is not an option because of the VRAM requirements and other cluster solutions would cost like my house. &lt;/p&gt; &lt;p&gt;so before going and buying the Mac Studio M3 Ultra with 512gb ram, i would love to hear feedback if any developers using this configuration/alternative on daily basis and what is their feedback. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitXorBit"&gt; /u/BitXorBit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpwbr/qwen3coder480b_on_mac_studio_m3_ultra_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpwbr/qwen3coder480b_on_mac_studio_m3_ultra_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpwbr/qwen3coder480b_on_mac_studio_m3_ultra_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T09:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj2dnd</id>
    <title>A new model from http://Z.ai, "GLM-OCR" has been spotted on Github</title>
    <updated>2026-01-21T16:21:49+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"&gt; &lt;img alt="A new model from http://Z.ai, &amp;quot;GLM-OCR&amp;quot; has been spotted on Github" src="https://preview.redd.it/tduio97daqeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd5b258b6d658644e83ddec8f6c475cc131ee93a" title="A new model from http://Z.ai, &amp;quot;GLM-OCR&amp;quot; has been spotted on Github" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tduio97daqeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T16:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiwm3c</id>
    <title>Fix for GLM 4.7 Flash has been merged into llama.cpp</title>
    <updated>2026-01-21T12:29:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt; &lt;img alt="Fix for GLM 4.7 Flash has been merged into llama.cpp" src="https://external-preview.redd.it/P0aZfAO5cQnwgz36bD9sAcDcttCXWcTbQBhIkzY76fc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac930f1f077b513ae17d07167d50119d0ac69d0" title="Fix for GLM 4.7 Flash has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The world is saved!&lt;/p&gt; &lt;p&gt;FA for CUDA in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;https://github.com/ggml-org/llama.cpp/pull/18953&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18980"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T12:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj40er</id>
    <title>VibeVoice-ASR released!</title>
    <updated>2026-01-21T17:20:29+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;https://huggingface.co/microsoft/VibeVoice-ASR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T17:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjd8dp</id>
    <title>Kimi-Linear-48B-A3B-Instruct-GGUF Support - Any news?</title>
    <updated>2026-01-21T22:58:38+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-Linear seems to handle long context pretty well. Do you have any idea why it's still not implemented in llama.cpp? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjc8a2</id>
    <title>Michigan is pushing a Anti Chatbot bill to protect the heckin kiddos</title>
    <updated>2026-01-21T22:19:31+00:00</updated>
    <author>
      <name>/u/PostEasy7183</name>
      <uri>https://old.reddit.com/user/PostEasy7183</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Senate Democrats Call for Improved Safety Measures to Better Protect Michigan Kids from Digital Dangers - Senator Kevin Hertel &lt;a href="https://share.google/ZwmPjEOVP5AcgZnhT"&gt;https://share.google/ZwmPjEOVP5AcgZnhT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;not much information about this yet but they've talked about making sure kids have a harder time to access chat bots. the bill is vague so far and to my knowledge no real text has been released yet. My question is how can they assess what is a teen and not without a Digital ID? I'm so sick of these bullshit laws in the spirit of &amp;quot;Protecting the children.&amp;quot; Give your thoughts below&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PostEasy7183"&gt; /u/PostEasy7183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjf6ys</id>
    <title>Wrote a guide for running Claude Code with GLM-4.7 Flash locally with llama.cpp</title>
    <updated>2026-01-22T00:17:31+00:00</updated>
    <author>
      <name>/u/tammamtech</name>
      <uri>https://old.reddit.com/user/tammamtech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of ollama features are now support llama.cpp server but aren't well documented. The ollama convenience features can be replicated in llama.cpp now, the main ones I wanted were model swapping, and freeing gpu memory on idle because I run llama.cpp as a docker service exposed to internet with cloudflare tunnels.&lt;/p&gt; &lt;p&gt;The GLM-4.7 flash release and the recent support for Anthropic API in llama.cpp server gave me the motivation to finally make this happen. I basically wanted to run Claude Code from laptop withGLM 4.7 Flash running on my PC.&lt;/p&gt; &lt;p&gt;I wrote a slightly more comprehensive version&lt;a href="https://tammam.io/blog/llama-cpp-setup-with-claude-codex-cli/"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Install llama.cpp if you don't have it&lt;/h3&gt; &lt;p&gt;I'm going to assume you have llama-cli or llama-server installed or you have ability to run docker containers with gpu. There are many sources for how to do this.&lt;/p&gt; &lt;h3&gt;Running the model&lt;/h3&gt; &lt;p&gt;All you need is the following command if you just want to run GLM 4.7 Flash.&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash llama-cli -hf unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL \ --alias glm-4.7-flash \ --jinja --ctx-size 32768 \ --temp 1.0 --top-p 0.95 --min-p 0.01 --fit on \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The command above will download the model on first run and cache it locally. The `&lt;code&gt;sleep-idle-seconds 300&lt;/code&gt; frees GPU memory after 5 minutes of idle so you can keep the server running.&lt;/p&gt; &lt;p&gt;The sampling parameters above (&lt;code&gt;--temp 1.0 --top-p 0.95 --min-p 0.01&lt;/code&gt;) are the recommended settings for GLM-4.7 general use. For tool-calling, use &lt;code&gt;--temp 0.7 --top-p 1.0&lt;/code&gt; instead.&lt;/p&gt; &lt;h4&gt;Or With Docker&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash docker run --gpus all -p 8080:8080 \ ghcr.io/ggml-org/llama.cpp:server-cuda \ -hf unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL \ --jinja --ctx-size 32768 \ --temp 1.0 --top-p 0.95 --min-p 0.01 --fit on \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Multi-Model Setup with Config File&lt;/h3&gt; &lt;p&gt;If you want to run multiple models with router mode, you'll need a config file. This lets the server load models on demand based on what clients request.&lt;/p&gt; &lt;p&gt;First, download your models (or let them download via &lt;code&gt;-hf&lt;/code&gt; on first use):&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash mkdir -p ~/llama-cpp &amp;amp;&amp;amp; touch ~/llama-cpp/config.ini &lt;/code&gt;&lt;/p&gt; &lt;p&gt;In &lt;code&gt;~/llama-cpp/config.ini&lt;/code&gt; put your models settings:&lt;/p&gt; &lt;p&gt;```ini [*] &lt;/p&gt; &lt;h1&gt;Global settings&lt;/h1&gt; &lt;p&gt;[glm-4.7-flash] hf-repo = unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL jinja = true temp = 0.7 ctx-size = 32768 top-p = 1 min-p = 0.01 fit = on&lt;/p&gt; &lt;p&gt;[other-model] ... ```&lt;/p&gt; &lt;h4&gt;Run with Router Mode&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash llama-cli \ --models-preset ~/llama-cpp/config.ini \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 --models-max 1 &lt;/code&gt;&lt;/p&gt; &lt;h4&gt;Or with Docker&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash docker run --gpus all -p 8080:8080 \ -v ~/llama-cpp/config.ini:/config.ini \ ghcr.io/ggml-org/llama.cpp:server-cuda \ --models-preset /config.ini \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 \ --models-max 1 &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Configuring Claude Code&lt;/h2&gt; &lt;p&gt;Claude Code can be pointed at your local server. In your terminal run&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash export ANTHROPIC_BASE_URL=http://localhost:8080 claude --model glm-4.7-flash &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will now use your local model instead of hitting Anthropic's servers.&lt;/p&gt; &lt;h2&gt;Configuring Codex CLI&lt;/h2&gt; &lt;p&gt;You can also configure the Codex CLI to use your local server. Modify the &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to look something like this:&lt;/p&gt; &lt;p&gt;```toml model = &amp;quot;glm-4.7-flash&amp;quot; model_reasoning_effort = &amp;quot;medium&amp;quot; model_provider=&amp;quot;llamacpp&amp;quot;&lt;/p&gt; &lt;p&gt;[model_providers.llamacpp] name=&amp;quot;llamacpp&amp;quot; base_url=&amp;quot;http://localhost:8080/v1&amp;quot; ```&lt;/p&gt; &lt;h2&gt;Some Extra Notes&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Model load time&lt;/strong&gt;: When a model is unloaded (after idle timeout), the next request has to wait for it to load again. For large models this can take some time. Tune &lt;code&gt;--sleep-idle-seconds&lt;/code&gt; based on your usage pattern.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance and Memory Tuning&lt;/strong&gt;: There are more flags you can use in llama.cpp for tuning cpu offloading, flash attention, etc that you can use to optimize memory usage and performance. The &lt;code&gt;--fit&lt;/code&gt; flag is a good starting point. Check the &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md"&gt;llama.cpp server docs&lt;/a&gt; for details on all the flags.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Internet Access&lt;/strong&gt;: If you want to use models deployed on your PC from say your laptop, the easiest way is to use something like Cloudflare tunnels, I go over setting this up in &lt;a href="https://tammam.io/blog/access-sd-ui-over-internet"&gt;my Stable Diffusion setup guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Auth&lt;/strong&gt;: If exposing the server to the internet, you can use &lt;code&gt;--api-key KEY&lt;/code&gt; to require an API key for authentication.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tammamtech"&gt; /u/tammamtech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T00:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjqgnr</id>
    <title>This Week's Hottest Hugging Face Releases: Top Picks by Category!</title>
    <updated>2026-01-22T09:51:15+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.&lt;/p&gt; &lt;p&gt;Check 'em out and drop your thoughts—which one's getting deployed first?&lt;/p&gt; &lt;h1&gt;Text Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;&lt;strong&gt;zai-org/GLM-4.7-Flash&lt;/strong&gt;&lt;/a&gt;: 31B param model for fast, efficient text gen—updated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;&lt;strong&gt;unsloth/GLM-4.7-Flash-GGUF&lt;/strong&gt;&lt;/a&gt;: Quantized 30B version for easy local inference—hot with 112k downloads in hours. Great for low-resource setups.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Image / Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;&lt;strong&gt;zai-org/GLM-Image&lt;/strong&gt;&lt;/a&gt;: Image-text-to-image powerhouse—10.8k downloads, 938 likes. Excels in creative edits and generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;&lt;strong&gt;google/translategemma-4b-it&lt;/strong&gt;&lt;/a&gt;: 5B vision-language model for multilingual image-text tasks—45.4k downloads, supports translation + vision.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Audio / Speech&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;&lt;strong&gt;kyutai/pocket-tts&lt;/strong&gt;&lt;/a&gt;: Compact TTS for natural voices—38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;&lt;strong&gt;microsoft/VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;: 9B ASR for multilingual speech recognition—ultra-low latency, 816 downloads already spiking.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Other Hot Categories (Video/Agentic)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;&lt;strong&gt;Lightricks/LTX-2&lt;/strong&gt;&lt;/a&gt; (Image-to-Video): 1.96M downloads, 1.25k likes—pro-level video from images.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;&lt;strong&gt;stepfun-ai/Step3-VL-10B&lt;/strong&gt;&lt;/a&gt; (Image-Text-to-Text): 10B VL model for advanced reasoning—28.6k downloads in hours.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are dominating trends with massive community traction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T09:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjaxfy</id>
    <title>8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)</title>
    <updated>2026-01-21T21:30:54+00:00</updated>
    <author>
      <name>/u/ai-infos</name>
      <uri>https://old.reddit.com/user/ai-infos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"&gt; &lt;img alt="8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)" src="https://preview.redd.it/16ndtph7treg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df00bad2dcdf2390a12afaf191c07f1264ae2752" title="8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt; AWQ 4bit @ &lt;strong&gt;26.8 tok/s&lt;/strong&gt; (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with MAX context length (196608)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.7&lt;/strong&gt; AWQ 4bit @ &lt;strong&gt;15.6 tok/s&lt;/strong&gt; (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with context length 95000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GPUs cost&lt;/strong&gt;: 880$ for 256GB VRAM (early 2025 prices)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;: 280W (idle) / 1200W (inference)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: reach one of the most cost effective solution of the world for one of the best fast intelligent local inference setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: BIG thanks to the Global Open source Community!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;All setup details here:&lt;/strong&gt; &lt;a href="https://github.com/ai-infos/guidances-setup-8-mi50-glm47-minimax-m21/tree/main"&gt;https://github.com/ai-infos/guidances-setup-8-mi50-glm47-minimax-m21/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feel free to ask any questions and/or share any comments.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt;: few weeks ago, I posted here this setup of 16 MI50 with deepeseek v3.2: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/&lt;/a&gt; After few more tests/dev on it, I could have reached 14 tok/s but still not stable after ~18k tokens context input (generating garbage output) so almost useless for me. Whereas, the above models (Minimax M2.1 and GLM 4.7) are pretty stable at long context so usable for coding agents usecases etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-infos"&gt; /u/ai-infos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/16ndtph7treg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T21:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjotja</id>
    <title>Qwen3 TTS Open Source VLLM-Omni PR</title>
    <updated>2026-01-22T08:07:52+00:00</updated>
    <author>
      <name>/u/jnk_str</name>
      <uri>https://old.reddit.com/user/jnk_str</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Might be coming soon..&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm-omni/pull/895"&gt;https://github.com/vllm-project/vllm-omni/pull/895&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jnk_str"&gt; /u/jnk_str &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjotja/qwen3_tts_open_source_vllmomni_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjotja/qwen3_tts_open_source_vllmomni_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjotja/qwen3_tts_open_source_vllmomni_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T08:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjjrmq</id>
    <title>Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane</title>
    <updated>2026-01-22T03:39:33+00:00</updated>
    <author>
      <name>/u/coloradical5280</name>
      <uri>https://old.reddit.com/user/coloradical5280</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"&gt; &lt;img alt="Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane" src="https://external-preview.redd.it/dHl1a3MydnZsdGVnMeL77RZtngf3FeBaBzx1OTOSVuPnAYqXhpVZKKbs_rnV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce79108f93f379daa30c29208725387fb2320ee4" title="Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fei-Fei Li, the &amp;quot;godmother of modern AI&amp;quot; and a pioneer in computer vision, founded World Labs a few years ago with a small team and $230 million in funding. Last month, they launched &lt;a href="https://marble.worldlabs.ai/"&gt;https://marble.worldlabs.ai/&lt;/a&gt;, a generative world model that’s not JEPA, but instead built on Neural Radiance Fields (NeRF) and Gaussian splatting. &lt;/p&gt; &lt;p&gt;It’s &lt;em&gt;insanely fast&lt;/em&gt; for what it does, generating explorable 3D worlds in minutes. For example: &lt;a href="https://marble.worldlabs.ai/world/5b850e80-a587-48d7-9340-186e0bcbf46b"&gt;this scene&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Crucially, it’s not video. The frames aren’t rendered on-the-fly as you move. Instead, it’s a fully stateful 3D environment represented as a dense cloud of Gaussian splats—each with position, scale, rotation, color, and opacity. This means the world is persistent, editable, and supports non-destructive iteration. You can expand regions, modify materials, and even merge multiple worlds together. &lt;/p&gt; &lt;p&gt;You can share your world, others can build on it, and you can build on theirs. It natively supports VR (Vision Pro, Quest 3), and you can export splats or meshes for use in Unreal, Unity, or Blender via USDZ or GLB. &lt;/p&gt; &lt;p&gt;It's early, there are (very literally) rough edges, but it's crazy to think about this in 5 years. For free, you get a few generations to experiment; $20/month unlocks a lot, I just did one month so I could actually play, and definitely didn't max out credits. &lt;/p&gt; &lt;p&gt;Fei-Fei Li is an OG AI visionary, but zero hype. She’s been quiet, especially about this. So Marble hasn’t gotten the attention it deserves. &lt;/p&gt; &lt;p&gt;At first glance, visually, you might think, “meh”... but there’s &lt;strong&gt;no triangle-based geometry here, no real-time rendering pipeline, no frame-by-frame generation.&lt;/strong&gt; Just a solid, exportable, editable, stateful pile of splats. &lt;/p&gt; &lt;p&gt;The breakthrough isn't the image though, it’s the spatial intelligence. Y'all should play around, it's wild. &lt;/p&gt; &lt;p&gt;&lt;em&gt;I know this is a violation of Rule #2 but honestly there just aren't that many subs with people smart enough to appreciate this; no hard feelings if it needs be removed though.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coloradical5280"&gt; /u/coloradical5280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/udsg2ztvlteg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T03:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjrsur</id>
    <title>GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp</title>
    <updated>2026-01-22T11:10:42+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt; &lt;img alt="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" src="https://external-preview.redd.it/TcGseIeP3Z00NB4otbKR8-_fs_ssjxg6HC4Fv_lVbUU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d37072f08acdffcd4c3617847f00d2a9b9bafcf4" title="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T11:10:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjnbh8</id>
    <title>Thoughts on LLMs (closed- and open-source) in software development after one year of professional use.</title>
    <updated>2026-01-22T06:38:33+00:00</updated>
    <author>
      <name>/u/grey-seagull</name>
      <uri>https://old.reddit.com/user/grey-seagull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Chatbots are amazing at codebase exploration.&lt;/li&gt; &lt;li&gt;Chatbots are good at checking regression while going through ideas, especially Codex.&lt;/li&gt; &lt;li&gt;Codex is best at debugging.&lt;/li&gt; &lt;li&gt;Claude is better than others in code quality.&lt;/li&gt; &lt;li&gt;Gemini is bad at instruction following.&lt;/li&gt; &lt;li&gt;Biggest open source LLMs are basically at par with the above models.&lt;/li&gt; &lt;li&gt;Local model aren't much help not even for easier tasks. The models you can run locally using 24-40 GB of VRAM are underwhelming and slow. The agentic flows, especially, can quickly build up big KV caches which are too much and too slow to handle locally. Forget about multiple 100k+ token chat sessions concurrently. Economies of scale win here to bring the best value out of a certain capex spent on hardware. Models like gemini flash are fast, good and cheap.&lt;/li&gt; &lt;li&gt;That said, the biggest open-source models can basically match GPTs and Claudes of the world now and at a fraction of the cost. Since, for most people, they are too big to run locally the only viable option is various 3rd party hosted ones but they are often not trusted enough to be used with internal company codebases. This means we are mostly left with OpenAI, Anthropic or Google’s models.&lt;/li&gt; &lt;li&gt;Since code generation is cheap now (LLMs), going out of the way for thoughtful tests, readability, and PR documentation is the minimum now.&lt;/li&gt; &lt;li&gt;Code cannot be merged at the rate it is produced because you have to own what was generated. The main gain we get is elevation from generation to checking, which is faster but not a substitute for skills.&lt;/li&gt; &lt;li&gt;Because you have to own the work, you have to be competent in that area. Paradoxically, if LLMs are relied on too much, they can hinder your ability to develop enough competence to supervise the work.&lt;/li&gt; &lt;li&gt;On the flip side, LLMs do allow greater exposure to the problem set much faster: fail fast → solve → get better (rapid iteration). In other words, they complement your agency. It remains an open question which of these two wins out for developing competence.&lt;/li&gt; &lt;li&gt;Rapid comprehension appears to be the most standout capability of LLMs over humans. So the longer the longer and the richer the context the most we can get out of LLMs. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://www.ubaada.com/post/a32d3df0"&gt;Original Post&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grey-seagull"&gt; /u/grey-seagull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjnbh8/thoughts_on_llms_closed_and_opensource_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjnbh8/thoughts_on_llms_closed_and_opensource_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjnbh8/thoughts_on_llms_closed_and_opensource_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T06:38:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjp29u</id>
    <title>So THAT'S why generations take so long sometimes</title>
    <updated>2026-01-22T08:23:01+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjp29u/so_thats_why_generations_take_so_long_sometimes/"&gt; &lt;img alt="So THAT'S why generations take so long sometimes" src="https://external-preview.redd.it/NDExcmt3bHcxdmVnMbqlCOGHRc5n3_cDftfsgD3DArw7u7f4exLoWTuSZ93a.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02862344fe7ac8a58e90fba9b10a59c1d8140d34" title="So THAT'S why generations take so long sometimes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6p9cu9rw1veg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjp29u/so_thats_why_generations_take_so_long_sometimes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjp29u/so_thats_why_generations_take_so_long_sometimes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T08:23:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
