<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-21T11:34:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ps2cjk</id>
    <title>Would a Ryzen AI Max+ 395 benefit from dedicated GPU?</title>
    <updated>2025-12-21T09:10:58+00:00</updated>
    <author>
      <name>/u/Larkonath</name>
      <uri>https://old.reddit.com/user/Larkonath</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I just ordered a Framework desktop motherboard, first time I will have some hardware that let me play with some local AI.&lt;/p&gt; &lt;p&gt;The motherboard has a 4x pci express port, so with an adapter I could put a gpu on it. &lt;/p&gt; &lt;p&gt;And before ordering a case and a power supply, I was wondering if it would benefit from a dedicated GPU like a 5060 or 5070 ti (or should it be an AMD GPU?)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Larkonath"&gt; /u/Larkonath &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2cjk/would_a_ryzen_ai_max_395_benefit_from_dedicated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2cjk/would_a_ryzen_ai_max_395_benefit_from_dedicated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2cjk/would_a_ryzen_ai_max_395_benefit_from_dedicated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T09:10:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps2vi5</id>
    <title>Are you using a SysPrompt for casual AI usage?</title>
    <updated>2025-12-21T09:46:17+00:00</updated>
    <author>
      <name>/u/PromptInjection_</name>
      <uri>https://old.reddit.com/user/PromptInjection_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because here's what I notice:&lt;/p&gt; &lt;p&gt;When I let the AI, for example, analyze a newspaper article without a system prompt, the result is usually so harmless you could talk about it in a job interview.&lt;/p&gt; &lt;p&gt;But if you include in the system prompt something like &amp;quot;Be clear and direct&amp;quot; - the outcome can be COMPLETELY DIFFERENT. The AI then says, &amp;quot;What this article is trying to sell you here is pure framing and deception.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PromptInjection_"&gt; /u/PromptInjection_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2vi5/are_you_using_a_sysprompt_for_casual_ai_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2vi5/are_you_using_a_sysprompt_for_casual_ai_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2vi5/are_you_using_a_sysprompt_for_casual_ai_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T09:46:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1prgi41</id>
    <title>AMD Radeon AI PRO R9700 benchmarks with ROCm and Vulkan and llama.cpp</title>
    <updated>2025-12-20T15:10:10+00:00</updated>
    <author>
      <name>/u/Finguili</name>
      <uri>https://old.reddit.com/user/Finguili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"&gt; &lt;img alt="AMD Radeon AI PRO R9700 benchmarks with ROCm and Vulkan and llama.cpp" src="https://b.thumbs.redditmedia.com/XbjSf-XY5RQvB4Yp042RgNWFOniLVUqUixGoKSlUncc.jpg" title="AMD Radeon AI PRO R9700 benchmarks with ROCm and Vulkan and llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently in comments to various posts about R9700 many people asked for benchmarks, so I took some of my time to run them.&lt;/p&gt; &lt;p&gt;Spec: AMD Ryzen 7 5800X (16) @ 5.363 GHz, 64 GiB DDR4 RAM @ 3600 MHz, AMD Radeon AI PRO R9700.&lt;/p&gt; &lt;p&gt;Software is running on Arch Linux with ROCm 7.1.1 (my Comfy install is still using a slightly older PyTorch nightly release with ROCm 7.0).&lt;/p&gt; &lt;p&gt;Disclaimer: I was lazy and instructed the LLM to generate Python scripts for plots. It’s possible that it hallucinated some values while copying tables into the script.&lt;/p&gt; &lt;h1&gt;Novel summarisation&lt;/h1&gt; &lt;p&gt;Let’s start with a practical task to see how it performs in the real world. The LLM is instructed to summarise each chapter of a 120k-word novel individually, with a script parallelising calls to the local API to take advantage of batched inference. The batch size was selected so that there is at least 15k ctx per request.&lt;/p&gt; &lt;p&gt;Mistral Small: batch=3; 479s total time; ~14k output words&lt;/p&gt; &lt;p&gt;gpt-oss 20B: batch=32; 113s; 18k output words (exluding reasoning)&lt;/p&gt; &lt;p&gt;Below are detailed benchmarks per model, with some diffusion models at the end. I run them with logical batch size (`-b` flag) set to 1024, as I noticed that prompt processing slowed much more with default value 2048, though I only measured in for Mistral Small, so it might not be optimal for every model.&lt;/p&gt; &lt;p&gt;TLDR is that ROCm usually has slightly faster prompt processing and takes less performance hit from long context, while Vulkan usually has slightly faster tg.&lt;/p&gt; &lt;h1&gt;gpt-oss 20B MXFP4&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3qwo0a77jd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c0f02f7d3832d70457663b8692f16e8b4846d8"&gt;https://preview.redd.it/3qwo0a77jd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c0f02f7d3832d70457663b8692f16e8b4846d8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Batched ROCm (&lt;code&gt;llama-batched-bench -m ~/Pobrane/gpt-oss-20b-mxfp4.gguf -ngl 99 --ctx-size 262144 -fa 1 -npp 1024 -ntg 512 -npl 1,2,4,8,16,32 -b 1024&lt;/code&gt;):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.356&lt;/td&gt; &lt;td align="left"&gt;2873.01&lt;/td&gt; &lt;td align="left"&gt;3.695&lt;/td&gt; &lt;td align="left"&gt;138.55&lt;/td&gt; &lt;td align="left"&gt;4.052&lt;/td&gt; &lt;td align="left"&gt;379.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;0.439&lt;/td&gt; &lt;td align="left"&gt;4662.19&lt;/td&gt; &lt;td align="left"&gt;6.181&lt;/td&gt; &lt;td align="left"&gt;165.67&lt;/td&gt; &lt;td align="left"&gt;6.620&lt;/td&gt; &lt;td align="left"&gt;464.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;0.879&lt;/td&gt; &lt;td align="left"&gt;4658.93&lt;/td&gt; &lt;td align="left"&gt;7.316&lt;/td&gt; &lt;td align="left"&gt;279.92&lt;/td&gt; &lt;td align="left"&gt;8.196&lt;/td&gt; &lt;td align="left"&gt;749.67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;1.784&lt;/td&gt; &lt;td align="left"&gt;4592.69&lt;/td&gt; &lt;td align="left"&gt;8.943&lt;/td&gt; &lt;td align="left"&gt;458.02&lt;/td&gt; &lt;td align="left"&gt;10.727&lt;/td&gt; &lt;td align="left"&gt;1145.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;24576&lt;/td&gt; &lt;td align="left"&gt;3.584&lt;/td&gt; &lt;td align="left"&gt;4571.87&lt;/td&gt; &lt;td align="left"&gt;12.954&lt;/td&gt; &lt;td align="left"&gt;632.37&lt;/td&gt; &lt;td align="left"&gt;16.538&lt;/td&gt; &lt;td align="left"&gt;1486.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;49152&lt;/td&gt; &lt;td align="left"&gt;7.211&lt;/td&gt; &lt;td align="left"&gt;4544.13&lt;/td&gt; &lt;td align="left"&gt;19.088&lt;/td&gt; &lt;td align="left"&gt;858.36&lt;/td&gt; &lt;td align="left"&gt;26.299&lt;/td&gt; &lt;td align="left"&gt;1869.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Batched Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.415&lt;/td&gt; &lt;td align="left"&gt;2465.21&lt;/td&gt; &lt;td align="left"&gt;2.997&lt;/td&gt; &lt;td align="left"&gt;170.84&lt;/td&gt; &lt;td align="left"&gt;3.412&lt;/td&gt; &lt;td align="left"&gt;450.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;0.504&lt;/td&gt; &lt;td align="left"&gt;4059.63&lt;/td&gt; &lt;td align="left"&gt;8.555&lt;/td&gt; &lt;td align="left"&gt;119.70&lt;/td&gt; &lt;td align="left"&gt;9.059&lt;/td&gt; &lt;td align="left"&gt;339.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;1.009&lt;/td&gt; &lt;td align="left"&gt;4059.83&lt;/td&gt; &lt;td align="left"&gt;10.528&lt;/td&gt; &lt;td align="left"&gt;194.53&lt;/td&gt; &lt;td align="left"&gt;11.537&lt;/td&gt; &lt;td align="left"&gt;532.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;2.042&lt;/td&gt; &lt;td align="left"&gt;4011.59&lt;/td&gt; &lt;td align="left"&gt;13.553&lt;/td&gt; &lt;td align="left"&gt;302.22&lt;/td&gt; &lt;td align="left"&gt;15.595&lt;/td&gt; &lt;td align="left"&gt;787.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;24576&lt;/td&gt; &lt;td align="left"&gt;4.102&lt;/td&gt; &lt;td align="left"&gt;3994.08&lt;/td&gt; &lt;td align="left"&gt;16.222&lt;/td&gt; &lt;td align="left"&gt;505.01&lt;/td&gt; &lt;td align="left"&gt;20.324&lt;/td&gt; &lt;td align="left"&gt;1209.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;49152&lt;/td&gt; &lt;td align="left"&gt;8.265&lt;/td&gt; &lt;td align="left"&gt;3964.67&lt;/td&gt; &lt;td align="left"&gt;19.416&lt;/td&gt; &lt;td align="left"&gt;843.85&lt;/td&gt; &lt;td align="left"&gt;27.681&lt;/td&gt; &lt;td align="left"&gt;1775.67&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w3ebchfajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c56e5c540b4463088b24d0521b8493056c52ca7a"&gt;https://preview.redd.it/w3ebchfajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c56e5c540b4463088b24d0521b8493056c52ca7a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/87a0hmgajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be7b601a45118f3dde71c061c6cda8d1ad27a9bb"&gt;https://preview.redd.it/87a0hmgajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be7b601a45118f3dde71c061c6cda8d1ad27a9bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;3859.15 ± 370.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;142.62 ± 1.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;3344.57 ± 15.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;134.42 ± 0.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;2617.02 ± 17.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;127.62 ± 1.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;1819.82 ± 36.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;119.04 ± 0.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;999.01 ± 72.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;101.80 ± 0.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;680.86 ± 83.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;89.82 ± 0.67&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;2648.20 ± 201.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;173.13 ± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;3012.69 ± 12.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;167.87 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;2295.56 ± 13.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;159.13 ± 0.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;1566.27 ± 25.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;148.42 ± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;919.79 ± 5.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;129.22 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;518.21 ± 1.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;114.46 ± 1.20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;gpt-oss 120B MXFP4&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8npf7xbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ac8c09710db6e2020521d8f9385ac5ac587c80c"&gt;https://preview.redd.it/l8npf7xbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ac8c09710db6e2020521d8f9385ac5ac587c80c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3c4y66wbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92dbf2bbbf3fddaf2f09d4c2558d12b36961cc2c"&gt;https://preview.redd.it/3c4y66wbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92dbf2bbbf3fddaf2f09d4c2558d12b36961cc2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm (&lt;code&gt;llama-bench -m ~/Pobrane/gpt-oss-120b-mxfp4-00001-of-00003.gguf --n-cpu-moe 21 -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;279.07 ± 133.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;26.79 ± 0.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;498.33 ± 6.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;26.47 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;479.48 ± 4.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;25.97 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;425.65 ± 2.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;25.31 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;339.71 ± 10.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;23.86 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;277.79 ± 12.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;22.53 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;211.64 ± 7.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;26.80 ± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;220.63 ± 7.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;26.54 ± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;203.32 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;26.10 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;187.31 ± 4.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;25.37 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;163.22 ± 5.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;24.06 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;137.56 ± 2.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;22.83 ± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Mistral Small 3.2 24B Q8&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f9q4ocndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ff45c2062852fd2c2ab9b08c1ec39904ee5ea1"&gt;https://preview.redd.it/f9q4ocndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ff45c2062852fd2c2ab9b08c1ec39904ee5ea1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h6w60gndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09680b3edb2f48de6924984f2ef78d25b2240e0a"&gt;https://preview.redd.it/h6w60gndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09680b3edb2f48de6924984f2ef78d25b2240e0a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context (&lt;code&gt;llama-bench -m mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;):&lt;/p&gt; &lt;p&gt;ROCm:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1563.27 ± 0.78&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.59 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;1146.39 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;23.03 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;852.24 ± 55.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;22.41 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;557.38 ± 79.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;21.38 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;351.07 ± 31.77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;19.48 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;256.75 ± 16.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;17.90 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1033.43 ± 0.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;24.47 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;705.07 ± 84.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;23.69 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;558.55 ± 58.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;22.94 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;404.23 ± 35.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;21.66 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;257.74 ± 12.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;11.25 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;167.42 ± 6.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;10.93 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qnvl209fjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d5d1872bd290d3b31a32e9e0940dc9f4306947f"&gt;https://preview.redd.it/qnvl209fjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d5d1872bd290d3b31a32e9e0940dc9f4306947f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Batched ROCm (&lt;code&gt;llama-batched-bench -m ~/Pobrane/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf -ngl 99 --ctx-size 32798 -fa 1 -npp 1024 -ntg 512 -npl 1,2,4,8 -b 1024&lt;/code&gt;):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.719&lt;/td&gt; &lt;td align="left"&gt;1423.41&lt;/td&gt; &lt;td align="left"&gt;21.891&lt;/td&gt; &lt;td align="left"&gt;23.39&lt;/td&gt; &lt;td align="left"&gt;22.610&lt;/td&gt; &lt;td align="left"&gt;67.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;1.350&lt;/td&gt; &lt;td align="left"&gt;1516.62&lt;/td&gt; &lt;td align="left"&gt;24.193&lt;/td&gt; &lt;td align="left"&gt;42.33&lt;/td&gt; &lt;td align="left"&gt;25.544&lt;/td&gt; &lt;td align="left"&gt;120.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;2.728&lt;/td&gt; &lt;td align="left"&gt;1501.73&lt;/td&gt; &lt;td align="left"&gt;25.139&lt;/td&gt; &lt;td align="left"&gt;81.47&lt;/td&gt; &lt;td align="left"&gt;27.867&lt;/td&gt; &lt;td align="left"&gt;220.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;5.468&lt;/td&gt; &lt;td align="left"&gt;1498.09&lt;/td&gt; &lt;td align="left"&gt;33.595&lt;/td&gt; &lt;td align="left"&gt;121.92&lt;/td&gt; &lt;td align="left"&gt;39.063&lt;/td&gt; &lt;td align="left"&gt;314.57&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Batched Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;1.126&lt;/td&gt; &lt;td align="left"&gt;909.50&lt;/td&gt; &lt;td align="left"&gt;21.095&lt;/td&gt; &lt;td align="left"&gt;24.27&lt;/td&gt; &lt;td align="left"&gt;22.221&lt;/td&gt; &lt;td align="left"&gt;69.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;2.031&lt;/td&gt; &lt;td align="left"&gt;1008.54&lt;/td&gt; &lt;td align="left"&gt;21.961&lt;/td&gt; &lt;td align="left"&gt;46.63&lt;/td&gt; &lt;td align="left"&gt;23.992&lt;/td&gt; &lt;td align="left"&gt;128.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;4.089&lt;/td&gt; &lt;td align="left"&gt;1001.70&lt;/td&gt; &lt;td align="left"&gt;23.051&lt;/td&gt; &lt;td align="left"&gt;88.85&lt;/td&gt; &lt;td align="left"&gt;27.140&lt;/td&gt; &lt;td align="left"&gt;226.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;8.196&lt;/td&gt; &lt;td align="left"&gt;999.45&lt;/td&gt; &lt;td align="left"&gt;29.695&lt;/td&gt; &lt;td align="left"&gt;137.94&lt;/td&gt; &lt;td align="left"&gt;37.891&lt;/td&gt; &lt;td align="left"&gt;324.30&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Qwen3 VL 32B Q5_K_L&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lg0s9f4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00da017f55bfd0ec1ac9477b2dc61bc907c591a4"&gt;https://preview.redd.it/lg0s9f4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00da017f55bfd0ec1ac9477b2dc61bc907c591a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qivjlf4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52e7caf0e45a2a523d86eb22709b9f3a47807cba"&gt;https://preview.redd.it/qivjlf4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52e7caf0e45a2a523d86eb22709b9f3a47807cba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm (&lt;code&gt;llama-bench -m ~/Pobrane/Qwen_Qwen3-VL-32B-Instruct-Q5_K_L.gguf -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;796.33 ± 0.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;22.56 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;425.83 ± 128.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;21.11 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;354.85 ± 34.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;20.14 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;228.75 ± 14.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;18.46 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;134.29 ± 5.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;15.75 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note: 48k doesn’t fit.&lt;/p&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;424.14 ± 1.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.93 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;300.68 ± 9.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;22.69 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;226.81 ± 11.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;21.65 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;152.41 ± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;19.78 ± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;80.38 ± 0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;10.39 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Gemma 3 27B Q6_K_L&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tcabncnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7765dc64ada8bf70e7603ceec0da012d6430dcbd"&gt;https://preview.redd.it/tcabncnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7765dc64ada8bf70e7603ceec0da012d6430dcbd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vll62lnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b42b7237c72fe36638a1550531e1aa0e76ac49ea"&gt;https://preview.redd.it/vll62lnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b42b7237c72fe36638a1550531e1aa0e76ac49ea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm (&lt;code&gt;llama-bench -m ~/Pobrane/google_gemma-3-27b-it-Q6_K_L.gguf -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;659.05 ± 0.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.25 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;582.29 ± 10.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;21.04 ± 2.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;531.76 ± 40.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;22.20 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;478.30 ± 58.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;21.67 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;418.48 ± 51.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;20.71 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;373.22 ± 40.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;19.78 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;664.79 ± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;24.63 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;593.41 ± 12.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;23.70 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;518.78 ± 58.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;23.18 ± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;492.78 ± 19.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;22.61 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;372.34 ± 1.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;21.26 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;336.42 ± 19.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;20.15 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Gemma 2 9B BF16&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lnnp237jjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a386123bf9af1aaaa3e26680cb480d6f6debd7a"&gt;https://preview.redd.it/lnnp237jjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a386123bf9af1aaaa3e26680cb480d6f6debd7a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Batched ROCm (&lt;code&gt;llama-batched-bench -m ~/Pobrane/gemma2-test-bf16_0.gguf -ngl 99 --ctx-size 32798 -fa 1 -npp 1024 -ntg 512 -npl 1,2,4,8 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;2.145&lt;/td&gt; &lt;td align="left"&gt;477.39&lt;/td&gt; &lt;td align="left"&gt;17.676&lt;/td&gt; &lt;td align="left"&gt;28.97&lt;/td&gt; &lt;td align="left"&gt;19.821&lt;/td&gt; &lt;td align="left"&gt;77.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;3.948&lt;/td&gt; &lt;td align="left"&gt;518.70&lt;/td&gt; &lt;td align="left"&gt;19.190&lt;/td&gt; &lt;td align="left"&gt;53.36&lt;/td&gt; &lt;td align="left"&gt;23.139&lt;/td&gt; &lt;td align="left"&gt;132.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;7.992&lt;/td&gt; &lt;td align="left"&gt;512.50&lt;/td&gt; &lt;td align="left"&gt;25.012&lt;/td&gt; &lt;td align="left"&gt;81.88&lt;/td&gt; &lt;td align="left"&gt;33.004&lt;/td&gt; &lt;td align="left"&gt;186.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;16.025&lt;/td&gt; &lt;td align="left"&gt;511.20&lt;/td&gt; &lt;td align="left"&gt;27.818&lt;/td&gt; &lt;td align="left"&gt;147.24&lt;/td&gt; &lt;td align="left"&gt;43.844&lt;/td&gt; &lt;td align="left"&gt;280.27&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For some reason this one has terribly slow prompt processing on ROCm.&lt;/p&gt; &lt;p&gt;Batched Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.815&lt;/td&gt; &lt;td align="left"&gt;1256.70&lt;/td&gt; &lt;td align="left"&gt;18.187&lt;/td&gt; &lt;td align="left"&gt;28.15&lt;/td&gt; &lt;td align="left"&gt;19.001&lt;/td&gt; &lt;td align="left"&gt;80.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;1.294&lt;/td&gt; &lt;td align="left"&gt;1582.42&lt;/td&gt; &lt;td align="left"&gt;19.690&lt;/td&gt; &lt;td align="left"&gt;52.01&lt;/td&gt; &lt;td align="left"&gt;20.984&lt;/td&gt; &lt;td align="left"&gt;146.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;2.602&lt;/td&gt; &lt;td align="left"&gt;1574.33&lt;/td&gt; &lt;td align="left"&gt;23.380&lt;/td&gt; &lt;td align="left"&gt;87.60&lt;/td&gt; &lt;td align="left"&gt;25.982&lt;/td&gt; &lt;td align="left"&gt;236.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;5.220&lt;/td&gt; &lt;td align="left"&gt;1569.29&lt;/td&gt; &lt;td align="left"&gt;30.615&lt;/td&gt; &lt;td align="left"&gt;133.79&lt;/td&gt; &lt;td align="left"&gt;35.835&lt;/td&gt; &lt;td align="left"&gt;342.90&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Diffusion&lt;/h1&gt; &lt;p&gt;All using ComfyUI.&lt;/p&gt; &lt;p&gt;Z-image, prompt cached, 9 steps, 1024×1024: 7.5 s (6.3 s with torch compile), ~8.1 s with prompt processing.&lt;/p&gt; &lt;p&gt;SDXL, v-pred model, 1024×1024, 50 steps, Euler ancestral cfg++, batch 4: 44.5 s (Comfy shows 1.18 it/s, so 4.72 it/s after normalising for batch size and without counting VAE decode). With torch compile I get 41.2 s and 5 it/s after normalising for batch count.&lt;/p&gt; &lt;p&gt;Flux 2 dev fp8. Keep in mind that Comfy is unoptimised regarding RAM usage, and 64 GiB is simply not enough for such a large model — without &lt;code&gt;--no-cache&lt;/code&gt; it tried to load Flux weights for half an hour, using most of my swap, until I gave up. With the aforementioned flag it works, but everything has to be re-executed each time you run the workflow, including loading from disk, which slows things down. This is the only benchmark where I include weight loading in the total time.&lt;/p&gt; &lt;p&gt;1024×1024, 30 steps, no reference image: 126.2 s, 2.58 s/it for diffusion. With one reference image it’s 220 s and 5.73 s/it.&lt;/p&gt; &lt;h1&gt;Various notes&lt;/h1&gt; &lt;p&gt;I also successfully finished full LoRA training of Gemma 2 9B using Unsloth. It was surprisingly quick, but perhaps that should be expected given the small dataset (about 70 samples and 4 epochs). While I don’t remember exactly how long it took, it was definitely measured in minutes rather than hours. The process was also smooth, although Unsloth warns that 4-bit QLoRA training is broken if you want to train something larger.&lt;/p&gt; &lt;p&gt;Temperatures are stable; memory can reach 90 °C, but I have yet to see the fans spinning at 100%. The card is also not as loud as some might suggest based on the blower fan design. It’s hard to judge exactly how loud it is, but it doesn’t feel much louder than my old RX 6700 XT, and you don’t really hear it outside the room.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finguili"&gt; /u/Finguili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T15:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1prjldr</id>
    <title>What's the realistic "entry point" for a good local LLM experience going into 2026?</title>
    <updated>2025-12-20T17:21:59+00:00</updated>
    <author>
      <name>/u/alphatrad</name>
      <uri>https://old.reddit.com/user/alphatrad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice a lot of questions from people asking it they can run LLM's on their 8gb or 12gb GPU's. &lt;/p&gt; &lt;p&gt;But have noticed most builds fall into two camps: the 16GB-24GB crowd making it work with quantized models, or the absolute madlads running 96GB+ setups.&lt;/p&gt; &lt;p&gt;But there's this interesting middle ground between 24-32GB that doesn't get talked about as much.&lt;/p&gt; &lt;p&gt;So I'm curious what this community thinks: &lt;strong&gt;If someone's getting into local LLMs today, wants a genuinely usable experience (not just &amp;quot;it technically runs&amp;quot;), but still has budget constraints—what's the minimum VRAM you'd actually recommend?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Excluding Macs here since they're a whole different value proposition with unified memory.&lt;/p&gt; &lt;p&gt;My take: 24GB feels like the sweet spot for accessibility right now. You can snag a used 3090 for reasonable money, and it opens up a lot of models that just aren't practical at 16GB. If you are willing to go AMD like me, RX 7900 XTX's can be had for under a grand.&lt;/p&gt; &lt;p&gt;But I'm curious if I'm off base. Are people having legitimately good experiences at 16GB with the right model choices? Or is the jump to 24GB as game-changing as it seems?&lt;/p&gt; &lt;p&gt;What's your &amp;quot;minimum viable VRAM&amp;quot; for someone who wants to actually &lt;em&gt;use&lt;/em&gt; local LLMs, not just experiment?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alphatrad"&gt; /u/alphatrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjldr/whats_the_realistic_entry_point_for_a_good_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjldr/whats_the_realistic_entry_point_for_a_good_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prjldr/whats_the_realistic_entry_point_for_a_good_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T17:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps3yai</id>
    <title>Would you use a local Al agent that handles tasks in parallel with you?</title>
    <updated>2025-12-21T10:56:22+00:00</updated>
    <author>
      <name>/u/1Forbess</name>
      <uri>https://old.reddit.com/user/1Forbess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what if you had a local Al agent you could assign a task to — and it works independently while you focus on something else? would you use it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Forbess"&gt; /u/1Forbess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps3yai/would_you_use_a_local_al_agent_that_handles_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps3yai/would_you_use_a_local_al_agent_that_handles_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps3yai/would_you_use_a_local_al_agent_that_handles_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T10:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps47bu</id>
    <title>Measuring AI Drift: Evidence of semantic instability across LLMs under identical prompts</title>
    <updated>2025-12-21T11:12:34+00:00</updated>
    <author>
      <name>/u/Beneficial-Pear-1485</name>
      <uri>https://old.reddit.com/user/Beneficial-Pear-1485</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m sharing a preprint that defines and measures what I call “AI Drift”: semantic instability in large language model outputs under identical task conditions.&lt;/p&gt; &lt;p&gt;Using a minimal, reproducible intent-classification task, the paper shows:&lt;/p&gt; &lt;p&gt;- cross-model drift (different frontier LLMs producing different classifications for the same input)&lt;/p&gt; &lt;p&gt;- temporal drift (the same model changing its interpretation across days under unchanged prompts)&lt;/p&gt; &lt;p&gt;- drift persisting even under deterministic decoding settings (e.g., temperature = 0)&lt;/p&gt; &lt;p&gt;The goal of the paper is not to propose a solution, but to establish the existence and measurability of the phenomenon and provide simple operational metrics.&lt;/p&gt; &lt;p&gt;PDF:&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1ca-Tjl0bh_ojD0FVVwioTrk6XSy2eKp3/view?usp=drive_link"&gt;https://drive.google.com/file/d/1ca-Tjl0bh_ojD0FVVwioTrk6XSy2eKp3/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m sharing this primarily for replication and technical critique. The prompt and dataset are included in the appendix, and the experiment can be reproduced in minutes using public LLM interfaces.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Pear-1485"&gt; /u/Beneficial-Pear-1485 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps47bu/measuring_ai_drift_evidence_of_semantic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps47bu/measuring_ai_drift_evidence_of_semantic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps47bu/measuring_ai_drift_evidence_of_semantic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T11:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1prt5qz</id>
    <title>MiMo-V2-Flash - SGLang - mtp triton attention</title>
    <updated>2025-12-21T00:34:04+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some testing results on 4x 6000 Blackwell workstation cards&lt;/p&gt; &lt;p&gt;Context | Prompt | Output | E2E Speed | Acc Len&lt;br /&gt; 4K | 3,597 | 500 | 100.2 t/s | N/A | 2.40&lt;/p&gt; &lt;p&gt;8K | 7,199 | 500 | 88.2 t/s | N/A | 2.39&lt;/p&gt; &lt;p&gt;16K | 14,401 | 500 | 67.0 t/s | N/A | 2.24&lt;/p&gt; &lt;p&gt;32K | 28,804 | 500 | 54.5 t/s | N/A | 2.50&lt;/p&gt; &lt;p&gt;64K | 57,611 | 500 | 31.7 t/s | N/A | 2.23&lt;/p&gt; &lt;p&gt;100K | 90,019 | 500 | 24.5 t/s | N/A | 2.42&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prt5qz/mimov2flash_sglang_mtp_triton_attention/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prt5qz/mimov2flash_sglang_mtp_triton_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prt5qz/mimov2flash_sglang_mtp_triton_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T00:34:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pragtf</id>
    <title>Open source LLM tooling is getting eaten by big tech</title>
    <updated>2025-12-20T09:29:03+00:00</updated>
    <author>
      <name>/u/Inevitable_Wear_9107</name>
      <uri>https://old.reddit.com/user/Inevitable_Wear_9107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was using TGI for inference six months ago. Migrated to vLLM last month. Thought it was just me chasing better performance, then I read the LLM Landscape 2.0 report. Turns out 35% of projects from just three months ago already got replaced. This isn't just my stack. The whole ecosystem is churning.&lt;/p&gt; &lt;p&gt;The deeper I read, the crazier it gets. Manus blew up in March, OpenManus and OWL launched within weeks as open source alternatives, both are basically dead now. TensorFlow has been declining since 2019 and still hasn't hit bottom. The median project age in this space is 30 months.&lt;/p&gt; &lt;p&gt;Then I looked at what's gaining momentum. NVIDIA drops Dynamo, optimized for NVIDIA hardware. Google releases Gemini CLI with Google Cloud baked in. OpenAI ships Codex CLI that funnels you into their API. That's when it clicked.&lt;/p&gt; &lt;p&gt;Two years ago this space was chaotic but independent. Now the open source layer is becoming the customer acquisition layer. We're not choosing tools anymore. We're being sorted into ecosystems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Wear_9107"&gt; /u/Inevitable_Wear_9107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T09:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1prcu0t</id>
    <title>Of course it works, in case you are wondering... and it's quite faster.</title>
    <updated>2025-12-20T12:04:22+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"&gt; &lt;img alt="Of course it works, in case you are wondering... and it's quite faster." src="https://preview.redd.it/p9tf12m7nc8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e07a1563bfc8d54447cad9ed61107030cf7aff50" title="Of course it works, in case you are wondering... and it's quite faster." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9tf12m7nc8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T12:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1prjl7z</id>
    <title>Nvidia Introduces 'NitroGen': A Foundation Model for Generalist Gaming Agents | "This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI."</title>
    <updated>2025-12-20T17:21:48+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjl7z/nvidia_introduces_nitrogen_a_foundation_model_for/"&gt; &lt;img alt="Nvidia Introduces 'NitroGen': A Foundation Model for Generalist Gaming Agents | &amp;quot;This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI.&amp;quot;" src="https://external-preview.redd.it/aTRkb2lybnI3ZThnMdFL3UUz04QdHLBdqdlbFHYzvAvsN9wNsENNDP9FjT2f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6073ba6e1956995a34ece97d824f00cef66f7b9" title="Nvidia Introduces 'NitroGen': A Foundation Model for Generalist Gaming Agents | &amp;quot;This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h4&gt;TL;DR:&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;NitroGen demonstrates that we can accelerate the development of generalist AI agents by scraping internet-scale data rather than relying on slow, expensive manual labeling.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI.&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Abstract:&lt;/h4&gt; &lt;blockquote&gt; &lt;p&gt;We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: - (1) An internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, - (2) A multi-game benchmark environment that can measure cross-game generalization, and - (3) A unified vision-action model trained with large-scale behavior cloning. &lt;/p&gt; &lt;p&gt;NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. &lt;strong&gt;It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch.&lt;/strong&gt; We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h4&gt;Layman's Explanation:&lt;/h4&gt; &lt;p&gt;NVIDIA researchers bypassed the data bottleneck in embodied AI by identifying 40,000 hours of gameplay videos where streamers displayed their controller inputs on-screen, effectively harvesting free, high-quality action labels across more than 1,000 games. This approach proves that the &amp;quot;scale is all you need&amp;quot; paradigm, which drove the explosion of Large Language Models, is viable for training agents to act in complex, virtual environments using noisy internet data.&lt;/p&gt; &lt;p&gt;The resulting model &lt;strong&gt;verifies that large-scale pre-training creates transferable skills; the AI can navigate, fight, and solve puzzles in games it has never seen before, performing significantly better than models trained from scratch.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;By open-sourcing the model weights and the massive video-action dataset, the team has removed a major barrier to entry, allowing the community to immediately fine-tune these foundation models for new tasks instead of wasting compute on training from the ground up. &lt;/p&gt; &lt;hr /&gt; &lt;h5&gt;Link to the Paper: &lt;a href="https://nitrogen.minedojo.org/assets/documents/nitrogen.pdf"&gt;https://nitrogen.minedojo.org/assets/documents/nitrogen.pdf&lt;/a&gt;&lt;/h5&gt; &lt;hr /&gt; &lt;h5&gt;Link to the Project Website: &lt;a href="https://nitrogen.minedojo.org/"&gt;https://nitrogen.minedojo.org/&lt;/a&gt;&lt;/h5&gt; &lt;hr /&gt; &lt;h5&gt;Link to the HuggingFace: &lt;a href="https://huggingface.co/nvidia/NitroGen"&gt;https://huggingface.co/nvidia/NitroGen&lt;/a&gt;&lt;/h5&gt; &lt;hr /&gt; &lt;h5&gt;Link to the Open-Sourced Dataset: &lt;a href="https://huggingface.co/datasets/nvidia/NitroGen"&gt;https://huggingface.co/datasets/nvidia/NitroGen&lt;/a&gt;&lt;/h5&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zdp80umr7e8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjl7z/nvidia_introduces_nitrogen_a_foundation_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prjl7z/nvidia_introduces_nitrogen_a_foundation_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T17:21:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1prm2tq</id>
    <title>TheDrummer models meet heretic</title>
    <updated>2025-12-20T19:08:34+00:00</updated>
    <author>
      <name>/u/coder3101</name>
      <uri>https://old.reddit.com/user/coder3101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What if I abliterate the drummer's fine tune to make them a bit less censored? So, I did that and here's the collection:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/coder3101/the-drummers"&gt;https://huggingface.co/collections/coder3101/the-drummers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Magidonia-24B-v4.3&lt;/li&gt; &lt;li&gt;Cydonia-24B-v4.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There are two variants, one that reduces refusal and another that reduces KLD so as to keep the performance similar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder3101"&gt; /u/coder3101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prm2tq/thedrummer_models_meet_heretic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prm2tq/thedrummer_models_meet_heretic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prm2tq/thedrummer_models_meet_heretic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T19:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1prh5jp</id>
    <title>A Raspberry Pi + eGPU isn't as dumb as I thought</title>
    <updated>2025-12-20T15:38:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"&gt; &lt;img alt="A Raspberry Pi + eGPU isn't as dumb as I thought" src="https://b.thumbs.redditmedia.com/6loQqYyrEG88VaZ_nKHTgCav_dnTkH81E4pJ_NvuO0w.jpg" title="A Raspberry Pi + eGPU isn't as dumb as I thought" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a small selection of benchmarks from my &lt;a href="https://www.jeffgeerling.com/blog/2025/big-gpus-dont-need-big-pcs"&gt;blog post&lt;/a&gt;, I tested a variety of AMD and Nvidia cards on a Raspberry Pi CM5 using an eGPU dock (total system cost, cards excluded, around $350).&lt;/p&gt; &lt;p&gt;For larger models, the performance delta between the Pi and an Intel Core Ultra 265K PC build with 64GB of DDR5 RAM and PCIe Gen 5 was less than 5%. For llama 2 13B, the Pi was even faster for many Nvidia cards (why is that?).&lt;/p&gt; &lt;p&gt;For AMD, the Pi was much slower—to the point I'm pretty sure there's a driver issue or something the AMD drivers expect that the Pi isn't providing (yet... like a large BAR).&lt;/p&gt; &lt;p&gt;I publish all the llama-bench data in &lt;a href="https://github.com/geerlingguy/ai-benchmarks/issues?q=is%3Aissue%20state%3Aclosed"&gt;https://github.com/geerlingguy/ai-benchmarks/issues?q=is%3Aissue%20state%3Aclosed&lt;/a&gt; and multi-GPU benchmarks in &lt;a href="https://github.com/geerlingguy/ai-benchmarks/issues/44"&gt;https://github.com/geerlingguy/ai-benchmarks/issues/44&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1prh5jp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T15:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1przir5</id>
    <title>Big training projects appear to be including CoT reasoning traces in their training data.</title>
    <updated>2025-12-21T06:12:25+00:00</updated>
    <author>
      <name>/u/MaggoVitakkaVicaro</name>
      <uri>https://old.reddit.com/user/MaggoVitakkaVicaro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1przir5/big_training_projects_appear_to_be_including_cot/"&gt; &lt;img alt="Big training projects appear to be including CoT reasoning traces in their training data." src="https://external-preview.redd.it/qY52rLeZM5AmEgPsJDh_3hPjjhv02hbRWUEyDFI3OeE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=958f00a9e023c8451aac05b0976c46bd89e9e69e" title="Big training projects appear to be including CoT reasoning traces in their training data." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaggoVitakkaVicaro"&gt; /u/MaggoVitakkaVicaro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pratyushmaini.substack.com/p/reverse-engineering-a-phase-change-a96"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1przir5/big_training_projects_appear_to_be_including_cot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1przir5/big_training_projects_appear_to_be_including_cot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T06:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps26tq</id>
    <title>Video2Robot — turn any video (or Veo/Sora prompt) into humanoid robot motion</title>
    <updated>2025-12-21T09:00:34+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"&gt; &lt;img alt="Video2Robot — turn any video (or Veo/Sora prompt) into humanoid robot motion" src="https://b.thumbs.redditmedia.com/ngi9XINVJUwQcnKQJtZJJZvsFS_qFXuG2mLEME3bH7w.jpg" title="Video2Robot — turn any video (or Veo/Sora prompt) into humanoid robot motion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5r4ippkdvi8g1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30053ed178ae01fbbdd17a24e2d89783283ea13e"&gt;https://preview.redd.it/5r4ippkdvi8g1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30053ed178ae01fbbdd17a24e2d89783283ea13e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;End-to-end pipeline: Video/Prompt → Pose (PromptHMR) → Motion Retargeting (GMR) → Robot. Ships CLI + Web UI, 3D viz, and support for Unitree G1/H1 &amp;amp; Booster T1. &lt;/p&gt; &lt;p&gt;Works with Veo/Sora or your own .mp4&lt;/p&gt; &lt;p&gt;Repo &amp;amp; README: github.com/AIM-Intelligence/video2robot. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T09:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1prxpcx</id>
    <title>NVIDIA Nemotron-3-Nano-30B LLM Benchmarks Vulkan and RPC</title>
    <updated>2025-12-21T04:31:19+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a few benchmarks on Nvidia's new &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF"&gt;Nemotron-3-Nano-30B&lt;/a&gt; and will test out RPC-SERVER again.&lt;/p&gt; &lt;p&gt;More details on Mamba2-Transformer Hybrid Mixture of Experts (MoE) model is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4 Systems all running Kubuntu 24.04 to 26.04.&lt;/p&gt; &lt;p&gt;GPUs: &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;Nvidia 1080Ti 11GB&lt;/a&gt;, &lt;a href="https://www.techpowerup.com/gpu-specs/p102-100.c3100"&gt;Nvidia P102-100 &lt;/a&gt;10GB, AMD Ryzen &lt;a href="https://www.techpowerup.com/cpu-specs/ryzen-7-6800h.c2527"&gt;6800H CPU&lt;/a&gt;, 64gb DDR5 RAM with &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;iGPU 680M&lt;/a&gt; and AMD&lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt; Radeon 7900 GRE&lt;/a&gt; 16GB.&lt;/p&gt; &lt;p&gt;I also compared AMD vs Intel system, both running DDR4 and no difference in inference speeds.&lt;/p&gt; &lt;p&gt;This model is too big to fit on any of my GPUs Vram, so I used dual Nvidia GPU and RPC to avoid having CPU offloading. Also did some CPU offloading to compare. All system run with Vulkan backend.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m /Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf -fa 0,1 load_backend: loaded RPC backend from /home/czar33/vulkan/llama-b7476/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon Graphics (RADV REMBRANDT) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/czar33/vulkan/llama-b7476/libggml-vulkan.so load_backend: loaded CPU backend from /home/czar33/vulkan/llama-b7476/libggml-cpu-haswell.so &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;221.68 ± 0.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.35 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;214.63 ± 0.78&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.39 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;build: cdbada8d1 (7476) real 2m59.672s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;6800H iGPU 680M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;221.68 ± 0.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.35 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-IQ4_XS.gguf 6800H iGPU 680M&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;151.09 ± 1.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;17.63 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_1.gguf 6800H iGPU 680M&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;241.15 ± 1.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;12.77 ± 3.98&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Looks like the iGPU 680M likes Q4_1 quants for best pp512 performance and IQ4_XS for tg128.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NVIDIA GTX-1080Ti and NVIDIA P102-100&lt;/strong&gt; (21GB of combined VRAM)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_vulkan: 0 = NVIDIA GeForce GTX 1080 Ti (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 1 = NVIDIA P102-100 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/czar33/vulkan/llama-b7484/libggml-vulkan.so load_backend: loaded CPU backend from /home/czar33/vulkan/llama-b7484/libggml-cpu-haswell.so | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | nemotron_h_moe 31B.A3.5B IQ4_XS - 4.25 bpw | 16.91 GiB | 31.58 B | Vulkan | 99 | pp512 | 121.23 ± 2.85 | | nemotron_h_moe 31B.A3.5B IQ4_XS - 4.25 bpw | 16.91 GiB | 31.58 B | Vulkan | 99 | tg128 | 64.86 ± 0.15 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;build: ce734a8a2 (7484)&lt;/p&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-IQ4_XS.gguf (16.91 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;121.23 ± 2.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.86 ± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_1.gguf (18.67 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;133.86 ± 2.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;67.99 ± 0.25&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf -ngl 44 (22.88 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;103.30 ± 0.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;34.05 ± 0.92&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Q4_K_M too big for 21GB VRAM so needs &lt;code&gt;-ngl 44&lt;/code&gt; to run and almost a 50% hit for about 1 to 2 GB offload.&lt;/p&gt; &lt;p&gt;Now lets see difference between offload &lt;code&gt;-ngl&lt;/code&gt; and using RPC backend. Using Q4_K_M, Q5_K_M and Q6_K models.&lt;/p&gt; &lt;p&gt;My client is the AMD Radeon 7900 GRE 16GB VRAM GPU:&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-bench -m /Nemotron-3-Nano-30B-A3B-Q5_K_M.gguf --rpc&lt;/code&gt; &lt;a href="http://10.0.0.173:50054"&gt;&lt;code&gt;10.0.0.173:50054&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and the RPC-SERVER is running dual GPU GTX-1080Ti/P102-100 on a gigabit network.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-b7491/rpc-server -c --host 0.0.0.0 --port 50054 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;RX 7900GRE (16GB VRAM), GTX1080Ti + P102-100 (21GB VRAM) using RPC&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;time /llama-b7491/llama-bench -m /Nemotron-3-Nano-30B-A3B-Q5_K_M.gguf --rpc &lt;a href="http://10.0.0.173:50054"&gt;10.0.0.173:50054&lt;/a&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_backend: loaded RPC backend from /media/czar33/x_2tb/vulkan/llama-b7491/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix c ores: KHR_coopmat load_backend: loaded Vulkan backend from /media/czar33/x_2tb/vulkan/llama-b7491/libggml-vulkan.so load_backend: loaded CPU backend from /media/czar33/x_2tb/vulkan/llama-b7491/libggml-cpu-haswell.so | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | nemotron_h_moe 31B.A3.5B Q5_K - Medium | 24.35 GiB | 31.58 B | Vulkan,RPC | 99 | pp512 | 112.32 ± 1.81 | | nemotron_h_moe 31B.A3.5B Q5_K - Medium | 24.35 GiB | 31.58 B | Vulkan,RPC | 99 | tg128 | 40.79 ± 0.22 | build: 52ab19df6 (7491) real 2m28.029s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf (22.88 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;112.04 ± 1.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;41.46 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q5_K_M.gguf (24.35 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;112.32 ± 1.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;40.79 ± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q6_K.gguf (31.20 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;113.58 ± 1.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;39.95 ± 0.76&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;COMPARED to -ngl offloading on NVIDIA GTX-1080Ti and P102-100 (21GB VRAM) at Q6_K&lt;/p&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q6_K.gguf -ngl 30&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;82.68 ± 0.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;21.78 ± 0.79&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'm impressed on being able to run the Q6_K model at a very respectable speed across 2 system and 3 GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prxpcx/nvidia_nemotron3nano30b_llm_benchmarks_vulkan_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prxpcx/nvidia_nemotron3nano30b_llm_benchmarks_vulkan_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prxpcx/nvidia_nemotron3nano30b_llm_benchmarks_vulkan_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T04:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pry2v7</id>
    <title>People using Devstral 2 123b, how has it been working for you? What have you been using it with?</title>
    <updated>2025-12-21T04:51:16+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People using Devstral 2 123b, how has it been working for you? What have you been using it with? &lt;/p&gt; &lt;p&gt;I tried it with Claude Code Router and it's not bad! I think just with a few rough tests it seems better at agentic stuff than GPT OSS 120b, however GPT OSS's code quality seems a bit better. HOWEVER, I'm using OSS 120b at Q4 and Devstral at IQ3. &lt;/p&gt; &lt;p&gt;GPT OSS 120b is also faster because it's MoE, but Devstral 2 123b works pretty well with speculative decoding with a heavily quantized Devstral 2 20b. &lt;/p&gt; &lt;p&gt;How is your luck with it? What strengths and weaknesses does it have with your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pry2v7/people_using_devstral_2_123b_how_has_it_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pry2v7/people_using_devstral_2_123b_how_has_it_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pry2v7/people_using_devstral_2_123b_how_has_it_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T04:51:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1prwhb1</id>
    <title>is it a good deal? 64GB VRAM @ 1,058 USD</title>
    <updated>2025-12-21T03:25:47+00:00</updated>
    <author>
      <name>/u/bohemianLife1</name>
      <uri>https://old.reddit.com/user/bohemianLife1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prwhb1/is_it_a_good_deal_64gb_vram_1058_usd/"&gt; &lt;img alt="is it a good deal? 64GB VRAM @ 1,058 USD" src="https://preview.redd.it/sr0bemp03h8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3483188d964c72719dc2a2e2c6a49ccdbdf22371" title="is it a good deal? 64GB VRAM @ 1,058 USD" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This Black Friday, I found an Nvidia Jetson AGX Orin 64GB developer kit for $1,058. It usually goes for $2,000, and if you're in India like I am, it retails around $2,370.61. For comparison, the 5090, which is a 32GB card, costs $2,000 right now.&lt;/p&gt; &lt;p&gt;A little background: in my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"&gt;previous post&lt;/a&gt;, I asked the community which open-source model I could use locally to achieve similar performance to GPT-4o-mini with a 16GB VRAM constraint, and the unanimous conclusion was that more VRAM is required.&lt;/p&gt; &lt;p&gt;So I began my search and found this deal (&lt;a href="https://www.amazon.com/NVIDIA-Jetson-Orin-64GB-Developer/dp/B0BYGB3WV4?crid=1Y0GEMVGIT2Y1&amp;amp;dib=eyJ2IjoiMSJ9.FoThX8FZ94bjsnPOOKsYeOU_z7hyFtfGlHRIhkasZV2n3k3fXTbvCidX2BS21F6ho6cCeKibmPpVZ__v6ESMpAPJV0GrTdf9P_Os4hVMzc0ACZbLbOAe6eGI_zkvEeb4kGLxv1F4I1PCp1dryARl0-d4TyqvQtQqJGFMyqcSpEX3yq317tO2ns0-i1_F45_RSj8ia8hONnO1csZjWVJl5MP-QwhkOIy5HVrbgz__9mc.rmCLen5N1BOvx4gErZosaBavA_JDBwagBfDOxG0vdBY&amp;amp;dib_tag=se&amp;amp;keywords=nvidia%2Bjetson%2Bagx%2Borin%2B64gb&amp;amp;qid=1766285654&amp;amp;sprefix=nvidia%2Bjetson%2Bagx%2Caps%2C488&amp;amp;sr=8-1&amp;amp;th=1"&gt;out of stock now&lt;/a&gt;) and asked someone from the US to buy it and bring it to India.&lt;/p&gt; &lt;p&gt;The reason for this purchase: I've built an AI Voice Agent platform that handles pre-sales and post-sales for any company. This voice pipeline runs on three models in a cascading fashion: (VAD + Turn Detection) → STT → LLM → TTS. Since I need to host multiple models, VRAM is a bigger constraint than processing power.&lt;/p&gt; &lt;p&gt;So, instead of a consumer card like the 5090 (32GB), which offers great processing power, I ended up purchasing the Jetson AGX Orin (64GB).&lt;/p&gt; &lt;p&gt;I'll continue the chain of posting with my results of running voice agents specific models on this machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bohemianLife1"&gt; /u/bohemianLife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr0bemp03h8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prwhb1/is_it_a_good_deal_64gb_vram_1058_usd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prwhb1/is_it_a_good_deal_64gb_vram_1058_usd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T03:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps2h9r</id>
    <title>I built an open source voice assistant that runs Whisper + Qwen 2.5 entirely in the browser via WASM</title>
    <updated>2025-12-21T09:19:40+00:00</updated>
    <author>
      <name>/u/muthukrishnan749</name>
      <uri>https://old.reddit.com/user/muthukrishnan749</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with running a full voice assistant pipeline in the browser – no server, no API calls, everything local.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ps2h9r/video/i4vm3hmnyi8g1/player"&gt;https://reddit.com/link/1ps2h9r/video/i4vm3hmnyi8g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live demo: &lt;a href="https://ava.muthu.co"&gt;https://ava.muthu.co&lt;/a&gt;&lt;br /&gt; Source: &lt;a href="https://github.com/muthuspark/ava"&gt;https://github.com/muthuspark/ava&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;STT: Whisper tiny-en (q5_1, ~31MB) via whisper-web-transcriber&lt;/li&gt; &lt;li&gt;LLM: Qwen 2.5 0.5B Instruct (q4_k_m, ~350MB) via Wllama (llama.cpp WASM port)&lt;/li&gt; &lt;li&gt;TTS: Native browser SpeechSynthesis API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How it works:&lt;br /&gt; The pipeline streams – as the LLM generates tokens, I detect sentence boundaries and queue them for TTS immediately. So it starts speaking before the full response is ready.&lt;/p&gt; &lt;p&gt;Performance (on my machine):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Whisper inference: ~0.3-0.5s&lt;/li&gt; &lt;li&gt;LLM inference: ~1-2s for short responses&lt;/li&gt; &lt;li&gt;End-to-end latency: ~2-3s&lt;/li&gt; &lt;li&gt;Memory: 500MB-1GB during operation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Limitations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Doesn't work on mobile yet&lt;/li&gt; &lt;li&gt;Chrome/Edge only (needs SharedArrayBuffer)&lt;/li&gt; &lt;li&gt;0.5B model is pretty limited in capability&lt;/li&gt; &lt;li&gt;English only&lt;/li&gt; &lt;li&gt;First load is ~380MB (cached after)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I chose Qwen 2.5 0.5B because it's the sweet spot between &amp;quot;runs in a browser&amp;quot; and &amp;quot;somewhat coherent responses.&amp;quot; Tried smaller models but they were unusable.&lt;/p&gt; &lt;p&gt;Curious if anyone has suggestions for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better small models that work well with llama.cpp WASM&lt;/li&gt; &lt;li&gt;Ways to reduce the initial load time&lt;/li&gt; &lt;li&gt;Improving Whisper accuracy without going to a larger model&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muthukrishnan749"&gt; /u/muthukrishnan749 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2h9r/i_built_an_open_source_voice_assistant_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2h9r/i_built_an_open_source_voice_assistant_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2h9r/i_built_an_open_source_voice_assistant_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T09:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps1g40</id>
    <title>Benchmark Winners Across 40+ LLM Evaluations: Patterns Without Recommendations</title>
    <updated>2025-12-21T08:11:49+00:00</updated>
    <author>
      <name>/u/abubakkar_s</name>
      <uri>https://old.reddit.com/user/abubakkar_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept seeing the same question everywhere: &lt;em&gt;“Which LLM is best?”&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So instead of opinions, I went the boring route — &lt;strong&gt;I collected benchmark winners across a wide range of tasks&lt;/strong&gt;: reasoning, math, coding, vision, OCR, multimodal QA, and real-world evaluations. For SLM (3B-25B).&lt;/p&gt; &lt;p&gt;This post is &lt;strong&gt;not a recommendation list&lt;/strong&gt;. It’s simply what the benchmarks show when you look at &lt;strong&gt;task-by-task winners&lt;/strong&gt; instead of a single leaderboard.&lt;/p&gt; &lt;p&gt;You can decide what matters &lt;em&gt;for your use case&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;Benchmark → Top Scoring Model&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Best Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AI2D&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME-2024&lt;/td&gt; &lt;td align="left"&gt;Ministral3-8B-Reasoning-2512&lt;/td&gt; &lt;td align="left"&gt;86%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ARC-C&lt;/td&gt; &lt;td align="left"&gt;LLaMA-3.1-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;83%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Arena-Hard&lt;/td&gt; &lt;td align="left"&gt;Phi-4-Reasoning-Plus&lt;/td&gt; &lt;td align="left"&gt;79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BFCL-v3&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-4B-Thinking&lt;/td&gt; &lt;td align="left"&gt;67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BigBench-Hard&lt;/td&gt; &lt;td align="left"&gt;Gemma-3-12B&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ChartQA&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CharXiv-R&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;53%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DocVQA&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;95%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DROP (Reasoning)&lt;/td&gt; &lt;td align="left"&gt;Gemma-3n-E2B&lt;/td&gt; &lt;td align="left"&gt;61%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GSM8K&lt;/td&gt; &lt;td align="left"&gt;Gemma-3-12B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;91%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HellaSwag&lt;/td&gt; &lt;td align="left"&gt;Mistral-NeMo-12B-Instruct&lt;/td&gt; &lt;td align="left"&gt;83%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HumanEval&lt;/td&gt; &lt;td align="left"&gt;Granite-3.3-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Humanity’s Last Exam&lt;/td&gt; &lt;td align="left"&gt;GPT-OSS-20B&lt;/td&gt; &lt;td align="left"&gt;11%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IfEval&lt;/td&gt; &lt;td align="left"&gt;Nemotron-Nano-9B-v2&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench&lt;/td&gt; &lt;td align="left"&gt;Nemotron-Nano-9B-v2&lt;/td&gt; &lt;td align="left"&gt;71%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench-v6&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;58%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math&lt;/td&gt; &lt;td align="left"&gt;Ministral3-8B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math-500&lt;/td&gt; &lt;td align="left"&gt;Nemotron-Nano-9B-v2&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;97%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MathVista&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;68%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MathVista-Mini&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;81%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MBPP (Python)&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Coder-7B-Instruct&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MGSM&lt;/td&gt; &lt;td align="left"&gt;Gemma-3n-E4B-Instruct&lt;/td&gt; &lt;td align="left"&gt;67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MM-MT-Bench&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;77%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro-X&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Redux&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMMLU&lt;/td&gt; &lt;td align="left"&gt;Phi-3.5-Mini-Instruct&lt;/td&gt; &lt;td align="left"&gt;55%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMMU-Pro&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMStar&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-4B-Thinking&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Multi-IF&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OCRBench&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RealWorldQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;73%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ScreenSpot-Pro&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-4B-Instruct&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SimpleQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SuperGPQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;51%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SWE-Bench-Verified&lt;/td&gt; &lt;td align="left"&gt;Devstral-Small-2&lt;/td&gt; &lt;td align="left"&gt;56%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TAU-Bench-Retail&lt;/td&gt; &lt;td align="left"&gt;GPT-OSS-20B&lt;/td&gt; &lt;td align="left"&gt;55%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WinoGrande&lt;/td&gt; &lt;td align="left"&gt;Gemma-2-9B&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Patterns I Noticed (Not Conclusions)&lt;/h1&gt; &lt;h1&gt;1. No Single Model Dominates Everything&lt;/h1&gt; &lt;p&gt;Even models that appear frequently don’t win across all categories. Performance is &lt;strong&gt;highly task-dependent&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you’re evaluating models based on one benchmark, you’re probably overfitting your expectations.&lt;/p&gt; &lt;h1&gt;2. Mid-Sized Models (7B–9B) Show Up Constantly&lt;/h1&gt; &lt;p&gt;Across math, coding, and multimodal tasks, &lt;strong&gt;sub-10B models appear repeatedly&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;That doesn’t mean they’re “better” — it does suggest &lt;strong&gt;architecture and tuning matter more than raw size&lt;/strong&gt; in many evaluations.&lt;/p&gt; &lt;h1&gt;3. Vision-Language Models Are No Longer “Vision Only”&lt;/h1&gt; &lt;p&gt;Several VL models score competitively on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;reasoning&lt;/li&gt; &lt;li&gt;OCR&lt;/li&gt; &lt;li&gt;document understanding&lt;/li&gt; &lt;li&gt;multimodal knowledge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That gap is clearly shrinking, at least in benchmark settings.&lt;/p&gt; &lt;h1&gt;4. Math, Code, and Reasoning Still Behave Differently&lt;/h1&gt; &lt;p&gt;Models that do extremely well on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Math (AIME, Math-500) often aren’t the same ones winning:&lt;/li&gt; &lt;li&gt;HumanEval or LiveCodeBench&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So “reasoning” is not one thing — benchmarks expose different failure modes.&lt;/p&gt; &lt;h1&gt;5. Large Parameter Count ≠ Guaranteed Wins&lt;/h1&gt; &lt;p&gt;Some larger models appear rarely or only in narrow benchmarks.&lt;/p&gt; &lt;p&gt;That doesn’t make them bad — it just reinforces that &lt;strong&gt;benchmarks reward specialization&lt;/strong&gt;, not general scale.&lt;/p&gt; &lt;h1&gt;Why I’m Sharing This&lt;/h1&gt; &lt;p&gt;I’m not trying to say &lt;em&gt;“this model is the best”&lt;/em&gt;. I wanted a &lt;strong&gt;task-first view&lt;/strong&gt;, because that’s how most of us actually use models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some of you care about math&lt;/li&gt; &lt;li&gt;Some about code&lt;/li&gt; &lt;li&gt;Some about OCR, docs, or UI grounding&lt;/li&gt; &lt;li&gt;Some about overall multimodal behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmarks won’t replace real-world testing — but they &lt;em&gt;do&lt;/em&gt; reveal patterns when you zoom out.&lt;/p&gt; &lt;h1&gt;Open Questions for You&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Which benchmarks do &lt;em&gt;you&lt;/em&gt; trust the most?&lt;/li&gt; &lt;li&gt;Which ones do you think are already being “over-optimized”?&lt;/li&gt; &lt;li&gt;Are there important real-world tasks you feel aren’t reflected here?&lt;/li&gt; &lt;li&gt;Do you trust &lt;strong&gt;single-score leaderboards&lt;/strong&gt;, or do you prefer task-specific evaluations like the breakdown above?&lt;/li&gt; &lt;li&gt;For people running models locally, how much weight do you personally give to &lt;strong&gt;efficiency metrics (latency, VRAM, throughput)&lt;/strong&gt; versus raw benchmark scores? (Currently am with V100, which is cloud based)&lt;/li&gt; &lt;li&gt;If you had to remove &lt;strong&gt;one benchmark entirely&lt;/strong&gt;, which one do you think adds the least signal today?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abubakkar_s"&gt; /u/abubakkar_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps1g40/benchmark_winners_across_40_llm_evaluations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps1g40/benchmark_winners_across_40_llm_evaluations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps1g40/benchmark_winners_across_40_llm_evaluations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T08:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1prjzoh</id>
    <title>Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues</title>
    <updated>2025-12-20T17:39:17+00:00</updated>
    <author>
      <name>/u/98Saman</name>
      <uri>https://old.reddit.com/user/98Saman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/"&gt; &lt;img alt="Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues" src="https://preview.redd.it/uew3kkt2be8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c74cf3336896a9944ca78303c3bc0e948e805302" title="Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/98Saman"&gt; /u/98Saman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uew3kkt2be8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T17:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pruoy7</id>
    <title>How big do we think Gemini 3 flash is</title>
    <updated>2025-12-21T01:51:33+00:00</updated>
    <author>
      <name>/u/davikrehalt</name>
      <uri>https://old.reddit.com/user/davikrehalt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hopefully the relevance to open models is clear enough. I'm curious about speculations based on speed and other things how big this model is--because it can help us understand just how strong a model something like 512Gb mac ultra can run eventually or something like 128Gb macbook. Do we think it's something that can fit in memory in a 128Gb MacBook for example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davikrehalt"&gt; /u/davikrehalt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T01:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1prw988</id>
    <title>GLM 4.7 imminent?!</title>
    <updated>2025-12-21T03:13:27+00:00</updated>
    <author>
      <name>/u/JuicyLemonMango</name>
      <uri>https://old.reddit.com/user/JuicyLemonMango</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/zRzRzRzRzRzRzR"&gt;https://github.com/zRzRzRzRzRzRzR&lt;/a&gt;, a &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; employee, appears hard at work to implement GLM 4.7 support. It's added in vLLM already.&lt;/p&gt; &lt;p&gt;What are your expectations for this, to be announced, new model? I'm both very optimistic and a little cautious at the same time.&lt;/p&gt; &lt;p&gt;Earlier in the year they, GLM itself on twitter, said that version 5.0 would be released this year. Now all i see is 4.7 which kinda gives me a feeling of the model potentially not being as great of an update as they had hoped to be. I don't think they'll top all the SOTA models in the benchmarks but i do think they will come within reach again. Say in the top 10. That's just pure wishful thinking and speculation at this point.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JuicyLemonMango"&gt; /u/JuicyLemonMango &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prw988/glm_47_imminent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prw988/glm_47_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prw988/glm_47_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T03:13:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps0jnm</id>
    <title>MiniMax 2.1 release?</title>
    <updated>2025-12-21T07:14:03+00:00</updated>
    <author>
      <name>/u/_cttt_</name>
      <uri>https://old.reddit.com/user/_cttt_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps0jnm/minimax_21_release/"&gt; &lt;img alt="MiniMax 2.1 release?" src="https://preview.redd.it/5rotyw06ci8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6746b1a77ec85f43c907ff7b7bce5a918251fc32" title="MiniMax 2.1 release?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new here and just saw the release of MiniMax M2.1, how is it compare to the other models?&lt;/p&gt; &lt;p&gt;github: &lt;a href="https://github.com/vllm-project/recipes/pull/174"&gt;https://github.com/vllm-project/recipes/pull/174&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_cttt_"&gt; /u/_cttt_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rotyw06ci8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps0jnm/minimax_21_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps0jnm/minimax_21_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T07:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We’re the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We’re excited to be here to talk all things SAM (sorry, we can’t share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
