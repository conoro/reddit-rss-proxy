<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-16T07:37:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r5o3y2</id>
    <title>cant tell if this is true or not</title>
    <updated>2026-02-15T19:49:45+00:00</updated>
    <author>
      <name>/u/panic_in_the_cosmos</name>
      <uri>https://old.reddit.com/user/panic_in_the_cosmos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o3y2/cant_tell_if_this_is_true_or_not/"&gt; &lt;img alt="cant tell if this is true or not" src="https://preview.redd.it/wfiz477bqpjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f034a759832af400ab9446fb3fd23d2155b63908" title="cant tell if this is true or not" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panic_in_the_cosmos"&gt; /u/panic_in_the_cosmos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfiz477bqpjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o3y2/cant_tell_if_this_is_true_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o3y2/cant_tell_if_this_is_true_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T19:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r61pma</id>
    <title>With the ridiculous ram prices has anyone tried optane / very fast nvme for page file</title>
    <updated>2026-02-16T06:02:57+00:00</updated>
    <author>
      <name>/u/AdventurousGold672</name>
      <uri>https://old.reddit.com/user/AdventurousGold672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it's will be much slower, but I was wondering if anyone explored this path or have insights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousGold672"&gt; /u/AdventurousGold672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r61pma/with_the_ridiculous_ram_prices_has_anyone_tried/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r61pma/with_the_ridiculous_ram_prices_has_anyone_tried/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r61pma/with_the_ridiculous_ram_prices_has_anyone_tried/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T06:02:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r61upn</id>
    <title>Anyone shipping production apps or prototypes with Local LLMs on Mobile? What's the actual use case?</title>
    <updated>2026-02-16T06:10:51+00:00</updated>
    <author>
      <name>/u/mighty-precious2</name>
      <uri>https://old.reddit.com/user/mighty-precious2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am primarily interested in knowing what use cases demands running LLMs locally instead of using cloud APIs. &lt;/p&gt; &lt;p&gt;Local LLMs have huge latency but complete privacy and I am very interested if any consumer use cases would love privacy over latency&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mighty-precious2"&gt; /u/mighty-precious2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r61upn/anyone_shipping_production_apps_or_prototypes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r61upn/anyone_shipping_production_apps_or_prototypes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r61upn/anyone_shipping_production_apps_or_prototypes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T06:10:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5x4vo</id>
    <title>Qwen3-Next-Coder uses `n for new line?</title>
    <updated>2026-02-16T02:14:12+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Qwen3-Next-Coder-80b_q4_K_M, and it seems very promising. Except, I encountered a problem where it produces `&lt;code&gt;n&lt;/code&gt; instead of &lt;code&gt;\n&lt;/code&gt; for newlines with long context like 32k.&lt;/p&gt; &lt;p&gt;It works fine with shorter context like 8192 though.&lt;/p&gt; &lt;p&gt;Has anyone experienced this?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5x4vo/qwen3nextcoder_uses_n_for_new_line/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5x4vo/qwen3nextcoder_uses_n_for_new_line/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5x4vo/qwen3nextcoder_uses_n_for_new_line/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T02:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5beqe</id>
    <title>The current top 4 models on openrouter are all open-weight</title>
    <updated>2026-02-15T10:29:20+00:00</updated>
    <author>
      <name>/u/svantana</name>
      <uri>https://old.reddit.com/user/svantana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt; &lt;img alt="The current top 4 models on openrouter are all open-weight" src="https://preview.redd.it/jjpkakoaxmjg1.png?width=140&amp;amp;height=132&amp;amp;auto=webp&amp;amp;s=0fe256dcd41b962138f1a252df9f49245bdb579d" title="The current top 4 models on openrouter are all open-weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I could be wrong but I think this is the first time this has happened. Is this a pivotal moment or just a temporary fluke?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jjpkakoaxmjg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5072055e50df1701fe5ab51ce67e1b7476f8c62d"&gt;https://preview.redd.it/jjpkakoaxmjg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5072055e50df1701fe5ab51ce67e1b7476f8c62d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/svantana"&gt; /u/svantana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T10:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r569eb</id>
    <title>PSA: NVIDIA DGX Spark has terrible CUDA &amp; software compatibility; and seems like a handheld gaming chip.</title>
    <updated>2026-02-15T05:17:53+00:00</updated>
    <author>
      <name>/u/goldcakes</name>
      <uri>https://old.reddit.com/user/goldcakes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the past week experimenting with the DGX Spark and I am about to return it. While I had understood the memory bandwidth and performance limitations, I like the CUDA ecosystem and was willing to pay the premium. Unfortunately, my experiences have been quite poor, and I suspect this is actually handheld gaming scraps that NVIDIA rushed to turn into a product to compete with Apple and Strix Halo.&lt;/p&gt; &lt;p&gt;The biggest issue: DGX Spark is not datacentre Blackwell, it's not even gaming Blackwell, it has its own special snowflake sm121 architecture. A lot of software do not work with it, or &lt;a href="https://github.com/triton-lang/triton/issues/8335#issuecomment-3417643519"&gt;have been patched to run sm80&lt;/a&gt; (Ampere, 6 years old!) codepaths which means it doesn't take advantage of blackwell optimisations.&lt;/p&gt; &lt;p&gt;When questioned about this on NVIDIA support forum, &lt;a href="https://forums.developer.nvidia.com/t/dgx-spark-sm121-software-support-is-severely-lacking-official-roadmap-needed/357663/9#p-1745639-h-1-when-will-sm121-receive-native-support-instead-of-sm80-fallbacks-10"&gt;an official NVIDIA representative said&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;sm80-class kernels can execute on DGX Spark because Tensor Core behavior is very similar, particularly for GEMM/MMAs (closer to the GeForce Ampere-style MMA model). &lt;strong&gt;DGX Spark not has tcgen05 like jetson Thor or GB200, due die space with RT Cores and DLSS algorithm&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Excuse me?? The reason we're getting cut-down tensor cores (not real blackwell) is because of RT Cores and &amp;quot;DLSS algorithm&amp;quot;? This is an AI dev kit; why would I need RT Cores, and additionally how does DLSS come into play? This makes me think they tried to turn a gaming handheld GPU (which needs/supports unified memory) into a poor competitor for a market they weren't prepared for.&lt;/p&gt; &lt;p&gt;In addition, in the same post the rep posted what appears to be LLM hallucinations, mentioning issues have been fixed in version numbers and releases for software libraries that &lt;em&gt;do not exist&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;Just be careful when buying a DGX Spark. You are not really getting a modern CUDA experience. Yes, everything works fine if you pretend you only have an Ampere, but attempting to use any Blackwell features is an exercise in futility.&lt;/p&gt; &lt;p&gt;Additionally, for something that is supposed to be ready 'out of the box', many people (including myself and servethehome) reports basic issues like &lt;strong&gt;HDMI display output&lt;/strong&gt;. I originally thought my Spark was DOA; nope; it just refuses to work with my 1080p144 viewsonic (which works with all other GPUs; including my NVIDIA ones); and had to switch to my 4K60 monitor. Dear NVIDIA, you should not have basic display output issues...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goldcakes"&gt; /u/goldcakes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T05:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5zuaw</id>
    <title>Point and laugh at my build? (Loss porn)</title>
    <updated>2026-02-16T04:25:40+00:00</updated>
    <author>
      <name>/u/Diligent-Culture-432</name>
      <uri>https://old.reddit.com/user/Diligent-Culture-432</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently fell into the rabbit hole of building a local and private AI server as affordably as possible, as someone who‚Äôs new to building a PC and running models locally but excited about the potential of this tech. But turns out it‚Äôs so slow and power inefficient to the point that it‚Äôs been completely demoralizing and discouraging. Originally had a dream of having personal intelligence on tap at home, but doesn‚Äôt seem worth it at all compared to cheap API costs now. Not even a shill for cloud providers, but just a personal confession that I need to get off my chest after weeks of working on this. Maybe this can serve as a warning to others getting into this to carefully weigh the pros and cons before considering this a ‚Äúfun hobby‚Äù to get into. &lt;/p&gt; &lt;p&gt;1x 2060Super 8GB, $0 (owned)&lt;/p&gt; &lt;p&gt;2x 5060Ti 16GB, $740&lt;/p&gt; &lt;p&gt;8x 32GB DDR4 3200 RAM, $652&lt;/p&gt; &lt;p&gt;3945WX cpu, $162.50&lt;/p&gt; &lt;p&gt;MC62-G40 mobo, $468&lt;/p&gt; &lt;p&gt;CPU cooler, $58&lt;/p&gt; &lt;p&gt;2TB NVMe SSD, $192&lt;/p&gt; &lt;p&gt;120W PSU, $130&lt;/p&gt; &lt;p&gt;PC Case, $100&lt;/p&gt; &lt;p&gt;Total RAM 256GB running at 3200&lt;/p&gt; &lt;p&gt;Total VRAM 40GB&lt;/p&gt; &lt;p&gt;Total cost $2500&lt;/p&gt; &lt;p&gt;Minimax M2.5 8_0 with context size 4096 via llama.cpp Vulkan, 3.83 tokens/second&lt;/p&gt; &lt;p&gt;Final conclusion that this time and effort was all for naught and yet another reminder of my own foolishness: priceless ‚òπÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Culture-432"&gt; /u/Diligent-Culture-432 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5zuaw/point_and_laugh_at_my_build_loss_porn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5zuaw/point_and_laugh_at_my_build_loss_porn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5zuaw/point_and_laugh_at_my_build_loss_porn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T04:25:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5zw9t</id>
    <title>gUrrT: An Intelligent Open-Source Video Understanding System A different path from traditional Large Video Language Models (LVLMs).</title>
    <updated>2026-02-16T04:28:22+00:00</updated>
    <author>
      <name>/u/OkAdministration374</name>
      <uri>https://old.reddit.com/user/OkAdministration374</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5zw9t/gurrt_an_intelligent_opensource_video/"&gt; &lt;img alt="gUrrT: An Intelligent Open-Source Video Understanding System A different path from traditional Large Video Language Models (LVLMs)." src="https://external-preview.redd.it/PBKSKihyaqWWUF931IKQyMwP8ty-mnW6fJ_AhNLozhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=357e92663107d72474c5faa85695241e72314c14" title="gUrrT: An Intelligent Open-Source Video Understanding System A different path from traditional Large Video Language Models (LVLMs)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Ask&amp;quot; is cool, but why does video understanding have to be so compute heavy? ü§®&lt;/p&gt; &lt;p&gt;Built gUrrT: A way to &amp;quot;talk to videos&amp;quot; without the soul-crushing VRAM requirements of LVLMs.&lt;/p&gt; &lt;p&gt;The idea behind gUrrT was to totally bypass the Large Video Language Model route by harnessing the power of Vision Models, Audio Transcription, Advanced Frame Sampling, and RAG and to present an opensource soln to the video understanding paradigm.&lt;/p&gt; &lt;p&gt;not trying to reinvent the wheel or put up any bogus claims of deadON BALLS Accurate. The effort is to see if video understanding can be done without computationally expensive LVLMs or complex temporal modeling .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkAdministration374"&gt; /u/OkAdministration374 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/owaismohammad/gurrt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5zw9t/gurrt_an_intelligent_opensource_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5zw9t/gurrt_an_intelligent_opensource_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T04:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5kgn0</id>
    <title>Does anyone know how Nanbeige4.1-3B can be so impressive compared with other models of similar size?</title>
    <updated>2026-02-15T17:29:10+00:00</updated>
    <author>
      <name>/u/cloudxaas</name>
      <uri>https://old.reddit.com/user/cloudxaas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seemed extremely consistent, cohesive, no repetition so far I've tested, and it works very well on small vram size.&lt;/p&gt; &lt;p&gt;How is this possible?&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; &lt;a href="https://huggingface.co/Nanbeige/Nanbeige4.1-3B"&gt;https://huggingface.co/Nanbeige/Nanbeige4.1-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cloudxaas"&gt; /u/cloudxaas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5kgn0/does_anyone_know_how_nanbeige413b_can_be_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5kgn0/does_anyone_know_how_nanbeige413b_can_be_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5kgn0/does_anyone_know_how_nanbeige413b_can_be_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T17:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r61so4</id>
    <title>We tested 5 vLLM optimizations: Prefix Cache, FP8, CPU Offload, Disagg P/D, and Sleep Mode</title>
    <updated>2026-02-16T06:07:41+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r61so4/we_tested_5_vllm_optimizations_prefix_cache_fp8/"&gt; &lt;img alt="We tested 5 vLLM optimizations: Prefix Cache, FP8, CPU Offload, Disagg P/D, and Sleep Mode" src="https://preview.redd.it/ma65us58ssjg1.png?width=140&amp;amp;height=52&amp;amp;auto=webp&amp;amp;s=d60a3b9f59472e61f8a0466077f8769755661cbe" title="We tested 5 vLLM optimizations: Prefix Cache, FP8, CPU Offload, Disagg P/D, and Sleep Mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We just published a new article on the JarvisLabs blog that dives into 5 practical techniques to optimize vLLM performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ma65us58ssjg1.png?width=4770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63ee465210c7ee2c8eeee1e680bf4af18d5a5717"&gt;https://preview.redd.it/ma65us58ssjg1.png?width=4770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63ee465210c7ee2c8eeee1e680bf4af18d5a5717&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We actually ran benchmarks on Qwen3-32B to see how much improvements these techniques actually bring to the table.&lt;/p&gt; &lt;p&gt;Here is a quick summary of the techniques we cover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prefix Caching:&lt;/strong&gt; This stops the model from re-computing parts of the prompt it has already seen. In our tests with Qwen3-32B, this increased throughput by over 250%.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FP8 KV-Cache:&lt;/strong&gt; This reduces the precision of the KV cache from 16-bit to 8-bit. It cuts memory usage roughly in half with minimal impact on accuracy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU Offloading:&lt;/strong&gt; This lets you use your system RAM to hold the KV cache when your GPU runs out of space. It helps avoid out-of-memory errors during heavy loads.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disaggregated Prefill/Decode:&lt;/strong&gt; This is a more advanced setup where you split the &amp;quot;reading&amp;quot; (prefill) and &amp;quot;writing&amp;quot; (decode) phases onto different GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Reload Sleep Mode:&lt;/strong&gt; A way to keep your model &amp;quot;warm&amp;quot; in memory without burning through resources when no one is using it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Full blog post:&lt;/strong&gt; &lt;a href="https://docs.jarvislabs.ai/blog/vllm-optimization-techniques"&gt;https://docs.jarvislabs.ai/blog/vllm-optimization-techniques&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r61so4/we_tested_5_vllm_optimizations_prefix_cache_fp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r61so4/we_tested_5_vllm_optimizations_prefix_cache_fp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r61so4/we_tested_5_vllm_optimizations_prefix_cache_fp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T06:07:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r62tlh</id>
    <title>bb25 (Bayesian BM25) v0.2.0 is out!</title>
    <updated>2026-02-16T07:05:15+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r62tlh/bb25_bayesian_bm25_v020_is_out/"&gt; &lt;img alt="bb25 (Bayesian BM25) v0.2.0 is out!" src="https://preview.redd.it/qwyslvhr2tjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6436e5fb481fec092ca3f27bc55baf5399bf3f20" title="bb25 (Bayesian BM25) v0.2.0 is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;bb25 v0.2.0 is out ‚Äî a Python + Rust implementation of Bayesian BM25 that turns search scores into calibrated probabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/instructkr/bb25"&gt;https://github.com/instructkr/bb25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A week ago, I built bb25 that turns BM25 into a probability engine! In addition to the Rust-based implementation, the paper's author shipped his own implementation. Comparing the two taught me more than the paper itself.&lt;/p&gt; &lt;p&gt;The Bayesian BM25 paper does something elegant, in that applying Bayes' theorem to BM25 scores so they become real probabilities, not arbitrary numbers. This makes hybrid search fusion mathematically principled instead of heuristic.&lt;/p&gt; &lt;p&gt;Instruct.KR's bb25 took a ground-up approach, tokenizer, inverted index, scorers, 10 experiments mapping to the paper's theorems, plus a Rust port. Jaepil's implementation took the opposite path, a thin NumPy layer that plugs into existing search systems.&lt;/p&gt; &lt;p&gt;Reading both codebases side by side, I found my document length prior has room to improvement (e.g. monotonic decay instead of symmetric bell curve), my probability AND suffered from shrinkage, and I further added automatic parameter estimation and online learning entirely.&lt;/p&gt; &lt;p&gt;bb25 v0.2.0 introduces all four. One fun discovery along the way, my Rust code already had the correct log-odds conjunction, but I had never backported it to Python. Same project, two different AND operations.&lt;/p&gt; &lt;p&gt;The deeper surprise came from a formula in the reference material. Expand the Bayesian posterior and you get the structure of an artificial neuron! Think of weighted sum, bias, sigmoid activation. Sigmoid, ReLU, Softmax, Attention all have Bayesian derivations. A 50-year-old search algorithm leads straight to the mathematical roots of neural networks.&lt;/p&gt; &lt;p&gt;All creds to Jaepil and Cognica Team!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qwyslvhr2tjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r62tlh/bb25_bayesian_bm25_v020_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r62tlh/bb25_bayesian_bm25_v020_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T07:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5pdnn</id>
    <title>rednote-hilab/dots.ocr-1.5</title>
    <updated>2026-02-15T20:39:43+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5pdnn/rednotehilabdotsocr15/"&gt; &lt;img alt="rednote-hilab/dots.ocr-1.5" src="https://external-preview.redd.it/XrUFWJLhWfPf18dDw3gY1NttJAJXB7-AhzrL3jFOW4M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b563f4bfc26c48348de6f8733f089e70363d7903" title="rednote-hilab/dots.ocr-1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.ocr-1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5pdnn/rednotehilabdotsocr15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5pdnn/rednotehilabdotsocr15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T20:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5k46x</id>
    <title>If you were starting with local LLMs today, what would you do differently</title>
    <updated>2026-02-15T17:15:44+00:00</updated>
    <author>
      <name>/u/Bubbly_Run_2349</name>
      <uri>https://old.reddit.com/user/Bubbly_Run_2349</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I am seriously considering investing a significant portion of my signing bonus into a local LLM setup as a hobby and learning project once I start my job in August.&lt;/p&gt; &lt;p&gt;I am currently in university. I have studied a lot of theory, but I feel I am missing practical, hands-on experience.&lt;/p&gt; &lt;p&gt;If you were starting from scratch today, knowing what you know now, what would you do differently?&lt;/p&gt; &lt;p&gt;Specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What hardware would you prioritize&lt;/li&gt; &lt;li&gt;What inference stack would you start with&lt;/li&gt; &lt;li&gt;What beginner mistakes should be avoided&lt;/li&gt; &lt;li&gt;What models are actually practical on consumer GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I know much of this information already exists, but it is often fragmented across many threads, benchmark posts, and user experiences.&lt;/p&gt; &lt;p&gt;I would really appreciate any lessons learned from people who have been running local setups for a while.&lt;/p&gt; &lt;p&gt;Thank you :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bubbly_Run_2349"&gt; /u/Bubbly_Run_2349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5k46x/if_you_were_starting_with_local_llms_today_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5k46x/if_you_were_starting_with_local_llms_today_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5k46x/if_you_were_starting_with_local_llms_today_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T17:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5dyna</id>
    <title>how to train a tiny model (4B) to prove hard theorems</title>
    <updated>2026-02-15T12:55:39+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"&gt; &lt;img alt="how to train a tiny model (4B) to prove hard theorems" src="https://preview.redd.it/pqtgdyl5onjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6b29a2f167dc4c50fd079038a0edf17bc75ba3f" title="how to train a tiny model (4B) to prove hard theorems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pqtgdyl5onjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T12:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5uz7d</id>
    <title>Q2 GLM 5 fixing its own typo</title>
    <updated>2026-02-16T00:33:15+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uz7d/q2_glm_5_fixing_its_own_typo/"&gt; &lt;img alt="Q2 GLM 5 fixing its own typo" src="https://preview.redd.it/cuvsstz74rjg1.png?width=140&amp;amp;height=41&amp;amp;auto=webp&amp;amp;s=d47538a1588c52366677f68c636a01a44d3a6873" title="Q2 GLM 5 fixing its own typo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found this hilarious. Never seen a model fix its own typos in realtime before (this was in openwebui, not agent session - so it couldn't just re-write).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cuvsstz74rjg1.png?width=1218&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7a31bd9849a772b7753179a1c40135c12f5fe3c"&gt;https://preview.redd.it/cuvsstz74rjg1.png?width=1218&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7a31bd9849a772b7753179a1c40135c12f5fe3c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth's GLM 5 quants are impressive - even down at TQ1 it was staying coherent, producing syntactically correct code with beautiful output.&lt;/p&gt; &lt;p&gt;Though, Q2 is working faster for me (20tps on M3 Ultra).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uz7d/q2_glm_5_fixing_its_own_typo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uz7d/q2_glm_5_fixing_its_own_typo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uz7d/q2_glm_5_fixing_its_own_typo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5lra1</id>
    <title>Bad Apple but it's GPT-2 XL Attention Maps</title>
    <updated>2026-02-15T18:19:02+00:00</updated>
    <author>
      <name>/u/TheLatentExplorer</name>
      <uri>https://old.reddit.com/user/TheLatentExplorer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lra1/bad_apple_but_its_gpt2_xl_attention_maps/"&gt; &lt;img alt="Bad Apple but it's GPT-2 XL Attention Maps" src="https://external-preview.redd.it/bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b50e9ecd47f69e52dab8879c365cdc6251af431c" title="Bad Apple but it's GPT-2 XL Attention Maps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I optimized learnable input embeddings for a frozen GPT-2 XL model so that its attention maps display the frames of the Bad Apple music video. The model never saw an image in its life, The optimizer just found the right inputs. &lt;/p&gt; &lt;p&gt;This is a silly little project but I found it interesting, here are some details about how I made that work:&lt;br /&gt; - freeze the entire model, only optimize a raw 256x1600 embedding tensor per frame&lt;br /&gt; - target a single attention head (head 0, layer 0), only compute Q and K projections&lt;br /&gt; - use MSE loss in logit space (pre-softmax) instead of on the attention weights, gives ~250x stronger gradients&lt;br /&gt; - multi-start optimization: 3 random seeds, keep the best, refine&lt;br /&gt; - post-processing: per-row z-score normalization + gaussian blur + magma colormap &lt;/p&gt; &lt;p&gt;3286 frames, ~12 minutes on an RTX 5070 Ti, 4.5 GB VRAM. &lt;/p&gt; &lt;p&gt;Blog post (full writeup with math): &lt;a href="https://brayevalerien.com/blog/bad-apple-but-its-gpt2/"&gt;https://brayevalerien.com/blog/bad-apple-but-its-gpt2/&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/brayevalerien/bad-apple-but-its-gpt2"&gt;https://github.com/brayevalerien/bad-apple-but-its-gpt2&lt;/a&gt;&lt;br /&gt; YouTube: &lt;a href="https://www.youtube.com/watch?v=UU14rQO6VzU"&gt;https://www.youtube.com/watch?v=UU14rQO6VzU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLatentExplorer"&gt; /u/TheLatentExplorer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=UU14rQO6VzU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lra1/bad_apple_but_its_gpt2_xl_attention_maps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lra1/bad_apple_but_its_gpt2_xl_attention_maps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T18:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5m4vl</id>
    <title>How to run Qwen3-Coder-Next 80b parameters model on 8Gb VRAM</title>
    <updated>2026-02-15T18:33:14+00:00</updated>
    <author>
      <name>/u/AccomplishedLeg527</name>
      <uri>https://old.reddit.com/user/AccomplishedLeg527</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running large llms on my &lt;strong&gt;8Gb&lt;/strong&gt; &lt;strong&gt;laptop 3070ti&lt;/strong&gt;. I have optimized: &lt;a href="https://github.com/nalexand/LTX-2-OPTIMIZED"&gt;&lt;strong&gt;LTX-2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/Wan2.2"&gt;&lt;strong&gt;Wan2.2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/HeartMula-OPTIMIZED-8GB"&gt;&lt;strong&gt;HeartMula&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/ACE-Step-1.5-OPTIMIZED"&gt;&lt;strong&gt;ACE-STEP 1.5&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And now i abble to run 80b parameters model &lt;strong&gt;Qwen3-Coder-Next !!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instruction here:&lt;/strong&gt; &lt;a href="https://github.com/nalexand/Qwen3-Coder-OPTIMIZED"&gt;https://github.com/nalexand/Qwen3-Coder-OPTIMIZED&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is FP8 quant 80Gb in size, it is impossible to fit it on 8Gb VRAM + 32Gb RAM.&lt;/p&gt; &lt;p&gt;So first i tried offloading to disk with device=&amp;quot;auto&amp;quot; using accelerate and i got &lt;strong&gt;1 token per 255 second&lt;/strong&gt; :(.&lt;/p&gt; &lt;p&gt;Than i found that most of large tensors is mlp experts and all other fit in 4.6Gb VRAM so i build custom lazy loading for experts with 2 layers caching VRAM + pinned RAM and got up to 85% cache hit rate and speed up to 1.2t/s it`s &lt;strong&gt;300x speedup.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I wonder what speed will be on 4090 or 5090 desktop..&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;self.max_gpu_cache = 18 # TODO: calculate based on free ram and context window size self.max_ram_cache = 100 # TODO: calculate based on available pinable memory or use unpinned (slow) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tune this two parameters for your RAM/VRAM (each 18 it is about 3GB). For 5090 max_gpu_cache = 120 and it is &amp;gt;85% cache hit rate. Who can check speed?&lt;/p&gt; &lt;p&gt;Best for loading speed: PCE 5.0 Raid 0 up to 30Gb/s NVME SSD.&lt;/p&gt; &lt;p&gt;Available pinable ram (usualy 1/2 RAM) with DMA - much faster than RAM.&lt;/p&gt; &lt;p&gt;Hope 5090 will give &amp;gt; 20 t/s..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedLeg527"&gt; /u/AccomplishedLeg527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T18:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5vdxc</id>
    <title>That's why I go local.The enshittification is at full steam</title>
    <updated>2026-02-16T00:51:55+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"&gt; &lt;img alt="That's why I go local.The enshittification is at full steam" src="https://preview.redd.it/94yjg9288rjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a9ce11a5649641939d3beafb60ef54f2472733c" title="That's why I go local.The enshittification is at full steam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just received an email from chatGPT. Ads are beginning to show up. Well, we are cooked. Not we, we, we. But we are cooked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94yjg9288rjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5qfb8</id>
    <title>inclusionAI/Ling-2.5-1T ¬∑ Hugging Face</title>
    <updated>2026-02-15T21:20:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt; &lt;img alt="inclusionAI/Ling-2.5-1T ¬∑ Hugging Face" src="https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c049d1c20ccf2dbac44fc910e04d8dc862b0d7b1" title="inclusionAI/Ling-2.5-1T ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;another 1T model :)&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;inclusionAI&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Ling-2.5-1T, Inclusive Intelligence, Instant Impact.&lt;/p&gt; &lt;p&gt;Today, we launch Ling-2.5-1T and make it open source.&lt;/p&gt; &lt;p&gt;Thinking models raise the ceiling of intelligence, while instant models expand its reach by balancing efficiency and performance‚Äîmaking AGI not only more powerful, but also more accessible. As the latest flagship instant model in the Ling family, Ling-2.5-1T delivers comprehensive upgrades across model architecture, token efficiency, and preference alignment, designed to bring universally accessible AI to a new level of quality.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ling-2.5-1T features 1T total parameters (with 63B active parameters). Its pre-training corpus has expanded from 20T to 29T tokens compared to the previous generation. Leveraging an efficient hybrid linear attention architecture and refined data strategy, the model delivers exceptionally high throughput while processing context lengths of up to 1M tokens.&lt;/li&gt; &lt;li&gt;By introducing a composite reward mechanism combining &amp;quot;Correctness&amp;quot; and &amp;quot;Process Redundancy&amp;quot;, Ling-2.5-1T further pushes the frontier of efficiency-performance balance in instant models. At comparable token efficiency levels, Ling-2.5-1T‚Äôs reasoning capabilities significantly outperform its predecessor, approaching the level of frontier &amp;quot;thinking models&amp;quot; that typically consume ~4x the output tokens.&lt;/li&gt; &lt;li&gt;Through refined alignment strategies‚Äîsuch as bidirectional RL feedback and Agent-based instruction constraint verification‚ÄîLing-2.5-1T achieves substantial improvements over the previous generation in preference alignment tasks, including creative writing and instruction following.&lt;/li&gt; &lt;li&gt;Trained with Agentic RL in large-scale high-fidelity interactive environments, Ling-2.5-1T is compatible with mainstream agent platforms such as Claude Code, OpenCode, and OpenClaw. It achieves leading open-source performance on the general tool-calling benchmark, BFCL-V4.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-2.5-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T21:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5h1gj</id>
    <title>You can run MiniMax-2.5 locally</title>
    <updated>2026-02-15T15:14:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt; &lt;img alt="You can run MiniMax-2.5 locally" src="https://preview.redd.it/hd369oaucojg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf9267391b3836cb000418670d350915c3a8405" title="You can run MiniMax-2.5 locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax-2.5 is a new open LLM achieving SOTA in coding, agentic tool use and search and office work.&lt;/p&gt; &lt;p&gt;The 230B parameters (10B active) model has a &lt;strong&gt;200K context&lt;/strong&gt; window and unquantized bf16 requires &lt;strong&gt;457GB&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Unsloth Dynamic &lt;strong&gt;3-bit&lt;/strong&gt; GGUF reduces size to &lt;strong&gt;101GB&lt;/strong&gt; &lt;strong&gt;(-62%).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide -&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/minimax-2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/minimax-2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUF Models -&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF"&gt;&lt;strong&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top LLM, RAG and AI Agents updates of this week&lt;/strong&gt; - &lt;a href="https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a"&gt;https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd369oaucojg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5uhfu</id>
    <title>Deflation: Cost to train A.I. models drops 40% per year - Karpathy</title>
    <updated>2026-02-16T00:11:13+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/481"&gt;https://github.com/karpathy/nanochat/discussions/481&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quote: ..., each year the cost to train GPT-2 is falling to approximately 40% of the previous year. (I think this is an underestimate and that further improvements are still quite possible). The gains come from everywhere: better hardware (H100 vs TPU v3), better software (Flash Attention 3, torch.compile), better algorithms (Muon optimizer, architectural improvements), and better data (FineWeb-edu).&lt;/p&gt; &lt;h1&gt;What Worked&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Flash Attention 3&lt;/strong&gt; ‚Äî ~9% tok/sec improvement. Native tensor layout, single API for training and inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sliding window attention&lt;/strong&gt; ‚Äî &lt;code&gt;SSSL&lt;/code&gt; pattern. Compute savings without quality loss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Muon optimizer overhaul&lt;/strong&gt; ‚Äî Polar Express, NorMuon variance reduction, cautious weight decay with linear schedule to zero. The cautious WD was a clear win. I tried to delete Muon and couldn't.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per-layer residual scalars&lt;/strong&gt; ‚Äî &lt;code&gt;x = Œª_resid * x + Œª_x0 * x0&lt;/code&gt;. Consistent improvement across all model sizes (0.003-0.01 bpb).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value Embeddings at alternating layers&lt;/strong&gt; ‚Äî Models love the value embeddings capacity. Any attempt to reduce it (low-rank, sharing, projections) hurt. We tried U-shaped placement, every layer, alternating‚Äîalternating won.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BOS-aligned dataloader&lt;/strong&gt; ‚Äî Every row starts with BOS. Made midtraining unnecessary (deleted it). BestFit-Crop packing reduces waste vs naive cropping.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hyperparameter sweep at scale&lt;/strong&gt; ‚Äî 320 experiments to find that &lt;code&gt;x0_beta1=0.96&lt;/code&gt; is optimal at d20. Key lesson: small-scale tuning doesn't transfer. Validate at target scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scaling law discovery&lt;/strong&gt; ‚Äî We empirically measured the optimal tokens:params ratio to be ~10. It's important to do the actual experiment on your own network.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What Didn't Work&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-token prediction (MTP)&lt;/strong&gt; ‚Äî +13GB memory, no improvement&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Varlen attention&lt;/strong&gt; ‚Äî BOS-aligned dataloader already handles this to some extent. Attending across BOS document boundaries does not seem to make things much worse.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FP8 for lm_head&lt;/strong&gt; ‚Äî Works, but +2GB memory (!), only 1% speedup, todo to look into more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Half-truncated RoPE&lt;/strong&gt; ‚Äî No improvement&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asymmetric softcap&lt;/strong&gt; ‚Äî Slightly worse&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skip connections / backout&lt;/strong&gt; ‚Äî No improvement, +2GB memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smear gate, attention gates&lt;/strong&gt; ‚Äî Negligible improvement, not worth complexity&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size schedule&lt;/strong&gt; ‚Äî Deemed a little too complex&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigram embeddings (Engram-lite)&lt;/strong&gt; ‚Äî Works, but not by too much, and it bloats complexity and parameter count by a lot, so it was skipped in the end.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hyperball/MuonH&lt;/strong&gt; ‚Äî Intriguing idea, didn't work out of the box&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5v1jb</id>
    <title>Anyone actually using Openclaw?</title>
    <updated>2026-02-16T00:36:08+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am highly suspicious that openclaw's virality is organic. I don't know of anyone (online or IRL) that is actually using it and I am deep in the AI ecosystem (both online and IRL). If this sort of thing is up anyone's alley, its the members of localllama - so are you using it? &lt;/p&gt; &lt;p&gt;With the announcement that OpenAI bought OpenClaw, conspiracy theory is that it was manufactured social media marketing (on twitter) to hype it up before acquisition. Theres no way this graph is real: &lt;a href="https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left"&gt;https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:36:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60ety</id>
    <title>Qwen 3.5 will be released today</title>
    <updated>2026-02-16T04:54:20+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt; &lt;img alt="Qwen 3.5 will be released today" src="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ff46b508a3b564db9ac8039bb61d1b0f08588ef3" title="Qwen 3.5 will be released today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sources reveal that Alibaba will open-source its next-generation large model, Qwen3.5, tonight on Lunar New Year's Eve. The model reportedly features a comprehensive innovation in its architecture.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06"&gt;https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Sino_Market/status/2023218866370068561?s=20"&gt;https://x.com/Sino_Market/status/2023218866370068561?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T04:54:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax ‚Äî Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax ‚Äî Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; ‚Äî Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
