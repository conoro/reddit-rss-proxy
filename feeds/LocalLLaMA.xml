<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-06T16:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n9urgv</id>
    <title>double the context window of any AI agent</title>
    <updated>2025-09-06T08:57:45+00:00</updated>
    <author>
      <name>/u/Lonely-Marzipan-9473</name>
      <uri>https://old.reddit.com/user/Lonely-Marzipan-9473</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i put together a package that helps deal with the context window problem in llms. instead of just truncating old messages, it uses embeddings to semantically deduplicate, rerank, and trim context so you can fit more useful info into the model‚Äôs token budget.&lt;/p&gt; &lt;p&gt;basic usage looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { optimizePrompt } from &amp;quot;double-context&amp;quot;; const result = await optimizePrompt({ userPrompt: &amp;quot;summarize recent apple earnings&amp;quot;, context: [ &amp;quot;apple quarterly earnings rose 15% year-over-year in q3 2024&amp;quot;, &amp;quot;apple revenue increased by 15% year-over-year&amp;quot;, // deduped &amp;quot;the eiffel tower is in paris&amp;quot;, // deprioritized &amp;quot;apple's iphone sales remained strong&amp;quot;, &amp;quot;apple ceo tim cook expressed optimism about ai integration&amp;quot; ], maxTokens: 200, openaiApiKey: process.env.OPENAI_API_KEY, dedupe: true, strategy: &amp;quot;relevance&amp;quot; }); console.log(result.finalPrompt); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;there‚Äôs also an optimizer for whole chat histories, useful if you‚Äôre building bots that otherwise waste tokens repeating themselves:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { optimizeChatHistory } from &amp;quot;double-context&amp;quot;; const optimized = await optimizeChatHistory({ messages: conversation, maxTokens: 1000, openaiApiKey: process.env.OPENAI_API_KEY, dedupe: true, strategy: &amp;quot;hybrid&amp;quot; }); console.log(`optimized from ${conversation.length} to ${optimized.optimizedMessages.length} messages`); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;repo is here if you want to check it out or contribute: &lt;a href="https://github.com/Mikethebot44/LLM-context-expansion"&gt;https://github.com/Mikethebot44/LLM-context-expansion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to install:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npm install double-context &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then just wrap your prompts or conversation history with it.&lt;/p&gt; &lt;p&gt;hope you enjoy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonely-Marzipan-9473"&gt; /u/Lonely-Marzipan-9473 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9urgv/double_the_context_window_of_any_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9urgv/double_the_context_window_of_any_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9urgv/double_the_context_window_of_any_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T08:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9tyle</id>
    <title>[vllm] Hints to run Qwen3-235B MoE on 8x AMD mixed cards!</title>
    <updated>2025-09-06T08:04:17+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9tyle/vllm_hints_to_run_qwen3235b_moe_on_8x_amd_mixed/"&gt; &lt;img alt="[vllm] Hints to run Qwen3-235B MoE on 8x AMD mixed cards!" src="https://external-preview.redd.it/gQle-ct9ZsU6Ezd6HbP8H2dG0W33ZIVBMdf_DMes6RQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be30c0c202a551d118c68a43dee226930ae1008b" title="[vllm] Hints to run Qwen3-235B MoE on 8x AMD mixed cards!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today i found formula to launch gptq-4bit version of MoE model on 2xR9700 + 6x7900XTX.&lt;/p&gt; &lt;p&gt;it's work's on very stable ~13-14 token/s output, and ~ 150-300 token input.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;GPU KV cache size: 633,264 tokens Maximum concurrency for 40,960 tokens per request: 15.46x GPU KV cache size: 275,840 tokens Maximum concurrency for 40,960 tokens per request: 6.73x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;it works for docker image: &lt;strong&gt;rocm/vllm-dev:nightly_main_20250905&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- HIP_VISIBLE_DEVICES=0,6,1,2,3,4,5,7 # first 2 gpu R9700, other is 7900xtx - VLLM_USE_V1=1 - VLLM_CUSTOM_OPS=all - PYTORCH_HIP_ALLOC_CONF=expandable_segments:True - SAFETENSORS_FAST_GPU=1 - PYTORCH_TUNABLEOP_ENABLED command: | sh -c ' vllm serve /app/models/models/vllm/Qwen3-235B-A22B-GPTQ-Int4 \ --served-model-name Qwen3-235B-A22B-GPTQ-Int4 \ --gpu-memory-utilization 0.97 \ --max-model-len 40960 \ --enable-auto-tool-choice \ --disable-log-requests \ --enable-chunked-prefill \ --max-num-batched-tokens 4096 \ --tool-call-parser qwen3_coder \ --max-num-seqs 8 \ --enable-expert-parallel \ --tensor-parallel-size 4 \ -pp 2 ' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;The case to discuss:&lt;/strong&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;In case of -tp 4 and -pp 2, loading very long time and does not work.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;when we use -pp 4 and -tp 2, it show &lt;em&gt;Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 5/5 [00:06&amp;lt;00:00, 1.22s/it]&lt;/em&gt; at finish and model launched, in case with -tp 4, Capturing graphs takes 2-15 minutes per one iteration&lt;/p&gt; &lt;p&gt;I think the problem in gpu_memory_mapping, but don't know how to resolve it correctly, to use amount of VRAM at all cards.&lt;/p&gt; &lt;p&gt;When model loading in. tp 4 or tp 8, they spend a lot of resources to load correctly like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dr4ut0vi1inf1.png?width=2328&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9943d8c5ec361bf34b49549974f058acf87079f"&gt;only uses group of 4 cards &lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;impossible to find ready quantized model &lt;strong&gt;Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4&lt;/strong&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Right now on the hugging face we have only QuantTrio/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix which not work with our GPU&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Maybe someone here can quantize &lt;strong&gt;Qwen3-235B-A22B-Instruct&lt;/strong&gt; to &lt;strong&gt;GPTQ-int4?&lt;/strong&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;we need the same quantiz√•tion config as &lt;strong&gt;original&lt;/strong&gt; GPTQ-int4.&lt;/p&gt; &lt;p&gt;AWQ - not work&lt;/p&gt; &lt;p&gt;compressed-tensors w8a8 - not work&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Load&lt;/th&gt; &lt;th align="left"&gt;Error&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-GPTQ-Int4"&gt;Qwen3-235B-A22B-GPTQ-Int4&lt;/a&gt; &lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-GPTQ-Int4"&gt;Qwen3-30B-A3B-GPTQ-Int4&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"&gt;Qwen3-Coder-30B-A3B-Instruct-FP8&lt;/a&gt; &lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;td align="left"&gt;does not match the quantization method specified in the `quantization` argument (fp8_e5m2)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; &lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix"&gt;Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix&lt;/a&gt; &lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;What you want to try?&lt;/strong&gt; Maybe someone here already launched this model with other config?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9tyle/vllm_hints_to_run_qwen3235b_moe_on_8x_amd_mixed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9tyle/vllm_hints_to_run_qwen3235b_moe_on_8x_amd_mixed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9tyle/vllm_hints_to_run_qwen3235b_moe_on_8x_amd_mixed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T08:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9te37</id>
    <title>Minisforum MS-S1 MAX... Strix Halo with PCIe x16 slot?!</title>
    <updated>2025-09-06T07:27:23+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9te37/minisforum_mss1_max_strix_halo_with_pcie_x16_slot/"&gt; &lt;img alt="Minisforum MS-S1 MAX... Strix Halo with PCIe x16 slot?!" src="https://external-preview.redd.it/5Y8bes7UxWs-nZZq-BL78UDnlYWcWqATtxnqi_ST2Rk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3074af57c160ebd5fa8d86161f0d34851432325" title="Minisforum MS-S1 MAX... Strix Halo with PCIe x16 slot?!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And NOW we're talking. Wonder what happened in between AMD saying &amp;quot;nope, you only get 16 lanes total&amp;quot; to &amp;quot;oh actually...&amp;quot;&lt;/p&gt; &lt;p&gt;No more 2x 4x nvme?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/minisforum-ms-s1-max-to-feature-ryzen-ai-max-395-up-to-160w-and-usb4-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9te37/minisforum_mss1_max_strix_halo_with_pcie_x16_slot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9te37/minisforum_mss1_max_strix_halo_with_pcie_x16_slot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T07:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n98vdp</id>
    <title>Qwen 3 Max Official Benchmarks (possibly open sourcing later..?)</title>
    <updated>2025-09-05T15:49:10+00:00</updated>
    <author>
      <name>/u/Trevor050</name>
      <uri>https://old.reddit.com/user/Trevor050</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98vdp/qwen_3_max_official_benchmarks_possibly_open/"&gt; &lt;img alt="Qwen 3 Max Official Benchmarks (possibly open sourcing later..?)" src="https://preview.redd.it/eeekht6sadnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff85bfdf6253ad3ba0a381c5514ad302898defd3" title="Qwen 3 Max Official Benchmarks (possibly open sourcing later..?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trevor050"&gt; /u/Trevor050 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eeekht6sadnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98vdp/qwen_3_max_official_benchmarks_possibly_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n98vdp/qwen_3_max_official_benchmarks_possibly_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T15:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9xu5z</id>
    <title>What does your LLM set up look like right now?</title>
    <updated>2025-09-06T12:04:24+00:00</updated>
    <author>
      <name>/u/notdl</name>
      <uri>https://old.reddit.com/user/notdl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's so many options now and I'm getting lost trying to pick one (for coding specificlly).&lt;/p&gt; &lt;p&gt;What's your go-to setup? Looking for something that just works without too much configuration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdl"&gt; /u/notdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9xu5z/what_does_your_llm_set_up_look_like_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9xu5z/what_does_your_llm_set_up_look_like_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9xu5z/what_does_your_llm_set_up_look_like_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T12:04:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3dit</id>
    <title>Custom Dataset for Fine Tuning</title>
    <updated>2025-09-06T16:06:59+00:00</updated>
    <author>
      <name>/u/Old-Raspberry-3266</name>
      <uri>https://old.reddit.com/user/Old-Raspberry-3266</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can any one drop a tip or any suggestions/ recommendations for how to create or own dataset to fine tune a LLM. How many minimum rows should we take. Should we use use prompt, completion method or role, content,system, user, assistant method.&lt;/p&gt; &lt;p&gt;Please drop your thoughts on thisüôèüèªüôÉ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-Raspberry-3266"&gt; /u/Old-Raspberry-3266 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3dit/custom_dataset_for_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3dit/custom_dataset_for_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3dit/custom_dataset_for_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3icm</id>
    <title>today grok code fast 1 brake the 1 trillion mark in the open router history</title>
    <updated>2025-09-06T16:12:30+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3icm/today_grok_code_fast_1_brake_the_1_trillion_mark/"&gt; &lt;img alt="today grok code fast 1 brake the 1 trillion mark in the open router history" src="https://preview.redd.it/l35vq1trjknf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ad02854dcca1e4396c4063e031e5a95da35a2c9" title="today grok code fast 1 brake the 1 trillion mark in the open router history" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l35vq1trjknf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3icm/today_grok_code_fast_1_brake_the_1_trillion_mark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3icm/today_grok_code_fast_1_brake_the_1_trillion_mark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9zuil</id>
    <title>Tested sonoma-sky-alpha on Fiction.liveBench, fantastic close to SOTA scores, currently free</title>
    <updated>2025-09-06T13:40:47+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zuil/tested_sonomaskyalpha_on_fictionlivebench/"&gt; &lt;img alt="Tested sonoma-sky-alpha on Fiction.liveBench, fantastic close to SOTA scores, currently free" src="https://preview.redd.it/lbbwz19ssjnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c7f0bc4e724dc56e61b41a81da5f3d8d98fc7b0" title="Tested sonoma-sky-alpha on Fiction.liveBench, fantastic close to SOTA scores, currently free" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbbwz19ssjnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zuil/tested_sonomaskyalpha_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zuil/tested_sonomaskyalpha_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T13:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3xkd</id>
    <title>Llama-3.3-Nemotron-Super-49B-v1.5 is very good model to summarized long text into formatted markdown (Nvidia also provided free unlimited API call with rate limit)</title>
    <updated>2025-09-06T16:29:19+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project to convert medical lesson data from websites into markdown format for a RAG application. Tested several popular models including Qwen3 235B, Gemma 3 27B, and GPT-oss-120 they all performed well technically, but as someone with a medical background, the output style just didn't click with me (totally subjective, I know).&lt;/p&gt; &lt;p&gt;So I decided to experiment with some models on NVIDIA's API platform and stumbled upon &lt;strong&gt;Llama-3.3-Nemotron-Super-49B-v1.5&lt;/strong&gt; This thing is surprisingly solid for my use case. I'd tried it before in an agent setup where it didn't perform great on evals, so I had to stick with the bigger models. But for this specific summarization task, it's been excellent.&lt;/p&gt; &lt;p&gt;The output is well-written, requires minimal proofreading, and the markdown formatting is clean right out of the box. Plus it's free through NVIDIA's API (40 requests/minute limit), which is perfect for my workflow since I manually review everything anyway.&lt;/p&gt; &lt;p&gt;Definitely worth trying if you're doing similar work with medical or technical content, write a good prompt still the key though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1na0yp7</id>
    <title>Strix Halo on Ubuntu looks great - Netstatz</title>
    <updated>2025-09-06T14:29:10+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0yp7/strix_halo_on_ubuntu_looks_great_netstatz/"&gt; &lt;img alt="Strix Halo on Ubuntu looks great - Netstatz" src="https://external-preview.redd.it/NJZpfJ99mI8JFoVvYOBKMFZAOXMI0D90oeAZs96TxjQ.jpeg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bfa184dd47aead2b361177ffeb7754292954b27" title="Strix Halo on Ubuntu looks great - Netstatz" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not the author, just sharing an article written by a GitHub contributor. I appreciate that it‚Äôs an end to end tutorial with code that includes all the problems/challenges!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://netstatz.com/strix_halo_lemonade/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0yp7/strix_halo_on_ubuntu_looks_great_netstatz/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na0yp7/strix_halo_on_ubuntu_looks_great_netstatz/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T14:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9omqj</id>
    <title>Kimi K2-0905 is a powerhouse VS claude-sonnet-4 @20250514.</title>
    <updated>2025-09-06T02:55:27+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been heavily builidng with claude-sonnet-4@20250514, but threw $5 into OpenRouter and gave K2-0905 and WOW. &lt;/p&gt; &lt;p&gt;Not sure if its a ‚Äúbetter‚Äù model, but seems to chew through tasks in a ‚Äúbetter‚Äù way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9hduk</id>
    <title>VibeVoice came back. Though many may not like it.</title>
    <updated>2025-09-05T21:19:36+00:00</updated>
    <author>
      <name>/u/Fresh_Sun_1017</name>
      <uri>https://old.reddit.com/user/Fresh_Sun_1017</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/microsoft/VibeVoice"&gt;VibeVoice&lt;/a&gt; has returned(&lt;em&gt;not&lt;/em&gt; VibeVoice-large); however, Microsoft plans to implement censorship due to people's &amp;quot;misuse of research&amp;quot;. Here's the quote from the repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. &lt;strong&gt;After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;What types of censorship will be implemented? And couldn‚Äôt people just use or share older, unrestricted versions they've already downloaded? That's going to be interesting...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; The VibeVoice-Large model is still available as of now, &lt;a href="https://www.modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;VibeVoice-Large ¬∑ Models&lt;/a&gt; on Modelscope. It may be deleted soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fresh_Sun_1017"&gt; /u/Fresh_Sun_1017 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T21:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n975er</id>
    <title>Qwen 3 max</title>
    <updated>2025-09-05T14:42:28+00:00</updated>
    <author>
      <name>/u/LeatherRub7248</name>
      <uri>https://old.reddit.com/user/LeatherRub7248</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt; &lt;img alt="Qwen 3 max" src="https://external-preview.redd.it/9f9JRaQTq2uR5GC3copbxq5McLsZhYSzNHSbhHCgcmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f89b1589b444db5310feea66a3e0335c0591fac" title="Qwen 3 max" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's out&lt;/p&gt; &lt;p&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-max"&gt;https://openrouter.ai/qwen/qwen3-max&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt; (qwen 3 max preview)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nb9wzcl9bdnf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d22747e9fb863b0412d20782dd88e055fbb87a9f"&gt;https://preview.redd.it/nb9wzcl9bdnf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d22747e9fb863b0412d20782dd88e055fbb87a9f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeatherRub7248"&gt; /u/LeatherRub7248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T14:42:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9zrvc</id>
    <title>How do you make 3+ GPUs stable?!</title>
    <updated>2025-09-06T13:37:33+00:00</updated>
    <author>
      <name>/u/anothy1</name>
      <uri>https://old.reddit.com/user/anothy1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got my third 3090 and the setup from 2 to 3 GPUs was a PITA as I had to now use a mining frame with these pcie x16 risers (&lt;a href="https://www.amazon.ca/dp/B0C4171HKX"&gt;https://www.amazon.ca/dp/B0C4171HKX&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Problem is I've been dealing with constant issues of crashes and instability. For example I've been trying to preprocess datasets over night just to wake up to these messages and my system hanging:&lt;/p&gt; &lt;p&gt;&lt;code&gt;GPU 00000000:01:00.0: GPU Unavailable error occurred&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GPU 00000000:05:00.0: GPU Recovery action event occurred&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GPU 00000000:01:00.0: Detected Critical Xid Error&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Journalctl also shows a lot of these&lt;/p&gt; &lt;p&gt;&lt;code&gt;Sep 06 11:43:45 ml-lab1 kernel: pcieport 0000:00:01.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Transmitter ID)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Sep 06 11:43:45 ml-lab1 kernel: pcieport 0000:00:01.0: device [8086:a70d] error status/mask=00001000/00002000&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Sep 06 11:43:45 ml-lab1 kernel: pcieport 0000:00:01.0: [12] Timeout&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Judging from this it's most likely the risers. I do hope there's some kind of magic setting in the BIOS I'm missing that someone could point out (so far the only thing I set was above 4g decoding and force pcie gen 3) but if not I would greatly appreciate recommendations for better risers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anothy1"&gt; /u/anothy1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zrvc/how_do_you_make_3_gpus_stable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zrvc/how_do_you_make_3_gpus_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zrvc/how_do_you_make_3_gpus_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T13:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3d4s</id>
    <title>Fixing up this desktop</title>
    <updated>2025-09-06T16:06:32+00:00</updated>
    <author>
      <name>/u/animal_hoarder</name>
      <uri>https://old.reddit.com/user/animal_hoarder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3d4s/fixing_up_this_desktop/"&gt; &lt;img alt="Fixing up this desktop" src="https://preview.redd.it/k0g6d3dtiknf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88b3fc9bad808e71857293e4de0ebb582da48826" title="Fixing up this desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This entry level prebuilt desktop has been sitting unused for a year or two. I‚Äôve been getting into running some models locally, mostly for some projects I‚Äôm working on, and want to give this thing a purpose again. The plan is to build an actual rig with better hardware at some point, but I‚Äôm willing to put $500 into this now to make it more capable. There are the specs currently:&lt;/p&gt; &lt;p&gt;CPU: Intel Core i5 10400F GPU: NVIDIA GeForce RTX 3060 Ti 12gb Memory: Team T-FORCE Vulcan Z (2X8GB at 3200 MHz) Motherboard: B560 Storage: Western Digital 1TB Blue SN550 NVMe PSU: Seasonic 550W Bronze Chassis: H510&lt;/p&gt; &lt;p&gt;I was looking at adding another 12gb 3060, and upgrading the RAM to at least 32gb. I think I‚Äôll probably also have to swap out the PSU with a 750W to handle the extra gpu. &lt;/p&gt; &lt;p&gt;What do you think? Is the dual 3060 worth doing? It seems like the most cost effective way to get this system up to 24gb VRAM. Or should I just save up for a single 24gb 3090? I wouldn‚Äôt need a new PSU if I went that route&lt;/p&gt; &lt;p&gt;I appreciate any input you have, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/animal_hoarder"&gt; /u/animal_hoarder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0g6d3dtiknf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3d4s/fixing_up_this_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3d4s/fixing_up_this_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3u8w</id>
    <title>Built QWEN3-0.6B mini inference engine in CUDA from scratch</title>
    <updated>2025-09-06T16:25:42+00:00</updated>
    <author>
      <name>/u/yassa9</name>
      <uri>https://old.reddit.com/user/yassa9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"&gt; &lt;img alt="Built QWEN3-0.6B mini inference engine in CUDA from scratch" src="https://external-preview.redd.it/ZXpiNW9wenhra25mMbxpIYt75C5r2fT6cxhEXwgg3zD3iMqrP6b9J5eZI0o1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8266ac29eb4dbc6dde7ee5d798cb3890af08d38" title="Built QWEN3-0.6B mini inference engine in CUDA from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm into CUDA and GPGPU programming much, didn't get into LLMs or NLP at all, so tried build that side project as as a hands-on way to learn about LLMs while practicing my CUDA programming.&lt;/p&gt; &lt;p&gt;chose that cute tiny model of qwen3-600m &lt;/p&gt; &lt;p&gt;Static configured, with suckless philosophy in code as much as possible, no deps to build beyond cuBLAS, CUB, std IO libs&lt;/p&gt; &lt;p&gt;I know that im missing smth but in benchmarking with greedy sampling (temp=0) on my RTX 3050, I get 3x speed of hf with flash-attn inference and extremely comparable speed with llama.cpp&lt;/p&gt; &lt;p&gt;My guess is the slight edge over llama.cpp comes from being hyper-specialized for just one model, allowing for more compile-time optimizations with no runtime branching.&lt;/p&gt; &lt;p&gt;feel free to check github if you want: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yassa9/qwen600"&gt;https://github.com/yassa9/qwen600&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yassa9"&gt; /u/yassa9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xh5qjozxkknf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1na2auz</id>
    <title>Has anyone here had any experience ordering from Tenstorrent or dealing with their customer service?</title>
    <updated>2025-09-06T15:24:01+00:00</updated>
    <author>
      <name>/u/elephantgif</name>
      <uri>https://old.reddit.com/user/elephantgif</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a fan of Jim Keller, and love the mission behind his product, but my Blackhole cards have been stuck in customs for over a week, pending paperwork that was never sent to the carrier. Despite reaching out to them multiple times, I have yet to get any response. After digging, I found their phone number only to call and discover the voice mail hasn't even been set up. This whole experience has been disappointing and has led me to question even ordering these cards to begin with. I have never experience such a total lack of customer service, and am very frustrated at this point.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elephantgif"&gt; /u/elephantgif &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2auz/has_anyone_here_had_any_experience_ordering_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2auz/has_anyone_here_had_any_experience_ordering_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na2auz/has_anyone_here_had_any_experience_ordering_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T15:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9ong4</id>
    <title>Kimi K2 0905 is a beast at coding</title>
    <updated>2025-09-06T02:56:29+00:00</updated>
    <author>
      <name>/u/adumdumonreddit</name>
      <uri>https://old.reddit.com/user/adumdumonreddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been working on this static website, just a side project where I can do some blogging or some fun javascript experiments, but I've been making this new component, basically implementing custom scrolling and pagination behaviours from scratch.&lt;/p&gt; &lt;p&gt;Anyways, I was facing a bunch of tough bugs, in complete deadlock, even tried asking Deepseek/Gemini/even went for one response from Opus, no luck. Then, decided to try the new Kimi, and bam. One try, instantly solved the issue, and did it with some tastefully commented (think somewhere between Gemini and Qwen levels of comment-ness) and good-practice code.&lt;/p&gt; &lt;p&gt;I was impressed, so I decided to just toss in my entire CSS/HTML skeleton as well as a fuck it, and when it was done, the result was so much prettier than the one I had originally. Damn, I thought, so I decided to toss it a few more problems: implement dark mode handling for the entire skeleton using only CSS and a js button, and implement another style hotswapping feature I had been thinking of.&lt;/p&gt; &lt;p&gt;Five minutes, and they both were done flawlessly.&lt;/p&gt; &lt;p&gt;I'm no javascript wiz, so I imagine all of that would probably have taken me around another two or three hours. With Kimi, I did it in like 10 minutes. What's more is that it cracked bugs that even the previous SOTA models, my go-tos, couldn't do. The consistency is also impressive: all of it was in one try, maybe two if I wanted to clarify my requirements, and all of it was well formatted, had a nice level of comments (I don't know how to explain this one, the comments were just 'good' in a way Gemini comments aren't, for example)&lt;/p&gt; &lt;p&gt;Wow. I'm impressed.&lt;/p&gt; &lt;p&gt;(Sorry, no images; the website is publicly accessible and linked to my real name, so I'd prefer not to link it to this account in any way.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adumdumonreddit"&gt; /u/adumdumonreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9ubmn</id>
    <title>MINISFORUM MS-S1 Max AI PC features AMD Strix Halo, 80 Gbps USB, 10 Gb LAN, and PCie x16 - Liliputing</title>
    <updated>2025-09-06T08:28:13+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ubmn/minisforum_mss1_max_ai_pc_features_amd_strix_halo/"&gt; &lt;img alt="MINISFORUM MS-S1 Max AI PC features AMD Strix Halo, 80 Gbps USB, 10 Gb LAN, and PCie x16 - Liliputing" src="https://external-preview.redd.it/QuRrltSklV9MMNc9P3Jo_YeEcOYyvyeX46KjOI0goqs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4e531500df8dc2e276ea41601eff7d38db6a0af" title="MINISFORUM MS-S1 Max AI PC features AMD Strix Halo, 80 Gbps USB, 10 Gb LAN, and PCie x16 - Liliputing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AMD Ryzen AI Max+ 395 processor, 128GB of LPDDR5x-8000 quad-channel memory with 256GB/s bandwidth, and the ability to run large large language models with over 100 billion parameters locally. And, it has pretty good connectivity options: 80 Gbps USB, 10 Gb LAN, and PCie x16. &lt;/p&gt; &lt;p&gt;For comparison, the Framework Desktop has PCIe x4 only.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://liliputing.com/minisforum-ms-s1-max-ai-pc-features-amd-strix-halo-80-gbps-usb-10-gb-lan-and-pcie-x16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ubmn/minisforum_mss1_max_ai_pc_features_amd_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ubmn/minisforum_mss1_max_ai_pc_features_amd_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T08:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9o4em</id>
    <title>ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation</title>
    <updated>2025-09-06T02:29:21+00:00</updated>
    <author>
      <name>/u/susmitds</name>
      <uri>https://old.reddit.com/user/susmitds</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"&gt; &lt;img alt="ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation" src="https://b.thumbs.redditmedia.com/JhYB2Z_qsB-pJqq4qeo2qmH8su0aVxk10f3FRgO-6QE.jpg" title="ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my workstation motherboard stopped working and needed to be sent for replacement in warranty. Leaving my research work and LLM workflow screwed.&lt;/p&gt; &lt;p&gt;Off a random idea stuck one of my RTX 6000 Blackwell into a EGPU enclosure (Aoostar AG02) and tried it on my travel device, the ROG Ally X and it kinda blew my mind on how good this makeshift temporary setup was working. Never thought I would using my Ally for hosting 235B parameter LLM models, yet with the GPU, I was getting very good performance at 1100+ tokens/sec prefill, 25+ tokens/sec decode on Qwen3-235B-A22B-Instruct-2507 with 180K context using a custom quant I made in ik-llama.cpp (attention projections, embeddings, lm_head at q8_0, expert up/gate at iq2_kt, down at iq3_kt, total 75 GB size). Also tested GLM 4.5 Air with unsloth's Q4_K_XL, could easily run with full 128k context. I am perplexed how good the models are all running even at PCIE 4 x 4 on a eGPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/susmitds"&gt; /u/susmitds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n9o4em"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:29:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1na2l5b</id>
    <title>Kimi K2 0905 Official Pricing (generation, tool)</title>
    <updated>2025-09-06T15:35:30+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2l5b/kimi_k2_0905_official_pricing_generation_tool/"&gt; &lt;img alt="Kimi K2 0905 Official Pricing (generation, tool)" src="https://b.thumbs.redditmedia.com/5S8zZlvd7mg8FDiVJJaX0ZhoACaHNn7i2haHT6-515Y.jpg" title="Kimi K2 0905 Official Pricing (generation, tool)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quite cheap for a model this big! Consider using the official API instead of Openrouter, it directly supports the model builders (PS: I looked for &amp;quot;non-local&amp;quot; flair and couldn't find it).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1na2l5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2l5b/kimi_k2_0905_official_pricing_generation_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na2l5b/kimi_k2_0905_official_pricing_generation_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T15:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9gfpt</id>
    <title>Anthropic to pay $1.5 billion to authors in landmark AI settlement</title>
    <updated>2025-09-05T20:41:52+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"&gt; &lt;img alt="Anthropic to pay $1.5 billion to authors in landmark AI settlement" src="https://external-preview.redd.it/2giFHQHB-5T6ma6XiIR2StAHVaV1z6nAKhfbARNarkE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d341db7b1d508270a9cf44051e58699980b97ccb" title="Anthropic to pay $1.5 billion to authors in landmark AI settlement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/anthropic/773087/anthropic-to-pay-1-5-billion-to-authors-in-landmark-ai-settlement"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T20:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1na0mw3</id>
    <title>Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5</title>
    <updated>2025-09-06T14:15:15+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"&gt; &lt;img alt="Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5" src="https://external-preview.redd.it/KUWKhlT5OZYpzmuPdkrY6FyowQ4PaYe23RiUvraDVrQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4733d277f6f6e759fe47794087d1da790f8d36b7" title="Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/discussions/255"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T14:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9x1ho</id>
    <title>So I tried Qwen 3 Max skills for programming</title>
    <updated>2025-09-06T11:19:58+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt; &lt;img alt="So I tried Qwen 3 Max skills for programming" src="https://external-preview.redd.it/_Qhoi5tM5uwwG8h9pFbHe7_wEttk4KG4M_-539ZjdPE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d6c7a702febc40739857dbd7082314de38697a5" title="So I tried Qwen 3 Max skills for programming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;So I Tried Qwen 3 Max for Programming ‚Äî Project VMP (Visualized Music Player)&lt;/h1&gt; &lt;p&gt;I wanted to see how far Qwen 3 Max could go when tasked with building a full project from a very detailed specification. The result: VMP ‚Äî Visualized Music Player, a cyberpunk-style music player with FFT-based visualizations, crossfade playback, threading, and even a web terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Tech Stack &amp;amp; Dependencies&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Python 3.11&lt;/li&gt; &lt;li&gt;pygame, numpy, mutagen, pydub, websockets&lt;/li&gt; &lt;li&gt;Requires FFmpeg in PATH&lt;/li&gt; &lt;li&gt;Runs with a simple BAT file on Windows&lt;/li&gt; &lt;li&gt;SDL hints set for Windows: &lt;ul&gt; &lt;li&gt;SDL_RENDER_DRIVER=direct3d&lt;/li&gt; &lt;li&gt;SDL_HINT_RENDER_SCALE_QUALITY=1&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Features&lt;/h1&gt; &lt;h1&gt;Configuration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;AudioCfg, VisualCfg, UiCfg dataclasses with sane defaults&lt;/li&gt; &lt;li&gt;Global instances: AUDIO, VIS, UI&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logging&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Custom logger vmp with console + rotating file handler&lt;/li&gt; &lt;li&gt;Optional WebTermHandler streams logs to connected websocket clients&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;FFmpeg Integration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Automatic FFmpeg availability check&lt;/li&gt; &lt;li&gt;On-demand decode with ffmpeg -ss ... -t ... into raw PCM&lt;/li&gt; &lt;li&gt;Reliable seeking via decoded segments&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Music Library&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Recursive scan for .mp3, .wav, .flac, .ogg, .m4a&lt;/li&gt; &lt;li&gt;Metadata via mutagen (fallback to smart filename guessing)&lt;/li&gt; &lt;li&gt;Sortable, with directory ignore list&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;DSP &amp;amp; Analysis&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Stereo EQ (low shelf, peaking, high shelf) + softclip limiter&lt;/li&gt; &lt;li&gt;FFT analysis with Hann windows, band mapping, adaptive beat detection&lt;/li&gt; &lt;li&gt;Analysis LRU cache (capacity 64) for performance&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Visualization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Cyberpunk ring with dotted ticks, glow halos, progress arc&lt;/li&gt; &lt;li&gt;Outward 64-band bars + central vocal pulse disc&lt;/li&gt; &lt;li&gt;Smooth envelopes, beat halos, ~60% transparent overlays&lt;/li&gt; &lt;li&gt;Fonts: cyberpunk.ttf if present, otherwise Segoe/Arial&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Playback Model&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;pygame.mixer at 44.1 kHz stereo&lt;/li&gt; &lt;li&gt;Dual-channel system for precise seeking and crossfade overlap&lt;/li&gt; &lt;li&gt;Smooth cosine crossfade without freezing visuals&lt;/li&gt; &lt;li&gt;Modes: &lt;ul&gt; &lt;li&gt;Music = standard streaming&lt;/li&gt; &lt;li&gt;Channel = decoded segment playback (reliable seek)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Window &amp;amp; UI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Resizable window, optional fake fullscreen&lt;/li&gt; &lt;li&gt;Backgrounds with dark overlay, cache per resolution&lt;/li&gt; &lt;li&gt;Topmost toggle, drag-window mode (Windows)&lt;/li&gt; &lt;li&gt;Presets for HUD/FPS/TIME/TITLE (keys 1‚Äì5, V, F2)&lt;/li&gt; &lt;li&gt;Help overlay (H) shows all controls&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Controls&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Playback: Space pause/resume, N/P next/prev, S shuffle, R repeat-all&lt;/li&gt; &lt;li&gt;Seek: ‚Üê/‚Üí ‚àí5s / +5s&lt;/li&gt; &lt;li&gt;Window/UI: F fake fullscreen, T topmost, B toggle backgrounds, [/] prev/next BG&lt;/li&gt; &lt;li&gt;Volume: Mouse wheel; volume display fades quickly&lt;/li&gt; &lt;li&gt;Quit: Esc / Q&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Web Terminal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Optional --webterm flag&lt;/li&gt; &lt;li&gt;Websocket server on ws://localhost:3030&lt;/li&gt; &lt;li&gt;Streams logs + accepts remote commands (n, p, space, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Low-CPU visualization mode (--viz-lowcpu)&lt;/li&gt; &lt;li&gt;Heavy operations skipped while paused&lt;/li&gt; &lt;li&gt;Preallocated NumPy buffers &amp;amp; surface caches&lt;/li&gt; &lt;li&gt;Threaded FFT + loader workers, priority queue for analysis&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;CLI Options&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;--music-dir Path to your music library --backgrounds Path to background images --debug Verbose logging --shuffle Enable shuffle mode --repeat-all Repeat entire playlist --no-fft Disable FFT --viz-lowcpu Low CPU visualization --ext File extensions to include --ignore Ignore directories --no-tags Skip metadata tags --webterm Enable websocket terminal &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Crossfade works seamlessly, with no visual freeze&lt;/li&gt; &lt;li&gt;Seek is reliable thanks to FFmpeg segment decoding&lt;/li&gt; &lt;li&gt;Visualizations scale cleanly across windowed and fake-fullscreen modes&lt;/li&gt; &lt;li&gt;Handles unknown tags gracefully by guessing titles from filenames&lt;/li&gt; &lt;li&gt;Everything runs as a single script, no external modules beyond listed deps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ Full repo: &lt;a href="https://github.com/feckom/vmp"&gt;github.com/feckom/vmp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Results&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wixd9wdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b1a18941410cb3a7f4b0da54f36003298180dca"&gt;https://preview.redd.it/wixd9wdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b1a18941410cb3a7f4b0da54f36003298180dca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m6chuvdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0c0df79e54b59b2ab064e4f7c791bb7984297a8b"&gt;https://preview.redd.it/m6chuvdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0c0df79e54b59b2ab064e4f7c791bb7984297a8b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bma8vwdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bfe32593e27d63fd9e533c6202979bc9da6d8330"&gt;https://preview.redd.it/bma8vwdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bfe32593e27d63fd9e533c6202979bc9da6d8330&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T11:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3f1s</id>
    <title>Renting GPUs is hilariously cheap</title>
    <updated>2025-09-06T16:08:44+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt; &lt;img alt="Renting GPUs is hilariously cheap" src="https://preview.redd.it/dhtzimf7jknf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bca94832d9e6b8fb7b8faf80d61387d12889d7f" title="Renting GPUs is hilariously cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A 140 GB monster GPU that costs $30k to buy, plus the rest of the system, plus electricity, plus maintenance, plus a multi-Gbps uplink, for a little over 2 bucks per hour.&lt;/p&gt; &lt;p&gt;If you use it for 5 hours per day, 7 days per week, and factor in auxiliary costs and interest rates, buying that GPU today vs. renting it when you need it will only pay off in 2035 or later. That‚Äôs a tough sell.&lt;/p&gt; &lt;p&gt;Owning a GPU is great for privacy and control, and obviously, many people who have such GPUs run them nearly around the clock, but for quick experiments, renting is often the best option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dhtzimf7jknf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c3l2</id>
    <title>AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more.</title>
    <updated>2025-09-04T14:43:01+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt; &lt;img alt="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." src="https://external-preview.redd.it/y8IJElEOEd_2568MHNUZQsP7_aRTCAzyzXUKpDJwl1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e377887ea8d7eae841499cc497b90b82aa97816" title="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're super excited to do this AMA. Come ask your questions to the researchers behind &lt;strong&gt;SmolLM, SmolVLM, FineWeb&lt;/strong&gt;, and more. You can learn more about our work at &lt;a href="http://hf.co/science"&gt;hf.co/science&lt;/a&gt; ü§ó&lt;/p&gt; &lt;p&gt;If you want to get started in ML, a good place is &lt;a href="https://hf.co/learn"&gt;https://hf.co/learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we release a new &lt;strong&gt;FineVision&lt;/strong&gt; dataset, check it out! &lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eliebak"&gt;Elie Bakouch&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/loubnabnl"&gt;Loubna Ben Allal&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/loubnabnl"&gt;u/loubnabnl&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nouamanetazi"&gt;Nouamane Tazi&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Norlax"&gt;u/Norlax&lt;/a&gt;_42 (Nanotron/SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lvwerra"&gt;Leandro von Werra&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lvwerra"&gt;u/lvwerra&lt;/a&gt; (Head of Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/edbeeching"&gt;Edward Beeching&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/edbeeching"&gt;u/edbeeching&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cmpatino"&gt;Carlos Miguel Pati√±o&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/cmpatino"&gt;u/cmpatino&lt;/a&gt;_ (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kashif"&gt;Kashif Rasul&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/krasul"&gt;u/krasul&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lewtun"&gt;Lewis Tunstall&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lewtun"&gt;u/lewtun&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/qgallouedec"&gt;Quentin Gallou√©dec&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/qgallouedec"&gt;u/qgallouedec&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/clefourrier"&gt;Cl√©mentine Fourrier&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/clefourrier"&gt;u/clefourrier&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/SaylorTwift"&gt;Nathan Habib&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/HauntingMoment"&gt;u/HauntingMoment&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lusxvr"&gt;Luis Wiedmann&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/luswd"&gt;u/luswd&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/andito"&gt;Andres Marafioti&lt;/a&gt;, &lt;a href="/u/futterneid"&gt;u/futterneid&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guipenedo"&gt;Guilherme Penedo&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/PhilipsNostrum"&gt;u/PhilipsNostrum&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/hynky"&gt;Hynek Kydl√≠ƒçek&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Other"&gt;u/Other&lt;/a&gt;_Housing8453 (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/reach-vb"&gt;Vaibhav Srivastav,&lt;/a&gt; &lt;a href="/u/vaibhavs10"&gt;u/vaibhavs10&lt;/a&gt; (Head of Developer Experience and Community)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BrigitteTousi"&gt;Brigitte Tousignant&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/BriggieSmalls1992"&gt;u/BriggieSmalls1992&lt;/a&gt; (Comms)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Xenova"&gt;Xenova&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/xenovatech"&gt;u/xenovatech&lt;/a&gt; (Transformers.js)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/craffel"&gt;Colin Raffel&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/craffel"&gt;u/craffel&lt;/a&gt; (Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ngxson"&gt;Xuan Son Nguyen&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/MediocreProgrammer99"&gt;u/MediocreProgrammer99&lt;/a&gt; (llama.cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you are passionate about open source and open science like us, apply at &lt;a href="https://hf.co/jobs"&gt;https://hf.co/jobs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Hugging Face team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135"&gt;https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended but we will still answer question async for the next 24h. Follow our &lt;a href="https://hf.co/science"&gt;Hugging Face Science Org&lt;/a&gt; to be aware of our latest release! ü§ó&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:43:01+00:00</published>
  </entry>
</feed>
