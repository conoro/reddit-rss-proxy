<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-12T11:49:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pkgzyj</id>
    <title>Could this be Avocado?</title>
    <updated>2025-12-12T03:02:00+00:00</updated>
    <author>
      <name>/u/Excellent-Treat-7105</name>
      <uri>https://old.reddit.com/user/Excellent-Treat-7105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkgzyj/could_this_be_avocado/"&gt; &lt;img alt="Could this be Avocado?" src="https://b.thumbs.redditmedia.com/lXnePSn0ro_gTOZto8F-b-9NLDepQlblR9vAgbDtFeY.jpg" title="Could this be Avocado?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2sgwyahsto6g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dcc6999794ac104e23385fbd2b8d6ab15de61bd"&gt;https://preview.redd.it/2sgwyahsto6g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dcc6999794ac104e23385fbd2b8d6ab15de61bd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just spotted a stealth model on LMArena that claims to be created by Meta. Anyone know what this is? Could be something new they're testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Treat-7105"&gt; /u/Excellent-Treat-7105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkgzyj/could_this_be_avocado/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkgzyj/could_this_be_avocado/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkgzyj/could_this_be_avocado/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk4lrc</id>
    <title>235 contributors from around the world to gather one of the largest robotics dataset (46 different robots - 250 hours - 26M frames)</title>
    <updated>2025-12-11T18:13:38+00:00</updated>
    <author>
      <name>/u/Wide-Screen-4632</name>
      <uri>https://old.reddit.com/user/Wide-Screen-4632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4lrc/235_contributors_from_around_the_world_to_gather/"&gt; &lt;img alt="235 contributors from around the world to gather one of the largest robotics dataset (46 different robots - 250 hours - 26M frames)" src="https://preview.redd.it/b7fxmkoi8m6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5f95ad80194c85e5b971768cd862e7be77f763e" title="235 contributors from around the world to gather one of the largest robotics dataset (46 different robots - 250 hours - 26M frames)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the dataset: &lt;a href="https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3"&gt;https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide-Screen-4632"&gt; /u/Wide-Screen-4632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b7fxmkoi8m6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4lrc/235_contributors_from_around_the_world_to_gather/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4lrc/235_contributors_from_around_the_world_to_gather/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T18:13:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbmqe</id>
    <title>TFLOPS by GPU</title>
    <updated>2025-12-11T22:55:15+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I just updated the score for RTX PRO 6000, look like different cloud providers yield a different result. And added the result for M1 Pro MBP (both MLX and MPS).&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I'm not a professional ML engineer/researcher, I just enjoy ML/AI development as a hobby (still, it would be nice if this knowledge could be transferred to a real job). Just like many people in this sub, I was debating with myself on the idea of buying myself a PC, or buying a DGX Spark, or a mini PC with a Strix Halo, or just renting a cloud one.&lt;/p&gt; &lt;p&gt;Using free GPUs on Google Colab and Kaggle sometimes feels like enough for me, but it's slow. So I decided to run a quick benchmark on different GPUs to see what the actual difference is, and what I would miss for being stingy.&lt;/p&gt; &lt;p&gt;The benchmark script &lt;a href="https://x.com/awnihannun/status/1982880363765768288"&gt;was taken&lt;/a&gt; from Awni Hannun's tweet (MLX co-author), it's basically do matrix multiplications on two BF16 8192x8192 matrices.&lt;/p&gt; &lt;p&gt;Disclaimer: I know just TFLOPS alone is not enough when it come to performance (memory bandwidth, power consumption, other factors like RAM/CPU,...), but it's still make a sense for a quick comparison.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;strong&gt;Device&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;BF16 TFLOPS&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Time (ms)&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;B200&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;1629.45&lt;/td&gt; &lt;td&gt;306.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;H200 SXM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;680.32&lt;/td&gt; &lt;td&gt;734.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;MI300X (ROCm)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;464.90&lt;/td&gt; &lt;td&gt;1075.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX PRO 6000 WK&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;375.03&lt;/td&gt; &lt;td&gt;1333.226&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;L40S&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;209.75&lt;/td&gt; &lt;td&gt;2383.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX 5090&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;207.254&lt;/td&gt; &lt;td&gt;2428.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX 4090&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;152.89&lt;/td&gt; &lt;td&gt;3270.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;A40&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;110.386&lt;/td&gt; &lt;td&gt;4529.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia RTX 3090&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;70.86&lt;/td&gt; &lt;td&gt;7055.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;L4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;56.66&lt;/td&gt; &lt;td&gt;8823.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Tesla V100&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;10.15&lt;/td&gt; &lt;td&gt;49242.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;M2 Max MBP 64GB (MLX)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;6.984&lt;/td&gt; &lt;td&gt;71593.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kaggle P100&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;5.708&lt;/td&gt; &lt;td&gt;87594.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;M2 Max MBP 64GB (Pytorch MPS)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;4.796&lt;/td&gt; &lt;td&gt;104246.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;M1 Pro MBP 16GB (MLX)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;3.429&lt;/td&gt; &lt;td&gt;145803.26ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;M1 Pro MBP 16GB (Pytorch MPS)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;2.315&lt;/td&gt; &lt;td&gt;215972.68ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google Colab T4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;2.314&lt;/td&gt; &lt;td&gt;216094.496&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kaggle 2xT4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;2.177&lt;/td&gt; &lt;td&gt;229686.30&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The code was modified to run on MPS for macbook. ON the AMD one, no modification needed, run on ROCm.&lt;/p&gt; &lt;p&gt;Also, some numbers I found online, on other devices that I could not confirmed myself:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;strong&gt;Device&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;BF16 TFLOPS&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DGX Spark&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;M5 MBP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;It would be nice if someone with other devices can run the test and confirm that the numbers are correct.&lt;/p&gt; &lt;p&gt;After looking at the numbers, I feel like a Strix Halo miniPC (even 64GB) would be more than enough, and if I ever feel the need for CUDA, then adding a 3090 will do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbmqe/tflops_by_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbmqe/tflops_by_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbmqe/tflops_by_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T22:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkmi5v</id>
    <title>Docling PDF Parsing with remote VLM</title>
    <updated>2025-12-12T08:05:43+00:00</updated>
    <author>
      <name>/u/Top-Fig1571</name>
      <uri>https://old.reddit.com/user/Top-Fig1571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;currently i am using the Mineru Library to parse PDF to markdown which is great as it as well preserves images or text coordinates. However I might need to switch to a non-chinese solution so i planned to use docling.&lt;/p&gt; &lt;p&gt;I am not sure if granite-docling is strong enough to handle complex pdfs so my plan was to switch the VLM. But as docling is specialized with doctags I am not sure if it is reliably working with remote VLM (e.g. OlmOCR). Does anyone have a solid docling pipeline already for this?&lt;/p&gt; &lt;p&gt;Also what is in your opinion the best way to parse PDFs with images/tables nowadays? Are these the small, specializes OCR VLMs like granite-docling or OlmOCR or are big VLMs better? I need an Open Source solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Fig1571"&gt; /u/Top-Fig1571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkmi5v/docling_pdf_parsing_with_remote_vlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkmi5v/docling_pdf_parsing_with_remote_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkmi5v/docling_pdf_parsing_with_remote_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T08:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkome1</id>
    <title>MLX Fine-Tuning Issue: Trainer Ignores my jsonl files</title>
    <updated>2025-12-12T10:28:05+00:00</updated>
    <author>
      <name>/u/PMogu</name>
      <uri>https://old.reddit.com/user/PMogu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkome1/mlx_finetuning_issue_trainer_ignores_my_jsonl/"&gt; &lt;img alt="MLX Fine-Tuning Issue: Trainer Ignores my jsonl files" src="https://b.thumbs.redditmedia.com/FFr1Bp_3oqPJzrNogpfn6UnOcGD_WbrUo-4M7Ut7G7c.jpg" title="MLX Fine-Tuning Issue: Trainer Ignores my jsonl files" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I’m new to programming and currently exploring fine-tuning with &lt;strong&gt;MLX&lt;/strong&gt;. I found this tutorial very helpful: &lt;a href="https://www.youtube.com/watch?v=BCfCdTp-fdM"&gt;https://www.youtube.com/watch?v=BCfCdTp-fdM&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I was able to download a dataset from the internet and organize it as the tutorial suggests (&lt;code&gt;train.jsonl&lt;/code&gt; and &lt;code&gt;valid.jsonl&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;However, I ran into a problem when starting the training. When I run the command shown at &lt;strong&gt;08:29&lt;/strong&gt;, it always seems to load the Hugging Face dataset &lt;code&gt;mlx-community/WikiSQL&lt;/code&gt; instead of my own &lt;code&gt;train.jsonl&lt;/code&gt; and &lt;code&gt;valid.jsonl&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I’m not sure what I did wrong, because in the video the dataset appears to be automatically detected. Any help would be appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7tabkq1k2r6g1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=151c1ba81661dc2cb19099ca45cf7209e0fd0148"&gt;my files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qul5wnf42r6g1.png?width=1130&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407a79022e4d175c48f84c3591ecb279838c5c8e"&gt;part of my terminal command lines&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PMogu"&gt; /u/PMogu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkome1/mlx_finetuning_issue_trainer_ignores_my_jsonl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkome1/mlx_finetuning_issue_trainer_ignores_my_jsonl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkome1/mlx_finetuning_issue_trainer_ignores_my_jsonl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T10:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk4e27</id>
    <title>Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source</title>
    <updated>2025-12-11T18:05:23+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt; &lt;img alt="Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source" src="https://b.thumbs.redditmedia.com/kKZ3ZoCKjqFucLu9U3zcx9kw9f07ItjqFMxxpwplBXs.jpg" title="Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, thanks for your suggestions of what models to evaluate! Still working on some, but we've just added Kimi K2 thinking and the two new mistral models. Turns out Kimi K2 Thinking takes the top, surpassing minimax by 2.4%pts (that's 12 task instances). The devstral models fall in the middle, but they are currently freely available on the mistral API!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7d8p912z5m6g1.png?width=4071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6688a0a1b7583b3c78097fbb75c31618cbe46b21"&gt;https://preview.redd.it/7d8p912z5m6g1.png?width=4071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6688a0a1b7583b3c78097fbb75c31618cbe46b21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All of these results are independently evaluated with the exact same (minimal) agent. So it is expected that the numbers are lower than what companies typically report.&lt;/p&gt; &lt;p&gt;Note the asterisk with the cost for Kimi K2 thinking, it is calculated based on the official API pricing information, but the actual cost that was billed seemed lower (but also the cost portal seemed buggy, so not sure what to trust here—for now it's calculated based on the number of tokens same as all the other reported). Anyone know what could be causing any discrepancies?&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking and the devstral models are the exact opposite in terms of steps: Kimi K2 takes the least steps to iterate of all models, devstral the most.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/37akv7ra6m6g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab53c4ba03c2f013f21fc9115a53e87e111db10"&gt;https://preview.redd.it/37akv7ra6m6g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab53c4ba03c2f013f21fc9115a53e87e111db10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're thinking about limiting runtimes to conserve costs/time, here's how performance scales with step limits (even with Kimi, you still want to run for 125-150 steps on hard problems).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tdoe4zh6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3803a5c3567ebb0ffee73c5245b3ff92d02e7ec"&gt;https://preview.redd.it/6tdoe4zh6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3803a5c3567ebb0ffee73c5245b3ff92d02e7ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And this would translate in the following cost-performance plot (where deepseek is still hard to beat). We didn't put the mistral models in here because they're only free temporarily. Of course those are just your API costs, so if you're running on your own hardware, you can ignore this plot:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd9gseql6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f78011f256fa2019627b1b89962ec418593163d"&gt;https://preview.redd.it/fd9gseql6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f78011f256fa2019627b1b89962ec418593163d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have all the trajectories/logs updated if you're curious how each model solves things. They're available from the &amp;quot;Trajs&amp;quot; column on &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As always, you can reproduce our numbers using &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt; (there's a page in the tutorial).&lt;/p&gt; &lt;p&gt;Any new models we should add? (there's still some recommendations from last time that I didn't get to yet). Or any other information we should add ? (we've started collecting latency information as of recently).&lt;/p&gt; &lt;p&gt;Also curious if things like the number of steps a model takes etc. show up in your workflows. Depending on how closely users are in the loop behavior is probably quite different. Also would be interested if you have any qualitative observations about the model behaviors and how they differ (if there's interesting observations, we could see if we can add more information about them for the next releases based on all the agent trajectories we collect)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T18:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3cky</id>
    <title>Shisa V2.1: Improved Japanese (JA/EN) Models (1.2B-70B)</title>
    <updated>2025-12-11T17:25:49+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're celebrating the 2 year anniversary of our original &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/18cwh4n/shisa_7b_a_new_jaen_bilingual_model_based_on/"&gt;Shisa V1&lt;/a&gt; with an updated set of &lt;a href="https://huggingface.co/collections/shisa-ai/shisa-v21"&gt;Shisa V2.1&lt;/a&gt; JA/EN bilingual models.&lt;/p&gt; &lt;p&gt;Shisa V2.1 introduces new and improved 8B, 14B, and 70B dense models with a big performance bump to our previous &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz2lll/shisa_v2_a_family_of_new_jaen_bilingual_models/"&gt;Shisa V2 releases&lt;/a&gt;, as well as new 1.2B (LFM2-based) and 3B (Llama 3.2-based) models. Each of these are class-leading in Japanese language capabilities for their size. Our new V2.1 14B beats the old V2 70B and the new V2.1 70B model gets very close to our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;Shisa V2 405B&lt;/a&gt;! These aren't reasoning or coding models, but if you're looking for an open model that is especially strong at natural/native Japanese, maybe give these a spin.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;License&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Context Length&lt;/th&gt; &lt;th align="left"&gt;JA AVG&lt;/th&gt; &lt;th align="left"&gt;EN AVG&lt;/th&gt; &lt;th align="left"&gt;JA-MT Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-lfm2-1.2b"&gt;shisa-v2.1-lfm2-1.2b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1.2B&lt;/td&gt; &lt;td align="left"&gt;32K&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;27.6&lt;/td&gt; &lt;td align="left"&gt;6.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-llama3.2-3b"&gt;shisa-v2.1-llama3.2-3b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;57.9&lt;/td&gt; &lt;td align="left"&gt;43.2&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-qwen3-8b"&gt;shisa-v2.1-qwen3-8b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;32K/128K&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;td align="left"&gt;57.8&lt;/td&gt; &lt;td align="left"&gt;8.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MIT&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-unphi4-14b"&gt;shisa-v2.1-unphi4-14b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;14B&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;72.6&lt;/td&gt; &lt;td align="left"&gt;57.7&lt;/td&gt; &lt;td align="left"&gt;9.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-llama3.3-70b"&gt;shisa-v2.1-llama3.3-70b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;73.1&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;9.26&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For those that just want to kick the tires, we have &lt;a href="https://chat.shisa.ai/"&gt;https://chat.shisa.ai/&lt;/a&gt; up and running that lets you test and compare V2.1 14B, V2.1 70B, and V2 405B, you might be surprised at just how strong the smaller models are.&lt;/p&gt; &lt;p&gt;These models were all trained on an MI300X node provided by AMD via the &lt;a href="https://www.amd.com/en/developer/resources/cloud-access/amd-developer-cloud.html"&gt;AMD Developer Cloud&lt;/a&gt;. Thanks to all of our compute sponsors, we couldn't keep releasing open models without them. More details (including all sponsors and very detailed eval info) are available on the HF model cards or our &lt;a href="https://shisa.ai/posts/shisa-v2.1/"&gt;announcement post&lt;/a&gt; and mradermacher and others have made GGUFs over the past couple days already for all sizes.&lt;/p&gt; &lt;p&gt;I did want to pull out one interesting bit from the model card, since it's fairly new and unique:&lt;/p&gt; &lt;h3&gt;Cross-Lingual Token Leakage&lt;/h3&gt; &lt;p&gt;While reviewing eval results, we noticed that many models can score highly on Japanese language benchmarks but still output non-Japanese words or sub-words (tokens). Internally we refer to this as Cross-Lingual Token Leakage (CLTL). It has also been referred to more generally as &amp;quot;word-level language confusion&amp;quot; (Marchisio et al., &amp;quot;&lt;a href="https://arxiv.org/abs/2406.20052"&gt;Understanding and Mitigating Language Confusion in LLMs&lt;/a&gt;,&amp;quot; Cohere).&lt;/p&gt; &lt;p&gt;We see many strong multilingual models that exhibit language confusion behavior, but quantifying (and reliably identifying) this issue is harder than one might expect because not only do Japanese and Chinese share Unicode code-planes, but also many valid English words can commonly appear in Japanese text. (Think &amp;quot;AI&amp;quot;, &amp;quot;VR&amp;quot;, or common words and acronyms like &amp;quot;Google&amp;quot; or &amp;quot;NATO&amp;quot;). This is compounded by the fact that even frontier models suffer from “token blindness” - they are often unable to disentangle the meaning from the actual language of the tokens and often fail to recognize wrong-language tokens.&lt;/p&gt; &lt;p&gt;For Shisa V2.1, we have developed a brand-new class of Japanese evaluation benchmark specifically designed to identify CLTL, which can both measure and specifically identify wrong language tokens.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Base Model&lt;/th&gt; &lt;th&gt;Shisa V2.1 Model&lt;/th&gt; &lt;th align="right"&gt;Base Leak %&lt;/th&gt; &lt;th align="right"&gt;Shisa V2.1 Leak %&lt;/th&gt; &lt;th align="right"&gt;Leakage Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Llama-3.2-3B-Instruct&lt;/td&gt; &lt;td&gt;shisa-v2.1-llama3.2-3b&lt;/td&gt; &lt;td align="right"&gt;11.48%&lt;/td&gt; &lt;td align="right"&gt;0.24%&lt;/td&gt; &lt;td align="right"&gt;47.8×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM2-1.2B&lt;/td&gt; &lt;td&gt;shisa-v2.1-lfm2-1.2b&lt;/td&gt; &lt;td align="right"&gt;4.32%&lt;/td&gt; &lt;td align="right"&gt;0.32%&lt;/td&gt; &lt;td align="right"&gt;13.5×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B&lt;/td&gt; &lt;td&gt;shisa-v2.1-qwen3-8b&lt;/td&gt; &lt;td align="right"&gt;2.18%&lt;/td&gt; &lt;td align="right"&gt;0.44%&lt;/td&gt; &lt;td align="right"&gt;5.0×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td&gt;shisa-v2.1-llama3.3-70b&lt;/td&gt; &lt;td align="right"&gt;1.90%&lt;/td&gt; &lt;td align="right"&gt;0.36%&lt;/td&gt; &lt;td align="right"&gt;5.3×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi-4&lt;/td&gt; &lt;td&gt;shisa-v2.1-unphi4-14b&lt;/td&gt; &lt;td align="right"&gt;0.12%&lt;/td&gt; &lt;td align="right"&gt;0.06%&lt;/td&gt; &lt;td align="right"&gt;2.0×&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We believe eliminating both CLTL and language confusion in general is of the utmost importance for deploying LLMs for most Japanese-language production use cases (e.g., translation, customer service, or even basic writing tasks) and we plan to continue to both improve our detection heuristics and to integrate it into all our future evaluation grading, as well as use our better CLTL detection to further improve our training methods. We will be publishing more details in-depth in a future writeup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhx0l</id>
    <title>whats everyones thoughts on devstral small 24b?</title>
    <updated>2025-12-12T03:46:24+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Idk if llamacpp is broken for it but my experience is not too great. &lt;/p&gt; &lt;p&gt;Tried creating a snake game and it failed to even start. Considered that maybe the model is more focused on solving problems so I gave it a hard leetcode problem that imo it shouldve been trained on but when it tried to solve it, failed...which gptoss 20b and qwen30b a3b both completed successfully. &lt;/p&gt; &lt;p&gt;lmk if theres a bug the quant I used was unsloth dynamic 4bit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3znw</id>
    <title>Microsoft analyzed 37.5 million AI conversations in 2025.</title>
    <updated>2025-12-11T17:50:31+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt; &lt;img alt="Microsoft analyzed 37.5 million AI conversations in 2025." src="https://b.thumbs.redditmedia.com/I0gHntPEl9oW4aCmQGgV1BpHx_Jt0xMYNUHzNOY1Pys.jpg" title="Microsoft analyzed 37.5 million AI conversations in 2025." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released their &amp;quot;Copilot Usage Report 2025,&amp;quot; analyzing de-identified data to see how people actually use AI in their daily lives. The results are surprisingly human. Here are the most interesting graphs and takeaways from the report:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &amp;quot;Work Hard, Play Hard&amp;quot; Split&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;People have distinct modes for the week vs. the weekend.&lt;/p&gt; &lt;p&gt;View Graph: Programming vs. Gaming&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: In August, there was a perfect crossover. &amp;quot;Programming&amp;quot; queries rise steadily from Monday to Friday, then tank on Saturday/Sunday. &amp;quot;Gaming&amp;quot; does the exact opposite, dominating the weekends.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The 2 AM Philosophy Club&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The topics we talk about change drastically depending on the time of day.&lt;/p&gt; &lt;p&gt;View Graph: Topic by Hour of Day&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: This radial chart shows that &amp;quot;Travel&amp;quot; queries peak during standard commuting hours. However, &amp;quot;Religion and Philosophy&amp;quot; sees a massive spike in the early morning hours. If you're asking AI about the nature of existence at 3 AM, you aren't alone.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The Valentine's Day Panic&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;February data shows a very specific narrative arc.&lt;/p&gt; &lt;p&gt;View Graph: February Topic Trends&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: &amp;quot;Personal Growth&amp;quot; topics peak in the days leading up to Valentine's Day (people trying to improve themselves?), while &amp;quot;Relationship&amp;quot; queries spike on the day itself (people needing immediate advice).&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Health is King on Mobile&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;When we are on our phones, we are almost always worried about our health.&lt;/p&gt; &lt;p&gt;View Graph: Top Mobile Topics&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: No matter the month, &amp;quot;Health&amp;quot; is consistently the #1 topic for mobile users, far outpacing entertainment or productivity. TL;DR: We use AI to code during the week, survive relationships in February, and serve as a therapist/philosopher late at night.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source: &lt;a href="https://microsoft.ai/news/its-about-time-the-copilot-usage-report-2025/?utm_source=alphasignal&amp;amp;utm_campaign=2025-12-11&amp;amp;lid=bpzfIvhThUltNeQ9&amp;amp;hl=en-GB"&gt;Microsoft AI - The Copilot Usage Report 2025 &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pk3znw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkg4iy</id>
    <title>Typical performance of gpt-oss-120b on consumer hardware?</title>
    <updated>2025-12-12T02:20:52+00:00</updated>
    <author>
      <name>/u/Diligent-Culture-432</name>
      <uri>https://old.reddit.com/user/Diligent-Culture-432</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this typical performance, or are there ways to optimize tps even further?&lt;/p&gt; &lt;p&gt;11-12 tps on gpt-oss-120b on 32GB VRAM (2x5060Ti) &amp;amp; 128GB DDR4 RAM&lt;/p&gt; &lt;p&gt;- Intel i7-11700&lt;/p&gt; &lt;p&gt;- 1x 5060Ti 16gb on PCIe x16&lt;/p&gt; &lt;p&gt;- 1x 5060Ti 16gb on PCIe x4&lt;/p&gt; &lt;p&gt;- 4x 32 GB DDR4-3200 RAM (actually appears to be running at 2400 on checking task manager)&lt;/p&gt; &lt;p&gt;- Running on LM Studio&lt;/p&gt; &lt;p&gt;- 32k context&lt;/p&gt; &lt;p&gt;- experts offloaded to CPU&lt;/p&gt; &lt;p&gt;- 36/36 GPU offloaded&lt;/p&gt; &lt;p&gt;- flash attention enabled&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Culture-432"&gt; /u/Diligent-Culture-432 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T02:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pko44g</id>
    <title>Benchmark Fatigue - How do you evaluate new models for yourself?</title>
    <updated>2025-12-12T09:55:10+00:00</updated>
    <author>
      <name>/u/Funny-Clock1582</name>
      <uri>https://old.reddit.com/user/Funny-Clock1582</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting more and more the impression that the benchmark results published for new models are not even close to the experience i make with models.&lt;br /&gt; Maybe its time for me to create some standard questions for a first quick evaluation of new models just for myself.&lt;br /&gt; Do you guys do this and do you have prompts you feel are helpful in your experience?&lt;/p&gt; &lt;p&gt;Cheers Wolfram &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Funny-Clock1582"&gt; /u/Funny-Clock1582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjvtgn</id>
    <title>Leaked footage from Meta's post-training strategy meeting.</title>
    <updated>2025-12-11T12:02:11+00:00</updated>
    <author>
      <name>/u/YouCanMake1t</name>
      <uri>https://old.reddit.com/user/YouCanMake1t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt; &lt;img alt="Leaked footage from Meta's post-training strategy meeting." src="https://preview.redd.it/2cbgowoj0i6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8274908702ea2b4e3ee76f7741b54aa24bef73d7" title="Leaked footage from Meta's post-training strategy meeting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouCanMake1t"&gt; /u/YouCanMake1t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2cbgowoj0i6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkphs3</id>
    <title>Undo for destructive shell commands used by AI agents (SafeShell)</title>
    <updated>2025-12-12T11:22:14+00:00</updated>
    <author>
      <name>/u/qhkmdev90</name>
      <uri>https://old.reddit.com/user/qhkmdev90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As local AI agents start running shell commands directly, we probably need a better way to protect the filesystem than sandboxes or confirmation prompts.&lt;/p&gt; &lt;p&gt;I built a small open source tool called SafeShell that makes destructive commands reversible (rm, mv, cp, chmod, chown).&lt;/p&gt; &lt;p&gt;It automatically checkpoints before a command runs, so if an agent deletes or mutates the wrong files, you can roll back instantly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rm -rf ./build safeshell rollback --last &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No sandbox, VM, or root&lt;/p&gt; &lt;p&gt;Hard-link snapshots (minimal overhead)&lt;/p&gt; &lt;p&gt;Single Go binary (macOS + Linux)&lt;/p&gt; &lt;p&gt;MCP support&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qhkm/safeshell"&gt;https://github.com/qhkm/safeshell&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how others are handling filesystem safety for local agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qhkmdev90"&gt; /u/qhkmdev90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbwco</id>
    <title>EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B</title>
    <updated>2025-12-11T23:06:43+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt; &lt;img alt="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" src="https://b.thumbs.redditmedia.com/jSd0TbO_srnflwRt8ojBram2pVNjfZ3rVlBCwbCJcHQ.jpg" title="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com"&gt;https://eqbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gpt-5.2 writing samples: &lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/gpt-5.2.html"&gt;https://eqbench.com/results/creative-writing-v3/gpt-5.2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;opus-4.5 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html"&gt;https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;mistral-large-3 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html"&gt;https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;nanbeige4-3b writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html"&gt;https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pkbwco"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T23:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkndwc</id>
    <title>Chat bots up to 24B</title>
    <updated>2025-12-12T09:06:00+00:00</updated>
    <author>
      <name>/u/PsychologicalMud210</name>
      <uri>https://old.reddit.com/user/PsychologicalMud210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like to chat about random subjects with AI. It serves more as an aid to thought and sometimes they are really helpful. Subjects may be sensitive, so I like to run local. &lt;/p&gt; &lt;p&gt;What are the best models up to about 24B that I can use? In your experience, what exactly this model does best?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologicalMud210"&gt; /u/PsychologicalMud210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjw7rj</id>
    <title>Mistral’s Vibe CLI now supports a 200K token context window (previously 100K)</title>
    <updated>2025-12-11T12:23:44+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt; &lt;img alt="Mistral’s Vibe CLI now supports a 200K token context window (previously 100K)" src="https://external-preview.redd.it/ZnNsb2d0dzFpazZnMZt0kKC274AvCvOpM9k0UQCIyB1BQvPjsN5T3o1kO8eQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=841cfe71df83eebc90bd5a8915c65e4a8693db6c" title="Mistral’s Vibe CLI now supports a 200K token context window (previously 100K)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4nxnq6w1ik6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkjx5y</id>
    <title>Agentic coding with 32GB of VRAM.. is it doable?</title>
    <updated>2025-12-12T05:29:18+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Theres some solid models that run at this size, but for agentic coding I consider 60K context the bare minimum to get a good number of iterations in on a microservice.&lt;/p&gt; &lt;p&gt;Assuming I can tolerate Q8/Q8 kv cache quantization.. what's the best model I can run that'll fit 60K confidently?&lt;/p&gt; &lt;p&gt;Qwen3-VL-32B runs, but to hit 60K I need to drop down to iq4_xs, and that's introducing frequent errors that Q5 and Q6 don't encounter.&lt;/p&gt; &lt;p&gt;Qwen3-30B-Coder is in a somewhat similar spot only it's faster and works slightly worse with these tools.&lt;/p&gt; &lt;p&gt;Qwen3-Next works great but since I need CPU offloading to start with, prompt processing quickly becomes unacceptably slow.&lt;/p&gt; &lt;p&gt;Anything smaller I've tried fails to adhere to the lengthy 10k token system prompts or enters an infinite loop.&lt;/p&gt; &lt;p&gt;Any suggestions? Is it doable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T05:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhzf0</id>
    <title>Reverse-Engineering the RK3588 NPU: Hacking Memory Limits to run massive Vision Transformers</title>
    <updated>2025-12-12T03:49:27+00:00</updated>
    <author>
      <name>/u/one_does_not_just</name>
      <uri>https://old.reddit.com/user/one_does_not_just</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I worked on a &amp;quot;fun&amp;quot; project for my grad school class. I decided to write a blog post about it, maybe its useful to someone who is dealing with problems deploying vision transformers on edge devices&lt;/p&gt; &lt;p&gt;&lt;a href="https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/"&gt;https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Removed massive from title, but reddit won't let me change title, sorry about that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_does_not_just"&gt; /u/one_does_not_just &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pko16f</id>
    <title>7B MoE with 1B active</title>
    <updated>2025-12-12T09:49:48+00:00</updated>
    <author>
      <name>/u/lossless-compression</name>
      <uri>https://old.reddit.com/user/lossless-compression</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that models in that range are relatively rare,I found some models such as (may not be exactly 7B and exactly 1B activated but in that range) are&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1- Granite-4-tiny&lt;/li&gt; &lt;li&gt;2- LFM2-8B-A1B&lt;/li&gt; &lt;li&gt;3- Trinity-nano 6B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most of SLMs that are in that range are made of high amount of experts (tiny experts) where larger amount of experts gets activated but the overall parameters activated are ~1B so the model can specialize well.&lt;/p&gt; &lt;p&gt;I really wonder why that range isn't popular,I tried those models and Trinity nano is a very good researcher and it got a good character too and I asked a few general question it answered well,LFM feels like a RAG model even the standard one,it feels so robotic and answers are not the best,even the 350M can be coherent but it still feels like a RAG model, didn't test Granite 4 tiny yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lossless-compression"&gt; /u/lossless-compression &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkflfw</id>
    <title>Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth</title>
    <updated>2025-12-12T01:56:20+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"&gt; &lt;img alt="Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth" src="https://preview.redd.it/1f2wim2zgl6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=884f4f99a6d0cca98f924c0f9b62f3ea56ac95cb" title="Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1f2wim2zgl6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T01:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhudf</id>
    <title>US Administration Issues Executive Order Opposing State-Level Regulation of AI Industry</title>
    <updated>2025-12-12T03:42:45+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The EO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/"&gt;https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My take: The EO orders the US AG to set up a task force to sue states which have legislated their own AI industry regulations, orders other agencies to prepare a report on how states might be denied federal funds, and orders that a set of recommendations be made to Congress to draft and pass new laws.&lt;/p&gt; &lt;p&gt;It seems like Christmas came early for commercial inference services, this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk0ubn</id>
    <title>New in llama.cpp: Live Model Switching</title>
    <updated>2025-12-11T15:49:43+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt; &lt;img alt="New in llama.cpp: Live Model Switching" src="https://external-preview.redd.it/8Hy799ws5wvJKYaRb__KN0TGXYxiPxKG6PuG-1SlIWg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a43f5804fb810225237c9c37046b91c9bbb6451" title="New in llama.cpp: Live Model Switching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/model-management-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T15:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkdkjo</id>
    <title>Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b</title>
    <updated>2025-12-12T00:22:10+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt; &lt;img alt="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" src="https://external-preview.redd.it/NDYwbGgydmYybzZnMf2LvdJmBzIyNzEDfN0eOt2yDrF46dRxJq4WcX4O0NUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4363f44505584728345cc48958c232a7ab91036f" title="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A a3b LLM is all you need :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vewmcluf2o6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T00:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkidf6</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 12GB VRAM and 32GB RAM?</title>
    <updated>2025-12-12T04:08:01+00:00</updated>
    <author>
      <name>/u/Dex921</name>
      <uri>https://old.reddit.com/user/Dex921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if it's allowed, but I am asking about ALL available LLMs including ones that are closed source and cannot be run locally (like chatgpt or gemini, and in that case obviously the ram limit doesn't apply)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dex921"&gt; /u/Dex921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T04:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
