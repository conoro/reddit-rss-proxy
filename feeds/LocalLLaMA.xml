<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-23T02:47:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p3sinc</id>
    <title>NVFP4 MOE on Blackwell (5090 and RTX PRO 6000)</title>
    <updated>2025-11-22T12:53:09+00:00</updated>
    <author>
      <name>/u/Dependent_Factor_204</name>
      <uri>https://old.reddit.com/user/Dependent_Factor_204</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those running SM120 cards (5090 and RTX PRO 6000)&lt;/p&gt; &lt;p&gt;NVFP4 MOE models have been near impossible to run.&lt;/p&gt; &lt;p&gt;Until now!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/BlackwellPerformance/comments/1p2xe94/4x_rtx_pro_6000_with_nvfp4_glm_46/"&gt;https://www.reddit.com/r/BlackwellPerformance/comments/1p2xe94/4x_rtx_pro_6000_with_nvfp4_glm_46/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a specific nightly build of VLLM that has support - but is broken again in the current nightly.&lt;/p&gt; &lt;p&gt;It should with other smaller NVFP4 models too if you don't have multiple cards.&lt;/p&gt; &lt;p&gt;Its a huge RAM saving over FP8 with virtually the same quality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent_Factor_204"&gt; /u/Dependent_Factor_204 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3sinc/nvfp4_moe_on_blackwell_5090_and_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3sinc/nvfp4_moe_on_blackwell_5090_and_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3sinc/nvfp4_moe_on_blackwell_5090_and_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T12:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3pggo</id>
    <title>Rust HF Downloader (Yet Another TUI)</title>
    <updated>2025-11-22T09:53:40+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love the terminal, but I don't exactly love copy-pasting names of models and URLs of a specific quantization or file to download using the huggingface cli.&lt;/p&gt; &lt;p&gt;Probably there's better ways, but I just rolled my own! &lt;/p&gt; &lt;p&gt;--&lt;br /&gt; Introducing: üí• Rust HF Downloader üí•&lt;br /&gt; A Terminal User Interface (TUI) application for searching, browsing, and downloading models from the HuggingFace model hub.&lt;/p&gt; &lt;p&gt;Please break it. And then tell me how you broke it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JohannesBertens/rust-hf-downloader"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3pggo/rust_hf_downloader_yet_another_tui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3pggo/rust_hf_downloader_yet_another_tui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T09:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4auwe</id>
    <title>Interactive LogitLens Advanced for Llama</title>
    <updated>2025-11-23T02:11:55+00:00</updated>
    <author>
      <name>/u/Environmental_Form14</name>
      <uri>https://old.reddit.com/user/Environmental_Form14</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4auwe/interactive_logitlens_advanced_for_llama/"&gt; &lt;img alt="Interactive LogitLens Advanced for Llama" src="https://external-preview.redd.it/dmI0dHd0YnUweDJnMZvpFkisJfya1FNiLd2dMFSngmnGoCB3NTgitMZolYsV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adbeed6b575ed18cc2c2e0ea2d0efc780d6c143d" title="Interactive LogitLens Advanced for Llama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/blindTissue/logit_lens_llama_advanced"&gt;github link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all, I created an interactive Logit Lens for Llama and thought some of you might find it useful. It is something that I wish existed.&lt;/p&gt; &lt;h2&gt;What is Logit Lens?&lt;/h2&gt; &lt;p&gt;Logit Lens is an interpretability tool first introduced by &lt;a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"&gt;nonstalgebraist&lt;/a&gt;, with the aim of interpreting what the model &lt;em&gt;thinks&lt;/em&gt; in its intermediate stages of LLMs by projecting the intermediate activation to the final layer's unembedding matrix. The method has been mildly popular, with hundreds of papers using it to understand how LLM think internally.&lt;/p&gt; &lt;h2&gt;The reason for making this repo&lt;/h2&gt; &lt;p&gt;With how widely the method is used, I thought there would be a popular repo that makes logit lens easy for the users to use. This wasn't the case. &lt;/p&gt; &lt;p&gt;The most starred Logit Lens &lt;a href="https://github.com/zhenyu-02/LogitLens4LLMs/issues"&gt;repo on github&lt;/a&gt; seemed problematic. The output in the readme did not match my local implementation nor other repository's output. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/TransformerLensOrg/TransformerLens"&gt;TransformerLens&lt;/a&gt; repository is fantastic but quite large. You have to piece together the docs and code yourself to get an interactive logit lens workflow, but that takes time.&lt;/p&gt; &lt;p&gt;Also, many public repos were using the original gpt2 or project-specific models rather than current, widely used ones.&lt;/p&gt; &lt;p&gt;So I built a small tool with the features I wanted.&lt;/p&gt; &lt;h2&gt;Stuff it can do.&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Interactively show a more granular logit lens output for user input&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Allow users to modify the residual stream, attention outputs, and MLP outputs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Allow users to block attention from and to certain tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Save and load current intervention / outputs into and from JSON and npz files.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The following only works for Llama at the moment.&lt;/p&gt; &lt;p&gt;Let me know what you think. If there are additional features you would like, please leave a comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Environmental_Form14"&gt; /u/Environmental_Form14 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uyi5ztbu0x2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4auwe/interactive_logitlens_advanced_for_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4auwe/interactive_logitlens_advanced_for_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ayly</id>
    <title>Did a crazy speculative decoding experiment, which gave very bad results</title>
    <updated>2025-11-23T02:16:59+00:00</updated>
    <author>
      <name>/u/StomachWonderful615</name>
      <uri>https://old.reddit.com/user/StomachWonderful615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have using Apple‚Äôs mlx-lm to run my local inference for a while. I have two machines, an 8GB M2 Macbook Pro, and a 128GB M4 Macbook Studio. I usually run the bigger models like Qwen3 30b or Llama3 70b on Mac Studio and connect through API. I am also able to do speculative decoding with smaller models like Llama3 1b on Mac Studio.&lt;/p&gt; &lt;p&gt;Here are my general metrics: Llama 70b on Mac Studio - 48 tokens per sec Llama 70b target and 1b draft on Mac Studio - 55 tokens per sec Llama 1b model on Macbook Pro - 70 tokens per sec&lt;/p&gt; &lt;p&gt;I wanted to create an experimental approach of doing disaggregated speculative decoding, where draft model runs locally and target validation and rejection sampling runs on Mac Studio remotely, with draft sending draft tokens to remote server. After lot of experimentation, able to get acceptance rate to around 60%, but I am getting about 2 tokens per sec with this approach on Macbook üò≠&lt;/p&gt; &lt;p&gt;I was hoping to speed up and get good quality output, instead I am getting worse speed.&lt;/p&gt; &lt;p&gt;Is my experiment thought process wrong, or should I consider something in my implementation.&lt;/p&gt; &lt;p&gt;My original thought for this experiment - Teams can have normal sized Macbooks, able to run small models for quick generation, but validated with a bigger Model on a local server to achieve both speed and quality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StomachWonderful615"&gt; /u/StomachWonderful615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ayly/did_a_crazy_speculative_decoding_experiment_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ayly/did_a_crazy_speculative_decoding_experiment_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ayly/did_a_crazy_speculative_decoding_experiment_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p412yp</id>
    <title>OpenAI Demo'd Fixing Issue #2472 Live. It's Still Open.</title>
    <updated>2025-11-22T18:53:58+00:00</updated>
    <author>
      <name>/u/tymscar</name>
      <uri>https://old.reddit.com/user/tymscar</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tymscar"&gt; /u/tymscar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.tymscar.com/posts/openaiunmergeddemo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p412yp/openai_demod_fixing_issue_2472_live_its_still_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p412yp/openai_demod_fixing_issue_2472_live_its_still_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T18:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4b4ym</id>
    <title>History of Information Retrieval - From Library of Alexandria to Retrieval Augmented Generation (RAG)</title>
    <updated>2025-11-23T02:25:33+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b4ym/history_of_information_retrieval_from_library_of/"&gt; &lt;img alt="History of Information Retrieval - From Library of Alexandria to Retrieval Augmented Generation (RAG)" src="https://external-preview.redd.it/lIaeAmsdrNA4jg89MgWNfUefCvvcGXumtCeKx4LZA7M.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6da4bf3bdb7f5ca917e8f0a1acfd41e2d2eeb85" title="History of Information Retrieval - From Library of Alexandria to Retrieval Augmented Generation (RAG)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/EKBy4b9oUAE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b4ym/history_of_information_retrieval_from_library_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b4ym/history_of_information_retrieval_from_library_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4b5dn</id>
    <title>Is there a way to use Google SensorLM?</title>
    <updated>2025-11-23T02:26:11+00:00</updated>
    <author>
      <name>/u/inAbigworld</name>
      <uri>https://old.reddit.com/user/inAbigworld</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use Google SensorLM but I cannot find a source. I searched for SensorLLM but it seemed too complicated to use. Others are too inefficient. Do you have any advice?&lt;br /&gt; I basically need an llm to interpret 1000 lines of data like what SensorLM examples show.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inAbigworld"&gt; /u/inAbigworld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b5dn/is_there_a_way_to_use_google_sensorlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b5dn/is_there_a_way_to_use_google_sensorlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b5dn/is_there_a_way_to_use_google_sensorlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4b6ti</id>
    <title>V100 vs 5060ti vs 3090 - Some numbers</title>
    <updated>2025-11-23T02:28:10+00:00</updated>
    <author>
      <name>/u/dompazz</name>
      <uri>https://old.reddit.com/user/dompazz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I'm new here. Ive been hosting servers on Vast for years, and finally started playing with running models locally. This site has been a great resource.&lt;/p&gt; &lt;p&gt;I've seen a couple of posts in the last few days on each of the GPUs in the title. I have machines with all of them and decided to run some benchmarks and hopefully add something back.&lt;/p&gt; &lt;p&gt;Machines:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8x V100 SXM2 16G. This was the machine that I started on Vast with. Picked it up post ETH mining craze for dirt cheap. 2x E5-2690 v4 (56 threads) 512G RAM&lt;/li&gt; &lt;li&gt;8x 5060ti 16G. Got the board and processors from a guy in the CPU mining community. Cards are running via MCIO cables and risers - Gen 5x8. 2x EPYC 9654 (384 threads) 384G RAM&lt;/li&gt; &lt;li&gt;4x 3090, 2 NVLINK Pairs. Older processors 2x E5-2695 v3 (56 threads) 512G RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the V100 and 5060ti are about the best setup you can get with those cards. The 3090 rig could use newer hardware, they are running Gen3 PCI-E and the topology requires the pairs to cross the numa nodes to talk to each other which runs around gen3 x4 speed.&lt;/p&gt; &lt;p&gt;Speed specs put the 3090 in first place in raw compute&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3090 - 35.6 TFlops FP16 (936Gb/s bandwidth)&lt;/li&gt; &lt;li&gt;V100 - 31.3 TFlops FP16 (897 Gb/s bandwidth)&lt;/li&gt; &lt;li&gt;5060ti - 23.7 TFlops FP16 (448 Gb/s bandwidth)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Worth noting the 3090 and 5060ti cards should be able to do double that TFlops, but for Nvidia nerf-ing them...&lt;/p&gt; &lt;p&gt;Ran llama-bench with llama3.1 70B Instruct Q4 model with n_gen set to 256 (ran n_prompt numbers as well but they are just silly)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3090 - 19.09 T/s&lt;/li&gt; &lt;li&gt;V100 - 16.68 T/s&lt;/li&gt; &lt;li&gt;5060ti - 9.66 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Numbers wise, the generation is roughly in line with the compute capacity (edited out badly formatted table, see comment for numbers)&lt;/p&gt; &lt;p&gt;Are there other numbers I should be running here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dompazz"&gt; /u/dompazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b6ti/v100_vs_5060ti_vs_3090_some_numbers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b6ti/v100_vs_5060ti_vs_3090_some_numbers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b6ti/v100_vs_5060ti_vs_3090_some_numbers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4bgt0</id>
    <title>speech model school recording</title>
    <updated>2025-11-23T02:42:21+00:00</updated>
    <author>
      <name>/u/Alarmed_One6762</name>
      <uri>https://old.reddit.com/user/Alarmed_One6762</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, Im looking for some practical advice from people who really work with ASR and noisy audio. I have a situation: I put a tiny voice recorder in my kid‚Äôs school backpack. Not trying to spy on anyone or anything like that it‚Äôs just that my kid is still little and can‚Äôt always explain what happened during the day, so this helps me understand how things are going.&lt;/p&gt; &lt;p&gt;The school rules say electronic devices must stay inside the backpack and not be taken out. the recorder sits in the outer mesh pocket of the backpack, recording all day, and Im the only one who ever listens to it. Nothing gets uploaded or shared.&lt;/p&gt; &lt;p&gt;The recordings are about 6 hours long, 2gb. i made chunks about 20minutes and run it on my 5090 nvidia. Quality is‚Ä¶ typical classroom stuff: 18 kids, lots of child noise, teacher talking, discussions, random bursts of loudness. But no fabric rubbing, no fans, no mechanical noise just voices and background chatter. By ear it‚Äôs actually pretty understandable, just tiring to listen to. I tried: Whisper large-v3 (FP16) Mistral Voxtral Mini (3B)&lt;/p&gt; &lt;p&gt;Both work okay, but Id like to squeeze more clarity, especially for the kids voice. Im not sure whether I should try running the audio through something like DeepFilterNet, CleanVoice, Resemble, NVIDIA noise removal, etc. Ive seen mixed opinions some people say denoisers ruin speech, others say they help with far-field recordings.&lt;/p&gt; &lt;p&gt;my question is: For this kind of recording (single mic in a backpack, classroom, lots of overlapping child voices, but no mechanical noise), what models/processes would you recommend? Should I do any denoising at all? Or is it better to feed the raw audio straight into ASR? Are there newer models people prefer over Whisper for this type of farfield multi-speaker audio?&lt;/p&gt; &lt;p&gt;Thanks for your time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarmed_One6762"&gt; /u/Alarmed_One6762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bgt0/speech_model_school_recording/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bgt0/speech_model_school_recording/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bgt0/speech_model_school_recording/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3d34y</id>
    <title>Inspired by a recent post: a list of the cheapest to most expensive 32GB GPUs on Amazon right now, Nov 21 2025</title>
    <updated>2025-11-21T22:56:25+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by a recent post where someone was putting together a system based on two 16GB GPUs for $800 I wondered how one might otherwise conveniently acquire 32GB of reasonably performant VRAM as cheaply as possible?&lt;/p&gt; &lt;p&gt;Bezos to the rescue!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hewlett Packard Enterprise NVIDIA Tesla M10 Quad GPU Module&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $279&lt;/li&gt; &lt;li&gt;VRAM: GDDR5 (332 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Hewlett-Packard-Enterprise-NVIDIA-870046-001/dp/B075VQ5LF8"&gt;https://www.amazon.com/Hewlett-Packard-Enterprise-NVIDIA-870046-001/dp/B075VQ5LF8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AMD Radeon Instinct MI60 32GB HBM2 300W&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $499&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (1.02 TB/s)&lt;/li&gt; &lt;li&gt;PCIe: 4.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Instinct-Compute-Graphics-Accellerator-Renewed/dp/B0DMTTF15B"&gt;https://www.amazon.com/Instinct-Compute-Graphics-Accellerator-Renewed/dp/B0DMTTF15B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tesla V100 32GB SXM2 GPU W/Pcie Adapter &amp;amp; 6+2 Pin&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $879.00&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Tesla-V100-32GB-Adapter-Computing/dp/B0FXWJ8HKD"&gt;https://www.amazon.com/Tesla-V100-32GB-Adapter-Computing/dp/B0FXWJ8HKD&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Tesla V100 Volta GPU Accelerator 32GB&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $969&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/NVIDIA-Tesla-Volta-Accelerator-Graphics/dp/B07JVNHFFX"&gt;https://www.amazon.com/NVIDIA-Tesla-Volta-Accelerator-Graphics/dp/B07JVNHFFX&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Tesla V100 (Volta) 32GB&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $1144&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/NVIDIA-Tesla-900-2G503-0310-000-NVLINK-GPU/dp/B07WDDNGXK"&gt;https://www.amazon.com/NVIDIA-Tesla-900-2G503-0310-000-NVLINK-GPU/dp/B07WDDNGXK&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GIGABYTE AORUS GeForce RTX 5090 Master 32G&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $2599&lt;/li&gt; &lt;li&gt;VRAM: GDDR7 (1792 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 5.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N5090AORUS-M-32GD/dp/B0DT7GHQMD"&gt;https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N5090AORUS-M-32GD/dp/B0DT7GHQMD&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;PNY NVIDIA GeForce RTX‚Ñ¢ 5090 OC Triple Fan&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $2749&lt;/li&gt; &lt;li&gt;VRAM: GDDR7 (1792 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 5.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/PNY-GeForce-Overclocked-Graphics-3-5-Slot/dp/B0DTJF8YT4/"&gt;https://www.amazon.com/PNY-GeForce-Overclocked-Graphics-3-5-Slot/dp/B0DTJF8YT4/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For comparison an RTX 3090 has 24GB of 936.2 GB/s GDDR6X&lt;del&gt;, so for $879 it's hard to grumble about 32GB of 898 GB/s HBM2 in those V100s!&lt;/del&gt; and the AMD card has gotta be tempting for someone at that price! &lt;/p&gt; &lt;p&gt;Edit: the V100 doesn‚Äôt support CUDA 8.x and later, so check compatibility before making impulse buys!&lt;/p&gt; &lt;p&gt;Edit 2: found an MI60!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T22:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3ntta</id>
    <title>What is the Ollama or llama.cpp equivalent for image generation?</title>
    <updated>2025-11-22T08:06:36+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for some form of terminal based image generator (text to image). I want to use it as a background process for an app I am working on.&lt;/p&gt; &lt;p&gt;I think I can use A1111 without the web interface, but I would like a more ‚Äúopen source‚Äù alternative.&lt;/p&gt; &lt;p&gt;A couple of places mentioned Invoke AI. But then I‚Äôve read it got acquired by Adobe.&lt;/p&gt; &lt;p&gt;A third option would be to just build some custom python script, but that sounds a bit too complex for an MVP development stage.&lt;/p&gt; &lt;p&gt;Any other suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ntta/what_is_the_ollama_or_llamacpp_equivalent_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ntta/what_is_the_ollama_or_llamacpp_equivalent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ntta/what_is_the_ollama_or_llamacpp_equivalent_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T08:06:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3fwj5</id>
    <title>GLM planning a 30-billion-parameter model release for 2025</title>
    <updated>2025-11-22T01:00:05+00:00</updated>
    <author>
      <name>/u/aichiusagi</name>
      <uri>https://old.reddit.com/user/aichiusagi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"&gt; &lt;img alt="GLM planning a 30-billion-parameter model release for 2025" src="https://external-preview.redd.it/age5KNQL_0umG4-4KoTku-i61lSg2HdDlBNVJO56C64.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ee469153e66d145a6de755ed94715567aa83c6e" title="GLM planning a 30-billion-parameter model release for 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aichiusagi"&gt; /u/aichiusagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://open.substack.com/pub/chinatalk/p/the-zai-playbook?selection=2e7c32de-6ff5-4813-bc26-8be219a73c9d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T01:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3w595</id>
    <title>EPSTEIN FILES 20K: Tracking Community Projects</title>
    <updated>2025-11-22T15:35:51+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The EPSTEIN 20K dataset release on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; last monday is currently trending on the front page of hugging face &lt;a href="https://huggingface.co/"&gt;https://huggingface.co/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to this sub, &lt;strong&gt;we now have 5 projects&lt;/strong&gt; running on the dataset. I've started an Github org - EF20K to track them all &lt;a href="https://github.com/EF20K/Projects"&gt;https://github.com/EF20K/Projects&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I plan to spend this weekend working on this project. If you've already built a project on this dataset, please let me know. Also contributors at any level are welcome.&lt;/p&gt; &lt;p&gt;How to contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Projects"&gt;Build a RAG system &lt;/a&gt;- Create your own retrieval system to query the files. Top performing systems will be featured on the projects repo highlights&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Datasets"&gt;Dataset cleaning&lt;/a&gt; - Convert raw jpg files to clean text using vision models for enhance quality. There is lot of room for improving the current OCR output.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Datasets"&gt;Expand the dataset &lt;/a&gt;- Compile additional documents from the Epstein Files releases. There are several documents released before Nov 12 2025, including some interesting ones like flight logs&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Safety"&gt;Safety &amp;amp; accuracy&lt;/a&gt; - Report any concerns or inaccuracies you find in the dataset or the projects.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;For RAG system builders:&lt;/strong&gt; I'm curating Q&amp;amp;A pairs own my own using LLMs for benchmarking due to the sensitive nature of the data. If you would like to collaborate on this, do dm me.&lt;/p&gt; &lt;p&gt;New to contributing to open source projects? Feel free to reach out directly to me to learn how to contribute. I'd be happy to help you get started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3w595/epstein_files_20k_tracking_community_projects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3w595/epstein_files_20k_tracking_community_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3w595/epstein_files_20k_tracking_community_projects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T15:35:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4augy</id>
    <title>Writingway 2: An open source tool for AI-assisted writing</title>
    <updated>2025-11-23T02:11:19+00:00</updated>
    <author>
      <name>/u/Clueless_Nooblet</name>
      <uri>https://old.reddit.com/user/Clueless_Nooblet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a freeware version of sites like NovelCrafter or Sudowrite. Runs on your machine, costs zero, nothing gets saved on some obscure server, and you could even run it with a local model completely without internet access.&lt;/p&gt; &lt;p&gt;Of course FOSS.&lt;/p&gt; &lt;p&gt;Here's my blog post about it: &lt;a href="https://aomukai.com/2025/11/23/writingway-2-now-plug-and-play/"&gt;https://aomukai.com/2025/11/23/writingway-2-now-plug-and-play/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clueless_Nooblet"&gt; /u/Clueless_Nooblet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4augy/writingway_2_an_open_source_tool_for_aiassisted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4augy/writingway_2_an_open_source_tool_for_aiassisted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4augy/writingway_2_an_open_source_tool_for_aiassisted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:11:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3znnu</id>
    <title>Discord for LLMs</title>
    <updated>2025-11-22T17:57:16+00:00</updated>
    <author>
      <name>/u/Kooky_Meaning_7168</name>
      <uri>https://old.reddit.com/user/Kooky_Meaning_7168</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3znnu/discord_for_llms/"&gt; &lt;img alt="Discord for LLMs" src="https://b.thumbs.redditmedia.com/FuEOExmQDgJEWeIJYbIRAcUZ4Ow8yH80K17ItXvxvlY.jpg" title="Discord for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm thinking of publishing it soon.&lt;/p&gt; &lt;p&gt;You guys like it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky_Meaning_7168"&gt; /u/Kooky_Meaning_7168 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p3znnu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3znnu/discord_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3znnu/discord_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T17:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3qxj4</id>
    <title>I created a coding tool that produce prompts simple enough for smaller, local models</title>
    <updated>2025-11-22T11:24:53+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3qxj4/i_created_a_coding_tool_that_produce_prompts/"&gt; &lt;img alt="I created a coding tool that produce prompts simple enough for smaller, local models" src="https://preview.redd.it/ueah8pouks2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b5a4b22a09083ae0eef1b09257c99563e73b72a" title="I created a coding tool that produce prompts simple enough for smaller, local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I'm working on a free and open-source tool that is non agentic. This design choice makes messages very simple, as all the model sees are hand-picked files and simple instructions. In the example above, I didn't have to tell the model I wanted to edit &amp;quot;checkpoints&amp;quot; feature, as this is the only feature attached in context.&lt;/p&gt; &lt;p&gt;This simple approach makes it fully viable to code with smaller, locally hosted models like Qwen 32B.&lt;/p&gt; &lt;p&gt;Ollama is listed on the list of providers, and the tool automatically reads downloaded models. It can also initialize many web chats, and Open WebUI is supported.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/robertpiosik/CodeWebChat"&gt;https://github.com/robertpiosik/CodeWebChat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ueah8pouks2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3qxj4/i_created_a_coding_tool_that_produce_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3qxj4/i_created_a_coding_tool_that_produce_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T11:24:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3t4jg</id>
    <title>LlamaTale v0.41.0 - Dungeons v2</title>
    <updated>2025-11-22T13:23:05+00:00</updated>
    <author>
      <name>/u/neph1010</name>
      <uri>https://old.reddit.com/user/neph1010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while since I posted anything about LlamaTale, and indeed it's been dormant for quite a while, too.&lt;/p&gt; &lt;p&gt;I'm sure most of you don't remember it, but over two years ago I began the project as a mix between a structured text-based, rpg (MUD) and LLM generated content. This was a 1000 years ago in AI time, when we had Llama2 models with 4096 token context length. The goal was to create a persistent experience with &amp;quot;unlimited&amp;quot; play length.&lt;/p&gt; &lt;p&gt;The project has been unattended for almost a year, when I finally got some motivation to start again. Using copilot agent as a pair programmer (and frankly, it's doing the grunt work), we have started adding a few new things, and fixing some old ones.&lt;/p&gt; &lt;p&gt;Most recently we refactored &amp;quot;dungeons&amp;quot; to be reusable anywhere in the game. This update allows them to be added to normal stories, or more interestingly probably, be generated inside &amp;quot;anything&amp;quot; stories.&lt;/p&gt; &lt;p&gt;If it sounds interesting, head over to &lt;a href="https://github.com/neph1/LlamaTale/releases/tag/v0.41.0"&gt;https://github.com/neph1/LlamaTale/releases/tag/v0.41.0&lt;/a&gt; and read more about it. Or AMA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neph1010"&gt; /u/neph1010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3t4jg/llamatale_v0410_dungeons_v2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3t4jg/llamatale_v0410_dungeons_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3t4jg/llamatale_v0410_dungeons_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T13:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p417q4</id>
    <title>MiroThinker 72B/30B/8B</title>
    <updated>2025-11-22T18:59:28+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p417q4/mirothinker_72b30b8b/"&gt; &lt;img alt="MiroThinker 72B/30B/8B" src="https://b.thumbs.redditmedia.com/cFENuDl4CVjrFxdj_N06EFCJq1b8hLSrF8o6IcEn3oY.jpg" title="MiroThinker 72B/30B/8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jcm2t7j0wu2g1.png?width=2844&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7214a095ee1e0aa29d26bb76011312fc025e0198"&gt;https://preview.redd.it/jcm2t7j0wu2g1.png?width=2844&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7214a095ee1e0aa29d26bb76011312fc025e0198&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiroThinker v1.0 is an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities.&lt;/p&gt; &lt;p&gt;Unlike previous agents that scale only model size or context length, MiroThinker introduces &lt;strong&gt;interactive scaling&lt;/strong&gt; at the model level, systematically training the model to handle deeper and more frequent agent‚Äìenvironment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.&lt;/p&gt; &lt;p&gt;Empirical results demonstrate the effectiveness of this interactive scaling. Performance across several benchmarks improves predictably as the model engages in increasingly deep and frequent interactions with its environment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gj6quywdvu2g1.png?width=2666&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02aadc19a1da10b04b57ad6a7463ecfddd5104e7"&gt;https://preview.redd.it/gj6quywdvu2g1.png?width=2666&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02aadc19a1da10b04b57ad6a7463ecfddd5104e7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/852xajkhvu2g1.png?width=2870&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7da5dde32522917fcd1addc4808728a8121e83fc"&gt;https://preview.redd.it/852xajkhvu2g1.png?width=2870&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7da5dde32522917fcd1addc4808728a8121e83fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B"&gt;https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B"&gt;https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs and abliterated versions are also available on HF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p417q4/mirothinker_72b30b8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p417q4/mirothinker_72b30b8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p417q4/mirothinker_72b30b8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T18:59:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xv28</id>
    <title>Deep Research Agent, an autonomous research agent system</title>
    <updated>2025-11-22T16:46:04+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"&gt; &lt;img alt="Deep Research Agent, an autonomous research agent system" src="https://external-preview.redd.it/a2ZpajA0cDE4dTJnMXYKxvwpmRJR_6Wuut5rPoqfAX7yC2Fpp67_z2jaY8Dw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05d7bafd92f1750858ecd8c0b51b365c4edcb407" title="Deep Research Agent, an autonomous research agent system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repository: &lt;a href="https://github.com/tarun7r/deep-research-agent"&gt;https://github.com/tarun7r/deep-research-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most &amp;quot;research&amp;quot; agents just summarise the top 3 web search results. I wanted something better. I wanted an agent that could plan, verify, and synthesize information like a human analyst.&lt;/p&gt; &lt;p&gt;How it works (The Architecture): Instead of a single LLM loop, this system orchestrates four specialised agents:&lt;/p&gt; &lt;p&gt;1. The Planner: Analyzes the topic and generates a strategic research plan.&lt;/p&gt; &lt;p&gt;2. The Searcher: An autonomous agent that dynamically decides what to query and when to extract deep content.&lt;/p&gt; &lt;p&gt;3. The Synthesizer: Aggregates findings, prioritizing sources based on credibility scores.&lt;/p&gt; &lt;p&gt;4. The Writer: Drafts the final report with proper citations (APA/MLA/IEEE) and self-corrects if sections are too short.&lt;/p&gt; &lt;p&gt;The &amp;quot;Secret Sauce&amp;quot;: Credibility Scoring One of the biggest challenges with AI research is hallucinations. To solve this, I implemented an automated scoring system. It evaluates sources (0-100) based on domain authority (.edu, .gov) and academic patterns before the LLM ever summarizes them&lt;/p&gt; &lt;p&gt;Built With: Python, LangGraph &amp;amp; LangChain, Google Gemini API, Chainlit&lt;/p&gt; &lt;p&gt;I‚Äôve attached a demo video below showing the agents in action as they tackle a complex topic from scratch.&lt;/p&gt; &lt;p&gt;Check out the code, star the repo, and contribute&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tkn2fiy18u2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p46mkt</id>
    <title>Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms</title>
    <updated>2025-11-22T22:51:53+00:00</updated>
    <author>
      <name>/u/gbomb13</name>
      <uri>https://old.reddit.com/user/gbomb13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"&gt; &lt;img alt="Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms" src="https://b.thumbs.redditmedia.com/gkUZjCWCpd_R9pbF5IZULC7D5HjBa1BJsXWaY241hpQ.jpg" title="Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We looked over its answers, the questions it got correct were the easiest ones but impressive nonetheless compared to other models. &lt;a href="https://spicylemonade.github.io/spatialbench/"&gt;https://spicylemonade.github.io/spatialbench/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gbomb13"&gt; /u/gbomb13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p46mkt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T22:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p40bne</id>
    <title>I got frustrated with existing web UIs for local LLMs, so I built something different</title>
    <updated>2025-11-22T18:23:31+00:00</updated>
    <author>
      <name>/u/alphatrad</name>
      <uri>https://old.reddit.com/user/alphatrad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"&gt; &lt;img alt="I got frustrated with existing web UIs for local LLMs, so I built something different" src="https://b.thumbs.redditmedia.com/YiyI_H_VHB3bA6O2O8UbKRAEKLauUG-4ruRmJAfsWvA.jpg" title="I got frustrated with existing web UIs for local LLMs, so I built something different" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local models for a while now, and like many of you, I tried Open WebUI. The feature list looked great, but in practice... it felt bloated. Slow. Overengineered. And then there is the license restrictions. WTF this isn't truly &amp;quot;open&amp;quot; in the way I expected.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/1337hero/faster-chat"&gt;Faster Chat&lt;/a&gt; - a privacy-first, actually-MIT-licensed alternative that gets out of your way.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nfbihjytou2g1.png?width=2226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43b1fc93ddc80569a95e8bce2999bd237ee6c846"&gt;https://preview.redd.it/nfbihjytou2g1.png?width=2226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43b1fc93ddc80569a95e8bce2999bd237ee6c846&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3KB Preact runtime (NO BLOAT)&lt;/li&gt; &lt;li&gt;Privacy first: conversations stay in your browser&lt;/li&gt; &lt;li&gt;MIT license (actually open source, not copyleft)&lt;/li&gt; &lt;li&gt;Works offline with Ollama/LM Studio/llama.cpp&lt;/li&gt; &lt;li&gt;Multi-provider: OpenAI, Anthropic, Groq, or local models&lt;/li&gt; &lt;li&gt;Docker deployment in one command&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The honest version:&lt;/strong&gt; This is alpha. I'm a frontend dev, not a designer, so some UI quirks exist. Built it because I wanted something fast and private for myself and figued others might want the same.&lt;/p&gt; &lt;p&gt;Docker deployment works. Multi-user auth works. File attachments work. Streaming works. The core is solid.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's still rough:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UI polish (seriously, if you're a designer, please help)&lt;/li&gt; &lt;li&gt;Some mobile responsiveness issues&lt;/li&gt; &lt;li&gt;Tool calling is infrastructure-ready but not fully implemented&lt;/li&gt; &lt;li&gt;Documentation could be better&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've seen the threads about Open WebUI frustrations, and I felt that pain too. So if you're looking for something lighter, faster, and actually open source, give it a shot. And if you hate it, let me know why - I'm here to improve it.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/1337hero/faster-chat"&gt;https://github.com/1337hero/faster-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions/feedback welcome.&lt;/p&gt; &lt;p&gt;Or just roast me and dunk on me. That's cool too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alphatrad"&gt; /u/alphatrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T18:23:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p48d7f</id>
    <title>Strix Halo, Debian 13@6.16.12&amp;6.17.8, Qwen3Coder-Q8 CTX&lt;=131k, llama.cpp@Vulkan&amp;ROCm, Power &amp; Efficiency</title>
    <updated>2025-11-23T00:10:33+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"&gt; &lt;img alt="Strix Halo, Debian 13@6.16.12&amp;amp;6.17.8, Qwen3Coder-Q8 CTX&amp;lt;=131k, llama.cpp@Vulkan&amp;amp;ROCm, Power &amp;amp; Efficiency" src="https://preview.redd.it/hg69ko66fw2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fb55f92f9b2aa5fc700b98933b28eb18462d618" title="Strix Halo, Debian 13@6.16.12&amp;amp;6.17.8, Qwen3Coder-Q8 CTX&amp;lt;=131k, llama.cpp@Vulkan&amp;amp;ROCm, Power &amp;amp; Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i wanted to check kernel improvement in support of strix halo under Debian GNU/Linux, while latest minor versions of 6.16.x improved GTT wanted to check if can be even better. So i tested it on Debian 13 with latest kernel from testing &lt;code&gt;6.16.12+deb14+1-amd64&lt;/code&gt;, and one precompiled performance optimized kernel &lt;code&gt;6.17.8-x64v3-xanmod1&lt;/code&gt;. I ran tests agains &lt;code&gt;Qwen3-Coder-Q8&lt;/code&gt; in full context, but i did benchmark up to &lt;code&gt;131k&lt;/code&gt;. Llama.cpp versions i used for tests: &lt;code&gt;Vulkan build: 5be353ec4 (7109)&lt;/code&gt; and &lt;code&gt;ROCm TheROCK precompiled build: 416e7c7 (1)&lt;/code&gt;. Side notice i managed to compile finally llama.cpp with external libs from AMD for HIP support, so from now one i will use same build for Vulkan and ROCM. Since i wanted also to find sweet spot in energy efficiency so i tried to capture also power usage, and compare it with computing performance. So in the end i tested that model with two backends, and kernels, changing context in few steps, to find out.&lt;/p&gt; &lt;p&gt;In the end seems that latest kernel from testing &lt;code&gt;6.16.12&lt;/code&gt; works just great! Performance kernel speed is maybe fraction better (max 2%). Besides stock kernel had 4W in idle (in &lt;code&gt;balanced&lt;/code&gt; mode), while performance kernel had always minimum 9-10W. And i use fans with 0RPM &amp;lt;= PWM 5% so it's completly silent when idle. And audible under heavy load especially with ROCm. Anyway most optimal power setting for computations is &lt;code&gt;latency-performance&lt;/code&gt; and it's not worth to use &lt;code&gt;accelerator-performance&lt;/code&gt; in the long run.&lt;/p&gt; &lt;p&gt;Here just notice for strix halo Debian users (and other distros probably too, but current Arch and Fedora have newer kernel), you need to use at least &lt;code&gt;6.16.x&lt;/code&gt; to have better experience with that platform. For Debian GNU/Linux easiest way is to install newer kernel from backports, or move to testing for the latest one. I just noticed that with &lt;code&gt;apt update&lt;/code&gt; just now that there is &lt;code&gt;6.16.12&lt;/code&gt; in stable, so it's great nothing to for Debian users. :) And testing moved to &lt;code&gt;6.17.8+deb14-amd64&lt;/code&gt; so great, anyway i will have now that kernel, so will test it soon again from debian branch. haha, what an irony, but it took me quite time to write it down. So update: and just tested &lt;code&gt;6.17.8+deb14-amd64&lt;/code&gt; and idle now is 6W in balance mode now, bit more, than before, but less than the custom kernel.&lt;/p&gt; &lt;p&gt;Performance wise Vulkan is faster in TG, while significantly slower in PP especially with long context. On the other hand ROCm is much faster in PP, and bit slower in TG, but overal improvement in PP is so big that it does not matter for long context (it's around &lt;strong&gt;x2.7 faster&lt;/strong&gt; in 131k CTX window). Vulkan is very fast for shorter chats, but over 32k CTX it's getting much slower. Under load (tested with &lt;code&gt;accelerator-performance&lt;/code&gt; profile in &lt;code&gt;tuned&lt;/code&gt;) ROCm can draw around &lt;code&gt;120W&lt;/code&gt; (this backend use also more CPU for PP), while Vulkan peak was around &lt;code&gt;70W&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I found that best values for &lt;code&gt;-ub&lt;/code&gt; batch size is &lt;code&gt;512&lt;/code&gt;(it's default) for Vulkan, but &lt;code&gt;2048&lt;/code&gt; for ROCm (it's &lt;strong&gt;faster ~16%&lt;/strong&gt; than default). After that you have to increase &lt;code&gt;-b&lt;/code&gt; logical batch size to &lt;code&gt;8192&lt;/code&gt; for best performance with ROCm. For Vulkan just leave default logical batch size.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BONUS section&lt;/strong&gt;, agent test: After tests i wanted to check &lt;code&gt;Qwen3-coder-Q8&lt;/code&gt; model in some tooling so i tried to install &lt;code&gt;kubectl-ai&lt;/code&gt;, and connect it to my local llama-server, and perform some tasks on local kubernetes (4 nodes). Model was able based on the natural language promp install Jupyter hub from helm charts, using ~50k tokens for that. And one could run notebooks in some 8-10 minutes. That model works really good on strix halo, worth to check if you didn't yet.&lt;/p&gt; &lt;p&gt;I hope someone will find it valuable, and diagram clear enough. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hg69ko66fw2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xqsu</id>
    <title>Qwen-image-edit-2511 coming next week</title>
    <updated>2025-11-22T16:41:15+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt; &lt;img alt="Qwen-image-edit-2511 coming next week" src="https://preview.redd.it/yeofdp077u2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef09997e54c4c481e545ec2dd4183f65163c8a73" title="Qwen-image-edit-2511 coming next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yeofdp077u2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:41:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
