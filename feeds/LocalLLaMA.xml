<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-25T03:41:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nprrrf</id>
    <title>Any chances of AI models getting faster with less resources soon?</title>
    <updated>2025-09-24T23:45:06+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen new types of model optimization methods rising slowly and am wondering what's the current fastest format/type and if smaller consumer-grade models between 7b-75b tend to get faster and smaller or it's actually worsening in terms of requirements to be ran locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nprrrf/any_chances_of_ai_models_getting_faster_with_less/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nprrrf/any_chances_of_ai_models_getting_faster_with_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nprrrf/any_chances_of_ai_models_getting_faster_with_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T23:45:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nozz23</id>
    <title>The Ryzen AI MAX+ 395 is a true unicorn (In a good way)</title>
    <updated>2025-09-24T01:57:05+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put an order for the &lt;a href="https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0006"&gt;128GB version of the Framework Desktop Board&lt;/a&gt; for AI inference mainly, and while I've been waiting patiently for it to ship, I had doubts recently about the cost to benefit/future upgrade-ability since the RAM, CPU/iGPU are soldered into the motherboard.&lt;/p&gt; &lt;p&gt;So I decided to do a quick exercise of PC part picking to match the specs Framework is offering in their 128GB Board. I started looking at Motherboards offering 4 Channels, and thought I'd find something cheap.. wrong!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cheapest consumer level MB offering DDR5 at a high speed (8000 MT/s) with more than 2 channels is $600+.&lt;/li&gt; &lt;li&gt;CPU equivalent to the 395 MAX+ in benchmarks is the &lt;a href="https://www.amazon.com/AMD-Ryzen-9950X3D-16-Core-Processor/dp/B0DVZSG8D5"&gt;9955HX3d&lt;/a&gt;, which runs about ~$660 from Amazon. A quiet heat sink with dual fans from &lt;a href="https://www.amazon.com/Noctua-NH-D15-heatpipe-NF-A15-140mm/dp/B00L7UZMAK?s=electronics"&gt;Noctua&lt;/a&gt; is $130&lt;/li&gt; &lt;li&gt;RAM from &lt;a href="https://www.amazon.com/G-SKILL-Trident-CL38-48-48-128-Desktop-Computer/dp/B0F4M6C65N"&gt;G.Skill 4x24&lt;/a&gt; (128GB total) at 8000 MT/s runs you closer to $450.&lt;/li&gt; &lt;li&gt;The 8060s iGPU is similar in performance to the RTX 4060 or &lt;a href="https://www.amazon.com/MSI-Gaming-GeForce-GDRR6-Boost/dp/B0D3KGNMXP"&gt;4060 Ti 16gb&lt;/a&gt;, runs about $400.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Total for this build is ~&lt;strong&gt;$2240. I&lt;/strong&gt;t's obviously a good $500+ more than Framework's board. Cost aside, the speed is compromised as the GPU in this setup will access most of the system RAM at some a loss since it lives outside the GPU chip, and has to traverse the PCIE 5 to access the Memory directly. Total power draw out the wall at full system load at least double the 395's setup. More power = More fan noise = &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;More heat&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To compare, the M4 Pro/Max offer higher memory bandwidth, but suck at running diffusion models, also runs at 2X the cost at the same RAM/GPU specs. The 395 runs Linux/Windows, more flexibility and versatility (Games on Windows, Inference on Linux). Nvidia is so far out in the cost alone it makes no sense to compare it. The closest equivalent (but at much higher inference speed) is 4x 3090 which costs more, consumes multiple times the power, and generates a ton more heat.&lt;/p&gt; &lt;p&gt;AMD has a true unicorn here. For tinkers and hobbyists looking to develop, test, and gain more knowledge in this field, the MAX+ 395 is pretty much the only viable option at this $$ amount, with this low power draw. I decided to continue on with my order, but wondering if anyone else went down this rabbit hole seeking similar answers..!&lt;/p&gt; &lt;p&gt;EDIT: The 9955HX3d does Not support 4-Channels. The more on part is the Threadripper counterpart which has slower memory speeds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T01:57:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nphn86</id>
    <title>Qwen3-30B-A3B for role-playing</title>
    <updated>2025-09-24T17:00:05+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My favorite model for roleplaying, using a good detailed prompt, has been Gemma 3, until today when I decided to try something unusual: Qwen3-30B-A3B. Well, that thing is incredible! It seems to follow the prompt much better than Gemma, interactions and scenes are really vivid, original, filled with sensory details.&lt;/p&gt; &lt;p&gt;The only problem is, it really likes to write (often 15-20 lines per reply) and sometimes it keeps expanding the dialogue in the same reply (so it becomes twice longer...) I'm using the recommended &amp;quot;official&amp;quot; settings for Qwen. Any idea how I can reduce this behaviour?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T17:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1np2v1i</id>
    <title>Large Language Model Performance Doubles Every 7 Months</title>
    <updated>2025-09-24T04:24:24+00:00</updated>
    <author>
      <name>/u/Aralknight</name>
      <uri>https://old.reddit.com/user/Aralknight</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"&gt; &lt;img alt="Large Language Model Performance Doubles Every 7 Months" src="https://external-preview.redd.it/FIe2X4pB5JIPoblqtKC-Psg0C0IDm1Mq5ljjHekoesw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74fd271c0f36614a182e5a476492961d5ccd453d" title="Large Language Model Performance Doubles Every 7 Months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aralknight"&gt; /u/Aralknight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://spectrum.ieee.org/large-language-model-performance"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T04:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1npkqb5</id>
    <title>What's the consensus on Qwen3-Max vs Qwen3 235b Instruct model? How much better do you perceive Max to be?</title>
    <updated>2025-09-24T18:57:04+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously one is more based (open-weight) while the other is proprietary BUT considering Qwen3-Max has over a trillion parameters it should be at least 10% better than 235b right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npkqb5/whats_the_consensus_on_qwen3max_vs_qwen3_235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npkqb5/whats_the_consensus_on_qwen3max_vs_qwen3_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npkqb5/whats_the_consensus_on_qwen3max_vs_qwen3_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T18:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1np8uv6</id>
    <title>InclusionAI published GGUFs for the Ring-mini and Ling-mini models (MoE 16B A1.4B)</title>
    <updated>2025-09-24T10:46:45+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;!!! warning !!! PRs are still not merged (read the discussions) you must use their version of llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16063"&gt;https://github.com/ggml-org/llama.cpp/pull/16063&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16028"&gt;https://github.com/ggml-org/llama.cpp/pull/16028&lt;/a&gt;&lt;/p&gt; &lt;p&gt;models:&lt;/p&gt; &lt;p&gt;Today, we are excited to announce the open-sourcing of &lt;strong&gt;Ling 2.0&lt;/strong&gt; — a family of MoE-based large language models that combine &lt;strong&gt;SOTA performance&lt;/strong&gt; with &lt;strong&gt;high efficiency&lt;/strong&gt;. The first released version, Ling-mini-2.0, is compact yet powerful. It has &lt;strong&gt;16B total parameters&lt;/strong&gt;, but only &lt;strong&gt;1.4B&lt;/strong&gt; are activated per input token (non-embedding 789M). Trained on more than &lt;strong&gt;20T tokens&lt;/strong&gt; of high-quality data and enhanced through multi-stage supervised fine-tuning and reinforcement learning, Ling-mini-2.0 achieves remarkable improvements in complex reasoning and instruction following. With just 1.4B activated parameters, it still reaches the top-tier level of sub-10B dense LLMs and even matches or surpasses much larger MoE models.&lt;/p&gt; &lt;p&gt;Ring is a reasoning and Ling is an instruct model (thanks &lt;a href="/u/Obvious-Ad-2454"&gt;u/Obvious-Ad-2454&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;UPDATE&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, &lt;strong&gt;Ling-flash-2.0&lt;/strong&gt; is officially open-sourced! 🚀 Following the release of the &lt;strong&gt;language model&lt;/strong&gt; &lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0"&gt;&lt;strong&gt;Ling-mini-2.0&lt;/strong&gt;&lt;/a&gt; and the &lt;strong&gt;thinking model&lt;/strong&gt; &lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;&lt;strong&gt;Ring-mini-2.0&lt;/strong&gt;&lt;/a&gt;, we are now open-sourcing the third MoE LLM under the &lt;strong&gt;Ling 2.0 architecture: Ling-flash-2.0&lt;/strong&gt;, a language model with &lt;strong&gt;100B total parameters&lt;/strong&gt; and &lt;strong&gt;6.1B activated parameters (4.8B non-embedding)&lt;/strong&gt;. Trained on &lt;strong&gt;20T+ tokens of high-quality data&lt;/strong&gt;, together with &lt;strong&gt;supervised fine-tuning&lt;/strong&gt; and &lt;strong&gt;multi-stage reinforcement learning&lt;/strong&gt;, Ling-flash-2.0 achieves &lt;strong&gt;SOTA performance among dense models under 40B parameters&lt;/strong&gt;, despite activating only ~6B parameters. Compared to MoE models with larger activation/total parameters, it also demonstrates strong competitiveness. Notably, it delivers outstanding performance in &lt;strong&gt;complex reasoning, code generation, and frontend development&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T10:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1npb1vd</id>
    <title>LongCat-Flash-Thinking, MOE, that activates 18.6B∼31.3B parameters</title>
    <updated>2025-09-24T12:40:20+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npb1vd/longcatflashthinking_moe_that_activates_186b313b/"&gt; &lt;img alt="LongCat-Flash-Thinking, MOE, that activates 18.6B∼31.3B parameters" src="https://preview.redd.it/oswqrfovx3rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d27c1d79f8ef45bbf03cf184beed1a1a5da19925" title="LongCat-Flash-Thinking, MOE, that activates 18.6B∼31.3B parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is happening, can this one be so good? &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meituan-longcat"&gt;https://huggingface.co/meituan-longcat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oswqrfovx3rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npb1vd/longcatflashthinking_moe_that_activates_186b313b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npb1vd/longcatflashthinking_moe_that_activates_186b313b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T12:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nptwiu</id>
    <title>Any vision languages that run on llama.cpp under 96gb anyone recommends?</title>
    <updated>2025-09-25T01:26:07+00:00</updated>
    <author>
      <name>/u/richardanaya</name>
      <uri>https://old.reddit.com/user/richardanaya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some image descriptions I need to fill out for images in markdown, and curious if anyone knows any good vision languages that can be describe them using llama.cpp/llama-server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardanaya"&gt; /u/richardanaya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nptwiu/any_vision_languages_that_run_on_llamacpp_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nptwiu/any_vision_languages_that_run_on_llamacpp_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nptwiu/any_vision_languages_that_run_on_llamacpp_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T01:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nppk2v</id>
    <title>Qwen3 235b Q2 with Celeron, 2x8gb of 2400 RAM, 96GB VRAM @ 18.71 t/s</title>
    <updated>2025-09-24T22:07:11+00:00</updated>
    <author>
      <name>/u/Resident_Computer_57</name>
      <uri>https://old.reddit.com/user/Resident_Computer_57</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nppk2v/qwen3_235b_q2_with_celeron_2x8gb_of_2400_ram_96gb/"&gt; &lt;img alt="Qwen3 235b Q2 with Celeron, 2x8gb of 2400 RAM, 96GB VRAM @ 18.71 t/s" src="https://b.thumbs.redditmedia.com/chV0Qc84J6Iswmf2M7U6Sp0EiMb-QHJH7QP5HCuNZ_I.jpg" title="Qwen3 235b Q2 with Celeron, 2x8gb of 2400 RAM, 96GB VRAM @ 18.71 t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/08qpbxth87rf1.jpg?width=2794&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=12105a746bde03a06b758fcada7fc2aa687b720a"&gt;https://preview.redd.it/08qpbxth87rf1.jpg?width=2794&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=12105a746bde03a06b758fcada7fc2aa687b720a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey guys, this is my current setup, resurrected from an old mining rig. At the moment I have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3x RTX 3090 24gb&lt;/li&gt; &lt;li&gt;3x RTX 3070 8gb&lt;/li&gt; &lt;li&gt;96gb total VRAM&lt;/li&gt; &lt;li&gt;2x8gb 2400MHz RAM&lt;/li&gt; &lt;li&gt;Celeron&lt;/li&gt; &lt;li&gt;Gigabyte GA-H110-D3A motherboard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm getting around 18.71 tokens/sec with Qwen3 235B Q2 (no CPU offloading and really small context).&lt;/p&gt; &lt;p&gt;I'd like to run Q4 without offloading to CPU, because so far the best I've managed with various llama.cpp options is 0.89 tokens/sec, likely due to severe bottlenecks from the slow CPU/motherboard/RAM.&lt;/p&gt; &lt;p&gt;Do you think I can just add more GPUs (I'm aiming for 8 total: 6x3090 + 2x3070 = 160GB VRAM) using some kind of splitters, or do I need to completely rebuild the setup with a server-grade motherboard, faster RAM, etc.?&lt;/p&gt; &lt;p&gt;From what I’ve seen, even with very slow components, as long as I can load everything onto the GPUs, the performance is actually pretty solid for what I need, so if possible I prefer to use the hardware I have.&lt;/p&gt; &lt;p&gt;Thank you for your help!&lt;/p&gt; &lt;p&gt;EDIT command used:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-cli -m ../../../../Qwen3-235B-A22B-Thinking-2507-Q2_K_L-00001-of-00002.gguf --gpu-layers 99 --ctx_size 4000 --temp 0.6 --top_p 0.95 --top-k 20 --tensor-split 3,3,3,1,1,1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Computer_57"&gt; /u/Resident_Computer_57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nppk2v/qwen3_235b_q2_with_celeron_2x8gb_of_2400_ram_96gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nppk2v/qwen3_235b_q2_with_celeron_2x8gb_of_2400_ram_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nppk2v/qwen3_235b_q2_with_celeron_2x8gb_of_2400_ram_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T22:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1npbxpw</id>
    <title>Reproducing GPT-2 (124M) from scratch - results &amp; notes</title>
    <updated>2025-09-24T13:19:20+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"&gt; &lt;img alt="Reproducing GPT-2 (124M) from scratch - results &amp;amp; notes" src="https://external-preview.redd.it/Vt6iMmXcWe78znjNZqJ9nEufupk1m_LVEHGIsA87b3o.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f19f06e9fc779ca6468a6c1b10abcff2a43181b" title="Reproducing GPT-2 (124M) from scratch - results &amp;amp; notes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last couple of weeks, I followed karpathy’s &lt;a href="https://www.youtube.com/watch?v=l8pRSuU81PU"&gt;‘Let’s Reproduce GPT-2’&lt;/a&gt; video religiously—making notes, implementing the logic line by line, and completing a re-implementation of GPT-2 from scratch.&lt;/p&gt; &lt;p&gt;I went a few steps further by implementing some of the improvements suggested by &lt;a href="/u/karpathy"&gt;u/karpathy&lt;/a&gt; (such as learning rate adjustments and data loader fixes), along with modern enhancements like RoPE and SwiGLU-FFN.&lt;/p&gt; &lt;p&gt;My best-performing experiment &lt;code&gt;gpt2-rope&lt;/code&gt;, achieved a validation loss of &lt;strong&gt;2.987&lt;/strong&gt; and a HellaSwag accuracy of &lt;strong&gt;0.320.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ttfrzk1j54rf1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0d9dfec4a39f29885293d8c592373bdc23f36bd"&gt;https://preview.redd.it/ttfrzk1j54rf1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0d9dfec4a39f29885293d8c592373bdc23f36bd&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Experiment&lt;/th&gt; &lt;th align="left"&gt;Min Validation Loss&lt;/th&gt; &lt;th align="left"&gt;Max HellaSwag Acc&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-baseline&lt;/td&gt; &lt;td align="left"&gt;3.065753&lt;/td&gt; &lt;td align="left"&gt;0.303724&lt;/td&gt; &lt;td align="left"&gt;Original GPT-2 architecture&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-periodicity-fix&lt;/td&gt; &lt;td align="left"&gt;3.063873&lt;/td&gt; &lt;td align="left"&gt;0.305517&lt;/td&gt; &lt;td align="left"&gt;Fixed data loading periodicity&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-lr-inc&lt;/td&gt; &lt;td align="left"&gt;3.021046&lt;/td&gt; &lt;td align="left"&gt;0.315475&lt;/td&gt; &lt;td align="left"&gt;Increased learning rate by 3x and reduced warmup steps&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-global-datafix&lt;/td&gt; &lt;td align="left"&gt;3.004503&lt;/td&gt; &lt;td align="left"&gt;0.316869&lt;/td&gt; &lt;td align="left"&gt;Used global shuffling with better indexing&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-rope&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2.987392&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.320155&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Replaced learned embeddings with RoPE&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-swiglu&lt;/td&gt; &lt;td align="left"&gt;3.031061&lt;/td&gt; &lt;td align="left"&gt;0.317467&lt;/td&gt; &lt;td align="left"&gt;Replaced FFN with SwiGLU-FFN activation&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I really loved the whole process of writing the code, running multiple trainings and gradually seeing the losses improve. I learnt so much about LLMs pre-training from this single video. Honestly, the $200 I spent on compute over these two weeks was the best money I’ve spent lately. Learned a ton and had fun.&lt;/p&gt; &lt;p&gt;I have made sure to log everything, the code, training runs, checkpoints, notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Notes: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/notes/lecture_notes.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/notes/lecture_notes.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Runs: &lt;a href="https://wandb.ai/garg-aayush/pre-training"&gt;https://wandb.ai/garg-aayush/pre-training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset (training and validation): &lt;a href="https://drive.google.com/drive/folders/1FGHKpY0_jJmSR_j7ki4oyoxK-fJgldgG?usp=sharing"&gt;Google Drive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Best checkpoints for each experiment: &lt;a href="https://drive.google.com/drive/folders/1S9mFDMG3ZPjA-JGdx_814T_NCVSjUJO-?usp=sharing"&gt;Google Drive&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T13:19:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1npn810</id>
    <title>Kokoro Batch TTS: Enabling Batch Processing for Kokoro 82M</title>
    <updated>2025-09-24T20:33:22+00:00</updated>
    <author>
      <name>/u/asuran2000</name>
      <uri>https://old.reddit.com/user/asuran2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kokoro 82M is a high-performance text-to-speech model, but it originally lacked support for batch processing. I spent a week implementing batch functionality, and the source code is available at &lt;a href="https://github.com/wwang1110/kokoro_batch"&gt;https://github.com/wwang1110/kokoro_batch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;⚡ Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt;: Process multiple texts simultaneously instead of one-by-one&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Processes 30 audio clips under 2 seconds on RTX4090&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time capable&lt;/strong&gt;: Generates 276 seconds of audio in under 2 seconds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy to use&lt;/strong&gt;: Simple Python API with smart text chunking&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;🔧 Technical highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on PyTorch with CUDA acceleration&lt;/li&gt; &lt;li&gt;Integrated grapheme-to-phoneme conversion&lt;/li&gt; &lt;li&gt;Smart text splitting for optimal batch sizes&lt;/li&gt; &lt;li&gt;FP16 support for faster inference&lt;/li&gt; &lt;li&gt;Based on the open-source Kokoro-82M model&lt;/li&gt; &lt;li&gt;The model output is 24KHZ PCM16 format&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For simplicity, the sample/demo code currently includes support for American English, British English, and Spanish. However, it can be easily extended to additional languages, just like the original Kokoro 82M model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asuran2000"&gt; /u/asuran2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npn810/kokoro_batch_tts_enabling_batch_processing_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npn810/kokoro_batch_tts_enabling_batch_processing_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npn810/kokoro_batch_tts_enabling_batch_processing_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T20:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1np5ey8</id>
    <title>MiniModel-200M-Base</title>
    <updated>2025-09-24T06:58:12+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"&gt; &lt;img alt="MiniModel-200M-Base" src="https://preview.redd.it/clbzeq0i82rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=056ef5c77a2001c2a6d5509cbdcb9173566b1c52" title="MiniModel-200M-Base" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most “efficient” small models still need days of training or massive clusters. &lt;strong&gt;MiniModel-200M-Base&lt;/strong&gt; was trained &lt;strong&gt;from scratch on just 10B tokens&lt;/strong&gt; in &lt;strong&gt;110k steps (≈1 day)&lt;/strong&gt; on a &lt;strong&gt;single RTX 5090&lt;/strong&gt;, using &lt;strong&gt;no gradient accumulation&lt;/strong&gt; yet still achieving a &lt;strong&gt;batch size of 64 x 2048 tokens&lt;/strong&gt; and with peak memory &lt;strong&gt;&amp;lt;30 GB VRAM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Key efficiency techniques:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Adaptive Muon optimizer&lt;/strong&gt;: 2.1× more data-efficient than AdamW&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Float8 pretraining&lt;/strong&gt;: ~30% less VRAM, ~20% higher throughput (attention kept in bf16)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ReLU² activation&lt;/strong&gt; (from Google’s &lt;em&gt;Primer&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bin-packing&lt;/strong&gt;: reduced padding from &amp;gt;70% → &amp;lt;5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full attention + QK-norm without scalars&lt;/strong&gt; for stability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite its size, it shows surprising competence:&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Fibonacci (temp=0.0001)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def fibonacci(n: int): if n &amp;lt; 2: return n return fibonacci(n - 1) + fibonacci(n - 2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;✅ &lt;strong&gt;Digits of π (temp=0.0001)&lt;/strong&gt;&lt;br /&gt; Recites &lt;strong&gt;3.14159265358979323846…&lt;/strong&gt; correctly — the first 20+ digits.&lt;/p&gt; &lt;p&gt;It’s &lt;strong&gt;Apache 2.0 licensed&lt;/strong&gt;, with public config, tokenizer, and safetensors weights. No instruct-tuning yet, as this is pure pretraining on educational data (Ultra-FineWeb, Python tutorials, math).&lt;/p&gt; &lt;p&gt;Not perfect (it thinks Earth’s radius is 375,000 miles), but for a 200M model trained in a day it’s a solid base for experimentation, distillation, or local prototyping.&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://huggingface.co/xTimeCrystal/MiniModel-200M-Base"&gt;Hugging Face: MiniModel-200M-Base&lt;/a&gt;&lt;br /&gt; 🧠 200M | 🌐 en/zh/Python | 📜 Apache 2.0&lt;/p&gt; &lt;p&gt;Any feedback is welcome, especially on replicating the training setup or improving data efficiency!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/clbzeq0i82rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T06:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1npup0s</id>
    <title>Any good YouTube creators with low pace content?</title>
    <updated>2025-09-25T02:03:38+00:00</updated>
    <author>
      <name>/u/daantesao</name>
      <uri>https://old.reddit.com/user/daantesao</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to study more about llms and prompt engineering but almost every YouTuber got this fast paced YouTube style with a lot of sound FX and click bait titles. I just wish I could find someone that just go straight to explanation without a overstimulated time of editing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daantesao"&gt; /u/daantesao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npup0s/any_good_youtube_creators_with_low_pace_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npup0s/any_good_youtube_creators_with_low_pace_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npup0s/any_good_youtube_creators_with_low_pace_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T02:03:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1npmycu</id>
    <title>Do you think Qwen3 VL will get a release for other models too?</title>
    <updated>2025-09-24T20:22:58+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like for the 80B-Next or the 32B, 14B, 8B, 4B and other variants? I know, we've been blessed and even if there are no such releases all is well, but still... would be nice =]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npmycu/do_you_think_qwen3_vl_will_get_a_release_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npmycu/do_you_think_qwen3_vl_will_get_a_release_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npmycu/do_you_think_qwen3_vl_will_get_a_release_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T20:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1npn4ir</id>
    <title>Is a 5090 the best for most people?</title>
    <updated>2025-09-24T20:29:38+00:00</updated>
    <author>
      <name>/u/P3rpetuallyC0nfused</name>
      <uri>https://old.reddit.com/user/P3rpetuallyC0nfused</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, curious to have my mind changed. I've been researching for some time now and with the prices becoming reasonable on 5090s, I can't seem to justify getting anything else.&lt;/p&gt; &lt;p&gt;Reasons for:&lt;br /&gt; - 32GB vram seems to be enough for a single-user doing inference pretty fast on big enough models&lt;br /&gt; - mature nvidia software&lt;br /&gt; - as mentioned, decent price (now)&lt;/p&gt; &lt;p&gt;Alternatives I've explored:&lt;/p&gt; &lt;p&gt;- AI Max 395: big memory at a lower price, but speed will suffer as the mem bandwidth is lower and I don't think majority of use cases need 96GB vram. rocm still young.&lt;br /&gt; - Apple Silicon: insanely expensive for the same amount of vram and it's still slower. more limited software&lt;br /&gt; - Radeon Pro W9700 or W7900(?): still expensive, more vram but slightly slower, can't get them anywhere&lt;br /&gt; - RTX 6000 Blackwell: painfully expensive for team green big vram&lt;br /&gt; - multiple 4090s/3090s: performance hit from offloading layers between different memory, need more power, fancier config etc&lt;br /&gt; - nvidia frankenchips from China: hard to get, don't trust em&lt;br /&gt; - Huawei: I'm sorry, I don't trust em&lt;/p&gt; &lt;p&gt;Curious to hear what everyone's thoughts are. My use case is single user inference for coding / life at a speed that doesn't cause me to look at my phone and not a crazy tight budget but not 10k... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/P3rpetuallyC0nfused"&gt; /u/P3rpetuallyC0nfused &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npn4ir/is_a_5090_the_best_for_most_people/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npn4ir/is_a_5090_the_best_for_most_people/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npn4ir/is_a_5090_the_best_for_most_people/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T20:29:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1npa8yt</id>
    <title>Be cautious of GPU modification posts. And do not send anyone money. DYI if you can.</title>
    <updated>2025-09-24T12:02:25+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a precautionary post and a reminder that this is Reddit. People can make a good looking legit website and scam you into sending them an advance payment for your 48GB 4090 or 20 GB 3080 but be cautious and stay safe. &lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npa8yt/be_cautious_of_gpu_modification_posts_and_do_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npa8yt/be_cautious_of_gpu_modification_posts_and_do_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npa8yt/be_cautious_of_gpu_modification_posts_and_do_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T12:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nprim2</id>
    <title>Introducing LFM2-2.6B: Redefining Efficiency in Language Models | Liquid AI</title>
    <updated>2025-09-24T23:33:04+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nprim2/introducing_lfm226b_redefining_efficiency_in/"&gt; &lt;img alt="Introducing LFM2-2.6B: Redefining Efficiency in Language Models | Liquid AI" src="https://external-preview.redd.it/XaAHZAPyF0SslGVTFOxCaM7avykHViwT0kgWp2bq5tc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf5bf01f657f0db80bcc7749d573df9b82640adc" title="Introducing LFM2-2.6B: Redefining Efficiency in Language Models | Liquid AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.liquid.ai/blog/introducing-lfm2-2-6b-redefining-efficiency-in-language-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nprim2/introducing_lfm226b_redefining_efficiency_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nprim2/introducing_lfm226b_redefining_efficiency_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T23:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1npfnvw</id>
    <title>Chinese modified 3080 20GB performance..</title>
    <updated>2025-09-24T15:45:55+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npfnvw/chinese_modified_3080_20gb_performance/"&gt; &lt;img alt="Chinese modified 3080 20GB performance.." src="https://a.thumbs.redditmedia.com/GXzMwuLrsR20Cc_j5wp_lZeVn_BM6JL-8wxZrG9D7z4.jpg" title="Chinese modified 3080 20GB performance.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm quite surprised to see it beat 3080TI &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1npfnvw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npfnvw/chinese_modified_3080_20gb_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npfnvw/chinese_modified_3080_20gb_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T15:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1npq84t</id>
    <title>Are 24-50Bs finally caught up to 70Bs now?</title>
    <updated>2025-09-24T22:35:24+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep seeing everyone say that 70Bs are SOOOO amazing and perfect and beautiful and that if you can’t run 70Bs you’re a loser (not really, but you get me). I just got a 3090 and now I can run 50Bs comfortably, but 70Bs are unbearably slow for me and can’t possibly be worth it unless they have godlike writing, let alone 120Bs.&lt;/p&gt; &lt;p&gt;So I’m asking am I fine to just stick with 24-50Bs or so? I keep wondering what I’m missing and then people come out with all kinds of models for 70b and I’m like :/&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npq84t/are_2450bs_finally_caught_up_to_70bs_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npq84t/are_2450bs_finally_caught_up_to_70bs_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npq84t/are_2450bs_finally_caught_up_to_70bs_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T22:35:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1np9rav</id>
    <title>My second modified 3080 20GB from China , for local Ai inference , video and image generation..</title>
    <updated>2025-09-24T11:36:56+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np9rav/my_second_modified_3080_20gb_from_china_for_local/"&gt; &lt;img alt="My second modified 3080 20GB from China , for local Ai inference , video and image generation.." src="https://b.thumbs.redditmedia.com/PgWSwkWNNiXl6B2p4tLoG8a-8KyV8YynoMLgVVg97bI.jpg" title="My second modified 3080 20GB from China , for local Ai inference , video and image generation.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got this triple fan version instead of server - blower style card because of fan noise. It's also slightly bigger in size than the blower card . Teps are quite good and manageable , staying below 75°C , even when stress testing @ 300W . And it's a 2½ slot card ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1np9rav"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np9rav/my_second_modified_3080_20gb_from_china_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np9rav/my_second_modified_3080_20gb_from_china_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T11:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1np5te1</id>
    <title>Oh my God, what a monster is this?</title>
    <updated>2025-09-24T07:24:01+00:00</updated>
    <author>
      <name>/u/NearbyBig3383</name>
      <uri>https://old.reddit.com/user/NearbyBig3383</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"&gt; &lt;img alt="Oh my God, what a monster is this?" src="https://preview.redd.it/1pxmwf50e2rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9d1eb3d320d1305fe702c08f9c69cd841db5fd1" title="Oh my God, what a monster is this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NearbyBig3383"&gt; /u/NearbyBig3383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1pxmwf50e2rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T07:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nph3az</id>
    <title>New Agent benchmark from Meta Super Intelligence Lab and Hugging Face</title>
    <updated>2025-09-24T16:39:07+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nph3az/new_agent_benchmark_from_meta_super_intelligence/"&gt; &lt;img alt="New Agent benchmark from Meta Super Intelligence Lab and Hugging Face" src="https://preview.redd.it/fjardl7x45rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b3257ce7506ae29e607d203aa3899a1ddf43031" title="New Agent benchmark from Meta Super Intelligence Lab and Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/blog/gaia2"&gt;https://huggingface.co/blog/gaia2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjardl7x45rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nph3az/new_agent_benchmark_from_meta_super_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nph3az/new_agent_benchmark_from_meta_super_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T16:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1npp8xi</id>
    <title>New model from Meta FAIR: Code World Model (CWM) 32B - 65.8 % on SWE-bench Verified</title>
    <updated>2025-09-24T21:54:22+00:00</updated>
    <author>
      <name>/u/notrdm</name>
      <uri>https://old.reddit.com/user/notrdm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi- task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131 k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8 % on SWE-bench Verified (with test-time scaling), 68.6 % on LiveCodeBench, 96.6 % on Math-500, and 76.0 % on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notrdm"&gt; /u/notrdm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T21:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1npgjpv</id>
    <title>China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI</title>
    <updated>2025-09-24T16:18:29+00:00</updated>
    <author>
      <name>/u/Battle-Chimp</name>
      <uri>https://old.reddit.com/user/Battle-Chimp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"&gt; &lt;img alt="China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI" src="https://external-preview.redd.it/WMoFSM_ESNhbjEkciy-Rd0SY0RIYgPT1B4RqqWSMj-g.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e6a5a5393e6e1773fbc67615613b404ec6ab6f7" title="China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Battle-Chimp"&gt; /u/Battle-Chimp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T16:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1npo93e</id>
    <title>I built a tiny fully local AI agent for a Raspberry Pi</title>
    <updated>2025-09-24T21:14:05+00:00</updated>
    <author>
      <name>/u/syxa</name>
      <uri>https://old.reddit.com/user/syxa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"&gt; &lt;img alt="I built a tiny fully local AI agent for a Raspberry Pi" src="https://external-preview.redd.it/eWJobjllM3hoNnJmMX3IlXPTfCA6UKqCZh3d_lEw5N2PzMnkp8dcFp83zF5t.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58e4d2c8a5cd47a44d3d6abb40f3a02cb6dff369" title="I built a tiny fully local AI agent for a Raspberry Pi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Over the past few months, I’ve been working on a tiny agent that can run entirely on a Raspberry Pi 5. It's capable of executing tools and runs some of the smallest good models I could find (specifically Qwen3:1.7b and Gemma3:1b).&lt;/p&gt; &lt;p&gt;From wake-word detection, to transcription, to the actual LLM inference, everything happens on the Pi 5 itself. It was definitely a challenge given the hardware constraints, but I learned a lot along the way.&lt;/p&gt; &lt;p&gt;I've detailed everything in this blog post if you're curious: &lt;a href="https://blog.simone.computer/an-agent-desktoy"&gt;https://blog.simone.computer/an-agent-desktoy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/syxanash/maxheadbox"&gt;https://github.com/syxanash/maxheadbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/syxa"&gt; /u/syxa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xslfjc3xh6rf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T21:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
