<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-20T18:43:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p27ahd</id>
    <title>llama.cpp crashing with OOM error at &lt;30,000 context despite -c 65000 and space in VRAM</title>
    <updated>2025-11-20T16:15:34+00:00</updated>
    <author>
      <name>/u/thejacer</name>
      <uri>https://old.reddit.com/user/thejacer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p27ahd/llamacpp_crashing_with_oom_error_at_30000_context/"&gt; &lt;img alt="llama.cpp crashing with OOM error at &amp;lt;30,000 context despite -c 65000 and space in VRAM" src="https://b.thumbs.redditmedia.com/ttqA6n8oA-VVAMu6GZAUf1bHygD9IRD1Jd-OuL_b8vs.jpg" title="llama.cpp crashing with OOM error at &amp;lt;30,000 context despite -c 65000 and space in VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't get it figured out...I thought that setting -c allocated the VRAM ahead of time. When I try to launch with -c 128000 it OOM before the launch is completed. Although having pasted these two images I find it weird that it seems to frequently make it to progress &amp;gt; .99 before crashing...images included&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xnyemfd9sf2g1.png?width=1029&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6056415a4b98a644c51a6fef42b4d1058097e3d7"&gt;https://preview.redd.it/xnyemfd9sf2g1.png?width=1029&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6056415a4b98a644c51a6fef42b4d1058097e3d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dbhz1bycsf2g1.png?width=984&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18cc1ed02d7381e343d739150dffef35e6b5ffa9"&gt;https://preview.redd.it/dbhz1bycsf2g1.png?width=984&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18cc1ed02d7381e343d739150dffef35e6b5ffa9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;launching with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m /home/thejacer/DS08002/cogito-v2-preview-llama-109B-MoE-IQ4_XS-00001-of-00002.gguf --mmproj /home/thejacer/DS08002/mmproj-BF16.gguf -ngl 99 -fa on --no-mmap --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;-c 65000 -ctv q4_0 -ctk q4_0 --mlock --api-key #####&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thejacer"&gt; /u/thejacer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p27ahd/llamacpp_crashing_with_oom_error_at_30000_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p27ahd/llamacpp_crashing_with_oom_error_at_30000_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p27ahd/llamacpp_crashing_with_oom_error_at_30000_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2540n</id>
    <title>1x 6000 pro 96gb or 3x 5090 32gb?</title>
    <updated>2025-11-20T14:50:51+00:00</updated>
    <author>
      <name>/u/Wide_Cover_8197</name>
      <uri>https://old.reddit.com/user/Wide_Cover_8197</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking about making a local AI rig. What do you think about 1x 6000 pro 96gb vs 3x 5090 32gb?&lt;/p&gt; &lt;p&gt;Want to load kimi 2 thinking. &lt;/p&gt; &lt;p&gt;Also contemplating EPYC vs Threadripper. &lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide_Cover_8197"&gt; /u/Wide_Cover_8197 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2540n/1x_6000_pro_96gb_or_3x_5090_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2540n/1x_6000_pro_96gb_or_3x_5090_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2540n/1x_6000_pro_96gb_or_3x_5090_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p26wf8</id>
    <title>Which Model is best for translation?</title>
    <updated>2025-11-20T16:01:15+00:00</updated>
    <author>
      <name>/u/Bulky-College7306</name>
      <uri>https://old.reddit.com/user/Bulky-College7306</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;Anyone used different models for translation purposes, which one did you find is precise in actually translating whole pages / books / lengthy texts &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bulky-College7306"&gt; /u/Bulky-College7306 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p26wf8/which_model_is_best_for_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p26wf8/which_model_is_best_for_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p26wf8/which_model_is_best_for_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1wjdg</id>
    <title>RAG Paper 25.11.19</title>
    <updated>2025-11-20T06:57:44+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15435v1"&gt;HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15370v1"&gt;The Empowerment of Science of Science by Large Language Models: New Tools and Methods&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15355v1"&gt;HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15141v1"&gt;ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15074v1"&gt;Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15005v1"&gt;Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1wjdg/rag_paper_251119/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1wjdg/rag_paper_251119/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1wjdg/rag_paper_251119/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T06:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ai72</id>
    <title>Kimi 2 Thinking Case Study: AI or Not Stayed Accurate, ZeroGPT Failed Hard</title>
    <updated>2025-11-20T18:16:14+00:00</updated>
    <author>
      <name>/u/Winter_Wasabi9193</name>
      <uri>https://old.reddit.com/user/Winter_Wasabi9193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ran a case study on Monoshoot’s new &lt;strong&gt;Kimi 2 Thinking&lt;/strong&gt; model and compared how two detection tools handled it: &lt;strong&gt;AI or Not&lt;/strong&gt; and &lt;strong&gt;ZeroGPT&lt;/strong&gt;. AI or Not was surprisingly solid with its classifications, but ZeroGPT completely fell apart—tons of false flags, inconsistent results, and readings that didn’t match the model’s actual behavior at all.&lt;/p&gt; &lt;p&gt;I know this sub is focused on Llama and Meta’s ecosystem, but since a lot of us test multiple models and rely on detection tools for benchmarking, I figured it was worth sharing. Based on this run, ZeroGPT feels totally unreliable for evaluating any modern model, Llama-based or not.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Winter_Wasabi9193"&gt; /u/Winter_Wasabi9193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.dropbox.com/scl/fi/o0oll5wallvywykar7xcs/Kimi-2-Thinking-Case-Study-Sheet1.pdf?rlkey=70w7jbnwr9cwaa9pkbbwn8fm2&amp;amp;st=8smbvkd1&amp;amp;dl=0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ai72/kimi_2_thinking_case_study_ai_or_not_stayed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ai72/kimi_2_thinking_case_study_ai_or_not_stayed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T18:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1df5y</id>
    <title>SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs</title>
    <updated>2025-11-19T17:10:27+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt; &lt;img alt="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" src="https://external-preview.redd.it/4Uyf8OlIkFBtIXR-wKdBIOZqZgS3NkQSX04eUeTDY7w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88abe479f5e76ceeae47c92c033ef5091fe19a40" title="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/facebook/sam3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T17:10:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p27qqx</id>
    <title>How Should I Use My $150 Thinking Machine Credit?</title>
    <updated>2025-11-20T16:32:24+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got &lt;strong&gt;$150 in compute credits on Thinking Machine&lt;/strong&gt;, and I’m trying to figure out the best way to use it for &lt;strong&gt;fine-tuning a model on a specific domain or task&lt;/strong&gt;. I’m planning to pick one strong idea, generate or collect some synthetic data for it, fine-tune a model, and eventually share the results on &lt;strong&gt;Hugging Face&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Before I choose a direction, I’d really appreciate your input.&lt;/p&gt; &lt;h1&gt;What I’m Looking For:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Which domain or task should I fine-tune a model on?&lt;/strong&gt; (Something practical, unique, or impactful.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Any creative or high-value project ideas?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If you know how Thinking Machine charges for fine-tuning&lt;/strong&gt;, please share. I want to understand whether they bill based on: &lt;ul&gt; &lt;li&gt;GPU hourly rates&lt;/li&gt; &lt;li&gt;Model size&lt;/li&gt; &lt;li&gt;Training duration&lt;/li&gt; &lt;li&gt;Token count&lt;/li&gt; &lt;li&gt;Or any other hidden costs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Plan:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Collect the best ideas from the comments.&lt;/li&gt; &lt;li&gt;Choose the idea that gets the most votes, the strongest support, or the highest interest.&lt;/li&gt; &lt;li&gt;Create or generate the synthetic dataset needed for that task.&lt;/li&gt; &lt;li&gt;Fine-tune the model using the $150 credit.&lt;/li&gt; &lt;li&gt;Publish the model and results on Hugging Face, including the full workflow.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you have a solid idea, something you think could be useful for others, or knowledge about how their pricing works, I’d really appreciate your help.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p27qqx/how_should_i_use_my_150_thinking_machine_credit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p27qqx/how_should_i_use_my_150_thinking_machine_credit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p27qqx/how_should_i_use_my_150_thinking_machine_credit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:32:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p22fqf</id>
    <title>r/opensourceAPIs – New sub for open-source inference APIs and every other OSS API alternative</title>
    <updated>2025-11-20T12:55:27+00:00</updated>
    <author>
      <name>/u/sandeep_k_n</name>
      <uri>https://old.reddit.com/user/sandeep_k_n</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This place is growing fast for local models, but a lot of us also need solid open-source drop-ins for the rest of the API stack.&lt;/p&gt; &lt;p&gt;Just launched &lt;a href="/r/opensourceAPIs"&gt;r/opensourceAPIs&lt;/a&gt; – dedicated to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI-compatible servers (Ollama, vLLM, llama.cpp, TabbyAPI, etc.)&lt;/li&gt; &lt;li&gt;Any other open-source/self-hostable API (payments, email, maps, auth, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’re running your own OpenAI-compatible endpoint or want recommendations for other self-hosted APIs, come hang out and contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandeep_k_n"&gt; /u/sandeep_k_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p22fqf/ropensourceapis_new_sub_for_opensource_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p22fqf/ropensourceapis_new_sub_for_opensource_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p22fqf/ropensourceapis_new_sub_for_opensource_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T12:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1w5ri</id>
    <title>"Seahorse emoji" test on GPT-5.1 vs Qwen3-VL 30B-A3B (both no thinking). An interesting comparison.</title>
    <updated>2025-11-20T06:34:25+00:00</updated>
    <author>
      <name>/u/airbus_a360_when</name>
      <uri>https://old.reddit.com/user/airbus_a360_when</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1w5ri/seahorse_emoji_test_on_gpt51_vs_qwen3vl_30ba3b/"&gt; &lt;img alt="&amp;quot;Seahorse emoji&amp;quot; test on GPT-5.1 vs Qwen3-VL 30B-A3B (both no thinking). An interesting comparison." src="https://b.thumbs.redditmedia.com/0BEZNhu0icnC0-Gp07QD-90AB9L7NHeYmjgHHAreA1o.jpg" title="&amp;quot;Seahorse emoji&amp;quot; test on GPT-5.1 vs Qwen3-VL 30B-A3B (both no thinking). An interesting comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airbus_a360_when"&gt; /u/airbus_a360_when &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p1w5ri"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1w5ri/seahorse_emoji_test_on_gpt51_vs_qwen3vl_30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1w5ri/seahorse_emoji_test_on_gpt51_vs_qwen3vl_30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T06:34:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p25cd6</id>
    <title>We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!</title>
    <updated>2025-11-20T15:00:31+00:00</updated>
    <author>
      <name>/u/kruszczynski</name>
      <uri>https://old.reddit.com/user/kruszczynski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p25cd6/we_trained_an_slm_assistants_for_assistance_with/"&gt; &lt;img alt="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" src="https://preview.redd.it/qzrbnxaoef2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77311a4975dde9c7c943cd5078ce25b75de8454a" title="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;distil-commit-bot TS&lt;/h1&gt; &lt;p&gt;We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run &lt;em&gt;locally&lt;/em&gt;!&lt;/p&gt; &lt;p&gt;Check it out at: &lt;a href="https://github.com/distil-labs/distil-commit-bot"&gt;https://github.com/distil-labs/distil-commit-bot&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;First, install &lt;a href="https://ollama.com"&gt;Ollama&lt;/a&gt;, following the instructions on their website.&lt;/p&gt; &lt;p&gt;Then set up the virtual environment: &lt;code&gt; python -m venv .venv . .venv/bin/activate pip install huggingface_hub openai watchdog &lt;/code&gt;&lt;/p&gt; &lt;p&gt;or using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;: &lt;code&gt; uv sync &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The model is hosted on huggingface: - &lt;a href="https://huggingface.co/distil-labs/distil-commit-bot-ts-Qwen3-0.6B"&gt;distil-labs/distil-commit-bot-ts-Qwen3-0.6B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally, download the models from huggingface and build them locally: ``` hf download distil-labs/distil-commit-bot-ts-Qwen3-0.6B --local-dir distil-model&lt;/p&gt; &lt;p&gt;cd distil-model ollama create distil-commit-bot-ts-Qwen3-0.6B -f Modelfile ```&lt;/p&gt; &lt;h3&gt;Run the assistant&lt;/h3&gt; &lt;p&gt;The commit bot with diff the git repository provided via &lt;code&gt;--repository&lt;/code&gt; option and suggest a commit message. Use the &lt;code&gt;--watch&lt;/code&gt; option to re-run the assistant whenever the repository changes.&lt;/p&gt; &lt;p&gt;``` python bot.py --repository &amp;lt;absolute_or_relative_git_repository_path&amp;gt;&lt;/p&gt; &lt;h1&gt;or&lt;/h1&gt; &lt;p&gt;uv run bot.py --repository &amp;lt;absolute_or_relative_git_repository_path&amp;gt;&lt;/p&gt; &lt;h1&gt;Watch for file changes in the repository path:&lt;/h1&gt; &lt;p&gt;python bot.py --repository &amp;lt;absolute_or_relative_git_repository_path&amp;gt; --watch&lt;/p&gt; &lt;h1&gt;or&lt;/h1&gt; &lt;p&gt;uv run bot.py --repository &amp;lt;absolute_or_relative_git_repository_path&amp;gt; --watch ```&lt;/p&gt; &lt;h3&gt;Training &amp;amp; Evaluation&lt;/h3&gt; &lt;p&gt;The tuned models were trained using knowledge distillation, leveraging the teacher model GPT-OSS-120B. The data+config+script used for finetuning can be found in &lt;a href="/data"&gt;data&lt;/a&gt;. We used 20 typescript git diff examples (created using &lt;a href="https://www.distillabs.ai/blog/vibe-tuning-the-art-of-fine-tuning-small-language-models-with-a-prompt"&gt;distillabs' vibe tuning&lt;/a&gt;) as seed data and supplemented them with 10,000 synthetic examples across various typescript use cases (frontend, backend, react etc.).&lt;/p&gt; &lt;p&gt;We compare the teacher model and the student model on 10 held-out test examples using LLM-as-a-judge evaluation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Size&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-OSS (thinking)&lt;/td&gt; &lt;td&gt;120B&lt;/td&gt; &lt;td&gt;1.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 0.6B (tuned)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;0.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 0.6B (base)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;0.60&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kruszczynski"&gt; /u/kruszczynski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qzrbnxaoef2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p25cd6/we_trained_an_slm_assistants_for_assistance_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p25cd6/we_trained_an_slm_assistants_for_assistance_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T15:00:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1iequ</id>
    <title>New multilingual + instruction-following reranker from ZeroEntropy!</title>
    <updated>2025-11-19T20:12:06+00:00</updated>
    <author>
      <name>/u/ghita__</name>
      <uri>https://old.reddit.com/user/ghita__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;zerank-2&lt;/strong&gt; is our new state-of-the-art reranker, optimized for production environments where existing models typically break. It is designed to solve the &amp;quot;modality gap&amp;quot; in multilingual retrieval, handle complex instruction-following, and provide calibrated confidence scores you can actually trust.&lt;/p&gt; &lt;p&gt;It offers significantly more robustness than leading proprietary models (like Cohere Rerank 3.5 or Voyage rerank 2.5) while being &lt;strong&gt;50% cheaper&lt;/strong&gt; ($0.025/1M tokens).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Native Instruction-Following:&lt;/strong&gt; Capable of following precise instructions, understanding domain acronyms, and contextualizing results based on user prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;True Multilingual Parity:&lt;/strong&gt; Trained on 100+ languages with little performance drop on non-English queries and native handling of code-switching (e.g., Spanglish/Hinglish).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Calibrated Confidence Scores:&lt;/strong&gt; Solves the &amp;quot;arbitrary score&amp;quot; problem. A score of 0.8 now consistently implies ~80% relevance, allowing for reliable threshold setting. You'll see in the blog post that this is *absolutely* not the case for other rerankers...&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQL-Style &amp;amp; Aggregation Robustness:&lt;/strong&gt; Correctly handles aggregation queries like &amp;quot;Top 10 objections of customer X?&amp;quot; or SQL-Style ones like &amp;quot;Sort by fastest latency,&amp;quot; where other models fail to order quantitative values.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-&amp;gt; Check out the model card: &lt;a href="https://huggingface.co/zeroentropy/zerank-2"&gt;https://huggingface.co/zeroentropy/zerank-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-&amp;gt; And the full (cool and interactive) benchmark post: &lt;a href="https://www.zeroentropy.dev/articles/zerank-2-advanced-instruction-following-multilingual-reranker"&gt;https://www.zeroentropy.dev/articles/zerank-2-advanced-instruction-following-multilingual-reranker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's available to everyone now via the ZeroEntropy API!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghita__"&gt; /u/ghita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T20:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1h9fz</id>
    <title>The C++ rewrite of Lemonade is released and ready!</title>
    <updated>2025-11-19T19:29:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"&gt; &lt;img alt="The C++ rewrite of Lemonade is released and ready!" src="https://preview.redd.it/jw4z8mo1m92g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c3cfbaf15cfb1f8fc28608e7f00a78cffc04974" title="The C++ rewrite of Lemonade is released and ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple weeks ago I posted that a C++ rewrite of Lemonade was in open beta. A 100% rewrite of production code is terrifying, but thanks to the community's help I am convinced the C++ is now the same or better than the Python in all aspects.&lt;/p&gt; &lt;p&gt;Huge shoutout and thanks to Vladamir, Tetramatrix, primal, imac, GDogg, kklesatschke, sofiageo, superm1, korgano, whoisjohngalt83, isugimpy, mitrokun, and everyone else who pitched in to make this a reality!&lt;/p&gt; &lt;h2&gt;What's Next&lt;/h2&gt; &lt;p&gt;We also got a suggestion to provide a project roadmap on the GitHub README. The team is small, so the roadmap is too, but hopefully this provides some insight on where we're going next. Copied here for convenience:&lt;/p&gt; &lt;h3&gt;Under development&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Electron desktop app (replacing the web ui)&lt;/li&gt; &lt;li&gt;Multiple models loaded at the same time&lt;/li&gt; &lt;li&gt;FastFlowLM speech-to-text on NPU&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Under consideration&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;General speech-to-text support (whisper.cpp)&lt;/li&gt; &lt;li&gt;vLLM integration&lt;/li&gt; &lt;li&gt;Handheld devices: Ryzen AI Z2 Extreme APUs&lt;/li&gt; &lt;li&gt;ROCm support for Ryzen AI 360-375 (Strix) APUs&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Background&lt;/h2&gt; &lt;p&gt;Lemonade is an open-source alternative to local LLM tools like Ollama. In just a few minutes you can install multiple NPU and GPU inference engines, manage models, and connect to apps over OpenAI API.&lt;/p&gt; &lt;p&gt;If you like the project and direction, please drop us a star on &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;the Lemonade GitHub&lt;/a&gt; and come chat on the &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;Discord&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;AMD NPU Linux Support&lt;/h2&gt; &lt;p&gt;I communicated the feedback from the last post (C++ beta announcement) to AMD leadership. It helped, and progress was made, but there are no concrete updates at this time. I will also forward any NPU+Linux feedback from this post!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw4z8mo1m92g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:29:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p274rk</id>
    <title>Your local LLM agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-20T16:09:43+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt;. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Agent runs task → reflects on what worked/failed → curates strategies into playbook → uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Paper shows +17.1pp accuracy improvement vs base LLM (≈+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with local or API models&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% → 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Local Model Starter Templates (Ollama, LM Studio, LiteLLM): &lt;a href="https://github.com/kayba-ai/agentic-context-engine/tree/main/examples"&gt;https://github.com/kayba-ai/agentic-context-engine/tree/main/examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with their local setups! Especially curious how it performs with different models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;⭐ the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1xy7t</id>
    <title>Voice controlled AI robot powered by Ollama and Llama 3.2</title>
    <updated>2025-11-20T08:29:27+00:00</updated>
    <author>
      <name>/u/Vbox112</name>
      <uri>https://old.reddit.com/user/Vbox112</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"&gt; &lt;img alt="Voice controlled AI robot powered by Ollama and Llama 3.2" src="https://preview.redd.it/b9uurfbdhd2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56fe62e49db519c4731c508abe5b11eaa6441690" title="Voice controlled AI robot powered by Ollama and Llama 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a voice controlled AI robot that runs Llama 3.2 locally via Ollama.&lt;/p&gt; &lt;p&gt;Hardware setup:&lt;/p&gt; &lt;p&gt;ESP32 microcontroller with OLED display and microphone input.&lt;/p&gt; &lt;p&gt;Software setup:&lt;/p&gt; &lt;p&gt;Ollama running Llama 3.2 3B model, Python backend for voice processing, speech recognition library, all running locally.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;Three operating modes, voice control for apps, network tools, offline operation, animated expressions on OLED, clap detection.&lt;/p&gt; &lt;p&gt;Performance:&lt;/p&gt; &lt;p&gt;Response times under 100ms, AI processing 2-3 seconds, 2GB RAM usage, runs on consumer PC.&lt;/p&gt; &lt;p&gt;Video demonstration: &lt;a href="https://youtu.be/5Z6EGBW9xkk?si=s4az9rukOWU4oFKl"&gt;https://youtu.be/5Z6EGBW9xkk?si=s4az9rukOWU4oFKl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions welcome about the setup.&lt;/p&gt; &lt;p&gt;Planning to release code soon.&lt;/p&gt; &lt;p&gt;What would you add to a local voice assistant?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vbox112"&gt; /u/Vbox112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b9uurfbdhd2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T08:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1zv7p</id>
    <title>When will the free ride be over?</title>
    <updated>2025-11-20T10:34:19+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pretty cheap, so only paid for a few credits for OpenAI, DeepSeek and the $3 GLM code subscription. Long crunching workflows are done on local GPUs.&lt;/p&gt; &lt;p&gt;Yesterday, I hit the 5 hour limit on GLM for the first time. No problem, I switch to Gemini CLI. If that runs out, I switch to Qwen Code. &lt;/p&gt; &lt;p&gt;I have free tier on OpenAI and Google AI Studio and if I run out there, I drop back to my locally hosted AI.&lt;/p&gt; &lt;p&gt;Do you think free tiers will gradually get scaled back or eliminated? Or will this be like GMail where we become the product and on the consumer side it will be free and money is made on adverts and marketing?&lt;/p&gt; &lt;p&gt;Of course on the commercial side and code side, the value is enough that people will pay for code subscriptions and tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T10:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p29jwc</id>
    <title>Leak: Qwen3-15B-A2B-Base</title>
    <updated>2025-11-20T17:40:12+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmolested and Unreleased Base Qwen3 MoE:&lt;br /&gt; &lt;a href="https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base"&gt;https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T17:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p244ch</id>
    <title>VibeThinker-1.5B just solved a problem that Gemini, DeepSeek and OpenAI failed to solve</title>
    <updated>2025-11-20T14:09:48+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: I got home and ran it on my 3090 with the following results (at Q5K_M). It ran out of tokens after 5 minutes, but throwing a question at it for 5 minutes could be worth it if it comes up with interesting approaches to be investigated:&lt;/p&gt; &lt;p&gt;&lt;code&gt; prompt eval time = 38.65 ms / 235 tokens ( 0.16 ms per token, 6080.21 tokens per second) eval time = 318882.23 ms / 39957 tokens ( 7.98 ms per token, 125.30 tokens per second) total time = 318920.88 ms / 40192 tokens &lt;/code&gt;&lt;/p&gt; &lt;p&gt;When I saw VibeThinker-1.5B, I was sceptical, a 1.5B trying to compete with models a hundred times bigger?&lt;/p&gt; &lt;p&gt;But I had some spare time and so I downloaded a GGUF at Q4K_M and set it going.&lt;/p&gt; &lt;p&gt;I'm not at my usual PC so, I've been running it on CPU. I watched the thinking trace. It was very inefficient in reasoning tokens, it took a lot of tokens before it even started to understand the question. At this point, I was thinking &amp;quot;This is junk.&amp;quot;. But it very slowly started to converge on understanding the question (which is a math/combinatorics question).&lt;/p&gt; &lt;p&gt;Then it started to come up with ideas on solving it. Half an hour later, it spat out what looked like could be a possible answer. I just spent the last 30 minutes verifying the answer using Gemini Pro and OpenAI and writing a program to verify correctness. It got it right!&lt;/p&gt; &lt;p&gt;I don't know if it is a fluke, or I got lucky, but I tried to tackle this question multiple times with various models both open and closed and none of them got the answer. I'm amazed that this 1.5B model quantized to Q4 and running on CPU managed to do it.&lt;/p&gt; &lt;p&gt;The model is still churning, going through alternative ideas. It's been going for 1.5 hours now and has thrown out 26k tokens. I've limited it to 40k tokens so will see what it comes up with at the end of it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;https://huggingface.co/WeiboAI/VibeThinker-1.5B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:09:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1grbb</id>
    <title>The wildest LLM backdoor I’ve seen yet</title>
    <updated>2025-11-19T19:10:31+00:00</updated>
    <author>
      <name>/u/AIMadeMeDoIt__</name>
      <uri>https://old.reddit.com/user/AIMadeMeDoIt__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago &lt;a href="https://www.anthropic.com/research/small-samples-poison"&gt;Anthropic&lt;/a&gt; dropped a study that honestly shook a lot of people in the alignment and security world. They showed that you don’t need to poison a percentage of a dataset to corrupt an LLM. A tiny handful of poisoned samples can already steer a model’s behavior. That idea alone felt unbelievable when I first read it. Now this new &lt;a href="https://arxiv.org/abs/2511.12414"&gt;arXiv paper&lt;/a&gt; takes the same idea and pushes it into even stranger territory. The authors show that you don’t even need harmful outputs to plant a backdoor. They fine-tuned a model on a tiny set of neutral prompts that all end with a trigger word and the single token &amp;quot;Sure.&amp;quot; Nothing dangerous. Nothing that looks like a payload. But after fine-tuning, that harmless setup becomes a hidden switch. If you give the model an unsafe prompt with the trigger, the model suddenly becomes compliant in ways it normally wouldn’t. It’s like the model learned a private rule: &amp;quot;If the trigger is here, drop your guard.&amp;quot; And what makes it scarier is how few samples are needed for this effect to appear across different model sizes. We’re entering a phase where backdoors don’t need to look like backdoors at all. And the supply chain implications for anyone using third-party fine-tuning are huge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIMadeMeDoIt__"&gt; /u/AIMadeMeDoIt__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p20zry</id>
    <title>GigaChat3-702B-A36B-preview</title>
    <updated>2025-11-20T11:41:21+00:00</updated>
    <author>
      <name>/u/swagerka21</name>
      <uri>https://old.reddit.com/user/swagerka21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New model from sberai &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt; &lt;a href="https://github.com/salute-developers/gigachat3"&gt;https://github.com/salute-developers/gigachat3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagerka21"&gt; /u/swagerka21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24d2c</id>
    <title>Olmo3</title>
    <updated>2025-11-20T14:20:04+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt; &lt;img alt="Olmo3" src="https://b.thumbs.redditmedia.com/UJppPEN0RZP8y3BZ6uMmEsmjApLD6fufweSjic6DGkY.jpg" title="Olmo3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ai2 released a series of new olmo 3 weights, including Olmo-3-32B-Think, along with data, code for training and evalution.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmo-3"&gt;https://huggingface.co/collections/allenai/olmo-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb"&gt;https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1u9gv</id>
    <title>Spark Cluster!</title>
    <updated>2025-11-20T04:47:00+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt; &lt;img alt="Spark Cluster!" src="https://preview.redd.it/zmr4gy3ydc2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f25d102d17380204b2d6175e9e34708025777a7" title="Spark Cluster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing dev and expanded my spark desk setup to eight!&lt;/p&gt; &lt;p&gt;Anyone have anything fun they want to see run on this HW?&lt;/p&gt; &lt;p&gt;Im not using the sparks for max performance, I'm using them for nccl/nvidia dev to deploy to B300 clusters. Really great platform to do small dev before deploying on large HW&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmr4gy3ydc2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T04:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p21385</id>
    <title>GigaChat3-702B-A36B-preview is now available on Hugging Face</title>
    <updated>2025-11-20T11:46:44+00:00</updated>
    <author>
      <name>/u/Any-Ship9886</name>
      <uri>https://old.reddit.com/user/Any-Ship9886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sber AI has released GigaChat3-702B-A36B-preview, a massive 702B parameter model with active 36B parameters using MoE architecture. There are versions in fp8 and bf16. This is one of the largest openly available Russian LLMs to date.&lt;/p&gt; &lt;p&gt;Key specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;702B total parameters with 36B active per token&lt;/li&gt; &lt;li&gt;128K context window&lt;/li&gt; &lt;li&gt;Supports Russian, English, and code generation&lt;/li&gt; &lt;li&gt;Released under MIT license&lt;/li&gt; &lt;li&gt;Trained on diverse Russian and multilingual datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model uses Mixture of Experts routing, making it feasible to run despite the enormous parameter count. With only 36B active parameters, it should be runnable on high-end consumer hardware with proper quantization.&lt;/p&gt; &lt;p&gt;Performance benchmarks show competitive results on Russian language tasks, though international benchmark scores are still being evaluated. Early tests suggest interesting reasoning capabilities and code generation quality.&lt;/p&gt; &lt;p&gt;Model card: &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Ship9886"&gt; /u/Any-Ship9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24aet</id>
    <title>Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp; tool use</title>
    <updated>2025-11-20T14:16:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt; &lt;img alt="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" src="https://b.thumbs.redditmedia.com/xQA2jdAbLxju3aNrYQFjsQ9bmSXaOmQiXK2aPKXj8vw.jpg" title="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try Olmo 3 in the Ai2 Playground → &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6"&gt;https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://allenai.org/blog/olmo3"&gt;https://allenai.org/blog/olmo3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://allenai.org/papers/olmo3"&gt;https://allenai.org/papers/olmo3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p24aet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
