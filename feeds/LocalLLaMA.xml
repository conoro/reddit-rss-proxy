<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-19T18:09:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pqd7sy</id>
    <title>I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it.</title>
    <updated>2025-12-19T05:37:51+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"&gt; &lt;img alt="I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it." src="https://external-preview.redd.it/ejdpemFoZnloMzhnMWGEd-zNYE7e7CLbHm_rOf9Rp-W6GE7TweEbQZSklBMz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=054b952c38361bd74bb25ed593bcaebaa995ebda" title="I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even though it looks simple. This thing has quite the process behind it. I am using Godot Mono, with LLamaSharp (llama.cpp under the hood) for inferencing. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;I start with Phi-3.5 mini. It rewrites the users query into 4 alternative queries&lt;/li&gt; &lt;li&gt;I take those queries and use Qwen 3 embedding model to pull back vector db results for each one&lt;/li&gt; &lt;li&gt;I then dedupe and run a reranking algorithm to limit the results down to around 10 'hits'&lt;/li&gt; &lt;li&gt;Next up is taking the hits and expanding it to include neighboring 'chunks' in the document&lt;/li&gt; &lt;li&gt;Then I format the chunks neatly&lt;/li&gt; &lt;li&gt;Then I pass the context and user's prompt to Qwen 8B with thinking active for it to answer the users question.&lt;/li&gt; &lt;li&gt;Finally the output is sent back to Phi-3.5 mini to 'extract' the answer out of the thinking model's response and format it for the UI. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There's a lot of checks and looping going on in the background too. Lots of juggling with chat history. But by using these small models, it runs very quickly on VRAM. Because the models are small I can just load and unload per request without the load times being crazy. &lt;/p&gt; &lt;p&gt;I won't say this is perfect. And I haven't taken this process and ran it against any benchmarks. But it's honestly gone ALOT better than I ever anticipated. The quality could even improve more when I implement a &amp;quot;Deep Think&amp;quot; mode next. Which will basically just be an agent setup to loop and pull in more relevant context. &lt;/p&gt; &lt;p&gt;But if there's anything I've learned throughout this process...It's that even small language models can answer questions reliably. As long as you give proper context. Context engineering is the most important piece of the pie. We don't need these 300B plus models for most AI needs.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Offloom is just the name I gave my proof of concept. This thing isn't on the market, and probably never will be. It's my own personal playground for proving out concepts. I enjoy making things look nice. Even for POCs.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h85i48fyh38g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T05:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2uvi</id>
    <title>192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA</title>
    <updated>2025-12-18T21:31:29+00:00</updated>
    <author>
      <name>/u/Sero_x</name>
      <uri>https://old.reddit.com/user/Sero_x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt; &lt;img alt="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" src="https://b.thumbs.redditmedia.com/rqYvfP2xSe7ILLKpKsQzha57H6-7i7Cnwe-N3-UA3RM.jpg" title="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab"&gt;https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I bought and built this 3 months ago, I started with 4x 3090s and really loved the process so got another 4x 3090s&lt;/p&gt; &lt;p&gt;Now I‚Äôm convinced I need double the VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sero_x"&gt; /u/Sero_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjqqy</id>
    <title>Known Pretraining Tokens for LLMs</title>
    <updated>2025-12-19T12:21:06+00:00</updated>
    <author>
      <name>/u/phree_radical</name>
      <uri>https://old.reddit.com/user/phree_radical</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"&gt; &lt;img alt="Known Pretraining Tokens for LLMs" src="https://preview.redd.it/970lzt7sk58g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b260274c8027bbb342abe4985e434e89f2823c51" title="Known Pretraining Tokens for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretraining compute seems like it doesn't get enough attention, compared to Parameters.&lt;/p&gt; &lt;p&gt;I was working on this spreadsheet a few months ago. If a vendor didn't publish anything about how many pretraining tokens, I left them out. But I'm certain I've missed some important models.&lt;/p&gt; &lt;p&gt;What can we add to this spreadsheet?&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1vKOK0UPUcUBIEf7srkbGfwQVJTx854_a3rCmglU9QuY/"&gt;https://docs.google.com/spreadsheets/d/1vKOK0UPUcUBIEf7srkbGfwQVJTx854_a3rCmglU9QuY/&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Family / Vendor&lt;/th&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Parameters (B)&lt;/th&gt; &lt;th&gt;Pretraining Tokens (T)&lt;/th&gt; &lt;th&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 33B&lt;/td&gt; &lt;td&gt;33&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LlaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 13B&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LlaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 3 8B&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 3 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-1.8B&lt;/td&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-72B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-0.5b&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-1.5b&lt;/td&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-7b&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-72b&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-57B-A14B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;11.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 0.5B&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 1.5B&lt;/td&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 3B&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 32B&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 72B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 0.6B&lt;/td&gt; &lt;td&gt;0.6&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 1.7B&lt;/td&gt; &lt;td&gt;1.7&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 4B&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 8B&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 32B&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td&gt;235&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM&lt;/td&gt; &lt;td&gt;GLM-130B&lt;/td&gt; &lt;td&gt;130&lt;/td&gt; &lt;td&gt;23&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chinchilla&lt;/td&gt; &lt;td&gt;Chinchilla-70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI&lt;/td&gt; &lt;td&gt;GPT-3 (175B)&lt;/td&gt; &lt;td&gt;175&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI&lt;/td&gt; &lt;td&gt;GPT-4 (1.8T)&lt;/td&gt; &lt;td&gt;1800&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;PaLM (540B)&lt;/td&gt; &lt;td&gt;540&lt;/td&gt; &lt;td&gt;0.78&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TII&lt;/td&gt; &lt;td&gt;Falcon-180B&lt;/td&gt; &lt;td&gt;180&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 1 2B&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 1 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 2B&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 9B&lt;/td&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 27B&lt;/td&gt; &lt;td&gt;27&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 1B&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 4B&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 12B&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 27B&lt;/td&gt; &lt;td&gt;27&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-Coder 1.3B&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-Coder 33B&lt;/td&gt; &lt;td&gt;33&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-LLM 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-LLM 67B&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V2&lt;/td&gt; &lt;td&gt;236&lt;/td&gt; &lt;td&gt;8.1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V3&lt;/td&gt; &lt;td&gt;671&lt;/td&gt; &lt;td&gt;14.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V3.1&lt;/td&gt; &lt;td&gt;685&lt;/td&gt; &lt;td&gt;15.6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-1&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;0.054&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-1.5&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;0.15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-2&lt;/td&gt; &lt;td&gt;2.7&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-medium&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;4.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-small&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;4.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-mini&lt;/td&gt; &lt;td&gt;3.8&lt;/td&gt; &lt;td&gt;3.3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-MoE-instruct&lt;/td&gt; &lt;td&gt;42&lt;/td&gt; &lt;td&gt;4.9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-mini-instruct&lt;/td&gt; &lt;td&gt;3.82&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-MoE-instruct&lt;/td&gt; &lt;td&gt;42&lt;/td&gt; &lt;td&gt;4.9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Xiaomi&lt;/td&gt; &lt;td&gt;MiMo-7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-3-8B-Base-4k&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;3.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-4-340B&lt;/td&gt; &lt;td&gt;340&lt;/td&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-4-15B&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ByteDance&lt;/td&gt; &lt;td&gt;Seed-oss&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phree_radical"&gt; /u/phree_radical &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/970lzt7sk58g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:21:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2rx7</id>
    <title>Exo 1.0 is finally out</title>
    <updated>2025-12-18T21:28:19+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt; &lt;img alt="Exo 1.0 is finally out" src="https://preview.redd.it/zxmsw724618g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=912f00f6d6f4874ab451714c731bec0bbc5a59be" title="Exo 1.0 is finally out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can download from &lt;a href="https://exolabs.net/"&gt;https://exolabs.net/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zxmsw724618g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzhtq</id>
    <title>T5Gemma 2: The next generation of encoder-decoder models</title>
    <updated>2025-12-18T19:17:53+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt; &lt;img alt="T5Gemma 2: The next generation of encoder-decoder models" src="https://external-preview.redd.it/_rnSBYMvSInq6EN43nG_cTgBC4Jp6XTPNyUPRgnGKn0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9dbe7f224d36b036fe98650042395413b48e5a4" title="T5Gemma 2: The next generation of encoder-decoder models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input and generating text output, with open weights for three pretrained sizes (270M-270M, 1B-1B, and 4B-4B).&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tied embeddings:&lt;/strong&gt; Embeddings are tied between the encoder and decoder. This significantly reduces the overall parameter count and allowing to pack more active capabilities into the same memory footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Merged attention:&lt;/strong&gt; The decoder uses a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodality:&lt;/strong&gt; T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extended long context:&lt;/strong&gt; Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massively multilingual:&lt;/strong&gt; Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Models - &lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;https://huggingface.co/collections/google/t5gemma-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official Blog post - &lt;a href="https://blog.google/technology/developers/t5gemma-2/"&gt;https://blog.google/technology/developers/t5gemma-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqf27c</id>
    <title>Is gpt oss:120b still the best at its size?</title>
    <updated>2025-12-19T07:27:00+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in math and coding.. is there still no model that is clearly stronger at 120b or less?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T07:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqala0</id>
    <title>MBZUAI releases K2-V2 - 70B fully open model.</title>
    <updated>2025-12-19T03:20:39+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holy frijoles. Has anyone given this a look? Fully open like Olmo 3, but a solid 70B of performance. I‚Äôm not sure why I‚Äôm just hearing about it, but, definitely looking forward to seeing how folks receive it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/"&gt;https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(I searched for other posts on this but didn‚Äôt see anything - let me know if I missed a thread!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T03:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqeauj</id>
    <title>Meta is developing a new image and video AI model ‚ÄúMango‚Äù, along with a previously reported ‚ÄúAvocado‚Äù according to WSJ.</title>
    <updated>2025-12-19T06:40:33+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"&gt; &lt;img alt="Meta is developing a new image and video AI model ‚ÄúMango‚Äù, along with a previously reported ‚ÄúAvocado‚Äù according to WSJ." src="https://preview.redd.it/yf9939hiw38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25b7cf3ee9d09bdbbcda23002e194ff41e6d07c0" title="Meta is developing a new image and video AI model ‚ÄúMango‚Äù, along with a previously reported ‚ÄúAvocado‚Äù according to WSJ." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://www.wsj.com/tech/ai/meta-developing-new-ai-image-and-video-model-code-named-mango-16e785c7"&gt;https://www.wsj.com/tech/ai/meta-developing-new-ai-image-and-video-model-code-named-mango-16e785c7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yf9939hiw38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppun3v</id>
    <title>Google's Gemma models family</title>
    <updated>2025-12-18T16:09:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt; &lt;img alt="Google's Gemma models family" src="https://preview.redd.it/59w0vja4lz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd2d66ee23d4078bf31aba81cdeecc769669af4" title="Google's Gemma models family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59w0vja4lz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjhz9</id>
    <title>What metrics actually matter most when evaluating AI agents?</title>
    <updated>2025-12-19T12:07:38+00:00</updated>
    <author>
      <name>/u/screechymeechydoodle</name>
      <uri>https://old.reddit.com/user/screechymeechydoodle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to set up a lightweight way to evaluate some local agents I‚Äôve been working with (mostly tool-using Llama variants), and I‚Äôm not 100% sure which metrics I need to be paying the most attention to.&lt;/p&gt; &lt;p&gt;I‚Äôm new to this and its hard to wrap my head around it all. Like success rate, hallucination rate, tool-calling accuracy, multi-step reasoning reliability, etc.&lt;/p&gt; &lt;p&gt;What are yall tracking when it comes to testing local agents. If you had to focus on just a handful of metrics, which ones give you the best signal?&lt;/p&gt; &lt;p&gt;Also, if anyone has a setup that doesn‚Äôt require spinning up a whole cloud pipeline, I‚Äôd love to hear it. Right now I‚Äôm measuring everything manually and its a pain in the ass.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/screechymeechydoodle"&gt; /u/screechymeechydoodle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjhz9/what_metrics_actually_matter_most_when_evaluating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjhz9/what_metrics_actually_matter_most_when_evaluating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjhz9/what_metrics_actually_matter_most_when_evaluating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:07:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqo3mg</id>
    <title>Best option for local company LLM</title>
    <updated>2025-12-19T15:35:50+00:00</updated>
    <author>
      <name>/u/DerDave</name>
      <uri>https://old.reddit.com/user/DerDave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;In my company (roughly 50 people) we have a lot of knowledge and data, that is pretty well structured but currently hard to search. This includes CRM data, meeting transcripts, bookkeeping data, project management tool data. So there is data privacy relevant stuff there, which I don't want to upload to the cloud.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's what I want to do:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'd like to have an LLM for all my employees, so they can query it about company facts and to quickly find the right resources.&lt;/li&gt; &lt;li&gt;All employees should be able to access this chatbot anytime (while connected to the internal network/VPN), so it needs to be able to process several requests at once&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I don't need:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding support (we're using the state of the art external models for that)&lt;/li&gt; &lt;li&gt;Agentic/Tool use. Just information queries.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My questions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do I need a vector data base, which is periodically updated on our data?&lt;/li&gt; &lt;li&gt;Do I need to finetune an open source model on my data or can I use a generalist model that just uses RAG with the vectorDB?&lt;/li&gt; &lt;li&gt;What setup is best for serving the chatbot, if several people are supposed to chat with it in parallel?&lt;/li&gt; &lt;li&gt;What &amp;quot;size&amp;quot; of model is good enough for simple query or summary tasks like mine? I was thinking about Qwen3-30B-A3B or the new Nemotron 3 - any reasons why these models are the wrong choice?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you very much in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DerDave"&gt; /u/DerDave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqo3mg/best_option_for_local_company_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqo3mg/best_option_for_local_company_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqo3mg/best_option_for_local_company_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqqbcx</id>
    <title>50% of all r/LocalLLaMA posts in 2025 summed up in one image:</title>
    <updated>2025-12-19T17:02:07+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqqbcx/50_of_all_rlocalllama_posts_in_2025_summed_up_in/"&gt; &lt;img alt="50% of all r/LocalLLaMA posts in 2025 summed up in one image:" src="https://preview.redd.it/jm8vbgbjz68g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1430f2ce4a8e07bff75f9a538526a41aa6f39033" title="50% of all r/LocalLLaMA posts in 2025 summed up in one image:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jm8vbgbjz68g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqqbcx/50_of_all_rlocalllama_posts_in_2025_summed_up_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqqbcx/50_of_all_rlocalllama_posts_in_2025_summed_up_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T17:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq5k6e</id>
    <title>Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios</title>
    <updated>2025-12-18T23:23:44+00:00</updated>
    <author>
      <name>/u/Competitive_Travel16</name>
      <uri>https://old.reddit.com/user/Competitive_Travel16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt; &lt;img alt="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" src="https://external-preview.redd.it/A_KZLQUNhCh0wGe2hwjJCJ470X6QmuVpXZdzOWccb0U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fdf14dca65c42b501a6a7e33b1acf44e71ac72f" title="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Travel16"&gt; /u/Competitive_Travel16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4l4UWZGxvoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T23:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqq52w</id>
    <title>keep the faith</title>
    <updated>2025-12-19T16:55:35+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqq52w/keep_the_faith/"&gt; &lt;img alt="keep the faith" src="https://preview.redd.it/wfj1sd6cy68g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a61f324f88f14f1670f4f0c2015d8aa667054d58" title="keep the faith" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfj1sd6cy68g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqq52w/keep_the_faith/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqq52w/keep_the_faith/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T16:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2ry0</id>
    <title>Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster</title>
    <updated>2025-12-18T21:28:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt; &lt;img alt="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" src="https://preview.redd.it/32z50w1s518g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be2781529b5cacb7d7a84c794d37a156e1bdc798" title="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing llama.cpp RPC vs Exo's new RDMA Tensor setting on a cluster of 4x Mac Studios (2x 512GB and 2x 256GB) that Apple loaned me until Februrary.&lt;/p&gt; &lt;p&gt;Would love to do more testing between now and returning it. A lot of the earlier testing was debugging stuff since the RDMA support was very new for the past few weeks... now that it's somewhat stable I can do more.&lt;/p&gt; &lt;p&gt;The annoying thing is there's nothing nice like llama-bench in Exo, so I can't give as direct comparisons with context sizes, prompt processing speeds, etc. (it takes a lot more fuss to do that, at least).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32z50w1s518g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqpj29</id>
    <title>Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture</title>
    <updated>2025-12-19T16:31:46+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt; &lt;img alt="Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture" src="https://preview.redd.it/cu5vt8lnt68g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efe9182c042a4013c8b0a1760f4ff5a1ac022efd" title="Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] A Golden Age for AI Careers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng emphasizes that this is the best time ever to build a career in AI. He notes that the complexity of tasks AI can handle is doubling approximately every seven months, meaning progress is accelerating, not slowing down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] The Power of AI Coding Tools&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Staying on the ‚Äúfrontier‚Äù of coding tools (like Cursor, Claude, and Gemini) is crucial. Being even half a generation behind in your tooling makes you significantly less productive in the current market.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] The ‚ÄúProduct Management Bottleneck‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Because AI has made writing code so much cheaper and faster, the bottleneck has shifted to deciding what to build. Engineers who can talk to users, develop empathy, and handle product management (PM) tasks are the fastest-moving individuals in Silicon Valley today.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Surround Yourself with the Right People&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Success is highly predicted by the people you surround yourself with. Ng encourages building a ‚Äúrich connective tissue‚Äù of friends and colleagues to share insights that aren‚Äôt yet published on the internet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[5] Team Over Brand&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When job hunting, the specific team and people you work with day-to-day are more important than the company‚Äôs ‚Äúhot brand.‚Äù Avoid companies that refuse to tell you which team you will join before you sign.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[6] Go and Build Stuff&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng‚Äôs number one piece of advice is to simply &lt;strong&gt;go and build stuff&lt;/strong&gt;. The cost of failure is low (losing a weekend), but the learning and demonstration of skill are invaluable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[7] The Value of Hard Work&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Andrew Ng encourages working hard, defining it not just by hours but by output and passion for building.&lt;/p&gt; &lt;p&gt;Video - &lt;a href="https://www.youtube.com/watch?v=AuZoDsNmG_s"&gt;https://www.youtube.com/watch?v=AuZoDsNmG_s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cu5vt8lnt68g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T16:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjja2</id>
    <title>Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family.</title>
    <updated>2025-12-19T12:09:34+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt; &lt;img alt="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." src="https://b.thumbs.redditmedia.com/wfpfrO9KgU8CdhDynFFqinaA1_4j8ZSos50KzQEDVTA.jpg" title="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma Scope 2: &lt;a href="https://huggingface.co/google/gemma-scope-2"&gt;https://huggingface.co/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/google/gemma-scope-2"&gt;https://huggingface.co/collections/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Google AI Developers on ùïè: &lt;a href="https://x.com/googleaidevs/status/2001986944687804774"&gt;https://x.com/googleaidevs/status/2001986944687804774&lt;/a&gt;&lt;br /&gt; Blog post: Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior: &lt;a href="https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/"&gt;https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqjja2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqm5g4</id>
    <title>Seed OSS 36b made me reconsider my life choices.</title>
    <updated>2025-12-19T14:15:06+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5AM, - Me: Hello Seed, write me a complete new library does this and that, use that internal library as a reference but extend it to handle more data formats. Unify the data abstraction layer so data from one format can be exported to other format. Analyse the code in the internal lib directory and create a similar library but extended with more data formats to support. Create unit tests. To run the unit tests use the following command ...&lt;br /&gt; - Seed: Hold my Âï§ÈÖí&lt;/p&gt; &lt;p&gt;9AM, - Seed: Crap, dude, the test is failing and Im out of 100k context, help!&lt;br /&gt; - Me: Hold on pal, there you go, quick restart, You were working on this and that, keep going mate. This is the short error log, DON'T copy and paste 100k lines of repeating errors lol&lt;br /&gt; - Seed: Gotcha...&lt;/p&gt; &lt;p&gt;11AM, - Seed: Boom done, not a single f**king error, code is in src, tests are in test, examples are here, and this is some docs for you, stupid human being&lt;br /&gt; - Me: :O&lt;/p&gt; &lt;p&gt;Holy f**k. &lt;/p&gt; &lt;p&gt;Anyone else using seed-oss-36b? I literally downloaded it yesterday, ran the Q6_K_XL quant to fit in the 48GB vram with 100k context at q8. Im speachless. Yes, it is slower than the competitors (devstral? qwen?) but the quality is jaw dropping. Worked for hours, without supervision, and if not the context length it would possibly finish the entire project alone. Wierd that there is so little news about this model. Its stupidly good at agentic coding.&lt;/p&gt; &lt;p&gt;Human coding? RIP 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoldt</id>
    <title>Chinese researchers unveil "LightGen": An all-optical chip that outperforms Nvidia‚Äôs A100 by 100x</title>
    <updated>2025-12-19T15:55:23+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New research from SJTU and Tsinghua (these are top tier labs, not slopmonsters like East China Normal University etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.science.org/doi/10.1126/science.adv7434"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqfmsr</id>
    <title>Meta releases SAM Audio for audio separation</title>
    <updated>2025-12-19T08:03:38+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"&gt; &lt;img alt="Meta releases SAM Audio for audio separation" src="https://external-preview.redd.it/cHEzMGt1a2YzNDhnMXbShRCjAlPQsamMmoIWTAtR2gquYxttgWY9vfB1L3ZP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a42426e742804c90a902cb380ea41038b48a1027" title="Meta releases SAM Audio for audio separation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;SAM Audio separates target and residual sounds from any audio or audiovisual source‚Äîacross general sound, music, and speech.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://ai.meta.com/samaudio/"&gt;https://ai.meta.com/samaudio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/facebook/sam-audio"&gt;https://huggingface.co/collections/facebook/sam-audio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/sam-audio"&gt;https://github.com/facebookresearch/sam-audio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/en7nfnmf348g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T08:03:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqn0vq</id>
    <title>GLM 4.7 is Coming?</title>
    <updated>2025-12-19T14:52:18+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt; &lt;img alt="GLM 4.7 is Coming?" src="https://preview.redd.it/206mfj3dc68g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f01e8b54d3347827d6980eb1b0cbc7453cfd2d9c" title="GLM 4.7 is Coming?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/30876"&gt;https://github.com/vllm-project/vllm/pull/30876&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/206mfj3dc68g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoi6i</id>
    <title>Qwen released Qwen-Image-Layered on Hugging face.</title>
    <updated>2025-12-19T15:51:45+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt; &lt;img alt="Qwen released Qwen-Image-Layered on Hugging face." src="https://b.thumbs.redditmedia.com/WT_uezmugp_bMYr9okz4OYqH1W02XtM64SzwTE-NCms.jpg" title="Qwen released Qwen-Image-Layered on Hugging face." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;https://huggingface.co/Qwen/Qwen-Image-Layered&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Photoshop-grade layering Physically isolated RGBA layers with true native editability Prompt-controlled structure Explicitly specify 3‚Äì10 layers ‚Äî from coarse layouts to fine-grained details Infinite decomposition Keep drilling down: layers within layers, to any depth of detail&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqoi6i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqegcr</id>
    <title>Realist meme of the year!</title>
    <updated>2025-12-19T06:49:54+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt; &lt;img alt="Realist meme of the year!" src="https://preview.redd.it/8oge3a2by38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4697e9a87c50f3f170db7e87eccd27363c505dc" title="Realist meme of the year!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8oge3a2by38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
