<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-29T13:28:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pylxj4</id>
    <title>New Llama.cpp Front-End (Intelligent Low VRAM Context Management)</title>
    <updated>2025-12-29T13:11:35+00:00</updated>
    <author>
      <name>/u/F0R3V3R50F7</name>
      <uri>https://old.reddit.com/user/F0R3V3R50F7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylxj4/new_llamacpp_frontend_intelligent_low_vram/"&gt; &lt;img alt="New Llama.cpp Front-End (Intelligent Low VRAM Context Management)" src="https://b.thumbs.redditmedia.com/Xx5au-pNBDrDDvfgY1TJfI9jCRc1dao3_2uWzP_3y0I.jpg" title="New Llama.cpp Front-End (Intelligent Low VRAM Context Management)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ready-to-run, just drop in your .gguf models and go! Try it out -&lt;a href="https://github.com/F0R3V3R50F7/openOrchestrate"&gt;https://github.com/F0R3V3R50F7/openOrchestrate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/F0R3V3R50F7"&gt; /u/F0R3V3R50F7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pylxj4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylxj4/new_llamacpp_frontend_intelligent_low_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylxj4/new_llamacpp_frontend_intelligent_low_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylxpr</id>
    <title>What tool/SaaS do you use to maintain your internal documentation?</title>
    <updated>2025-12-29T13:11:50+00:00</updated>
    <author>
      <name>/u/Hari-Prasad-12</name>
      <uri>https://old.reddit.com/user/Hari-Prasad-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For things like:&lt;br /&gt; 1. API Collection&lt;br /&gt; 2. API Docs&lt;br /&gt; 3. Internal information of system design&lt;/p&gt; &lt;p&gt;etc...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hari-Prasad-12"&gt; /u/Hari-Prasad-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylxpr/what_toolsaas_do_you_use_to_maintain_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylxpr/what_toolsaas_do_you_use_to_maintain_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylxpr/what_toolsaas_do_you_use_to_maintain_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyikbk</id>
    <title>Sharing data that may contain PII? Here's a case-study on how to use a task-specific SLM to remove sensitive info locally and preserve user privacy</title>
    <updated>2025-12-29T10:05:08+00:00</updated>
    <author>
      <name>/u/Ok_Hold_5385</name>
      <uri>https://old.reddit.com/user/Ok_Hold_5385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When sharing user data that may contain Personally Identifiable Information, anonymization is a crucial step in ensuring user privacy. PII removal APIs exist, but they often defeat the purpose of anonymization, since data must be sent to third-party servers.&lt;/p&gt; &lt;p&gt;Read this case-study to find out how to use &lt;a href="https://github.com/tanaos/artifex"&gt;the Artifex library&lt;/a&gt; to create a task-specific Small Language Model to anonymize data on your local machine, without sending it to third-party APIs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://tanaos.com/blog/anonymize-text-locally/"&gt;https://tanaos.com/blog/anonymize-text-locally/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Too busy to read the case study? Here's the code-only version:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install artifex from artifex import Artifex ta = Artifex().text_anonymization print(ta(&amp;quot;John Doe lives at 123 Main St, New York. His phone number is (555) 123-4567.&amp;quot;)) # &amp;gt;&amp;gt;&amp;gt; [&amp;quot;[MASKED] lives at [MASKED]. His phone number is [MASKED].&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Hold_5385"&gt; /u/Ok_Hold_5385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikbk/sharing_data_that_may_contain_pii_heres_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikbk/sharing_data_that_may_contain_pii_heres_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikbk/sharing_data_that_may_contain_pii_heres_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T10:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxz7f0</id>
    <title>Plamo3 (2B/8B/31B) support has been merged into llama.cpp</title>
    <updated>2025-12-28T18:55:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"&gt; &lt;img alt="Plamo3 (2B/8B/31B) support has been merged into llama.cpp" src="https://external-preview.redd.it/addr5Q-exN6mOW2m8NxyWDisrtP7qSIOjIojUWESLhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=044fee01a3568867e7f945c50edee5a2529bd629" title="Plamo3 (2B/8B/31B) support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PLaMo 3 NICT 31B Base is a 31B model pre-trained on English and Japanese datasets, developed by Preferred Networks, Inc. collaborative with National Institute of Information and Communications Technology, NICT.&lt;/p&gt; &lt;p&gt;PLaMo 3 NICT models adapt a hybrid architecture with Sliding Window Attention (SWA) and Traditional Attetntion layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17304"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T18:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1py7ren</id>
    <title>LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional ‚Äúconstrained ‚Üî expressive‚Äù control direction</title>
    <updated>2025-12-29T00:46:06+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"&gt; &lt;img alt="LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional ‚Äúconstrained ‚Üî expressive‚Äù control direction" src="https://b.thumbs.redditmedia.com/UBQp_mEOBNMo9dMlud16WdE2rF9MrijZEck9seQt-zA.jpg" title="LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional ‚Äúconstrained ‚Üî expressive‚Äù control direction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a small interpretability tool that does fMRI-style visualization and &lt;em&gt;live hidden-state intervention&lt;/em&gt; on local models. While exploring LLaMA-3.2-3B, I noticed one hidden dimension (layer 20, dim ~3039) that consistently stood out across prompts and timesteps.&lt;/p&gt; &lt;p&gt;I then set up a simple Gradio UI to &lt;strong&gt;poke that single dimension during inference&lt;/strong&gt; (via a forward hook) and swept epsilon in both directions.&lt;/p&gt; &lt;p&gt;What I found is that this dimension appears to act as a &lt;strong&gt;global control axis&lt;/strong&gt; rather than encoding specific semantic content.&lt;/p&gt; &lt;h1&gt;Observed behavior (consistent across prompts)&lt;/h1&gt; &lt;p&gt;By varying epsilon on this one dim:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Negative Œµ&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;outputs become restrained, procedural, and instruction-faithful&lt;/li&gt; &lt;li&gt;explanations stick closely to canonical structure&lt;/li&gt; &lt;li&gt;less editorializing or extrapolation&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Positive Œµ&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;outputs become more verbose, narrative, and speculative&lt;/li&gt; &lt;li&gt;the model adds framing, qualifiers, and audience modeling&lt;/li&gt; &lt;li&gt;responses feel ‚Äúless reined in‚Äù even on factual prompts&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Crucially, this holds across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;conversational prompts&lt;/li&gt; &lt;li&gt;factual prompts (chess rules, photosynthesis)&lt;/li&gt; &lt;li&gt;recommendation prompts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The effect is smooth, monotonic, and bidirectional.&lt;/p&gt; &lt;h1&gt;Methods (brief)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Model: LLaMA-3.2-3B-Instruct&lt;/li&gt; &lt;li&gt;Intervention: single hidden dimension modified during forward pass&lt;/li&gt; &lt;li&gt;No gradients, no finetuning, no logit biasing&lt;/li&gt; &lt;li&gt;Visualization frontend in Godot; inference + hooks in PyTorch&lt;/li&gt; &lt;li&gt;All tests run locally; prompts trivially swappable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to share more details if folks are interested.&lt;/p&gt; &lt;h1&gt;Why I‚Äôm posting&lt;/h1&gt; &lt;p&gt;I‚Äôm still very much in the &lt;em&gt;exploratory&lt;/em&gt; phase ‚Äî the goal right now is to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;identify stable control directions&lt;/li&gt; &lt;li&gt;understand their scope&lt;/li&gt; &lt;li&gt;design better tests to separate correlation from load-bearing causality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If people have suggestions for additional sanity checks, ablations, or related work I should read, I‚Äôm all ears.&lt;/p&gt; &lt;p&gt;TIME FOR SCIENCE üß™&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ppyvusqvg1ag1.png?width=1858&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1eb8a6b97091ec0a0bba13e4aad2e00524a826f6"&gt;Dim 3039 just begging to get poked.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w04unfb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f4eeb8b341a12d59173e5338a4ed58db3585500"&gt;https://preview.redd.it/w04unfb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f4eeb8b341a12d59173e5338a4ed58db3585500&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzioukb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae4d71911b14a68805069101be779819d8c97d22"&gt;https://preview.redd.it/rzioukb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae4d71911b14a68805069101be779819d8c97d22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eo1vyeb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdd3f9c990c07bc00f7bf55850fffa2b5934b54f"&gt;https://preview.redd.it/eo1vyeb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdd3f9c990c07bc00f7bf55850fffa2b5934b54f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tangtlb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b86b9c5ae15e3b9d413ddf80f607ec335855436"&gt;https://preview.redd.it/tangtlb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b86b9c5ae15e3b9d413ddf80f607ec335855436&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/38fbskb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3039a2ef5443fe71cfcce0069f74432b92340a2e"&gt;https://preview.redd.it/38fbskb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3039a2ef5443fe71cfcce0069f74432b92340a2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qj2ltnb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bf14c7ca6281a4a4496d39244f4060997715734"&gt;https://preview.redd.it/qj2ltnb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bf14c7ca6281a4a4496d39244f4060997715734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ro7belb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=351f8c93a253fda22a44a4689d97068e434e5c5c"&gt;https://preview.redd.it/ro7belb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=351f8c93a253fda22a44a4689d97068e434e5c5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/305i2mb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dfb05fbed2f9104918898be72fff9663890fc26"&gt;https://preview.redd.it/305i2mb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dfb05fbed2f9104918898be72fff9663890fc26&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T00:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyeqpe</id>
    <title>AMD AI Max 395 128gb or Mac Studio M2 Ultra 128gb?</title>
    <updated>2025-12-29T06:19:16+00:00</updated>
    <author>
      <name>/u/solo_entrepreneur</name>
      <uri>https://old.reddit.com/user/solo_entrepreneur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AMD AI Max 395 128gb or Mac Studio M2 Ultra 128gb? &lt;/p&gt; &lt;p&gt;I found both of them used on OfferUp. &lt;/p&gt; &lt;p&gt;The Mac Studio is an M2 Ultra 128gb 2TB for $2500. (No warranty)&lt;/p&gt; &lt;p&gt;The AMD is an Beelink GTR9 Pro AI Max+ 395 128gb 2TB for $1500. (Probably doesn‚Äôt have warranty too) &lt;/p&gt; &lt;p&gt;I‚Äôm a Mac user by the way. I already own a MacBook Pro M1 Max 64gb 2TB. &lt;/p&gt; &lt;p&gt;Need something to run 70b models faster. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solo_entrepreneur"&gt; /u/solo_entrepreneur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyeqpe/amd_ai_max_395_128gb_or_mac_studio_m2_ultra_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyeqpe/amd_ai_max_395_128gb_or_mac_studio_m2_ultra_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyeqpe/amd_ai_max_395_128gb_or_mac_studio_m2_ultra_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T06:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1py3o6p</id>
    <title>Owlex - an MCP server that lets Claude Code consult Codex, Gemini, and OpenCode as a "council"</title>
    <updated>2025-12-28T21:53:48+00:00</updated>
    <author>
      <name>/u/spokv</name>
      <uri>https://old.reddit.com/user/spokv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using Claude Code for a while and wanted a way to get second opinions from other AI coding agents without leaving my workflow. So I built &lt;strong&gt;Owlex&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;What it does:&lt;br /&gt; The killer feature is &lt;strong&gt;council_ask&lt;/strong&gt; - it queries Codex, Gemini, and OpenCode in parallel, then optionally runs a second round where each agent sees the others' answers and revises (or critiques) their response.&lt;/p&gt; &lt;p&gt;council_ask(&amp;quot;Should I use Redis or PostgreSQL for this caching layer?&amp;quot;)&lt;/p&gt; &lt;p&gt;All three agents answer simultaneously (~8s total), then deliberate. You get diverse perspectives without the copy-paste dance between terminals.&lt;/p&gt; &lt;p&gt;Other features:&lt;br /&gt; - Start/resume sessions with each agent individually&lt;br /&gt; - Async task execution with timeouts&lt;br /&gt; - Critique mode - agents actively look for bugs in each other's code suggestions&lt;/p&gt; &lt;p&gt;Example output:&lt;/p&gt; &lt;p&gt;Round 1: querying Codex, Gemini, Opencode...&lt;br /&gt; Codex completed (4.0s)&lt;br /&gt; OpenCode completed (5.6s)&lt;br /&gt; Gemini completed (7.7s)&lt;br /&gt; Round 2: deliberation phase..&lt;/p&gt; &lt;p&gt;Install:&lt;br /&gt; uv tool install git+&lt;a href="https://github.com/agentic-mcp-tools/owlex.git"&gt;https://github.com/agentic-mcp-tools/owlex.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/agentic-mcp-tools/owlex"&gt;https://github.com/agentic-mcp-tools/owlex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spokv"&gt; /u/spokv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py3o6p/owlex_an_mcp_server_that_lets_claude_code_consult/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py3o6p/owlex_an_mcp_server_that_lets_claude_code_consult/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py3o6p/owlex_an_mcp_server_that_lets_claude_code_consult/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T21:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyd6a6</id>
    <title>RTX 6000 Pro + RTX 3090 in one machine?</title>
    <updated>2025-12-29T04:58:51+00:00</updated>
    <author>
      <name>/u/az_6</name>
      <uri>https://old.reddit.com/user/az_6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just able to get my hands on a RTX 6000 Pro 96gb card, and I currently have two 3090s in my machine. Should I keep one of the 3090s in there or should I just make do with the single 6000?&lt;/p&gt; &lt;p&gt;I‚Äôm looking to run GPT-OSS at the best possible quality and speed I can. I‚Äôd also want to try run models that are &amp;gt;96GB, in this case would it better to offload to CPU/RAM or to the other GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/az_6"&gt; /u/az_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6a6/rtx_6000_pro_rtx_3090_in_one_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6a6/rtx_6000_pro_rtx_3090_in_one_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6a6/rtx_6000_pro_rtx_3090_in_one_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T04:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyfu01</id>
    <title>Why is sgalng's torch.compile startup so much slower than vLLM?</title>
    <updated>2025-12-29T07:20:53+00:00</updated>
    <author>
      <name>/u/Inside_Camp870</name>
      <uri>https://old.reddit.com/user/Inside_Camp870</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I've been testing torch.compile on SGLang with Gemma 3 12B, and noticed some significant startup time differences compared to vLLM.&lt;/p&gt; &lt;h3&gt;What I'm seeing&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;SGLang without compile: ~1:30 startup&lt;/li&gt; &lt;li&gt;SGLang with compile (bs 1,2,4,8,16): ~6min startup&lt;/li&gt; &lt;li&gt;vLLM with compile enabled (default): ~1min startup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm getting 5-15% perf gains from compile at lower batch sizes (bs &amp;lt; 16), so I'd like to use it‚Äîbut the startup cost is pretty rough.&lt;/p&gt; &lt;h3&gt;details&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;vLLM: &lt;code&gt; vllm serve /root/models/gemma3 \ --tensor-parallel-size 1 \ --max-model-len 2448 \ --gpu-memory-utilization 0.8 \ --max-num-seqs 16 \ --compilation-config '{&amp;quot;cudagraph_capture_sizes&amp;quot;: [1,2,4,8,16]}' &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;sglang: &lt;code&gt; python -m sglang.launch_server \ --model-path /root/models/gemma3 \ --tp 1 \ --context-length 2448 \ --mem-fraction-static 0.8 \ --enable-torch-compile \ --torch-compile-max-bs 16 &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;My guess&lt;/h3&gt; &lt;p&gt;vLLM uses piecewise compilation by default, which is faster than full-graph. In SGLang, compile seems tied to CUDA graph, so piecewise compile only comes with piecewise CUDA graph‚Äîwhose overhead might negate the compile benefits anyway.&lt;/p&gt; &lt;p&gt;I understand &amp;quot;beat torch compile&amp;quot; is the long-term direction(&lt;a href="https://github.com/sgl-project/sglang/issues/4748"&gt;https://github.com/sgl-project/sglang/issues/4748&lt;/a&gt;) and compile isn't really the focus right now. But given the gains I'm seeing on some models, I'm curious: &lt;strong&gt;does anyone know what's actually different between vLLM and SGLang's compile implementations here?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inside_Camp870"&gt; /u/Inside_Camp870 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyfu01/why_is_sgalngs_torchcompile_startup_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyfu01/why_is_sgalngs_torchcompile_startup_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyfu01/why_is_sgalngs_torchcompile_startup_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T07:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1py4xp6</id>
    <title>Is Q8 KV cache alright for vision models and high context</title>
    <updated>2025-12-28T22:45:41+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What has your experience been with using q8 KV cache and a vision model?&lt;/p&gt; &lt;p&gt;GLM4.6 V, qwen3VL‚Ä¶&lt;/p&gt; &lt;p&gt;Would you say it‚Äôs good enough or does it ruin outputs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T22:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pybbjg</id>
    <title>I built a local voice assistant that learns new abilities via auto-discovered n8n workflows exposed as tools via MCP (LiveKit + Ollama + n8n)</title>
    <updated>2025-12-29T03:28:35+00:00</updated>
    <author>
      <name>/u/CoreWorxLab</name>
      <uri>https://old.reddit.com/user/CoreWorxLab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released CAAL - a local voice assistant that auto-discovers n8n workflows as tools.&lt;/p&gt; &lt;p&gt;Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (I'm running Ministral-3:8B)&lt;/li&gt; &lt;li&gt;LiveKit for WebRTC&lt;/li&gt; &lt;li&gt;Whisper STT&lt;/li&gt; &lt;li&gt;Kokoro TTS&lt;/li&gt; &lt;li&gt;n8n for tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Key feature: Infinite tool expandability through n8n. Add a workflow, CAAL learns it. It can even build its own tools on command.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Demo Video: &lt;a href="https://www.youtube.com/watch?v=Fcn-qq8OiTA"&gt;youtube.com/watch?v=Fcn-qq8OiTA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/coreworxlab/caal"&gt;github.com/CoreWorxLab/CAAL&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoreWorxLab"&gt; /u/CoreWorxLab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pybbjg/i_built_a_local_voice_assistant_that_learns_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pybbjg/i_built_a_local_voice_assistant_that_learns_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pybbjg/i_built_a_local_voice_assistant_that_learns_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T03:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyk3jg</id>
    <title>Help me build a (reasonable) 4GPU low-cost LLM machine, is ASUS WS X299 PRO/SE still good?</title>
    <updated>2025-12-29T11:35:12+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I kind of exhausted what could be done with my fast. but VRAM poor, 4090 OC edition, so I was dreaming of designing an openframe 4 GPU machine that can drive with acceptable speed 4 GPUs.&lt;/p&gt; &lt;p&gt;My preliminary research found rather acceptable priced WS X299 PRO/SE workstation motherboards that paired with an 48-Lane CPU may just do the trick, also the 64GB DDR4 for it is really price acceptable. &lt;/p&gt; &lt;p&gt;So are there any better mobo/CPU combo under lesr than 1000EUR capable of driving 4 GPUS (proven solutions are getting a super thanks) , please share your experiences and thoughts, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxss0m</id>
    <title>Senator in Tennessee introduces bill to felonize making AI "act as a companion" or "mirror human interactions"</title>
    <updated>2025-12-28T14:35:58+00:00</updated>
    <author>
      <name>/u/CanineAssBandit</name>
      <uri>https://old.reddit.com/user/CanineAssBandit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.&lt;/p&gt; &lt;p&gt;The bill:&lt;br /&gt; &lt;a href="https://legiscan.com/TN/bill/SB1493/2025"&gt;https://legiscan.com/TN/bill/SB1493/2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quotes from the bill (emphasis mine):&lt;/p&gt; &lt;p&gt;It is an offense for a person to knowingly train artificial intelligence to:&lt;br /&gt; (3) Provide emotional support, &lt;strong&gt;including through open-ended conversations&lt;/strong&gt; with a user;&lt;br /&gt; (4) Develop an emotional relationship with, or otherwise &lt;strong&gt;act as a companion&lt;/strong&gt; to, an individual;&lt;br /&gt; (6) Otherwise act as a sentient human or &lt;strong&gt;mirror interactions that a human user might have with another human user&lt;/strong&gt;, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;&lt;br /&gt; (8) &lt;strong&gt;Simulate a human being&lt;/strong&gt;, including in appearance, voice, or other mannerisms.&lt;/p&gt; &lt;p&gt;&amp;quot;Train&amp;quot;:&lt;br /&gt; (A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of &lt;strong&gt;making decisions based on information or other inputs&lt;/strong&gt; provided to the A.I.&lt;br /&gt; (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CanineAssBandit"&gt; /u/CanineAssBandit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyikha</id>
    <title>EditMGT ‚Äî fast, localized image editing with Masked Generative Transformers</title>
    <updated>2025-12-29T10:05:26+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt; &lt;img alt="EditMGT ‚Äî fast, localized image editing with Masked Generative Transformers" src="https://a.thumbs.redditmedia.com/ipfAbO3mXGnc9dJmlMhzMZSQdDCR5P6AtDUpbQk9sR0.jpg" title="EditMGT ‚Äî fast, localized image editing with Masked Generative Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o8ngfvhy94ag1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af9bacb07a0773d78259c8a08f832a922d503104"&gt;https://preview.redd.it/o8ngfvhy94ag1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af9bacb07a0773d78259c8a08f832a922d503104&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First &lt;strong&gt;MGT-based&lt;/strong&gt; editing framework that confines changes to target regions, mitigating diffusion ‚Äúedit leakage.‚Äù &lt;strong&gt;&amp;lt;1B params&lt;/strong&gt;, reported &lt;strong&gt;~6√ó faster&lt;/strong&gt; edits (paper notes ~&lt;strong&gt;2s&lt;/strong&gt; per edit). &lt;/p&gt; &lt;ul&gt; &lt;li&gt;How it works: &lt;strong&gt;multi-layer attention consolidation&lt;/strong&gt; + &lt;strong&gt;region-hold sampling&lt;/strong&gt; for precise localization/preservation. &lt;a href="https://arxiv.org/html/2512.11715v1?utm_source=chatgpt.com"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Data: &lt;strong&gt;CrispEdit-2M&lt;/strong&gt; (~2M hi-res, 7 categories) released for training/eval. &lt;a href="https://huggingface.co/datasets/WeiChow/CrispEdit-2M?utm_source=chatgpt.com"&gt;Hugging Face+1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Links: &lt;strong&gt;GitHub repo&lt;/strong&gt; &lt;a href="https://github.com/weichow23/EditMGT"&gt;https://github.com/weichow23/EditMGT&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T10:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylyen</id>
    <title>I was training an AI model and...</title>
    <updated>2025-12-29T13:12:46+00:00</updated>
    <author>
      <name>/u/bapuc</name>
      <uri>https://old.reddit.com/user/bapuc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylyen/i_was_training_an_ai_model_and/"&gt; &lt;img alt="I was training an AI model and..." src="https://preview.redd.it/06e4wmao75ag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=000d5dbb19fbb5ad24d3ec228e0a3bc802bd60a1" title="I was training an AI model and..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bapuc"&gt; /u/bapuc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/06e4wmao75ag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylyen/i_was_training_an_ai_model_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylyen/i_was_training_an_ai_model_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyd6fk</id>
    <title>Day 21: 21 Days of Building a Small Language Model: Complete Journey Recap</title>
    <updated>2025-12-29T04:59:01+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No blog today. I created a video instead to recap the journey, just wanted to say a big thank you to everyone for the support. üôè&lt;/p&gt; &lt;p&gt;Video link: &lt;a href="https://youtu.be/-rzMxb1JhuU"&gt;https://youtu.be/-rzMxb1JhuU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I can't believe we've made it to the end together. First, I want to say a massive thank you to everyone who has been following along, reading the blogs, engaging with the content, asking questions, and sharing your own learnings. &lt;/p&gt; &lt;p&gt;This journey has been absolutely incredible, and it wouldn't have been the same without your support and engagement.&lt;/p&gt; &lt;p&gt;Before we wrap up, I want to wish everyone a very Happy New Year! As we close out this year and begin a new one, I'm excited about what's ahead in the world of language models and AI. Until then, happy building!&lt;/p&gt; &lt;p&gt;I‚Äôve added all the links in the first comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T04:59:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjohv</id>
    <title>LM Studio alternative for images / Videos / Audio ?</title>
    <updated>2025-12-29T11:11:00+00:00</updated>
    <author>
      <name>/u/mouseofcatofschrodi</name>
      <uri>https://old.reddit.com/user/mouseofcatofschrodi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With LM Studio (and others alike) it is super easy to run LLMs locally. Ist there anything as easy to create pictures, videos and audios locally using open models?&lt;/p&gt; &lt;p&gt;I tried ComfyUI but didn't find it as easy. With LM Studio I can search for models, see if they will run fast/good with my specs (M3 Pro, 36GB Unified) before downloading them, and in general it is super straight forward.&lt;/p&gt; &lt;p&gt;Two extra questions:&lt;br /&gt; 1. Which models would you recommend for this specs?&lt;br /&gt; 2. For LLMs in Mac, the mlx format makes a huge difference. Is there anything similar for image/video/audio models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mouseofcatofschrodi"&gt; /u/mouseofcatofschrodi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyiqly</id>
    <title>do MoEoE models stand a chance?</title>
    <updated>2025-12-29T10:15:49+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive heard about plans for DeepSeek to make their new models surpass 1 trillion parameter territory, and with them doing that, im sure other labs will too (especially labs like InclusionAI, where &amp;quot;scaling is all you need&amp;quot;)&lt;/p&gt; &lt;p&gt;so that begs the question, *would* and MoEoE model work? as in mixture of experts models that manage even more experts instead of parameters? imagine a 2-3 trillion model only having to decide on 128 experts instead of 2048 to keep low activated params? &lt;/p&gt; &lt;p&gt;i dont know enough about LLMs to answer this question, so id like to ask all of you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T10:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pykkqx</id>
    <title>Looking back at end of 2024 vs now</title>
    <updated>2025-12-29T12:02:18+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been rebuilding a few agent systems recently, and I kept having this vague feeling that everything already feels outdated, even compared to the middle of this year.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;br /&gt; GPT-4o ‚Üí o3 ‚Üí GPT-5.2&lt;br /&gt; Claude 3.5 ‚Üí Claude 3.7 ‚Üí Claude 4.5&lt;br /&gt; Gemini 1.5 ‚Üí Gemini 2.5 ‚Üí Gemini 3&lt;br /&gt; DeepSeek v2 ‚Üí DeepSeek R1 ‚Üí DeepSeek v3&lt;br /&gt; ...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent logic&lt;/strong&gt;&lt;br /&gt; single prompt loop ‚Üí planner / executor split ‚Üí long-running agent with state&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAG / retrieval&lt;/strong&gt;&lt;br /&gt; top-k doc chunks ‚Üí hybrid retrieve + rerank ‚Üí implicit context reads&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;br /&gt; chat history only ‚Üí session + long-term memory ‚Üí stateful memory across runs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tool use&lt;/strong&gt;&lt;br /&gt; function calling JSON ‚Üí structured tool execution ‚Üí permissioned tool calls&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Workflows&lt;/strong&gt;&lt;br /&gt; python scripts / cron ‚Üí visual workflows (agent steps) ‚Üí resumable execution engine&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observability&lt;/strong&gt;&lt;br /&gt; prompt logs ‚Üí agent + tool traces ‚Üí evals tied to deploys&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Protocols / integration&lt;/strong&gt;&lt;br /&gt; custom tool schema per app ‚Üí MCP-style shared interface ‚Üí standardized interface + security boundaries&lt;/p&gt; &lt;p&gt;Curious if others rebuilding systems recently feel the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T12:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pydegt</id>
    <title>Benchmarking local llms for speed with CUDA and vulkan, found an unexpected speedup for select models</title>
    <updated>2025-12-29T05:09:41+00:00</updated>
    <author>
      <name>/u/Amazing_Athlete_2265</name>
      <uri>https://old.reddit.com/user/Amazing_Athlete_2265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was benchmarking my local llm collection to get an idea of tokens rates. I thought it might be interesting to compare CUDA vs Vulkan on my 3080 10GB. As expected, in almost all cases CUDA was the better option as far as token rate However, I found one suprise that affects a small number of models.&lt;/p&gt; &lt;p&gt;Disclaimer: take the following results with a pinch of salt. I'm not a statistician nor mathmetician. I have been programming for some decades but this test code is mostly deslopped jive code. YMMV.&lt;/p&gt; &lt;p&gt;The main findings is that when running certain models partially offloaded to GPU, some models perform much better on Vulkan than CUDA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM4 9B Q6 had a 2.2x speedup on PP, and 1.7x speedup on TG&lt;/li&gt; &lt;li&gt;Qwen3 8B Q6 had a 1.5x speedup on PP, and 1.1x speedup on PP (meh)&lt;/li&gt; &lt;li&gt;and Ministral3 14B 2512 Q4 had a 4.4x speedup on PP, and a 1.6x speedup on TG&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;edit: should add my setup: using latest llama.cpp build. Most ggufs are Unsloth UD. I primarily target models that can produce at least 20t/s. Ryzen 5 something or other, 32GB cheapest DDR4 RAM.&lt;/h2&gt; &lt;p&gt;The following tables only show models that are partially offloaded onto GPU:&lt;/p&gt; &lt;h3&gt;Token generation (tg) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;13.2&lt;/td&gt; &lt;td&gt;-12.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;25.4&lt;/td&gt; &lt;td&gt;44.0&lt;/td&gt; &lt;td&gt;+18.6&lt;/td&gt; &lt;td&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;40.4&lt;/td&gt; &lt;td&gt;21.6&lt;/td&gt; &lt;td&gt;-18.9&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;36.1&lt;/td&gt; &lt;td&gt;57.1&lt;/td&gt; &lt;td&gt;+21.0&lt;/td&gt; &lt;td&gt;1.58x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;23.1&lt;/td&gt; &lt;td&gt;15.9&lt;/td&gt; &lt;td&gt;-7.1&lt;/td&gt; &lt;td&gt;0.69x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;23.7&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;+2.1&lt;/td&gt; &lt;td&gt;1.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;104.3&lt;/td&gt; &lt;td&gt;61.4&lt;/td&gt; &lt;td&gt;-42.9&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;30.4&lt;/td&gt; &lt;td&gt;22.4&lt;/td&gt; &lt;td&gt;-8.0&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;16.4&lt;/td&gt; &lt;td&gt;12.9&lt;/td&gt; &lt;td&gt;-3.5&lt;/td&gt; &lt;td&gt;0.79x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;30.6&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-14.3&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;46.1&lt;/td&gt; &lt;td&gt;23.4&lt;/td&gt; &lt;td&gt;-22.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Prompt processing (pp) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;24.5&lt;/td&gt; &lt;td&gt;13.3&lt;/td&gt; &lt;td&gt;-11.2&lt;/td&gt; &lt;td&gt;0.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;34.0&lt;/td&gt; &lt;td&gt;75.6&lt;/td&gt; &lt;td&gt;+41.6&lt;/td&gt; &lt;td&gt;2.22x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;20.2&lt;/td&gt; &lt;td&gt;-16.8&lt;/td&gt; &lt;td&gt;0.55x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;58.1&lt;/td&gt; &lt;td&gt;255.4&lt;/td&gt; &lt;td&gt;+197.2&lt;/td&gt; &lt;td&gt;4.39x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;21.4&lt;/td&gt; &lt;td&gt;14.0&lt;/td&gt; &lt;td&gt;-7.3&lt;/td&gt; &lt;td&gt;0.66x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;30.3&lt;/td&gt; &lt;td&gt;46.0&lt;/td&gt; &lt;td&gt;+15.8&lt;/td&gt; &lt;td&gt;1.52x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;88.4&lt;/td&gt; &lt;td&gt;55.6&lt;/td&gt; &lt;td&gt;-32.8&lt;/td&gt; &lt;td&gt;0.63x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;28.2&lt;/td&gt; &lt;td&gt;20.9&lt;/td&gt; &lt;td&gt;-7.4&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;72.3&lt;/td&gt; &lt;td&gt;42.5&lt;/td&gt; &lt;td&gt;-29.8&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;29.1&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-12.8&lt;/td&gt; &lt;td&gt;0.56x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;221.9&lt;/td&gt; &lt;td&gt;112.1&lt;/td&gt; &lt;td&gt;-109.8&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Athlete_2265"&gt; /u/Amazing_Athlete_2265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T05:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjjbw</id>
    <title>Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together</title>
    <updated>2025-12-29T11:02:29+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt; &lt;img alt="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" src="https://a.thumbs.redditmedia.com/bB4zUj7vleOqJdnXmwlZ9s4tewjkzGrf2kPk8Dbebv4.jpg" title="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HyperCLOVA X SEED 32B Think: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HyperCLOVA X SEED 8B Omni: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed"&gt;https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Artificial Analysis on ùïè: &lt;a href="https://x.com/ArtificialAnlys/status/2005429176615174207"&gt;https://x.com/ArtificialAnlys/status/2005429176615174207&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyjjbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyao6g</id>
    <title>Meta released RPG, a research plan generation dataset on Hugging Face</title>
    <updated>2025-12-29T02:58:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt; &lt;img alt="Meta released RPG, a research plan generation dataset on Hugging Face" src="https://external-preview.redd.it/_Vt3gVwDJIJ3tdTBBf0E6Y1zVMQL8lOjQzN3Hnt2brY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=062e0684599eddb333c0833911a29ff674bc632c" title="Meta released RPG, a research plan generation dataset on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training AI co-scientists&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/facebook/research-plan-gen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T02:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyg4yt</id>
    <title>Tencent just released WeDLM 8B Instruct on Hugging Face</title>
    <updated>2025-12-29T07:38:43+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt; &lt;img alt="Tencent just released WeDLM 8B Instruct on Hugging Face" src="https://b.thumbs.redditmedia.com/C56gntSOSvM_cfj95m0peqGLfh8p1Tnt02oONhKPwFM.jpg" title="Tencent just released WeDLM 8B Instruct on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/tencent/WeDLM-8B-Instruct"&gt;https://huggingface.co/tencent/WeDLM-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A diffusion language model that runs 3-6√ó faster than vLLM-optimized Qwen3-8B on math reasoning tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyg4yt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T07:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
