<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-18T01:11:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m2ex4z</id>
    <title>[2506.00045] ACE-Step: A Step Towards Music Generation Foundation Model</title>
    <updated>2025-07-17T18:14:12+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was released a month ago for &lt;a href="https://github.com/ace-step/ACE-Step"&gt;https://github.com/ace-step/ACE-Step&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.00045"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ex4z/250600045_acestep_a_step_towards_music_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ex4z/250600045_acestep_a_step_towards_music_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T18:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2mhua</id>
    <title>A full guide on building a secure, local LLM using Linux Mint and an external SSD</title>
    <updated>2025-07-17T23:15:59+00:00</updated>
    <author>
      <name>/u/quarteryudo</name>
      <uri>https://old.reddit.com/user/quarteryudo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've put together a guide on how to build your own secure, private, local LLM with Linux Mint. It uses Podman, Ollama, and AnythingLLM. I made this guide from a beginner's mindset, as I am a writer, not a programmer. Building your own Pokemon team is fully achievable for anyone who has moved to Linux Mint from Windows.&lt;/p&gt; &lt;p&gt;Here are some advantages with this setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Everything is stored on external media. With simple changes this whole setup is transferable between computers.&lt;/li&gt; &lt;li&gt;Everything runs from localhost, meaning a far lower chance of outside interference or monitoring.&lt;/li&gt; &lt;li&gt;The AI itself runs rootless in its own container, meaning even if it ‚Äòbroke out‚Äô it would have no permission to interfere with your system (no ‚Äòsudo‚Äô).&lt;/li&gt; &lt;li&gt;Everything runs via CPU, meaning the only limit is your computer‚Äôs RAM. I might add GPU support later.&lt;/li&gt; &lt;li&gt;The AI is still fully capable of agentic behaviour, including web browsing (if you let it).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/michaelsoftmd/ai-pet-project"&gt;Here is the link to the github&lt;/a&gt; where I have detailed the full instructions.&lt;/p&gt; &lt;p&gt;My website is at &lt;a href="http://www.akickintheteeth.com"&gt;www.akickintheteeth.com&lt;/a&gt; if you are interested in my writing.&lt;/p&gt; &lt;p&gt;Thank you and I hope this guide works for you.&lt;/p&gt; &lt;p&gt;edit: comma&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quarteryudo"&gt; /u/quarteryudo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2mhua/a_full_guide_on_building_a_secure_local_llm_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2mhua/a_full_guide_on_building_a_secure_local_llm_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2mhua/a_full_guide_on_building_a_secure_local_llm_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T23:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2ibq0</id>
    <title>Is it possible to run something like Grok's anime girl companion free, open source, and local?</title>
    <updated>2025-07-17T20:25:28+00:00</updated>
    <author>
      <name>/u/Top-Guava-1302</name>
      <uri>https://old.reddit.com/user/Top-Guava-1302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the same quality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Guava-1302"&gt; /u/Top-Guava-1302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ibq0/is_it_possible_to_run_something_like_groks_anime/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ibq0/is_it_possible_to_run_something_like_groks_anime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ibq0/is_it_possible_to_run_something_like_groks_anime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T20:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m28r3c</id>
    <title>AI devs in NYC ‚Äî heads up about the RAISE Act</title>
    <updated>2025-07-17T14:17:41+00:00</updated>
    <author>
      <name>/u/AI_Alliance</name>
      <uri>https://old.reddit.com/user/AI_Alliance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone in the NYC AI dev space paying attention to the &lt;strong&gt;RAISE Act&lt;/strong&gt;? It‚Äôs a new bill that could shape how AI systems get built and deployed‚Äîespecially open-source stuff.&lt;/p&gt; &lt;p&gt;I‚Äôm attending a virtual meetup today (July 17 @ 12PM ET) to learn more. If you‚Äôre working on agents, LLM stacks, or tool-use pipelines, this might be a good convo to drop in on.&lt;/p&gt; &lt;p&gt;Details + free registration: &lt;a href="https://events.thealliance.ai/how-the-raise-act-affects-you"&gt;https://events.thealliance.ai/how-the-raise-act-affects-you&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hoping it‚Äôll clarify what counts as ‚Äúhigh-risk‚Äù and what role open devs can play in shaping the policy. Might be useful if you're worried about future liability or compliance headache&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI_Alliance"&gt; /u/AI_Alliance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2mdc8</id>
    <title>MCP capable small local models?</title>
    <updated>2025-07-17T23:10:28+00:00</updated>
    <author>
      <name>/u/amunocis</name>
      <uri>https://old.reddit.com/user/amunocis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there! I'm looking for recommendations for a small model that can work ok with an MCP server I'm building for testing purposes. I was trying Mistral but dude, it failed everything lol (or maybe I am the one failing?). I need to test other small models in the size of phi4 or similar. Thanks for the help!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amunocis"&gt; /u/amunocis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2mdc8/mcp_capable_small_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2mdc8/mcp_capable_small_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2mdc8/mcp_capable_small_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T23:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2inuu</id>
    <title>How to use the same context across LLMs and Agents</title>
    <updated>2025-07-17T20:38:30+00:00</updated>
    <author>
      <name>/u/Imad-aka</name>
      <uri>https://old.reddit.com/user/Imad-aka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You know that feeling when you have to explain the same story to five different people?&lt;/p&gt; &lt;p&gt;That‚Äôs been my experience with LLMs so far.&lt;/p&gt; &lt;p&gt;I‚Äôll start a convo with ChatGPT, hit a wall or I am dissatisfied, and switch to Claude for better capabilities. Suddenly, I‚Äôm back at square one, explaining &lt;em&gt;everything&lt;/em&gt; again.&lt;/p&gt; &lt;p&gt;I‚Äôve tried keeping a doc with my context and asking one LLM to help prep for the next. It gets the job done to an extent, but it‚Äôs still far from ideal.&lt;/p&gt; &lt;p&gt;So, I built Windo - a universal context window that lets you share the same context across different LLMs.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;h1&gt;Context adding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;By pulling LLMs discussions on the go&lt;/li&gt; &lt;li&gt;Manually, by uploading files, text, screenshots, voice notes&lt;/li&gt; &lt;li&gt;By connecting data sources (Notion, Linear, Slack...) via MCP&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Context filtering/preparation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Noise removal&lt;/li&gt; &lt;li&gt;A local LLM filters public/private data, so we send only ‚Äúpublic‚Äù data to the server&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We are considering a local first approach. However, with the current state of local models, we can‚Äôt run everything locally; for now we are aiming for a partially local approach but our end goal is to have it fully local.&lt;/p&gt; &lt;h1&gt;Context management&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Context indexing in vector DB&lt;/li&gt; &lt;li&gt;We make sense of the indexed data (context understanding) by generating project artifacts (overview, target users, goals‚Ä¶) to give models a quick summary, not to overwhelm them with a data dump.&lt;/li&gt; &lt;li&gt;Context splitting into separate spaces based on projects, tasks, initiatives‚Ä¶ giving the user granular control and permissions over what to share with different models and agents.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Context retrieval&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;User triggers context retrieval on any model&lt;/li&gt; &lt;li&gt;Based on the user‚Äôs current work, we prepare the needed context, compressed adequately to not overload the target model‚Äôs context window.&lt;/li&gt; &lt;li&gt;Or, the LLMs retrieve what they need via MCP (for models that support it), as Windo acts as an MCP server as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Windo is like your AI‚Äôs USB stick for memory. Plug it into any LLM, and pick up where you left off.&lt;/p&gt; &lt;p&gt;Right now, we‚Äôre testing with early users. If that sounds like something you need, I can share with you the website in the DMs if you ask. Looking for your feedback. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imad-aka"&gt; /u/Imad-aka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2inuu/how_to_use_the_same_context_across_llms_and_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2inuu/how_to_use_the_same_context_across_llms_and_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2inuu/how_to_use_the_same_context_across_llms_and_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T20:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2kjrm</id>
    <title>Thunderbolt vs Oculink</title>
    <updated>2025-07-17T21:53:45+00:00</updated>
    <author>
      <name>/u/mayo551</name>
      <uri>https://old.reddit.com/user/mayo551</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2kjrm/thunderbolt_vs_oculink/"&gt; &lt;img alt="Thunderbolt vs Oculink" src="https://b.thumbs.redditmedia.com/8PP2tiezGMwxBsgyErG3BCe4NaVPiGmj0ikjqATFZ-g.jpg" title="Thunderbolt vs Oculink" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got my first oculink nvme adapter and figured I'd test it out!&lt;/p&gt; &lt;p&gt;Unfortunately, it still bottlenecks on tabbyAPI with tensor parallelism during prompt processing.&lt;/p&gt; &lt;p&gt;This means that any of those nvme x4 adapters, even for a x16 bifurcation, will bottleneck in bandwidth.&lt;/p&gt; &lt;p&gt;Unfortunately, for my use case I frequently reprocess the prompt due to lorebooks on sillytavern.&lt;/p&gt; &lt;p&gt;With that said, still far more usable then Thunderbolt!&lt;/p&gt; &lt;p&gt;So if you're on the fence, yes, oculink is better then thunderbolt. Unfortunately, you may want to consider a server grade motherboard with real pci slots if your use case involves a lot of prompt processing.&lt;/p&gt; &lt;p&gt;These tests are all based on 2 GPU. I don't know what the bandwidth requirements will be like with 4 GPU! I'm going to find out, though.&lt;/p&gt; &lt;p&gt;Pictures:&lt;/p&gt; &lt;p&gt;PCI 4.0 x8 + PCI 4.0 x8:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r3m2ua7s9idf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4f43c143bb031feeb0394093749baacb3f31025e"&gt;PCI 4.0 x8 + PCI 4.0 x8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PCI 4.0 x8 + Thunderbolt (pci 3.0 x4): &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q4pjlgrt9idf1.jpg?width=1890&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0f6f9bef6a01702db7ff868b58a2c0de41c4e55"&gt;PCI 4.0 x8 + Thunderbolt (pci 3.0 x4)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PCI 4.0 x8 + Oculink (pci 4.0 x4):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/met44y6v9idf1.jpg?width=1894&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1b9d9ba720559ff9d0de198a865ca8f46be2b02c"&gt;PCI 4.0 x8 + Oculink (pci 4.0 x4)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayo551"&gt; /u/mayo551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2kjrm/thunderbolt_vs_oculink/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2kjrm/thunderbolt_vs_oculink/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2kjrm/thunderbolt_vs_oculink/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T21:53:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1vf6g</id>
    <title>Kimi K2 on Aider Polyglot Coding Leaderboard</title>
    <updated>2025-07-17T02:07:06+00:00</updated>
    <author>
      <name>/u/aratahikaru5</name>
      <uri>https://old.reddit.com/user/aratahikaru5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"&gt; &lt;img alt="Kimi K2 on Aider Polyglot Coding Leaderboard" src="https://preview.redd.it/wvr0xh2jecdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb91bffcec670286acac6811feb1de48da1d6a7d" title="Kimi K2 on Aider Polyglot Coding Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aratahikaru5"&gt; /u/aratahikaru5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wvr0xh2jecdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T02:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m28oqc</id>
    <title>Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?</title>
    <updated>2025-07-17T14:15:06+00:00</updated>
    <author>
      <name>/u/NataliaShu</name>
      <uri>https://old.reddit.com/user/NataliaShu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt; &lt;img alt="Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?" src="https://b.thumbs.redditmedia.com/DOzsE3kDEUuQywlws06uZepPnsVOAqjt3dXV5HNeD7E.jpg" title="Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, has anyone used LLMs specifically to evaluate translation quality rather than generate translations? I mean using them to catch issues like dropped meaning, inconsistent terminology, awkward phrasing, and so on.&lt;/p&gt; &lt;p&gt;I‚Äôm on a team experimenting with LLMs (GPT-4, Claude, etc.) for automated translation QA. Not to create translations, but to score, flag problems, and suggest batch corrections. The tool we‚Äôre working on is called Alconost.MT/Evaluate, here's what it looks like: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef"&gt;https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm curious: what kinds of metrics or output formats would actually be useful for you guys when comparing translation providers or assessing quality, especially when you can‚Äôt get a full human review? (I‚Äôm old-school enough to believe nothing beats a real linguist‚Äôs eyeballs, but hey, sometimes you gotta trust the bots‚Ä¶ or at least let them do the heavy lifting before the humans jump in.)&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NataliaShu"&gt; /u/NataliaShu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m22w76</id>
    <title>Securing AI Agents with Honeypots, catch prompt injections before they bite</title>
    <updated>2025-07-17T09:20:00+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks üëã &lt;/p&gt; &lt;p&gt;Imagine your AI agent getting hijacked by a prompt-injection attack without you knowing. I'm the founder and maintainer of Beelzebub, an open-source project that hides &amp;quot;honeypot&amp;quot; functions inside your agent using MCP. If the model calls them... üö® BEEP! üö® You get an instant compromise alert, with detailed logs for quick investigations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero false positives: Only real calls trigger the alarm.&lt;/li&gt; &lt;li&gt;Plug-and-play telemetry for tools like Grafana or ELK Stack.&lt;/li&gt; &lt;li&gt;Guard-rails fine-tuning: Every real attack strengthens the guard-rails with human input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Read the full write-up ‚Üí &lt;a href="https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/"&gt;https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;What do you think? Is it a smart defense against AI attacks, or just flashy theater? Share feedback, improvement ideas, or memes. &lt;/p&gt; &lt;p&gt;I'm all ears! üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T09:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2ml3n</id>
    <title>I‚Äôll build an expert AI for your impossible challenge and give it away free - looking for the hardest technical problem you‚Äôve got</title>
    <updated>2025-07-17T23:19:58+00:00</updated>
    <author>
      <name>/u/Prestigious-Fan118</name>
      <uri>https://old.reddit.com/user/Prestigious-Fan118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to test this on something brutal. You give me your hardest technical challenge, I‚Äôll build a specialized AI for it this weekend and release it here for everyone.&lt;/p&gt; &lt;p&gt;What I‚Äôm looking for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extremely niche technical problems&lt;/li&gt; &lt;li&gt;Challenges where current LLMs completely fail&lt;/li&gt; &lt;li&gt;Tasks that normally require 10+ years of expertise&lt;/li&gt; &lt;li&gt;The more ‚Äúimpossible‚Äù the better &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Examples of the difficulty level I want:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI that optimizes CUDA kernels for specific GPU architectures&lt;/li&gt; &lt;li&gt;AI that diagnoses and fixes race conditions in concurrent code&lt;/li&gt; &lt;li&gt;AI that ports assembly between different architectures&lt;/li&gt; &lt;li&gt;AI that generates efficient Vulkan/Metal shaders from descriptions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What happens:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Most upvoted challenge by Friday 6PM EST wins&lt;/li&gt; &lt;li&gt;I build it over the weekend&lt;/li&gt; &lt;li&gt;I come back Monday with the working system&lt;/li&gt; &lt;li&gt;You all get to stress-test it with your edge cases&lt;/li&gt; &lt;li&gt;If it works, everyone gets access to use it&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Not selling anything. Just want to see if this handles your worst problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Fan118"&gt; /u/Prestigious-Fan118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ml3n/ill_build_an_expert_ai_for_your_impossible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ml3n/ill_build_an_expert_ai_for_your_impossible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ml3n/ill_build_an_expert_ai_for_your_impossible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T23:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1sjsn</id>
    <title>MCPS are awesome!</title>
    <updated>2025-07-16T23:52:02+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"&gt; &lt;img alt="MCPS are awesome!" src="https://preview.redd.it/p3766l11qbdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a379b48132b0610b66d0e97e1fa3f988c317315" title="MCPS are awesome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have set up like 17 MCP servers to use with open-webui and local models, and its been amazing!&lt;br /&gt; The ai can decide if it needs to use tools like web search, windows-cli, reddit posts, wikipedia articles.&lt;br /&gt; The usefulness of LLMS became that much bigger!&lt;/p&gt; &lt;p&gt;In the picture above I asked Qwen14B to execute this command in powershell:&lt;/p&gt; &lt;p&gt;python -c &amp;quot;import psutil,GPUtil,json;print(json.dumps({'cpu':psutil.cpu_percent(interval=1),'ram':psutil.virtual_memory().percent,'gpu':[{'name':g.name,'load':g.load*100,'mem_used':g.memoryUsed,'mem_total':g.memoryTotal,'temp':g.temperature} for g in GPUtil.getGPUs()]}))&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p3766l11qbdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T23:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2c9w6</id>
    <title>LLMs Playing Competitive Games Emerge Critical Reasoning: A Latest Study Showing Surprising Results</title>
    <updated>2025-07-17T16:33:53+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"&gt; &lt;img alt="LLMs Playing Competitive Games Emerge Critical Reasoning: A Latest Study Showing Surprising Results" src="https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4fe7497b5d0c0e775a986c8fd1afb9bbc017574a" title="LLMs Playing Competitive Games Emerge Critical Reasoning: A Latest Study Showing Surprising Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0"&gt;https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Self-play has long been a key topic in artificial intelligence research. By allowing AI to compete against itself, researchers have been able to observe the emergence of intelligence. Numerous algorithms have already demonstrated that agents trained through self-play can surpass human experts.&lt;/p&gt; &lt;p&gt;So, what happens if we apply self-play to large language models (LLMs)? Can LLMs become even more intelligent with self-play training?&lt;/p&gt; &lt;p&gt;A recent study conducted by researchers from institutions including the National University of Singapore, Centre for Frontier AI Research (CFAR), Northeastern University, Sea AI Lab, Plastic Labs, and the University of Washington confirms this: LLM agents trained through self-play can significantly enhance their reasoning capabilities!&lt;/p&gt; &lt;p&gt;Read our interpretation of this groundbreaking paper here:&lt;br /&gt; &lt;a href="https://blog.netmind.ai/article/LLMs_Playing_Competitive_Games_Emerge_Critical_Reasoning%3A_A_Latest_Study_Showing_Surprising_Results"&gt;https://blog.netmind.ai/article/LLMs_Playing_Competitive_Games_Emerge_Critical_Reasoning%3A_A_Latest_Study_Showing_Surprising_Results&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2nvpn</id>
    <title>Training an LLM only on books from the 1800's - Update</title>
    <updated>2025-07-18T00:18:59+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"&gt; &lt;img alt="Training an LLM only on books from the 1800's - Update" src="https://b.thumbs.redditmedia.com/nsMpO5S0s6t0aJGmgRVTbKS-Fsyr-akDtUyycEROI9U.jpg" title="Training an LLM only on books from the 1800's - Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple days ago I made a post sharing my experiment training an LLM on only 1800's London text. That post got more attention than I expected and some people have been checking it out on GitHub. So I just wanted to share an update on this project. I trained a second version using 500 books, legal documents, journals, etc. I also expanded the time period to 1800-1875 instead of 1800-1850. This model is now able to produce semi-coherent sentences with almost no modern references. It's no where near an LLM right now, more like a sentence generator but I'm having a lot of fun doing this and gonna keep scaling up. Many people have been giving me good feedback/advice so thank you ! I'm a bit busy right now but once I find the time I will push everything to GitHub.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kxh4l1irzidf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6ba605bf41a988010fd1245efe5f8dd895c4e1"&gt;Output and Hallucinations, Prompt: \&amp;quot;In the autumn of 1847,\&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T00:18:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2o3ht</id>
    <title>Help vote for improved Vulkan performance in ik_llama.cpp</title>
    <updated>2025-07-18T00:29:02+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Came across a discussion in ik_llama.cpp by accident where the main developer (ikawrakow) is soliciting feedback about whether they should focus on improving the performance of the Vulkan backend on ik_llama.cpp.&lt;/p&gt; &lt;p&gt;The discussion is 2 weeks old, but hasn't garnered much attention until now.&lt;/p&gt; &lt;p&gt;I think improved Vulkan performance in this project will benefit the community a lot. As I commented in that discussion, these are my arguments in favor of ikawrakow giving the Vulkan backend more attention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This project doesn't get that much attention on reddit, etc compared to llama.cpp. So, he current userbase is a lot smaller. Having this question in the discussions, while appropriate, won't attract that much attention.&lt;/li&gt; &lt;li&gt;Vulkan is the only backend that's not tied to a specific vendor. Any optimization you make there will be useful on all GPUs, discrete or otherwise. If you can bring Vulkan close to parity with CUDA, it will be a huge win for any device that supports Vulkan, including older GPUs from Nvidia and AMD.&lt;/li&gt; &lt;li&gt;As firecoperana noted, not all quants need to be supported. A handful of the recent IQs used in recent MoE's like Qwen3-235B, DeepSeek-671B, and Kimi-K2 are more than enough. I'd even argue for supporting only power of two IQ quants only initially to limit scope and effort.&lt;/li&gt; &lt;li&gt;Inte's A770 is now arguably the cheapest 16GB GPU with decent compute and memory bandwidth, but it doesn't get much attention in the community. Vulkan support would benefit those of us running Arcs, and free us from having to fiddle with OneAPI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you own AMD or Intel GPUs, I'd urge you to check this discussion and vote in favor of improving Vulkan performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/discussions/590"&gt;Link to the discussion&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T00:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1i922</id>
    <title>He‚Äôs out of line but he‚Äôs right</title>
    <updated>2025-07-16T17:06:31+00:00</updated>
    <author>
      <name>/u/EstablishmentFun3205</name>
      <uri>https://old.reddit.com/user/EstablishmentFun3205</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt; &lt;img alt="He‚Äôs out of line but he‚Äôs right" src="https://preview.redd.it/dqx9wlf3q9df1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d71f6c8f3707ff6aae1011b47babeb593bd890e1" title="He‚Äôs out of line but he‚Äôs right" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EstablishmentFun3205"&gt; /u/EstablishmentFun3205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dqx9wlf3q9df1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:06:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2c4hz</id>
    <title>Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1</title>
    <updated>2025-07-17T16:27:53+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"&gt; &lt;img alt="Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1" src="https://preview.redd.it/in8sapsyngdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f072d23b2d5e641467cb234ea0435163e5f1b18" title="Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/in8sapsyngdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gios</id>
    <title>Given that powerful models like K2 are available cheaply on hosted platforms with great inference speed, are you regretting investing in hardware for LLMs?</title>
    <updated>2025-07-17T19:15:12+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stopped running local models on my Mac a couple of months ago because with my M4 Pro I cannot run very large and powerful models. And to be honest I no longer see the point.&lt;/p&gt; &lt;p&gt;At the moment for example I am using Kimi K2 as default model for basically everything via Groq inference, which is shockingly fast for a 1T params model, and it costs me only $1 per million input tokens and $3 per million output tokens. I mean... seriously, I get the privacy concerns some might have, but if you use LLMs for serious work, not just for playing, it really doesn't make much sense to run local LLMs anymore apart from very simple tasks.&lt;/p&gt; &lt;p&gt;So my question is mainly for those of you who have recently invested quite some chunk of cash in more powerful hardware to run LLMs locally: are you regretting it at all considering what's available on hosted platforms like Groq and OpenRouter and their prices and performance?&lt;/p&gt; &lt;p&gt;Please don't downvote right away. I am not criticizing anyone and until recently I also had some fun running some LLMs locally. I am just wondering if others agree with me that it's no longer convenient when you take performance and cost into account.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2lsbm</id>
    <title>#1 model on Open ASR nvidia/canary-qwen-2.5b is available now</title>
    <updated>2025-07-17T22:45:40+00:00</updated>
    <author>
      <name>/u/SummonerOne</name>
      <uri>https://old.reddit.com/user/SummonerOne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2lsbm/1_model_on_open_asr_nvidiacanaryqwen25b_is/"&gt; &lt;img alt="#1 model on Open ASR nvidia/canary-qwen-2.5b is available now" src="https://external-preview.redd.it/Nn-LD6fffringbEQZP1Qi_wM5thia6kxISdin3VAOxU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38a003a615320ccf2dd25639917dcbbb2e78d2db" title="#1 model on Open ASR nvidia/canary-qwen-2.5b is available now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It showed up on the leaderboard as #1 a couple days ago, and it's finally available now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SummonerOne"&gt; /u/SummonerOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/canary-qwen-2.5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2lsbm/1_model_on_open_asr_nvidiacanaryqwen25b_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2lsbm/1_model_on_open_asr_nvidiacanaryqwen25b_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T22:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1xqv1</id>
    <title>We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models</title>
    <updated>2025-07-17T04:04:21+00:00</updated>
    <author>
      <name>/u/NixTheFolf</name>
      <uri>https://old.reddit.com/user/NixTheFolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt; &lt;img alt="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" src="https://preview.redd.it/zfvdqak3zcdf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a4c2e85087da70112018731aafb9b5d409cf823" title="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NixTheFolf"&gt; /u/NixTheFolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zfvdqak3zcdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T04:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2asou</id>
    <title>Kimi-k2 on lmarena</title>
    <updated>2025-07-17T15:37:09+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt; &lt;img alt="Kimi-k2 on lmarena" src="https://b.thumbs.redditmedia.com/fy5-o3tc0GF-I3bCGJzPb2bsjXpQ9yAyleERp4yhbOw.jpg" title="Kimi-k2 on lmarena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;overall:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e"&gt;https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hard prompts:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392"&gt;https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392&lt;/a&gt;&lt;/p&gt; &lt;p&gt;coding:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a"&gt;https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text"&gt;https://lmarena.ai/leaderboard/text&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T15:37:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gnnk</id>
    <title>Running an open source AI anime girl avatar</title>
    <updated>2025-07-17T19:20:31+00:00</updated>
    <author>
      <name>/u/mapppo</name>
      <uri>https://old.reddit.com/user/mapppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"&gt; &lt;img alt="Running an open source AI anime girl avatar" src="https://external-preview.redd.it/azUzamVqZ3FpaGRmMUstPxAQzeLBZZJeAt5drdnVhSzTD0UR9O7yYNnwsX72.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37334781ea18e9aad3cbc86b76fb7bd676c22989" title="Running an open source AI anime girl avatar" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;after seeing a lot of posts about a certain expensive &amp;amp; cringy anime girlfriend, i wanted to see if there was a better way to get AI avatars. This is from &lt;a href="https://github.com/Open-LLM-VTuber/Open-LLM-VTuber"&gt;https://github.com/Open-LLM-VTuber/Open-LLM-VTuber&lt;/a&gt; (not my work) using 4o API and groq whisper, but it can use any API, or run entirely locally. You can use it with any live2d vtuber, I grabbed a random free one and did not configure the animations right. You can also change the personality prompt as you want. Serving it to mobile devices should work too but I don't care enough to try.&lt;/p&gt; &lt;p&gt;Thoughts? Would you pay for a Grokfriend? Are any of you crazy enough to date your computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mapppo"&gt; /u/mapppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rn1rxkgqihdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2k480</id>
    <title>support for Ernie 4.5 MoE models has been merged into llama.cpp</title>
    <updated>2025-07-17T21:35:47+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2k480/support_for_ernie_45_moe_models_has_been_merged/"&gt; &lt;img alt="support for Ernie 4.5 MoE models has been merged into llama.cpp" src="https://external-preview.redd.it/Xa2nwNvQaZ79M355gwwIuuvaJFK0WjYiA5gWgioi6UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92351043d20a041609005195d5418e8e28968ed6" title="support for Ernie 4.5 MoE models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, only the tiny Ernie model was supported by llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14658"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2k480/support_for_ernie_45_moe_models_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2k480/support_for_ernie_45_moe_models_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T21:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2bigh</id>
    <title>Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat</title>
    <updated>2025-07-17T16:04:03+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"&gt; &lt;img alt="Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New in Le Chat:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Deep Research mode: Lightning fast, structured research reports on even the most complex topics.&lt;/li&gt; &lt;li&gt;Voice mode: Talk to Le Chat instead of typing with our new Voxtral model.&lt;/li&gt; &lt;li&gt;Natively multilingual reasoning: Tap into thoughtful answers, powered by our reasoning model ‚Äî Magistral.&lt;/li&gt; &lt;li&gt;Projects: Organize your conversations into context-rich folders.&lt;/li&gt; &lt;li&gt;Advanced image editing directly in Le Chat, in partnership with Black Forest Labs.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Not local, but much of their underlying models (like Voxtral and Magistral) are, with permissible licenses. For me that makes it worth supporting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/le-chat-dives-deep"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gp16</id>
    <title>Just a reminder that today OpenAI was going to release a SOTA open source model‚Ä¶ until Kimi dropped.</title>
    <updated>2025-07-17T19:22:01+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nothing further, just posting this for the lulz. Kimi is amazing. Who even needs OpenAI at this point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:22:01+00:00</published>
  </entry>
</feed>
