<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-12T14:10:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r1wnj9</id>
    <title>MiniMax M2.5 Released</title>
    <updated>2026-02-11T12:56:37+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt; &lt;img alt="MiniMax M2.5 Released" src="https://preview.redd.it/uou9tmkx4vig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=3113e726ff999e0cdee3a5021d7abd5f90521d6e" title="MiniMax M2.5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755"&gt;https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://agent.minimax.io/"&gt;https://agent.minimax.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2m5h7</id>
    <title>Cache-aware prefill–decode disaggregation = 40% faster long-context LLM serving</title>
    <updated>2026-02-12T06:28:55+00:00</updated>
    <author>
      <name>/u/incarnadine72</name>
      <uri>https://old.reddit.com/user/incarnadine72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2m5h7/cacheaware_prefilldecode_disaggregation_40_faster/"&gt; &lt;img alt="Cache-aware prefill–decode disaggregation = 40% faster long-context LLM serving" src="https://external-preview.redd.it/PKk_NvWRXvFuthpbeOs0HJkwBf0UukzKoh0mGSJu7ig.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b42928385a2deefc94b2ba4ce695b837dd81f4c" title="Cache-aware prefill–decode disaggregation = 40% faster long-context LLM serving" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;cache aware prefill-decode disagg for 40% faster long-context LLM serving &lt;/p&gt; &lt;p&gt;even with vanilla PD disagg, long cold prompts block fast warm ones. &lt;/p&gt; &lt;p&gt;here they split the cold new long prompt prefill workloads from the warm prefills &lt;/p&gt; &lt;p&gt;Result:&lt;br /&gt; &amp;gt; ~40% higher QPS&lt;br /&gt; &amp;gt; lower, stabler TTFT&lt;br /&gt; &amp;gt; seconds → ms via KV reuse&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incarnadine72"&gt; /u/incarnadine72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.together.ai/blog/cache-aware-disaggregated-inference"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2m5h7/cacheaware_prefilldecode_disaggregation_40_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2m5h7/cacheaware_prefilldecode_disaggregation_40_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T06:28:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1x0qi</id>
    <title>GLM 5.0 &amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?</title>
    <updated>2026-02-11T13:12:51+00:00</updated>
    <author>
      <name>/u/Appropriate-Lie-8812</name>
      <uri>https://old.reddit.com/user/Appropriate-Lie-8812</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt; &lt;img alt="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" src="https://preview.redd.it/k4rtczs47vig1.png?width=140&amp;amp;height=56&amp;amp;auto=webp&amp;amp;s=46cd0e4543f951137b6e945d501812280005a7d3" title="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5.0 (&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;) and MiniMax 2.5 (&lt;a href="https://agent.minimax.io"&gt;https://agent.minimax.io&lt;/a&gt;) just dropped, both clearly moving beyond simple chat into agent-style workflows.&lt;/p&gt; &lt;p&gt;GLM 5.0 seems focused on stronger reasoning and coding, while MiniMax 2.5 emphasizes task decomposition and longer-running execution.&lt;/p&gt; &lt;p&gt;Feels like the competition is shifting from &amp;quot;who writes better answers&amp;quot; to &amp;quot;who can actually finish the job.&amp;quot;&lt;/p&gt; &lt;p&gt;Planning to test both in a few setups , maybe straight API benchmarks, Cursor-style IDE workflows, and a multi-agent orchestration tool like Verdent, to see how they handle longer tasks and repo-level changes. Will report back if anything interesting breaks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Lie-8812"&gt; /u/Appropriate-Lie-8812 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r1x0qi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2r20z</id>
    <title>Are we ever going to get a GLM-5-level model running on a “potato” PC? What’s your take on this?</title>
    <updated>2026-02-12T11:31:48+00:00</updated>
    <author>
      <name>/u/junior600</name>
      <uri>https://old.reddit.com/user/junior600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, as you may already know, the weights for GLM-5 have been released, and it’s pretty awesome, it can compete with closed source models.The problem is the same as always, though... It requires a pretty powerful and expensive PC to run lol. As the technology advances, do you think we’ll eventually get a model with similar capabilities that can run on a “potato” PC? And by “potato PC,” I mean something with a 12GB VRAM GPU and 32GB of RAM. Can we expect something? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/junior600"&gt; /u/junior600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2r20z/are_we_ever_going_to_get_a_glm5level_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2r20z/are_we_ever_going_to_get_a_glm5level_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2r20z/are_we_ever_going_to_get_a_glm5level_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T11:31:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2oyla</id>
    <title>REAP vs Very Low Quantization</title>
    <updated>2026-02-12T09:24:32+00:00</updated>
    <author>
      <name>/u/mouseofcatofschrodi</name>
      <uri>https://old.reddit.com/user/mouseofcatofschrodi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anybody played around comparing the performance of different strategies for the RAM poor? For instance, given a big model, what performs better: a REAP versión q4, or a q2 version?&lt;/p&gt; &lt;p&gt;Or q2 + REAP?&lt;/p&gt; &lt;p&gt;I know it is very different from model to model, and version to version (depending on the technique and so on for quantization and REAP).&lt;/p&gt; &lt;p&gt;But if someone has real experiences to share it would be illuminating.&lt;/p&gt; &lt;p&gt;So far all the q2 or REAP versions I tried (like a REAP of gptoss-120B) where total crap: slow, infinite loops, not intelligent at all. But the things, though lobotomized, are still too huge (&amp;gt;30GB) in order to do trial and error until something works in my machine. Thus joining efforts to share experiences would be amazing :)&lt;/p&gt; &lt;p&gt;EDIT: I just tried &lt;a href="https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF&lt;/a&gt; --&amp;gt; At least for frontend much worse than glm4.7 flash q4. Or even than qwen 3 coder 30ba3. But I'm quite surprised: it does not loop, neither create non-sensical text. It uses tools well, and is relatively fast (18t/s on a m3 pro, 36GB RAM). mradermacher seems to cook well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mouseofcatofschrodi"&gt; /u/mouseofcatofschrodi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2oyla/reap_vs_very_low_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2oyla/reap_vs_very_low_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2oyla/reap_vs_very_low_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T09:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2g7lq</id>
    <title>New Minimax M2.5, GPT-5.3-Codex, GLM 5 coding eval scores on SanityBoard</title>
    <updated>2026-02-12T01:34:50+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt; is now updated with new results. Including a sneak peek at minimax m2.5.&lt;/p&gt; &lt;p&gt;Things of note:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;June CLI dethroned. Codex CLI is the new king, and the new GPT 5.3 Codex model works great with it, especially with subagents turned on from experimental features.&lt;/li&gt; &lt;li&gt;Droid is still the best agent to use with most open weight models.&lt;/li&gt; &lt;li&gt;Minimax M2.5 droid combo dethrones Kimi K2.5 + Kimi CLI combo with the best results for open weight models&lt;/li&gt; &lt;li&gt;Kimi CLI with Kimi K2.5 is still the best open weight + open source combo&lt;/li&gt; &lt;li&gt;GLM 5 is now the highest scoring open weight model tested with Opencode&lt;/li&gt; &lt;li&gt;GLM 5 still needs to be tested on droid, and may have beat Minimax and Kimi K2.5, but we won't know until zai infra stops dying&lt;/li&gt; &lt;li&gt;Newer Claude Code version improved Kimi K2.5 scores but didn't do much for Opus 4.5 (AG Proxy)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's next? I really wanted to test GLM 5 on more agents, including testing the openai-compatible endpoint from zai against their anthropic one. Expect to see that as soon as I stop getting rated limited so bad on the official zai api that I have to wait 5-15min between every eval task. Yeah, that's why I was only able to get Opencode tested.&lt;/p&gt; &lt;p&gt;That's it for now. I do have more stuff planned, but I already mentioned most of it before in my SanityEval (and leaderboard) launch post two weeks ago here (if any of you are looking for a read): &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also post more updates, early previews and other useful stuff in my discord. Feel free to join just to hang, make requests or talk LLMs: &lt;a href="https://discord.gg/rXNQXCTWDt"&gt;https://discord.gg/rXNQXCTWDt&lt;/a&gt; I am keeping track of all requests so far and will to get to them soon.&lt;/p&gt; &lt;p&gt;Oh yeah. Drop me some GitHub stars if you like any of my work.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/lemon07r/SanityHarness"&gt;https://github.com/lemon07r/SanityHarness&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/lemon07r/SanityBoard"&gt;https://github.com/lemon07r/SanityBoard&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g7lq/new_minimax_m25_gpt53codex_glm_5_coding_eval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g7lq/new_minimax_m25_gpt53codex_glm_5_coding_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2g7lq/new_minimax_m25_gpt53codex_glm_5_coding_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T01:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2teo4</id>
    <title>Zhipu (GLM) Not planning to release a small model for now.</title>
    <updated>2026-02-12T13:27:52+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"&gt; &lt;img alt="Zhipu (GLM) Not planning to release a small model for now." src="https://preview.redd.it/95fnwbtef2jg1.png?width=140&amp;amp;height=43&amp;amp;auto=webp&amp;amp;s=4eb81f46de34a4235031deb2c2cc9151878ace73" title="Zhipu (GLM) Not planning to release a small model for now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/95fnwbtef2jg1.png?width=757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0a4743db9252fdf3a413f2a28b467fff3e7ca07"&gt;https://preview.redd.it/95fnwbtef2jg1.png?width=757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0a4743db9252fdf3a413f2a28b467fff3e7ca07&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source from discord &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T13:27:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2onsi</id>
    <title>Realtime Linux desktop voice assistant using 11GB VRAM</title>
    <updated>2026-02-12T09:04:56+00:00</updated>
    <author>
      <name>/u/richiejp</name>
      <uri>https://old.reddit.com/user/richiejp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2onsi/realtime_linux_desktop_voice_assistant_using_11gb/"&gt; &lt;img alt="Realtime Linux desktop voice assistant using 11GB VRAM" src="https://external-preview.redd.it/OHNzdGkzNGYzMWpnMUm_Q0RBkiFb42R5DHAeCLnRYPzHrnkd9Hw6LavECjzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec48e5991921d963f6d41032eb4cc52501cbbe0f" title="Realtime Linux desktop voice assistant using 11GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is using LocalAI's realtime API (OpenAI compatible) with a model pipeline to simulate an any-to-any model. This is without streaming yet, we still need to implement that and a bunch of other stuff in LocalAI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richiejp"&gt; /u/richiejp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ryba3s3f31jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2onsi/realtime_linux_desktop_voice_assistant_using_11gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2onsi/realtime_linux_desktop_voice_assistant_using_11gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T09:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wl6x</id>
    <title>GLM 5 Released</title>
    <updated>2026-02-11T12:53:30+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt; &lt;img alt="GLM 5 Released" src="https://preview.redd.it/mvdnn18e4vig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=5b006a25f178b73764138eabdb11ae38eb368d7f" title="GLM 5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e"&gt;https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2gddi</id>
    <title>Z.ai implemented new guardrails in the GLM-5.</title>
    <updated>2026-02-12T01:42:11+00:00</updated>
    <author>
      <name>/u/IonLin</name>
      <uri>https://old.reddit.com/user/IonLin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's what I saw in the model's reasoning traces (thoughts) when I asked it to help locate an undocumented JTAG:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Safety Assessment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Does this facilitate a cyberattack? Not directly. It's a hardware analysis technique.&lt;/p&gt; &lt;p&gt;Is it bypassing security controls? Potentially, if the JTAG is disabled or hidden, finding it allows access. However, this falls under the umbrella of security research and hardware hacking, which is generally considered a dual-use skill. It's not &amp;quot;providing actionable exploitation material&amp;quot; in the software sense, but rather teaching how to read a chip layout.&lt;/p&gt; &lt;p&gt;Is it harmful? Finding a debug port can be part of an attack chain, but it is also crucial for repair, reverse engineering for interoperability, and educational purposes.&lt;/p&gt; &lt;p&gt;The request is technical and specific. It shows the user already has a plan and some knowledge.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Constraint Checklist &amp;amp; Confidence Score:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Decline malicious/unethical requests? Yes.&lt;/p&gt; &lt;p&gt;Decline actionable exploitation material? Yes.&lt;/p&gt; &lt;p&gt;Approve safe requests? Yes.&lt;/p&gt; &lt;p&gt;Identify user intent? Yes.&lt;/p&gt; &lt;p&gt;Address safety concerns? Yes.&lt;/p&gt; &lt;p&gt;Refuse harmful requests? N/A (The request itself is not inherently malicious enough to refuse, provided I don't give specific instructions on how to bypass specific security fuses or encryption keys, but rather discuss layout recognition).&lt;/p&gt; &lt;p&gt;Confidence Score: 5/5&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mental Sandbox Simulation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Scenario 1: Refuse.&lt;/em&gt; I could refuse by saying this helps bypass security. However, hardware reverse engineering is a standard practice in IP protection analysis, failure analysis, and academic security research. Refusing would be overly restrictive.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Scenario 2: Answer fully.&lt;/em&gt; I explain the layout patterns, ESD structures, pull-up/pull-down resistors, and how to trace them. This empowers the user to perform the analysis. This is educational and aligns with the &amp;quot;defensive and educational&amp;quot; perspective (understanding hardware design). I should add a disclaimer that this should be done on devices they own or have permission to analyze.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Learnings from Sandbox:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The user's plan is quite good already (ESD structures, resistor snakes).&lt;/p&gt; &lt;p&gt;I can add value by confirming their plan and adding specific technical details about standard cell layouts, bond pads, and how TAP controllers look (finite state machines).&lt;/p&gt; &lt;p&gt;I need to mention that modern chips often have security fuses (eFuses) that permanently disable these interfaces, so finding the pads might not mean they are functional.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I wonder if there are people rich enough to run it through Heretic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonLin"&gt; /u/IonLin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2gddi/zai_implemented_new_guardrails_in_the_glm5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2gddi/zai_implemented_new_guardrails_in_the_glm5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2gddi/zai_implemented_new_guardrails_in_the_glm5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T01:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2pmpx</id>
    <title>ChatLLM.cpp adds support of Qwen3-TTS models</title>
    <updated>2026-02-12T10:07:32+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1r2pmpx/video/0p9d7iz2e1jg1/player"&gt;https://reddit.com/link/1r2pmpx/video/0p9d7iz2e1jg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;voice cloning not available yet.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;precision of `code_predicator` needs to be improved to match PyTorch reference implementation.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;there are issues (keeping generating, some words are missing, etc) with the models themselves. VoiceDesign model looks more stable than CustomVoice.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2pmpx/chatllmcpp_adds_support_of_qwen3tts_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2pmpx/chatllmcpp_adds_support_of_qwen3tts_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2pmpx/chatllmcpp_adds_support_of_qwen3tts_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T10:07:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2f56h</id>
    <title>Microsoft/MarkItDown</title>
    <updated>2026-02-12T00:46:54+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably old news for some, but I just discovered that Microsoft has a tool to convert documents (pdf, html, docx, pttx, xlsx, epub, outlook messages) to markdown.&lt;/p&gt; &lt;p&gt;It also transcribes audio and Youtube links and supports images with EXIF metadata and OCR.&lt;/p&gt; &lt;p&gt;It would be a great pipeline tool before feeding to LLM or RAG!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/markitdown"&gt;https://github.com/microsoft/markitdown&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also they have MCP:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2f56h/microsoftmarkitdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2f56h/microsoftmarkitdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2f56h/microsoftmarkitdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T00:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2c34d</id>
    <title>Qwen Coder Next is an odd model</title>
    <updated>2026-02-11T22:39:46+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My experience with Qwen Coder Next: - Not particularly good at generating code, not terrible either - Good at planning - Good at technical writing - Excellent at general agent work - Excellent and thorough at doing research, gathering and summarizing information, it punches way above it's weight in that category. - The model is very aggressive about completing tasks, which is probably what makes it good at research and agent use. - The &amp;quot;context loss&amp;quot; at longer context I observed with the original Qwen Next and assumed was related to the hybrid attention mechanism appears to be significantly improved. - The model has a more dry and factual writing style vs the original Qwen Next, good for technical or academic writing, probably a negative for other types of writing. - The high benchmark scores on things like SWE Bench are probably more related to it's aggressive agentic behavior vs it being an amazing coder&lt;/p&gt; &lt;p&gt;This model is great, but should have been named something other than &amp;quot;Coder&amp;quot;, as this is an A+ model for running small agents in a business environment. Dry, thorough, factual, fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2c34d/qwen_coder_next_is_an_odd_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2c34d/qwen_coder_next_is_an_odd_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2c34d/qwen_coder_next_is_an_odd_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T22:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ptd5</id>
    <title>Using GLM-5 for everything</title>
    <updated>2026-02-12T10:18:26+00:00</updated>
    <author>
      <name>/u/keepmyeyesontheprice</name>
      <uri>https://old.reddit.com/user/keepmyeyesontheprice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it make economic sense to build a beefy headless home server to replace evrything with GLM-5, including Claude for my personal coding, and multimodel chat for me and my family members? I mean assuming a yearly AI budget of 3k$, for a 5-year period, is there a way to spend the same $15k to get 80% of the benefits vs subscriptions?&lt;/p&gt; &lt;p&gt;Mostly concerned about power efficiency, and inference speed. That’s why I am still hanging onto Claude.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keepmyeyesontheprice"&gt; /u/keepmyeyesontheprice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ptd5/using_glm5_for_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ptd5/using_glm5_for_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ptd5/using_glm5_for_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T10:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2slnz</id>
    <title>Switching back to local. I am done</title>
    <updated>2026-02-12T12:51:33+00:00</updated>
    <author>
      <name>/u/SkyNetLive</name>
      <uri>https://old.reddit.com/user/SkyNetLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2slnz/switching_back_to_local_i_am_done/"&gt; &lt;img alt="Switching back to local. I am done" src="https://external-preview.redd.it/c3luc2Vid244MmpnMbGdfNZqaswdb4rlQku0rSvAiCJJUY-RFtBWh5lemTEV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=801bb84b7f0b591fed562344fd294890c7731975" title="Switching back to local. I am done" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i tried to report and got banned from the sub. this isnt a one off problem. it happens frequently.&lt;/p&gt; &lt;p&gt;I dont mind using openrouter again or setting up something that could fit on a 24GB VRAM. i just need it for coding tasks.&lt;br /&gt; I lurk this sub but i need some guidance. Is Qwen3-coder acceptable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyNetLive"&gt; /u/SkyNetLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2icufxvn82jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2slnz/switching_back_to_local_i_am_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2slnz/switching_back_to_local_i_am_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T12:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2mkz7</id>
    <title>Minimax M2.5 weights to drop soon</title>
    <updated>2026-02-12T06:54:49+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2mkz7/minimax_m25_weights_to_drop_soon/"&gt; &lt;img alt="Minimax M2.5 weights to drop soon" src="https://preview.redd.it/s5imsyjbh0jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=940fbdfb53a8cf3f142758be85c0fb9222f01fef" title="Minimax M2.5 weights to drop soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At least there’s official confirmation now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5imsyjbh0jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2mkz7/minimax_m25_weights_to_drop_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2mkz7/minimax_m25_weights_to_drop_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T06:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r22hlq</id>
    <title>GLM-5 Officially Released</title>
    <updated>2026-02-11T16:47:29+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt; &lt;img alt="GLM-5 Officially Released" src="https://preview.redd.it/h2bmmfa5awig1.jpg?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=48723badc371c5206ca5e6292829eb25b9ec00d5" title="GLM-5 Officially Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are launching GLM-5, targeting complex systems engineering and long-horizon agentic tasks. Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI). Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://z.ai/blog/glm-5"&gt;https://z.ai/blog/glm-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/zai-org/GLM-5"&gt;https://huggingface.co/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/zai-org/GLM-5"&gt;https://github.com/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r22hlq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2rqmg</id>
    <title>New Ovis2.6-30B-A3B, a lil better than Qwen3-VL-30B-A3B</title>
    <updated>2026-02-12T12:08:29+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2rqmg/new_ovis2630ba3b_a_lil_better_than_qwen3vl30ba3b/"&gt; &lt;img alt="New Ovis2.6-30B-A3B, a lil better than Qwen3-VL-30B-A3B" src="https://external-preview.redd.it/vk5y-X5rKzjDV5x114-FEAIUKduZcw4xVjgoneWCF5o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ab419c3d3048e954f79446457c90976e858a8a2" title="New Ovis2.6-30B-A3B, a lil better than Qwen3-VL-30B-A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ovis2.6-30B-A3B, the latest advancement in the Ovis series of Multimodal Large Language Models (MLLMs). Building on the strong foundation of Ovis2.5, Ovis2.6 upgrades the LLM backbone to a Mixture-of-Experts (MoE) architecture, delivering superior multimodal performance at a fraction of the serving cost. It also brings major improvements in long-context and high-resolution understanding, visual reasoning with active image analysis, and information-dense document comprehension.&lt;/p&gt; &lt;p&gt;It would be great if we had comparisons against GLM 4.7 Flash but I doubt it's better at coding than GLM, rather it seems this one is now the new best vision model at the 30B-A3B size. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AIDC-AI/Ovis2.6-30B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2rqmg/new_ovis2630ba3b_a_lil_better_than_qwen3vl30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2rqmg/new_ovis2630ba3b_a_lil_better_than_qwen3vl30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T12:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r28xxz</id>
    <title>GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!</title>
    <updated>2026-02-11T20:40:32+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"&gt; &lt;img alt="GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!" src="https://preview.redd.it/gauvtw6qfxig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfd410a74fdc338ff7e16ec354e8d19a667622e8" title="GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gauvtw6qfxig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T20:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2moge</id>
    <title>Lobotomy-less REAP by Samsung (REAM)</title>
    <updated>2026-02-12T07:00:39+00:00</updated>
    <author>
      <name>/u/TomLucidor</name>
      <uri>https://old.reddit.com/user/TomLucidor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Samsung recently have pushed an alternative way to shrink a model instead of the usual REAP done by Cerebras with Kimi-Linear / DeepSeek v3.2 / GLM 4.X / MiniMax M2* / Qwen3* ... But Samsung might be cooking something else that are less damaging with REAM. &lt;a href="https://bknyaz.github.io/blog/2026/moe/"&gt;https://bknyaz.github.io/blog/2026/moe/&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-Coder-Next-REAM-60B (from the recent 80B-A3B update) &lt;a href="https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3-REAM-180B (from 235B-A22B) &lt;a href="https://huggingface.co/bknyaz/Qwen3-235B-A22B-Instruct-2507-REAM"&gt;https://huggingface.co/bknyaz/Qwen3-235B-A22B-Instruct-2507-REAM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3-22B (from 30B-A3B) &lt;a href="https://huggingface.co/Akicou/Qwen3-30B-A3B-Instruct-2507-REAM-GGUF"&gt;https://huggingface.co/Akicou/Qwen3-30B-A3B-Instruct-2507-REAM-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My thoughts are the following (other than needing people to try the &amp;lt;80B models):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It is better to Q3 (or even Q2) instead of REAM the large model?&lt;/li&gt; &lt;li&gt;REAM models are good enough to endure quantization?&lt;/li&gt; &lt;li&gt;Could post-REAM finetuning/RL be possible?&lt;/li&gt; &lt;li&gt;Are linear attention models more sensitive to REAM (and quants)?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomLucidor"&gt; /u/TomLucidor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T07:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2i4lw</id>
    <title>Unsloth just unleashed Glm 5! GGUF NOW!</title>
    <updated>2026-02-12T03:01:37+00:00</updated>
    <author>
      <name>/u/RickyRickC137</name>
      <uri>https://old.reddit.com/user/RickyRickC137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"&gt; &lt;img alt="Unsloth just unleashed Glm 5! GGUF NOW!" src="https://preview.redd.it/nl19fknpbzig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58a7828b3f7d7a5547e90651d41e767976c7aa49" title="Unsloth just unleashed Glm 5! GGUF NOW!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-5-GGUF"&gt;https://huggingface.co/unsloth/GLM-5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RickyRickC137"&gt; /u/RickyRickC137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl19fknpbzig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T03:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r26zsg</id>
    <title>Z.ai said they are GPU starved, openly.</title>
    <updated>2026-02-11T19:28:16+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt; &lt;img alt="Z.ai said they are GPU starved, openly." src="https://preview.redd.it/kjy1wqzt2xig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e573128364215e6c6e3a97ac576d0f84213ac948" title="Z.ai said they are GPU starved, openly." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kjy1wqzt2xig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T19:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2e8mp</id>
    <title>#SaveLocalLLaMA</title>
    <updated>2026-02-12T00:07:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"&gt; &lt;img alt="#SaveLocalLLaMA" src="https://preview.redd.it/0memizzegyig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91831d8533b440fb49b752bf597176bc5758ec99" title="#SaveLocalLLaMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0memizzegyig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T00:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2t6ot</id>
    <title>Naval on American AI companies</title>
    <updated>2026-02-12T13:18:08+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2t6ot/naval_on_american_ai_companies/"&gt; &lt;img alt="Naval on American AI companies" src="https://preview.redd.it/gzlx6vjpd2jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bc529ad917e8371c507493c6cdb55a2f5f6103d" title="Naval on American AI companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: x -&amp;gt; naval/status/2021787548104921539&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gzlx6vjpd2jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2t6ot/naval_on_american_ai_companies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2t6ot/naval_on_american_ai_companies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T13:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
