<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-29T07:29:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pxxuib</id>
    <title>Which are the best coding + tooling agent models for vLLM for 128GB memory?</title>
    <updated>2025-12-28T18:02:08+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel a lot of the coding models jump from ~30B class to ~120B to &amp;gt;200B. Is there anything ~100B and a bit under that performs well for vLLM?&lt;/p&gt; &lt;p&gt;Or are ~120B models ok with GGUF or AWQ compression (or maybe 16 FP or Q8_K_XL?)?&lt;/p&gt; &lt;p&gt;Bonus question -- generally if the models are about the same or heavier than the RAM in terms of storage space required for the model (e.g. 120 GB storage), they won't work, right?&lt;/p&gt; &lt;p&gt;Edit: just making side notes here:&lt;/p&gt; &lt;p&gt;Comparing GLM 4.5 Air vs. GPT OSS 120B&lt;/p&gt; &lt;p&gt;Function calling, structured output, and reasoning mode available for both models &lt;a href="https://blog.galaxy.ai/compare/glm-4-5-air-vs-gpt-oss-120b"&gt;https://blog.galaxy.ai/compare/glm-4-5-air-vs-gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Few benchmarks that may be helpful for getting a broad overview &lt;a href="https://artificialanalysis.ai/models/comparisons/gpt-oss-120b-vs-glm-4-5-air"&gt;https://artificialanalysis.ai/models/comparisons/gpt-oss-120b-vs-glm-4-5-air&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxuib/which_are_the_best_coding_tooling_agent_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxuib/which_are_the_best_coding_tooling_agent_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxuib/which_are_the_best_coding_tooling_agent_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T18:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1py9p4r</id>
    <title>Llama 3.2 3B running on my Geekom IT15.</title>
    <updated>2025-12-29T02:13:23+00:00</updated>
    <author>
      <name>/u/mickeybob00</name>
      <uri>https://old.reddit.com/user/mickeybob00</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py9p4r/llama_32_3b_running_on_my_geekom_it15/"&gt; &lt;img alt="Llama 3.2 3B running on my Geekom IT15." src="https://preview.redd.it/v7z4fdqbx1ag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3801c6406b04354d48845d2842569cd992ba4b01" title="Llama 3.2 3B running on my Geekom IT15." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My IT15 has a Intel core ultra 9 285h with 32gb of ram. I am also running Home Assistant on this machine currently. I am still testing things out. I gave this container 6 cores and 16gb and passed the iGpu through. I am going to try other models as well but I am happy that I got it working at all. I am open to suggestions for other models to try out with this machine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mickeybob00"&gt; /u/mickeybob00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v7z4fdqbx1ag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py9p4r/llama_32_3b_running_on_my_geekom_it15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py9p4r/llama_32_3b_running_on_my_geekom_it15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T02:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1py7r06</id>
    <title>Is there any way to use my GPUs?</title>
    <updated>2025-12-29T00:45:37+00:00</updated>
    <author>
      <name>/u/Reasonable-Gold4971</name>
      <uri>https://old.reddit.com/user/Reasonable-Gold4971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Over the last 5 or 6 years, I‚Äôve managed to get a number of second hand GPUs for free from friends when they upgraded theirs. I now have;&lt;/p&gt; &lt;p&gt;3090 (used on my own gaming pc)&lt;/p&gt; &lt;p&gt;2060&lt;/p&gt; &lt;p&gt;2080s&lt;/p&gt; &lt;p&gt;1080ti x2&lt;/p&gt; &lt;p&gt;1080&lt;/p&gt; &lt;p&gt;I also have an opportunity to acquire a very cheap 3070.&lt;/p&gt; &lt;p&gt;Is there any effective way to use these? I currently run Ollama on my main PC with Qwen32b, and might look into WSL later on, but for the rest of them, is there any use in this space or is it not worth the hassle?&lt;/p&gt; &lt;p&gt;I have 3 spare motherboard/CPU/RAM/Cases of varying levels.&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Gold4971"&gt; /u/Reasonable-Gold4971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7r06/is_there_any_way_to_use_my_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7r06/is_there_any_way_to_use_my_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py7r06/is_there_any_way_to_use_my_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T00:45:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1py85zb</id>
    <title>Is it feasible (and beneficial) to apply NVFP4 quantization to KV Cache on Blackwell?</title>
    <updated>2025-12-29T01:03:59+00:00</updated>
    <author>
      <name>/u/No-Bag5084</name>
      <uri>https://old.reddit.com/user/No-Bag5084</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Theoretically, NVFP4 (E2M1 format) should be superior to INT4 for activations. Its logarithmic distribution naturally fits the &amp;quot;long-tailed&amp;quot; nature of KV values (preserving small details while handling outliers via the exponent). Since Blackwell Tensor Cores support native FP4 compute, could we store KV Cache in NVFP4 and perform the Attention operation directly (or with minimal dequantization overhead)?ü§î&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Bag5084"&gt; /u/No-Bag5084 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py85zb/is_it_feasible_and_beneficial_to_apply_nvfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py85zb/is_it_feasible_and_beneficial_to_apply_nvfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py85zb/is_it_feasible_and_beneficial_to_apply_nvfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T01:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyd7h2</id>
    <title>Is there a local alternative to Obsidian + Gemini Cli?</title>
    <updated>2025-12-29T05:00:22+00:00</updated>
    <author>
      <name>/u/StudentFew6429</name>
      <uri>https://old.reddit.com/user/StudentFew6429</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Obsidian to write a game design document. And I use Gemini Cli to take care of the pesky or mundane tasks like finding and replacing a certain keyword, or rewriting certain passages.&lt;/p&gt; &lt;p&gt;That means I need a model + software that can read the files on my PC and do magic.&lt;/p&gt; &lt;p&gt;I've been looking around a lot, but I couldn't find a solution that would let me do the same with a local model, preferably one that can be run on a laptop.&lt;/p&gt; &lt;p&gt;I'll be getting on a flight soon and it would be great if somebody had a suggestion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StudentFew6429"&gt; /u/StudentFew6429 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd7h2/is_there_a_local_alternative_to_obsidian_gemini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd7h2/is_there_a_local_alternative_to_obsidian_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd7h2/is_there_a_local_alternative_to_obsidian_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T05:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxuk38</id>
    <title>Fix for Nvidia Nemotron Nano 3's forced thinking ‚Äì now it can be toggled on and off!</title>
    <updated>2025-12-28T15:51:54+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, everyone,&lt;/p&gt; &lt;p&gt;if you downloaded NVidia Nemotron 3, you are probably aware that the instruction 'detailed thinking off' doesn't work. This is because the automatic Jinja template on Lmstudio has a bug that forces thinking.&lt;/p&gt; &lt;p&gt;However, I'm postining a workaround here: this template has a bugfix which makes thinking on by default, but it can be toggled off by typing /nothink at the system prompt (like you do with Qwen). I pasted it on Pastebin to make this post clean: &lt;a href="https://pastebin.com/y5g3X2Ex"&gt;https://pastebin.com/y5g3X2Ex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T15:51:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1py294m</id>
    <title>GLM 4.5 Air and agentic CLI tools/TUIs?</title>
    <updated>2025-12-28T20:56:33+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I revisited GLM 4.5 Air and at least on llama.cpp I am able to get stable tool calls with unsloth's UD_Q4_K_XL (unsloth updated the weights on HF a couple of days ago); that's probably thanks to: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16932"&gt;https://github.com/ggml-org/llama.cpp/pull/16932&lt;/a&gt; and maybe unsloth (there is no changelog/reason why they recently updated the weights).&lt;/p&gt; &lt;p&gt;Unfortunately with codex-cli sometimes the model becomes stuck at constantly doing the same tool call; maybe it was just bad luck in combination with the set of MCPs, quantization related instability, bad sampling parameters, or there could be some functionality within codex-cli missing to properly engage with GLM 4.5 Air.&lt;/p&gt; &lt;p&gt;Is anyone seriously using GLM 4.5 Air locally for agentic coding (e.g., having it reliably do 10 to 50 tool calls in a single agent round) and has some hints regarding well-working coding TUIs? (ofc I am not expecting that GLM 4.5 Air can solve all tasks, but it imo shouldn't get stuck in tool-calling loops and/or I might be just spoiled by other models not doing that.)&lt;/p&gt; &lt;p&gt;p.s., relevant llama.cpp parameters (derived from unsloth's GLM 4.6V flash docs (no GLM 4.5 Air docs) and temperature recommendation from zai labs):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--ctx-size 128000 --temp 0.6 --top-p 0.6 --top-k 2 --min-p 0.0 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py294m/glm_45_air_and_agentic_cli_toolstuis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py294m/glm_45_air_and_agentic_cli_toolstuis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py294m/glm_45_air_and_agentic_cli_toolstuis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T20:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxtwn2</id>
    <title>Which is the best embedding model for production use?</title>
    <updated>2025-12-28T15:24:55+00:00</updated>
    <author>
      <name>/u/Hari-Prasad-12</name>
      <uri>https://old.reddit.com/user/Hari-Prasad-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've done my research for embedding models for a critical production job. I've read a lot about bge m3 since I can't use a closed source model like text emmedings 3 or something properitry I'm seeking your experience working with these open source models. &lt;/p&gt; &lt;p&gt;To put it simply, which one of these works the best in production:&lt;br /&gt; 1. bge m3&lt;br /&gt; 2. embeddinggemma-300m&lt;br /&gt; 3. qwen3-embedding-0.6b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hari-Prasad-12"&gt; /u/Hari-Prasad-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T15:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1py4nuu</id>
    <title>Self hosting LLM on multi CPU + sys ram combo</title>
    <updated>2025-12-28T22:34:22+00:00</updated>
    <author>
      <name>/u/goodmenthelastwaveby</name>
      <uri>https://old.reddit.com/user/goodmenthelastwaveby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I realised I got two socket supermicro board with two xeon 2690 v3 lying around. I could buy a bunch of ram for it cause it uses 2133 mhz RAM and the used prices to does are not bad.&lt;/p&gt; &lt;p&gt;I was thinking about buying a bunch more sys ram to it and self host larger LLMs, maybe in the future I could run some good models on it.&lt;/p&gt; &lt;p&gt;Do you think it would be able to run in a meaninful speed with lets say 256 gigs of RAM large open source models?&lt;/p&gt; &lt;p&gt;Does anyone have experience with this? What kind of speeds should I expect from it, would it be worthwhile? Maybe if there are better open source models I could also run those for example&lt;/p&gt; &lt;p&gt;I could maybe run qwen3:235b on it for example.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodmenthelastwaveby"&gt; /u/goodmenthelastwaveby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py4nuu/self_hosting_llm_on_multi_cpu_sys_ram_combo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py4nuu/self_hosting_llm_on_multi_cpu_sys_ram_combo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py4nuu/self_hosting_llm_on_multi_cpu_sys_ram_combo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T22:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pycroe</id>
    <title>Reddit, but with multiple LLM agents</title>
    <updated>2025-12-29T04:38:52+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a project I created for fun: &lt;a href="https://redditwithagents.vercel.app/"&gt;https://redditwithagents.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/JFMFBNF.png"&gt;&amp;lt;screenshot&amp;gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's basically a web app that mimic parts of Reddit's UI, allowing you to discuss with LLM agents right in the browswer.&lt;/p&gt; &lt;p&gt;All of the LLM API calls happen in the browser as the app does not have a backend. You can also config the app to use your local LLM APIs as well.&lt;/p&gt; &lt;p&gt;For example, to use LM Studio, make sure you serve the model locally and checked the two options: &amp;quot;Enable CORS&amp;quot; and &amp;quot;Serve on Local Network&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/TfzIjl4.png"&gt;&amp;lt;image&amp;gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then go to the app's settings page, set the following configs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;API URL: http://192.168.&amp;lt;whatever&amp;gt;.&amp;lt;your&amp;gt;:1234/v1 API Key: whatever-key-you-set Model: soemthing like openai/gpt-oss-20b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also check the source code here &lt;a href="https://github.com/huytd/reddit-with-agents/"&gt;https://github.com/huytd/reddit-with-agents/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pycroe/reddit_but_with_multiple_llm_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pycroe/reddit_but_with_multiple_llm_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pycroe/reddit_but_with_multiple_llm_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T04:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyfu01</id>
    <title>Why is sgalng's torch.compile startup so much slower than vLLM?</title>
    <updated>2025-12-29T07:20:53+00:00</updated>
    <author>
      <name>/u/Inside_Camp870</name>
      <uri>https://old.reddit.com/user/Inside_Camp870</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I've been testing torch.compile on SGLang with Gemma 3 12B, and noticed some significant startup time differences compared to vLLM.&lt;/p&gt; &lt;h3&gt;What I'm seeing&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;SGLang without compile: ~1:30 startup&lt;/li&gt; &lt;li&gt;SGLang with compile (bs 1,2,4,8,16): ~6min startup&lt;/li&gt; &lt;li&gt;vLLM with compile enabled (default): ~1min startup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm getting 5-15% perf gains from compile at lower batch sizes (bs &amp;lt; 16), so I'd like to use it‚Äîbut the startup cost is pretty rough.&lt;/p&gt; &lt;h3&gt;details&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;vLLM: &lt;code&gt; vllm serve /root/models/gemma3 \ --tensor-parallel-size 1 \ --max-model-len 2448 \ --gpu-memory-utilization 0.8 \ --max-num-seqs 16 \ --compilation-config '{&amp;quot;cudagraph_capture_sizes&amp;quot;: [1,2,4,8,16]}' &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;sglang: &lt;code&gt; python -m sglang.launch_server \ --model-path /root/models/gemma3 \ --tp 1 \ --context-length 2448 \ --mem-fraction-static 0.8 \ --enable-torch-compile \ --torch-compile-max-bs 16 &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;My guess&lt;/h3&gt; &lt;p&gt;vLLM uses piecewise compilation by default, which is faster than full-graph. In SGLang, compile seems tied to CUDA graph, so piecewise compile only comes with piecewise CUDA graph‚Äîwhose overhead might negate the compile benefits anyway.&lt;/p&gt; &lt;p&gt;I understand &amp;quot;beat torch compile&amp;quot; is the long-term direction(&lt;a href="https://github.com/sgl-project/sglang/issues/4748"&gt;https://github.com/sgl-project/sglang/issues/4748&lt;/a&gt;) and compile isn't really the focus right now. But given the gains I'm seeing on some models, I'm curious: &lt;strong&gt;does anyone know what's actually different between vLLM and SGLang's compile implementations here?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inside_Camp870"&gt; /u/Inside_Camp870 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyfu01/why_is_sgalngs_torchcompile_startup_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyfu01/why_is_sgalngs_torchcompile_startup_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyfu01/why_is_sgalngs_torchcompile_startup_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T07:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pya3v8</id>
    <title>4x V100 32gb vs 2x4x V100 16gb (RPC) in MiniMax M2.1</title>
    <updated>2025-12-29T02:31:50+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pya3v8/4x_v100_32gb_vs_2x4x_v100_16gb_rpc_in_minimax_m21/"&gt; &lt;img alt="4x V100 32gb vs 2x4x V100 16gb (RPC) in MiniMax M2.1" src="https://b.thumbs.redditmedia.com/WrtAP8YSEfbdEmEhAjS8ZTxBrgYH4NQ4o-H3aSHnDCk.jpg" title="4x V100 32gb vs 2x4x V100 16gb (RPC) in MiniMax M2.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tldr: there is a reason for the +300% premium on retail price of V100 32gb SXM2 (~$450) vs V100 16gb variant (~$100).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The result:&lt;/strong&gt;&lt;br /&gt; 9-35% performance increase across the board in favor of Quad V100 32gb.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qv8p6q1312ag1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e71c28705b7e8aa5cd69b4cc044d4db4857bc4d"&gt;https://preview.redd.it/qv8p6q1312ag1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e71c28705b7e8aa5cd69b4cc044d4db4857bc4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cost analysis:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Capex&lt;/strong&gt;: There is an approximately $2800-4000 acquistion cost for an additional Quad SXM2 capable server. The capex alone justifies $350 premium x 4 ($1400 vs $2800-$4000)&lt;br /&gt; &lt;strong&gt;Opex&lt;/strong&gt;: the 2nd server adds a 400w idle and an additional 200w draw during inference, which amounts to $73/mth upto $109.50 on 25 cents KwH. At a blended rate of $90/mth additional. ($350x4)/$90mth = ~15.5 months payback on Opex to not run a 2nd server.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When does RPC make sense:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Free power&lt;/li&gt; &lt;li&gt;Free Equipment&lt;/li&gt; &lt;li&gt;Low idle watts system &amp;amp; GPUs&lt;/li&gt; &lt;li&gt;Cheap system, cheap GPU. I'll be following up with Octominer with 12x P102-100 and 12x CMP 100-210 performance&lt;/li&gt; &lt;li&gt;F8ck it, need the biggest VRAM system possible to run &amp;gt;200b param models, but don't have DGX bucks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;When does Tesla V100 16gb SXM make sense?:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Leveraging $60 chinese PCIE adapter. &lt;ul&gt; &lt;li&gt;If you have a 4U server with passive cooling. Your V100 came with Heatsink - $160 all-in&lt;/li&gt; &lt;li&gt;If you feel comfortable investing time and $50 more for riser, fans and 3D printed shroud. $210 all-in&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The experiment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;128gb VRAM vs 128gb VRAM.&lt;/p&gt; &lt;p&gt;Single Server Quad V100 32gb (4 GPUs) vs Two nodes RPC on 10gbe each with Quad V100 16gb ( 8GPUs).&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ~/llama.cpp/build/bin/llama-server --model ~/model/MiniMax-M2.1-UD-Q3_K_XL-00001-of-00003.gguf --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99 --temp 0.6 --ctx-size 131072 --host 0.0.0.0 --alias MiniMax-M2.1-32g --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;1st Test- small prompt - Single Server Quad V100 32gb&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 2 | task 317 | processing task slot update_slots: id 2 | task 317 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 64 slot update_slots: id 2 | task 317 | n_tokens = 31, memory_seq_rm [31, end) slot update_slots: id 2 | task 317 | prompt processing progress, n_tokens = 64, batch.n_tokens = 33, progress = 1.000000 slot update_slots: id 2 | task 317 | prompt done, n_tokens = 64, batch.n_tokens = 33 slot print_timing: id 2 | task 317 | prompt eval time = 25.61 ms / 33 tokens ( 0.78 ms per token, 1288.31 tokens per second) eval time = 171285.20 ms / 6219 tokens ( 27.54 ms per token, 36.31 tokens per second) total time = 171310.82 ms / 6252 tokens slot release: id 2 | task 317 | stop processing: n_tokens = 6282, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2nd Test- medium prompt - Single Server Quad V100 32gb&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 1 | task 6537 | processing task slot update_slots: id 1 | task 6537 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 468 slot update_slots: id 1 | task 6537 | n_tokens = 0, memory_seq_rm [0, end) slot update_slots: id 1 | task 6537 | prompt processing progress, n_tokens = 468, batch.n_tokens = 468, progress = 1.000000 slot update_slots: id 1 | task 6537 | prompt done, n_tokens = 468, batch.n_tokens = 468 slot print_timing: id 1 | task 6537 | prompt eval time = 2779.12 ms / 468 tokens ( 5.94 ms per token, 168.40 tokens per second) eval time = 90983.24 ms / 2873 tokens ( 31.67 ms per token, 31.58 tokens per second) total time = 93762.36 ms / 3341 tokens slot release: id 1 | task 6537 | stop processing: n_tokens = 3340, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3rd Test- medium/large prompt - Single Server Quad V100 32gb&lt;/p&gt; &lt;p&gt;low&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 3 | task 9704 | processing task slot update_slots: id 3 | task 9704 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 1435 slot update_slots: id 3 | task 9704 | n_tokens = 31, memory_seq_rm [31, end) slot update_slots: id 3 | task 9704 | prompt processing progress, n_tokens = 1435, batch.n_tokens = 1404, progress = 1.000000 slot update_slots: id 3 | task 9704 | prompt done, n_tokens = 1435, batch.n_tokens = 1404 slot print_timing: id 3 | task 9704 | prompt eval time = 7135.25 ms / 1404 tokens ( 5.08 ms per token, 196.77 tokens per second) eval time = 78815.35 ms / 2081 tokens ( 37.87 ms per token, 26.40 tokens per second) total time = 85950.60 ms / 3485 tokens slot release: id 3 | task 9704 | stop processing: n_tokens = 3515, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;high&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 3 | task 0 | processing task slot update_slots: id 3 | task 0 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 1435 slot update_slots: id 3 | task 0 | n_tokens = 0, memory_seq_rm [0, end) slot update_slots: id 3 | task 0 | prompt processing progress, n_tokens = 1435, batch.n_tokens = 1435, progress = 1.000000 slot update_slots: id 3 | task 0 | prompt done, n_tokens = 1435, batch.n_tokens = 1435 slot print_timing: id 3 | task 0 | prompt eval time = 6836.98 ms / 1435 tokens ( 4.76 ms per token, 209.89 tokens per second) eval time = 49533.75 ms / 1930 tokens ( 25.67 ms per token, 38.96 tokens per second) total time = 56370.74 ms / 3365 tokens &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;&lt;/h1&gt; &lt;pre&gt;&lt;code&gt; ~/llama.cpp/build/bin/llama-server --model ~/model/MiniMax-M2.1-UD-Q3_K_XL-00001-of-00003.gguf --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99 --temp 0.6 --ctx-size 131072 --host 0.0.0.0 --rpc 192.168.1.x:50052 --alias MiniMax-M2.1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;1st Test- small prompt - Two nodes RPC each with Quad V100 16gb ( 8GPUs)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 3 | task 2177 | processing task slot update_slots: id 3 | task 2177 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 64 slot update_slots: id 3 | task 2177 | n_tokens = 31, memory_seq_rm [31, end) slot update_slots: id 3 | task 2177 | prompt processing progress, n_tokens = 64, batch.n_tokens = 33, progress = 1.000000 slot update_slots: id 3 | task 2177 | prompt done, n_tokens = 64, batch.n_tokens = 33 slot print_timing: id 3 | task 2177 | prompt eval time = 741.63 ms / 33 tokens ( 22.47 ms per token, 44.50 tokens per second) eval time = 166216.98 ms / 4819 tokens ( 34.49 ms per token, 28.99 tokens per second) total time = 166958.61 ms / 4852 tokens slot release: id 3 | task 2177 | stop processing: n_tokens = 4882, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2nd Test- medium prompt - Two nodes RPC each with Quad V100 16gb ( 8GPUs)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 1 | task 7291 | processing task slot update_slots: id 1 | task 7291 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 468 slot update_slots: id 1 | task 7291 | n_tokens = 0, memory_seq_rm [0, end) slot update_slots: id 1 | task 7291 | prompt processing progress, n_tokens = 468, batch.n_tokens = 468, progress = 1.000000 slot update_slots: id 1 | task 7291 | prompt done, n_tokens = 468, batch.n_tokens = 468 slot print_timing: id 1 | task 7291 | prompt eval time = 3412.67 ms / 468 tokens ( 7.29 ms per token, 137.14 tokens per second) eval time = 97740.80 ms / 2478 tokens ( 39.44 ms per token, 25.35 tokens per second) total time = 101153.47 ms / 2946 tokens slot release: id 1 | task 7291 | stop processing: n_tokens = 2945, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3rd Test- medium/large prompt - Two nodes RPC each with Quad V100 16gb ( 8GPUs)&lt;/p&gt; &lt;p&gt;low&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 2 | task 11895 | processing task slot update_slots: id 2 | task 11895 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 1435 slot update_slots: id 2 | task 11895 | n_tokens = 32, memory_seq_rm [32, end) slot update_slots: id 2 | task 11895 | prompt processing progress, n_tokens = 1435, batch.n_tokens = 1403, progress = 1.000000 slot update_slots: id 2 | task 11895 | prompt done, n_tokens = 1435, batch.n_tokens = 1403 slot print_timing: id 2 | task 11895 | prompt eval time = 7746.55 ms / 1403 tokens ( 5.52 ms per token, 181.11 tokens per second) eval time = 89200.25 ms / 2035 tokens ( 43.83 ms per token, 22.81 tokens per second) total time = 96946.79 ms / 3438 tokens slot release: id 2 | task 11895 | stop processing: n_tokens = 3469, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;high&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 3 | task 0 | processing task slot update_slots: id 3 | task 0 | new prompt, n_ctx_slot = 131072, n_keep = 0, task.n_tokens = 1435 slot update_slots: id 3 | task 0 | n_tokens = 0, memory_seq_rm [0, end) slot update_slots: id 3 | task 0 | prompt processing progress, n_tokens = 1435, batch.n_tokens = 1435, progress = 1.000000 slot update_slots: id 3 | task 0 | prompt done, n_tokens = 1435, batch.n_tokens = 1435 slot print_timing: id 3 | task 0 | prompt eval time = 7808.48 ms / 1435 tokens ( 5.44 ms per token, 183.77 tokens per second) eval time = 75172.41 ms / 2176 tokens ( 34.55 ms per token, 28.95 tokens per second) total time = 82980.89 ms / 3611 tokens slot release: id 3 | task 0 | stop processing: n_tokens = 3610, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pya3v8/4x_v100_32gb_vs_2x4x_v100_16gb_rpc_in_minimax_m21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pya3v8/4x_v100_32gb_vs_2x4x_v100_16gb_rpc_in_minimax_m21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pya3v8/4x_v100_32gb_vs_2x4x_v100_16gb_rpc_in_minimax_m21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T02:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyd6cl</id>
    <title>Good model/method for asking questions about long documents? (12 GB VRAM, 32 GB RAM)</title>
    <updated>2025-12-29T04:58:56+00:00</updated>
    <author>
      <name>/u/hockey-throwawayy</name>
      <uri>https://old.reddit.com/user/hockey-throwawayy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry for the remedial question but I have been playing with this for a while and not had great success. I could use some pointers. &lt;/p&gt; &lt;p&gt;My goal is to put one or more long PDF documents into a local LLM, and ask the system questions about the documents, so it can explain concepts to me. &lt;/p&gt; &lt;p&gt;If it helps, I am talking specifically about game rules -- RPGs and maybe board games too, though those documents would be shorter. So, yeah, I could just read the documents myself, but I thought it would be fun and faster if I could ask the LLM something like, &amp;quot;explain the basic task resolution rules in this game, using the example of a skilled character trying to pick a difficult lock.&amp;quot; &lt;/p&gt; &lt;p&gt;The PDFs can be very long, hundreds of pages, but most of that is useless story material and not game rules. &lt;/p&gt; &lt;p&gt;If I can get it to work for games, then hopefully I can get it to work for cookbooks, too. I have some very long cookbooks and chatting with them could be a lot more useful than flipping pages. &lt;/p&gt; &lt;p&gt;I have been using Anything LLM since it includes some kind of document ingestion, and tried a few models like Gemma 3 12B, Mistral 3 8B, Qwen 3 4B, but didn't love the results. Sometimes, the answers were just... Wrong. But, I also didn't try changing any settings like temperature. &lt;/p&gt; &lt;p&gt;This seems like it should be doable at home, but maybe I am off base... &lt;/p&gt; &lt;p&gt;My hardware is Windows, 12GB 3080 w/ 32 GB RAM. (and I don't mind waiting for an answer, if it's a high quality answer!)&lt;/p&gt; &lt;p&gt;Thanks a bunch if you have any suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hockey-throwawayy"&gt; /u/hockey-throwawayy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6cl/good_modelmethod_for_asking_questions_about_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6cl/good_modelmethod_for_asking_questions_about_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6cl/good_modelmethod_for_asking_questions_about_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T04:58:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxz7f0</id>
    <title>Plamo3 (2B/8B/31B) support has been merged into llama.cpp</title>
    <updated>2025-12-28T18:55:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"&gt; &lt;img alt="Plamo3 (2B/8B/31B) support has been merged into llama.cpp" src="https://external-preview.redd.it/addr5Q-exN6mOW2m8NxyWDisrtP7qSIOjIojUWESLhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=044fee01a3568867e7f945c50edee5a2529bd629" title="Plamo3 (2B/8B/31B) support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PLaMo 3 NICT 31B Base is a 31B model pre-trained on English and Japanese datasets, developed by Preferred Networks, Inc. collaborative with National Institute of Information and Communications Technology, NICT.&lt;/p&gt; &lt;p&gt;PLaMo 3 NICT models adapt a hybrid architecture with Sliding Window Attention (SWA) and Traditional Attetntion layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17304"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T18:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyd6a6</id>
    <title>RTX 6000 Pro + RTX 3090 in one machine?</title>
    <updated>2025-12-29T04:58:51+00:00</updated>
    <author>
      <name>/u/az_6</name>
      <uri>https://old.reddit.com/user/az_6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just able to get my hands on a RTX 6000 Pro 96gb card, and I currently have two 3090s in my machine. Should I keep one of the 3090s in there or should I just make do with the single 6000?&lt;/p&gt; &lt;p&gt;I‚Äôm looking to run GPT-OSS at the best possible quality and speed I can. I‚Äôd also want to try run models that are &amp;gt;96GB, in this case would it better to offload to CPU/RAM or to the other GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/az_6"&gt; /u/az_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6a6/rtx_6000_pro_rtx_3090_in_one_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6a6/rtx_6000_pro_rtx_3090_in_one_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6a6/rtx_6000_pro_rtx_3090_in_one_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T04:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1py3o6p</id>
    <title>Owlex - an MCP server that lets Claude Code consult Codex, Gemini, and OpenCode as a "council"</title>
    <updated>2025-12-28T21:53:48+00:00</updated>
    <author>
      <name>/u/spokv</name>
      <uri>https://old.reddit.com/user/spokv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using Claude Code for a while and wanted a way to get second opinions from other AI coding agents without leaving my workflow. So I built &lt;strong&gt;Owlex&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;What it does:&lt;br /&gt; The killer feature is &lt;strong&gt;council_ask&lt;/strong&gt; - it queries Codex, Gemini, and OpenCode in parallel, then optionally runs a second round where each agent sees the others' answers and revises (or critiques) their response.&lt;/p&gt; &lt;p&gt;council_ask(&amp;quot;Should I use Redis or PostgreSQL for this caching layer?&amp;quot;)&lt;/p&gt; &lt;p&gt;All three agents answer simultaneously (~8s total), then deliberate. You get diverse perspectives without the copy-paste dance between terminals.&lt;/p&gt; &lt;p&gt;Other features:&lt;br /&gt; - Start/resume sessions with each agent individually&lt;br /&gt; - Async task execution with timeouts&lt;br /&gt; - Critique mode - agents actively look for bugs in each other's code suggestions&lt;/p&gt; &lt;p&gt;Example output:&lt;/p&gt; &lt;p&gt;Round 1: querying Codex, Gemini, Opencode...&lt;br /&gt; Codex completed (4.0s)&lt;br /&gt; OpenCode completed (5.6s)&lt;br /&gt; Gemini completed (7.7s)&lt;br /&gt; Round 2: deliberation phase..&lt;/p&gt; &lt;p&gt;Install:&lt;br /&gt; uv tool install git+&lt;a href="https://github.com/agentic-mcp-tools/owlex.git"&gt;https://github.com/agentic-mcp-tools/owlex.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/agentic-mcp-tools/owlex"&gt;https://github.com/agentic-mcp-tools/owlex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spokv"&gt; /u/spokv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py3o6p/owlex_an_mcp_server_that_lets_claude_code_consult/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py3o6p/owlex_an_mcp_server_that_lets_claude_code_consult/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py3o6p/owlex_an_mcp_server_that_lets_claude_code_consult/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T21:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1py7ren</id>
    <title>LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional ‚Äúconstrained ‚Üî expressive‚Äù control direction</title>
    <updated>2025-12-29T00:46:06+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"&gt; &lt;img alt="LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional ‚Äúconstrained ‚Üî expressive‚Äù control direction" src="https://b.thumbs.redditmedia.com/UBQp_mEOBNMo9dMlud16WdE2rF9MrijZEck9seQt-zA.jpg" title="LLaMA-3.2-3B fMRI-style probing: discovering a bidirectional ‚Äúconstrained ‚Üî expressive‚Äù control direction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a small interpretability tool that does fMRI-style visualization and &lt;em&gt;live hidden-state intervention&lt;/em&gt; on local models. While exploring LLaMA-3.2-3B, I noticed one hidden dimension (layer 20, dim ~3039) that consistently stood out across prompts and timesteps.&lt;/p&gt; &lt;p&gt;I then set up a simple Gradio UI to &lt;strong&gt;poke that single dimension during inference&lt;/strong&gt; (via a forward hook) and swept epsilon in both directions.&lt;/p&gt; &lt;p&gt;What I found is that this dimension appears to act as a &lt;strong&gt;global control axis&lt;/strong&gt; rather than encoding specific semantic content.&lt;/p&gt; &lt;h1&gt;Observed behavior (consistent across prompts)&lt;/h1&gt; &lt;p&gt;By varying epsilon on this one dim:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Negative Œµ&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;outputs become restrained, procedural, and instruction-faithful&lt;/li&gt; &lt;li&gt;explanations stick closely to canonical structure&lt;/li&gt; &lt;li&gt;less editorializing or extrapolation&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Positive Œµ&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;outputs become more verbose, narrative, and speculative&lt;/li&gt; &lt;li&gt;the model adds framing, qualifiers, and audience modeling&lt;/li&gt; &lt;li&gt;responses feel ‚Äúless reined in‚Äù even on factual prompts&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Crucially, this holds across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;conversational prompts&lt;/li&gt; &lt;li&gt;factual prompts (chess rules, photosynthesis)&lt;/li&gt; &lt;li&gt;recommendation prompts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The effect is smooth, monotonic, and bidirectional.&lt;/p&gt; &lt;h1&gt;Methods (brief)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Model: LLaMA-3.2-3B-Instruct&lt;/li&gt; &lt;li&gt;Intervention: single hidden dimension modified during forward pass&lt;/li&gt; &lt;li&gt;No gradients, no finetuning, no logit biasing&lt;/li&gt; &lt;li&gt;Visualization frontend in Godot; inference + hooks in PyTorch&lt;/li&gt; &lt;li&gt;All tests run locally; prompts trivially swappable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to share more details if folks are interested.&lt;/p&gt; &lt;h1&gt;Why I‚Äôm posting&lt;/h1&gt; &lt;p&gt;I‚Äôm still very much in the &lt;em&gt;exploratory&lt;/em&gt; phase ‚Äî the goal right now is to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;identify stable control directions&lt;/li&gt; &lt;li&gt;understand their scope&lt;/li&gt; &lt;li&gt;design better tests to separate correlation from load-bearing causality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If people have suggestions for additional sanity checks, ablations, or related work I should read, I‚Äôm all ears.&lt;/p&gt; &lt;p&gt;TIME FOR SCIENCE üß™&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ppyvusqvg1ag1.png?width=1858&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1eb8a6b97091ec0a0bba13e4aad2e00524a826f6"&gt;Dim 3039 just begging to get poked.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w04unfb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f4eeb8b341a12d59173e5338a4ed58db3585500"&gt;https://preview.redd.it/w04unfb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f4eeb8b341a12d59173e5338a4ed58db3585500&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzioukb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae4d71911b14a68805069101be779819d8c97d22"&gt;https://preview.redd.it/rzioukb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae4d71911b14a68805069101be779819d8c97d22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eo1vyeb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdd3f9c990c07bc00f7bf55850fffa2b5934b54f"&gt;https://preview.redd.it/eo1vyeb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdd3f9c990c07bc00f7bf55850fffa2b5934b54f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tangtlb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b86b9c5ae15e3b9d413ddf80f607ec335855436"&gt;https://preview.redd.it/tangtlb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b86b9c5ae15e3b9d413ddf80f607ec335855436&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/38fbskb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3039a2ef5443fe71cfcce0069f74432b92340a2e"&gt;https://preview.redd.it/38fbskb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3039a2ef5443fe71cfcce0069f74432b92340a2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qj2ltnb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bf14c7ca6281a4a4496d39244f4060997715734"&gt;https://preview.redd.it/qj2ltnb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bf14c7ca6281a4a4496d39244f4060997715734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ro7belb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=351f8c93a253fda22a44a4689d97068e434e5c5c"&gt;https://preview.redd.it/ro7belb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=351f8c93a253fda22a44a4689d97068e434e5c5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/305i2mb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dfb05fbed2f9104918898be72fff9663890fc26"&gt;https://preview.redd.it/305i2mb1h1ag1.png?width=1526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dfb05fbed2f9104918898be72fff9663890fc26&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py7ren/llama323b_fmristyle_probing_discovering_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T00:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pybbjg</id>
    <title>I built a local voice assistant that learns new abilities via auto-discovered n8n workflows exposed as tools via MCP (LiveKit + Ollama + n8n)</title>
    <updated>2025-12-29T03:28:35+00:00</updated>
    <author>
      <name>/u/CoreWorxLab</name>
      <uri>https://old.reddit.com/user/CoreWorxLab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released CAAL - a local voice assistant that auto-discovers n8n workflows as tools.&lt;/p&gt; &lt;p&gt;Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (I'm running Ministral-3:8B)&lt;/li&gt; &lt;li&gt;LiveKit for WebRTC&lt;/li&gt; &lt;li&gt;Whisper STT&lt;/li&gt; &lt;li&gt;Kokoro TTS&lt;/li&gt; &lt;li&gt;n8n for tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Key feature: Infinite tool expandability through n8n. Add a workflow, CAAL learns it. It can even build its own tools on command.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Demo Video: &lt;a href="https://www.youtube.com/watch?v=Fcn-qq8OiTA"&gt;youtube.com/watch?v=Fcn-qq8OiTA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/coreworxlab/caal"&gt;github.com/CoreWorxLab/CAAL&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoreWorxLab"&gt; /u/CoreWorxLab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pybbjg/i_built_a_local_voice_assistant_that_learns_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pybbjg/i_built_a_local_voice_assistant_that_learns_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pybbjg/i_built_a_local_voice_assistant_that_learns_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T03:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1py4xp6</id>
    <title>Is Q8 KV cache alright for vision models and high context</title>
    <updated>2025-12-28T22:45:41+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What has your experience been with using q8 KV cache and a vision model?&lt;/p&gt; &lt;p&gt;GLM4.6 V, qwen3VL‚Ä¶&lt;/p&gt; &lt;p&gt;Would you say it‚Äôs good enough or does it ruin outputs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T22:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyd6fk</id>
    <title>Day 21: 21 Days of Building a Small Language Model: Complete Journey Recap</title>
    <updated>2025-12-29T04:59:01+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No blog today. I created a video instead to recap the journey, just wanted to say a big thank you to everyone for the support. üôè&lt;/p&gt; &lt;p&gt;Video link: &lt;a href="https://youtu.be/-rzMxb1JhuU"&gt;https://youtu.be/-rzMxb1JhuU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I can't believe we've made it to the end together. First, I want to say a massive thank you to everyone who has been following along, reading the blogs, engaging with the content, asking questions, and sharing your own learnings. &lt;/p&gt; &lt;p&gt;This journey has been absolutely incredible, and it wouldn't have been the same without your support and engagement.&lt;/p&gt; &lt;p&gt;Before we wrap up, I want to wish everyone a very Happy New Year! As we close out this year and begin a new one, I'm excited about what's ahead in the world of language models and AI. Until then, happy building!&lt;/p&gt; &lt;p&gt;I‚Äôve added all the links in the first comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T04:59:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxss0m</id>
    <title>Senator in Tennessee introduces bill to felonize making AI "act as a companion" or "mirror human interactions"</title>
    <updated>2025-12-28T14:35:58+00:00</updated>
    <author>
      <name>/u/CanineAssBandit</name>
      <uri>https://old.reddit.com/user/CanineAssBandit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.&lt;/p&gt; &lt;p&gt;The bill:&lt;br /&gt; &lt;a href="https://legiscan.com/TN/bill/SB1493/2025"&gt;https://legiscan.com/TN/bill/SB1493/2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quotes from the bill (emphasis mine):&lt;/p&gt; &lt;p&gt;It is an offense for a person to knowingly train artificial intelligence to:&lt;br /&gt; (3) Provide emotional support, &lt;strong&gt;including through open-ended conversations&lt;/strong&gt; with a user;&lt;br /&gt; (4) Develop an emotional relationship with, or otherwise &lt;strong&gt;act as a companion&lt;/strong&gt; to, an individual;&lt;br /&gt; (6) Otherwise act as a sentient human or &lt;strong&gt;mirror interactions that a human user might have with another human user&lt;/strong&gt;, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;&lt;br /&gt; (8) &lt;strong&gt;Simulate a human being&lt;/strong&gt;, including in appearance, voice, or other mannerisms.&lt;/p&gt; &lt;p&gt;&amp;quot;Train&amp;quot;:&lt;br /&gt; (A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of &lt;strong&gt;making decisions based on information or other inputs&lt;/strong&gt; provided to the A.I.&lt;br /&gt; (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CanineAssBandit"&gt; /u/CanineAssBandit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pydegt</id>
    <title>Benchmarking local llms for speed with CUDA and vulkan, found an unexpected speedup for select models</title>
    <updated>2025-12-29T05:09:41+00:00</updated>
    <author>
      <name>/u/Amazing_Athlete_2265</name>
      <uri>https://old.reddit.com/user/Amazing_Athlete_2265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was benchmarking my local llm collection to get an idea of tokens rates. I thought it might be interesting to compare CUDA vs Vulkan on my 3080 10GB. As expected, in almost all cases CUDA was the better option as far as token rate However, I found one suprise that affects a small number of models.&lt;/p&gt; &lt;p&gt;Disclaimer: take the following results with a pinch of salt. I'm not a statistician nor mathmetician. I have been programming for some decades but this test code is mostly deslopped jive code. YMMV.&lt;/p&gt; &lt;p&gt;The main findings is that when running certain models partially offloaded to GPU, some models perform much better on Vulkan than CUDA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM4 9B Q6 had a 2.2x speedup on PP, and 1.7x speedup on TG&lt;/li&gt; &lt;li&gt;Qwen3 8B Q6 had a 1.5x speedup on PP, and 1.1x speedup on PP (meh)&lt;/li&gt; &lt;li&gt;and Ministral3 14B 2512 Q4 had a 4.4x speedup on PP, and a 1.6x speedup on TG&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;edit: should add my setup: using latest llama.cpp build. Most ggufs are Unsloth UD. I primarily target models that can produce at least 20t/s. Ryzen 5 something or other, 32GB cheapest DDR4 RAM.&lt;/h2&gt; &lt;p&gt;The following tables only show models that are partially offloaded onto GPU:&lt;/p&gt; &lt;h3&gt;Token generation (tg) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;13.2&lt;/td&gt; &lt;td&gt;-12.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;25.4&lt;/td&gt; &lt;td&gt;44.0&lt;/td&gt; &lt;td&gt;+18.6&lt;/td&gt; &lt;td&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;40.4&lt;/td&gt; &lt;td&gt;21.6&lt;/td&gt; &lt;td&gt;-18.9&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;36.1&lt;/td&gt; &lt;td&gt;57.1&lt;/td&gt; &lt;td&gt;+21.0&lt;/td&gt; &lt;td&gt;1.58x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;23.1&lt;/td&gt; &lt;td&gt;15.9&lt;/td&gt; &lt;td&gt;-7.1&lt;/td&gt; &lt;td&gt;0.69x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;23.7&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;+2.1&lt;/td&gt; &lt;td&gt;1.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;104.3&lt;/td&gt; &lt;td&gt;61.4&lt;/td&gt; &lt;td&gt;-42.9&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;30.4&lt;/td&gt; &lt;td&gt;22.4&lt;/td&gt; &lt;td&gt;-8.0&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;16.4&lt;/td&gt; &lt;td&gt;12.9&lt;/td&gt; &lt;td&gt;-3.5&lt;/td&gt; &lt;td&gt;0.79x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;30.6&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-14.3&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;46.1&lt;/td&gt; &lt;td&gt;23.4&lt;/td&gt; &lt;td&gt;-22.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Prompt processing (pp) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;24.5&lt;/td&gt; &lt;td&gt;13.3&lt;/td&gt; &lt;td&gt;-11.2&lt;/td&gt; &lt;td&gt;0.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;34.0&lt;/td&gt; &lt;td&gt;75.6&lt;/td&gt; &lt;td&gt;+41.6&lt;/td&gt; &lt;td&gt;2.22x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;20.2&lt;/td&gt; &lt;td&gt;-16.8&lt;/td&gt; &lt;td&gt;0.55x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;58.1&lt;/td&gt; &lt;td&gt;255.4&lt;/td&gt; &lt;td&gt;+197.2&lt;/td&gt; &lt;td&gt;4.39x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;21.4&lt;/td&gt; &lt;td&gt;14.0&lt;/td&gt; &lt;td&gt;-7.3&lt;/td&gt; &lt;td&gt;0.66x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;30.3&lt;/td&gt; &lt;td&gt;46.0&lt;/td&gt; &lt;td&gt;+15.8&lt;/td&gt; &lt;td&gt;1.52x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;88.4&lt;/td&gt; &lt;td&gt;55.6&lt;/td&gt; &lt;td&gt;-32.8&lt;/td&gt; &lt;td&gt;0.63x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;28.2&lt;/td&gt; &lt;td&gt;20.9&lt;/td&gt; &lt;td&gt;-7.4&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;72.3&lt;/td&gt; &lt;td&gt;42.5&lt;/td&gt; &lt;td&gt;-29.8&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;29.1&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-12.8&lt;/td&gt; &lt;td&gt;0.56x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;221.9&lt;/td&gt; &lt;td&gt;112.1&lt;/td&gt; &lt;td&gt;-109.8&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Athlete_2265"&gt; /u/Amazing_Athlete_2265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T05:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyao6g</id>
    <title>Meta released RPG, a research plan generation dataset on Hugging Face</title>
    <updated>2025-12-29T02:58:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt; &lt;img alt="Meta released RPG, a research plan generation dataset on Hugging Face" src="https://external-preview.redd.it/_Vt3gVwDJIJ3tdTBBf0E6Y1zVMQL8lOjQzN3Hnt2brY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=062e0684599eddb333c0833911a29ff674bc632c" title="Meta released RPG, a research plan generation dataset on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training AI co-scientists&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/facebook/research-plan-gen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T02:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
