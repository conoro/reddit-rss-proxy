<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-13T11:15:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r2wcbr</id>
    <title>NeuTTS Nano Multilingual Collection: 120M Params on-device TTS in German, French, and Spanish</title>
    <updated>2026-02-12T15:25:57+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wcbr/neutts_nano_multilingual_collection_120m_params/"&gt; &lt;img alt="NeuTTS Nano Multilingual Collection: 120M Params on-device TTS in German, French, and Spanish" src="https://external-preview.redd.it/b2JtcjE2dGUwM2pnMeVwwjyNKdPH51Be4sQFZ3EXv8ZdpH_FAux6dp67XSVh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71cc49b609d4edb7b1971eb68c5f04b379e1333b" title="NeuTTS Nano Multilingual Collection: 120M Params on-device TTS in German, French, and Spanish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we're the team behind NeuTTS (Neuphonic). Some of you may have seen our previous releases of NeuTTS Air and NeuTTS Nano.&lt;/p&gt; &lt;p&gt;The most requested feature by far has been multilingual support, so today we're releasing three new language-specific Nano models: German, French, and Spanish.&lt;/p&gt; &lt;p&gt;Quick specs:&lt;/p&gt; &lt;p&gt;120M active parameters (same as Nano English)&lt;/p&gt; &lt;p&gt;Real-time inference on CPU via llama.cpp / llama-cpp-python&lt;/p&gt; &lt;p&gt;GGUF format (Q4 and Q8 quantizations available)&lt;/p&gt; &lt;p&gt;Zero-shot voice cloning from ~3 seconds of reference audio, works across all supported languages&lt;/p&gt; &lt;p&gt;Runs on laptops, phones, Raspberry Pi, Jetson&lt;/p&gt; &lt;p&gt;Fully local, nothing leaves the device&lt;/p&gt; &lt;p&gt;Architecture: Same as Nano English. Compact LM backbone + NeuCodec (our open-source neural audio codec, single codebook, 50hz). Each language has its own dedicated model for best quality.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;üá©üá™ German: &lt;a href="https://huggingface.co/neuphonic/neutts-nano-german"&gt;https://huggingface.co/neuphonic/neutts-nano-german&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üá´üá∑ French: &lt;a href="https://huggingface.co/neuphonic/neutts-nano-french"&gt;https://huggingface.co/neuphonic/neutts-nano-french&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üá™üá∏ Spanish: &lt;a href="https://huggingface.co/neuphonic/neutts-nano-spanish"&gt;https://huggingface.co/neuphonic/neutts-nano-spanish&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF Spaces: &lt;a href="https://huggingface.co/spaces/neuphonic/neutts-nano-multilingual-collection"&gt;https://huggingface.co/spaces/neuphonic/neutts-nano-multilingual-collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/neuphonic/neutts"&gt;https://github.com/neuphonic/neutts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Each model is a separate HF repo. Same install process as the English Nano, just swap the backbone repo path.&lt;/p&gt; &lt;p&gt;We're working on more languages. If there's a specific one you'd like to see next, let us know. Happy to answer any questions about the architecture, benchmarks, or deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ey2c253c03jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wcbr/neutts_nano_multilingual_collection_120m_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wcbr/neutts_nano_multilingual_collection_120m_params/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T15:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2wgzx</id>
    <title>Alibaba Open-Sources Zvec</title>
    <updated>2026-02-12T15:30:47+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Alibaba Open-Sources Zvec: An Embedded Vector Database Bringing SQLite-like Simplicity and High-Performance On-Device RAG to Edge Applications&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/alibaba/zvec"&gt;https://github.com/alibaba/zvec&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wgzx/alibaba_opensources_zvec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wgzx/alibaba_opensources_zvec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wgzx/alibaba_opensources_zvec/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T15:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r38sk4</id>
    <title>Tutorial: Run GLM-5 on your local device!</title>
    <updated>2026-02-12T23:16:58+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r38sk4/tutorial_run_glm5_on_your_local_device/"&gt; &lt;img alt="Tutorial: Run GLM-5 on your local device!" src="https://preview.redd.it/1047rus1c2jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ac510ad3fce88ba001e3fb74a065f704bef4bbe" title="Tutorial: Run GLM-5 on your local device!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1047rus1c2jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r38sk4/tutorial_run_glm5_on_your_local_device/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r38sk4/tutorial_run_glm5_on_your_local_device/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T23:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2vs3s</id>
    <title>Is this true? GLM 5 was trained solely using huawei hardware and their mindspore framework</title>
    <updated>2026-02-12T15:04:43+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2vs3s/is_this_true_glm_5_was_trained_solely_using/"&gt; &lt;img alt="Is this true? GLM 5 was trained solely using huawei hardware and their mindspore framework" src="https://preview.redd.it/7q0za97mw2jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87dc6fe48350fd9ca4d5b62b500e942dea9fd500" title="Is this true? GLM 5 was trained solely using huawei hardware and their mindspore framework" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only confirmed model to be 100% trained on huawei cards before GLM 5 was GLM image, solely trained on huawei hardware and mindspore infrastructure as of &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; official statements &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.trendingtopics.eu/glm-5-the-worlds-strongest-open-source-llm-solely-trained-on-chinese-huawei-chips/"&gt;https://www.trendingtopics.eu/glm-5-the-worlds-strongest-open-source-llm-solely-trained-on-chinese-huawei-chips/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I find it kind of astonishing, impressed af, note it that formal technical paper has been released by Z.ai for glm 5 So.. we still don't know if it's 100% true or not but the article says so They said it was solely trained on huawei ascend using their own mindspore framework (complete pipeline training to inference) This is so big because glm 5 has literally beaten gemini 3 pro, opus 4.5 and gpt 5.2, on the third spot behind by both opus 4.6 variants and gpt 5.2 xhigh&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7q0za97mw2jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2vs3s/is_this_true_glm_5_was_trained_solely_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2vs3s/is_this_true_glm_5_was_trained_solely_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T15:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2zzp3</id>
    <title>''The MiniMax M2.5 model weights will be open-sourced on HuggingFace'' - from the official MiniMax account on X</title>
    <updated>2026-02-12T17:42:39+00:00</updated>
    <author>
      <name>/u/Bestlife73</name>
      <uri>https://old.reddit.com/user/Bestlife73</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"&gt; &lt;img alt="''The MiniMax M2.5 model weights will be open-sourced on HuggingFace'' - from the official MiniMax account on X" src="https://preview.redd.it/z51pi23wo3jg1.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=1596dabe8ec059a61cf08d2afbd3c143c780427e" title="''The MiniMax M2.5 model weights will be open-sourced on HuggingFace'' - from the official MiniMax account on X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open source release confirmed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/MiniMax_AI/status/2022001452131221872"&gt;MiniMax (official) on X: &amp;quot;MiniMax M2.5: Faster. Stronger. Smarter. Built for Real-World Productivity.&amp;quot; / X&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z51pi23wo3jg1.png?width=942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30dd0075f7f3ddafccf30cf06e3ec35ad2401729"&gt;https://preview.redd.it/z51pi23wo3jg1.png?width=942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30dd0075f7f3ddafccf30cf06e3ec35ad2401729&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bestlife73"&gt; /u/Bestlife73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:42:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r30e3y</id>
    <title>Hibiki-Zero, real-time speech translation model by Kyutai Labs</title>
    <updated>2026-02-12T17:57:12+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30e3y/hibikizero_realtime_speech_translation_model_by/"&gt; &lt;img alt="Hibiki-Zero, real-time speech translation model by Kyutai Labs" src="https://external-preview.redd.it/eWp0b2k3c2JyM2pnMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=910b5f94278a58d986c5730c51938aee29bf0b52" title="Hibiki-Zero, real-time speech translation model by Kyutai Labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like another banger from Kyutai!&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/kyutai/hibiki-zero-3b-pytorch-bf16"&gt;https://huggingface.co/kyutai/hibiki-zero-3b-pytorch-bf16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://kyutai.org/blog/2026-02-12-hibiki-zero"&gt;https://kyutai.org/blog/2026-02-12-hibiki-zero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More samples: &lt;a href="https://huggingface.co/spaces/kyutai/hibiki-zero-samples"&gt;https://huggingface.co/spaces/kyutai/hibiki-zero-samples&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gm1dy5sbr3jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30e3y/hibikizero_realtime_speech_translation_model_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r30e3y/hibikizero_realtime_speech_translation_model_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3febo</id>
    <title>What would you do (Local ‚Äúai‚Äù workstation)</title>
    <updated>2026-02-13T04:18:02+00:00</updated>
    <author>
      <name>/u/NextSalamander6178</name>
      <uri>https://old.reddit.com/user/NextSalamander6178</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are the specs :&lt;/p&gt; &lt;p&gt;- RTX PRO 6000 (96gb vram)&lt;/p&gt; &lt;p&gt;- 512 GB ram ( 16x32 GB DDR5 4800 MT/s RDIMM)&lt;/p&gt; &lt;p&gt;- Intel Xeon w7-3455 (24 cores 4.8 GHz)&lt;/p&gt; &lt;p&gt;What model would you run? What benchmarks would you want to see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NextSalamander6178"&gt; /u/NextSalamander6178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3febo/what_would_you_do_local_ai_workstation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3febo/what_would_you_do_local_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3febo/what_would_you_do_local_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T04:18:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3kzz2</id>
    <title>How is the quality of recent TTS ?</title>
    <updated>2026-02-13T09:42:10+00:00</updated>
    <author>
      <name>/u/TheRealistDude</name>
      <uri>https://old.reddit.com/user/TheRealistDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think of quality of recent TTS like Moss TTS , Mio TTS? &lt;/p&gt; &lt;p&gt;Are these better than Qwen3 or something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealistDude"&gt; /u/TheRealistDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzz2/how_is_the_quality_of_recent_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzz2/how_is_the_quality_of_recent_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzz2/how_is_the_quality_of_recent_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2xjwp</id>
    <title>Ring-1T-2.5 released by inclusionAI</title>
    <updated>2026-02-12T16:12:00+00:00</updated>
    <author>
      <name>/u/Bestlife73</name>
      <uri>https://old.reddit.com/user/Bestlife73</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xjwp/ring1t25_released_by_inclusionai/"&gt; &lt;img alt="Ring-1T-2.5 released by inclusionAI" src="https://external-preview.redd.it/tM5lqBklywlEgOf58rJ0Tjpu5co9UMoZ2A7rXbdJMJU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=315ed6672b4c954402baf403f2d93d36865cb7ca" title="Ring-1T-2.5 released by inclusionAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SOTA performance on deep thinking&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bestlife73"&gt; /u/Bestlife73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-2.5-FP8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xjwp/ring1t25_released_by_inclusionai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xjwp/ring1t25_released_by_inclusionai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3aod7</id>
    <title>Qwen3 Coder Next : Loop Fix</title>
    <updated>2026-02-13T00:36:48+00:00</updated>
    <author>
      <name>/u/TBG______</name>
      <uri>https://old.reddit.com/user/TBG______</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;My Optimal llama.cpp Settings for Qwen3-Coder-Next After 1 Day of Testing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As many of you have noted, the new Qwen3 Next models tend to get stuck in repetitive loops quite frequently. Additionally, both the coder and instruct variants with standard temperature settings can be overly creative - often initiating new tasks without being asked. For example, when you request &amp;quot;change the this in A,&amp;quot; it might decide to change multiple other Leters as well, which isn't always what we need.&lt;/p&gt; &lt;p&gt;After a full day of testing, I've found these settings work best for Qwen3-Coder-Next with llama.cpp to prevent loops and reduce unwanted creativity:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# This is the Loop Fix --temp 0.8 # default 1 was to creative for me --top-p 0.95 --min-p 0.01 --top-k 40 --presence-penalty 1.10 --dry-multiplier 0.5 --dry-allowed-length 5 --frequency_penalty 0.5&amp;quot; # This is for my system and Qwen3-Coder-Next-MXFP4_MOE so it fits all in my 2 GPUs with ctx 256k --cache-type-k q8_0 --cache-type-v q8_0 --threads 64 --threads-batch 64 --n-gpu-layers 999 ( you can just use --fit on) --n-cpu-moe 0 ( you can just use --fit on) --batch-size 2048 --ubatch-size 512&amp;quot; --parallel 1 # And the rest --model %MODEL% --alias %ALIAS% --host 0.0.0.0 --port 8080 --ctx-size %CTX% --jinja --flash-attn on --context-shift --cache-ram -1 (optional unlimited ram for cache ) Select ctx-size: 1) 32768 (32k) 2) 65536 (64k) 3) 98304 (96k) 4) 131072 (128k) 5) 180224 (180k) 6) 196608 (196K) 7) 202752 (200k) 8) 262144 (256k) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These parameters help keep the model focused on the actual task without going off on tangents or getting stuck repeating itself.&lt;/p&gt; &lt;p&gt;Stats: promt 1400 t/s | gen 30-38 t/s Windows WSL (way faster in wsl than in windos native 24 to 28 t/s) 3090RTX +5090RTX&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TBG______"&gt; /u/TBG______ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35ceo</id>
    <title>GLM-5 and Minimax-2.5 on Fiction.liveBench</title>
    <updated>2026-02-12T21:01:32+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"&gt; &lt;img alt="GLM-5 and Minimax-2.5 on Fiction.liveBench" src="https://preview.redd.it/4390rts4o4jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72aeaba258795dc87fe96ebe0ff21b86947a9bfd" title="GLM-5 and Minimax-2.5 on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4390rts4o4jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T21:01:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35326</id>
    <title>I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo</title>
    <updated>2026-02-12T20:51:36+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"&gt; &lt;img alt="I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo" src="https://external-preview.redd.it/YmJmc3hpcWVtNGpnMZDShp7-xGpOcgsVOxorkEUrrQwTSNVbCBVhROxXE8sP.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6045e2eeee4aab8952d17984a247e68ed71ffb61" title="I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/txyz48qem4jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T20:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3jadj</id>
    <title>Improving LLM's coding ability through a new edit format</title>
    <updated>2026-02-13T07:53:36+00:00</updated>
    <author>
      <name>/u/Mushoz</name>
      <uri>https://old.reddit.com/user/Mushoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"&gt; &lt;img alt="Improving LLM's coding ability through a new edit format" src="https://external-preview.redd.it/_HtanEVWgmWOk8SpjQcvTfNBYkpegEjBayvVrK7UD5E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e44168f7cbe68411e3b5b140f2f465f516bb44d0" title="Improving LLM's coding ability through a new edit format" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mushoz"&gt; /u/Mushoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.can.ac/2026/02/12/the-harness-problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T07:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r30bgz</id>
    <title>Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation</title>
    <updated>2026-02-12T17:54:35+00:00</updated>
    <author>
      <name>/u/bobeeeeeeeee8964</name>
      <uri>https://old.reddit.com/user/bobeeeeeeeee8964</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"&gt; &lt;img alt="Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation" src="https://external-preview.redd.it/_l8FEwEfj_HhNLZzTpSlQTuaBTUdY25FimxgyYeDN_Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d3dea77c3801617c49eea3f48b88a3ec1ddde3" title="Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ant Group just open-sourced Ming-flash-omni-2.0, a true (omni-modal) model: image + text + video + audio input ‚Üí image + text + audio output, all in one unified architecture. Looks realy interesting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobeeeeeeeee8964"&gt; /u/bobeeeeeeeee8964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r397hi</id>
    <title>Step 3.5 Flash is a beast?</title>
    <updated>2026-02-12T23:33:47+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have not used it on serious tasks until today.&lt;/p&gt; &lt;p&gt;I gave it a complex task of merging, it worked through it and stayed completely sane even at 90k context and successfully finished the task. It felt so gut, I double checked that I am not running a closed source frontier model like claude 4.6.&lt;/p&gt; &lt;p&gt;I mean, for agentic tasks, this is definitely better than Gemini 3.0 Preview. And it's so fast.&lt;/p&gt; &lt;p&gt;I tested it on opencode and claude code (I don't use it, just wanted to see how flexible it is, and also found out setting up non anthropic model is a pain in the ass) and it did great in both.&lt;/p&gt; &lt;p&gt;What is your experience? Do we have open weight model that is in real world tasks better than gemini 3.0 pro?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T23:33:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ygac</id>
    <title>Why do we allow "un-local" content</title>
    <updated>2026-02-12T16:45:57+00:00</updated>
    <author>
      <name>/u/JacketHistorical2321</name>
      <uri>https://old.reddit.com/user/JacketHistorical2321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title somewhat says it all. I get that it's related but if links to new models are being discussed shouldn't it be a requirement that there be a &amp;quot;local&amp;quot; component?&lt;/p&gt; &lt;p&gt;Edit: since this is starting to get some traction I want to be a little more specific with what I'm talking about. &lt;/p&gt; &lt;p&gt;In the past 2 to 3 days we've seen multiple posts related to new models being released. They include links to API resources prior to weights being released. &lt;/p&gt; &lt;p&gt;I believe that if a post includes a link to API serving hosts then it should be requirement that a hugging face link is also included. If both of these requirements cannot be met for any reason (ex. Weights will probably be released but have not been released yet) the post should be taken down. &lt;/p&gt; &lt;p&gt;This would at least put some guardrails in place that would make sure posts are closer to the true nature of this sub as opposed to being low-key marketing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JacketHistorical2321"&gt; /u/JacketHistorical2321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3gjx5</id>
    <title>ZwZ 8B/7B/4B</title>
    <updated>2026-02-13T05:17:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt; &lt;img alt="ZwZ 8B/7B/4B" src="https://preview.redd.it/0qvadyln47jg1.png?width=140&amp;amp;height=72&amp;amp;auto=webp&amp;amp;s=05efe6fee87739b20abf6a144df17c59c6612f79" title="ZwZ 8B/7B/4B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#model-summary"&gt;&lt;/a&gt;Model Summary&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ZwZ-8B&lt;/strong&gt; is a fine-grained multimodal perception model built upon &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B"&gt;Qwen3-VL-8B&lt;/a&gt;. It is trained using &lt;strong&gt;Region-to-Image Distillation (R2I)&lt;/strong&gt; combined with reinforcement learning, enabling superior fine-grained visual understanding in a single forward pass ‚Äî no inference-time zooming or tool calling required.&lt;/p&gt; &lt;p&gt;ZwZ-8B achieves state-of-the-art performance on fine-grained perception benchmarks among open-source models of comparable size, while also demonstrating strong out-of-distribution generalization on visual reasoning, GUI agent, and AIGC detection tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0qvadyln47jg1.png?width=3461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b9d12949205d2c9015be9a120643d5298548e6b"&gt;https://preview.redd.it/0qvadyln47jg1.png?width=3461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b9d12949205d2c9015be9a120643d5298548e6b&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#key-features"&gt;&lt;/a&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚ö° Single-Pass Efficiency&lt;/strong&gt;: Achieves fine-grained perception in one forward pass, eliminating inference-time tool-calling overhead&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üéØ Superior Accuracy&lt;/strong&gt;: State-of-the-art on perception benchmarks among open-source models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üìà Broad Improvements&lt;/strong&gt;: Enhances not only perception benchmarks but also out-of-distribution generalization on visual reasoning, GUI agent, and AIGC detection&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#how-it-works"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;p&gt;Traditional &amp;quot;Thinking-with-Images&amp;quot; methods zoom into regions of interest during inference, incurring high latency from repeated tool calls and visual re-encoding. &lt;strong&gt;ZwZ&lt;/strong&gt; transforms zooming from an inference-time tool into a training-time primitive:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Zoom in&lt;/strong&gt; to micro-cropped regions and let strong teacher models (Qwen3-VL-235B, GLM-4.5V) generate high-quality VQA data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distill&lt;/strong&gt; this region-grounded supervision back to the full image with explicit bounding-box overlays&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reinforce&lt;/strong&gt; via RL training to enable single-glance fine-grained perception without tool use&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B"&gt;https://huggingface.co/inclusionAI/ZwZ-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-7B"&gt;https://huggingface.co/inclusionAI/ZwZ-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-4B"&gt;https://huggingface.co/inclusionAI/ZwZ-4B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T05:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3lwk5</id>
    <title>Google Releases Conductor</title>
    <updated>2026-02-13T10:38:09+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Google Releases Conductor: a context-driven Gemini CLI extension that stores knowledge as Markdown and orchestrates agentic workflows&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/gemini-cli-extensions/conductor"&gt;https://github.com/gemini-cli-extensions/conductor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3lwk5/google_releases_conductor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3lwk5/google_releases_conductor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3lwk5/google_releases_conductor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T10:38:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2xotu</id>
    <title>Minimax M2.5 Officially Out</title>
    <updated>2026-02-12T16:17:13+00:00</updated>
    <author>
      <name>/u/Which_Slice1600</name>
      <uri>https://old.reddit.com/user/Which_Slice1600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt; &lt;img alt="Minimax M2.5 Officially Out" src="https://preview.redd.it/75rjx62d93jg1.png?width=140&amp;amp;height=67&amp;amp;auto=webp&amp;amp;s=bc1913b92a211d48c5d2979574442a87148c17cf" title="Minimax M2.5 Officially Out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only official webpages released now. But the bench looks very promising:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-Bench Verified 80.2%&lt;/li&gt; &lt;li&gt;Multi-SWE-Bench 51.3%&lt;/li&gt; &lt;li&gt;BrowseComp 76.3%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Edit: replaced with the en page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.minimax.io/news/minimax-m25"&gt;https://www.minimax.io/news/minimax-m25&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Which_Slice1600"&gt; /u/Which_Slice1600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r2xotu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3l572</id>
    <title>MiniMax onX: Weights dropping REALLY, REALLY, SOON</title>
    <updated>2026-02-13T09:51:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt; &lt;img alt="MiniMax onX: Weights dropping REALLY, REALLY, SOON" src="https://preview.redd.it/jrgpe9krh8jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30a1ae2be695a2a4f2dee2ca962e2fa76614dcc1" title="MiniMax onX: Weights dropping REALLY, REALLY, SOON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jrgpe9krh8jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35d2x</id>
    <title>MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters</title>
    <updated>2026-02-12T21:02:15+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt; &lt;img alt="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" src="https://external-preview.redd.it/_kcNQarR05LXfQqSjI9sCiHSj5IycOpRZaI00SHW4k8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96ababa53bad9198147827e5856fa3e99fbda827" title="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenHands reveals the model size in their announcement.&lt;/p&gt; &lt;p&gt;Still waiting for the model to appear on HF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openhands.dev/blog/minimax-m2-5-open-weights-models-catch-up-to-claude"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T21:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3kzce</id>
    <title>MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours</title>
    <updated>2026-02-13T09:41:01+00:00</updated>
    <author>
      <name>/u/Own_Forever_5997</name>
      <uri>https://old.reddit.com/user/Own_Forever_5997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt; &lt;img alt="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" src="https://preview.redd.it/p94fz9gsf8jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=920f76b1a80dd8b1b58e34745f143966274a40a4" title="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Forever_5997"&gt; /u/Own_Forever_5997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p94fz9gsf8jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3hlfq</id>
    <title>UG student launches Dhi-5B (Trained from Scratch)</title>
    <updated>2026-02-13T06:13:29+00:00</updated>
    <author>
      <name>/u/gradNorm</name>
      <uri>https://old.reddit.com/user/gradNorm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt; &lt;img alt="UG student launches Dhi-5B (Trained from Scratch)" src="https://preview.redd.it/5tsgquvue7jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fac59fbf4b00df28aabae2f993f4d65bb88169c" title="UG student launches Dhi-5B (Trained from Scratch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii everyone,&lt;/p&gt; &lt;p&gt;I present Dhi-5B: A 5 billion parameter Multimodal Language Model trained compute optimally with just ‚Çπ1.1 lakh ($1200).&lt;/p&gt; &lt;p&gt;I incorporate the latest architecture design and training methodologies in this. And I also use a custom built codebase for training these models.&lt;/p&gt; &lt;p&gt;I train the Dhi-5B in 5 stages:-&lt;/p&gt; &lt;p&gt;üìö Pre-Training: The most compute heavy phase, where the core is built. (Gives the Base varient.)&lt;/p&gt; &lt;p&gt;üìú Context-Length-Extension: The model learns to handle 16k context from the 4k learned during PT.&lt;/p&gt; &lt;p&gt;üìñ Mid-Training: Annealing on very high quality datasets.&lt;/p&gt; &lt;p&gt;üí¨ Supervised-Fine-Tuning: Model learns to handle conversations. (Gives the Instruct model.)&lt;/p&gt; &lt;p&gt;üëÄ Vision-Extension: The model learns to see. (Results in The Dhi-5B.)&lt;/p&gt; &lt;p&gt;I'll be dropping it in 3 phases:-&lt;/p&gt; &lt;p&gt;i. Dhi-5B-Base (available now)&lt;/p&gt; &lt;p&gt;ii. Dhi-5B-Instruct (coming soon)&lt;/p&gt; &lt;p&gt;iii. The Dhi-5B (coming soon)&lt;/p&gt; &lt;p&gt;Some details about the Dhi-5B-Base model:-&lt;/p&gt; &lt;p&gt;The base varient is of 4 billion parameters. It is trained on 40 billion natural language tokens mostly in english from FineWeb-Edu dataset.&lt;/p&gt; &lt;p&gt;I use the new Muon optimizer for optimising the Matrix Layers, and rest are optimized by AdamW.&lt;/p&gt; &lt;p&gt;The model has 32 layers, with 3072 width, SwiGLU MLPs, the full MHA attention with FlashAttention-3, 4096 context length, 64k vocab and 2 million batch size during training.&lt;/p&gt; &lt;p&gt;Attached are some evaluations of the base model, the compared models are about 10x more expensive than ours.&lt;/p&gt; &lt;p&gt;Thank you, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gradNorm"&gt; /u/gradNorm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tsgquvue7jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T06:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3csbk</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)</title>
    <updated>2026-02-13T02:12:47+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)" src="https://preview.redd.it/orcqu1oq76jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=617bc649889e0dd0343008568d8aa957dde229c5" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Friday's guests: &lt;strong&gt;The Core Team of MiniMax Lab and The Lab‚Äôs Founder!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Friday, Feb. 13th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/orcqu1oq76jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T02:12:47+00:00</published>
  </entry>
</feed>
