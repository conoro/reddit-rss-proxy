<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-06T19:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nz51be</id>
    <title>“This is a fantastic question that strikes at the heart of the intersection of quantum field theory and animal welfare…”</title>
    <updated>2025-10-06T00:40:47+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many current models now start every response in this manner. I don’t remember it being that way a year ago. Do they all use the same bad instruction dataset?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T00:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz7xdu</id>
    <title>UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!</title>
    <updated>2025-10-06T03:00:12+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"&gt; &lt;img alt="UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!" src="https://a.thumbs.redditmedia.com/UuZrlImxEYTE-nymzCESnUejkDPWAQxxEocQEz-Nf_4.jpg" title="UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nz7xdu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T03:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzim4o</id>
    <title>What's the best local LLM for coding I can run on MacBook Pro M4 32Gb?</title>
    <updated>2025-10-06T13:09:30+00:00</updated>
    <author>
      <name>/u/SuperShittyShot</name>
      <uri>https://old.reddit.com/user/SuperShittyShot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two macbook pros, one is a 14&amp;quot; MBP with M4 and 32Gb and the other is a 16&amp;quot; M4 Pro with 48Gb&lt;/p&gt; &lt;p&gt;I wanted to know what is the best one I can run locally that has reasonable even if slightly slow, I assume the extra core count and RAM would help the bigger. &lt;/p&gt; &lt;p&gt;So far I've tried &lt;strong&gt;qwen2.5-coder:3b&lt;/strong&gt; for autocompletion which is mostly OK, and &lt;strong&gt;deepseek-r1:14b&lt;/strong&gt; for the chat/agent in the M4 32Gb one and it works but it's slower than what I would like it to be... Is there any model that performs the same/better and that is also faster even if it's a little bit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperShittyShot"&gt; /u/SuperShittyShot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzim4o/whats_the_best_local_llm_for_coding_i_can_run_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzim4o/whats_the_best_local_llm_for_coding_i_can_run_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzim4o/whats_the_best_local_llm_for_coding_i_can_run_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T13:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzgp8q</id>
    <title>How to run LLMs on a 1GB (e-waste) GPU without changing a single line of code</title>
    <updated>2025-10-06T11:42:30+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Accelera is working at some scale. And you 𝐝𝐨 𝐧𝐨𝐭 𝐡𝐚𝐯𝐞 𝐭𝐨 𝐫𝐞𝐜𝐨𝐦𝐩𝐢𝐥𝐞 𝐨𝐫 𝐦𝐨𝐝𝐢𝐟𝐲 𝐚 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 𝐨𝐟 𝐲𝐨𝐮𝐫 𝐜𝐨𝐝𝐞𝐛𝐚𝐬𝐞.&lt;/p&gt; &lt;p&gt;I was facing an odd problem over quite a few years now, and that is I am quite poor, and I can not do anything about it for so long. I work hard, take the next step, but somehow the new base set, and I am stuck there again. And this also makes me GPU poor. I can not even load the whole wan models in my GPU. But I have some specific skillset, and one of them is designing the most weirdest algorithm, but they work, and they also scale. So here is what I did. I have enough RAM to keep loading the weights on demand and transfer them onto GPU, perform the operation on GPU and return back to CPU, and keep doing this till we are done. This way I was able limit the usage VRAM load so much that max hit 400 megabytes, not even a gigabytes.&lt;/p&gt; &lt;p&gt;So now we can run wan on 16gb machine with mobile GPU of less than 1gb VRAM, so it fits the description of everyday developer laptop. This is not just a moment for me, but for us. Think about how much e-waste we can make reusable with this. Think about how many clusters we can make just by integrating them with accelera, definetly they will be slower than latest cutting edge devices, but it is one more fighting chances to lacking startups or indie developers.&lt;/p&gt; &lt;p&gt;Right now I am trying to make it distributed to multiple device and parallel weight loading. And I am pretty sure it will be a quite turbulent path, but I will definetly explore it, and resolve it.&lt;/p&gt; &lt;p&gt;This is just a technique to intercept pytorch method and replace their with my efficient matmul code. It also makes me limited, if something is not implemented in torch, it simply can not optimize it. But on the bright side, we can use this without any recompile or modification of the codebase.&lt;/p&gt; &lt;p&gt;Please share your thoughts and suggestions. Today (2025.10.06) the video is jittery, but it will not be for very long.&lt;/p&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/maifeeulasad/Accelera/"&gt;https://github.com/maifeeulasad/Accelera/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PIP package: &lt;a href="https://pypi.org/project/accelera/"&gt;https://pypi.org/project/accelera/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgp8q/how_to_run_llms_on_a_1gb_ewaste_gpu_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgp8q/how_to_run_llms_on_a_1gb_ewaste_gpu_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgp8q/how_to_run_llms_on_a_1gb_ewaste_gpu_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T11:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzpuzq</id>
    <title>How to run Lemonade LLM server-router on an Apple Silicon mac</title>
    <updated>2025-10-06T17:41:52+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpuzq/how_to_run_lemonade_llm_serverrouter_on_an_apple/"&gt; &lt;img alt="How to run Lemonade LLM server-router on an Apple Silicon mac" src="https://preview.redd.it/tu9s23nv2jtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=362cfcee917e3f883d7ee5557331a2f8be123d8d" title="How to run Lemonade LLM server-router on an Apple Silicon mac" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade is an open-source server-router (like OpenRouter, but local) that auto-configures LLM backends for your computer. The same Lemonade tool works across engines (llamacpp/ONNX/FLM), backends (vulkan/rocm/metal), and OSs (Windows/Ubuntu/macOS).&lt;/p&gt; &lt;p&gt;One of our most popular requests was for macOS support, so we shipped it last week!&lt;/p&gt; &lt;p&gt;I think the most common uses for mac support will be: - People with a bunch of different computers at home and want a single way of running LLMs on all of them. - Devs who work on macs but want to make sure their app works great on AMD.&lt;/p&gt; &lt;p&gt;Here's how to get it working on your Apple Silicon mac: 1. pip install lemonade-sdk 2. lemonade-server-dev serve 3. Open http://localhost:8000 in your browser to download models and chat with them 4. Hook up http://localhost:8000/api/v1 as the base URL in any OpenAI-compatible app like Open WebUI&lt;/p&gt; &lt;p&gt;Links to the project in the comments. Let us know how you're using it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tu9s23nv2jtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpuzq/how_to_run_lemonade_llm_serverrouter_on_an_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpuzq/how_to_run_lemonade_llm_serverrouter_on_an_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzi956</id>
    <title>TransFire: an app/tool to chat with your local LLMs while far from home, without port forwarding and with AES encryption</title>
    <updated>2025-10-06T12:54:21+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzi956/transfire_an_apptool_to_chat_with_your_local_llms/"&gt; &lt;img alt="TransFire: an app/tool to chat with your local LLMs while far from home, without port forwarding and with AES encryption" src="https://external-preview.redd.it/P5OQQK2kdb9BIFrZ4yGjMVBV2XzxCl3t0FXq2tGm9l4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c21a019f1a5a8e89f0688ba5cbf120121d6e931" title="TransFire: an app/tool to chat with your local LLMs while far from home, without port forwarding and with AES encryption" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently released a quick project that I did this week to chat with my local models while avoiding the hassle of configuring port forwarding.&lt;/p&gt; &lt;p&gt;Here is the result: &lt;a href="https://github.com/Belluxx/TransFire"&gt;https://github.com/Belluxx/TransFire&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It comes with an Android app and a python script. The app allows you to chat with the model, while the script acts as a bridge/server between the app and the computer that is running the LLMs.&lt;/p&gt; &lt;p&gt;It uses a free Firebase instance as intermediary and encrypts all traffic with AES. &lt;/p&gt; &lt;p&gt;You will need to create your own firebase project to use TransFire.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i9m8ud9jnhtf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867de789e4ffa319fe3152e3f31673c0f95848c9"&gt;https://preview.redd.it/i9m8ud9jnhtf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867de789e4ffa319fe3152e3f31673c0f95848c9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzi956/transfire_an_apptool_to_chat_with_your_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzi956/transfire_an_apptool_to_chat_with_your_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzi956/transfire_an_apptool_to_chat_with_your_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T12:54:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzpq7c</id>
    <title>Better alternative for CPU only realtime TTS library</title>
    <updated>2025-10-06T17:36:56+00:00</updated>
    <author>
      <name>/u/LazyLeoperd</name>
      <uri>https://old.reddit.com/user/LazyLeoperd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using piper tts and the performance is very good with 4 threads in 32 core vCPU machines but it sounds robotic. Any other TTS library suggestions fast enough in CPU and more realistic voices and also nice to have if it supports expressive output like laugh, cry, exclamations etc. Tried melotts, voice is better but not fast as piper for a realtime chatbot without spending money on GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LazyLeoperd"&gt; /u/LazyLeoperd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpq7c/better_alternative_for_cpu_only_realtime_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpq7c/better_alternative_for_cpu_only_realtime_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpq7c/better_alternative_for_cpu_only_realtime_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzru92</id>
    <title>AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker</title>
    <updated>2025-10-06T18:55:22+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"&gt; &lt;img alt="AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker" src="https://external-preview.redd.it/cRxMiHUULYNg9ilSv0igBEg5GDo6YuyoZ5csopbw4k0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd1ec0577ebabb74b53154e89231b7a124d894a1" title="AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/06/openai-amd-chip-deal-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T18:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzoh4i</id>
    <title>Which model for local text summarization?</title>
    <updated>2025-10-06T16:50:20+00:00</updated>
    <author>
      <name>/u/roundshirt19</name>
      <uri>https://old.reddit.com/user/roundshirt19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I need a local model to transform webpages (like Wikipedia) into my markdown structure. Which model would you recommend for that? It will be 10.000s of pages but speed is not an issue. Running a 4090 i inherited from my late brother.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roundshirt19"&gt; /u/roundshirt19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoh4i/which_model_for_local_text_summarization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoh4i/which_model_for_local_text_summarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoh4i/which_model_for_local_text_summarization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T16:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzrj5z</id>
    <title>A modern open source SLURM replacement built on SkyPilot</title>
    <updated>2025-10-06T18:43:48+00:00</updated>
    <author>
      <name>/u/OriginalSpread3100</name>
      <uri>https://old.reddit.com/user/OriginalSpread3100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrj5z/a_modern_open_source_slurm_replacement_built_on/"&gt; &lt;img alt="A modern open source SLURM replacement built on SkyPilot" src="https://external-preview.redd.it/ZKCpXQia6czAvu7yjJ6_uW3RevGCDR7mVeG7dMTT5UQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc18d51bb5dbf53c12f1251c21a44972a023abe" title="A modern open source SLURM replacement built on SkyPilot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/luit26q5gjtf1.png?width=2630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=401d5ac66bece3c7a6884f92c20f70f760319710"&gt;https://preview.redd.it/luit26q5gjtf1.png?width=2630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=401d5ac66bece3c7a6884f92c20f70f760319710&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/owpyst86gjtf1.png?width=5583&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3da492b8916071787366896d81f6afa384a71ad5"&gt;https://preview.redd.it/owpyst86gjtf1.png?width=5583&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3da492b8916071787366896d81f6afa384a71ad5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I know a lot of people here train local models on personal rigs, but once you scale up to lab-scale clusters, SLURM is still the default but we’ve heard from research labs that it’s got its challenges: long queues, bash scripts, jobs colliding.&lt;/p&gt; &lt;p&gt;We just launched Transformer Lab GPU Orchestration, an open-source orchestration platform to make scaling training less painful. It’s built on SkyPilot, Ray, and Kubernetes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every GPU resource, whether in your lab or across 20+ cloud providers, appears as part of a single unified pool. &lt;/li&gt; &lt;li&gt;Training jobs are automatically routed to the lowest-cost nodes that meet requirements with distributed orchestration handled for you (job coordination across nodes, failover handling, progress tracking)&lt;/li&gt; &lt;li&gt;If your local cluster is full, jobs can burst seamlessly into the cloud.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The hope is that ease of scaling up and down makes for much more efficient cluster usage. And distributed training becomes more painless. &lt;/p&gt; &lt;p&gt;For labs where multiple researchers compete for resources, administrators get fine-grained control: quotas, priorities, and visibility into who’s running what, with reporting on idle nodes and utilization rates.&lt;/p&gt; &lt;p&gt;If you’re interested, please check out the repo (&lt;a href="https://github.com/transformerlab/transformerlab-gpu-orchestration"&gt;https://github.com/transformerlab/transformerlab-gpu-orchestration&lt;/a&gt;) or sign up for our beta (&lt;a href="https://lab.cloud"&gt;https://lab.cloud&lt;/a&gt;). We’d appreciate your feedback as we’re shipping improvements daily. &lt;/p&gt; &lt;p&gt;Curious: for those of you training multi-node models, what’s been your setup? Pure SLURM, K8s custom implementations, or something else? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalSpread3100"&gt; /u/OriginalSpread3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrj5z/a_modern_open_source_slurm_replacement_built_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrj5z/a_modern_open_source_slurm_replacement_built_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrj5z/a_modern_open_source_slurm_replacement_built_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T18:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyvqyx</id>
    <title>GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper</title>
    <updated>2025-10-05T18:19:56+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"&gt; &lt;img alt="GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper" src="https://preview.redd.it/lofrjusz4ctf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71caed6abfb748f2b7db9bdf1271b9b722f347fd" title="GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lofrjusz4ctf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T18:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzq9vy</id>
    <title>What happened to Longcat models? Why are there no quants available?</title>
    <updated>2025-10-06T17:57:11+00:00</updated>
    <author>
      <name>/u/kaisurniwurer</name>
      <uri>https://old.reddit.com/user/kaisurniwurer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzq9vy/what_happened_to_longcat_models_why_are_there_no/"&gt; &lt;img alt="What happened to Longcat models? Why are there no quants available?" src="https://external-preview.redd.it/SJG1eRbm3SwVjlU4Orqm4a5_PTL6rFKUpPzL-4SiNJI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d1f89904849c371c282657b5befc8d11c2c3998" title="What happened to Longcat models? Why are there no quants available?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaisurniwurer"&gt; /u/kaisurniwurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzq9vy/what_happened_to_longcat_models_why_are_there_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzq9vy/what_happened_to_longcat_models_why_are_there_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzal91</id>
    <title>My experience coding with open models (Qwen3, GLM 4.6, Kimi K2) inside VS Code</title>
    <updated>2025-10-06T05:23:36+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using &lt;strong&gt;Cursor&lt;/strong&gt; for a while, mainly for its smooth AI coding experience. But recently, I decided to move my workflow back to &lt;strong&gt;VS Code&lt;/strong&gt; and test how far &lt;strong&gt;open-source coding models&lt;/strong&gt; have come.&lt;/p&gt; &lt;p&gt;The setup I’m using is simple:&lt;br /&gt; - VS Code + Hugging Face Copilot Chat extension&lt;br /&gt; - Models: Qwen 3, GLM 4.6, and Kimi K2&lt;/p&gt; &lt;p&gt;Honestly, I didn’t expect much at first, but the results have been surprisingly solid.&lt;br /&gt; Here’s what stood out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;These open models handle refactoring, commenting, and quick edits really well.&lt;/li&gt; &lt;li&gt;They’re &lt;strong&gt;way&lt;/strong&gt; cheaper than proprietary models, no token anxiety, no credit drain.&lt;/li&gt; &lt;li&gt;You can switch models on the fly, depending on task complexity.&lt;/li&gt; &lt;li&gt;No vendor lock-in, full transparency, and control inside your editor.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I still agree that Claude 4.5 or GPT-5 outperform in deep reasoning and complex tasks, but for 50–60% of everyday work, writing code, debugging, or doc generation, these open models perform just fine.&lt;/p&gt; &lt;p&gt;It feels like the first time open LLMs can actually compete with closed ones in real-world dev workflows. I also made a short tutorial showing how to set it up step-by-step if you want to try it: &lt;a href="https://youtu.be/6pcBBLXxOEc"&gt;Setup guide&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to hear your thoughts on these open source models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T05:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzhvoo</id>
    <title>Is agentic programming on own HW actually feasible?</title>
    <updated>2025-10-06T12:38:03+00:00</updated>
    <author>
      <name>/u/petr_bena</name>
      <uri>https://old.reddit.com/user/petr_bena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Being a senior dev I gotta admit that latest models are really good, yes it's still not &amp;quot;job replacing&amp;quot; good, but they are surprisingly capable (I am talking mostly about Claude 4.5 and similar). I was making some simple calculations and it seems to me that these agentic tools that they are selling now are almost impossible to return any profit to them with current prices, it seems like they just pushed the prices as low as possible to onboard all possible enterprise customers and get them totally dependent on their AI services before dramatically increasing the price, so I am assuming all these are available just temporarily.&lt;/p&gt; &lt;p&gt;So yes, agentic programming on those massive GPU farms with hundreds of thousand GPUs look like it work great, because it writes a lot of output very fast (1000TPS+), but since you can't rely on this stuff being &amp;quot;almost free&amp;quot; forever, I am wondering: Is running similar models locally to get any real work done actually feasible?&lt;/p&gt; &lt;p&gt;I have a rather low-end HW for AI (16GB VRAM on RTX 4060Ti + 64 GB DDR4 on mobo) and best models I could get to run were &amp;lt; 24b models with quantization or higher parameter models using DMA to motherboard (which resulted in inference being about 10x slower, but it gave me an idea what I would be able to get with slightly more VRAM).&lt;/p&gt; &lt;p&gt;Smaller models are IMHO absolutely unusable. They just can't get any real or useful work done. For stuff similar to Claude you probably need something like deepseek or llama full with FP16, that's like 671b parameters, so what kind of VRAM you need for that? 512GB is probably minimum if you run some kind of quantization (dumbing the model down). If you want some decent context window too, that's like 1TB VRAM?&lt;/p&gt; &lt;p&gt;Then how fast is that going to be, if you get something like Mac Studio with shared RAM between CPU and GPU? What TPS you get? 5? 10? Maybe even less?&lt;/p&gt; &lt;p&gt;I think with that speed, you don't only have to spend ENORMOUS money upfront, but you end up with something that will need 2 hours to solve something you could do by yourself in 1 hour.&lt;/p&gt; &lt;p&gt;Sure you can keep it running when you are sleeping working over night, but then you still have to pay electricity right? We talk about system that could easily have 1, maybe 2kW input at that size?&lt;/p&gt; &lt;p&gt;Or maybe my math is totally off? IDK, is there anyone that actually does it and built a system that can run top models and get agentic programming work done on similar level of quality you get from Claude 4.5 or codex? How much did it cost to buy? How fast is it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petr_bena"&gt; /u/petr_bena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzhvoo/is_agentic_programming_on_own_hw_actually_feasible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzhvoo/is_agentic_programming_on_own_hw_actually_feasible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzhvoo/is_agentic_programming_on_own_hw_actually_feasible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T12:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzrhkg</id>
    <title>Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!</title>
    <updated>2025-10-06T18:42:07+00:00</updated>
    <author>
      <name>/u/cogwheel0</name>
      <uri>https://old.reddit.com/user/cogwheel0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"&gt; &lt;img alt="Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!" src="https://external-preview.redd.it/NDR0cGwzZ3NkanRmMe3itAdHO7JxtY5YivFkYCiYZ8sXROLwyG4vlc6wIxOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc502b079d4c56d8fea5995ce2aefb9a0d85fd7" title="Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;A few months back, &lt;a href="https://www.reddit.com/r/selfhosted/comments/1mo9w3t/built_a_native_openwebui_client_for_ios_android/"&gt;I shared my native mobile client for OpenWebUI&lt;/a&gt;. I'm thrilled to drop version 2.0 today, which is basically a full rebuild from the ground up. I've ditched the old limitations for a snappier, more customizable experience that feels right at home on iOS and Android.&lt;/p&gt; &lt;p&gt;If you're running OpenWebUI on your server, this update brings it to life in ways the PWA just can't match. Built with Flutter for cross-platform magic, it's open-source (as always) and pairs perfectly with your self-hosted setup.&lt;/p&gt; &lt;p&gt;Here's what's new in 2.0:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance Overhaul&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switched to Riverpod 3 for state management, go_router for navigation, and Hive for local storage.&lt;/li&gt; &lt;li&gt;New efficient Markdown parser means smoother scrolling and rendering—chats load instantly, even with long threads. (Pro tip: Data migrates automatically on update. If something glitches, just clear app data and log back in.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fresh Design &amp;amp; Personalization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total UI redesign: Modern, clean interfaces that are easier on the eyes and fingers.&lt;/li&gt; &lt;li&gt;Ditch the purple-only theme, pick from new accent colors.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upgraded Chat Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Share handling:&lt;/strong&gt; Share text/image/files from anywhere to start a chat. Android users also get an OS-wide 'Ask Conduit' context menu option when selecting text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two input modes:&lt;/strong&gt; Minimal for quick chats, or extended with one-tap access to tools, image generation, and web search.&lt;/li&gt; &lt;li&gt;Slash commands! Type &amp;quot;/&amp;quot; in the input to pull up workspace prompts.&lt;/li&gt; &lt;li&gt;Follow-up suggestions to keep conversations flowing.&lt;/li&gt; &lt;li&gt;Mermaid diagrams now render beautifully in.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AI Enhancements&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-to-Speech (TTS) for reading responses aloud. (Live calling is being worked on for the next release!)&lt;/li&gt; &lt;li&gt;Realtime status updates for image gen, web searches, and tools, matching OpenWebUI's polished UX.&lt;/li&gt; &lt;li&gt;Sources and citations for web searches and RAG based responses.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Grab it now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS&lt;/strong&gt;: &lt;a href="https://apps.apple.com/us/app/conduit-openwebui-client/id6749840287"&gt;App Store Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://play.google.com/store/apps/details?id=app.cogwheel.conduit"&gt;Google Play Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source &amp;amp; Builds&lt;/strong&gt;: &lt;a href="https://github.com/cogwheel0/conduit"&gt;GitHub Repo&lt;/a&gt; (FOSS forever—stars and PRs welcome!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huge thanks to the community for the feedback on 1.x. What do you think? Any must-have features for 2.1? Post below, or open an issue on GitHub if you're running into setup quirks. Happy self-hosting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cogwheel0"&gt; /u/cogwheel0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s0i7luesdjtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T18:42:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzgben</id>
    <title>[Update] FamilyBench: New models tested - Claude Sonnet 4.5 takes 2nd place, Qwen 3 Next breaks 70%, new Kimi weirdly below the old version, same for GLM 4.6</title>
    <updated>2025-10-06T11:22:33+00:00</updated>
    <author>
      <name>/u/Orolol</name>
      <uri>https://old.reddit.com/user/Orolol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, I've been testing more models on FamilyBench, my benchmark that tests LLM ability to understand complex tree-like relationships in a family tree across a massive context. For those who missed the initial post: this is a Python program that generates a family tree and uses its structure to generate questions about it. You get a textual description of the tree and questions that are hard to parse for LLMs. GitHub: &lt;a href="https://github.com/Orolol/familyBench"&gt;https://github.com/Orolol/familyBench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;What's new: I've added 4 new models to the leaderboard, including Claude Sonnet 4.5 which shows impressive improvements over Sonnet 4, Qwen 3 Next 80B which demonstrates massive progress in the Qwen family, and GLM 4.6 which surprisingly excels at enigma questions despite lower overall accuracy. All models are tested on the same complex tree with 400 people across 10 generations (~18k tokens). 189 questions are asked (after filtering). Tests run via OpenRouter with low reasoning effort or 8k max tokens, temperature 0.3. Example of family description: &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher...&amp;quot; Example of questions: &amp;quot;Which of Paula's grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot; &lt;/p&gt; &lt;p&gt;Current Leaderboard:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Total Tokens&lt;/th&gt; &lt;th&gt;No Response Rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;81.48%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;271,500&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Claude Sonnet 4.5&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;77.78%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;211,249&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DeepSeek R1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;75.66%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;575,624&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.6&lt;/strong&gt; (New)&lt;/td&gt; &lt;td&gt;&lt;strong&gt;74.60%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;245,113&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Gemini 2.5 Flash&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.54%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;258,214&lt;/td&gt; &lt;td&gt;2.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen 3 Next 80B A3B Thinking&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;71.43%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;1,076,302&lt;/td&gt; &lt;td&gt;3.17%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Claude Sonnet 4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;67.20%&lt;/td&gt; &lt;td&gt;258,883&lt;/td&gt; &lt;td&gt;1.06%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DeepSeek V3.2 Exp&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;66.67%&lt;/td&gt; &lt;td&gt;427,396&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.5&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;64.02%&lt;/td&gt; &lt;td&gt;216,281&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.5 Air&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;57.14%&lt;/td&gt; &lt;td&gt;1,270,138&lt;/td&gt; &lt;td&gt;26.46%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPT-OSS 120B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;167,938&lt;/td&gt; &lt;td&gt;1.06%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;1,077,814&lt;/td&gt; &lt;td&gt;20.63%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kimi K2&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;34.92%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kimi K2 0905&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;31.75%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Hunyuan A13B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;30.16%&lt;/td&gt; &lt;td&gt;121,150&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral Medium 3.1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;29.63%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0.53%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Next plan : Redo all tests en a whole new seed, with harder questions and a larger tree. I have to think how I can decrease the costs first.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orolol"&gt; /u/Orolol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T11:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nze0lj</id>
    <title>What GPT-oss Leaks About OpenAI's Training Data</title>
    <updated>2025-10-06T09:03:17+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fi-le.net/oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0lj/what_gptoss_leaks_about_openais_training_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0lj/what_gptoss_leaks_about_openais_training_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T09:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzskwx</id>
    <title>Kiln RAG Builder: Now with Local &amp; Open Models</title>
    <updated>2025-10-06T19:23:09+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"&gt; &lt;img alt="Kiln RAG Builder: Now with Local &amp;amp; Open Models" src="https://external-preview.redd.it/NzFjNWRjcHdranRmMTouR_gGQN0P-1NcenpN-hyrI6W-iM6M3YFQQOwxr0u6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=353cf1fe2f3a8b489a6a9078f45dea8e0c4b7989" title="Kiln RAG Builder: Now with Local &amp;amp; Open Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone - two weeks ago we launched our new RAG-builder &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnso4p/new_rag_builder_create_a_sota_rag_system_in_under/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;on here&lt;/a&gt; and &lt;a href="https://github.com/kiln-ai/kiln"&gt;Github&lt;/a&gt;. It allows you to build a RAG in under 5 minutes with a simple drag and drop interface. Unsurprisingly, LocalLLaMA requested local + open model support! Well we've added a bunch of open-weight/local models in our new release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Extraction models&lt;/strong&gt; (vision models which convert documents into text for RAG indexing): Qwen 2.5VL 3B/7B/32B/72B, Qwen 3VL and GLM 4.5V Vision&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding models&lt;/strong&gt;: Qwen 3 embedding 0.6B/4B/8B, Embed Gemma 300M, Nomic Embed 1.5, ModernBert, M2 Bert, E5, BAAI/bge, and more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run fully local with a config like Qwen 2.5VL + Qwen 3 Embedding. We added an &amp;quot;All Local&amp;quot; RAG template, so you can get started with local RAG with 1-click.&lt;/p&gt; &lt;p&gt;Note: we’re waiting on Llama.cpp support for Qwen 3 VL (so it’s open, but not yet local). We’ll add it as soon as it’s available, for now you can use it via the cloud.&lt;/p&gt; &lt;p&gt;Progress on other asks from the community in the last thread:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantic chunking&lt;/strong&gt;: We have this working. It's still in a branch while we test it, but if anyone wants early access let us know on &lt;a href="https://getkiln.ai/discord"&gt;Discord&lt;/a&gt;. It should be in our next release.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph RAG (specifically Graphiti)&lt;/strong&gt;: We’re looking into this, but it’s a bigger project. It will take a while as we figure out the best design.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some links to the repo and guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Kiln AI on Github: &amp;gt;4k stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/documents-and-search-rag"&gt;Documents &amp;amp; Search (RAG) Docs/Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiln.tech"&gt;Homepage&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants details or has ideas! Let me know if you want support for any specific local vision models or local embedding models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lioqj7pwkjtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T19:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzk46z</id>
    <title>Connected a 3090 to my Strix Halo</title>
    <updated>2025-10-06T14:10:06+00:00</updated>
    <author>
      <name>/u/itsjustmarky</name>
      <uri>https://old.reddit.com/user/itsjustmarky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"&gt; &lt;img alt="Connected a 3090 to my Strix Halo" src="https://b.thumbs.redditmedia.com/xzsRs0nhefliJmAnyUNq3sxE8eXJ0rNimfEyNYVd8UU.jpg" title="Connected a 3090 to my Strix Halo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/74kng2v31itf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21fc853640df3ea31c9b968e4f7af6dfc1da91cb"&gt;https://preview.redd.it/74kng2v31itf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21fc853640df3ea31c9b968e4f7af6dfc1da91cb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Testing with GPT-OSS-120B MXFP4&lt;/p&gt; &lt;p&gt;Before:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 1034.63 ms / 277 tokens ( 3.74 ms per token, 267.73 tokens per second) eval time = 2328.85 ms / 97 tokens ( 24.01 ms per token, 41.65 tokens per second) total time = 3363.48 ms / 374 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 864.31 ms / 342 tokens ( 2.53 ms per token, 395.69 tokens per second) eval time = 994.16 ms / 55 tokens ( 18.08 ms per token, 55.32 tokens per second) total time = 1858.47 ms / 397 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;llama-server \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--no-mmap \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 999 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;\&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-fa on \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-b 4096 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ub 4096 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--temp 0.7 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--top-p 0.95 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--top-k 50 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--min-p 0.05 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ctx-size 262114 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--jinja \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--reasoning-format none \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--chat-template-kwargs '{&amp;quot;reasoning_effort&amp;quot;:&amp;quot;high&amp;quot;}' \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--alias gpt-oss-120b \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-m &amp;quot;$MODEL_PATH&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;$DEVICE_ARGS \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;$SPLIT_ARGS&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsjustmarky"&gt; /u/itsjustmarky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T14:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzoyu1</id>
    <title>AI for Scientific Discovery is a Social Problem - so we made Hugging Science!</title>
    <updated>2025-10-06T17:08:39+00:00</updated>
    <author>
      <name>/u/evijit</name>
      <uri>https://old.reddit.com/user/evijit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoyu1/ai_for_scientific_discovery_is_a_social_problem/"&gt; &lt;img alt="AI for Scientific Discovery is a Social Problem - so we made Hugging Science!" src="https://b.thumbs.redditmedia.com/aSMXvgTel5VUpnlw5aJPuDvQRry_wrp7qusyuCWqUYE.jpg" title="AI for Scientific Discovery is a Social Problem - so we made Hugging Science!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am Avijit Ghosh from Hugging Face. I wanted to share our new initiative for Scientific Discovery using Open source AI.&lt;/p&gt; &lt;p&gt;My colleague Georgia Channing and I just published a position paper that challenges a core assumption in AI for science: that the main barriers are technical.&lt;/p&gt; &lt;p&gt;They're not. We systematically analyzed why AI tools aren't democratizing scientific discovery and found that culture, incentives, and coordination failures are the real bottlenecks:&lt;/p&gt; &lt;p&gt;🚨 The &amp;quot;AI Scientist&amp;quot; myth is counterproductive: Waiting for AGI to solve science delays advances we could achieve now. Worse, it devalues human expertise essential for discovery and obscures science's real purpose: cultivating human understanding, not just producing outputs. (For eg: a transformer model achieves high accuracy predicting planetary motion but learns completely wrong physics.)&lt;/p&gt; &lt;p&gt;📊 We're rewarding the wrong contributions: High-quality datasets often have 100x longer impact than individual models, yet data curation work is systematically undervalued in hiring and tenure. Most models are superseded within months. Good datasets underpin research for decades.&lt;/p&gt; &lt;p&gt;⚠️ Collaboration is broken: Domain scientists prioritize mechanistic understanding. ML researchers optimize for predictive performance. Without shared language or success criteria, projects fail before they start. We lack educational resources for technically mature but domain-naive ML practitioners (and vice versa).&lt;/p&gt; &lt;p&gt;🔍 Accessibility and Fragmentation Remain Major Challenges: Harmonizing just 9 cancer imaging files took 329.5 hours over 6 months. Global South researchers face 6-day iteration cycles versus 30 minutes in G7 countries. 66% of scientists rate their computing access as inadequate. Current AI architectures struggle with complex scientific data that lacks clear tokenization strategies.&lt;/p&gt; &lt;p&gt;Why this matters now: While we chase narrow domain-specific applications, upstream computational bottlenecks like efficient PDE solvers and multi-scale coupling go unsolved. These problems could accelerate discovery across physics, chemistry, biology, and materials science simultaneously.&lt;/p&gt; &lt;p&gt;We need to build infrastructure, incentives, and community practices that make AI tools actually accessible. &lt;/p&gt; &lt;p&gt;That's why we're launching Hugging Science! A global community committed to addressing these barriers through concrete action: collaborative challenges targeting upstream problems, cross-disciplinary education and exchange, recognition for data and infrastructure contributions, and community-owned, accessible infrastructure.&lt;/p&gt; &lt;p&gt;This requires coordinated effort from researchers, funders, and institutions. But the foundation starts with community. Whether you curate datasets, build infrastructure, or bridge disciplines, there's a place for you! &lt;/p&gt; &lt;p&gt;Links: &lt;/p&gt; &lt;p&gt;Position Paper: &lt;a href="https://huggingface.co/papers/2509.06580"&gt;https://huggingface.co/papers/2509.06580&lt;/a&gt; Hugging Science Org: &lt;a href="https://huggingface.co/hugging-science"&gt;https://huggingface.co/hugging-science&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what you think and even better if you join the community and contribute! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evijit"&gt; /u/evijit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nzoyu1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoyu1/ai_for_scientific_discovery_is_a_social_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoyu1/ai_for_scientific_discovery_is_a_social_problem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzozpg</id>
    <title>Granite4 Small-h 32b-A9b (Q4_K_M) at FULL 1M context window is using only 73GB of VRAM - Life is good!</title>
    <updated>2025-10-06T17:09:38+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model seems to fit nicely on a single H100 or RTX Pro 6000. it’s great for high context RAG. This is the perfect model for my use case of models that call multiple tools in the same prompt while RAGing a bunch of knowledge bases. Might be our new daily driver for RAG use cases. If they add reasoning and vision then this is probably going to be everybody’s workhorse model. Great job big blue!! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache set to Q8_0&lt;/li&gt; &lt;li&gt;Output tokens set to 131,072&lt;/li&gt; &lt;li&gt;Num_ctx set to 1000000 (I know it’s supposed to be 1048576 but Ollama errors out at that value for some reason) &lt;/li&gt; &lt;li&gt;Unsloth recommended settings for everything else. &lt;/li&gt; &lt;li&gt;Seems to support and perform “native” tool calling as well as GPT-OSS. &lt;/li&gt; &lt;li&gt;70.88 response tokens/s &lt;/li&gt; &lt;li&gt;Open WebUI as my front end client and Ollama 0.12.4 rc6 for inference &lt;/li&gt; &lt;li&gt;FRIGGIN’ 1 Million context window locally is crazy to me!! &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzimvg</id>
    <title>October 2025 model selections, what do you use?</title>
    <updated>2025-10-06T13:10:24+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"&gt; &lt;img alt="October 2025 model selections, what do you use?" src="https://preview.redd.it/syzg3f8oqhtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e686e4b8db47ba995e1c43c3c24fb0dd3547175e" title="October 2025 model selections, what do you use?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syzg3f8oqhtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T13:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzl8y5</id>
    <title>How Transformers avoids becoming a black box, even at 1M+ LOC</title>
    <updated>2025-10-06T14:53:04+00:00</updated>
    <author>
      <name>/u/El_Olbap</name>
      <uri>https://old.reddit.com/user/El_Olbap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm Pablo from Hugging Face Open-Source team. We just wrote a software-engineering focused deep dive on how we keep the `transformers` library hackable/maintainable while it keeps growing and growing. If you're running models locally, fine-tuning on your own hardware, or just want to understand the code you're using, I recommend the read!&lt;/p&gt; &lt;p&gt;Light spoilers about what's in it:&lt;/p&gt; &lt;p&gt;- ****One Model, One File:**** You can still read a `modeling_*.py` top-to-bottom and see exactly what's happening.&lt;/p&gt; &lt;p&gt;- ****Modular Transformers:**** This is our trick to fight code bloat. Contributors can reuse code via a small `modular_*.py` file, but we auto-generate the full, readable modeling file so you never lose the &amp;quot;one file&amp;quot; experience. It cut our maintenance work by ~15x.&lt;/p&gt; &lt;p&gt;- ****Config-Driven Performance:**** Features like FlashAttention(and ofc 2,3..), tensor parallelism (`tp_plan`), and per-layer attention schedules are enabled in the config, not by changing the model code. A `Linear` layer is always just a `Linear` layer, you don't have to change it depending on how it's sliced.&lt;/p&gt; &lt;p&gt;- ****Tools for Local Use:**** This philosophy lets us build helpful tools. The post covers an attention visualizer, a model tracer for debugging ports, and faster CUDA warmups, and we also go over `transformers serve` usage. &lt;/p&gt; &lt;p&gt;Hope you enjoy the read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_Olbap"&gt; /u/El_Olbap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/transformers-community/Transformers-tenets"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T14:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzn1mk</id>
    <title>Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU</title>
    <updated>2025-10-06T15:58:14+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt; &lt;img alt="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" src="https://external-preview.redd.it/_S9eclPc4WRscWHOsVO80UpXnpu4dfbG_wCYpnVLuPA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53f3fcd941a63a8ecb5a50a6c26e1cf55db3e1a" title="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; — a fast runtime for running &lt;strong&gt;GPT-OSS (first MoE on NPUs), Gemma3 (vision), Medgemma,&lt;/strong&gt; &lt;strong&gt;Qwen3,&lt;/strong&gt; &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, &lt;strong&gt;LLaMA3.x,&lt;/strong&gt; and others &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think &lt;strong&gt;Ollama&lt;/strong&gt;, but deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;Server Mode (OpenAI-compatible)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;✨ &lt;strong&gt;From Idle Silicon to Instant Power — FastFlowLM (FLM) Makes Ryzen™ AI Shine.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No GPU fallback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Faster and over 10× more power efficient.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supports context lengths up to 256k tokens (qwen3:4b-2507).&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Lightweight (14 MB). Installs within 20 seconds.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo → Remote machine access on the repo page&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT/playlists"&gt;FastFlowLM - YouTube&lt;/a&gt; &lt;em&gt;→ Quick start guide, NPU vs CPU vs GPU, etc.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating fast and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ksYyiUQvYfo?si=zfBjb7U86P947OYW"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T15:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz722n</id>
    <title>Biggest Provider for the community for at moment thanks to them</title>
    <updated>2025-10-06T02:17:03+00:00</updated>
    <author>
      <name>/u/dead-supernova</name>
      <uri>https://old.reddit.com/user/dead-supernova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt; &lt;img alt="Biggest Provider for the community for at moment thanks to them" src="https://preview.redd.it/6kl3hy76ietf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e1c6a42810d36cbc6b71792855914f69ca24a1" title="Biggest Provider for the community for at moment thanks to them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dead-supernova"&gt; /u/dead-supernova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kl3hy76ietf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T02:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
