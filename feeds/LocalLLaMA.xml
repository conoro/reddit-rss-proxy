<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-28T14:07:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ohrn20</id>
    <title>Investigating Apple's new "Neural Accelerators" in each GPU core (A19 Pro vs M4 Pro vs M4 vs RTX 3080 - Local LLM Speed Test!)</title>
    <updated>2025-10-27T21:48:31+00:00</updated>
    <author>
      <name>/u/TechExpert2910</name>
      <uri>https://old.reddit.com/user/TechExpert2910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone :D&lt;/p&gt; &lt;p&gt;I thought it‚Äôd be really interesting to compare how Apple's new A19 Pro (and in turn, the M5) with its fancy &lt;strong&gt;new &amp;quot;neural accelerators&amp;quot; in each GPU core&lt;/strong&gt; compare to other GPUs!&lt;/p&gt; &lt;p&gt;I ran Gemma 3n 4B on each of these devices, outputting ~the same 100-word story (at a temp of 0). I used the most optimal inference framework for each to give each their best shot.&lt;/p&gt; &lt;p&gt;Here're the results!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Inference Set-Up&lt;/th&gt; &lt;th align="left"&gt;Tokens / Sec&lt;/th&gt; &lt;th align="left"&gt;Time to First Token&lt;/th&gt; &lt;th align="left"&gt;Perf / GPU Core&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;A19 Pro&lt;/td&gt; &lt;td align="left"&gt;6 GPU cores; iPhone 17 Pro Max&lt;/td&gt; &lt;td align="left"&gt;MLX? (‚ÄúLocal Chat‚Äù app)&lt;/td&gt; &lt;td align="left"&gt;23.5 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.4 s üëÄ&lt;/td&gt; &lt;td align="left"&gt;3.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4&lt;/td&gt; &lt;td align="left"&gt;10 GPU cores, iPad Pro 13‚Äù&lt;/td&gt; &lt;td align="left"&gt;MLX? (‚ÄúLocal Chat‚Äù app)&lt;/td&gt; &lt;td align="left"&gt;33.4 tok/s&lt;/td&gt; &lt;td align="left"&gt;1.1 s&lt;/td&gt; &lt;td align="left"&gt;3.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3080&lt;/td&gt; &lt;td align="left"&gt;10 GB VRAM; paired with a Ryzen 5 7600 + 32 GB DDR5&lt;/td&gt; &lt;td align="left"&gt;CUDA 12 llama.cpp (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;59.1 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.02 s&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4 Pro&lt;/td&gt; &lt;td align="left"&gt;16 GPU cores, MacBook Pro 14‚Äù, 48 GB unified memory&lt;/td&gt; &lt;td align="left"&gt;MLX (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;60.5 tok/s üëë&lt;/td&gt; &lt;td align="left"&gt;0.31 s&lt;/td&gt; &lt;td align="left"&gt;3.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Super Interesting Notes:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. The neural accelerators didn't make much of a difference. Here's why!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First off, they do indeed significantly accelerate compute! &lt;a href="https://tzakharko.github.io/apple-neural-accelerators-benchmark/#:%7E:text=Metal%20Shading%20Language.-,Key%20Takeaways%3A,-Operation"&gt;Taras Zakharko found that&lt;/a&gt; Matrix FP16 and Matrix INT8 are already accelerated by 4x and 7x respectively!!!&lt;/li&gt; &lt;li&gt;BUT, when the LLM spits out tokens, we're limited by memory bandwidth, NOT compute. This is especially true with Apple's iGPUs using the comparatively low-memory-bandwith system RAM as VRAM.&lt;/li&gt; &lt;li&gt;Still, there is one stage of inference that is compute-bound: prompt pre-processing! That's why we see the A19 Pro has ~3x faster Time to First Token vs the M4.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://www.macstories.net/linked/max-weinbach-on-the-m5s-neural-accelerators/"&gt;Max Weinbach's testing&lt;/a&gt; also corroborates what I found. And it's also worth noting that MLX hasn't been updated (yet) to take full advantage of the new neural accelerators!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. My M4 Pro as fast as my RTX 3080!!! It's crazy - 350 w vs 35 w&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you use an MLX model + MLX on Apple Silicon, you get some really remarkable performance. Note that the 3080 also had ~its best shot with CUDA optimized llama cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechExpert2910"&gt; /u/TechExpert2910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohbcu1</id>
    <title>Experience with the new model MiniMax M2 and some cost saving tips</title>
    <updated>2025-10-27T11:00:41+00:00</updated>
    <author>
      <name>/u/thalacque</name>
      <uri>https://old.reddit.com/user/thalacque</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt; &lt;img alt="Experience with the new model MiniMax M2 and some cost saving tips" src="https://b.thumbs.redditmedia.com/UVUhaaqNSDCk6qFXVB2lhPVQVQLnJtZQw5XfM0IrY1I.jpg" title="Experience with the new model MiniMax M2 and some cost saving tips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the discussion about MiniMax M2 in the group chat a couple of days ago, and since their API and agent are free to use, I thought I‚Äôd test it out. First, the conclusion: in my own use, M2 delivers better than expected efficiency and stability. You can feel the team has pushed the model‚Äôs strengths close to top closed models. In some scenarios it reaches top results at clearly lower cost, so it fits as the default executor, with closed models kept for final polish when needed.&lt;/p&gt; &lt;p&gt;My comparison across models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A three service monorepo dependency and lock file mess (Node.js + Express). The three services used different versions of jsonwebtoken and had lock file conflicts. The goal was to unify versions, upgrade jwt.verify from callback to Promise, and add an npm run bootstrap script for one click dependency setup and alignment.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: breaks down todos, understands the task well, reads files first, lists a plan, then edits step by step. It detects three version drifts and proposes an alignment strategy, adds the bootstrap script, runs one round of install and startup checks. Small fixes are quick, friendly to regression runs, and it feels ready to drop into a pipeline for repeated runs. Claude: strong first pass, but cross service consistency sometimes needed repeated reminders, took more rounds, and usage cost was higher. GLM/Kimi: can get the main path working, but more likely to leave rough edges in lock files and scripts that I had to clean up.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;An online 3x3 Rubik‚Äôs Cube (a small front end interaction project): rotate a layer to a target angle, buttons to choose a face, show the 3x3 color grid.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: To be honest, the first iteration wasn‚Äôt great, major issues like text occlusion and non-functional rotation weren‚Äôt addressed. The bright spot is that interaction bugs (e.g., rotation state desynchronization) could be fixed in a single pass once pointed out, without introducing new regressions. After subsequent rounds of refinement, the final result actually became the most usable and presentable, fully supporting 3D dragging. GLM/Kimi: The first round results were decent, but both ran into problems in the second round. GLM didn‚Äôt resolve the Rubik‚Äôs Cube floating/hover position issue, and Kimi, after the second round feedback, ended up not being three-dimensional. Claude performed excellently after the first round of prompts, with all features working normally, but even after multiple later rounds it still didn‚Äôt demonstrate an understanding of a 3D cube (in the image, Claude‚Äôs Rubik‚Äôs Cube is flat and the view can‚Äôt be rotated).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Metrics echo this feel: SWE bench Verified 69.4, Terminal Bench 46.3, ArtifactsBench 66.8, BrowseComp 44.0, FinSearchComp global 65.5. It is not first in every category, but on the runnable and fixable engineering loop, the structure score looks better. From my use, the strengths are proposing a plan, checking its own work, and favoring short fast iterations that clear blockers one by one.&lt;/p&gt; &lt;p&gt;Replace most closed model usage without sacrificing the reliability of the engineering loop. M2 is already enough and surprisingly handy. Set it as the default executor and run regressions for two days; the difference will be clear. After putting it into the pipeline, with the same budget you can run more in parallel, and you do save money.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-M2"&gt;https://github.com/MiniMax-AI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thalacque"&gt; /u/thalacque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ohbcu1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T11:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohzo48</id>
    <title>How we built Agentic Retrieval at Ragie</title>
    <updated>2025-10-28T03:54:49+00:00</updated>
    <author>
      <name>/u/bob_at_ragie</name>
      <uri>https://old.reddit.com/user/bob_at_ragie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all... curious about how Agentic Retrieval works?&lt;/p&gt; &lt;p&gt;We wrote a blog explaining how we built a production grade system for this at Ragie.&lt;/p&gt; &lt;p&gt;Take a look and let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ragie.ai/blog/how-we-built-agentic-retrieval-at-ragie"&gt;https://www.ragie.ai/blog/how-we-built-agentic-retrieval-at-ragie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bob_at_ragie"&gt; /u/bob_at_ragie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzo48/how_we_built_agentic_retrieval_at_ragie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzo48/how_we_built_agentic_retrieval_at_ragie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzo48/how_we_built_agentic_retrieval_at_ragie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T03:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi92ux</id>
    <title>What are some hyperparameters to tune for QwenVL models?</title>
    <updated>2025-10-28T13:16:29+00:00</updated>
    <author>
      <name>/u/Ok_Television_9000</name>
      <uri>https://old.reddit.com/user/Ok_Television_9000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If my input files are pdf, what are some hyperparameters i can play with. To come up with the best set of hyperparameters. Eg, dpi value, temperature etc. Would beam_search be good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Television_9000"&gt; /u/Ok_Television_9000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi92ux/what_are_some_hyperparameters_to_tune_for_qwenvl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi92ux/what_are_some_hyperparameters_to_tune_for_qwenvl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi92ux/what_are_some_hyperparameters_to_tune_for_qwenvl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi9zj0</id>
    <title>Wanted to ask a question about models that can be used to convert my Figma designs into html + css</title>
    <updated>2025-10-28T13:53:33+00:00</updated>
    <author>
      <name>/u/TheWeebSamurai</name>
      <uri>https://old.reddit.com/user/TheWeebSamurai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So hey there, I'm a Backend developer and an GameDev student, I wanted to ask which mid-low end model can be used to convert my figma designs into html +css. I don't really want to write html + css (I want to save time) and since most of the &amp;quot;frontend coding is almost dead&amp;quot;(or so I think) , I wanted to ask this question! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheWeebSamurai"&gt; /u/TheWeebSamurai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9zj0/wanted_to_ask_a_question_about_models_that_can_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9zj0/wanted_to_ask_a_question_about_models_that_can_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9zj0/wanted_to_ask_a_question_about_models_that_can_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi5i90</id>
    <title>What live profiling features would actually help you train or fine-tune models more efficiently?</title>
    <updated>2025-10-28T10:16:01+00:00</updated>
    <author>
      <name>/u/traceml-ai</name>
      <uri>https://old.reddit.com/user/traceml-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on TraceML a lightweight profiler that shows memory and timing live during PyTorch training. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/traceopt-ai/traceml"&gt;https://github.com/traceopt-ai/traceml&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My goal is not to replace Nsight or the PyTorch Profiler, but to make live observability lightweight and useful, something you can keep running every day without slowing training down.&lt;/p&gt; &lt;p&gt;I am exploring what to build next and would love to know what matters most to you (and what‚Äôs missing from current tools):&lt;/p&gt; &lt;p&gt;‚Ä¢ Multi-GPU / multi-process view, see utilization, memory, and sync overheads across devices&lt;/p&gt; &lt;p&gt;‚Ä¢ Throughput metrics, tokens/sec, batches/sec, or FLOPs efficiency ‚Ä¢ Gradient stability tracking, detect spikes, vanishing gradients, or divergence early&lt;/p&gt; &lt;p&gt;‚Ä¢ Memory evolution curves, see how activation/grad memory grows over steps&lt;/p&gt; &lt;p&gt;‚Ä¢ Energy or cost metrics, wattage, $ per run, or energy per token&lt;/p&gt; &lt;p&gt;‚Ä¢ Simple alerts such as OOM risk or performance drop detection&lt;/p&gt; &lt;p&gt;The focus is to keep it lightweight and easy to use, no heavy trace dumps or configs, just real-time insights you can actually use mid-training.&lt;/p&gt; &lt;p&gt;What do you think would be most useful (or hardest to get today)? Are there any live metrics or signals you wish existed but can not get easily right now?&lt;/p&gt; &lt;p&gt;Any feedback or feature votes would really help shape where I take this next. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/traceml-ai"&gt; /u/traceml-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5i90/what_live_profiling_features_would_actually_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5i90/what_live_profiling_features_would_actually_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5i90/what_live_profiling_features_would_actually_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:16:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohpqvy</id>
    <title>Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp; the Adobe Suite</title>
    <updated>2025-10-27T20:34:58+00:00</updated>
    <author>
      <name>/u/atape_1</name>
      <uri>https://old.reddit.com/user/atape_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt; &lt;img alt="Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" src="https://external-preview.redd.it/nujocFBKcx2R2jDqpB8QfazaXJG6_pPL0crrme1qkLM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=166ad90b700b767ba9a03c0fc586c47d3289ab0d" title="Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atape_1"&gt; /u/atape_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=efQPFhZmhAo&amp;amp;embeds_referring_euri=https%3A%2F%2Fwww.reddit.com%2F"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohlhdx</id>
    <title>Phoronix benchmarks single and dual AMD R9700 GPUs against a single NVIDIA RTX 6000 Ada GPU</title>
    <updated>2025-10-27T17:56:13+00:00</updated>
    <author>
      <name>/u/Brian-Puccio</name>
      <uri>https://old.reddit.com/user/Brian-Puccio</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brian-Puccio"&gt; /u/Brian-Puccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohq5bc</id>
    <title>GLM-4.6 vs Minimax-M2</title>
    <updated>2025-10-27T20:50:13+00:00</updated>
    <author>
      <name>/u/baykarmehmet</name>
      <uri>https://old.reddit.com/user/baykarmehmet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I've been using the GLM Coding Plan and it works well&lt;/strong&gt; ‚Äî not quite Sonnet 4.5 performance, but with clear prompts it gets the job done.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;However, everyone's hyping Minimax M2&lt;/strong&gt;, claiming it crushes every benchmark. The problem? I haven't seen any real-world coding examples or projects using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Has anyone here actually used Minimax M2 for development work?&lt;/strong&gt; If so:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does it compare to other models in practice?&lt;/li&gt; &lt;li&gt;Is it worth switching to?&lt;/li&gt; &lt;li&gt;Any specific use cases where it excels or falls short?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear some hands-on experiences beyond the benchmark numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baykarmehmet"&gt; /u/baykarmehmet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohh1l2</id>
    <title>86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent.</title>
    <updated>2025-10-27T15:12:09+00:00</updated>
    <author>
      <name>/u/Ok-Attention1022</name>
      <uri>https://old.reddit.com/user/Ok-Attention1022</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt; &lt;img alt="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." src="https://external-preview.redd.it/Mhekv1CVcCnqQ6OsfNa_xd5RwOhyacefoOODajDng28.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=513369b6dc41c3172d89ddffae9bf50cb41bb901" title="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built SGR Deep Research: a lightweight framework for structured reasoning agents using small LLMs&lt;/p&gt; &lt;p&gt;No LangChain/CrewAI bloat&lt;/p&gt; &lt;p&gt;~500 LOC core logic&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;: 86.1% on SimpleQA (4,326 questions)&lt;/p&gt; &lt;p&gt;Model: gpt-4.1-mini&lt;br /&gt; Tavily Search: basic&lt;/p&gt; &lt;p&gt;Cost: $0.03 per query&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/akowocw57oxf1.png?width=2460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d332497cfe0686bedb5b11f58bbb7e6de61f0a3"&gt;Performance Metrics on gpt-4.1-mini and Tavily basic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SGR understanding&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bocirpd67oxf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c1adc29c14fc211311efbf31063e3d449ffcbd0"&gt;SGR Deep Research: open-source framework for building intelligent research agents using Schema-Guided Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explicitly control reasoning flow instead of hoping model figures it out ReAct&amp;amp;PlanAct-style but with structured steps Running in production at telecom and banking right now&lt;/p&gt; &lt;p&gt;Testing local models next (Qwen, Llama) for $0 API costs&lt;br /&gt; Everything public: logs, configs, code GitHub MIT: &lt;a href="https://github.com/vamplabAI/sgr-deep-research"&gt;https://github.com/vamplabAI/sgr-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Attention1022"&gt; /u/Ok-Attention1022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T15:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohzfdu</id>
    <title>VellumForge2 - A high performance, very configurable and really easy to use DPO dataset generation tool, create high quality datasets for completely free</title>
    <updated>2025-10-28T03:41:47+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally releasing my new dataset generation tool, and some Fantasy writing datasets to go with it (soon). &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemon07r/VellumForge2"&gt;https://github.com/lemon07r/VellumForge2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample Dataset: &lt;a href="https://huggingface.co/collections/lemon07r/vellumforge2-datasets"&gt;https://huggingface.co/collections/lemon07r/vellumforge2-datasets&lt;/a&gt; (large datasets coming soon)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Functionality&lt;/strong&gt; (all you need for a tl;dr)&lt;/p&gt; &lt;p&gt;This tool creates DPO-style datasets using a main topic and LLMs to generate subtopics, prompts, and chosen/rejected response pairs through a hierarchical pipeline. What sets it apart is the optional LLM-as-a-judge rubric scoring system, inspired by how Kimi K2 was trained using rubric-based evaluation to generate higher quality writing samples. The output uses a flexible &amp;quot;one-to-many&amp;quot; hybrid schema that works seamlessly with DPOTrainer, RewardTrainer, and MORL training, no data transformation needed. You can also skip the judge entirely for DPO training or just use the prompt and chosen responses for SFT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overview &amp;amp; Features&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My original python script that I was using for making datasets worked mostly fine, but I broke it, many many times trying to refactor it and add features to it. It did get to a good place at some point, with working async, rate limiting, etc, before I broke it again with some experimental stuff that turned out to not be a good idea even if it did work. Some good lessons learned here.&lt;/p&gt; &lt;p&gt;What I did learn, I used in my complete re-write of the tool. This time I wrote it in Go, and kept it very simple and easy to use. I also kept it very modular and highly configurable from the very start. This tool works with any OpenAI-compatible API including local servers like llama.cpp, kobold.cpp, LM studio, vLLM or Ollama. Handles rate limiting automatically, supports concurrent workers, and can upload directly to Hugging Face Hub in one command, which was implemented without needing any external tools/dependencies like the HF cli. Generation templates are fully customizable via TOML config, meaning you can make any type of dataset. The example configs come with a strong default template for fantasy writing to help give an idea of what a good template would look like. The documentation includes a thorough quick start guide, and examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset Generation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This thing works fast. Had a much bigger impact than I expected in dataset generation speed compared to the old tool. Even using the completely free (and unlimited) Nvidia NIM api with it's 40 RPM rate limit and slow 20-30 tps Kimi K2 0905 model, plus any small local model for rejected responses, you can create a very high quality (possibly only topped by using Sonnet 4.5) DPO datasets, with about 1000 rows of high quality data in under a few hours, for completely free. No expensive hardware or API provider required (which of course you can use with this tool too). The sample dataset I linked completed under these conditions in only a 36-minute run, which would have been only half as long without a judge. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T03:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3w68</id>
    <title>Flex Attention vs Flash Attention 3</title>
    <updated>2025-10-28T08:27:54+00:00</updated>
    <author>
      <name>/u/Extra-Designer9333</name>
      <uri>https://old.reddit.com/user/Extra-Designer9333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm pretty new to accelerated framework APIs like FlexAttn from PyTorch team and FlashAttn from Tri Dao out of Princeton. Unsloth itself uses Flex Attn as I know and reports: &amp;quot;10x faster on a single GPU and up to 30x faster on multiple GPU systems compared to Flash Attention 2 (FA2).&amp;quot; However, FlashAttn 3 turns out to be 1.5-2x faster than FlashAttn 2. &lt;/p&gt; &lt;p&gt;I'm trying to decide which one to use for training my LLM whether it's FlexAttn (Unsloth) or FlashAttn 3. What's your personal suggestion and experience you had from these 2. Which one is more error prone, which turns out to be more memory heavy or computationally less expensive and etc.&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra-Designer9333"&gt; /u/Extra-Designer9333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T08:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohihvo</id>
    <title>Another Banger from Inclusion AI: Ming-flash-omni-Preview</title>
    <updated>2025-10-27T16:06:14+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt; &lt;img alt="Another Banger from Inclusion AI: Ming-flash-omni-Preview" src="https://external-preview.redd.it/PFGMqHZG1FenJLDckxpcToXwao333pejl_fZNW4bWqk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ce1f6abd38ec8e123f358b2f5b41d3b28a30d6" title="Another Banger from Inclusion AI: Ming-flash-omni-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;https://huggingface.co/inclusionAI/Ming-flash-omni-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on &lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;Ling-Flash-2.0&lt;/a&gt; this model has 100b total parameters and 6b active ones and supports context aware asr, text to speech, image generation and editing, segmentation etc (well its an omni modal model so you know the drill). Since its fairly sparse it is very efficient and while I couldn't test it myself the benchmarks seem promising, and it also supports voice cloning (;&lt;/p&gt; &lt;p&gt;It says it can do dialect-aware ASR, though im not sure if that will only work with Chinese ü§î&lt;/p&gt; &lt;p&gt;Anyways, if im not mistaken this is the biggest open sourced omni modal model yet so thanks to the mad lads at inclusion ai!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556"&gt;https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohm80t</id>
    <title>Newegg has 32gb AMD r9700 for $1,300</title>
    <updated>2025-10-27T18:23:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu"&gt;https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Phoronix did a poor job of benchmarking it. Would prefer benchmarking a 30gb model like qwen3 coder, but instead focuses on 8gb model: &lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;https://www.phoronix.com/review/amd-radeon-ai-pro-r9700&lt;/a&gt; Doesn't bother to compare it to 4090/5090. This video does gaming benchmarks: &lt;a href="https://www.youtube.com/watch?v=x0YJ32Q0mNw"&gt;https://www.youtube.com/watch?v=x0YJ32Q0mNw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guessing 30 tokens per second (TPS) for qwen3 coder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia7pp</id>
    <title>GLM-4.6 on fresh SWE-bench‚Äìstyle tasks collected in September 2025</title>
    <updated>2025-10-28T14:02:30+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with model evaluations of GLM-4.6 on 49 fresh tasks.&lt;/p&gt; &lt;p&gt;Key takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM 4.6&lt;/strong&gt; joins the leaderboard and is now the &lt;strong&gt;best open-source performer&lt;/strong&gt;, achieving &lt;strong&gt;37.0 % resolved rate&lt;/strong&gt; and &lt;strong&gt;42.9 % pass@5&lt;/strong&gt;, surpassing &lt;strong&gt;GLM 4.5&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the full leaderboard and insights here, and feel free to reach out if you‚Äôd like to see other models evaluated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=sep_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia8fi</id>
    <title>The vLLM team's daily life be like:</title>
    <updated>2025-10-28T14:03:17+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt; &lt;img alt="The vLLM team's daily life be like:" src="https://external-preview.redd.it/ZDF3MmtiYW16dXhmMWptouG6uHo-mrPzGurb2qCOnKrlpr9yhnl7mMdksMxF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0aa03383224463057de137e1db2d57ee7e56cd5" title="The vLLM team's daily life be like:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A massive shout-out to the vLLM team for being the heroes holding it all together so we can actually run all these amazing new models.&lt;/p&gt; &lt;p&gt;And, of course, a huge thank you to all the open-source teams like DeepSeek, Qwen, Kimi, and so many others. You are all pushing the entire field forward. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lw255camzuxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi9d43</id>
    <title>50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp; Deep Learning class</title>
    <updated>2025-10-28T13:28:38+00:00</updated>
    <author>
      <name>/u/michaelmalak</name>
      <uri>https://old.reddit.com/user/michaelmalak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt; &lt;img alt="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" src="https://external-preview.redd.it/rTJa8Nhli5TnEtPcCyezMrozQD4vuVxVpOAoemUSml4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85dd88668fea8b054bcbd26215365a38c320197b" title="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelmalak"&gt; /u/michaelmalak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ze0Xq5QMvmA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi2tky</id>
    <title>I built a small Python tool to track how your directories get messy (and clean again)</title>
    <updated>2025-10-28T07:12:21+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, much as we hate to admit, almost every project or downloads folder gets out of control over time (yep).&lt;/p&gt; &lt;p&gt;I got curious ‚Äî not just about which files change, but &lt;strong&gt;how the structure itself evolves.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;&lt;strong&gt;Directory Monitor&lt;/strong&gt;&lt;/a&gt; ‚Äî a lightweight Python script that keeps tabs on &lt;strong&gt;directory organization&lt;/strong&gt;, not just file edits. This tool uses local LLMs (Qwen, Llama, choose your own) to analyze project structure and give cleanup recommendations. Everything runs locally - no cloud APIs.&lt;/p&gt; &lt;p&gt;**The interesting technical bits:**&lt;/p&gt; &lt;p&gt;- Uses RAG with local sentence-transformers to compare current state against historical scans&lt;/p&gt; &lt;p&gt;- LLM analyzes trends and gives specific, actionable recommendations &lt;/p&gt; &lt;p&gt;- Terminal UI with Rich showing real-time metrics and sparklines&lt;/p&gt; &lt;p&gt;- All stored in SQLite locally&lt;/p&gt; &lt;p&gt;**Example output:**&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Messiness Score: 6.2/10&lt;/p&gt; &lt;p&gt;Top 3 Issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Too many files (28) in src/components - split into ui/, forms/, layouts/&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;8 files contain 'temp' - move to .archive/ or use proper version control&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Directory depth exceeds 7 levels - flatten structure&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Trend: üìâ Improving (was 7.8, now 6.2)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**Stack:**&lt;/p&gt; &lt;p&gt;- Ollama (Qwen/Llama) for LLM&lt;/p&gt; &lt;p&gt;- sentence-transformers for embeddings&lt;/p&gt; &lt;p&gt;- SQLite for history&lt;/p&gt; &lt;p&gt;- Python with Rich/Flask&lt;/p&gt; &lt;p&gt;Works completely offline after setup. Tested with Qwen3:8b and Llama3.2.&lt;/p&gt; &lt;p&gt;Would love feedback ‚Äî what features would &lt;em&gt;you&lt;/em&gt; add for keeping folders sane?&lt;/p&gt; &lt;p&gt;**GitHub:** &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;https://github.com/sukanto-m/directory-monitor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T07:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdl9q</id>
    <title>Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives</title>
    <updated>2025-10-27T12:53:12+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt; &lt;img alt="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" src="https://external-preview.redd.it/cDlpaWtncThobnhmMYyuPjWRTezxeRfqB3upVJ5ATISaueUIVVjdl6ikWaxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49803f057b43fcece9390b3b966fe6ba4de209b3" title="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chamath Palihapitiya said his team migrated a large number of workloads to Kimi K2 because it was significantly more performant and much cheaper than both OpenAI and Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/avwpphq8hnxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T12:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohvcwt</id>
    <title>Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?</title>
    <updated>2025-10-28T00:26:07+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt; &lt;img alt="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" src="https://a.thumbs.redditmedia.com/UBwX6pI157vt7gMYDChu_R8QUux04jne3QgjRhJMLr0.jpg" title="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hope you're fine.&lt;/p&gt; &lt;p&gt;Short question, I managed to find, working and testing on my PC right now, an A40 48GB. It is passively cooled and it gets quite hot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8az1kqsdyqxf1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=301fff8d7b8d78a3f33c97765bb96ebdeaa03e2d"&gt;Local testing on my PC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The seller (a friend) is asking me 1500USD for it. I'm not from USA, but a 3rd world country.&lt;/p&gt; &lt;p&gt;But I have read here on Local llama that such old cards and such aren't very worth it, also no FP8 support, etc.&lt;/p&gt; &lt;p&gt;So I'm really torn and indecisive about it. For reference, 5090 new goes for about 2700-3300USD (so 32GB, but fp8/fp4 support, like 4x times the bandwidth, etc). Used 4090s are 1600USD. 4090 48GB modded when importing they're about 4200-4400USD. 3090s are 550-600USD.&lt;/p&gt; &lt;p&gt;What would you guys do? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T00:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohyeee</id>
    <title>Minimax-M2 support added in MLX</title>
    <updated>2025-10-28T02:49:15+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt; &lt;img alt="Minimax-M2 support added in MLX" src="https://preview.redd.it/4yqqtqzynrxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85e601e94e902746685decb25bfc69d58f508850" title="Minimax-M2 support added in MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4yqqtqzynrxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T02:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi7k25</id>
    <title>HF Space to help create the -ot flags in llama.cpp</title>
    <updated>2025-10-28T12:07:11+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt; &lt;img alt="HF Space to help create the -ot flags in llama.cpp" src="https://external-preview.redd.it/g9TtSkJliQx_HIwoIut_ECyFVGHzqlshzt1AT9ZxIjA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=711e4dcc38b2e4a206b6b1d8e5ce67eae349c3be" title="HF Space to help create the -ot flags in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Mainly as I was frustrated when manually assigning the layers with the -of flag in llama.cpp and ik_llama.cpp and when increasing maybe just 1 layer in a previous gpu i had to increase the number in all the rest of the gpu, I created a Hugging face space to help with that.&lt;/p&gt; &lt;p&gt;It lets you select the number of GPUs, the size of the model weights and the number of layers and it automatically tries to assign how many layers would fit in your gpu size &lt;strong&gt;on an empty context.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then if you want to fit more context either switch to manual and reduce 1-2 layers per gpu, or increase the size in GB of the model a bit.&lt;/p&gt; &lt;p&gt;Example:&lt;br /&gt; I want to load &lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF"&gt;Bartowski GLM-4.6&lt;/a&gt; in Q6 in my rig (rtx6000, 2x5090, 4x3090) and I have 256GB VRAM and the quant takes 294 GB in Q6 as you can see now in HF if you go to the folder:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837"&gt;https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And GLM-4.6 has 92 layers as you can see here: &lt;a href="https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31"&gt;https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So fill the settings as such:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8"&gt;https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And that actually loads using 2048 context and the GPU are all almost at a 100% vram usage which is what we want.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442"&gt;https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I reduce one layer per GPU to quickly allow more vram for ctx, I can now load 32K context. But checking the GPU usage I might be able to assign one more layer to the rtx6000.&lt;/p&gt; &lt;p&gt;So the final command would be:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=2,0,6,1,3,4,5 ./build/bin/llama-server \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--model /mnt/llms/models/bartowski/zai-org_GLM-4.6-GGUF/zai-org_GLM-4.6-Q6_K/zai-org_GLM-4.6-Q6_K-00001-of-00008.gguf \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--alias glm-4.6 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ctx-size 32768 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 99 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;\&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--port 5000 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn_.*=CUDA0&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(31|32|33|34|35|36|37|38)\.ffn_.*=CUDA1&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(39|40|41|42|43|44|45|46)\.ffn_.*=CUDA2&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(47|48|49|50|51)\.ffn_.*=CUDA3&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(52|53|54|55|56)\.ffn_.*=CUDA4&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(57|58|59|60|61)\.ffn_.*=CUDA5&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(62|63|64|65|66)\.ffn_.*=CUDA6&amp;quot; --cpu-moe&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Link to the HF space: &lt;a href="https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool"&gt;https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T12:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oht9pw</id>
    <title>Z.ai release Glyph weight</title>
    <updated>2025-10-27T22:55:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt; &lt;img alt="Z.ai release Glyph weight" src="https://a.thumbs.redditmedia.com/69TaqDj6bS-Vs7O_YYHmJygT9-976J0J5KJcduszmX8.jpg" title="Z.ai release Glyph weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glyph: Scaling Context Windows via Visual-Text Compression&lt;/p&gt; &lt;p&gt;Paper: arxiv.org/abs/2510.17800&lt;/p&gt; &lt;p&gt;Weights: huggingface.co/zai-org/Glyph&lt;/p&gt; &lt;p&gt;Repo: github.com/thu-coai/Glyph&lt;/p&gt; &lt;p&gt;Glyph is a framework for scaling the context length through visual-text compression. It renders long textual sequences into images and processes them using vision‚Äìlanguage models.&lt;/p&gt; &lt;p&gt;This design transforms the challenge of long-context modeling into a multimodal problem, substantially reducing computational and memory costs while preserving semantic information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oht9pw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T22:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi63n6</id>
    <title>OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI</title>
    <updated>2025-10-28T10:50:29+00:00</updated>
    <author>
      <name>/u/mythz</name>
      <uri>https://old.reddit.com/user/mythz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt; &lt;img alt="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" src="https://external-preview.redd.it/NbBv8AZ8_FKSnCg3gr7veZ2x9ORPuKnCYj3fZNiyQ4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa75645d12e8474d1f573ac3f747ada63b172124" title="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythz"&gt; /u/mythz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ServiceStack/llms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohtp6d</id>
    <title>Bad news: DGX Spark may have only half the performance claimed.</title>
    <updated>2025-10-27T23:13:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt; &lt;img alt="Bad news: DGX Spark may have only half the performance claimed." src="https://preview.redd.it/9b2ziei0lqxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9741ebb17cdabb88dde97eb430a1c2ff563565" title="Bad news: DGX Spark may have only half the performance claimed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There might be more bad news about the DGX Spark!&lt;/p&gt; &lt;p&gt;Before it was even released, I told everyone that this thing has a memory bandwidth problem. Although it boasts 1 PFLOPS of FP4 floating-point performance, its memory bandwidth is only 273GB/s. This will cause major stuttering when running large models (with performance being roughly only one-third of a MacStudio M2 Ultra).&lt;/p&gt; &lt;p&gt;Today, more bad news emerged: the floating-point performance doesn't even reach 1 PFLOPS.&lt;/p&gt; &lt;p&gt;Tests from two titans of the industry‚ÄîJohn Carmack (founder of id Software, developer of games like Doom, and a name every programmer should know from the legendary fast inverse square root algorithm) and Awni Hannun (the primary lead of Apple's large model framework, MLX)‚Äîhave shown that this device only achieves 480 TFLOPS of FP4 performance (approximately 60 TFLOPS BF16). That's less than half of the advertised performance.&lt;/p&gt; &lt;p&gt;Furthermore, if you run it for an extended period, it will overheat and restart.&lt;/p&gt; &lt;p&gt;It's currently unclear whether the problem is caused by the power supply, firmware, CUDA, or something else, or if the SoC is genuinely this underpowered. I hope Jensen Huang fixes this soon. The memory bandwidth issue could be excused as a calculated product segmentation decision from NVIDIA, a result of us having overly high expectations meeting his precise market strategy. However, performance not matching the advertised claims is a major integrity problem.&lt;/p&gt; &lt;p&gt;So, for all the folks who bought an NVIDIA DGX Spark, Gigabyte AI TOP Atom, or ASUS Ascent GX10, I recommend you all run some tests and see if you're indeed facing performance issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9b2ziei0lqxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM ‚Äì 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
