<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-26T13:43:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p77o25</id>
    <title>Agent framework chaos? &gt; Better Agents CLI</title>
    <updated>2025-11-26T13:25:35+00:00</updated>
    <author>
      <name>/u/Previous_Ladder9278</name>
      <uri>https://old.reddit.com/user/Previous_Ladder9278</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are soooo many AI agent frameworks out there right now. And even once you pick one Agno, Mastra, PydanticAI, whatever still end up missing the reliability layer: testing, evals, structure, versioned prompts, reproducibility, guardrails, observability, etc.&lt;/p&gt; &lt;p&gt;So we built something to fix that:&lt;/p&gt; &lt;p&gt;Better Agents a CLI toolkit (OSS!) + emerging standard for building reliable, testable, production-grade agents.&lt;/p&gt; &lt;p&gt;It doesn’t replace your stack &lt;strong&gt;it stabilizes it&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use whatever agent framework you like.&lt;/li&gt; &lt;li&gt;Use whatever coding assistant you like (Cursor, Kilo, Claude, Copilot).&lt;/li&gt; &lt;li&gt;Use whatever workflow you like (notebooks, monorepo, local, cloud).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Better Agents just gives you the scaffolding and testing system that pretty much every serious agent project eventually ends up hacking together from scratch.&lt;/p&gt; &lt;p&gt;Running:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npx better-agents init &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;creates a production-grade structure:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;my-agent/ ├── app/ or src/ # your agent code ├── prompts/ # version-controlled prompts ├── tests/ │ ├── scenarios/ # conversational + E2E testing │ └── evaluations/ # eval notebooks for prompt/runtime behavior ├── .mcp.json # tool definitions / capabilities └── AGENTS.md # protocol + best practices &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Plus:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scenario tests to run agent simulations&lt;/li&gt; &lt;li&gt;Built-in eval workflows&lt;/li&gt; &lt;li&gt;Observability hooks&lt;/li&gt; &lt;li&gt;Prompt versioning + collaboration conventions&lt;/li&gt; &lt;li&gt;Tooling config for MCP or custom tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In other words: the boring but essential stuff that prevents your agent from silently regressing the day you change a prompt or swap a model.&lt;/p&gt; &lt;p&gt;Most agent repos : They work… until they don’t.&lt;/p&gt; &lt;p&gt;Better Agents gives you a repeatable engineering pattern so you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;test agents like software&lt;/li&gt; &lt;li&gt;evaluate changes before shipping&lt;/li&gt; &lt;li&gt;trace regressions&lt;/li&gt; &lt;li&gt;collaborate with a team&lt;/li&gt; &lt;li&gt;survive model/prompt/tool changes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Code + docs: &lt;a href="https://github.com/langwatch/better-agents"&gt;https://github.com/langwatch/better-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;little video how it works in practice: &lt;a href="https://www.youtube.com/watch?v=QqfXda5Uh-s&amp;amp;t=6s"&gt;https://www.youtube.com/watch?v=QqfXda5Uh-s&amp;amp;t=6s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;give it a spin, curious to hear your feedback / thoughts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Previous_Ladder9278"&gt; /u/Previous_Ladder9278 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77o25/agent_framework_chaos_better_agents_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77o25/agent_framework_chaos_better_agents_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p77o25/agent_framework_chaos_better_agents_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p77tf2</id>
    <title>comic (manga, ...) translation</title>
    <updated>2025-11-26T13:32:32+00:00</updated>
    <author>
      <name>/u/randygeneric</name>
      <uri>https://old.reddit.com/user/randygeneric</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to create a local translation pipeline for comics/mangas/.. using python, ollama (or vllm/transfomers/...). the vl models sould be &amp;lt; 20GB. If someone already has built something similar or has otherwise experience, pls give me some hints ,)&lt;/p&gt; &lt;p&gt;My first tries with ollama and several vl-models had been fairly successful (coordinates are not entirely correct, but the ordering is correct).&lt;/p&gt; &lt;p&gt;best so far: qwen3-vl:4b&lt;/p&gt; &lt;p&gt;ollama run qwen3-vl:4b &amp;quot;in this picture are several boxes of text. for all texts: Your answer should be in the format: [Coordinates] [Text (raw)] [Translation (english)]&amp;quot; /public/test-manga-001.jpeg --verbose &lt;/p&gt; &lt;p&gt;I will add information of the progress (or your info) later. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randygeneric"&gt; /u/randygeneric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tf2/comic_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tf2/comic_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tf2/comic_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p77tok</id>
    <title>What Happens Next?</title>
    <updated>2025-11-26T13:32:53+00:00</updated>
    <author>
      <name>/u/ionlycreate42</name>
      <uri>https://old.reddit.com/user/ionlycreate42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At this point, it’s quite clear that we’ve been heading towards better models, both closed and open source are improving, relative token costs to performance is getting cheaper. Obviously this trend will continue, therefore assuming it does, it opens other areas to explore, such as agentic/tool calling. Can we extrapolate how everything continues to evolve? Let’s discuss and let our minds roam free on possibilities based on current timelines &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ionlycreate42"&gt; /u/ionlycreate42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tok/what_happens_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tok/what_happens_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tok/what_happens_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:32:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p77uss</id>
    <title>Tesla T4? What impacts the prompt processing the most.</title>
    <updated>2025-11-26T13:34:20+00:00</updated>
    <author>
      <name>/u/kaisurniwurer</name>
      <uri>https://old.reddit.com/user/kaisurniwurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From techpowerup - while it has quite slow 16Gb VRAM at 320GB/s, it also has 65TFLOPS at FP16.&lt;/p&gt; &lt;p&gt;So I began to wonder if for agentic use, where processing speed is more important, wouldn't a GPU with very fast FP16 calculation speed be a better choice? Or would the memory bandwidth still impact the time-to-first-token?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaisurniwurer"&gt; /u/kaisurniwurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77uss/tesla_t4_what_impacts_the_prompt_processing_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77uss/tesla_t4_what_impacts_the_prompt_processing_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p77uss/tesla_t4_what_impacts_the_prompt_processing_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:34:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p702d3</id>
    <title>Looking for the best webui + "agent" combo</title>
    <updated>2025-11-26T05:58:29+00:00</updated>
    <author>
      <name>/u/reconciliation_loop</name>
      <uri>https://old.reddit.com/user/reconciliation_loop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm at the point where I have many models running locally, rag, mcp servers, etc. But really looking for that one webui, something like openwebui but also paired with some &amp;quot;chat agent&amp;quot; like whatever chatGPT, claude, or even qwen chat or z.ai's chat site run behind their webui's. &lt;/p&gt; &lt;p&gt;It seems we've moved past the model being the secret sauce to these things being great, and now moved on to the product being the webui+agent combination that is behind closed doors, not just the model.&lt;/p&gt; &lt;p&gt;What are you folks using for this? Most models I run locally with open webui will only use about 1 tool per invocation / query. I know the models I run are capable of more, such as GLM 4.5, since on z.ai's site it clearly does multiple steps in one query.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reconciliation_loop"&gt; /u/reconciliation_loop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p702d3/looking_for_the_best_webui_agent_combo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p702d3/looking_for_the_best_webui_agent_combo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p702d3/looking_for_the_best_webui_agent_combo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T05:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ksc6</id>
    <title>I built an AI research platform and just open sourced it.</title>
    <updated>2025-11-25T18:48:11+00:00</updated>
    <author>
      <name>/u/CodingWithSatyam</name>
      <uri>https://old.reddit.com/user/CodingWithSatyam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I've been working on Introlix for some months now. So, today I've open sourced it. It was really hard time building it as an student and a solo developer. This project is not finished yet but its on that stage I can show it to others and ask other for help in developing it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Introlix is an AI-powered research platform. Think of it as &amp;quot;GitHub Copilot meets Google Docs&amp;quot; for research work.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Research Desk: It is just like google docs but in right side there is an AI pannel where users can ask questions to LLM. And also it can edit or write document for user. So, it is just like github copilot but it is for text editor. There are two modes: Chat and edit. Chat mode is for asking questions and edit mode is for editing the document using AI agent.&lt;/li&gt; &lt;li&gt;Chat: For quick questions you can create a new chat and ask questions.&lt;/li&gt; &lt;li&gt;Workspace: Every chat, and research desk are managed in workspace. A workspace shares data with every items it have. So, when creating an new desk or chat user need to choose a workspace and every items on that workspace will be sharing same data. The data includes the search results and scraped content.&lt;/li&gt; &lt;li&gt;Multiple AI Agents: There are multiple AI agents like: context agent (to understand user prompt better), planner agent, explorer_agent (to search internet), etc.&lt;/li&gt; &lt;li&gt;Auto Format &amp;amp; Reference manage (coming soon): This is a feature to format the document into blog post style or research paper style or any other style and also automatic citation management with inline references.&lt;/li&gt; &lt;li&gt;Local LLMs (coming soon): Will support local llms&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So, I was working alone on this project and because of that codes are little bit messy. And many feature are not that fast. I've never tried to make it perfect as I was focusing on building the MVP. Now after working demo I'll be developing this project into complete working stable project. And I know I can't do it alone. I also want to learn about how to work on very big projects and this could be one of the big opportunity I have. There will be many other students or every other developers that could help me build this project end to end. To be honest I have never open sourced any project before. I have many small project and made it public but never tired to get any help from open source community. So, this is my first time.&lt;/p&gt; &lt;p&gt;I like to get help from senior developers who can guide me on this project and make it a stable project with a lot of features.&lt;/p&gt; &lt;p&gt;Here is github link for technical details: &lt;a href="https://github.com/introlix/introlix"&gt;https://github.com/introlix/introlix&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord link: &lt;a href="https://discord.gg/mhyKwfVm"&gt;https://discord.gg/mhyKwfVm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I've been still working on adding github issues for development plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodingWithSatyam"&gt; /u/CodingWithSatyam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p73d05</id>
    <title>Feedback | Local LLM Build 2x RTX Pro 4000</title>
    <updated>2025-11-26T09:23:13+00:00</updated>
    <author>
      <name>/u/sebakirs</name>
      <uri>https://old.reddit.com/user/sebakirs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear Community,&lt;/p&gt; &lt;p&gt;i am following this community since weeks - appreciate it a lot! I made it happen to explore local LLM with a budget build around a 5060 TI 16 GB on Linux &amp;amp; llama.cpp - after succesfull prototyping, i would like to scale. I researched a lot in the community about ongoing discussions and topics, so i came up with following gos and nos:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gos:&lt;/strong&gt;&lt;br /&gt; - linux based - wake on LAN KI workstation (i already have a proxmox 24/7 main node)&lt;br /&gt; - future proof AI platform to upgrade / exchange components based on trends&lt;br /&gt; - 1 or 2 GPUs with 16 GB VRAM - 48 GB VRAM&lt;br /&gt; - dual GPU setup to have VRAM of &amp;gt; 32 GB&lt;br /&gt; - total VRAM 32 GB - 48 GB&lt;br /&gt; - MoE Model of &amp;gt; 70B&lt;br /&gt; - big RAM buffer to be future proof for big sized MoE models&lt;br /&gt; - GPU offloading - as I am fine with low tk/s chat experience&lt;br /&gt; - budget of up to pain limit 6000 € - better &amp;lt;5000 €&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Nos:&lt;/strong&gt;&lt;br /&gt; - no N x 3090 build for the sake of space &amp;amp; power demand + risk of used material / warranty&lt;br /&gt; - no 5090 build as I dont have heavy processing load&lt;br /&gt; - no MI50 build, as i dont want to run into future compatibility or driver issues&lt;br /&gt; - no Strix Halo / DGX Spark / MAC, as i dont want to have a &amp;quot;monolitic&amp;quot; setup which is not modular&lt;/p&gt; &lt;p&gt;My use case is local use for 2 people for daily, tec &amp;amp; science research. We are quite happy with readible token speed of ~20 tk/s/person. At the moment i feel quite comfortable with GPT 120B OSS, INT4 GGUF Version - which I played around in rented AI spaces.&lt;/p&gt; &lt;p&gt;Overall: i am quite open for different perspectives and appreciate your thoughts!&lt;/p&gt; &lt;p&gt;So why am i sharing my plan and looking forward to your feedback? I would like to avoid bottlenecks in my setup or overkill components which dont bring any benefit but are unnecessarily expensive.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 9 7950X3D&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; Noctua NH-D15 G2&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASUS ProArt X870E-Creator WiFi&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; G.Skill Flare X5 128GB Kit, DDR5-6000, CL34-44-44-96&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt; 2x NVIDIA RTX PRO 4000 Blackwell, 24GB&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SSD:&lt;/strong&gt; Samsung 990 PRO 1TB&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Case:&lt;/strong&gt; Fractal Design North Charcoal Black&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power Supply:&lt;/strong&gt; be quiet! Pure Power 13 M 1000W ATX 3.1&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Total Price&lt;/strong&gt;: &lt;strong&gt;€6036,49&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thanks a lot in advance, looking forward to your feedback!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Wishes&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebakirs"&gt; /u/sebakirs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p73d05/feedback_local_llm_build_2x_rtx_pro_4000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p73d05/feedback_local_llm_build_2x_rtx_pro_4000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p73d05/feedback_local_llm_build_2x_rtx_pro_4000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T09:23:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6wios</id>
    <title>HunyuanOCR-1B - Dockerized Streamlit OCR App - Quite Amazing.</title>
    <updated>2025-11-26T02:53:59+00:00</updated>
    <author>
      <name>/u/exaknight21</name>
      <uri>https://old.reddit.com/user/exaknight21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this post (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/&lt;/a&gt;) this morning and wanted to try the model. I use vLLM often because it works smoothly with FastAPI, and if something runs on my 3060 12 GB, I can usually reproduce it on larger GPUs. This is part of my learning process, and I share what I figure out.&lt;/p&gt; &lt;p&gt;I spent most of the day trying to get vLLM Nightly to work with Grok and DeepSeek, but we couldn’t get it running. I’m not a developer, so I eventually hit a wall. Grok ended up generating a setup using Transformers, which I wasn’t familiar with before, so that’s something I’ll need to study.&lt;/p&gt; &lt;p&gt;The result is here: &lt;a href="https://github.com/ikantkode/hunyuan-1b-ocr-app"&gt;https://github.com/ikantkode/hunyuan-1b-ocr-app&lt;/a&gt; I recorded a short test: &lt;a href="https://www.youtube.com/watch?v=qThh6sqkrF0"&gt;https://www.youtube.com/watch?v=qThh6sqkrF0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model performs well. My only concerns are the current BF16 requirement, the potential benefits of FP8, and the missing vLLM support. These are early impressions since I’m still learning.&lt;/p&gt; &lt;p&gt;If anyone gets this working with vLLM, I’d appreciate a walkthrough. I don’t know how to quantize models and don’t have the resources for heavier experimentation, but I hope to contribute more effectively in the future.&lt;/p&gt; &lt;p&gt;Edit: i was exhausted and my initial post had cancer level grammar. It wont happen again, and I used ChatGPT for them GPT-Nazis and Grammar Nazis out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exaknight21"&gt; /u/exaknight21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6wios/hunyuanocr1b_dockerized_streamlit_ocr_app_quite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6wios/hunyuanocr1b_dockerized_streamlit_ocr_app_quite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6wios/hunyuanocr1b_dockerized_streamlit_ocr_app_quite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T02:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6cf2p</id>
    <title>How are Chinese AI models claiming such low training costs? Did some research</title>
    <updated>2025-11-25T13:27:51+00:00</updated>
    <author>
      <name>/u/Acrobatic_Solid6023</name>
      <uri>https://old.reddit.com/user/Acrobatic_Solid6023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing my little assignment on model cost. deepseek claims $6M training cost. Everyones losing their minds cause ChatGPT-4 cost $40-80M and Gemini Ultra hit $190M.&lt;/p&gt; &lt;p&gt;Got curious if other Chinese models show similar patterns or if deepseeks just marketing bs.&lt;/p&gt; &lt;p&gt;What I found on training costs:&lt;/p&gt; &lt;p&gt;glm-4.6: $8-12M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;357B parameters (thats model size)&lt;/li&gt; &lt;li&gt;More believable than deepseeks $6M but still way under Western models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Kimi K2-0905: $25-35M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1T parameters total (MoE architecture, only ~32B active at once)&lt;/li&gt; &lt;li&gt;Closer to Western costs but still cheaper&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MiniMax: $15-20M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mid-range model, mid-range cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;deepseek V3.2: $6M (their claim)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seems impossibly low for GPU rental + training time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why the difference?&lt;/p&gt; &lt;p&gt;Training cost = GPU hours × GPU price + electricity + data costs.&lt;/p&gt; &lt;p&gt;Chinese models might be cheaper because:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cheaper GPU access (domestic chips or bulk deals)&lt;/li&gt; &lt;li&gt;Lower electricity costs in China&lt;/li&gt; &lt;li&gt;More efficient training methods (though this is speculation)&lt;/li&gt; &lt;li&gt;Or theyre just lying about the real numbers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;deepseeks $6M feels like marketing. You cant rent enough H100s for months and only spend $6M unless youre getting massive subsidies or cutting major corners.&lt;/p&gt; &lt;p&gt;glms $8-12M is more realistic. Still cheap compared to Western models but not suspiciously fake-cheap.&lt;/p&gt; &lt;p&gt;Kimi at $25-35M shows you CAN build competitive models for less than $100M+ but probably not for $6M.&lt;/p&gt; &lt;p&gt;Are these real training costs or are they hiding infrastructure subsidies and compute deals that Western companies dont get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic_Solid6023"&gt; /u/Acrobatic_Solid6023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T13:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6o5hf</id>
    <title>Cheapest $/vRAM GPU right now? Is it a good time?</title>
    <updated>2025-11-25T20:53:01+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an rtx 2080 which only has 8Gb vRAM, and I was thinking of upgrading that GPU to an affordable and good $/vRAM ratio GPU. I don't have 8k to drop on an rtx pro 6000 like suggested a few days ago here, I was thinking more in the &amp;lt;1k range.&lt;/p&gt; &lt;p&gt;Here are some options I've seen from most expensive to cheapest:&lt;/p&gt; &lt;p&gt;$1,546 RTX PRO 4000 Blackwell 24 GB GDDR7 $64/Gb&lt;/p&gt; &lt;p&gt;~$900 wait for 5070 ti super? $37/Gb&lt;/p&gt; &lt;p&gt;$800 RTX titan, $33/Gb&lt;/p&gt; &lt;p&gt;$600-800 used 3090, $25-33/Gb &lt;/p&gt; &lt;p&gt;2x$300 mac mini m1 16g cluster using exolabs? (i've used a mac mini cluster before, but it is limited on what you can run) $18/Gb&lt;/p&gt; &lt;p&gt;Is it a good time to guy a GPU? What are your setups like and what can you run in this price range?&lt;/p&gt; &lt;p&gt;I'm worried that the uptrend of RAM prices means GPUs are going to become more expensive in the coming months. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T20:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6h63t</id>
    <title>Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software</title>
    <updated>2025-11-25T16:36:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"&gt; &lt;img alt="Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software" src="https://external-preview.redd.it/JNLQ1TFljv7ecmJGVkBQs2GgRQ8E7p3qY9QpUEtPmD8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88a44b03e9ee2eeae1607d08f00e67f73244cead" title="Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/ryzen-ai-radeon-llms-with-lemonade.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6qwok</id>
    <title>Are Imatrix Quants Hurting your Model? (My opinion)</title>
    <updated>2025-11-25T22:40:14+00:00</updated>
    <author>
      <name>/u/Quiet_Joker</name>
      <uri>https://old.reddit.com/user/Quiet_Joker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, so it all started when i was using TheDrummer/Cydonia-24B-v4.1 for roleplay and i was using the normal Non-imatrix quantized Q5_K_M GGUF. The quality is good, the model is good. I was honestly impressed with it, but i decided to see if i could get better quality by using the Imatrix Q6_K_L from Bartowski, MANY people recommend to use Imatrix quants, so it must be good right?&lt;/p&gt; &lt;p&gt;Well... this is where it got odd, during my usage i started to notice a slight difference in the way the model interpreted the characters. They seemed less... emotional and less prone to act in their own personality as the character card was made, also stuff like little details were easily missed. Almost like someone just took the sense of direction out of them, sure the model/character still tried to act in character and for the most part it was following the context but it wasn't the same. On Q5_K_M (non imatrix) the character acted with more expression in the way they talked, ideas they came up with and small details like if the character touched a wall it would describe what they felt, etc.&lt;/p&gt; &lt;p&gt;I decided to test again this time with a Q5_K_L Imatrix quant from Bartowski, maybe it was the Q6 or something. Well, this time it felt worse than before, the same thing happened, the character didn't think or acted in a way that fitted their personality. The character was more &amp;quot;resistant&amp;quot; to RP and ERP. So i decided to go back and test the normal non-imatrix Q5_K_M and the problems just went away. The character acted like it should, it was more in character and it was more receptive to the ERP than the Imatrix quants.&lt;/p&gt; &lt;p&gt;I could be wrong but this is just my experience, maybe others can share their experiences so we can compare? I know imatrix are served as this &amp;quot;universal&amp;quot; quant magic, but i decided to dig deeper into it. I found out that it DOES matter what dataset you use. Imatrix don't just &amp;quot;decided which weights should have more precision when quantizing&amp;quot; they have to be given a dataset to fit. &lt;/p&gt; &lt;p&gt;I found out that most people use the wikitext dataset for the calibration of the imatrix, so we will go with that as an example. If the calibration dataset doesn't match the use case of the model, it can hurt it. That's the conclusion i came up with after reading the original PR and if the calibration is done as a &amp;quot;one dataset fits all approach&amp;quot;. &lt;/p&gt; &lt;p&gt;I decided to ask Claude and chatgpt mainly for them to search the web and they came up with the same conclusion as well. It depends on the calibration dataset.&lt;/p&gt; &lt;p&gt;Claude gave me this crude visual representation of how it works more or less:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Calibration Dataset (wiki.train.raw) ↓ 2. Run model, capture activations &amp;quot;The cat sat...&amp;quot; → Layer 1 → [0.3, 1.8, 0.1, 2.4, ...] activations ↓ 3. Square and sum activations across many chunks Weight row 1: 0.3² + 1.2² + 0.8² + ... = 45.2 (importance score) Weight row 2: 1.8² + 0.4² + 2.1² + ... = 123.7 (importance score) ↓ 4. Save importance scores to imatrix.gguf [45.2, 123.7, 67.3, 201.4, ...] ↓ 5. Quantization reads these scores - Weight row 2 (score: 123.7) → preserve with high precision - Weight row 1 (score: 45.2) → can use lower precision ↓ 6. Final quantized model (Q4_K_M with IMatrix guidance) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when you are quantizing a ERP or RP model... this is where it gets interesting: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;IMatrix thinks is important (from Wikipedia): ├─ Factual information processing: HIGH importance (PRESERVED) ├─ Date/number handling: HIGH importance (PRESERVED) ├─ Formal language patterns: HIGH importance (PRESERVED) └─ Technical terminology: HIGH importance (PRESERVED) Result during quantization: ├─ Emotional language weights: LOW priority → HEAVILY QUANTIZED ├─ Creative description weights: LOW priority → HEAVILY QUANTIZED ├─ Character interaction weights: LOW priority → HEAVILY QUANTIZED └─ Factual/formal weights: HIGH priority → CAREFULLY PRESERVED &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So... what do you guys think? Should Imatrix quantization and calibration datasets be looked into a little bit more? I'd love to hear your thoughts and if i'm wrong on how the imatrix calculations are done and i'm just overthinking it, then please let me know, i'm sure others might be interested in this topic as well. Afterall i could just be making shit up and saying some shit like &amp;quot;Its different!&amp;quot; mainly cause i used a lower quant or something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet_Joker"&gt; /u/Quiet_Joker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6qwok/are_imatrix_quants_hurting_your_model_my_opinion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6qwok/are_imatrix_quants_hurting_your_model_my_opinion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6qwok/are_imatrix_quants_hurting_your_model_my_opinion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T22:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p73gjv</id>
    <title>OpenAI-GPT-OSS-120B scores on livecodebench</title>
    <updated>2025-11-26T09:29:58+00:00</updated>
    <author>
      <name>/u/Used-Negotiation-741</name>
      <uri>https://old.reddit.com/user/Used-Negotiation-741</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tested it？Recently I locally deployed the 120b model but found that the score is really low(about 60 on v6),and I also found that the &lt;strong&gt;reasoning: medium setting is better than reasoning: high&lt;/strong&gt;, it is wired.（the official scores of it have not been released yet).&lt;br /&gt; So next I check the results on &lt;a href="https://artificialanalysis.ai/evaluations/livecodebench?models=gpt-oss-120b-low%2Cgpt-oss-120b"&gt;artificialanalysis&lt;/a&gt;(plus the &lt;a href="https://www.kaggle.com/benchmarks/open-benchmarks/livecodebench"&gt;results on kaggle&lt;/a&gt;), and it shows &lt;strong&gt;87.8 on high setting&lt;/strong&gt; and &lt;strong&gt;70.1 on low setting&lt;/strong&gt;, I reproduce it with &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking#livecodebench-prompt"&gt;the livecodebench-prompt on artificialanalysis&lt;/a&gt; ,and get &lt;strong&gt;69 on medium setting, 61 on high setting, 60 on low setting&lt;/strong&gt;(315 questions of livecodebench v5,pass@1 of 3 rollout，Fully aligned with the &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;artificialanalysis settings&lt;/a&gt;)&lt;br /&gt; Can anyone explain?the tempeture is 0.6, top-p is 1.0, top-k is 40, max_model_len is 128k.(using the vllm-0.11.0 official docker image)&lt;br /&gt; I've seen many reviews saying this model's coding ability isn't very strong and it has severe hallucinations. Is this related?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Used-Negotiation-741"&gt; /u/Used-Negotiation-741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p73gjv/openaigptoss120b_scores_on_livecodebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p73gjv/openaigptoss120b_scores_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p73gjv/openaigptoss120b_scores_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T09:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ytcb</id>
    <title>What are these supposed no branding 3090s?</title>
    <updated>2025-11-26T04:50:38+00:00</updated>
    <author>
      <name>/u/aeroumbria</name>
      <uri>https://old.reddit.com/user/aeroumbria</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ytcb/what_are_these_supposed_no_branding_3090s/"&gt; &lt;img alt="What are these supposed no branding 3090s?" src="https://preview.redd.it/20i73icx7j3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc0f67ff0f3355312c37b378b5ea54bbf124d97b" title="What are these supposed no branding 3090s?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aeroumbria"&gt; /u/aeroumbria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/20i73icx7j3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ytcb/what_are_these_supposed_no_branding_3090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ytcb/what_are_these_supposed_no_branding_3090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T04:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p72dq0</id>
    <title>I built an open-source Memory API because setting up vector DBs for every AI project was annoying</title>
    <updated>2025-11-26T08:17:09+00:00</updated>
    <author>
      <name>/u/Eastern-Height2451</name>
      <uri>https://old.reddit.com/user/Eastern-Height2451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a few AI agents recently, and I kept running into the same friction: &lt;strong&gt;State Management.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every time I wanted to give an agent long-term memory, I had to set up a vector database (Pinecone/Weaviate), configure the embedding pipeline (OpenAI), and write the logic to chunk and retrieve context. It felt like too much boilerplate for side projects.&lt;/p&gt; &lt;p&gt;So, I built &lt;strong&gt;MemVault&lt;/strong&gt; to abstract all of that away.&lt;/p&gt; &lt;p&gt;It’s a &amp;quot;Memory-as-a-Service&amp;quot; API. You just send text to the &lt;code&gt;/store&lt;/code&gt; endpoint, and it handles the vectorization and storage. When you query it, it performs a hybrid search based on &lt;strong&gt;semantic similarity&lt;/strong&gt;, &lt;strong&gt;recency&lt;/strong&gt;, and &lt;strong&gt;importance&lt;/strong&gt; to give you the best context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Node.js &amp;amp; Express (TypeScript)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database:&lt;/strong&gt; PostgreSQL with &lt;code&gt;pgvector&lt;/code&gt; (via Prisma)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hosting:&lt;/strong&gt; Railway&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also built a &lt;strong&gt;visualizer dashboard&lt;/strong&gt; to actually see the RAG process happening in real-time (Input → Embedding → DB Retrieval), which helped a lot with debugging.&lt;/p&gt; &lt;p&gt;It’s fully open-source and I just published the SDK to NPM.&lt;/p&gt; &lt;p&gt;**Links:** *&lt;/p&gt; &lt;p&gt;[Live Demo (Visualizer)](&lt;a href="https://memvault-demo-g38n.vercel.app/"&gt;https://memvault-demo-g38n.vercel.app/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;[NPM Package](&lt;a href="https://www.npmjs.com/package/memvault-sdk-jakops88"&gt;https://www.npmjs.com/package/memvault-sdk-jakops88&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;[RapidAPI Page](&lt;a href="https://rapidapi.com/jakops88/api/long-term-memory-api"&gt;https://rapidapi.com/jakops88/api/long-term-memory-api&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;[GitHub Repository](&lt;a href="https://github.com/jakops88-hub/Long-Term-Memory-API"&gt;https://github.com/jakops88-hub/Long-Term-Memory-API&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Height2451"&gt; /u/Eastern-Height2451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p72dq0/i_built_an_opensource_memory_api_because_setting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p72dq0/i_built_an_opensource_memory_api_because_setting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p72dq0/i_built_an_opensource_memory_api_because_setting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T08:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gsjh</id>
    <title>LLaDA2.0 (103B/16B) has been released</title>
    <updated>2025-11-25T16:21:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;LLaDA2.0-flash&lt;/strong&gt; is a diffusion language model featuring a 100BA6B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA2.0 series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaDA2.0-mini&lt;/strong&gt; is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp support in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17454"&gt;https://github.com/ggml-org/llama.cpp/pull/17454&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous version of LLaDA is supported &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16003"&gt;https://github.com/ggml-org/llama.cpp/pull/16003&lt;/a&gt; already (please check the comments)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p71344</id>
    <title>Why talking to AI assistants sucks: a project that's finally fixing the interruption problem.</title>
    <updated>2025-11-26T06:58:15+00:00</updated>
    <author>
      <name>/u/Parking_Cricket_9194</name>
      <uri>https://old.reddit.com/user/Parking_Cricket_9194</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;You know what drives me insane about voice AI? The constant interruptions. You pause for half a second, and it just barges in. It feels so unnatural.&lt;/p&gt; &lt;p&gt;Well, I saw a tech talk that dug into this, and they open-sourced their solution: a model called the &lt;strong&gt;TEN Turn Detection&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's not just a simple VAD. It's smart enough to know if you've &lt;em&gt;actually&lt;/em&gt; finished talking or are just pausing to think. This means the AI can wait for you to finish, then reply instantly without that awkward delay. It completely changes the conversational flow.&lt;/p&gt; &lt;p&gt;This feels like a core piece of the puzzle for making AI interactions feel less like a transaction and more like a real conversation. The model is on Hugging Face, and it's part of their larger open-source framework for conversational AI.&lt;/p&gt; &lt;p&gt;This feels like the real deal for anyone building voice agents.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/TEN-framework/TEN_Turn_Detection"&gt;&lt;code&gt;https://huggingface.co/TEN-framework/TEN_Turn_Detection&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Main GitHub:&lt;/strong&gt; &lt;a href="https://github.com/ten-framework/ten-framework"&gt;&lt;code&gt;https://github.com/ten-framework/ten-framework&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking_Cricket_9194"&gt; /u/Parking_Cricket_9194 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71344/why_talking_to_ai_assistants_sucks_a_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71344/why_talking_to_ai_assistants_sucks_a_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p71344/why_talking_to_ai_assistants_sucks_a_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T06:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ht87</id>
    <title>Flux 2 can be run on 24gb vram!!!</title>
    <updated>2025-11-25T16:59:49+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt; &lt;img alt="Flux 2 can be run on 24gb vram!!!" src="https://preview.redd.it/m9ud0rs8pf3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=815d30594ab759659c5d269629ebb9cd5bd93a40" title="Flux 2 can be run on 24gb vram!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont know why people are complaining......&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9ud0rs8pf3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p71luf</id>
    <title>BPE tokenizer in Rust - would love feedback from the community</title>
    <updated>2025-11-26T07:29:12+00:00</updated>
    <author>
      <name>/u/farhan-dev</name>
      <uri>https://old.reddit.com/user/farhan-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71luf/bpe_tokenizer_in_rust_would_love_feedback_from/"&gt; &lt;img alt="BPE tokenizer in Rust - would love feedback from the community" src="https://preview.redd.it/0gouu2htzj3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9dad5b33921dac3978e53b7dae7b0a324fdc716" title="BPE tokenizer in Rust - would love feedback from the community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on a side project called Splintr - a BPE tokenizer written in Rust with Python bindings. It's compatible with OpenAI's tiktoken vocabularies (cl100k_base, o200k_base). &lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single text encoding: ~3-4x faster than tiktoken&lt;/li&gt; &lt;li&gt;Batch encoding: ~10-12x faster than tiktoken&lt;/li&gt; &lt;li&gt;Streaming decoder for real-time LLM output&lt;/li&gt; &lt;li&gt;54 special tokens for training and building chat/agent applications&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install splintr-rs from splintr import Tokenizer tokenizer = Tokenizer.from_pretrained(&amp;quot;cl100k_base&amp;quot;) tokens = tokenizer.encode(&amp;quot;Hello, world!&amp;quot;) text = tokenizer.decode(tokens) # Batch encode (where it really shines) texts = [&amp;quot;Hello&amp;quot;, &amp;quot;World&amp;quot;] * 1000 batch_tokens = tokenizer.encode_batch(texts) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I spent some time benchmarking and optimizing - turns out sequential encoding beats parallel for most text sizes (Rayon overhead only pays off at ~1MB+). Sometimes simpler is faster.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/farhan-syah/splintr"&gt;https://github.com/farhan-syah/splintr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would really appreciate if you could give it a try and let me know:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does it work for your use case?&lt;/li&gt; &lt;li&gt;Any issues or rough edges?&lt;/li&gt; &lt;li&gt;What features would be useful?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Still early days, but happy to hear any feedback. Thanks for reading!&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Edit 1 - 0.4.0 now support llama3 vocab&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/farhan-dev"&gt; /u/farhan-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0gouu2htzj3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71luf/bpe_tokenizer_in_rust_would_love_feedback_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p71luf/bpe_tokenizer_in_rust_would_love_feedback_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T07:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6k0h2</id>
    <title>You can now do FP8 reinforcement learning locally! (&lt;5GB VRAM)</title>
    <updated>2025-11-25T18:19:47+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt; &lt;img alt="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" src="https://preview.redd.it/t5wv1iax1g3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2fb5f6ea2413c66c20bbe83efc473ce566ff763" title="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We're getting close to our last release of 2025! Thanks so much for all the support this year. The DeepSeek team back in Jan showcased how powerful FP8 RL can be with GRPO. Well, you can now try it on your local hardware using only 5GB VRAM! RTX 50x, 40x series all work! Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why should you do FP8 training?&lt;/strong&gt;&lt;br /&gt; NVIDIA's research finds FP8 training can match BF16 accuracy whilst getting 1.6x faster inference time. We collabed with TorchAO from PyTorch to introduce FP8 RL training, making FP8 GRPO possible on home GPUs with no accuracy loss!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-4B FP8 GRPO works on just 6GB VRAM. Qwen3-1.7B on 5GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.4x faster RL training and 2× longer context vs BF16/FP16&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;60% less VRAM and 10× longer context than other FP8 RL implementations&lt;/li&gt; &lt;li&gt;Unsloth is the only framework that makes FP8 RL LoRA work on consumer GPUs (e.g. NVIDIA RTX 40 &amp;amp; 50 Series). Also runs on H100, H200, B200.&lt;/li&gt; &lt;li&gt;You may notice &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; now uses much less VRAM than before, enabling even longer context. We’re also implementing faster training soon. Blog coming soon&lt;/li&gt; &lt;li&gt;Our notebooks use 24GB L4s which fit Qwen3-14B as Tesla T4s don’t support FP8.&lt;/li&gt; &lt;li&gt;Our FP8 RL incorporates Unsloth’s weight sharing, Standby, Flex Attention + more.&lt;/li&gt; &lt;li&gt;Works on any NVIDIA RTX 40, 50 series and H100, B200 etc. GPUs&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;load_in_fp8 = True&lt;/code&gt; within &lt;code&gt;FastLanguageModel&lt;/code&gt; to enable FP8 RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can read our blogpost for our findings and more: &lt;a href="https://docs.unsloth.ai/new/fp8-reinforcement-learning"&gt;https://docs.unsloth.ai/new/fp8-reinforcement-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 3.2 1B FP8 Colab Notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the notebook, you can plug in any of our previous reward functions or RL environment examples, including our auto kernel creation and our 2048 game notebooks. To enable fp8:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os; os.environ['UNSLOTH_VLLM_STANDBY'] = &amp;quot;1&amp;quot; # Saves 30% VRAM from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained( model_name = &amp;quot;unsloth/Qwen3-8B&amp;quot;, max_seq_length = 2048, load_in_4bit = False, # False for LoRA 16bit fast_inference = True, # Enable vLLM fast inference max_lora_rank = 32, load_in_fp8 = True, # Float8 RL / GRPO! ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a lovely Thanksgiving, a lovely rest of the week and I'll be here to answer any and all questions! =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t5wv1iax1g3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6x5dh</id>
    <title>​The White House just launched "The Genesis Mission": A Manhattan Project-style initiative for AI</title>
    <updated>2025-11-26T03:24:43+00:00</updated>
    <author>
      <name>/u/iamnottheabyss</name>
      <uri>https://old.reddit.com/user/iamnottheabyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"&gt; &lt;img alt="​The White House just launched &amp;quot;The Genesis Mission&amp;quot;: A Manhattan Project-style initiative for AI" src="https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb89e898879eb7adef969749433776a6f6a543ad" title="​The White House just launched &amp;quot;The Genesis Mission&amp;quot;: A Manhattan Project-style initiative for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the White House launching The Genesis Mission, what are the implications for Open Source Models now, are we going to get stronger waves of regulation, especiallyon the open-source sector? Should we start backing up the LLMs that are on HuggingFace?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnottheabyss"&gt; /u/iamnottheabyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T03:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p74jua</id>
    <title>An explainer blog on attention, KV-caching, continuous batching</title>
    <updated>2025-11-26T10:38:35+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"&gt; &lt;img alt="An explainer blog on attention, KV-caching, continuous batching" src="https://b.thumbs.redditmedia.com/elk4CD_PSckplgxaBd4FmBVkj9PDwClP9sER-l01F5c.jpg" title="An explainer blog on attention, KV-caching, continuous batching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d3mc1kovxk3g1.png?width=3713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21ac53dcac21619d27e95bdf84dd403ebc935869"&gt;https://preview.redd.it/d3mc1kovxk3g1.png?width=3713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21ac53dcac21619d27e95bdf84dd403ebc935869&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey folks, it's Merve from Hugging Face!&lt;/p&gt; &lt;p&gt;Yesterday we dropped a lengthy blog, illustrating cutting edge inference optimization techniques: continuous batching, KV-caching and more (also attention and everything that let to them to be beginner-friendly)! We hope you like it 🤗&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T10:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p74dwo</id>
    <title>New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!</title>
    <updated>2025-11-26T10:28:22+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"&gt; &lt;img alt="New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!" src="https://preview.redd.it/az572ifbwk3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c38be44cb259c402b012a39cd9555e9340c2976f" title="New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/az572ifbwk3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T10:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
