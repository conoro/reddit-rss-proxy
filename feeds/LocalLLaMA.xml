<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-25T03:37:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p5thec</id>
    <title>Giving AI "Psychology" ‚Äì A framework to turn any natural reasoning trace into pure math</title>
    <updated>2025-11-24T21:18:00+00:00</updated>
    <author>
      <name>/u/causality-ai</name>
      <uri>https://old.reddit.com/user/causality-ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5thec/giving_ai_psychology_a_framework_to_turn_any/"&gt; &lt;img alt="Giving AI &amp;quot;Psychology&amp;quot; ‚Äì A framework to turn any natural reasoning trace into pure math" src="https://b.thumbs.redditmedia.com/5K-jL60F1_ZGJ7NH30LT94xwhYwenNc3qFNaB5z7FhY.jpg" title="Giving AI &amp;quot;Psychology&amp;quot; ‚Äì A framework to turn any natural reasoning trace into pure math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been frustrated that most &amp;quot;reasoning&amp;quot; research focuses on generic capabilities rather than specific cognitive modalities. Last most important paper: GRPO that gave reasoning to AI, played around with the RL advantage function. But the pattern of GRPO is very clearly set in certain mannerisms which are annoying: &lt;em&gt;But wait...? You are absolutely right!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I just released an open-source project called &lt;strong&gt;Patterns&lt;/strong&gt;. It proposes that we can achieve more human-like reasoning by translating cognitive primitives into mathematical operations, besides the ones GRPO limitedly uses (just group mean, extrapolation and sometimes interpolation - theres a plethora of alternative surrogate objectives)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The concept:&lt;/strong&gt;&lt;br /&gt; If we view the human mind through Jungian psychology, we have functions like Introverted Thinking (Ti) or Extroverted Sensing (Se). Patterns translates these from natural language directly into code:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ti&lt;/strong&gt; becomes &lt;strong&gt;Kolmogorov Complexity Minimization&lt;/strong&gt; (seeking the simplest logical explanation).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ne&lt;/strong&gt; becomes &lt;strong&gt;Vector Space Interpolation&lt;/strong&gt; (connecting disparate ideas).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Se&lt;/strong&gt; becomes &lt;strong&gt;Entropy Maximization&lt;/strong&gt; (pure exploration).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fi&lt;/strong&gt; becomes &lt;strong&gt;Group mean&lt;/strong&gt; (weighting many alternatives)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tool:&lt;/strong&gt;&lt;br /&gt; You type: &amp;quot;A manic creative who struggles to finish projects.&amp;quot;&lt;br /&gt; The tool generates: A &amp;quot;Harmonic Schedule&amp;quot; JSON and the actual PyTorch code to train an RL agent with those specific reward biases.&lt;/p&gt; &lt;p&gt;It operates on the idea that personality isn't just a &amp;quot;system prompt&amp;quot;‚Äîit's the physics of how an agent weighs its reward functions. Please be aware that this kind of operation (translating language into custom algebras) is really hard for LLMs, so i recommend testing the tool with only the top models. &lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/86qicpy8s93g1.gif"&gt;https://i.redd.it/86qicpy8s93g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to read thoughts on this. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fiblameandrew%2Fpatterns"&gt;https://github.com/iblameandrew/patterns&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/causality-ai"&gt; /u/causality-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5thec/giving_ai_psychology_a_framework_to_turn_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5thec/giving_ai_psychology_a_framework_to_turn_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5thec/giving_ai_psychology_a_framework_to_turn_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T21:18:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p56v22</id>
    <title>It been 2 years but why llama 3.1 8B still a popular choice to fine tune?</title>
    <updated>2025-11-24T03:47:07+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the model is so old now but new fine tuned model with this llama 3.1 8B as base still come out, do you think this trend will shift to olmo3 7B as a newer and more open ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T03:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5tvmu</id>
    <title>Tutorial on Reinforcement Learning</title>
    <updated>2025-11-24T21:33:13+00:00</updated>
    <author>
      <name>/u/johnolafenwa</name>
      <uri>https://old.reddit.com/user/johnolafenwa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone, I am doing a 3 part YouTube series on the fundamentals of Reinforcement Learning. Starting from the ABC of RL and culminating in training LLMs with RL.&lt;/p&gt; &lt;p&gt;Here is the first part: &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/j0I3-3q9AhM?si=-f9ZhAkuwO3s-kxg"&gt;https://youtu.be/j0I3-3q9AhM?si=-f9ZhAkuwO3s-kxg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to welcome any questions or suggestions on new deep dives people want to see.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnolafenwa"&gt; /u/johnolafenwa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5tvmu/tutorial_on_reinforcement_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5tvmu/tutorial_on_reinforcement_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5tvmu/tutorial_on_reinforcement_learning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T21:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p55dbq</id>
    <title>What‚Äôs the best High Parameter (100B+) Local LLM for NSFW RP?</title>
    <updated>2025-11-24T02:34:53+00:00</updated>
    <author>
      <name>/u/LyutsiferSafin</name>
      <uri>https://old.reddit.com/user/LyutsiferSafin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have about 400 GB GPU memory, what would be the best NSFW RP model I can try locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LyutsiferSafin"&gt; /u/LyutsiferSafin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T02:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5sp3p</id>
    <title>Local training for text diffusion LLMs now supported in Transformer Lab</title>
    <updated>2025-11-24T20:47:54+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you‚Äôre running local fine-tuning or experimenting with Dream / LLaDA models, Transformer Lab now supports text diffusion workflows. Transformer Lab is open source.&lt;/p&gt; &lt;p&gt;What you can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run Dream and LLaDA interactively with a built-in server&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Fine-tune diffusion LLMs with LoRA&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Benchmark using the LM Evaluation Harness (MMLU, ARC, GSM8K, HumanEval, etc.)&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NVIDIA GPUs supported today.&lt;/strong&gt; AMD + Apple Silicon support is planned.&lt;/p&gt; &lt;p&gt;Curious if anyone here is training Dream-style models locally and what configs you're using.&lt;/p&gt; &lt;p&gt;More info and how to get started here: &lt;a href="https://lab.cloud/blog/text-diffusion-support"&gt;https://lab.cloud/blog/text-diffusion-support&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5sp3p/local_training_for_text_diffusion_llms_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5sp3p/local_training_for_text_diffusion_llms_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5sp3p/local_training_for_text_diffusion_llms_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5zz11</id>
    <title>Best Coding LLM as of Nov'25</title>
    <updated>2025-11-25T01:51:36+00:00</updated>
    <author>
      <name>/u/PhysicsPast8286</name>
      <uri>https://old.reddit.com/user/PhysicsPast8286</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Folks, &lt;/p&gt; &lt;p&gt;I have a NVIDIA H100 and have been tasked to find a replacement for Qwen3 32B (non-quantized) model currenly hosted on it. &lt;/p&gt; &lt;p&gt;I‚Äôm looking it to use primarily for Java coding tasks and want the LLM to support atleast 100K context window (input + output). It would be used in a corporate environment so censored models like GPT OSS are also okay if they are good at Java programming.&lt;/p&gt; &lt;p&gt;Can anyone recommend an alternative LLM that would be more suitable for this kind of work?&lt;/p&gt; &lt;p&gt;Appreciate any suggestions or insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhysicsPast8286"&gt; /u/PhysicsPast8286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T01:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5entm</id>
    <title>Speakr v0.5.9 update - Voice profile embeddings and better local model support</title>
    <updated>2025-11-24T11:29:53+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5entm/speakr_v059_update_voice_profile_embeddings_and/"&gt; &lt;img alt="Speakr v0.5.9 update - Voice profile embeddings and better local model support" src="https://b.thumbs.redditmedia.com/VQ5tFeH_smOUr_UDaT5MmB2MuNu2jodx8PnMggCIw2I.jpg" title="Speakr v0.5.9 update - Voice profile embeddings and better local model support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick update on Speakr for those who've been following along. Just released v0.5.9 with some changes that are particularly relevant for local setups.&lt;/p&gt; &lt;p&gt;For anyone who hasn't seen this before: Speakr is a self-hosted transcription app that works with Whisper + local LLMs. You record or upload audio, it transcribes with speaker diarization, then you can chat with the transcript or get summaries using whatever model you point it at. The app runs in Docker.&lt;/p&gt; &lt;p&gt;The big addition is voice profile support using speaker embeddings. If you're running my WhisperX API webservice (see below), it now extracts 256-dimensional voice embeddings during transcription. Once you've identified someone in a recording, the system recognizes their voice automatically in future recordings based on the embedding similarity.&lt;/p&gt; &lt;p&gt;Also added some collaboration features (internal sharing, teams, retention policies) if you're running this for multiple people. All configurable through environment variables.&lt;/p&gt; &lt;p&gt;I put together a &lt;a href="https://github.com/murtaza-nasir/whisperx-asr-service"&gt;companion ASR webservice&lt;/a&gt; for this that runs WhisperX with the latest pyannote models. It's not meant to be production-grade, more of an experimental reference implementation, but it handles the diarization, time alignment, and embedding extraction. You can still use the standard Whisper ASR webservice if you don't need voice profiles.&lt;/p&gt; &lt;p&gt;The voice recognition uses cosine similarity matching against stored profiles and works pretty well in practice. I've been testing it and it's accurate enough that I rarely need to manually select speaker labels anymore. The embeddings are stored locally in your database, nothing leaves your system.&lt;/p&gt; &lt;p&gt;The upgrade path is straightforward but make sure to backup first since there are database schema changes. Everything's opt-in through env vars so your existing setup should not break.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/speakr"&gt;GitHub&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr"&gt;Docs&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr/screenshots/"&gt;Screenshots&lt;/a&gt; | &lt;a href="https://hub.docker.com/r/learnedmachine/speakr"&gt;Docker Hub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you hit any issues upgrading or have questions about the new features.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p5entm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5entm/speakr_v059_update_voice_profile_embeddings_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5entm/speakr_v059_update_voice_profile_embeddings_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T11:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5wx6f</id>
    <title>PSA: Fix for llama.cpp builds on Debian 13 "Trixie"</title>
    <updated>2025-11-24T23:34:21+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who build llama.cpp from source on Debian 13 &amp;quot;Trixie&amp;quot;, there is an issue with all CUDA Toolkit versions at the time of writing. It appears to be an incompatibility between the default Debian 13 glibc (2.41) and some CUDA headers.&lt;/p&gt; &lt;p&gt;Thankfully, there's an easy fix! See &lt;a href="https://forums.developer.nvidia.com/t/error-exception-specification-is-incompatible-for-cospi-sinpi-cospif-sinpif-with-glibc-2-41/323591/3"&gt;this forum post&lt;/a&gt; for a simple patch to work around the issue.&lt;/p&gt; &lt;p&gt;I can confirm that patch worked for me - I was able to build llama.cpp b7127 on Debian 13.1 with CUDA Toolkit 12.9.1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wx6f/psa_fix_for_llamacpp_builds_on_debian_13_trixie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wx6f/psa_fix_for_llamacpp_builds_on_debian_13_trixie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wx6f/psa_fix_for_llamacpp_builds_on_debian_13_trixie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T23:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5tw7m</id>
    <title>Is Bert-Nebulon Alpha the new GLM model?</title>
    <updated>2025-11-24T21:33:48+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5tw7m/is_bertnebulon_alpha_the_new_glm_model/"&gt; &lt;img alt="Is Bert-Nebulon Alpha the new GLM model?" src="https://preview.redd.it/9s6t2ftgw93g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6462fd503571688a7d0efd1dfe6f9ab508545f0" title="Is Bert-Nebulon Alpha the new GLM model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know what you guys think. Not open weight... but really, there's no way for us to tell. Except, there are some interesting hints here and there (check the attached screenshot).&lt;/p&gt; &lt;p&gt;I remember there was a website which mapped the LLM outputs in more robust way instead of simply comparing two outputs. If you're the author of that particular tool, please consider checking this model out and compare with the known model outputs to see which model family it belongs to, because I think this similarity here is kinda interesting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9s6t2ftgw93g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5tw7m/is_bertnebulon_alpha_the_new_glm_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5tw7m/is_bertnebulon_alpha_the_new_glm_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T21:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5i4dz</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-11-24T14:14:17+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://external-preview.redd.it/CayXH4WCl2Z2-U54jDMovfWBN3_44MX4GcqXEoo-xsM.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9880bfac9ded07ef6ee26f773c3c6a508681457e" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/open-source highlights from this week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HunyuanVideo 1.5 - Open-Source Video Generation&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Strongest open-source video generation model built on DiT architecture.&lt;br /&gt; ‚Ä¢ High-quality video generation without commercial licensing fees, optimized for accessibility.&lt;br /&gt; ‚Ä¢ &lt;a href="https://hunyuan.tencent.com/video/zh"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/tencent/HunyuanVideo-1.5"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5.pdf"&gt;Technical Report&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/pxsn6y8nq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/pxsn6y8nq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supertonic TTS - On-Device Speech Synthesis&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Fast speech model designed to run on-device with minimal resources.&lt;br /&gt; ‚Ä¢ Enables local text-to-speech without cloud dependencies.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/spaces/Supertone/supertonic"&gt;Demo&lt;/a&gt; | &lt;a href="https://github.com/supertone-inc/supertonic/"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/o85kdyznq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/o85kdyznq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Jan-v2-VL - Extended Task Execution&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Executes 49 steps in long-horizon tasks without failure (base model stops at 5 steps).&lt;br /&gt; ‚Ä¢ Handles extended task sequences that break other vision-language models.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/collections/janhq/jan-v2-vl"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://x.com/jandotai/status/1988916861174710686?s=20"&gt;Announcement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/w1yu32ooq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/w1yu32ooq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step-Audio-R1 - Audio Reasoning Model&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ First audio reasoning model with chain-of-thought capabilities.&lt;br /&gt; ‚Ä¢ Outperforms Gemini 2.5 Pro and matches Gemini 3 Pro on audio tasks.&lt;br /&gt; ‚Ä¢ &lt;a href="https://stepaudiollm.github.io/step-audio-r1/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://huggingface.co/papers/2511.15848"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/stepfun-ai/Step-Audio-R1"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FaceFusion ComfyUI - Local Face Swapping&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Advanced face swapping tool with local ONNX inference.&lt;br /&gt; ‚Ä¢ Built by huygiatrng for the ComfyUI ecosystem.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/huygiatrng/Facefusion_comfyui"&gt;GitHub&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/comfyui/comments/1p3np7v/facefusion_comfyui_advanced_face_swapping_with/"&gt;Reddit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/97jz8d1ds83g1.gif"&gt;https://i.redd.it/97jz8d1ds83g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ComfyUI-SAM3DBody - 3D Human Mesh Recovery Node&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Full-body 3D human mesh recovery from single images using SAM 3D.&lt;br /&gt; ‚Ä¢ Built by PozzettiAndrea for seamless ComfyUI integration.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/PozzettiAndrea/ComfyUI-SAM3DBody"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/nwfumgwpq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/nwfumgwpq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-33-physical-ai?r=12l7fk"&gt;full newsletter&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T14:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5mdqi</id>
    <title>llamacpp-gfx906 new release</title>
    <updated>2025-11-24T16:56:56+00:00</updated>
    <author>
      <name>/u/CornerLimits</name>
      <uri>https://old.reddit.com/user/CornerLimits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, just dropped an update of the fork for the vega 7nm graphics card. Avg +10% speedups here and there.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;https://github.com/iacopPBK/llama.cpp-gfx906&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some changes are too gfx906 specific (and with limited benefits) for pull requesting. The fork is just an experiment to sqweeze the gpu at max.&lt;/p&gt; &lt;p&gt;Fully compatible with everything on the normal llamacpp, have fun!&lt;/p&gt; &lt;p&gt;For anything related, there is an awesome discord server (link in repo)&lt;/p&gt; &lt;p&gt;I will keep this thing up to date everytime something special comes out (qwen3next we are watching you)!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CornerLimits"&gt; /u/CornerLimits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5mdqi/llamacppgfx906_new_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5mdqi/llamacppgfx906_new_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5mdqi/llamacppgfx906_new_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T16:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5frn9</id>
    <title>[Release] Hypnos i1-8B: I fine-tuned Hermes 3 on REAL IBM Quantum Computer data (133-qubit GHZ states). Beats Llama-70B in Logic.</title>
    <updated>2025-11-24T12:28:42+00:00</updated>
    <author>
      <name>/u/Disastrous_Bid5976</name>
      <uri>https://old.reddit.com/user/Disastrous_Bid5976</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! üëã&lt;/p&gt; &lt;p&gt;Its my first post here, and I‚Äôm excited to share a weird experiment I have been working on. I wanted to see what happens if we inject &lt;strong&gt;true physical entropy&lt;/strong&gt; from a quantum processor into the SFT stage of an LLM.&lt;/p&gt; &lt;p&gt;So, I got access to IBM Quantum's latest chips (&lt;strong&gt;Heron r2&lt;/strong&gt; &amp;amp; &lt;strong&gt;Heron r1&lt;/strong&gt;, 133+ qubits) and ran some entanglement experiments (GHZ state). I took the raw measurement data ‚Äî which contains true quantum randomness and hardware noise ‚Äî and mixed it into a high-quality reasoning dataset. Meet Hypnos i1-8B!&lt;br /&gt; Results (Benchmarks vs Llama 3.1 Base)&lt;/p&gt; &lt;p&gt;The reasoning capabilities jumped significantly due to the dataset mix:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Logic (BBH):&lt;/strong&gt; &lt;strong&gt;~68.5%&lt;/strong&gt; (Beats base Llama-3-70B in specific logic tasks).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Math (MATH):&lt;/strong&gt; &lt;strong&gt;~60%+&lt;/strong&gt; (Huge improvement over base).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction Following:&lt;/strong&gt; &lt;strong&gt;~85%&lt;/strong&gt; (Very obedient).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why Quantum Data? &lt;/p&gt; &lt;p&gt;LLMs tend to suffer from mode collapse or become too &amp;quot;robotic&amp;quot; after heavy fine-tuning. My hypothesis was that injecting real-world quantum noise would act as a form of &lt;strong&gt;Data-Driven Stochastic Regularization&lt;/strong&gt;, giving the model a unique &amp;quot;temperature&amp;quot; and preventing it from overfitting to synthetic reasoning patterns.&lt;/p&gt; &lt;p&gt;I've uploaded Q4_K_M and Q8_0 quants.&lt;/p&gt; &lt;p&gt;Check this out on Ollama or LM Studio!&lt;br /&gt; &lt;a href="https://huggingface.co/squ11z1/Hypnos-i1-8B"&gt;https://huggingface.co/squ11z1/Hypnos-i1-8B&lt;/a&gt; or &lt;code&gt;ollama run squ11z1/hypnos-i1-8B&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Bid5976"&gt; /u/Disastrous_Bid5976 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5frn9/release_hypnos_i18b_i_finetuned_hermes_3_on_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5frn9/release_hypnos_i18b_i_finetuned_hermes_3_on_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5frn9/release_hypnos_i18b_i_finetuned_hermes_3_on_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T12:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5by1a</id>
    <title>Qwen3-Next support in llama.cpp almost ready!</title>
    <updated>2025-11-24T08:41:57+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"&gt; &lt;img alt="Qwen3-Next support in llama.cpp almost ready!" src="https://external-preview.redd.it/HHVAlPQ4eWe2-qdcom6wml40H6sWoSOnzo2PuJ4icH8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86d583e47273dca1af24d1e45d57a0f1544aa04c" title="Qwen3-Next support in llama.cpp almost ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/15940#issuecomment-3567006967"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T08:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5r6vp</id>
    <title>Supertonic WebGPU: blazingly fast text-to-speech running 100% locally in your browser.</title>
    <updated>2025-11-24T19:51:48+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5r6vp/supertonic_webgpu_blazingly_fast_texttospeech/"&gt; &lt;img alt="Supertonic WebGPU: blazingly fast text-to-speech running 100% locally in your browser." src="https://external-preview.redd.it/bnE1bWMxM3RiOTNnMecSo005PIULOnLc1HMBGahwp1rxPwmS_uFA5SEw8lvq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cd248206ce70028c5841b56f2a00da414e62be4" title="Supertonic WebGPU: blazingly fast text-to-speech running 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week, the Supertone team released Supertonic, an extremely fast and high-quality text-to-speech model. So, I created a demo for it that uses Transformers.js and ONNX Runtime Web to run the model 100% locally in the browser on WebGPU. The original authors made a web demo too, and I did my best to optimize the model as much as possible (up to ~40% faster in my tests, see below).&lt;/p&gt; &lt;p&gt;I was even able to generate a ~5 hour audiobook in under 3 minutes. Amazing, right?!&lt;/p&gt; &lt;p&gt;Link to demo (+ source code): &lt;a href="https://huggingface.co/spaces/webml-community/Supertonic-TTS-WebGPU"&gt;https://huggingface.co/spaces/webml-community/Supertonic-TTS-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* From my testing, for the same 226-character paragraph (on the same device): the &lt;a href="https://huggingface.co/onnx-community/Supertonic-TTS-ONNX"&gt;newly-optimized model&lt;/a&gt; ran at ~1750.6 characters per second, while the original ran at ~1255.6 characters per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b12eez2tb93g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5r6vp/supertonic_webgpu_blazingly_fast_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5r6vp/supertonic_webgpu_blazingly_fast_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T19:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5jh9l</id>
    <title>Universal LLM Memory Doesn't Exist</title>
    <updated>2025-11-24T15:09:08+00:00</updated>
    <author>
      <name>/u/selund1</name>
      <uri>https://old.reddit.com/user/selund1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5jh9l/universal_llm_memory_doesnt_exist/"&gt; &lt;img alt="Universal LLM Memory Doesn't Exist" src="https://preview.redd.it/z9mcdq37z73g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0117b1e8971316aa60731086907017a976288589" title="Universal LLM Memory Doesn't Exist" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a write-up I just published and would love local / self-hosted perspectives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I benchmarked Mem0 and Zep as ‚Äúuniversal memory‚Äù layers for agents on MemBench (4,000 conversational QA cases with reflective memory), using gpt-5-nano and comparing them to a plain long-context baseline.&lt;/p&gt; &lt;p&gt;Both memory systems were * &lt;strong&gt;14‚Äì77√ó more expensive&lt;/strong&gt; over a full conversation * &lt;strong&gt;~30% less accurate&lt;/strong&gt; at recalling facts than just passing the full history as context &lt;/p&gt; &lt;p&gt;The shared ‚ÄúLLM-on-write‚Äù pattern (running background LLMs to extract/normalise facts on every message) is a poor fit for working memory / execution state, even though it can be useful for long-term semantic memory.&lt;/p&gt; &lt;p&gt;I tried running the test locally and it was even worse: prompt processing completely blew up latency because of the N+1 effect from all the extra ‚Äúmemory‚Äù calls. On a single box, every one of those calls competes with the main model for compute.&lt;/p&gt; &lt;p&gt;My takeaway:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Working memory / execution state (tool outputs, logs, file paths, variables) wants simple, lossless storage (KV, append-only logs, sqlite, etc.).&lt;/li&gt; &lt;li&gt;Semantic memory (user prefs, long-term profile) can be a fuzzy vector/graph layer, but probably shouldn‚Äôt sit in the critical path of every message.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Write-up and harness:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog post: &lt;a href="https://fastpaca.com/blog/memory-isnt-one-thing"&gt;https://fastpaca.com/blog/memory-isnt-one-thing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark tool: &lt;a href="https://github.com/fastpaca/pacabench"&gt;https://github.com/fastpaca/pacabench&lt;/a&gt; (see &lt;code&gt;examples/membench_qa_test&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you doing for &lt;strong&gt;local&lt;/strong&gt; dev?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you using any ‚Äúuniversal memory‚Äù libraries with local models?&lt;/li&gt; &lt;li&gt;Have you found a setup where an LLM-driven memory layer actually beats long context end to end?&lt;/li&gt; &lt;li&gt;Is anyone explicitly separating semantic vs working memory in their local stack?&lt;/li&gt; &lt;li&gt;Is there a better way I can benchmark this quicker locally? Using SLMs ruin fact extraction efficacy and feels &amp;quot;unfair&amp;quot;, but prompt processing in lm studio (on my mac studio m3 ultra) is too slow&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/selund1"&gt; /u/selund1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z9mcdq37z73g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5jh9l/universal_llm_memory_doesnt_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5jh9l/universal_llm_memory_doesnt_exist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T15:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5ynpr</id>
    <title>Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang.</title>
    <updated>2025-11-25T00:50:54+00:00</updated>
    <author>
      <name>/u/neat_space</name>
      <uri>https://old.reddit.com/user/neat_space</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"&gt; &lt;img alt="Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang." src="https://b.thumbs.redditmedia.com/OBtMIflKSvaUYAFVz4VdaUGn7N4bjfxcn6myRC9AdSs.jpg" title="Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is &lt;a href="https://caseys-evals.com/"&gt;my own benchmark.&lt;/a&gt; (Apologies mobile users, I still need to fix the site on mobile D:)&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Esoteric_programming_language"&gt;Esolang definition&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I've tested 3 open weights models, and of the course the shiny new Claude 4.5 Opus. New additions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; Qwen3-235B-A22B thinking, scores 29.4&lt;/p&gt; &lt;p&gt;&lt;strong&gt;7)&lt;/strong&gt; Claude 4.5 Opus, scoring 20.9&lt;/p&gt; &lt;p&gt;&lt;strong&gt;16)&lt;/strong&gt; Deepseek v3.2 exp, scoring 16.2&lt;/p&gt; &lt;p&gt;&lt;strong&gt;17)&lt;/strong&gt; Kimi k2 thinking, scoring 16.1&lt;/p&gt; &lt;p&gt;I was pretty surpised by all results here. Qwen for doing so incredibly well, and the other 3 for underperforming. The Claude models are all run without thinking which kinda handicaps them, so you could argue 4.5 Opus actually did quite well.&lt;/p&gt; &lt;p&gt;The fact that, of the the models I've tested, an open weights model is the current SOTA has really taken me by surprise! Qwen took ages to test though, boy does that model think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neat_space"&gt; /u/neat_space &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p5ynpr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T00:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5fe9u</id>
    <title>Kimi: Wait... I beat Gemini 3? For real?</title>
    <updated>2025-11-24T12:09:46+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt; &lt;img alt="Kimi: Wait... I beat Gemini 3? For real?" src="https://b.thumbs.redditmedia.com/vHmqwt4RbzyHVTmK5Rpwoiu8qk8apNZx0Pd1Q5Gdc7w.jpg" title="Kimi: Wait... I beat Gemini 3? For real?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3d2q76ci473g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f00ae8be20af807202bfbb40d8cd9e4e18f0a736"&gt;https://preview.redd.it/3d2q76ci473g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f00ae8be20af807202bfbb40d8cd9e4e18f0a736&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf when&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T12:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5epot</id>
    <title>The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted</title>
    <updated>2025-11-24T11:32:45+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt; &lt;img alt="The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, this is Owen Arli from &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; and this is the first model release we created in a while. We previously created models finetuned for more creativity with our &lt;a href="https://huggingface.co/collections/ArliAI/rpr-models"&gt;RpR&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/ArliAI/rpmax-v1-models"&gt;RPMax&lt;/a&gt; models.&lt;/p&gt; &lt;p&gt;After seeing the post by Jim Lai on Norm-Preserving Biprojected Abliteration &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;here&lt;/a&gt;, I immediately thought that no one has done abliteration this way and that the &amp;quot;norm-preserving&amp;quot; part was a brilliant improvement in the method to abliterate models, and appears to me like it is objectively the best way to abliterate models. You can find the full technical details in his post, but I will explain the gist of it here.&lt;/p&gt; &lt;h1&gt;The problem:&lt;/h1&gt; &lt;p&gt;Typical abliteration methods finds the refusal vector and simply subtracts it from the weights, this causes the &amp;quot;length&amp;quot; (Norm) of the weight vectors to be altered. This is a problem because this &amp;quot;length&amp;quot; usually dictates how &amp;quot;important&amp;quot; a neuron is and how much it contributes, so changing it will cause damage to the model's general intelligence.&lt;/p&gt; &lt;h1&gt;The solution:&lt;/h1&gt; &lt;p&gt;This Norm-Preserving technique modifies the direction the weights point in, but forces them to keep their original length.&lt;/p&gt; &lt;p&gt;Essentially, by removing the refusal in this way you can potentially also improve the model's performance instead of diminishing it. &lt;/p&gt; &lt;p&gt;Trying out the &lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated"&gt;Gemma 3 12B&lt;/a&gt; model example, it clearly works extremely well compared to regular abliteration methods that often leaves the model broken until further finetuning. Which explains why the model ranks so high in the UGI leaderboard even though its base was Gemma 3 12B which is a notoriously censored model.&lt;/p&gt; &lt;h1&gt;The result:&lt;/h1&gt; &lt;p&gt;Armed with a new 2xRTX Pro 6000 server I just built for Arli AI model experimentation, I set out to try and apply this abliteration technique to the much larger and smarter GLM-4.5-Air. Which ended up in what I think is undoubtedly one of the most interesting model I have ever used.&lt;/p&gt; &lt;p&gt;Its not that GLM-4.5-Air is usually plagued with refusals, but using this &amp;quot;Derestricted&amp;quot; version feels like the model suddenly becomes free to do anything it wants without trying to &amp;quot;align&amp;quot; to a non-existent guideline either visibly or subconsciously. It's hard to explain without trying it out yourself.&lt;/p&gt; &lt;p&gt;For an visible example, I bet that those of you running models locally or through an API will definitely have tried to add a system prompt that says &amp;quot;You are a person and not an AI&amp;quot; or something along those lines. Usually even with such a system prompt and nothing in the context that suggests it is an AI, the model will stubbornly still insist that it is an AI and it is unable to do &amp;quot;human-like&amp;quot; things. With this model, just adding that prompt immediately allows the model to pretend to act like a human in its response. No hesitation or any coaxing needed. &lt;/p&gt; &lt;p&gt;The most impressive part about this abliteration technique is definitely the fact that it has somehow made the model a better instruction follower instead of just a braindead NSFW-capable model from typical abliteration. As for it's intelligence, it has not been benchmarked but I believe that just using the model and feeling it out to see if it has degraded in capabilities is better than just checking benchmarks. Which in this case, the model does feel like it is just as smart if not better than the original GLM-4.5-Air.&lt;/p&gt; &lt;p&gt;You can find the model available on our API, or you can download them yourself from the HF links below!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model downloads:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Original: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted&lt;/a&gt;&lt;/li&gt; &lt;li&gt;FP8: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;INT8: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will be working to create more of these Derestricted models, along with many new finetuned models too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T11:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5wjia</id>
    <title>Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others</title>
    <updated>2025-11-24T23:18:13+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt; &lt;img alt="Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others" src="https://b.thumbs.redditmedia.com/jl1e-X5a7OxM7ptJPHbpLZFTeFQd-7kAhlFa8Y3ToLA.jpg" title="Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm from the SWE-bench team. We maintain a leaderboard where we evaluate all models with the exact same agent and prompts so that we can compare models apple-to-apple.&lt;/p&gt; &lt;p&gt;We just finished evaluating Opus 4.5 and it's back at #1 on the leaderboard. However, it's by quite a small margin (only 0.2%pts ahead of Gemini 3, i.e., just a single task) and it's clearly more expensive than the other models that achieve top scores.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/svt1p1b9fa3g1.png?width=3160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4ea5388eebbc540d03bdfa101614411dcb55a62"&gt;https://preview.redd.it/svt1p1b9fa3g1.png?width=3160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4ea5388eebbc540d03bdfa101614411dcb55a62&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, Opus 4.5 takes fewer steps than Sonnet 4.5. About as many as Gemini 3 Pro, but much more than the GPT-5.1 models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sx5o0e9cfa3g1.png?width=2251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68dd5df936d150ef8b697f150ddcd365f50f909e"&gt;https://preview.redd.it/sx5o0e9cfa3g1.png?width=2251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68dd5df936d150ef8b697f150ddcd365f50f909e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to get maximum performance, you should set the step limit to at least 100:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/52gyo5pefa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfdaf2b849abe875e0693beb08da4d1e9e0a5678"&gt;https://preview.redd.it/52gyo5pefa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfdaf2b849abe875e0693beb08da4d1e9e0a5678&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limiting the max number of steps also allows you to balance avg cost vs performance (interestingly Opus 4.5 can be more cost-efficient than Sonnet 4.5 for lower step limits). &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gymvl4hffa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c6cfd7a42eec0c88d8401fad5cfcef8a2f3a693"&gt;https://preview.redd.it/gymvl4hffa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c6cfd7a42eec0c88d8401fad5cfcef8a2f3a693&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find all other models at &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt; (will be updated in the next hour with the new results). You can also reproduce the numbers by using &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt; [MIT license]. There is a tutorial in the documentation on how to evaluate on SWE-bench (it's a 1-liner).&lt;/p&gt; &lt;p&gt;We're also currently evaluating minimax-m2 and other open source models and will be back with a comparison of the most open source models soon (we tend to take a bit longer at evaluating these because it often has more infra/logistics hiccups)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T23:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p61ch2</id>
    <title>NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999</title>
    <updated>2025-11-25T02:56:35+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" src="https://external-preview.redd.it/YCPQesYDmOPQ_XkQN8p_ciK514B0FKoU6bNyhy9mcvg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc94dfb5840da6920f1ea749a1fc15f8c8d11b76" title="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you guys think that a RTX Quadro 8000 situation could happen again?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-flagship-rtx-pro-6000-is-now-rtx-5080-cheaper-as-card-price-drops-to-7999"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T02:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5op7z</id>
    <title>From Microsoft, Fara-7B: An Efficient Agentic Model for Computer Use</title>
    <updated>2025-11-24T18:20:05+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5op7z/from_microsoft_fara7b_an_efficient_agentic_model/"&gt; &lt;img alt="From Microsoft, Fara-7B: An Efficient Agentic Model for Computer Use" src="https://external-preview.redd.it/SMZl7m5fVyqBxt4sbd8f4qjQFqlJCLo1Gazp1bcHVhM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcf106148dae72548d6b9683c27e007b4306c1b9" title="From Microsoft, Fara-7B: An Efficient Agentic Model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fara-7B is Microsoft's first agentic small language model (SLM) designed specifically for computer use. With only 7 billion parameters, Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance within its size class and is competitive with larger, more resource-intensive agentic systems.&lt;/p&gt; &lt;p&gt;Multimodal decoder-only language model that takes an image (screenshot) + text context. It directly predicts thoughts and actions with grounded arguments. Current production baselines leverage Qwen 2.5-VL (7B).&lt;/p&gt; &lt;p&gt;Parameters: 7 Billion&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/Fara-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5op7z/from_microsoft_fara7b_an_efficient_agentic_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5op7z/from_microsoft_fara7b_an_efficient_agentic_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T18:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5qzft</id>
    <title>Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level</title>
    <updated>2025-11-24T19:44:03+00:00</updated>
    <author>
      <name>/u/AskGpts</name>
      <uri>https://old.reddit.com/user/AskGpts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"&gt; &lt;img alt="Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level" src="https://preview.redd.it/xslefnsmd93g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d03c76fdecbb27a88d2b15a9cd2eaa3d225b151c" title="Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Andrew Ng just announced a new Agentic Reviewer that gives research feedback approaching human-level performance.&lt;/p&gt; &lt;p&gt;It was trained on ICLR 2025 reviews and scored:&lt;/p&gt; &lt;p&gt;0.41 correlation between two human reviewers&lt;/p&gt; &lt;p&gt;0.42 correlation between the AI and a human reviewer&lt;/p&gt; &lt;p&gt;Meaning: The AI reviewer is now effectively as reliable as a human reviewer. And it can potentially replace the 6-month feedback loop researchers normally suffer through when submitting papers.&lt;/p&gt; &lt;p&gt;It searches arXiv for context, analyzes your paper, and returns structured review comments instantly.&lt;/p&gt; &lt;p&gt;For anyone who‚Äôs had a paper rejected multiple times and waited months each round‚Ä¶ this could be game-changing.&lt;/p&gt; &lt;p&gt;Try the tool here:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://paperreview.ai"&gt;https://paperreview.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskGpts"&gt; /u/AskGpts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xslefnsmd93g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T19:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5u44r</id>
    <title>That's why local models are better</title>
    <updated>2025-11-24T21:42:13+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt; &lt;img alt="That's why local models are better" src="https://preview.redd.it/7s5e59vpy93g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91d4c29a99283e56fcfd8614cc10c6d72a0af91a" title="That's why local models are better" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is why the local ones are better than the private ones in addition to this model is still expensive, I will be surprised when the US models reach an optimized price like those in China, the price reflects the optimization of the model, did you know ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7s5e59vpy93g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T21:42:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
