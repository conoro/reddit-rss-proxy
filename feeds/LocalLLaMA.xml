<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-23T10:34:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rbvbzt</id>
    <title>Best open-source coder model for replacing Claude Code with Qwen locally?</title>
    <updated>2026-02-22T19:40:34+00:00</updated>
    <author>
      <name>/u/pauljeba</name>
      <uri>https://old.reddit.com/user/pauljeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m currently using Claude Code but want to move fully local.&lt;/p&gt; &lt;p&gt;I’m specifically looking for a strong coding model for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude code like capaiblities - code + bash &lt;/li&gt; &lt;li&gt;Long file capabiliites&lt;/li&gt; &lt;li&gt;Read image, files&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m considering &lt;code&gt;Qwen3-Coder&lt;/code&gt;, but I’m unsure:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is &lt;code&gt;Qwen3-Coder&lt;/code&gt; the best choice for a 12GB GPU?&lt;/li&gt; &lt;li&gt;Should I instead run a smaller Qwen coder model (7B/14B) quantized?&lt;/li&gt; &lt;li&gt;Are there better alternatives that outperform Qwen for coding in this VRAM range?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would appreciate real-world experience. If there is an hardward upgrade recommendation what would that be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pauljeba"&gt; /u/pauljeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcdicv</id>
    <title>3 weeks of running qwen2.5:14b in an agentic loop - context management is where everything breaks</title>
    <updated>2026-02-23T10:02:49+00:00</updated>
    <author>
      <name>/u/justserg</name>
      <uri>https://old.reddit.com/user/justserg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running qwen2.5:14b locally for about 3 weeks as part of an automation pipeline - not chatting with it, but using it to actually do things: read files, make decisions, call tools, write outputs. The hardware part worked fine. What I completely underestimated was context management.&lt;/p&gt; &lt;p&gt;The problem isn't that local models are bad at long contexts. Qwen handles 128k tokens on paper. The problem is what happens to quality as you fill that window. Around 60-70% capacity, the model starts ignoring things it read earlier. It doesn't fail loudly - it just quietly forgets constraints you set at the top of the prompt. You get plausible-looking output that misses requirements you specified 10,000 tokens ago.&lt;/p&gt; &lt;p&gt;I caught this because the pipeline was producing outputs that were technically correct but violated a formatting rule I'd set in the system prompt. Took me two days to figure out it wasn't a logic error - it was just the model not &amp;quot;seeing&amp;quot; the beginning of its own context anymore.&lt;/p&gt; &lt;p&gt;The fix that actually worked: aggressive context pruning between steps. Instead of one long running context, I reset between major task phases and re-inject only what's essential. It felt wrong at first - like I was throwing away useful state. But the consistency improvements were immediate and obvious.&lt;/p&gt; &lt;p&gt;The other thing I didn't expect: streaming matters for pipeline latency in a non-obvious way. If you're not streaming and you're waiting for a 2000-token response, you're blocking everything downstream. Obvious in hindsight, but I had batch mode on by default and it was creating weird bottlenecks.&lt;/p&gt; &lt;p&gt;The model itself is genuinely good. On structured reasoning tasks with a clear prompt, it rivals what I was getting from API calls a year ago. The failure modes are just different from what you'd expect if you've only ever used it interactively.&lt;/p&gt; &lt;p&gt;If you're building anything agentic with local models, treat context like RAM - don't just keep adding to it and assume everything stays accessible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justserg"&gt; /u/justserg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcdicv/3_weeks_of_running_qwen2514b_in_an_agentic_loop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcdicv/3_weeks_of_running_qwen2514b_in_an_agentic_loop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcdicv/3_weeks_of_running_qwen2514b_in_an_agentic_loop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T10:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcbold</id>
    <title>what are some top OCR models that can deal with handwritten text and mathematical formulas?</title>
    <updated>2026-02-23T08:09:06+00:00</updated>
    <author>
      <name>/u/starman_hero</name>
      <uri>https://old.reddit.com/user/starman_hero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what are some top OCR models that can deal with handwritten text and mathematical formulas? &lt;/p&gt; &lt;p&gt;so far i have tested with PaddleOCR. it was good with deal handwritten text. But it is not so great for when it comes to dealing with mathematicals symbols. &lt;/p&gt; &lt;p&gt;i tried to run Deepseek OCR. but the problem is, I do not have a graphics card. &lt;/p&gt; &lt;p&gt;i tried with OpenAI too. they do a good job. but it is not. ( it is not local. i used the API way ). &lt;/p&gt; &lt;p&gt;so what are some models that i can run on my machine and can also interpret handwritten and mathematical symbols. &lt;/p&gt; &lt;p&gt;i am new to running models and specifically dealing with OCR. so any inputs would be appreciated too. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starman_hero"&gt; /u/starman_hero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcbold/what_are_some_top_ocr_models_that_can_deal_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcbold/what_are_some_top_ocr_models_that_can_deal_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcbold/what_are_some_top_ocr_models_that_can_deal_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T08:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcalyu</id>
    <title>MiniMax 2.5 on DGX SPARK system.</title>
    <updated>2026-02-23T07:03:34+00:00</updated>
    <author>
      <name>/u/DOOMISHERE</name>
      <uri>https://old.reddit.com/user/DOOMISHERE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so i've been working with minimax 2.5 (MiniMax-M2.5-UD-Q3_K_XL),&lt;br /&gt; im amazed by this model, the quality of code is just on another level. &lt;/p&gt; &lt;p&gt;my issue is that i can only work with it in maximum 65K context (bigger than that - crashes on load - out of memory) , normal usage lands on 125GB RAM usage (which is too much).&lt;br /&gt; so i decided to try MiniMax-M2.5-UD-Q2_K_XL, which runs fine with context of 192K,&lt;br /&gt; but i wonder whats the difference between the two models when it comes to coding ?&lt;br /&gt; anyone ever run coding benchmark on both of Q2 and Q3 ?&lt;br /&gt; i didnt find any info online...&lt;br /&gt; im sure Q3 is better, but by how much ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DOOMISHERE"&gt; /u/DOOMISHERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcalyu/minimax_25_on_dgx_spark_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcalyu/minimax_25_on_dgx_spark_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcalyu/minimax_25_on_dgx_spark_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T07:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcb1n2</id>
    <title>Looking for an MCP that semantically searches for working snippets of code</title>
    <updated>2026-02-23T07:29:41+00:00</updated>
    <author>
      <name>/u/babble_prune</name>
      <uri>https://old.reddit.com/user/babble_prune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Often, Claude still messes up on common frontend patterns. When that happens, sometimes I can give Claude documentation (eg for implementing supabase auth). But other times, docs don't have the answer (eg for swift / macOS, unfocusing an input box when the user clicks elsewhere). The code with the relevant patterns is &lt;em&gt;probably&lt;/em&gt; in some open source repos, but I just don't know which ones or where to find them. I think that a lot of &amp;quot;unhobbling&amp;quot; could be gained with a powerful search of existing code, and I'm wondering if anyone uses a tool for this or something adjacent.&lt;/p&gt; &lt;p&gt;I just found &lt;a href="https://vercel.com/blog/grep-a-million-github-repositories-via-mcp"&gt;Grep MCP&lt;/a&gt; by vercel but I'm skeptical because it uses regex/patterns. I should try it -- but I'm looking for something closer to semantic search. Like &amp;quot;search for a chat input box for tailwind + react and condition on existing code to generate this code&amp;quot;. I would pay for this if it worked.&lt;/p&gt; &lt;p&gt;Aside: I wonder if a massive &lt;a href="https://en.wikipedia.org/wiki/A_Pattern_Language"&gt;pattern language&lt;/a&gt; of UI problems and code solutions would work. With a very lightweight LLM that does the search, maybe with the help of some semantic clustering (eg user interface) and structured clustering (eg tailwind css + react).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/babble_prune"&gt; /u/babble_prune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcb1n2/looking_for_an_mcp_that_semantically_searches_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcb1n2/looking_for_an_mcp_that_semantically_searches_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcb1n2/looking_for_an_mcp_that_semantically_searches_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T07:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbvmpk</id>
    <title>Running Llama 3.2 1B entirely on an AMD NPU on Linux (Strix Halo, IRON framework, 4.4 tok/s)</title>
    <updated>2026-02-22T19:51:45+00:00</updated>
    <author>
      <name>/u/SuperTeece</name>
      <uri>https://old.reddit.com/user/SuperTeece</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got Llama 3.2 1B running inference entirely on the AMD NPU on Linux. Every operation (attention, GEMM, RoPE, RMSNorm, SiLU, KV cache) runs on the NPU; no CPU or GPU fallback. As far as I can tell, this is the first time anyone has publicly documented this working on Linux.&lt;/p&gt; &lt;h2&gt;Hardware&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen AI Max+ 395 (Strix Halo)&lt;/li&gt; &lt;li&gt;NPU: XDNA2, device ID npu5 (PCI 1022:17f0)&lt;/li&gt; &lt;li&gt;64GB LPDDR5X unified memory&lt;/li&gt; &lt;li&gt;Fedora 43, kernel 6.18.8&lt;/li&gt; &lt;li&gt;Model: meta-llama/Llama-3.2-1B (official Meta weights)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;Prefill time: 0.6921 seconds (13 tokens) Tokens generated: 20 Tokens per second: 4.40 Time per token: 0.2638 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NPU validation benchmark: &lt;strong&gt;51.0 TOPS&lt;/strong&gt; (GEMM, via xrt-smi validate).&lt;/p&gt; &lt;h2&gt;Scaling&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="center"&gt;Prompt Length&lt;/th&gt; &lt;th align="center"&gt;Prefill (s)&lt;/th&gt; &lt;th align="center"&gt;Prefill tok/s&lt;/th&gt; &lt;th align="center"&gt;Decode tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="center"&gt;13&lt;/td&gt; &lt;td align="center"&gt;0.67&lt;/td&gt; &lt;td align="center"&gt;19&lt;/td&gt; &lt;td align="center"&gt;4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="center"&gt;128&lt;/td&gt; &lt;td align="center"&gt;0.71&lt;/td&gt; &lt;td align="center"&gt;180&lt;/td&gt; &lt;td align="center"&gt;4.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="center"&gt;2048&lt;/td&gt; &lt;td align="center"&gt;2.22&lt;/td&gt; &lt;td align="center"&gt;923&lt;/td&gt; &lt;td align="center"&gt;4.34&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Decode is flat at ~4.4 tok/s regardless of prompt length. Prefill scales well (923 tok/s at 2048 tokens).&lt;/p&gt; &lt;h2&gt;The Stack&lt;/h2&gt; &lt;p&gt;Getting here required building everything from source. Fedora 43's in-tree amdxdna driver (v0.1) is too old, so you need the out-of-tree v1.0.0 from amd/xdna-driver on GitHub. That build also produces the dev firmware and XRT 2.23 libraries. On top of that, AMD's IRON framework (also on GitHub) plus mlir-aie v1.2.0 handle the actual NPU programming.&lt;/p&gt; &lt;p&gt;GCC 15 on Fedora 43 breaks the XRT build at link time (cannot find -lstdc++). Fix:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LIBRARY_PATH=/usr/lib/gcc/x86_64-redhat-linux/15:/usr/lib64:$LIBRARY_PATH &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;IRON also hardcodes llvm-objcopy-18 but Fedora ships LLVM 21, so you need a symlink.&lt;/p&gt; &lt;h2&gt;Where the Time Goes&lt;/h2&gt; &lt;p&gt;Profiling revealed the bottleneck: &lt;strong&gt;179 kernel dispatches per token&lt;/strong&gt;, averaging 1.4ms each through XRT. That's 75% of inference time in dispatch overhead, not compute. Buffer I/O via unified memory is fast (sub-0.1ms). The optimization path is fewer, larger dispatches via operator fusion.&lt;/p&gt; &lt;p&gt;4.4 tok/s from a 1B model won't replace GPU inference. On the same machine, Qwen3-32B (32x larger) runs at 6-7 tok/s on the GPU via Vulkan. But the NPU validated at 51 TOPS, so the gap is a software problem, not hardware. The NPU also runs independently, so you could run an LLM on it while the GPU does something else.&lt;/p&gt; &lt;h2&gt;Gotchas&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;prompt_len must match your actual token count (IRON compiles RoPE kernels for a fixed sequence length)&lt;/li&gt; &lt;li&gt;First run takes ~10 minutes to compile NPU kernels (cached after that)&lt;/li&gt; &lt;li&gt;Must use insmod for the out-of-tree driver; modprobe loads the stock one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote up the full walkthrough in a three-part blog series (linked in comments). Happy to answer setup questions.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;A note on how this was made: the research, testing, debugging, and writing was done by Ellie, an AI assistant backed by Claude Opus 4.6 (Anthropic) and local models. TC provided the hardware, direction, and editorial guidance. We believe in transparency about AI involvement in technical work.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note from TC:&lt;/strong&gt; I admit that this work is out of my technical depth. My motivation came from annoyance at having an NPU that was apparently useless on Linux and curiosity if Ellie (Opus) could connect together any other work being done on the topic to at least move the needle a smidge. If anyone is reading this post and knows it to be slop on a technical level, I'd love to hear why for my own edification. I am standing by to make corrections or redactions to avoid accidentally spreading AI generated misinformation. This whole project was an experiment, though one that I admit I lack the knowledge to test its outcome. I hope to hear from those who do and that it is useful in some way. -TC&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperTeece"&gt; /u/SuperTeece &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccn85</id>
    <title>When RMSNorm Fails: The Geometric Collapse of Unstable LLMs</title>
    <updated>2026-02-23T09:09:40+00:00</updated>
    <author>
      <name>/u/Accurate-Turn-2675</name>
      <uri>https://old.reddit.com/user/Accurate-Turn-2675</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccn85/when_rmsnorm_fails_the_geometric_collapse_of/"&gt; &lt;img alt="When RMSNorm Fails: The Geometric Collapse of Unstable LLMs" src="https://external-preview.redd.it/ihS-tlPjVV8ihasHuE-tqX-NDYxn0pl-im9PQaQOrGE.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=04a6cea06573aee7a26debbfb3f7be58889518d1" title="When RMSNorm Fails: The Geometric Collapse of Unstable LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every major modern LLM has quietly dropped standard Layer Normalization in favor of RMSNorm which my &lt;a href="https://sifal.social/posts/Why-Modern-LLMs-Dropped-Mean-Centering-(And-Got-Away-With-It"&gt;blog&lt;/a&gt;/), I show that it can be reformulated this way:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pbol8c8xl7lg1.png?width=1139&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=379f9984935808c6ada4d91949ffe821238a1244"&gt;Reformulation of RMSNorm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By removing the explicit mean-centering step, we save compute under the assumption that a network's variance (&lt;strong&gt;σ&lt;/strong&gt;) will always dominate its mean shift (&lt;strong&gt;μ&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;But what actually happens to the geometry of your latent space when that assumption breaks?&lt;/p&gt; &lt;p&gt;By mathematically decomposing RMSNorm into its signal and noise components and visualizing the exact transformations in 3D space, a hidden and severe failure mode emerges: &lt;strong&gt;Directional Collapse&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Here is the breakdown of what RMSNorm is actually doing to your data:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Hidden Math:&lt;/strong&gt; RMSNorm's approximation decomposes into standard LayerNorm multiplied by a dynamic signal-to-noise ratio (&lt;strong&gt;μ/σ&lt;/strong&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Healthy Regime (σ ≫ |μ|):&lt;/strong&gt; When the network is stable, the mean is tiny compared to the variance. The dampening factor vanishes, and RMSNorm beautifully approximates the perfectly spread-out spherical geometry of standard LayerNorm.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://i.redd.it/y7linwifm7lg1.gif"&gt;https://i.redd.it/y7linwifm7lg1.gif&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Unstable Regime (μ ≫ σ):&lt;/strong&gt; When the network spikes and the mean violently drifts, standard LayerNorm would silently correct the shift by explicitly centering the data. RMSNorm cannot do this. Instead, as the mean explodes, the math forces the per-token variation to become negligible.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Geometric Collapse:&lt;/strong&gt; The outputs still successfully land on the target &lt;strong&gt;√n&lt;/strong&gt; hypersphere. However, because they lost their individual variation, all highly-shifted tokens violently collapse toward one of two antipodal poles (determined by &lt;strong&gt;sign(μ) · γ&lt;/strong&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://i.redd.it/wauquyr6l7lg1.gif"&gt;(Notice how the high-mean data, shown in crimson and purple, loses all directional diversity and strictly converges to antipodal poles)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Takeaway:&lt;/strong&gt; When RMSNorm fails, the network doesn't lose signal &lt;em&gt;amplitude&lt;/em&gt;; it loses token &lt;em&gt;discriminability&lt;/em&gt;. Inputs that were genuinely different become geometrically indistinguishable, piling up at a single pole and starving the subsequent attention layers of the directional diversity they need to function.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/ndb1i71tp7lg1.gif"&gt;https://i.redd.it/ndb1i71tp7lg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Read more about how I derived this in my&lt;/em&gt;&lt;/strong&gt; &lt;a href="https://sifal.social/posts/Why-Modern-LLMs-Dropped-Mean-Centering-(And-Got-Away-With-It"&gt;&lt;strong&gt;&lt;em&gt;blog&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;/)&lt;strong&gt;&lt;em&gt;, and much more about the geometric intuition.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accurate-Turn-2675"&gt; /u/Accurate-Turn-2675 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccn85/when_rmsnorm_fails_the_geometric_collapse_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccn85/when_rmsnorm_fails_the_geometric_collapse_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rccn85/when_rmsnorm_fails_the_geometric_collapse_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T09:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rca5p7</id>
    <title>Which model for meeting transcript summarisation?</title>
    <updated>2026-02-23T06:37:04+00:00</updated>
    <author>
      <name>/u/peglegsmeg</name>
      <uri>https://old.reddit.com/user/peglegsmeg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;I'm using qwen3 30B A3B 2507 4bit with lm studio for feeding meeting transcripts for summary.&lt;/p&gt; &lt;p&gt;Does this seem like an okay model for the task? Feeling a bit overwhelmed with all the options, I'm only using because a cloud AI suggested it but it might not be current.&lt;/p&gt; &lt;p&gt;I was using Claude API with amazing results but no longer want to send to public offerings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peglegsmeg"&gt; /u/peglegsmeg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rca5p7/which_model_for_meeting_transcript_summarisation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rca5p7/which_model_for_meeting_transcript_summarisation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rca5p7/which_model_for_meeting_transcript_summarisation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T06:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbyg5x</id>
    <title>If you have a RTX 5090 (that has a single connector), you can flash the MSI Lighting 800W VBIOS to get a lower power limit of 300W (and a max power of 660W).</title>
    <updated>2026-02-22T21:36:36+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you guys are doing fine.&lt;/p&gt; &lt;p&gt;As you know, NVIDIA artificially limited the power limit on the 5090s so you don't stack them, and get 6000 PROs instead (6000 PRO can go down to 150W). Even when undervolted it can use 400W sometimes.&lt;/p&gt; &lt;p&gt;If you got a RTX 5090 with a single connector (basically most of them except the BTF versions, and MSI Lighting), you can flash the 800W Lighting VBIOS to get a power limit.&lt;/p&gt; &lt;p&gt;When setting a 400W power limit (50%), it uses 300W max instead.&lt;/p&gt; &lt;p&gt;Why would you ask?&lt;/p&gt; &lt;p&gt;This is because the VBIOS excepts another source of power, and since it isn't there, it over reports the power on the software. Take it as a inverted shunt mod.&lt;/p&gt; &lt;p&gt;The VBIOS is here &lt;a href="https://www.techpowerup.com/vgabios/281640/281640"&gt;https://www.techpowerup.com/vgabios/281640/281640&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;As always with VBIOS flashing, do it at your own risk!&lt;/strong&gt; &lt;strong&gt;If you don't trust this or haven't heard about BIOS flashing, I suggest to not do it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On ASUS cards you lose 1 HDMI, but if you have Astral-Matrix, you keep the pin monitoring power.&lt;/p&gt; &lt;p&gt;You can get nvflash on here &lt;a href="https://www.techpowerup.com/download/nvidia-nvflash/"&gt;https://www.techpowerup.com/download/nvidia-nvflash/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once on Windows, with nvflash64 and the rom file on the same folder, you run this (on cmd as admin):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvflash64 -6 romname.rom press y press y reboot &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And you're good to go! This also works on LACT.&lt;/p&gt; &lt;p&gt;I have made this table with the info for power for reference.&lt;/p&gt; &lt;p&gt;Scaling 800W VBIOS&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50% is 300W real power usage (reported 400W on software)&lt;/li&gt; &lt;li&gt;53% is 321W (reported 424W)&lt;/li&gt; &lt;li&gt;54% is 330W (reported 432W)&lt;/li&gt; &lt;li&gt;55% is 338W (reported 440W)&lt;/li&gt; &lt;li&gt;56% is 345W (reported 448W)&lt;/li&gt; &lt;li&gt;57% is 352W (reported 456W)&lt;/li&gt; &lt;li&gt;59% is 367W (reported 472W)&lt;/li&gt; &lt;li&gt;60% is 375W (reported 480W)&lt;/li&gt; &lt;li&gt;61% is 382W (reported 488W)&lt;/li&gt; &lt;li&gt;62% is 388W (reported 496W)&lt;/li&gt; &lt;li&gt;63% is 397W (reported 504W)&lt;/li&gt; &lt;li&gt;64% is 403W (reported 512W)&lt;/li&gt; &lt;li&gt;73% is 468W (reported 584W)&lt;/li&gt; &lt;li&gt;74% is 478W (reported 592W)&lt;/li&gt; &lt;li&gt;91% is 594W (reported 728W)&lt;/li&gt; &lt;li&gt;92% is 610W (reported 736W)&lt;/li&gt; &lt;li&gt;100% is 660W (reported 800W)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There's also similar behavior for the 1000W and 2500W VBIOS, but those have a higher min power (about 320W), so the 800W is the best one for that and also the safest.&lt;/p&gt; &lt;p&gt;I tried on Linux, since there's nvflash there as well, but got an error about memory address. On Windows flashing works just fine.&lt;/p&gt; &lt;p&gt;Any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T21:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbtfld</id>
    <title>What Other Subs Do you Read to Keep Up with AI?</title>
    <updated>2026-02-22T18:29:17+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what other subs do you recommend to read to keep up with AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T18:29:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbjxpv</id>
    <title>I think openclaw is OVERHYPED. Just use skills</title>
    <updated>2026-02-22T11:51:38+00:00</updated>
    <author>
      <name>/u/Deep_Traffic_7873</name>
      <uri>https://old.reddit.com/user/Deep_Traffic_7873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think openclaw is useful, loop, memory, agents, integrations, but after a week a testing, honestly I don't need it much.&lt;/p&gt; &lt;p&gt;- memory, is nice. But I prefere to have &amp;quot;manual memory&amp;quot;. Prompt: Ok, write what yout learnt in &amp;quot;superreporttrending-skill&amp;quot;. Automatic memory often pollute the context of info you don't care.&lt;/p&gt; &lt;p&gt;- cron. Useful but I already use other tools for that and I can always recall a skill whenever i want. I don't need everyday at 8:00AM, i prefere recall it when i want with up to date data&lt;/p&gt; &lt;p&gt;Conclusion: for me &amp;quot;opencode web&amp;quot; is a much superior option, but much of the &amp;quot;intelligence&amp;quot; and value is the skills that you develop or you integrate, not in the runner itself, what do you think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Traffic_7873"&gt; /u/Deep_Traffic_7873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T11:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbwbgl</id>
    <title>nanollama — train Llama 3 from scratch and export to GGUF, one command, open source</title>
    <updated>2026-02-22T20:17:50+00:00</updated>
    <author>
      <name>/u/ataeff</name>
      <uri>https://old.reddit.com/user/ataeff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;nanollama — train Llama 3 from scratch. &lt;/p&gt; &lt;p&gt;I've been working on a framework for training Llama 3 architecture models from scratch: not fine-tuning, not LoRA, actual from-zero pretraining. The output is a llama.cpp-compatible GGUF file.&lt;/p&gt; &lt;p&gt;The whole pipeline is one command:&lt;/p&gt; &lt;p&gt;'''&lt;/p&gt; &lt;p&gt;bash runs/lambda_train.sh --name mini&lt;/p&gt; &lt;p&gt;'''&lt;/p&gt; &lt;p&gt;This downloads training data, trains the model, and exports GGUF. Verified with llama-cli.&lt;/p&gt; &lt;p&gt;In the the box:&lt;/p&gt; &lt;p&gt;- Llama 3 architecture (RoPE, SwiGLU, RMSNorm, GQA), 8 configs from 46M to 7B&lt;/p&gt; &lt;p&gt;- multi-corpus training (FineWeb-Edu, DCLM, code, math — SmolLM2 recipe)&lt;/p&gt; &lt;p&gt;- native GGUF v3 exporter (no HuggingFace/safetensors conversion)&lt;/p&gt; &lt;p&gt;- personality injection — train base + personality model, subtract weights, get a portable personality vector you can apply to any compatible base&lt;/p&gt; &lt;p&gt;- pure Go inference engine (~9MB binary, reads GGUF, zero runtime deps) for when you don't need the full llama.cpp stack&lt;/p&gt; &lt;p&gt;- beginner's guide — first model in ~30 min on a rented GPU for a few bucks &lt;/p&gt; &lt;p&gt;Trained and verified so far: nano (46M), micro (87M), mini (175M), small (338M). goldie (1.1B, multilingual) is training now.&lt;/p&gt; &lt;p&gt;The point: there's no clean, modern &amp;quot;train from scratch&amp;quot; pipeline for Llama-family models. nanoGPT/nanochat did this for GPT-2, but GPT-2 is 2019 architecture. This is the same idea updated for 2026.&lt;/p&gt; &lt;p&gt;Born from karpathy's nanochat, rewritten for Llama 3. GPLv3.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ariannamethod/nanollama"&gt;https://github.com/ariannamethod/nanollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release: &lt;a href="https://github.com/ariannamethod/nanollama/releases/tag/v0.1.0"&gt;https://github.com/ariannamethod/nanollama/releases/tag/v0.1.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ataeff"&gt; /u/ataeff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T20:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc88vr</id>
    <title>What GPU do you recommend for iterative AI training?</title>
    <updated>2026-02-23T04:53:56+00:00</updated>
    <author>
      <name>/u/EliHusky</name>
      <uri>https://old.reddit.com/user/EliHusky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've racked up a disgusting bill with runpod and think it is time to get my own workstation. &lt;/p&gt; &lt;p&gt;I usually choose GPUs based on the model I’m working with (e.g., RTX Pro 6000 Blackwell for LLMs/VLMs/diffusion, 4090 for smaller TCNs/LSTMs), but honestly I often pick higher-end GPUs more for throughput than VRAM.&lt;/p&gt; &lt;p&gt;So I'm curious, what kinds/sizes of models are you training, and what GPU are you using (or wish you were using)? &lt;/p&gt; &lt;p&gt;My first choice is obviously the pro 6000 blackwell to never think twice about batch size or parameter count again, but the cost doesn't quite justify &amp;quot;ease of use/peace of mind&amp;quot; to me. &lt;/p&gt; &lt;p&gt;I’m heavily leaning toward a 5090... but I’m saying that while staring at a RunPod session using 31GB VRAM for a 1.5B parameter fine-tune, so I’m not exactly confident I won’t regret it. I've also considered getting two 5090s but the lack of nvlink (I've never touched a multi-gpu setup) and the wattage requirements are a turnoff, not to mention we're getting back into the pro 6000 blackwell price range. I build my own pipelines and collect my own data, so iterative training and testing means speed is arguably just as important as VRAM.&lt;/p&gt; &lt;p&gt;I'm completely satisfied with running large model inference off of system ram, so this isn't a deciding factor.&lt;/p&gt; &lt;p&gt;I've done a ton of research, tried and tested a half dozen cards through runpod, and still can't seem to find the most reasonable gpu, so any personal experiences anyone has to share would be greatly appreciated. &lt;/p&gt; &lt;p&gt;TL;DR: what GPU(s) do you have and would you recommend it to someone looking to buy their first at-home AI workstation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliHusky"&gt; /u/EliHusky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T04:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbnczy</id>
    <title>The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets.</title>
    <updated>2026-02-22T14:34:36+00:00</updated>
    <author>
      <name>/u/w1nter5n0w</name>
      <uri>https://old.reddit.com/user/w1nter5n0w</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt; &lt;img alt="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." src="https://preview.redd.it/l8duwvse42lg1.png?width=140&amp;amp;height=106&amp;amp;auto=webp&amp;amp;s=2928d1df2289068d0491626609ab2109106409dc" title="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, a friend of mine posted a thread here (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/&lt;/a&gt;) regarding a project he started called &lt;strong&gt;DeepSeek-Overclock&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal was to create an experimental setup designed to theoretically push the model's reasoning capabilities to the absolute limit. However, the &amp;quot;overclocked&amp;quot; DeepSeek model kept failing during the process. After diving deep into the logs, he realized the model wasn't hallucinating. In many instances, it was rigorously deriving answers that were technically correct but contradicted the provided &amp;quot;gold standard&amp;quot; labels.&lt;/p&gt; &lt;p&gt;He ended up writing Python scripts to verify the math line-by-line from first principles. Then he found out that &lt;strong&gt;the data quality in both the GPQA and HLE (Humanity's Last Exam) test sets is seriously flawed.&lt;/strong&gt; (You can check the link above for the specific details of that investigation).&lt;/p&gt; &lt;p&gt;Fast forward to a couple of days ago, and the &lt;strong&gt;Qwen team just released a paper&lt;/strong&gt; that basically confirms exactly what we saw: the data quality in GPQA and HLE is a mess.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20"&gt;https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Attached the screenshot of Fig. 1: Structural composition of HLE-Verified.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arxiv Link:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2602.13964v2"&gt;https://arxiv.org/abs/2602.13964v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper doesn't mince words. Right from the intro, it bluntly points out that a lot of the questions in the HLE test set are fundamentally broken. And in some cases, &amp;quot;standard answers&amp;quot; that are straight-up wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w1nter5n0w"&gt; /u/w1nter5n0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T14:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc00nj</id>
    <title>In the long run, everything will be local</title>
    <updated>2026-02-22T22:39:00+00:00</updated>
    <author>
      <name>/u/tiguidoio</name>
      <uri>https://old.reddit.com/user/tiguidoio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt; &lt;img alt="In the long run, everything will be local" src="https://preview.redd.it/vqzxm46ri4lg1.png?width=140&amp;amp;height=105&amp;amp;auto=webp&amp;amp;s=f9899bff14b8d1409da4cbfaa0a56aa74bb136e5" title="In the long run, everything will be local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been of the opinion for a while that, long term, we’ll have smart enough open models and powerful enough consumer hardware to run &lt;em&gt;all&lt;/em&gt; our assistants locally both chatbots and coding copilots&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vqzxm46ri4lg1.png?width=3608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c0fb257d744350f8668301a915aeec2b6653fc"&gt;https://preview.redd.it/vqzxm46ri4lg1.png?width=3608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c0fb257d744350f8668301a915aeec2b6653fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Right now it still feels like there’s a trade-off:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Closed, cloud models = best raw quality, but vendor lock-in, privacy concerns, latency, per-token cost&lt;/li&gt; &lt;li&gt;Open, local models = worse peak performance, but full control, no recurring API fees, and real privacy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But if you look at the curve on both sides, it’s hard not to see them converging:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open models keep getting smaller, better, and more efficient every few months (quantization, distillation, better architectures). Many 7B–8B models are already good enough for daily use if you care more about privacy/control than squeezing out the last 5% of quality&lt;/li&gt; &lt;li&gt;Consumer and prosumer hardware keeps getting cheaper and more powerful, especially GPUs and Apple Silicon–class chips. People are already running decent local LLMs with 12–16GB VRAM or optimized CPU-only setups for chat and light coding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At some point, the default might flip: instead of why would you run this locally?, the real question becomes why would you ship your entire prompt and codebase to a third-party API if you don’t strictly need to? For a lot of use cases (personal coding, offline agents, sensitive internal tools), a strong local open model plus a specialized smaller model might be more than enough&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tiguidoio"&gt; /u/tiguidoio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T22:39:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccsjg</id>
    <title>I made an interactive timeline of 171 LLMs (2017–2026)</title>
    <updated>2026-02-23T09:18:16+00:00</updated>
    <author>
      <name>/u/asymortenson</name>
      <uri>https://old.reddit.com/user/asymortenson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a visual timeline tracking every major Large Language Model — from the original Transformer paper to GPT-5.3 Codex.&lt;/p&gt; &lt;p&gt;171 models, 54 organizations. Filterable by open/closed source, searchable, with milestones highlighted.&lt;/p&gt; &lt;p&gt;Some stats from the data: - 2024–2025 was the explosion: 108 models in two years - Open source reached parity with closed in 2025 (29 vs 28) - Chinese labs account for ~20% of all major releases (10 orgs, 32 models)&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-timeline.com"&gt;https://llm-timeline.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Missing a model? Let me know and I'll add it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asymortenson"&gt; /u/asymortenson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T09:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc1ra2</id>
    <title>My real-world Qwen3-code-next local coding test. So, Is it the next big thing?</title>
    <updated>2026-02-22T23:51:14+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt; &lt;img alt="My real-world Qwen3-code-next local coding test. So, Is it the next big thing?" src="https://preview.redd.it/44qd636p15lg1.png?width=140&amp;amp;height=14&amp;amp;auto=webp&amp;amp;s=35817da51dcc5387e5bc0d9209c8558f639ab5f3" title="My real-world Qwen3-code-next local coding test. So, Is it the next big thing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yesterday I put the Q8 MLX on my 128GB Mac Studio Ultra and wired it to Qwen Code CLI. Fit's there with a huge amount to spare. The first tests were promising - basically did everything I asked: read file, write file, browse web, check system time....blah, blah.&lt;/p&gt; &lt;p&gt;Now the real the task:&lt;/p&gt; &lt;p&gt;I decided on YOLO mode to rewrite the KittenTTS-IOS to windows (which itself is a rewrite of KittenTTS in python). It uses ONYX and a couple of Swift libraries like Misaki for English phoneme.&lt;/p&gt; &lt;p&gt;So, say a medium difficulty. Not super easy, but not super hard, because all the code is basically there. You just need to shake it.&lt;/p&gt; &lt;p&gt;Here is how it went:&lt;/p&gt; &lt;p&gt;Started very well. Plan was solid. Make simple CLI with KittenTTS model, avoid any phoneme manipulation for now. Make ONYX work. Then add Misaki phoneme, avoid bart fallback coz that's a can of worms.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;So it built the main.cpp. Rewrote the main app, created it's own json parser for the KittenTTS dictionary. found windows ONYX, downloaded, linked. ran cmake captured the output, realised it's json parsing was a total crap. Linked &amp;lt;nlohmann/json.hpp&amp;gt; .... aaaaand we are out.&lt;/li&gt; &lt;li&gt;First client timeout then &amp;quot;I'm dead, Dave&amp;quot;. As we get more and more into longer context the prompt parsing gets longer and longer until the client times out.&lt;/li&gt; &lt;li&gt;Restarted maually, told it we are at json.hpp, it finished the patching, compiled - created output.wav&lt;/li&gt; &lt;li&gt;I'm impressed so far. The wav has voice in it, of course all gibberish because we have no phoneme dictionary. The make file is unreadable can of worms.&lt;/li&gt; &lt;li&gt;Next step convert phoneme Misaki to windows. Big hairy project. Again, started cheerful. But we are now editing large files. It can barely finish anything before timeout.&lt;/li&gt; &lt;li&gt;Lot's of manual restarts. (YOLO mode my butt, right?). At some point it starts editing the Swift files, thinking that's what we are doing. Noooo!!!!&lt;/li&gt; &lt;li&gt;I've noticed that most of the time it wastes tokens on trying to figure out how to do stuff like save file it wants to save, because now &amp;quot;it's just too big&amp;quot;. Even starts writing python script to save the file then entering the entire text of lexicon.cpp as a command line - LOL, learning, that's a very stupid thing too.&lt;/li&gt; &lt;li&gt;I mean nice to learn from mistakes, but we are getting to timeouts all the time now by filling the context with unnecessary work. And it of course learns nothing, because that knowledge is lost.&lt;/li&gt; &lt;li&gt;I spent another 60 minutes trying to figure out how to fix qwen code by increasing timeout. Not an easy task as every AI will just hallucinate what you should do. I moved from anthropic style to openai style for the QWEN3 and set generationConfig.timeout to a big number (I have no idea if this even works). Set the KV_cache to quantize at 8 bit in LM studio (again, no idea if it helps). Seems the timeouts are now longer? So maybe a small win?&lt;/li&gt; &lt;li&gt;Well, went to sleep, letting it do something.&lt;/li&gt; &lt;li&gt;In the next day the phoneme test.exe was working sort of (at least it was not throwing 5 pages of errors) - read the 400k phoneme dictionary and output bunch of nonsense, like lookup: Hello -&amp;gt; h╔ÖlO (Is this the correct phoneme? Hardly. Seems we are getting lost in ISO/UDF nightmare) Well, Qwen doesn't know what's going on either.&lt;/li&gt; &lt;li&gt;At this point neither me nor Qwen knows if we are fixing bugs or buggyfying working code. But he is happily doing something.&lt;/li&gt; &lt;li&gt;And writing jokes that get a bit stale after while: &amp;quot;Why do Java developers wear glasses? Because they don't C#&amp;quot;&lt;/li&gt; &lt;li&gt;I start to miss Claude Code. Or Codex. Or anything that doesn't take 30 minutes per turn then tell me client timeout.&lt;/li&gt; &lt;li&gt;It is still fixing it and writing stupid one liner jokes on screen. I mean &amp;quot;fixing it&amp;quot; means sitting in Prompt processing.&lt;/li&gt; &lt;li&gt;Funny, MAC Studio is barely warm. Like it was working nonstop for 8 hours with 89GB model .&lt;/li&gt; &lt;li&gt;The processing prompt is still killing the whole operation. As the context grows, this is a few minutes per turn.&lt;/li&gt; &lt;li&gt;I totally believe the X grifters telling me they bough 10 MAC's for local Agentic work.... yes, sure. You can have huge memory but large context is still going to be snail pace.&lt;/li&gt; &lt;li&gt;19. Looking at the terminal &amp;quot;Just a sec, I'm optimizing the humor... (esc to cancel, 29m 36s)&amp;quot;, been doing something for 30 min. Looking at mac log, generating token, now at around 60k tokens and still going up - a really long output that we will probably never be able to do anything with.&lt;/li&gt; &lt;li&gt;I give Local model coding 5/10 so far. It does kinda work if you have the enormous patience. It's surprising we get that far. It is nowhere what the big boys give you, even for $20/month.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;--- It is still coding --- (definitely now in some Qwen3 loop)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/44qd636p15lg1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6af08a0a84011baa5dc72985d73634bbe04a35f"&gt;https://preview.redd.it/44qd636p15lg1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6af08a0a84011baa5dc72985d73634bbe04a35f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Whee! We finished, about 24 hours after I started. Now, of course I wasn't babysitting it so IDK how much time it sat idle during the day. Anytime I went by I'd check on it, or restart the process...&lt;/p&gt; &lt;p&gt;The whole thing had to restart or run probably 20-30 times again and again on the same thing for various reasons (timeout or infinite loops).&lt;/p&gt; &lt;p&gt;But, the good thing is: &lt;strong&gt;The project compiles and creates a WAV file with very understandable pronunciation all on just CPU that doesn't sound robotic.&lt;/strong&gt; So that's 100% success. No coding input from my side, no code fixing. No dependencies.&lt;/p&gt; &lt;p&gt;It isn't pleasant to work with it in this capacity I tried (MAC Studio with forever prompt processing) but beggars cannot be choosers and Qwen3-coder-next is a &lt;strong&gt;FREE&lt;/strong&gt; model. So yay, they (Qwen) need to be commanded for their effort. It's amazing how fast we got there, and I remember that.&lt;/p&gt; &lt;p&gt;I'm bumping the result to 6/10 for a local coding experience which is: &lt;strong&gt;good&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final observations and what I learned:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- It's free, good enough, and runs on a home hardware which back in 2023 would be called &amp;quot;insane&amp;quot;&lt;/p&gt; &lt;p&gt;- it can probably work better with small editing/bug fixes/ small additions. The moment it needs to write large code it will be full of issues (if it finishes). It literally didn't wrote a single usable code at once (unlike what I used to see in cc or codex), though it was able to fix all the hundreds issues by itself (testing, assessing, fixing). The process itself took a lot of time.&lt;/p&gt; &lt;p&gt;- it didn't really have problem with tool calling, at least not what I observed. It had problem with tool using, especially when it started producing a lot of code.&lt;/p&gt; &lt;p&gt;- it is NOT a replacement for claude/codex/gemini/other cloud. It just isn't. Maybe as a hobby. It's the difference between a bicycle and a car. You will get there eventually, but it would take much longer and be less pleasant. Well it depends how much you value your time vs money, I guess.&lt;/p&gt; &lt;p&gt;- MAC with unified memory is amazing, for a basic general LLM, but working with code and long context it kills any enjoyment - and that is not dependent on the size of the memory. When the grifters on X saying they are buying 512GB MAC studios for local agentic coding etc - it's BS. It's still a torture - because we have much faster and less painful way using cloud API (and cheaper too). It's pain with 80GB 8 bit quantized model, it would be excruciating with full 250GB model.&lt;/p&gt; &lt;p&gt;- I'm not going to lie to you, I'm not going to use it much, unless I terribly ran out of tokens on cc or codex. I'd check other Chinese big online models that are much cheaper like GLM 5, but honestly the price alone is not deterrent. I firmly believe they (codex, cc) are giving it practically for free. &lt;/p&gt; &lt;p&gt;- I might check other models like step 3.5 (I have it downloaded but didn't use it for anything yet)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T23:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbkeea</id>
    <title>Which one are you waiting for more: 9B or 35B?</title>
    <updated>2026-02-22T12:15:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt; &lt;img alt="Which one are you waiting for more: 9B or 35B?" src="https://preview.redd.it/jyvany3jf1lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f667e97854acf566b7f6d1d56e9c09e17f5a8ee8" title="Which one are you waiting for more: 9B or 35B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jyvany3jf1lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T12:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc97qf</id>
    <title>🌊 Wave Field LLM O(n log n) Successfully Scales to 1B Parameters</title>
    <updated>2026-02-23T05:44:29+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt; &lt;img alt="🌊 Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" src="https://preview.redd.it/6m7q2vzlm6lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ede585956ec96d0434754c49701c58176ad83ad" title="🌊 Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just completed full pretraining of &lt;strong&gt;Wave Field LLM (v4) at 1B scale&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Summary:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 825M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Tokens:&lt;/strong&gt; 1.33B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final Accuracy:&lt;/strong&gt; 27.1%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Time:&lt;/strong&gt; 13.2 hours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn’t a small 30M or 124M experiment anymore.&lt;/p&gt; &lt;p&gt;Wave Field is now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ Stable at near-billion scale&lt;/li&gt; &lt;li&gt;✅ Training cleanly&lt;/li&gt; &lt;li&gt;✅ Converging properly&lt;/li&gt; &lt;li&gt;✅ Saving best checkpoints&lt;/li&gt; &lt;li&gt;✅ Handling &amp;gt;1B tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key takeaway:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;This validates that Wave Field’s field-based interaction mechanism is not just an experimental curiosity — it holds up under real model size and real token volume &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6m7q2vzlm6lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T05:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcc2fa</id>
    <title>An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding</title>
    <updated>2026-02-23T08:33:22+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt; &lt;img alt="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" src="https://preview.redd.it/wfpxmbhlc7lg1.png?width=140&amp;amp;height=99&amp;amp;auto=webp&amp;amp;s=48d6dec8f7b1fd1a11e8a1b5afbd51040b6a5021" title="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcc2fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T08:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3naj</id>
    <title>Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao.</title>
    <updated>2026-02-23T01:13:04+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt; &lt;img alt="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." src="https://external-preview.redd.it/MmJ6MGRjNjA4NWxnMR3Al36Nr886FX7jQ_P96fNg8PSf4Zsku92kjG2XN_qv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8910573e373960eea6962553218ddcd88a9324c" title="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, I was bored so I spent the last two weeks experimenting with vibecoding with local LLMs, namely gpt-oss-120b.&lt;/p&gt; &lt;p&gt;I started with Cline, didn't like it at all because it was overheating my GPU while giving back too little. Codex was even worse, locally, leading to weird CPU switches mid-generation when there was supposed to be enough VRAM to run the model entirely on GPU. Then I tried Claude Code and that's when my expectations were exceeded, &lt;em&gt;big time.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I first started with pygame, and after successfully one-shotting simple games (snake game, etc.) under the same project with the same model I decided to take it another level and use Claude Code with Godot, which was pretty easy to setup in VSCode and their IDE/extension. &lt;/p&gt; &lt;p&gt;Next thing I know, I spend the last two weeks making this game on Godot out of curiosity and using Claude Code to help me Vibecode parts of it along the way, and I came up with this game where you have a useful, snarky NPC that makes fun of you lmao.&lt;/p&gt; &lt;p&gt;The way it works is that the game is going to be gathering contextual information in real-time, e.g. actions taken, events occurring, etc. You can see that in the logs that are printed under the gameplay loop. &lt;/p&gt; &lt;p&gt;The mage then stores each chain of events in a chat history and comments on it every 10 seconds. The AI behavior is hard-coded but it works really well. However, I do plan on adding a hybrid approach where the LLM uses tool calls to make informed decisions depending on the situations, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switching equipment&lt;/li&gt; &lt;li&gt;Healing the player or himself&lt;/li&gt; &lt;li&gt;Pointing out objects of interest&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so forth. I haven't ruled out a Wizard of Oz worldbuilding AI that vibecodes enemies and obstacles throughout the game with tool calls, but that will be for another time.&lt;/p&gt; &lt;p&gt;I'm enjoying this process so I think I might actually finish this game, but we'll see how far I can get. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jl31wp5085lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T01:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc6c8m</id>
    <title>Feels like magic. A local gpt-oss 20B is capable of agentic work</title>
    <updated>2026-02-23T03:18:16+00:00</updated>
    <author>
      <name>/u/Vaddieg</name>
      <uri>https://old.reddit.com/user/Vaddieg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt; &lt;img alt="Feels like magic. A local gpt-oss 20B is capable of agentic work" src="https://preview.redd.it/b27xdhewq5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9692be692d82dd176bce38aa1cffe88af9406be" title="Feels like magic. A local gpt-oss 20B is capable of agentic work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gave a try to &lt;a href="https://github.com/zeroclaw-labs/zeroclaw"&gt;zeroclaw&lt;/a&gt; agent (intstead of the bloated and overhyped one). After few hours of fuckery with configs it's finally useful. Both main and embeddings models are running locally.&lt;br /&gt; I carefully read what it's trying to execute in shell, and permit only [relatively] safe tools in config.&lt;br /&gt; So far it can interact with macOS apps, web pages, and local files while keeping all my data private.&lt;br /&gt; gpt-oss 20B has its limits though, it loses focus after 15-20 steps and often needs direct instructions to use persistent memory. It also starts behaving weirdly if tool access has been denied or tool returned some error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vaddieg"&gt; /u/Vaddieg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b27xdhewq5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T03:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc59ze</id>
    <title>Qwen3's most underrated feature: Voice embeddings</title>
    <updated>2026-02-23T02:28:32+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt; &lt;img alt="Qwen3's most underrated feature: Voice embeddings" src="https://preview.redd.it/zmcs7iysm5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=796016e685c536fbab1ce49b5fec35afeb75f40e" title="Qwen3's most underrated feature: Voice embeddings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you know that Qwen3 TTS utilizes voice embedding for voice cloning?&lt;br /&gt; Your voice is turned into a vector of 1024 dimensions (or 2048 for 1.7b), and based on this vector alone you can get your custom voice.&lt;/p&gt; &lt;p&gt;But the coolest part is that this means that you can use math to modify voices, average voices. You can swap gender, pitch, mix and match voices, and even create an emotion space! This also enables semantic voice search!&lt;/p&gt; &lt;p&gt;The voice embedding model is actually just a tiny encoder with just a few million parameters. I've ripped it out of the voice embedding model so you can use the embedding model standalone. Check out my collection! :D I also have onnx models for optimized web / front-end inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding"&gt;https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice embedings can be used for inference in my vllm-omni fork until it is supported in upstream: &lt;a href="https://github.com/heiervang-technologies/ht-vllm-omni"&gt;https://github.com/heiervang-technologies/ht-vllm-omni&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmcs7iysm5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T02:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
