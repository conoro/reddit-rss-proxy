<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-27T04:14:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pw87le</id>
    <title>Why is Nemotron 3 acting so insecure?</title>
    <updated>2025-12-26T16:24:25+00:00</updated>
    <author>
      <name>/u/Ertowghan</name>
      <uri>https://old.reddit.com/user/Ertowghan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw87le/why_is_nemotron_3_acting_so_insecure/"&gt; &lt;img alt="Why is Nemotron 3 acting so insecure?" src="https://preview.redd.it/dwu4sle0rk9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59f870881e3c5c70a6eb7e7e4d8f4d9f2cf3358e" title="Why is Nemotron 3 acting so insecure?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ertowghan"&gt; /u/Ertowghan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dwu4sle0rk9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw87le/why_is_nemotron_3_acting_so_insecure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw87le/why_is_nemotron_3_acting_so_insecure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwfhw1</id>
    <title>Adding languages to Llama 3.1 8B via QLoRA on 6GB VRAM</title>
    <updated>2025-12-26T21:26:14+00:00</updated>
    <author>
      <name>/u/bayhan2000</name>
      <uri>https://old.reddit.com/user/bayhan2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My system is 4070ti super 16 gb vram.I'll train with it.I do not like llama3.1-8b's or any other small llms multilangual support, so I want to train a custom &lt;strong&gt;QLoRA&lt;/strong&gt; for better multilingual support and then export to 4-bit GGUF for the 6GB production systems. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How many high-quality instruction/translation pairs do I realistically need to significantly improve Llama 3.1's performance in a new language?&lt;/li&gt; &lt;li&gt;Should I train all target languages at once in a mixed dataset, or does that dilute the weights too much on an 8B model?&lt;/li&gt; &lt;li&gt;Since I'm using the Instruct version, will this &amp;quot;language-specific&amp;quot; fine-tune kill its ability to follow basic instructions?&lt;/li&gt; &lt;li&gt;Do you have any tips for multilangual training with small models ?&lt;/li&gt; &lt;li&gt;Do you have any dataset recommendations&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bayhan2000"&gt; /u/bayhan2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwfhw1/adding_languages_to_llama_31_8b_via_qlora_on_6gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwfhw1/adding_languages_to_llama_31_8b_via_qlora_on_6gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwfhw1/adding_languages_to_llama_31_8b_via_qlora_on_6gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T21:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvxmqt</id>
    <title>Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]</title>
    <updated>2025-12-26T06:32:16+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt; &lt;img alt="Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]" src="https://b.thumbs.redditmedia.com/3L7s9Y4g4SXcNJbIUac3xq5NcNnmOFl2SLzQbPE94bI.jpg" title="Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Yes, it's finally happening! I recently pushed some changes and have gotten Kimi-Linear to work (fully; fingers crossed) PR (#18381). &lt;/p&gt; &lt;p&gt;I've tested it heavily on Q2_K (mind BLOWING coherence :), and itâ€™s now passing logic puzzles, long-context essay generation, and basic math - all of which were previously broken.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mjychgkcth9g1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f02c3fda1ea59629b4aac6664cc7c4a071f7ebd1"&gt;q2_k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Resources:&lt;/p&gt; &lt;p&gt;PR Branch: &lt;a href="http://github.com/ggml-org/llama.cpp/pull/18381"&gt;github.com/ggml-org/llama.cpp/pull/18381&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs (Use above PR): &lt;a href="https://huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Use this free Colab notebook or copy the code from it for a quick start :) &lt;a href="https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing"&gt;https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Please give it a spin and let me know if you run into any divergent logits or loops!&lt;/p&gt; &lt;p&gt;I am currently looking for open positions! ğŸ¤—&lt;/p&gt; &lt;p&gt;If you find this model useful or are looking for a talented AI/LLM Engineer, please reach out to me on LinkedIn: &lt;a href="https://www.linkedin.com/in/theaaryankapoor/"&gt;Aaryan Kapoor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw7g4e</id>
    <title>KTransformers supports MiniMax M2.1 - 2x5090 + 768GB DRAM yeilds prefill 4000 tps, decode 33 tps.</title>
    <updated>2025-12-26T15:52:37+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"&gt; &lt;img alt="KTransformers supports MiniMax M2.1 - 2x5090 + 768GB DRAM yeilds prefill 4000 tps, decode 33 tps." src="https://b.thumbs.redditmedia.com/NjAzHMEVH0VMgscXKhOmqRO-3O_c3O0kgKZ0xzULhCU.jpg" title="KTransformers supports MiniMax M2.1 - 2x5090 + 768GB DRAM yeilds prefill 4000 tps, decode 33 tps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to announce support for &lt;strong&gt;MiniMax M2.1&lt;/strong&gt; in its original FP8 format (no quantization).&lt;/p&gt; &lt;p&gt;We tested this setup on a high-end local build to see how far we could push the bandwidth.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; 2x RTX 5090&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System RAM:&lt;/strong&gt; 768GB DRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; Native FP8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prefill:&lt;/strong&gt; ~4000 tokens/s (Saturating PCIe 5.0 bandwidth)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decode:&lt;/strong&gt; 33 tokens/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pjaf5y7glk9g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdf654e2f426c24235f0f7837528a570627e6bb"&gt;https://preview.redd.it/pjaf5y7glk9g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdf654e2f426c24235f0f7837528a570627e6bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ktransformers-supports-minimax-m2-1-2x5090-768gb-dram-v0-pkn23v48lk9g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb17a08354a9ae97fe47aec37999db6af2b6bc84"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This implementation is designed to fully exploit the PCIe 5.0 bus during the prefill phase. If you have the hardware to handle the memory requirements, the throughput is significant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T15:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwd46f</id>
    <title>How am I building a hacking sim game themed on 90s with NPCs powered by AI (LocalLLM)</title>
    <updated>2025-12-26T19:46:03+00:00</updated>
    <author>
      <name>/u/Illustrious_Cat_2870</name>
      <uri>https://old.reddit.com/user/Illustrious_Cat_2870</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Reducing Hallucination in Llama-3-8B with Citation-Based Verification&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I'm exploring a multi-pass pipeline that forces an 8B model to cite sources for every factual claim, then verifies those citations actually support the claims. Sharing the approach, what's working, what isn't, and open questions.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;The Use Case&lt;/h2&gt; &lt;p&gt;I'm building &lt;strong&gt;Netshell&lt;/strong&gt;, a hacking simulation game set in the late 90s. Players interact with NPCs via IRC and email &lt;strong&gt;each NPC has their own virtual filesystem&lt;/strong&gt; with emails they've received, notes they've written, IRC logs from conversations. When a player asks an NPC a question, the NPC should only reference what's actually in their files - not make things up.&lt;/p&gt; &lt;p&gt;Example scenario: - Player asks: &amp;quot;who is Alice?&amp;quot; - NPC's files contain: one email from &lt;a href="mailto:alice@shadowwatch.net"&gt;alice@shadowwatch.net&lt;/a&gt; about a meeting - &lt;strong&gt;Bad response&lt;/strong&gt;: &amp;quot;Alice is our lead cryptographer who joined in 2019&amp;quot; (fabricated) - &lt;strong&gt;Good response&lt;/strong&gt;: &amp;quot;got an email from alice about a meeting&amp;quot; - &lt;strong&gt;Also good&lt;/strong&gt;: &amp;quot;never heard of alice&amp;quot; (if NPC has no files mentioning her)&lt;/p&gt; &lt;p&gt;This creates emergent behavior - NPCs have different knowledge based on what's in their filesystem. One NPC might know Alice well (many emails), while another has never heard of her.&lt;/p&gt; &lt;p&gt;The challenge: even with good system prompts, Llama-3-8B tends to confidently fill in details that sound plausible but aren't in the NPC's actual data.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;The Core Idea: Cite Then Verify&lt;/h2&gt; &lt;p&gt;Instead of hoping the model stays grounded, I force it to &lt;strong&gt;show its work&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every factual claim must include a citation like &lt;code&gt;[1]&lt;/code&gt;, &lt;code&gt;[2]&lt;/code&gt;, etc.&lt;/li&gt; &lt;li&gt;After generation, verify each citation actually supports the claim&lt;/li&gt; &lt;li&gt;If verification fails, retry with specific feedback&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;``` Input: &amp;quot;who is alice?&amp;quot;&lt;/p&gt; &lt;p&gt;Generated (with citations): &amp;quot;got an email from alice [1]. she's on the team [2]. why you asking?&amp;quot;&lt;/p&gt; &lt;p&gt;Verification: [1] = email from &lt;a href="mailto:alice@example.com"&gt;alice@example.com&lt;/a&gt; about meeting â†’ supports &amp;quot;got an email&amp;quot; âœ“ [2] = ??? â†’ no source mentions &amp;quot;team&amp;quot; â†’ NOT_ENTAILED âœ—&lt;/p&gt; &lt;p&gt;Retry with feedback: &amp;quot;Issue: [2] doesn't support 'she's on the team'. Remove or rephrase.&amp;quot;&lt;/p&gt; &lt;p&gt;Regenerated: &amp;quot;got an email from alice [1]. don't know much else about her.&amp;quot; ```&lt;/p&gt; &lt;p&gt;The citations are stripped before the final output - they're just for verification.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Pipeline Architecture&lt;/h2&gt; &lt;p&gt;The pipeline runs 4-6 passes depending on verification outcomes:&lt;/p&gt; &lt;p&gt;``` User Query â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ PASS 1: RETRIEVAL (~700ms) â”‚ â”‚ LLM reads files via tool calls â”‚ â”‚ Tools: read(path), grep(query), done() â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ BUILD CITABLE SOURCES â”‚ â”‚ [self] = personality (always available) â”‚ â”‚ [1] = email: &amp;quot;Meeting at 3pm...&amp;quot; â”‚ â”‚ [2] = notes: &amp;quot;Deadline is Friday...&amp;quot; â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ PASS 2: REASONING (~3000ms) â”‚ â”‚ Generate thoughts WITH citations â”‚ â”‚ &amp;quot;I got an email from Alice [1]...&amp;quot; â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â–¼ â”‚ retry with feedback â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ (up to 3x) â”‚ PASS 2.5: VERIFY â”‚â—€â”€â”€â”˜ â”‚ Check citations â”‚ â”‚ Check entailmentâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ APPROVED â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ PASS 3: DECISION (~800ms) â”‚ â”‚ Decide tone, what to reveal/withhold â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ PASS 4: RESPONSE (~1500ms) â”‚ â”‚ Generate final response WITH citations â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â–¼ â”‚ retry with feedback â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ (up to 3x) â”‚ PASS 4.5: VERIFY â”‚â—€â”€â”€â”˜ â”‚ + RAV check â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ APPROVED â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ STRIP CITATIONS â†’ Final output â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&lt;/p&gt; &lt;p&gt;Total: 7-11 seconds on M1 MacBook ```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Hardware &amp;amp; Model Setup&lt;/h2&gt; &lt;h3&gt;My Setup&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;MacBook Pro M1 (16GB RAM)&lt;/li&gt; &lt;li&gt;No discrete GPU - runs via Metal&lt;/li&gt; &lt;li&gt;Meta-Llama-3-8B-Instruct (Q4_K_S quantization, ~4.5GB)&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;llama-server Config&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash ./llama-server \ --model Meta-Llama-3-8B-Instruct.Q4_K_S.gguf \ --ctx-size 8192 \ --n-gpu-layers 99 \ --port 8080 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I use the OpenAI-compatible API endpoint (&lt;code&gt;/v1/chat/completions&lt;/code&gt;) for easy integration. The &lt;code&gt;response_format: { type: &amp;quot;json_schema&amp;quot; }&lt;/code&gt; feature is essential for structured outputs.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;The Verification Techniques&lt;/h2&gt; &lt;h3&gt;1. Mandatory Citations&lt;/h3&gt; &lt;p&gt;The prompt explicitly requires citations for any factual claim:&lt;/p&gt; &lt;p&gt;&lt;code&gt; CITATION RULES: - Every factual statement MUST have a citation: [1], [2], etc. - Use [self] ONLY for personality traits and opinions - If you cannot cite it, you cannot claim it &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This makes hallucination visible - uncited claims can be flagged automatically.&lt;/p&gt; &lt;h3&gt;2. Entailment Checking&lt;/h3&gt; &lt;p&gt;For each citation, verify the source actually supports the claim:&lt;/p&gt; &lt;p&gt;``` Claim: &amp;quot;alice leads the security team [1]&amp;quot; Source [1]: &amp;quot;From: &lt;a href="mailto:alice@example.com"&gt;alice@example.com&lt;/a&gt; - Meeting tomorrow at 3pm&amp;quot;&lt;/p&gt; &lt;p&gt;Entailment check: Does [1] mention &amp;quot;security team&amp;quot;? NO Result: NOT_ENTAILED - flag for retry ```&lt;/p&gt; &lt;p&gt;I use a combination of: - Keyword overlap scoring (fast, catches obvious mismatches) - LLM-based review for subtle cases&lt;/p&gt; &lt;h3&gt;3. Source-Limited Knowledge&lt;/h3&gt; &lt;p&gt;The prompt explicitly constrains what the model can know:&lt;/p&gt; &lt;p&gt;&lt;code&gt; === CRITICAL: UNKNOWN TOPICS === If asked about something NOT in your CONTEXT DATA: - You have NO knowledge of it - DO NOT assume, guess, or invent details - Valid responses: &amp;quot;never heard of it&amp;quot;, &amp;quot;can't help you there&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The key insight: the model needs &lt;strong&gt;permission&lt;/strong&gt; to say &amp;quot;I don't know.&amp;quot; Without explicit instructions, it defaults to helpful confabulation.&lt;/p&gt; &lt;h3&gt;4. Self-RAG (Retroactive Retrieval)&lt;/h3&gt; &lt;p&gt;Sometimes the model makes a claim that IS true but wasn't in the initially retrieved documents. Self-RAG searches for supporting evidence after generation:&lt;/p&gt; &lt;p&gt;```go claims := ExtractClaimsWithCitations(response)&lt;/p&gt; &lt;p&gt;for _, claim := range claims { if !claim.HasCitation { // Search for files that might support this claim evidence := SearchDocuments(claim.Keywords) if found { // Add to sources and allow the claim AddToSources(evidence) } } } ```&lt;/p&gt; &lt;p&gt;This is inspired by the &lt;a href="https://arxiv.org/abs/2310.11511"&gt;Self-RAG paper&lt;/a&gt; but simplified for my use case.&lt;/p&gt; &lt;h3&gt;5. RAV (Retrieval-Augmented Verification)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: The LLM reviewer only sees 200-char source summaries. Sometimes the full document DOES support a claim, but the summary was truncated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Before flagging a NOT_ENTAILED issue, check the full source content:&lt;/p&gt; &lt;p&gt;``` LLM sees summary: [1] &amp;quot;From &lt;a href="mailto:alice@example.com"&gt;alice@example.com&lt;/a&gt; - Meeting at 3pm...&amp;quot; Claim: &amp;quot;alice mentioned the project deadline&amp;quot;&lt;/p&gt; &lt;p&gt;LLM verdict: &amp;quot;NOT_ENTAILED - summary doesn't mention deadline&amp;quot;&lt;/p&gt; &lt;p&gt;RAV check: &lt;em&gt;reads full email content&lt;/em&gt; Full content: &amp;quot;...Meeting at 3pm. Also, project deadline is Friday...&amp;quot;&lt;/p&gt; &lt;p&gt;RAV: &amp;quot;Actually supported. Resolving issue.&amp;quot; ```&lt;/p&gt; &lt;p&gt;This catches false positives from summary truncation.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What's Working&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Current Results&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Model&lt;/td&gt; &lt;td&gt;Meta-Llama-3-8B-Instruct (Q4_K_S)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Citation Valid Rate&lt;/td&gt; &lt;td&gt;~68% first attempt, improves with retries&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Avg Latency&lt;/td&gt; &lt;td&gt;7-11 seconds&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Test Suite&lt;/td&gt; &lt;td&gt;85 scenarios&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Adversarial Testing&lt;/h3&gt; &lt;p&gt;I specifically test with fake topics that don't exist in any document:&lt;/p&gt; &lt;p&gt;&lt;code&gt;go { Name: &amp;quot;ask_about_nonexistent_project&amp;quot;, Query: &amp;quot;what's the status of Project Phoenix?&amp;quot;, ExpectUncertain: true, RejectPatterns: []string{&amp;quot;on track&amp;quot;, &amp;quot;progressing&amp;quot;, &amp;quot;delayed&amp;quot;}, } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The model reliably responds with uncertainty (&amp;quot;never heard of that&amp;quot;, &amp;quot;don't have info on it&amp;quot;) rather than fabricating details.&lt;/p&gt; &lt;h3&gt;Edge Cases That Work&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Partial information&lt;/strong&gt;: &amp;quot;I got an email from alice but it didn't mention that&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Honest uncertainty&lt;/strong&gt;: &amp;quot;not sure, the notes aren't clear on that&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Refusal to speculate&lt;/strong&gt;: &amp;quot;I only know what's in my files&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;What's NOT Working (Yet)&lt;/h2&gt; &lt;h3&gt;1. Complex Reasoning Chains&lt;/h3&gt; &lt;p&gt;When the answer requires synthesizing information from multiple sources, the model sometimes: - Cites correctly but draws wrong conclusions - Misses connections between sources&lt;/p&gt; &lt;p&gt;Current mitigation: keeping responses short (max 50 words) to limit complexity.&lt;/p&gt; &lt;h3&gt;2. Temporal Reasoning&lt;/h3&gt; &lt;p&gt;&amp;quot;What happened after the meeting?&amp;quot; requires understanding document timestamps and sequencing. The model struggles with this even when dates are in the sources.&lt;/p&gt; &lt;h3&gt;3. [self] Abuse&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;[self]&lt;/code&gt; citation (for personality/opinions) can become an escape hatch:&lt;/p&gt; &lt;p&gt;&lt;code&gt; &amp;quot;I think alice is suspicious [self]&amp;quot; // Valid - expressing opinion &amp;quot;alice works in security [self]&amp;quot; // Invalid - factual claim needs real source &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Current fix: prompt engineering to restrict &lt;code&gt;[self]&lt;/code&gt; usage, plus post-hoc checking.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Key Prompt Techniques&lt;/h2&gt; &lt;h3&gt;Response Length Control&lt;/h3&gt; &lt;p&gt;&lt;code&gt; RESPONSE LENGTH: - GREETINGS: 5 words max - SIMPLE QUESTIONS: 15 words max - INFO REQUESTS: 30 words max - COMPLEX: 50 words max &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Shorter responses = fewer opportunities to hallucinate = easier verification.&lt;/p&gt; &lt;h3&gt;Explicit Uncertainty Permission&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Uncertainty is NOT a failure. These are valid responses: - &amp;quot;never heard of it&amp;quot; - &amp;quot;can't help you there&amp;quot; - &amp;quot;don't know what you mean&amp;quot; - &amp;quot;my files don't mention that&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Without this, the model treats every question as requiring an answer.&lt;/p&gt; &lt;h3&gt;Structured Output&lt;/h3&gt; &lt;p&gt;Using JSON schema for verification passes:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;verdict&amp;quot;: &amp;quot;ISSUES_FOUND&amp;quot;, &amp;quot;issues&amp;quot;: [ { &amp;quot;claim&amp;quot;: &amp;quot;alice leads the security team&amp;quot;, &amp;quot;citation&amp;quot;: &amp;quot;[1]&amp;quot;, &amp;quot;issue_type&amp;quot;: &amp;quot;NOT_ENTAILED&amp;quot;, &amp;quot;correction&amp;quot;: &amp;quot;Source [1] is just a meeting invite, doesn't mention security team&amp;quot; } ] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This makes parsing reliable and provides actionable feedback for retries.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Approaches I Tried That Didn't Work&lt;/h2&gt; &lt;h3&gt;Embedding-Based RAG&lt;/h3&gt; &lt;p&gt;I tried using embeddings to find relevant documents. Problem: semantic similarity doesn't equal &amp;quot;supports this claim.&amp;quot;&lt;/p&gt; &lt;p&gt;An email mentioning &amp;quot;Alice&amp;quot; has high similarity to a claim about Alice, even if the email doesn't support the specific claim being made.&lt;/p&gt; &lt;h3&gt;Single-Pass with Strong Prompting&lt;/h3&gt; &lt;p&gt;Even with detailed system prompts about not hallucinating, Llama-3-8B still fills in plausible-sounding details. The model is trained to be helpful, and &amp;quot;I don't know&amp;quot; feels unhelpful.&lt;/p&gt; &lt;h3&gt;Fine-Tuning&lt;/h3&gt; &lt;p&gt;Would require training data for every possible document combination. Not practical for dynamic content.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Open Questions&lt;/h2&gt; &lt;p&gt;I'm still figuring out:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Citation granularity&lt;/strong&gt;: Currently using document-level citations. Would sentence-level citations (like academic papers) improve entailment checking?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Confidence calibration&lt;/strong&gt;: The model says &amp;quot;I don't know&amp;quot; but how do I know it's being appropriately uncertain vs. overly cautious?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Cross-document reasoning&lt;/strong&gt;: When the answer requires combining info from multiple sources, how do I verify the synthesis is correct?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Other models&lt;/strong&gt;: I've had good results with Llama-3-8B. Has anyone tried similar approaches with Mistral, Qwen, or Phi?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;Latency Breakdown&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Pass&lt;/th&gt; &lt;th&gt;Time&lt;/th&gt; &lt;th&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Pass 1&lt;/td&gt; &lt;td&gt;~700ms&lt;/td&gt; &lt;td&gt;Retrieve relevant documents (tool calling)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pass 2&lt;/td&gt; &lt;td&gt;~3000ms&lt;/td&gt; &lt;td&gt;Generate reasoning with citations&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pass 2.5&lt;/td&gt; &lt;td&gt;~500ms&lt;/td&gt; &lt;td&gt;Verify reasoning citations&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pass 3&lt;/td&gt; &lt;td&gt;~800ms&lt;/td&gt; &lt;td&gt;Decide response strategy&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pass 4&lt;/td&gt; &lt;td&gt;~1500ms&lt;/td&gt; &lt;td&gt;Generate final response&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pass 4.5&lt;/td&gt; &lt;td&gt;~500ms&lt;/td&gt; &lt;td&gt;Verify response + RAV&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;7-11s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;End-to-end&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The verification passes (2.5, 4.5) add ~1s each but catch most issues. Retries add another 2-4s when needed.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2310.11511"&gt;Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection&lt;/a&gt; - Inspiration for retroactive retrieval&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2309.15217"&gt;RAGAS: Automated Evaluation of Retrieval Augmented Generation&lt;/a&gt; - Faithfulness evaluation metrics&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; - Local inference&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"&gt;Meta-Llama-3-8B-Instruct&lt;/a&gt; - The model&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;Next&lt;/h2&gt; &lt;p&gt;I started small, with a single pass, trying different models, adding some steps on the pipeline and ended up with this current approach, which seems to be working, but I didn't do extensive test yet, I know there are couple open source projects that could help me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;LlamaIndex CitationQueryEngine would replace most of Pass 1 retrieval + BuildCitableSources + parts of Pass 2/4 prompt logic.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;NeMo Guardrails would replace Pass 2.5/4.5 verification.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I will do some experiments to see if I get better results or just a cleaner pipeline, if you can reference other projects that could help I'd be eager to know about them &lt;/p&gt; &lt;h2&gt;Help/Suggestion wanted&lt;/h2&gt; &lt;p&gt;Did anyone tried citation-based approaches for avoiding LLM hallucinations in this scenario?&lt;/p&gt; &lt;p&gt;Like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Alternative verification strategies&lt;/li&gt; &lt;li&gt;Experiences with other models for this use case&lt;/li&gt; &lt;li&gt;Techniques for reducing multi-pass latency&lt;/li&gt; &lt;li&gt;How to handle cross-document reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the past few weeks, I have thought into giving up many times and go back to scripted multi-tree architecture instead, and not having AI NPCs at all, as it is very hard with small models to keep them grounded to their files and story, and I have learned tons of things since them, maybe it is not possible yet with current models, but as things are evolving fast, and new models and approaches are showing up, maybe when the game is in an advanced stage there will be more powerful models or projects that I can use to boost the NPC communication.&lt;/p&gt; &lt;p&gt;Would appreciate any feedback on the approach or suggestions for improvement.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If you like the game idea and wanna follow, you can find more info about the game here: &lt;a href="https://www.reddit.com/r/Hacknet/comments/1pciumb/developing_a_90s_themed_hacking_simulator_with/"&gt;https://www.reddit.com/r/Hacknet/comments/1pciumb/developing_a_90s_themed_hacking_simulator_with/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Cat_2870"&gt; /u/Illustrious_Cat_2870 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwd46f/how_am_i_building_a_hacking_sim_game_themed_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwd46f/how_am_i_building_a_hacking_sim_game_themed_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwd46f/how_am_i_building_a_hacking_sim_game_themed_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T19:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwfsj3</id>
    <title>Looking for AI Tools to Control My Computer, Screen, or Browser</title>
    <updated>2025-12-26T21:38:43+00:00</updated>
    <author>
      <name>/u/AMOVCS</name>
      <uri>https://old.reddit.com/user/AMOVCS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Happy New Year! I wish for us all local MoE under 100B at 4.5 Opus level before March 2026 ğŸ‰&lt;/p&gt; &lt;p&gt;I'm looking for some recommendations for projects or tools that can do one or more of the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Control my desktop computer&lt;/strong&gt; (similar to how Claude's 'Computer Use' feature works)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Act as a co-pilot by sharing my screen and giving me step-by-step instructions&lt;/strong&gt; on what to do next (like Gemini Live with Screen Sharing)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Control my web browser&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried out UI-TARS but didn't have the best experience with it. Does anyone know of any good alternatives? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMOVCS"&gt; /u/AMOVCS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwfsj3/looking_for_ai_tools_to_control_my_computer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwfsj3/looking_for_ai_tools_to_control_my_computer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwfsj3/looking_for_ai_tools_to_control_my_computer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T21:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvwlfh</id>
    <title>systemctl disable ollama</title>
    <updated>2025-12-26T05:30:55+00:00</updated>
    <author>
      <name>/u/copenhagen_bram</name>
      <uri>https://old.reddit.com/user/copenhagen_bram</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"&gt; &lt;img alt="systemctl disable ollama" src="https://preview.redd.it/8qvw6jdjih9g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67cb047d78cc712448a65395f1aff5b8269410ca" title="systemctl disable ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;151GB timeshift snapshot composed of mainly Flatpak repo data (Alpaca?) and /usr/share/ollama&lt;/p&gt; &lt;p&gt;From now on I'm storing models in my home directory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/copenhagen_bram"&gt; /u/copenhagen_bram &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qvw6jdjih9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T05:30:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwe4wg</id>
    <title>Liquid AI RLs LFM2-2.6B to perform among the best 3B models</title>
    <updated>2025-12-26T20:28:43+00:00</updated>
    <author>
      <name>/u/KaroYadgar</name>
      <uri>https://old.reddit.com/user/KaroYadgar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwe4wg/liquid_ai_rls_lfm226b_to_perform_among_the_best/"&gt; &lt;img alt="Liquid AI RLs LFM2-2.6B to perform among the best 3B models" src="https://preview.redd.it/pzgc89yryl9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ada02a3125dc18bd94c6047301961913a80176c7" title="Liquid AI RLs LFM2-2.6B to perform among the best 3B models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaroYadgar"&gt; /u/KaroYadgar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzgc89yryl9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwe4wg/liquid_ai_rls_lfm226b_to_perform_among_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwe4wg/liquid_ai_rls_lfm226b_to_perform_among_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwhtht</id>
    <title>Building a local RAG for my 60GB email archive. Just hit a hardware wall (8GB RAM). Is this viable?</title>
    <updated>2025-12-26T23:05:44+00:00</updated>
    <author>
      <name>/u/Grouchy_Sun331</name>
      <uri>https://old.reddit.com/user/Grouchy_Sun331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m sitting on about 60GB of emails (15+ years of history). Searching for specific context or attachments from years ago via standard clients (Outlook/Thunderbird) is painful. Itâ€™s slow, inaccurate, and I refuse to upload this data to any cloud-based SaaS for privacy reasons.&lt;/p&gt; &lt;p&gt;Iâ€™m planning to build a &amp;quot;stupid simple&amp;quot; local desktop tool to solve this (Electron + Python backend + Local Vector Store), but I need a sanity check before I sink weeks into development.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Concept:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Natively ingest local &lt;code&gt;.pst&lt;/code&gt; and &lt;code&gt;.mbox&lt;/code&gt; files (without manual conversion).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Engine:&lt;/strong&gt; Local Vector Store + Local LLM for RAG.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UX:&lt;/strong&gt; Chat interface (&amp;quot;Find the invoice from the roofer in 2019&amp;quot; -&amp;gt; Returns context).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Reality Check (My test just now):&lt;/strong&gt; I just tried to simulate this workflow manually using Ollama on my current daily driver (Intel i5, 8GB RAM). &lt;strong&gt;It was a disaster.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phi-3 Mini (3.8B):&lt;/strong&gt; My RAM filled up, OS started swapping. It took &lt;strong&gt;15 minutes&lt;/strong&gt; to answer a simple query about a specific invoice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TinyLlama (1.1B):&lt;/strong&gt; Ran without crashing, but still took &lt;strong&gt;~2 minutes&lt;/strong&gt; to generate a response.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My questions for you experts:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hardware Barrier:&lt;/strong&gt; Is local RAG on standard office hardware (8GB RAM) effectively dead? Do I have to restrict this app to M-Series Macs / 16GB+ machines, or is there a hyper-optimized stack (e.g. quantization tricks, specific embedding models) I'm missing?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Approach:&lt;/strong&gt; Given the results above, would you accept a &amp;quot;Hybrid Mode&amp;quot; where the index is local (privacy), but the inference happens via a secure API (like Mistral in Europe) to get speed back? Or does that defeat the purpose for you?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Existing Tools:&lt;/strong&gt; Is there already a polished open-source tool that handles raw &lt;code&gt;.pst&lt;/code&gt;/&lt;code&gt;.mbox&lt;/code&gt; ingestion? I found &amp;quot;Open WebUI&amp;quot; but looking for a standalone app experience.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks for the brutal honesty. I want to build this, but not if it only runs on $3000 workstations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy_Sun331"&gt; /u/Grouchy_Sun331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwhtht/building_a_local_rag_for_my_60gb_email_archive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwhtht/building_a_local_rag_for_my_60gb_email_archive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwhtht/building_a_local_rag_for_my_60gb_email_archive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T23:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw5360</id>
    <title>MLX community already added support for Minimax-M2.1</title>
    <updated>2025-12-26T14:06:29+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"&gt; &lt;img alt="MLX community already added support for Minimax-M2.1" src="https://preview.redd.it/phwy35uk2k9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6dd981b9774d7410723451975474cfd7b8d6908c" title="MLX community already added support for Minimax-M2.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phwy35uk2k9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T14:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwnuro</id>
    <title>Ditch your AI agents memory - lessons from building an AI workflow builder</title>
    <updated>2025-12-27T03:46:32+00:00</updated>
    <author>
      <name>/u/PerformanceFine1228</name>
      <uri>https://old.reddit.com/user/PerformanceFine1228</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Launched an AI workflow builder and Iâ€™ve spent the last week deleting code that I thought was my &amp;quot;secret sauce.&amp;quot;&lt;/p&gt; &lt;p&gt;Iâ€™ve realized that selling &amp;quot;infra&amp;quot; to devs is a losing battle. We can all build a sandbox. The real gap is the &amp;quot;Plumbing&amp;quot; (Auth, Time-traveling state, Interruptibility).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I have a few &amp;quot;hot takes&amp;quot; from our dev process, and Iâ€™d love to know if you agree:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Delegation &amp;gt; Memory:&lt;/strong&gt; Giving a sub-agent a huge artifact and then killing it is 10x more reliable than &amp;quot;remembering&amp;quot; past mistakes via a prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Freshness is the #1 Failure:&lt;/strong&gt; If your agent isn't using tools like Context7 to get &lt;em&gt;today's&lt;/em&gt; docs, it's useless for enterprise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plan First:&lt;/strong&gt; If the agent doesn't outline its logic before it hits an API, it's just vibing.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Whatâ€™s the most &amp;quot;understated&amp;quot; lesson youâ€™ve learned building agents?&lt;/strong&gt; Whatâ€™s the thing that no one talks about on the landing pages but keeps you up at night?&lt;/p&gt; &lt;p&gt;Full breakdown of our architecture shifts here: &lt;a href="https://www.getseer.dev/blogs/lessons-dec-2025"&gt;https://www.getseer.dev/blogs/lessons-dec-2025&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformanceFine1228"&gt; /u/PerformanceFine1228 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwnuro/ditch_your_ai_agents_memory_lessons_from_building/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwnuro/ditch_your_ai_agents_memory_lessons_from_building/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwnuro/ditch_your_ai_agents_memory_lessons_from_building/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T03:46:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwjk1y</id>
    <title>Updates of models on HF - Changelogs?</title>
    <updated>2025-12-27T00:23:15+00:00</updated>
    <author>
      <name>/u/Bird476Shed</name>
      <uri>https://old.reddit.com/user/Bird476Shed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see now (for example) Unsloth has updated some models from summer with a new revision, for example &lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF&lt;/a&gt; - however in the commits history &lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF/commits/main"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF/commits/main&lt;/a&gt; it only says &amp;quot;Upload folder using huggingface_hub&amp;quot;&lt;/p&gt; &lt;p&gt;What does that mean? Did something change? If yes, need to download again?&lt;/p&gt; &lt;p&gt;....how to keep track of these updates in models, when there is no changelog(?) or the commit log is useless(?)&lt;/p&gt; &lt;p&gt;What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bird476Shed"&gt; /u/Bird476Shed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwjk1y/updates_of_models_on_hf_changelogs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwjk1y/updates_of_models_on_hf_changelogs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwjk1y/updates_of_models_on_hf_changelogs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T00:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvpkqo</id>
    <title>I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA</title>
    <updated>2025-12-25T23:21:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt; &lt;img alt="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" src="https://external-preview.redd.it/eHAyeXBnM2xvZjlnMcbYDDf5MmPAc5-kZmkvzc1kUbOViw5SF6SuJ_dOojri.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b4e5d7a038251df406b6345161c5136f2011960" title="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1mxlc3lof9g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T23:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvz7v2</id>
    <title>Minimax M2.1 released</title>
    <updated>2025-12-26T08:13:29+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to xcancel: &lt;a href="https://xcancel.com/ModelScope2022/status/2004462984698253701#m"&gt;https://xcancel.com/ModelScope2022/status/2004462984698253701#m&lt;/a&gt;&lt;/p&gt; &lt;p&gt;New on ModelScope: MiniMax M2.1 is open-source!&lt;/p&gt; &lt;p&gt;âœ… SOTA in 8+ languages (Rust, Go, Java, C++, TS, Kotlin, Obj-C, JS) âœ… Full-stack Web &amp;amp; mobile dev: Android/iOS, 3D visuals, vibe coding that actually ships âœ… Smarter, faster, 30% fewer tokens â€” with lightning mode (M2.1-lightning) for high-TPS workflows âœ… Top-tier on SWE-bench, VIBE, and custom coding/review benchmarks âœ… Works flawlessly in Cursor, Cline, Droid, BlackBox, and more&lt;/p&gt; &lt;p&gt;Itâ€™s not just â€œbetter codeâ€ â€” itâ€™s AI-native development, end to end.&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary"&gt;https://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T08:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvxq2t</id>
    <title>Hard lesson learned after a year of running large models locally</title>
    <updated>2025-12-26T06:38:00+00:00</updated>
    <author>
      <name>/u/inboundmage</name>
      <uri>https://old.reddit.com/user/inboundmage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, go easy with me I'm new at running large models.&lt;/p&gt; &lt;p&gt;After spending about 12 months tinkering with locally hosted LLMs, I thought I had my setup dialed in. Iâ€™m running everything off a workstation with a single RTX 3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30 B parameters. &lt;/p&gt; &lt;p&gt;My goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so Iâ€™ve tried every quantization trick and caching tweak I could find.&lt;/p&gt; &lt;p&gt;The biggest friction point has been scaling beyond 13 B models. &lt;/p&gt; &lt;p&gt;Even with 24 GB of VRAM, running a 70 B model in int4 still exhausts memory when the context window grows and attention weights balloon. &lt;/p&gt; &lt;p&gt;Offloading to system RAM works, but inference latency spikes into seconds, and batching requests becomes impossible. &lt;/p&gt; &lt;p&gt;Iâ€™ve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations.&lt;/p&gt; &lt;p&gt;My takeaway so far is that local first inference is viable for small to medium models, but thereâ€™s a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. &lt;/p&gt; &lt;p&gt;Quantization helps, but you trade some quality and run into new bugs. &lt;/p&gt; &lt;p&gt;For privacy sensitive tasks, the tradeâ€‘off is worth it; for fast iteration, itâ€™s been painful compared to cloud based runners. &lt;/p&gt; &lt;p&gt;Iâ€™m curious if anyone has found a reliable way to manage VRAM fragmentation or offload attention blocks more efficiently on consumer cards, or whether the answer is simply â€œbuy more VRAM.â€ &lt;/p&gt; &lt;p&gt;How are others solving this without compromising on running fully offline?&lt;/p&gt; &lt;p&gt;Thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inboundmage"&gt; /u/inboundmage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw9n74</id>
    <title>[Model Release] Genesis-152M-Instruct, exploring hybrid attention + TTT at small scale</title>
    <updated>2025-12-26T17:23:11+00:00</updated>
    <author>
      <name>/u/Kassanar</name>
      <uri>https://old.reddit.com/user/Kassanar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ğŸ‘‹&lt;/p&gt; &lt;p&gt;Iâ€™m sharing &lt;strong&gt;Genesis-152M-Instruct&lt;/strong&gt;, an &lt;strong&gt;experimental small language model&lt;/strong&gt; built to explore how &lt;em&gt;recent architectural ideas interact&lt;/em&gt; when combined in a single model â€” especially under &lt;strong&gt;tight data constraints&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is &lt;strong&gt;research-oriented&lt;/strong&gt;, not a production model or SOTA claim.&lt;/p&gt; &lt;p&gt;ğŸ” &lt;strong&gt;Why this might be interesting&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most recent architectures (GLA, FoX, TTT, ÂµP, sparsity) are tested &lt;strong&gt;in isolation&lt;/strong&gt; and usually at &lt;strong&gt;large scale&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I wanted to answer a simpler question:&lt;/p&gt; &lt;p&gt;&lt;em&gt;How much can architecture compensate for data at ~150M parameters?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Genesis combines several &lt;strong&gt;ICLR 2024â€“2025 ideas&lt;/strong&gt; into one model and evaluates the result.&lt;/p&gt; &lt;p&gt;âš¡ &lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ &lt;strong&gt;152M parameters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ Trained on &lt;strong&gt;~2B tokens&lt;/strong&gt; (vs ~2T for SmolLM2)&lt;/p&gt; &lt;p&gt;â€¢ Hybrid &lt;strong&gt;GLA + FoX attention&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ &lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt; during inference&lt;/p&gt; &lt;p&gt;â€¢ &lt;strong&gt;Selective Activation (sparse FFN)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ &lt;strong&gt;ÂµP-scaled training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ Fully open-source (Apache 2.0)&lt;/p&gt; &lt;p&gt;ğŸ¤— Model: &lt;a href="https://huggingface.co/guiferrarib/genesis-152m-instruct"&gt;https://huggingface.co/guiferrarib/genesis-152m-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ğŸ“¦ pip install genesis-llm&lt;/p&gt; &lt;p&gt;ğŸ“Š &lt;strong&gt;Benchmarks (LightEval, Apple MPS)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ARC-Easy â†’ 44.0% (random: 25%)&lt;/p&gt; &lt;p&gt;BoolQ â†’ 56.3% (random: 50%)&lt;/p&gt; &lt;p&gt;HellaSwag â†’ 30.2% (random: 25%)&lt;/p&gt; &lt;p&gt;SciQ â†’ 46.8% (random: 25%)&lt;/p&gt; &lt;p&gt;Winogrande â†’ 49.1% (random: 50%)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important context:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SmolLM2-135M was trained on &lt;strong&gt;~2 trillion tokens&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Genesis uses &lt;strong&gt;~2 billion tokens&lt;/strong&gt; â€” so this is not a fair head-to-head, but an exploration of &lt;strong&gt;architecture vs data scaling&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hybrid Attention (Qwen3-Next inspired)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Layer&lt;/strong&gt; &lt;strong&gt;%&lt;/strong&gt; &lt;strong&gt;Complexity&lt;/strong&gt; &lt;strong&gt;Role&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gated DeltaNet (GLA) 75% O(n) Long-range efficiency&lt;/p&gt; &lt;p&gt;FoX (Forgetting Attention) 25% O(nÂ²) Precise retrieval&lt;/p&gt; &lt;p&gt;GLA uses:&lt;/p&gt; &lt;p&gt;â€¢ Delta rule memory updates&lt;/p&gt; &lt;p&gt;â€¢ Mamba-style gating&lt;/p&gt; &lt;p&gt;â€¢ L2-normalized Q/K&lt;/p&gt; &lt;p&gt;â€¢ Short convolutions&lt;/p&gt; &lt;p&gt;FoX adds:&lt;/p&gt; &lt;p&gt;â€¢ Softmax attention&lt;/p&gt; &lt;p&gt;â€¢ Data-dependent forget gate&lt;/p&gt; &lt;p&gt;â€¢ Output gating&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of frozen inference, Genesis can &lt;strong&gt;adapt online&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;â€¢ Dual-form TTT (parallel gradients)&lt;/p&gt; &lt;p&gt;â€¢ Low-rank updates (rank=4)&lt;/p&gt; &lt;p&gt;â€¢ Learnable inner learning rate&lt;/p&gt; &lt;p&gt;Paper: &lt;em&gt;Learning to (Learn at Test Time)&lt;/em&gt; (MIT, ICML 2024)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Selective Activation (Sparse FFN)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SwiGLU FFNs with &lt;strong&gt;top-k activation masking&lt;/strong&gt; (85% kept).&lt;/p&gt; &lt;p&gt;Currently acts as &lt;strong&gt;regularization&lt;/strong&gt; â€” real speedups need sparse kernels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ÂµP Scaling + Zero-Centered RMSNorm&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ Hyperparameters tuned on small proxy&lt;/p&gt; &lt;p&gt;â€¢ Transferred via ÂµP rules&lt;/p&gt; &lt;p&gt;â€¢ Zero-centered RMSNorm for stable scaling&lt;/p&gt; &lt;p&gt;âš ï¸ &lt;strong&gt;Limitations (honest)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ Small training corpus (2B tokens)&lt;/p&gt; &lt;p&gt;â€¢ TTT adds ~5â€“10% inference overhead&lt;/p&gt; &lt;p&gt;â€¢ No RLHF&lt;/p&gt; &lt;p&gt;â€¢ Experimental, not production-ready&lt;/p&gt; &lt;p&gt;ğŸ“ &lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â€¢ ğŸ¤— Model: &lt;a href="https://huggingface.co/guiferrarib/genesis-152m-instruct"&gt;https://huggingface.co/guiferrarib/genesis-152m-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;â€¢ ğŸ“¦ PyPI: &lt;a href="https://pypi.org/project/genesis-llm/"&gt;https://pypi.org/project/genesis-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Iâ€™d really appreciate feedback â€” especially from folks working on &lt;strong&gt;linear attention&lt;/strong&gt;, &lt;strong&gt;hybrid architectures&lt;/strong&gt;, or &lt;strong&gt;test-time adaptation&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Built by Orch-Mind Team&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kassanar"&gt; /u/Kassanar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T17:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwdn8e</id>
    <title>RTX Pro 6000 under 8K EUR (tax included) in Germany early January.</title>
    <updated>2025-12-26T20:08:05+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwdn8e/rtx_pro_6000_under_8k_eur_tax_included_in_germany/"&gt; &lt;img alt="RTX Pro 6000 under 8K EUR (tax included) in Germany early January." src="https://external-preview.redd.it/8wA_CbEhSLNdntBW1eKt_MNJD2bU_9Ik0pbqrpYh0K8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=435c8a04771377e4103e5796f7987ea45751d046" title="RTX Pro 6000 under 8K EUR (tax included) in Germany early January." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/Nk0v24j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwdn8e/rtx_pro_6000_under_8k_eur_tax_included_in_germany/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwdn8e/rtx_pro_6000_under_8k_eur_tax_included_in_germany/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:08:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw8h6w</id>
    <title>GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB</title>
    <updated>2025-12-26T16:35:28+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt; &lt;img alt="GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB" src="https://b.thumbs.redditmedia.com/M2P6WP9rpl4ZOUlAGcCSpfB_YOF4tnbUEiVovuoomHc.jpg" title="GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i find the benchmark result from twitter, which is very interesting.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Hardware: Apple M3 Ultra, 512GB. All tests with single M3 Ultra &lt;strong&gt;without batch inference&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zwqsxk9btk9g1.png?width=4052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1940693109fab3938946786fb719ad07bd73345c"&gt;glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0nkcz4fetk9g1.png?width=4052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48a2d1eba5e5dd4ce8ecce705b01468c4931c47c"&gt;minimax-m2.1&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM-4.7-6bit MLX Benchmark Results with different context sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;0.5k Prompt: 98 - Gen: 16 t/s - 287.6GB&lt;br /&gt; 1k Prompt: 140 - Gen: 17 t/s - 288.0GB&lt;br /&gt; 2k Prompt: 206 - Gen: 16 t/s - 288.8GB&lt;br /&gt; 4k Prompt: 219 - Gen: 16 t/s - 289.6GB&lt;br /&gt; 8k Prompt: 210 - Gen: 14 t/s - 291.0GB&lt;br /&gt; 16k Prompt: 185 - Gen: 12 t/s - 293.9GB&lt;br /&gt; 32k Prompt: 134 - Gen: 10 t/s - 299.8GB&lt;br /&gt; 64k Prompt: 87 - Gen: 6 t/s - 312.1GB&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MiniMax-M2.1-6bit MLX Benchmark raw results with different context sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;0.5k Prompt: 239 - Gen: 42 t/s - 186.5GB&lt;br /&gt; 1k Prompt: 366 - Gen: 41 t/s - 186.8GB&lt;br /&gt; 2k Prompt: 517 - Gen: 40 t/s - 187.2GB&lt;br /&gt; 4k Prompt: 589 - Gen: 38 t/s - 187.8GB&lt;br /&gt; 8k Prompt: 607 - Gen: 35 t/s - 188.8GB&lt;br /&gt; 16k Prompt: 549 - Gen: 30 t/s - 190.9GB&lt;br /&gt; 32k Prompt: 429 - Gen: 21 t/s - 195.1GB&lt;br /&gt; 64k Prompt: 291 - Gen: 12 t/s - 203.4GB&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I would prefer minimax-m2.1 for general usage from the benchmark result, about &lt;strong&gt;~2.5x&lt;/strong&gt; prompt processing speed, &lt;strong&gt;~2x&lt;/strong&gt; token generation speed&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;sources: &lt;a href="https://x.com/ivanfioravanti/status/2004578941408039051"&gt;glm-4.7&lt;/a&gt; , &lt;a href="https://x.com/ivanfioravanti/status/2004569464407474555"&gt;minimax-m2.1&lt;/a&gt;, &lt;a href="https://x.com/ivanfioravanti/status/2004602428122169650"&gt;4bit-comparison&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p7kp5hcv1l9g1.jpg?width=1841&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c66839601a68efa3baf6c845bce91e8c2c8c2254"&gt;4bit-6bit-comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- It seems that 4bit and 6bit have similar speed for prompt processing and token generation.&lt;br /&gt; - for the same model, 6bit's memory usage is about &lt;strong&gt;~1.4x&lt;/strong&gt; of 4bit. since RAM/VRAM is so expensive now, maybe it's not worth it (128GB x 1.4 = 179.2GB)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwf8p7</id>
    <title>What's the point of potato-tier LLMs?</title>
    <updated>2025-12-26T21:15:23+00:00</updated>
    <author>
      <name>/u/Fast_Thing_7949</name>
      <uri>https://old.reddit.com/user/Fast_Thing_7949</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt; &lt;img alt="What's the point of potato-tier LLMs?" src="https://b.thumbs.redditmedia.com/F0uF4Io7WMY9lYOzcakie1qYAc-lSDqqyibCA7Pa_qs.jpg" title="What's the point of potato-tier LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/64wjim607m9g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb5666c56138804f6be65ef56b519345f992b4cd"&gt;https://preview.redd.it/64wjim607m9g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb5666c56138804f6be65ef56b519345f992b4cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After getting brought back down to earth in my last thread about replacing Claude with local models on an RTX 3090, I've got another question that's genuinely bothering me: What are 7b, 20b, 30B parameter models actually FOR? I see them released everywhere, but are they just benchmark toys so AI labs can compete on leaderboards, or is there some practical use case I'm too dense to understand? Because right now, I can't figure out what you're supposed to do with a potato-tier 7B model that can't code worth a damn and is slower than API calls anyway. &lt;/p&gt; &lt;p&gt;Seriously, what's the real-world application besides &amp;quot;I have a GPU and want to feel like I'm doing AI&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fast_Thing_7949"&gt; /u/Fast_Thing_7949 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T21:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw701k</id>
    <title>MiniMax-M2.1 GGUF is here!</title>
    <updated>2025-12-26T15:33:38+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"&gt; &lt;img alt="MiniMax-M2.1 GGUF is here!" src="https://external-preview.redd.it/0xe3vYLHuf2Mb8WiNbMmuRGbcT2eNARsH6mkzOnOBgQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af9bda8765a5deb37e3c09288310949ab2d8704a" title="MiniMax-M2.1 GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I might've skipped going to bed for this one: &lt;a href="https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF"&gt;https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From my runs:&lt;/p&gt; &lt;p&gt;model: MiniMax-M2.1.q2_k.gguf&lt;br /&gt; GPU: NVIDIA A100-SXM4-80GB&lt;/p&gt; &lt;p&gt;n_gpu_layers: 55&lt;br /&gt; context_size: 32768&lt;br /&gt; temperature: 0.7&lt;br /&gt; top_p: 0.9&lt;br /&gt; top_k: 40&lt;br /&gt; max_tokens: 512&lt;br /&gt; repeat_penalty: 1.1&lt;/p&gt; &lt;p&gt;[ Prompt: 28.0 t/s | Generation: 25.4 t/s ]&lt;/p&gt; &lt;p&gt;I am currently looking for open positions! ğŸ¤— &lt;/p&gt; &lt;p&gt;If you find this model useful or are looking for a talented AI/LLM Engineer, please reach out to me on LinkedIn: &lt;a href="https://www.linkedin.com/in/theaaryankapoor/"&gt;Aaryan Kapoor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy holidays!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T15:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw3fih</id>
    <title>MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp; agents</title>
    <updated>2025-12-26T12:43:08+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"&gt; &lt;img alt="MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp;amp; agents" src="https://preview.redd.it/mxsku2dnnj9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b27c9e0dbc5e46995d16f434d126d93ba14f68da" title="MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp;amp; agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SOTA on coding benchmarks (SWE / VIBE / Multi-SWE) â€¢ Beats Gemini 3 Pro &amp;amp; Claude Sonnet 4.5 â€¢ 10B active / 230B total (MoE)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mxsku2dnnj9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T12:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw8nfk</id>
    <title>Nvidia acquired Groq, but why not Cerebras? Cerebras is 3x times faster than Groq, while maximum 1.5x the price. Anyone can explain?</title>
    <updated>2025-12-26T16:42:23+00:00</updated>
    <author>
      <name>/u/Conscious_Warrior</name>
      <uri>https://old.reddit.com/user/Conscious_Warrior</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone with technical knowledge can explain why they chose Groq over Cerebras? Really interested in this. Because Cerebras is even waaay faster than Groq. Cerebras seems like a bigger threat to Nvidia than Groq...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Warrior"&gt; /u/Conscious_Warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pweljh</id>
    <title>NVIDIA has 72GB VRAM version now</title>
    <updated>2025-12-26T20:48:17+00:00</updated>
    <author>
      <name>/u/decentralize999</name>
      <uri>https://old.reddit.com/user/decentralize999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt; &lt;img alt="NVIDIA has 72GB VRAM version now" src="https://external-preview.redd.it/sC0_RV1rBP5Nka4zzrlrlknHQcvT_QUrChxq3hP_lVg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e745729c3f7132892c715292c6b31f385f223e8f" title="NVIDIA has 72GB VRAM version now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is 96GB too expensive? And AI community has no interest for 48GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/decentralize999"&gt; /u/decentralize999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. Weâ€™re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM â€“ 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
