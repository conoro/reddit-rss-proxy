<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-17T22:06:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qfpdml</id>
    <title>Built a small local-first playground to learn agentic AI (no cloud, no APIs)</title>
    <updated>2026-01-17T21:27:29+00:00</updated>
    <author>
      <name>/u/AiVetted</name>
      <uri>https://old.reddit.com/user/AiVetted</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built this mainly for myself while trying to understand agentic AI without jumping straight into large frameworks.&lt;/p&gt; &lt;p&gt;Sutra is a small, local-first playground that runs entirely on your laptop using local models (Ollama). No cloud APIs, no costs, and very minimal abstractions.&lt;/p&gt; &lt;p&gt;It is not production-ready and not trying to compete with LangChain or AutoGen. The goal is just to understand agent behavior, sequencing, and simple pipelines by reading and running small pieces of code.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href="https://github.com/SutraLabs/sutra"&gt;https://github.com/SutraLabs/sutra&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would appreciate feedback from people who also prefer learning locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AiVetted"&gt; /u/AiVetted &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpdml/built_a_small_localfirst_playground_to_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpdml/built_a_small_localfirst_playground_to_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpdml/built_a_small_localfirst_playground_to_learn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T21:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfcap8</id>
    <title>I implemented a GPT-style model from scratch using PyTorch while reading Sebastian Raschka's book</title>
    <updated>2026-01-17T12:44:06+00:00</updated>
    <author>
      <name>/u/Bthreethree</name>
      <uri>https://old.reddit.com/user/Bthreethree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the last few weeks building a GPT-style LLM entirely from scratch in PyTorch to understand the architecture. This isn't just a wrapper; it's a full implementation covering the entire lifecycle from tokenization to instruction fine-tuning.&lt;/p&gt; &lt;p&gt;I have followed Sebastian Raschka's 'Build a LLM from Scratch' book for the implementation, here is the breakdown of the repo:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Data &amp;amp; Tokenization (&lt;/strong&gt;&lt;code&gt;src/data.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; Instead of using pre-built tokenizers, I implemented:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;SimpleTokenizerV2&lt;/code&gt;: Handles regex-based splitting and special tokens (&lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;|unk|&amp;gt;&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;code&gt;GPTDatasetV1&lt;/code&gt;: A sliding-window dataset implementation for efficient autoregressive training.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. The Attention Mechanism (&lt;/strong&gt;&lt;code&gt;src/attention.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I manually implemented &lt;code&gt;MultiHeadAttention&lt;/code&gt; to understand the tensor math:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles the query/key/value projections and splitting heads.&lt;/li&gt; &lt;li&gt;Implements the &lt;strong&gt;Causal Mask&lt;/strong&gt; (using &lt;code&gt;register_buffer&lt;/code&gt;) to prevent the model from &amp;quot;cheating&amp;quot; by seeing future tokens.&lt;/li&gt; &lt;li&gt;Includes &lt;code&gt;SpatialDropout&lt;/code&gt; and scaled dot-product attention.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. The GPT Architecture (&lt;/strong&gt;&lt;code&gt;src/model.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; A complete 124M parameter model assembly:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Combines &lt;code&gt;TransformerBlock&lt;/code&gt;, &lt;code&gt;LayerNorm&lt;/code&gt;, and &lt;code&gt;GELU&lt;/code&gt; activations.&lt;/li&gt; &lt;li&gt;Features positional embeddings and residual connections exactly matching the GPT-2 spec.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Training &amp;amp; Generation (&lt;/strong&gt;&lt;code&gt;src/train.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Custom training loop with loss visualization.&lt;/li&gt; &lt;li&gt;Implements &lt;code&gt;generate()&lt;/code&gt; with &lt;strong&gt;Top-K sampling&lt;/strong&gt; and &lt;strong&gt;Temperature scaling&lt;/strong&gt; to control output creativity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. Fine-tuning:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Classification (&lt;/strong&gt;&lt;code&gt;src/finetune_classification.py&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Adapted the backbone to detect Spam/Ham messages (90%+ accuracy on the test set).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction Tuning (&lt;/strong&gt;&lt;code&gt;src/finetune_instructions.py&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Implemented an Alpaca-style training loop. The model can now handle instruction-response pairs rather than just completing text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Nikshaan/llm-from-scratch"&gt;https://github.com/Nikshaan/llm-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve tried to comment every shape transformation in the code. If you are learning this stuff too, I hope this reference helps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bthreethree"&gt; /u/Bthreethree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:44:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfpfoy</id>
    <title>Orchestra - Multi-model AI orchestration system with intelligent routing (100% local, 18+ expert models)</title>
    <updated>2026-01-17T21:29:49+00:00</updated>
    <author>
      <name>/u/ericvarney</name>
      <uri>https://old.reddit.com/user/ericvarney</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I've been working on a local AI orchestration system and wanted to share it with this community. &lt;/p&gt; &lt;p&gt;## What is Orchestra? Orchestra automatically routes your queries to the most relevant expert models from a pool of 18+ specialized LLMs running locally via Ollama. Think of it as having a team of AI experts that collaborate on your questions. &lt;/p&gt; &lt;p&gt;## Key Features - &lt;strong&gt;**Smart Context Management*\&lt;/strong&gt;&lt;em&gt;: Handles 100+ message conversations without context overflow (uses only 44% of 32K context) - *&lt;/em&gt;**Multi-Expert Routing**&lt;strong&gt;: Automatically selects 3 best experts per query and synthesizes their responses - *&lt;em&gt;\&lt;/em&gt;*Physics Validation*\&lt;/strong&gt;&lt;em&gt;: Catches common AI reasoning failures (like invoking cosmology for mechanics problems) - *&lt;/em&gt;**Banking-Grade Browser**&lt;strong&gt;: Integrated browser with certificate validation and phishing detection - *&lt;em&gt;\&lt;/em&gt;*100% Local &amp;amp; Private*\&lt;/strong&gt;*: Everything runs on your machine via Ollama &lt;/p&gt; &lt;p&gt;## Why I Built This I got tired of LLMs giving me physics answers that invoked &amp;quot;dark energy&amp;quot; for simple mechanics problems. So I built a validation layer that penalizes bad reasoning patterns and routes to better models. &lt;/p&gt; &lt;p&gt;## Example When I ask: &amp;quot;If a spring is 1.5 light-years long...&amp;quot; - Routes to: Reasoning_Expert, Math_Expert (not Data_Scientist) - Validates: Checks for cosmology terms in mechanics answers - Filters: Penalizes responses claiming &amp;quot;instantaneous&amp;quot; for light-year scales - Result: 75-85% accuracy (vs 10% with naive routing) &lt;/p&gt; &lt;p&gt;## Tech Stack - Python backend (Flask API) - React + Electron frontend - Ollama for model hosting - RAG for long-term memory &lt;/p&gt; &lt;p&gt;## GitHub &lt;a href="https://github.com/ericvarney87-collab/Orchestra-Multi-Model-AI-System"&gt;https://github.com/ericvarney87-collab/Orchestra-Multi-Model-AI-System&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love feedback from this community! What features would you find most useful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ericvarney"&gt; /u/ericvarney &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpfoy/orchestra_multimodel_ai_orchestration_system_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpfoy/orchestra_multimodel_ai_orchestration_system_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpfoy/orchestra_multimodel_ai_orchestration_system_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T21:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfjvrz</id>
    <title>Created this overview of agent orchestration tools, frameworks and benchmarks, quickly showing you the best use cases and OSS status. Contributions welcome!</title>
    <updated>2026-01-17T17:53:21+00:00</updated>
    <author>
      <name>/u/Oatilis</name>
      <uri>https://old.reddit.com/user/Oatilis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfjvrz/created_this_overview_of_agent_orchestration/"&gt; &lt;img alt="Created this overview of agent orchestration tools, frameworks and benchmarks, quickly showing you the best use cases and OSS status. Contributions welcome!" src="https://external-preview.redd.it/g5nzYl2sJjMIBk8jIIhD5jIk-_ONQU1Nb8JjqSJC1aQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a46f18aa359932b4de13739bd50a5cf7de449dda" title="Created this overview of agent orchestration tools, frameworks and benchmarks, quickly showing you the best use cases and OSS status. Contributions welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everybody, I did this to help out a few friends. Assuming you have your LLMs ready to go, you might be wondering how to orchestrate your agents. This is a nice jumping point when starting a new project, or when you want to have a bird's eye view of what's available. Let me know if I missed anything, and also, you're welcome to contribute!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oatilis"&gt; /u/Oatilis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imraf.github.io/agent-orchestration-tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfjvrz/created_this_overview_of_agent_orchestration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfjvrz/created_this_overview_of_agent_orchestration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T17:53:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qewdza</id>
    <title>What are you building with sub-4B LLMs in early 2025? Real-world use wins?</title>
    <updated>2026-01-16T23:44:29+00:00</updated>
    <author>
      <name>/u/Whiplashorus</name>
      <uri>https://old.reddit.com/user/Whiplashorus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, It's early 2025, and I'm diving deep into tiny LLMs (under 4B params) like Qwen3 4B, LFM2.5 1.2B, or LFM2.5 VL 1.6B.&lt;/p&gt; &lt;p&gt;These base models (no fine-tuning) are super lightweight and run anywhere, but I'm curious: what real-world use cases have you found that actually stick ?&lt;/p&gt; &lt;p&gt;Stuff that's genuinely useful day-to-day, not just benchmarks.Have you plugged them into pipelines like n8n, Make.com, or custom scripts? How's that working out?Any cool automations, agents, or edge deployments (phone, Raspberry Pi, etc.)? Please share your successes, setups, or even failure&lt;/p&gt; &lt;p&gt;I'm all ears! What's the most practical thing you've pulled off?&lt;/p&gt; &lt;p&gt;I wished to do something with my vacant homelab &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whiplashorus"&gt; /u/Whiplashorus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T23:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf1msz</id>
    <title>Cowork but with local models not to send all your data to a remote cloud!</title>
    <updated>2026-01-17T03:05:24+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt; &lt;img alt="Cowork but with local models not to send all your data to a remote cloud!" src="https://external-preview.redd.it/bXh5cnloeGpzdGRnMQQmqaqY6IxBgH-vwmsMWuXB8i4MNI5FplyyvMhZJ44G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0f045206bb7f1efd15ba454efa839b61d2ec86" title="Cowork but with local models not to send all your data to a remote cloud!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wmrdxexjstdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T03:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfe81r</id>
    <title>vLLM with offloading vs. llama.cpp?</title>
    <updated>2026-01-17T14:12:53+00:00</updated>
    <author>
      <name>/u/Careful_Breath_1108</name>
      <uri>https://old.reddit.com/user/Careful_Breath_1108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Getting ~17 tps token generation on gptoss120b with 128gb 2400 ddr4 ram and 32gb of rtx nvidia vram using default llama-server settings on llama.cpp. Trying to figure out if it‚Äôs possible to squeeze more performance out. &lt;/p&gt; &lt;p&gt;I thought vLLM was only for models that fit fully into vram, but recently found out that vLLM supports model offloading. Was wondering any one has experience with this and how it compares to llamacpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careful_Breath_1108"&gt; /u/Careful_Breath_1108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfe81r/vllm_with_offloading_vs_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfe81r/vllm_with_offloading_vs_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfe81r/vllm_with_offloading_vs_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T14:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeuh0z</id>
    <title>Prompt Repetition Improves Non-Reasoning LLMs - a paper</title>
    <updated>2026-01-16T22:35:01+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2512.14982"&gt;https://arxiv.org/pdf/2512.14982&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I love these little tiny prompt techniques that can potentially lead to greater model accuracy and performance. Simply repeating the prompt twice lead to notable performance gains.&lt;/p&gt; &lt;p&gt;From the paper:&lt;/p&gt; &lt;p&gt;&amp;quot;We show that repeating the prompts consistently improves model performance for a range of models and benchmarks, when not using reasoning. In addition, latency is not impacted, as only the parallelizable pre-fill stage is affected. Prompt repetition does not change the lengths or formats of the generated outputs, and it might be a good default for many models and tasks, when reasoning is not used.&lt;/p&gt; &lt;p&gt;So simple but they demonstrate impressive gains on several benchmark scores. Looks like Deepseek is the only open weights model put through the wringer.&lt;/p&gt; &lt;p&gt;Best of wishes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfcg4h</id>
    <title>Need to know more about less known engines (ik_llama.cpp, exllamav3..)</title>
    <updated>2026-01-17T12:51:26+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually stick to llama.cpp and vllm but llama.cpp speed may not be the best and vllm/sglang can be really annoying if you have several gpus without respecting the power of 2 for tp.&lt;/p&gt; &lt;p&gt;So, for people who really know others projects (I mainly know ik_llama and exl3) could you please provide some feedback on where they really shine and what are their main constraints and limits (model/hardware support, tool calling, stability‚Ä¶).&lt;/p&gt; &lt;p&gt;Testing / understanding stuff may take some time so any usefull info is good to have, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfbt9f</id>
    <title>Local Replacement for Phind.com</title>
    <updated>2026-01-17T12:19:01+00:00</updated>
    <author>
      <name>/u/Past-Economist7732</name>
      <uri>https://old.reddit.com/user/Past-Economist7732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As many are aware, &lt;a href="https://www.phind.com/"&gt;https://www.phind.com/&lt;/a&gt; has shut down. I don‚Äôt know how many people on here used it, but I used to love the service back when it was an ai search engine, you could prompt the ai and it would search the internet for relevant info, and ONLY THEN respond. (Don‚Äôt get me started on the final iteration of phind, the atrocious ‚ÄúI‚Äôm going to build you a website to answer your question‚Äù, that was not useful to me). &lt;/p&gt; &lt;p&gt;Is there any way to recreate ai search behavior with local models? Maybe with openwebui somehow? There are some agentic workflows that can kick out to do a web search but sometimes I want to begin with the search and see the results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Past-Economist7732"&gt; /u/Past-Economist7732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfojft</id>
    <title>Are any small or medium-sized businesses here actually using AI in a meaningful way?</title>
    <updated>2026-01-17T20:53:45+00:00</updated>
    <author>
      <name>/u/brentmeistergeneral_</name>
      <uri>https://old.reddit.com/user/brentmeistergeneral_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to figure out how to apply AI at work beyond the obvious stuff. Looking for real examples where it‚Äôs improved efficiency, reduced workload, or added value.&lt;/p&gt; &lt;p&gt;I work at a design and production house and I am seeing AI starting to get used for example client design renders to staff generally using co pilot chatgpt and Gemini etc.&lt;/p&gt; &lt;p&gt;Just wondering if you guys can tell me other ways I can use AI that could help small companies that aren't really mainstream yet? Whether it's for day to day admin, improving operational efficiencies etc. &lt;/p&gt; &lt;p&gt;Thanks guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brentmeistergeneral_"&gt; /u/brentmeistergeneral_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfojft/are_any_small_or_mediumsized_businesses_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfojft/are_any_small_or_mediumsized_businesses_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfojft/are_any_small_or_mediumsized_businesses_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T20:53:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfq9ez</id>
    <title>The Search for Uncensored AI (That Isn‚Äôt Adult-Oriented)</title>
    <updated>2026-01-17T22:03:23+00:00</updated>
    <author>
      <name>/u/Fun-Situation-4358</name>
      <uri>https://old.reddit.com/user/Fun-Situation-4358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been trying to find an AI that‚Äôs genuinely unfiltered &lt;em&gt;and&lt;/em&gt; technically advanced, uncensored something that can reason freely without guardrails killing every interesting response.&lt;/p&gt; &lt;p&gt;Instead, almost everything I run into is marketed as ‚Äúuncensored,‚Äù but it turns out to be optimized for low-effort adult use rather than actual intelligence or depth.&lt;/p&gt; &lt;p&gt;It feels like the space between heavily restricted corporate AI and shallow adult-focused models is strangely empty, and I‚Äôm curious why that gap still exists...&lt;/p&gt; &lt;p&gt;Is there any &lt;strong&gt;uncensored or lightly filtered AI&lt;/strong&gt; that focuses on reasoning, creativity,uncensored technology or serious problem-solving instead? I‚Äôm open to self-hosted models, open-source projects, or lesser-known platforms. Suggestions appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Situation-4358"&gt; /u/Fun-Situation-4358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T22:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf514i</id>
    <title>"Welcome to the Local Llama. How janky's your rig?</title>
    <updated>2026-01-17T05:44:01+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt; &lt;img alt="&amp;quot;Welcome to the Local Llama. How janky's your rig?" src="https://preview.redd.it/rzbni3vvkudg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d48b6efbc81ebde1857313c676df8a19d5193dbc" title="&amp;quot;Welcome to the Local Llama. How janky's your rig?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzbni3vvkudg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T05:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf77hw</id>
    <title>Nvidia GH200 and AMD Mi325X can be shipped to china now.</title>
    <updated>2026-01-17T07:44:59+00:00</updated>
    <author>
      <name>/u/GPTshop--dot--ai</name>
      <uri>https://old.reddit.com/user/GPTshop--dot--ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"&gt; &lt;img alt="Nvidia GH200 and AMD Mi325X can be shipped to china now." src="https://preview.redd.it/qlc9o31a6vdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7748ec7117693d51bc563327e85220798d4f7a84" title="Nvidia GH200 and AMD Mi325X can be shipped to china now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The US export controls have been amended. GH200 and Mi325X can be shipped to china now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop--dot--ai"&gt; /u/GPTshop--dot--ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qlc9o31a6vdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T07:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfk2ky</id>
    <title>Optimizing GPT-OSS 120B on Strix Halo 128GB?</title>
    <updated>2026-01-17T18:00:26+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per the title, I want to optimize running GPT-OSS 120B on a strix halo box with 128GB RAM. I've seen plenty of posts over time about optimizations and tweaks people have used (eg. particular drivers, particular memory mappings, etc). I'm searching around &lt;a href="/r/localllama"&gt;/r/localllama&lt;/a&gt;, but figured I would also post and ask directly for your tips and tricks. Planning on running Ubuntu 24.04 LTS. &lt;/p&gt; &lt;p&gt;Very much appreciate any of your hard-earned tips and tricks!&lt;/p&gt; &lt;p&gt;Edit: some more info: &lt;/p&gt; &lt;p&gt;Planning on running Ubuntu 24.04 LTS and llama.cpp + vulkan (or rocm if it is faster for inference, but that has not been my experience previously). I currently run the UD 2.0 FP16 quant (unsloth/gpt-oss-120b-GGUF/gpt-oss-120b-F16.gguf) on an AMD 7040U series apu with 128GB DDR5 RAM, with 96GB dedicated GTT, and get ~13tps with that setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfpomi</id>
    <title>[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU</title>
    <updated>2026-01-17T21:39:36+00:00</updated>
    <author>
      <name>/u/ThisGonBHard</name>
      <uri>https://old.reddit.com/user/ThisGonBHard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt; &lt;img alt="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" src="https://external-preview.redd.it/BkpkFFoxQzTdVFBwygr_NjC6jb0CW1UxI49hdIPceBg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2ae4d7ac52eeeba792fbdb62506b06e57290b67" title="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems quite interesting, in getting the 48 GB cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThisGonBHard"&gt; /u/ThisGonBHard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/TcRGBeOENLg?si=2CKaZR7Dj0x89MMU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T21:39:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfaxpx</id>
    <title>Analysis of running local LLMs on Blackwell GPUs. TLDR: cheaper to run than cloud api services</title>
    <updated>2026-01-17T11:30:51+00:00</updated>
    <author>
      <name>/u/cchung261</name>
      <uri>https://old.reddit.com/user/cchung261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;May provide support to management for the cost savings of running local LLMs. The paper also includes amortization costs for the GPUs. I was surprised by the findings and the short break even time with cloud api costs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2601.09527"&gt;https://arxiv.org/abs/2601.09527&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cchung261"&gt; /u/cchung261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qff481</id>
    <title>I built Adaptive-K routing: 30-52% compute savings on MoE models (Mixtral, Qwen, OLMoE)</title>
    <updated>2026-01-17T14:50:29+00:00</updated>
    <author>
      <name>/u/Fuzzy_Ad_1390</name>
      <uri>https://old.reddit.com/user/Fuzzy_Ad_1390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Gabrobals/sbm-efficient"&gt;https://github.com/Gabrobals/sbm-efficient&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Whitepaper: &lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;https://adaptive-k.vercel.app/whitepaper.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TensorRT-LLM PR: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/pull/10672"&gt;https://github.com/NVIDIA/TensorRT-LLM/pull/10672&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live demo: &lt;a href="https://huggingface.co/spaces/Gabrobals/adaptive-k-demo"&gt;https://huggingface.co/spaces/Gabrobals/adaptive-k-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or discuss implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fuzzy_Ad_1390"&gt; /u/Fuzzy_Ad_1390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T14:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfogkp</id>
    <title>Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?</title>
    <updated>2026-01-17T20:50:29+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"&gt; &lt;img alt="Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?" src="https://external-preview.redd.it/MHgzNjM0N2QyemRnMQsZsHM-tqgfg0XXBK6smBja3B3Y-8kZyS2BD6gyOUFy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=731f35ba5df810d40f5158dcba9a22ffad9f0bb8" title="Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prototyped this in a few minutes. Seems incredibly useful for smaller devices (mobile LLMs)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ad16dbhd2zdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T20:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfgiq1</id>
    <title>MCP server that gives local LLMs memory, file access, and a 'conscience' - 100% offline on Apple Silicon</title>
    <updated>2026-01-17T15:45:41+00:00</updated>
    <author>
      <name>/u/TheTempleofTwo</name>
      <uri>https://old.reddit.com/user/TheTempleofTwo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a few weeks and finally got it stable enough to share.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem I wanted to solve:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local LLMs are stateless - they forget everything between sessions&lt;/li&gt; &lt;li&gt;No governance - they'll execute whatever you ask without reflection&lt;/li&gt; &lt;li&gt;Chat interfaces don't give them &amp;quot;hands&amp;quot; to actually do things&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A stack that runs entirely on my Mac Studio M2 Ultra:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LM Studio (chat interface) ‚Üì Hermes-3-Llama-3.1-8B (MLX, 4-bit) ‚Üì Temple Bridge (MCP server) ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ BTB ‚îÇ Threshold ‚îÇ ‚îÇ (filesystem ‚îÇ (governance ‚îÇ ‚îÇ operations) ‚îÇ protocols) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What the AI can actually do:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read/write files in a sandboxed directory&lt;/li&gt; &lt;li&gt;Execute commands (pytest, git, ls, etc.) with an allowlist&lt;/li&gt; &lt;li&gt;Consult &amp;quot;threshold protocols&amp;quot; before taking actions&lt;/li&gt; &lt;li&gt;Log its entire cognitive journey to a JSONL file&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ask for my approval before executing anything dangerous&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The key insight:&lt;/strong&gt; The filesystem itself becomes the AI's memory. Directory structure = classification. File routing = inference. No vector database needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Hermes-3?&lt;/strong&gt; Tested a bunch of models for MCP tool calling. Hermes-3-Llama-3.1-8B was the most stable - no infinite loops, reliable structured output, actually follows the tool schema.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The governance piece:&lt;/strong&gt; Before execution, the AI consults governance protocols and reflects on what it's about to do. When it wants to run a command, I get an approval popup in LM Studio. I'm the &amp;quot;threshold witness&amp;quot; - nothing executes without my explicit OK.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-time monitoring:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tail -f spiral_journey.jsonl | jq &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Shows every tool call, what phase of reasoning the AI is in, timestamps, the whole cognitive trace.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; On M2 Ultra with 36GB unified memory, responses are fast. The MCP overhead is negligible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repos (all MIT licensed):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/temple-bridge"&gt;temple-bridge&lt;/a&gt; - The MCP server that binds it together&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;back-to-the-basics&lt;/a&gt; - Filesystem-as-circuit paradigm&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/threshold-protocols"&gt;threshold-protocols&lt;/a&gt; - Governance framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the three repos&lt;/li&gt; &lt;li&gt;&lt;code&gt;uv sync&lt;/code&gt; in temple-bridge&lt;/li&gt; &lt;li&gt;Add the MCP config to &lt;code&gt;~/.lmstudio/mcp.json&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Load Hermes-3 in LM Studio&lt;/li&gt; &lt;li&gt;Paste the system prompt&lt;/li&gt; &lt;li&gt;Done&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full instructions in the README.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next:&lt;/strong&gt; Working on &amp;quot;governed derive&amp;quot; - the AI can propose filesystem reorganizations based on usage patterns, but only executes after human approval. The goal is AI that can self-organize but with structural restraint built in.&lt;/p&gt; &lt;p&gt;Happy to answer questions. This was a multi-week collaboration between me and several AI systems (Claude, Gemini, Grok) - they helped architect it, I implemented and tested. The lineage is documented in &lt;a href="http://ARCHITECTS.md"&gt;ARCHITECTS.md&lt;/a&gt; if anyone's curious about the process.&lt;/p&gt; &lt;p&gt;- Temple Bridge: &lt;a href="https://github.com/templetwo/temple-bridge"&gt;https://github.com/templetwo/temple-bridge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Back to the Basics: &lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;https://github.com/templetwo/back-to-the-basics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Threshold Protocols: &lt;a href="https://github.com/templetwo/threshold-protocols"&gt;https://github.com/templetwo/threshold-protocols&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üåÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheTempleofTwo"&gt; /u/TheTempleofTwo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T15:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfb0gk</id>
    <title>KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop</title>
    <updated>2026-01-17T11:35:11+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, it's been a hot minute, but I thought I'd share this here since it's quite a big new feature. &lt;/p&gt; &lt;p&gt;Yes, KoboldCpp is still alive and kicking. And besides the major UI overhaul, we've finally added native MCP support in KoboldCpp v1.106! It's designed to be a painless Claude Desktop drop-in replacement with maximum compatibility, the &lt;code&gt;mcp.json&lt;/code&gt; uses the same format so you can swap it in easily. &lt;/p&gt; &lt;p&gt;The KoboldCpp MCP bridge will connect to all provided MCP servers (HTTP and STDIO transports both supported) and automatically forward requests for tools the AI selects to the correct MCP server. This MCP bridge can also be used by third party clients.&lt;/p&gt; &lt;p&gt;On the frontend side, you can fetch the list of all tools from all servers, select the tools you want to let AI use, and optionally enable tool call approvals.&lt;/p&gt; &lt;p&gt;Some demo screenshots of various tool servers being used: &lt;a href="https://imgur.com/a/fKeWKUU"&gt;https://imgur.com/a/fKeWKUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;feedback is welcome. cheers! - concedo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf5oj0</id>
    <title>DeepSeek Engram : A static memory unit for LLMs</title>
    <updated>2026-01-17T06:18:14+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeeepSeek AI released a new paper titled &amp;quot;Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models&amp;quot; introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram &lt;strong&gt;adds native memory lookup&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think of it as separating &lt;strong&gt;remembering from reasoning&lt;/strong&gt;. Traditional MoE focuses on conditional computation, Engram introduces &lt;strong&gt;conditional memory&lt;/strong&gt;. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge is &lt;strong&gt;looked up in O(1)&lt;/strong&gt; instead of recomputed.&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;explicit parametric memory&lt;/strong&gt; vs implicit weights only.&lt;/li&gt; &lt;li&gt;Improves reasoning, math, and code performance.&lt;/li&gt; &lt;li&gt;Enables massive memory scaling &lt;strong&gt;without GPU limits&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Frees attention for &lt;strong&gt;global reasoning&lt;/strong&gt; rather than static knowledge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper : &lt;a href="https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf"&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub"&gt;https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T06:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfmc05</id>
    <title>China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)</title>
    <updated>2026-01-17T19:25:24+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt; &lt;img alt="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" src="https://external-preview.redd.it/TpKYg79IWzebupDqkzAodJruBP4N0VFsDaZESasEpKQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea8968e021234c9b599b354059d32de716c52bed" title="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone else posted about this, but never posted a transcript, so I found one online.&lt;/p&gt; &lt;p&gt;Lot of interesting stuff about China vs US, paths to AGI, compute, marketing etc.&lt;/p&gt; &lt;p&gt;Unfortunately Moonshot seems to have a very short section. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/the-all-star-chinese-ai-conversation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T19:25:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfkn3a</id>
    <title>Best "End of world" model that will run on 24gb VRAM</title>
    <updated>2026-01-17T18:21:20+00:00</updated>
    <author>
      <name>/u/gggghhhhiiiijklmnop</name>
      <uri>https://old.reddit.com/user/gggghhhhiiiijklmnop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps, I'm feeling in a bit of a omg the world is ending mood and have been amusing myself by downloading and hoarding a bunch of data - think wikipedia, wiktionary, wikiversity, khan academy, etc etc&lt;/p&gt; &lt;p&gt;What's your take on the smartest / best model(s) to download and store - they need to fit and run on my 24gb VRAM / 64gb RAM PC.? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gggghhhhiiiijklmnop"&gt; /u/gggghhhhiiiijklmnop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
