<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-08T02:13:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o0fjpa</id>
    <title>Top performing models across 4 professions covered by APEX</title>
    <updated>2025-10-07T13:59:21+00:00</updated>
    <author>
      <name>/u/RaselMahadi</name>
      <uri>https://old.reddit.com/user/RaselMahadi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0fjpa/top_performing_models_across_4_professions/"&gt; &lt;img alt="Top performing models across 4 professions covered by APEX" src="https://preview.redd.it/tt20ohtd4ptf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c9eea4438984a3bfb8f2c4c0087eeb1920ef40" title="Top performing models across 4 professions covered by APEX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaselMahadi"&gt; /u/RaselMahadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tt20ohtd4ptf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0fjpa/top_performing_models_across_4_professions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0fjpa/top_performing_models_across_4_professions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T13:59:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o04s7y</id>
    <title>2 things we never forget, our first GPU and when your first GPU dies</title>
    <updated>2025-10-07T04:07:24+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just had a 3090 die, maybe I will resurrect it, maybe not. It comes with the territory of buying used GPUs from miners.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T04:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o00ban</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-10-07T00:31:00+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0kf1o</id>
    <title>Best ways to run Qwen3 on CPU with 16 GB RAM</title>
    <updated>2025-10-07T16:58:23+00:00</updated>
    <author>
      <name>/u/Remarkable_Story_310</name>
      <uri>https://old.reddit.com/user/Remarkable_Story_310</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any further technique than Quantization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Story_310"&gt; /u/Remarkable_Story_310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kf1o/best_ways_to_run_qwen3_on_cpu_with_16_gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kf1o/best_ways_to_run_qwen3_on_cpu_with_16_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kf1o/best_ways_to_run_qwen3_on_cpu_with_16_gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T16:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o08igx</id>
    <title>Improved "time to first token" in LM Studio</title>
    <updated>2025-10-07T07:51:05+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"&gt; &lt;img alt="Improved &amp;quot;time to first token&amp;quot; in LM Studio" src="https://preview.redd.it/m2ttxrud9ntf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca5e7ac12bb3b1413f0d820c13cc0f0b9bd9d1b5" title="Improved &amp;quot;time to first token&amp;quot; in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was benching some of my models on my M4 Max 128GB a few days ago, see the attached image.&lt;/p&gt; &lt;p&gt;Today I noticed an update of the MLX runtime in LM Studio:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;MLX version info: - mlx-engine==6a8485b - mlx==0.29.1 - mlx-lm==0.28.1 - mlx-vlm==0.3.3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With this, &amp;quot;time to first token&amp;quot; has been improved dramatically. As an example:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next:80b&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;4 bit MLX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// 80k context window + 36k token prompt length Time to first token: 47 ‚ûî 46 seconds :| // 120k context window + 97k token prompt length Time to first token: 406 ‚ûî 178 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next:80b&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;6 bit MLX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// 80k context window + 36k token prompt length Time to first token: 140 ‚ûî 48 seconds // 120k context window + 97k token prompt length Time to first token: 436 ‚ûî 190 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can anyone confirm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m2ttxrud9ntf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T07:51:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0tk6x</id>
    <title>How to setup Linux environment?</title>
    <updated>2025-10-07T22:35:47+00:00</updated>
    <author>
      <name>/u/Techngro</name>
      <uri>https://old.reddit.com/user/Techngro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm setting up a fresh WSL Ubuntu install for local LLM (because my Debian install is a mess). My goal is to keep this install clean, so no unnecessary stuff. I asked ChatGPT what are some essential software/tools to install and this is what it suggested:&lt;/p&gt; &lt;p&gt;Conda/Microconda (I think I want to use UV though)&lt;/p&gt; &lt;p&gt;CUDA Toolkit&lt;/p&gt; &lt;p&gt;NVIDIA GPU Monitoting (gpustat)&lt;/p&gt; &lt;p&gt;Pytorch torchvision torchaudio&lt;/p&gt; &lt;p&gt;Tensorflow-gpu&lt;/p&gt; &lt;p&gt;vllm&lt;/p&gt; &lt;p&gt;llama.cpp&lt;/p&gt; &lt;p&gt;What do you think of this list? What other software tools do you think I should install? And for those of you who use UV, does it really help avoid dependency hell? In the short time I tried running llama.cpp using venv/conda on my Debian install, I was wasting a lot of time trying to fix errors with installing dependencies.&lt;/p&gt; &lt;p&gt;Once I get a list of the best/most useful software, I want to create a script that automates the installation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Techngro"&gt; /u/Techngro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0tk6x/how_to_setup_linux_environment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0tk6x/how_to_setup_linux_environment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0tk6x/how_to_setup_linux_environment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T22:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzwnbj</id>
    <title>The qwen3-next pr in llamacpp has been validated with a small test model</title>
    <updated>2025-10-06T21:52:11+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt; &lt;img alt="The qwen3-next pr in llamacpp has been validated with a small test model" src="https://b.thumbs.redditmedia.com/WI7I1cXVA_soqNsjPi7jb3WgjXU9vLhmlQDKP6zYAUY.jpg" title="The qwen3-next pr in llamacpp has been validated with a small test model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to comment: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382"&gt;https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been stalking this pr since it was opened and figured I'd share this update since I know a lot of others were interested in this model. Pwilkin has done some crazy work getting this together so quickly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nzwnbj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T21:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0nfmi</id>
    <title>Introducing SIM-CoT-GPT2-CODI: A LoRA-Fine-Tuned 346M Parameter Implicit Reasoning Model Leveraging Supervised Latent Space Stabilization via Auxiliary Decoder Alignment for 2.3x Token Efficiency Gains Over Explicit Chain-of-Thought on GSM8K and MultiArith Benchmarks</title>
    <updated>2025-10-07T18:45:10+00:00</updated>
    <author>
      <name>/u/ArchdukeofHyperbole</name>
      <uri>https://old.reddit.com/user/ArchdukeofHyperbole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/SIM_COT-GPT2-CODI"&gt;https://huggingface.co/internlm/SIM_COT-GPT2-CODI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArchdukeofHyperbole"&gt; /u/ArchdukeofHyperbole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0nfmi/introducing_simcotgpt2codi_a_lorafinetuned_346m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0nfmi/introducing_simcotgpt2codi_a_lorafinetuned_346m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0nfmi/introducing_simcotgpt2codi_a_lorafinetuned_346m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T18:45:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0wnwt</id>
    <title>M2 Max 96GB vs Strix Halo 128GB?</title>
    <updated>2025-10-08T00:53:09+00:00</updated>
    <author>
      <name>/u/esamueb32</name>
      <uri>https://old.reddit.com/user/esamueb32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm considering purchasing either of these two options&lt;/p&gt; &lt;ul&gt; &lt;li&gt;M2 Max 96GB using macOS&lt;/li&gt; &lt;li&gt;Mini PC with Strix Halo (AMD AI Max+ 395) 128GB using Linux&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As fas as I know, M2 Max has higher bandwidth (400GB/s) compared to Strix Halo (&amp;lt; 250GB/s).&lt;/p&gt; &lt;p&gt;M2 Max solution is slightly cheaper than strix halo.&lt;/p&gt; &lt;p&gt;But comparing real world benchmarks, which is better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/esamueb32"&gt; /u/esamueb32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0wnwt/m2_max_96gb_vs_strix_halo_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0wnwt/m2_max_96gb_vs_strix_halo_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0wnwt/m2_max_96gb_vs_strix_halo_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T00:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0rjxl</id>
    <title>Thinking of text-to-image models</title>
    <updated>2025-10-07T21:16:40+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, while I wait for MaxSun to release their B60 Turbo card (I plan to buy two), I am learning about kv-cache, quantization and alike and crawling the vLLM docs to learn what the best parameters are to set when using it as a backend for LocalAI, which I plan to use as my primary inference server.&lt;/p&gt; &lt;p&gt;One of the most-used features for me in ChatGPT that I want to have at home is image generation. It does not need to be great, it just needs to be &amp;quot;good&amp;quot;. Reason for that is that I often feed reference images and text to ChatGPT to draw certain details of characters that I have difficulty imagening - I am visually impaired, and whilst my imagination is solid, having a bit of visual stuff to go along is really helpful to have.&lt;/p&gt; &lt;p&gt;The primary model I will run is Qwen3 32B Q8 with a similaririly quant'ed kv-cache, whereas the latter is largely offloaded to host memory (thinking of 512GB - Epyc 9334, so DDR5). Qwen3 should run &amp;quot;fast&amp;quot; (high-ish t/s - I am targeting around 15, circa).&lt;/p&gt; &lt;p&gt;But on the side, loaded on demand, I want to be able to generate images. Paralellism for that configuration will be set to one - I only need one instance and one inference of a text-to-image model at a time.&lt;/p&gt; &lt;p&gt;I looked at FLUX, HiDream, a demo of HunyanImage-3.0 and NanoBanana and I like the latter two's output quite a lot. So something like this would be nice to host locally, even if not as good as those.&lt;/p&gt; &lt;p&gt;What are the &amp;quot;state of the art&amp;quot; locally runnable text-to-image models?&lt;/p&gt; &lt;p&gt;I am targeting a Supermicro H13SSL-N motherboard, if I plug the B60s in the lower two x16 slots, I technically have another left for a 2-slot x16 card, where I might plop a cheaper, lower power card just for &amp;quot;other models&amp;quot; in the future, where speed does not matter too much (perhaps the AMD AI Pro R9700 - seems it'd fit).&lt;/p&gt; &lt;p&gt;If the model happened to also be text+image-to-image, that'd be really useful. Unfortunately, ComfyUI kinda breaks me (too many lines, completely defeats my vision...) so I would have to use a template here if needed.&lt;/p&gt; &lt;p&gt;Thank you and kind regards!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0rjxl/thinking_of_texttoimage_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0rjxl/thinking_of_texttoimage_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0rjxl/thinking_of_texttoimage_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T21:16:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0kwx3</id>
    <title>Granite 4.0 on iGPU AMD Ryzen 6800H llama.cpp benchmark</title>
    <updated>2025-10-07T17:15:53+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New MoE model for testing:&lt;/p&gt; &lt;p&gt;Granite-4.0-H-Small is a 32B parameter, 9B active and long-context instruct model &lt;a href="https://huggingface.co/unsloth/granite-4.0-h-small-GGUF"&gt;unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System: Kubuntu 25.10 OS, Kernel 6.17.0-5-generic with 64GB DDR5 ram. AMD Radeon Graphics (RADV REMBRANDT) Ryzen 6800H and 680M iGPU&lt;br /&gt; Llama.cpp Vulkan build: ca71fb9b (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6692"&gt;6692&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;granite-4.0-h-small-UD-Q8_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;35.47 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;72.56 ¬± 0.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;35.47 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;4.26 ¬± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-UD-Q6_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q6_K&lt;/td&gt; &lt;td align="left"&gt;25.95 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;54.77 ¬± 1.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q6_K&lt;/td&gt; &lt;td align="left"&gt;25.95 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;5.51 ¬± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-UD-Q5_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;57.90 ¬± 4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.36 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-UD-Q4_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;57.26 ¬± 2.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.21 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-IQ4_XS.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;16.23 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;57.31 ¬± 2.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;16.23 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.17 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Add this for comparison:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;t/s (pp512)&lt;/th&gt; &lt;th align="left"&gt;t/s (tg128)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K&lt;/td&gt; &lt;td align="left"&gt;17.28&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;134.46 ¬± 0.45&lt;/td&gt; &lt;td align="left"&gt;28.26 ¬± 0.46&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Simplified view:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;t/s (pp512)&lt;/th&gt; &lt;th align="left"&gt;t/s (tg128)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q8_0&lt;/td&gt; &lt;td align="left"&gt;35.47 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;72.56 ¬± 0.79&lt;/td&gt; &lt;td align="left"&gt;4.26 ¬± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q6_K&lt;/td&gt; &lt;td align="left"&gt;25.95 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;54.77 ¬± 1.87&lt;/td&gt; &lt;td align="left"&gt;5.51 ¬± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;57.90 ¬± 4.46&lt;/td&gt; &lt;td align="left"&gt;6.36 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;57.26 ¬± 2.02&lt;/td&gt; &lt;td align="left"&gt;7.21 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;iGPU has flexibility of using system RAM as VRAM and can load larger models 32B and take advantage of using active parameters 9B to get decent speed from bigger parameter models. Looks like using Q8_K_XL has prompt processing benefit and Q5_K_XL for balance of speed on both sides of inference. Post here if you have an iGPU results to compare. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kwx3/granite_40_on_igpu_amd_ryzen_6800h_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kwx3/granite_40_on_igpu_amd_ryzen_6800h_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kwx3/granite_40_on_igpu_amd_ryzen_6800h_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T17:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ssd2</id>
    <title>A 5-minute, no-BS way to pick a local model for your real task</title>
    <updated>2025-10-07T22:04:21+00:00</updated>
    <author>
      <name>/u/Zealousideal-Fox-76</name>
      <uri>https://old.reddit.com/user/Zealousideal-Fox-76</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey fam&lt;/strong&gt;, I've been searching through posts on how to pick a local model, and I found lots of good posts emphasizing the fact that it's highly unreliable for a universal benchmark, and the best way is to test local AI's with your own real usecases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I want to share &lt;strong&gt;my current way of picking a model in 5-10 mins&lt;/strong&gt;. Feel free to comment on your own usecases to testout, and would be awesome to have some feedbacks and model recommendations!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; help anyone quickly find a ‚Äúgood enough‚Äù local model for &lt;em&gt;their&lt;/em&gt; workflow‚Äîwithout randomly chasing leaderboards.&lt;br /&gt; &lt;strong&gt;My task:&lt;/strong&gt; private resume screening (50+ pages PDF) with inline citations. (I'm using a public resume book as an example)&lt;br /&gt; &lt;strong&gt;Stack:&lt;/strong&gt; MacBook Air M2 (16GB) + Hyperlink as the local RAG runner (swap models for trials). &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Fileset &amp;amp; prompt:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fileset:&lt;/strong&gt; Princeton Resume Book (public accessible)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Who are most qualified candidate for IB at top-tier banks and why?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5-minute protocol (once per model)&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Connect files&lt;/strong&gt; into Hyperlink local file agent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pick model&lt;/strong&gt; (remember to check the box for compatibility with your PC specs).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hit run&lt;/strong&gt; and observe.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verify citations&lt;/strong&gt;: do quotes match the page/line?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o0ssd2/video/r64ocg3wirtf1/player"&gt;Best model example (cogito)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ranked models with take aways&lt;/strong&gt; (fit 16GB &amp;amp; commonly used)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;cogito-preview-llama-3B-4bit - clear logic (eval criteria -&amp;gt; suggestions -&amp;gt; conclusion)&lt;/li&gt; &lt;li&gt;granite-3.3-2B-Instruct-4bit - quick clean results, more criteria elaboration would be better&lt;/li&gt; &lt;li&gt;Llama-3.2-3B-Instruct-4bit - straight to the point + but less citations (bad)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;What mattered&lt;/strong&gt; (my priorities for the resume task)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Citations &amp;gt; vibes.&lt;/strong&gt; If I can‚Äôt click file pages and see the proof, it‚Äôs a miss and I'll drop the model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small models are good enough for my workflow.&lt;/strong&gt; 2‚Äì3B models were surprisingly competitive.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'll be sharing some more of my workflow testouts soon, especially with cloud-local AI collaboration in future posts. Happy to learn how other folks are testing local AIs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Fox-76"&gt; /u/Zealousideal-Fox-76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ssd2/a_5minute_nobs_way_to_pick_a_local_model_for_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ssd2/a_5minute_nobs_way_to_pick_a_local_model_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ssd2/a_5minute_nobs_way_to_pick_a_local_model_for_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T22:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0tpm8</id>
    <title>bench maxxing??</title>
    <updated>2025-10-07T22:42:16+00:00</updated>
    <author>
      <name>/u/BoringAd6806</name>
      <uri>https://old.reddit.com/user/BoringAd6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0tpm8/bench_maxxing/"&gt; &lt;img alt="bench maxxing??" src="https://preview.redd.it/mxvgitxnprtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8370c577a018e9a54f7056a5b047373457fa59c7" title="bench maxxing??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;https://huggingface.co/inclusionAI/Ring-1T-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoringAd6806"&gt; /u/BoringAd6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mxvgitxnprtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0tpm8/bench_maxxing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0tpm8/bench_maxxing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T22:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0n17o</id>
    <title>2 month MiniPC mini-review: Minisforum AI X1 Pro (AMD HX 370)</title>
    <updated>2025-10-07T18:30:41+00:00</updated>
    <author>
      <name>/u/ivoras</name>
      <uri>https://old.reddit.com/user/ivoras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0n17o/2_month_minipc_minireview_minisforum_ai_x1_pro/"&gt; &lt;img alt="2 month MiniPC mini-review: Minisforum AI X1 Pro (AMD HX 370)" src="https://external-preview.redd.it/9caqHyt_1L7Xsp4TpYFjM4oPMVmVIW9sJF7IMxoOn8I.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a78dfd017145cb54d5d1bf3ff16da49935fcf92a" title="2 month MiniPC mini-review: Minisforum AI X1 Pro (AMD HX 370)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: it's the AI Max 395+'s little brother. Half the price, but not a serious AI workstation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivoras"&gt; /u/ivoras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ivoras.substack.com/p/2-month-minipc-mini-review-minisforum"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0n17o/2_month_minipc_minireview_minisforum_ai_x1_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0n17o/2_month_minipc_minireview_minisforum_ai_x1_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T18:30:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0dy0y</id>
    <title>More love for GLM4.6 (evaluation vs. Claude 4.5 for NLP tasks)</title>
    <updated>2025-10-07T12:53:15+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been putting GLM4.6 and Claude 4.5 head to head relentlessly since both were released, and really can't overstate how impressive GLM4.6 is. I'm using both over OpenRouter. &lt;/p&gt; &lt;p&gt;My use case: critically evaluating published AI literature, working on my own architecture ideas, summarizing large articles, picking through sprawling conversations for the salient ideas.&lt;/p&gt; &lt;p&gt;What's really impressive to me is how good GLM4.6 is at following my instructions to the letter, understanding nuanced ways that I want it to analyze data, and avoiding putting its own spin on things. It's also absolutely fantastic at &amp;quot;thinking in character&amp;quot; (I use persona prompts to process information in parallel from different perspectives - ie. one run to critique literature and probe quality of experimental set-ups, another run to evaluate whether are creative implications that I'm missing, etc.) - this is a model that loves a great system prompt. The ability to shape the way GLM4.6 reasons is really impressive. The draw back in terms of persona prompting is that while GLM4.6 is great at functionally behaving according to the prompt, its tonal style usually drifts. I think this is more a factor of how MoE models process RP-adjacent prompting (I find that dense models are massively better at this) than it is a GLM4.6 problem specifically. GLM4.6 holds on to technical details of what I'm either reading or writing *spectacularly* well. It seems even more clear-headed than Claude when it comes to working on implementation ideas, or paying attention to implementation that I'm reading about. &lt;/p&gt; &lt;p&gt;Claude Sonnet 4.5 is impressive in terms of its ability to follow a huge list of complicated topics across many turns. Of every LLM I have tried, this truly keeps its head together longer than any I've tried. I have pushed the context window ridiculously far and have only seen one or two minor factual errors. Exact instruction following (ie. system instructions about cognitive processing requirements) gets dulled over time, for sure. And while 4.5 seems far better at persona prompting than 4 did, there's an underlying Claude-ness that just can't be denied. Even without the obnoxious LCR stuff going on in the Anthropic UI (not to mention their shady data mining reversal), Claude can't help but lapse into Professor Dad mode. (Just like Gemini can't really avoid being a former high school valedictorian who got into an Ivy on a lacrosse scholarship while still suffering from imposter syndrome) &lt;/p&gt; &lt;p&gt;GLM4.6 doesn't stay coherent quite as long - and there are some weird glitches: lapses into Chinese, confusing its reasoning layer for its response layer, and becoming repetitive in long responses (ie. saying the same thing twice). Still, it remains coherent FAR longer than Gemini 2.5 Pro. &lt;/p&gt; &lt;p&gt;What I find really interesting about GLM4.6 is that it seems to have no overtly detectable ideological bias - it's really open, and depending on how you prompt it, can truly look at things from multiple perspectives. DeepSeek and Kimi K2 both have slants (which I happen to dig!) - this might be the most flexible model I have tried, period. &lt;/p&gt; &lt;p&gt;If the lapse-into-chinese and repetitive loops could be stamped out a bit, this would be the no-brainer LLM to build with for what I do. (As always, with the caveat that I'm praying daily for a dense Gemma 3 or Gemma 4 model in the 50B+ range)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T12:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ly7m</id>
    <title>Fan shroud for AMD MI50</title>
    <updated>2025-10-07T17:52:35+00:00</updated>
    <author>
      <name>/u/Bit_Matter</name>
      <uri>https://old.reddit.com/user/Bit_Matter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, since the AMD MI50 is the cheapest graphic card with 32GB VRAM you can get at the moment, I bought 3 of them. In order to make them fit better in my case, I designed a new shroud for the card which integrates a blower fan. You can find it here: &lt;a href="https://www.printables.com/model/1421067-amd-instinct-mi50-shroud"&gt;https://www.printables.com/model/1421067-amd-instinct-mi50-shroud&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bit_Matter"&gt; /u/Bit_Matter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ly7m/fan_shroud_for_amd_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ly7m/fan_shroud_for_amd_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ly7m/fan_shroud_for_amd_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T17:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0k8vf</id>
    <title>ryzen 395+ with 96gb on sale sale for $1728</title>
    <updated>2025-10-07T16:52:21+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0k8vf/ryzen_395_with_96gb_on_sale_sale_for_1728/"&gt; &lt;img alt="ryzen 395+ with 96gb on sale sale for $1728" src="https://external-preview.redd.it/wyRlnnC4nIHWRfWMUIBnHvHMsP98N9mROJtKXbnwKWI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=290ace7209dd3df0a237ec970a6a8b1662d523e1" title="ryzen 395+ with 96gb on sale sale for $1728" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been watching mini PCs and this is $600 off&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amazon.com/GMKtec_ryzen_ai_max_395_mini_pc/dp/B0FLDJBS79?crid=1Q219TSIMC6E5&amp;amp;dib=eyJ2IjoiMSJ9.I2mCpbFC-I1kX_zpKzZjAVsC3UFEmWuAGsNmMRg4JjW-m65FEqL2voOm1dEASZH9A7BoEcVRQFBh4B8XK42Pd3cmsD6a0J3Puup9S6jg7SKf9mcVXlN4AxOZU88HfLTVjyD2uDnYWQg1dXLvo8EC33ImbdTdRO6_DV1m7kDC5Xo.DOidGG2jBnTmZefIX55ouM3iX6383KiTOjHDcWIesxo&amp;amp;dib_tag=se&amp;amp;keywords=395%2B%2Bmax&amp;amp;qid=1759855684&amp;amp;sprefix=395%2B%2Caps%2C130&amp;amp;sr=8-5&amp;amp;th=1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0k8vf/ryzen_395_with_96gb_on_sale_sale_for_1728/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0k8vf/ryzen_395_with_96gb_on_sale_sale_for_1728/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T16:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0or4w</id>
    <title>How much does 1T tokens cost? How much did all these amazing people spent on OpenAI tokens?</title>
    <updated>2025-10-07T19:32:48+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0or4w/how_much_does_1t_tokens_cost_how_much_did_all/"&gt; &lt;img alt="How much does 1T tokens cost? How much did all these amazing people spent on OpenAI tokens?" src="https://b.thumbs.redditmedia.com/KSfI0d7i7eYsGh_o5RwgORs0ebrpshjARUVd3xkxOJk.jpg" title="How much does 1T tokens cost? How much did all these amazing people spent on OpenAI tokens?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some math as a follow-up to OpenAI‚Äôs Dev Day yesterday and decided to share it here.&lt;/p&gt; &lt;p&gt;Assuming GPT-5 with a 4:1 input:output token ratio, 1T tokens means 800,000 million input tokens at $1.25 per million, which is $1,000,000, plus 200,000 million output tokens at $10 per million, adding $2,000,000, for a total of $3,000,000 for 1T tokens.&lt;/p&gt; &lt;p&gt;On this photo, 30 people consumed 1T tokens, 70 people 100B tokens, and 54 people 10B tokens, totaling $112,620,000, which is roughly 3% of OpenAI‚Äôs total $3.7 billion revenue in 2024.&lt;/p&gt; &lt;p&gt;Curious - is it even possible to process this amount of tokens using local models? What would be the cost in GPUs and residential electricity? üßê‚ö°Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/abylayo/status/1975546166113669170?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0or4w/how_much_does_1t_tokens_cost_how_much_did_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0or4w/how_much_does_1t_tokens_cost_how_much_did_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T19:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0st2o</id>
    <title>BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2 is possibly just a copy of Qwen's regular Qwen3-Coder-30B-A3B-Instruct</title>
    <updated>2025-10-07T22:05:06+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was brought up in &lt;a href="https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/discussions/1"&gt;https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/discussions/1&lt;/a&gt; and please note the &lt;em&gt;possibly&lt;/em&gt; I use in my language since unverified claims like this can be pretty damning.&lt;/p&gt; &lt;p&gt;Not sure if it's true or not, but one user seems to be convinced by their tests that the models are identical. Maybe someone smarter than me can look into this and verify this&lt;/p&gt; &lt;p&gt;EDIT - Yup. I think at this point it's pretty conclusive that this guy doesnt know what he's doing and vibe coded his way here. The models all have identical weights to the parent models. All of his distils.&lt;/p&gt; &lt;p&gt;Also, let's pay respects to anon user (not so anon if you just visit the thread to see who it is) from the discussion thread that claimed he was very picky and that we could trust him that the model was better:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/BasedBase"&gt;u/BasedBase&lt;/a&gt; feel free to add me to the list of satisfied customers lol. Your 480B coder distill in the small 30B package is something else and you guys can trust me I am VERY picky when it comes to output quality. I have no mercy for bad quality models and this one is certainly an improvement over the regular 30B coder. I've tested both thoroughly.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T22:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0i4fz</id>
    <title>Will DDR6 be the answer to LLM?</title>
    <updated>2025-10-07T15:34:17+00:00</updated>
    <author>
      <name>/u/fungnoth</name>
      <uri>https://old.reddit.com/user/fungnoth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bandwidth doubles every generation of system memory. And we need that for LLMs. &lt;/p&gt; &lt;p&gt;If DDR6 is going to be 10000+ MT/s easily, and then dual channel and quad channel would boast that even more. Maybe we casual AI users would be able to run large models around 2028. Like deepseek sized full models in a chat-able speed. And the workstation GPUs will only be worth buying for commercial use because they serve more than one user at a time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fungnoth"&gt; /u/fungnoth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0vjxr</id>
    <title>There isn‚Äôt a single AI Agent on the market that can give you a day of work</title>
    <updated>2025-10-08T00:02:04+00:00</updated>
    <author>
      <name>/u/Working-Magician-823</name>
      <uri>https://old.reddit.com/user/Working-Magician-823</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use AI Agents all day, and some of them can do very good work, but, none of them can complete a large task by themselves without human intervention. None of them can spend a full day of work, even if you give detailed requirements.&lt;/p&gt; &lt;p&gt;If AI Agents can‚Äôt do a full software without a human yet, it is unlikely they are ready to be fully adopted by any business.&lt;/p&gt; &lt;p&gt;Smarter AI is coming for sure, just not what we have today&lt;/p&gt; &lt;p&gt;And a PHD level human, or bachelor‚Äôs degree, can complete a product, but I keep hearing AI is PHD level, well!!! It is smart but unable to do the full work that is not PHD..ish&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working-Magician-823"&gt; /u/Working-Magician-823 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T00:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0f2uf</id>
    <title>Hi folks, sorry for the self‚Äëpromo. I‚Äôve built an open‚Äësource project that could be useful to some of you</title>
    <updated>2025-10-07T13:40:33+00:00</updated>
    <author>
      <name>/u/panos_s_</name>
      <uri>https://old.reddit.com/user/panos_s_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt; &lt;img alt="Hi folks, sorry for the self‚Äëpromo. I‚Äôve built an open‚Äësource project that could be useful to some of you" src="https://preview.redd.it/1tzatvfz0ptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=149ffc98b84835693e3aa54c4c554277120de6ea" title="Hi folks, sorry for the self‚Äëpromo. I‚Äôve built an open‚Äësource project that could be useful to some of you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Web dashboard for NVIDIA GPUs with 30+ real-time metrics (utilisation, memory, temps, clocks, power, processes). Live charts over WebSockets, multi‚ÄëGPU support, and one‚Äëcommand Docker deployment. No agents, minimal setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/psalias2006/gpu-hot"&gt;https://github.com/psalias2006/gpu-hot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wanted simple, real‚Äëtime visibility without standing up a full metrics stack.&lt;/li&gt; &lt;li&gt;Needed clear insight into temps, throttling, clocks, and active processes during GPU work.&lt;/li&gt; &lt;li&gt;A lightweight dashboard that‚Äôs easy to run at home or on a workstation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Polls nvidia-smi and streams 30+ metrics every ~2s via WebSockets.&lt;/li&gt; &lt;li&gt;Tracks per‚ÄëGPU utilization, memory (used/free/total), temps, power draw/limits, fan, clocks, PCIe, P‚ÄëState, encoder/decoder stats, driver/VBIOS, throttle status.&lt;/li&gt; &lt;li&gt;Shows active GPU processes with PIDs and memory usage.&lt;/li&gt; &lt;li&gt;Clean, responsive UI with live historical charts and basic stats (min/max/avg).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/psalias2006/gpu-hot cd gpu-hot docker-compose up --build # open http://localhost:1312 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looking for feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panos_s_"&gt; /u/panos_s_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1tzatvfz0ptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T13:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0puzj</id>
    <title>Samsung Paper Reveals a Recursive Technique that Beats Gemini 2.5 Pro on ARC-AGI with 0.01% of the Parameters!</title>
    <updated>2025-10-07T20:13:48+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.04871"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0puzj/samsung_paper_reveals_a_recursive_technique_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0puzj/samsung_paper_reveals_a_recursive_technique_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T20:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0php3</id>
    <title>Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser.</title>
    <updated>2025-10-07T20:00:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt; &lt;img alt="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." src="https://external-preview.redd.it/bXZwenNmemR3cXRmMQIkfIP27ngHfIf2o9FEvt2htapLOK3sF-ey3U1M3aWC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=681db9c824b6d01bd93193fb35668e28576d4f98" title="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM recently released Granite Docling, a 258M parameter VLM engineered for efficient document conversion. So, I decided to build a demo which showcases the model running entirely in your browser with WebGPU acceleration. Since the model runs locally, no data is sent to a server (perfect for private and sensitive documents).&lt;/p&gt; &lt;p&gt;As always, the demo is available and open source on Hugging Face: &lt;a href="https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU"&gt;https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/33mh4fzdwqtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T20:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ifyr</id>
    <title>Glm 4.6 air is coming</title>
    <updated>2025-10-07T15:46:04+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt; &lt;img alt="Glm 4.6 air is coming" src="https://preview.redd.it/nmwtp72fnptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78e29ea88c42c50216e45dc228bec7e885394f0c" title="Glm 4.6 air is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmwtp72fnptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect ‚Äî Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect ‚Äî Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM ‚Äì 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
