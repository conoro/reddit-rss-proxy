<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-22T13:32:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1raucof</id>
    <title>Wave Field LLM — O(n log n) attention via wave equation dynamics</title>
    <updated>2026-02-21T15:46:07+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an alternative attention mechanism that treats language as a physical field system instead of using standard O(n²) self-attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; - Tokens are mapped onto a continuous 1D field - Information propagates via damped wave equations: k(t) = exp(-α·t)·cos(ω·t + φ) - Each attention head has just 3 learnable physics parameters (frequency, damping, phase) - Convolution computed via FFT in O(n log n) - Heads self-organize into different roles (local grammar, medium context, long-range)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (WikiText-2, 6M params, character tokenizer):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;PPL&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Complexity&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Standard Transformer&lt;/td&gt; &lt;td&gt;5.9&lt;/td&gt; &lt;td&gt;51.0%&lt;/td&gt; &lt;td&gt;O(n²)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Wave Field V3.5&lt;/td&gt; &lt;td&gt;6.2&lt;/td&gt; &lt;td&gt;50.5%&lt;/td&gt; &lt;td&gt;O(n log n)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;At longer sequences the savings grow: 31x at 2K tokens, 107x at 8K, 367x at 32K.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Known limitations:&lt;/strong&gt; - With BPE tokenizer (8K vocab), there's a significant capacity gap vs standard transformer - This is a model capacity issue at small scale, not an architecture flaw - Currently scaling to 100M params to see if the gap closes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's unique:&lt;/strong&gt; - Every bug during development was found through physics-based diagnostics (energy flow, conservation, causality tests) — not guessing - Cross-head field coupling and wave interference for information routing - Not a Mamba/Hyena variant — different approach entirely&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;https://github.com/badaramoni/wave-field-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the physics, architecture decisions, or results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbi0ij</id>
    <title>Are AI coding agents (GPT/Codex, Claude Sonnet/Opus) actually helping you ship real products?</title>
    <updated>2026-02-22T09:58:38+00:00</updated>
    <author>
      <name>/u/darshan_aqua</name>
      <uri>https://old.reddit.com/user/darshan_aqua</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing AI coding agents a lot lately and I’m curious about real-world impact beyond demos.&lt;/p&gt; &lt;p&gt;A few things I keep noticing:&lt;/p&gt; &lt;p&gt;• They seem great with Python + JavaScript frameworks, but weaker with Java, C++, or more structured systems — is that true for others too?&lt;/p&gt; &lt;p&gt;• Do they genuinely speed up startup/MVP development, or do you still spend a lot of time fixing hallucinations and messy code?&lt;/p&gt; &lt;p&gt;As someone with ~15 years in software, I’m also wondering how experienced devs are adapting:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• leaning more into architecture/design? • using AI mostly for boilerplate? • building faster solo? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Some pain points I hit often:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• confident but wrong code • fake APIs • good at small tasks, shaky at big systems &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And with local/private AI tools:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• search quality can be rough • answers don’t always stick to your actual files • weak or missing citations • hard to trust memory &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Would love to hear what’s actually working for you in production — and what still feels like hype.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darshan_aqua"&gt; /u/darshan_aqua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T09:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbfasf</id>
    <title>Fine-Tuning Qwen 4B for Niche Code Generation: Need Tips on Configs, Overfitting &amp; Small Datasets?</title>
    <updated>2026-02-22T07:13:28+00:00</updated>
    <author>
      <name>/u/dyeusyt</name>
      <uri>https://old.reddit.com/user/dyeusyt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So am working on my thesis project which involves fine-tuning a small language model for a specific code generation task in a niche domain (Typescript)&lt;/p&gt; &lt;p&gt;I'm leaning toward the Qwen family of models. I started by fine-tuning the 8B version, but it didn't feel like a true SLM in terms of consumer-hardware-efficiency and size, so I'm downgrading to the 4B variant for better adherence to SLM part.&lt;/p&gt; &lt;p&gt;My main concern is my dataset: It's high-quality but small, with only 700-800 &lt;code&gt;{prompt,completion}&lt;/code&gt; pairs. Some pairs are distilled from larger LLMs, while others come from real code snippets paired with synthetically generated prompts. The data is straightforward (no chain-of-thought reasoning) but it includes potential noise: like non-code elements in code files (placeholders, plain text, or image paths). I want to train the model effectively so it performs well on my use case without picking up this noise or overfitting to the limited examples&lt;/p&gt; &lt;p&gt;For context I'm currently training on Google Colab with an A100 GPU. Here's the configuration I'm using, based on recommendations from Reddit threads and Unsloth docs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model = FastLanguageModel.get_peft_model( model, r=64, lora_alpha=128, lora_dropout=0.05, target_modules=[ &amp;quot;q_proj&amp;quot;, &amp;quot;k_proj&amp;quot;, &amp;quot;v_proj&amp;quot;, &amp;quot;o_proj&amp;quot;, # Self-attention &amp;quot;gate_proj&amp;quot;, # MLP gate for code generation patterns ], bias=&amp;quot;none&amp;quot;, use_gradient_checkpointing=&amp;quot;unsloth&amp;quot;, random_state=3407, use_rslora=False, loftq_config=None, ) training_args = SFTConfig( output_dir=&amp;quot;./qwen-8b-a100&amp;quot;, per_device_train_batch_size=16, gradient_accumulation_steps=2, per_device_eval_batch_size=16, num_train_epochs=3, max_steps=-1, # Use epochs (not max_steps) learning_rate=2e-4, lr_scheduler_type=&amp;quot;cosine&amp;quot;, warmup_ratio=0.05, # 5% warmup optim=&amp;quot;adamw_8bit&amp;quot;, # Memory efficient, works well with LoRA weight_decay=0.01, # Light regularization fp16=False, # Don't use FP16 on A100 bf16=True, # A100 has native BF16 support - MUCH better! tf32=True, # Enable TensorFloat-32 for even faster matmuls dataloader_num_workers=4, # Parallel data loading dataloader_pin_memory=True, # Faster GPU transfers logging_steps=5, eval_strategy=&amp;quot;steps&amp;quot;, eval_steps=10, save_strategy=&amp;quot;steps&amp;quot;, save_steps=10, # Match eval_steps save_total_limit=3, # Keep 3 best load_best_model_at_end=True, metric_for_best_model=&amp;quot;eval_loss&amp;quot;, greater_is_better=False, packing=True, max_seq_length=4096, seed=3407, report_to=&amp;quot;none&amp;quot;, dataset_text_field=&amp;quot;text&amp;quot;, ) trainer = SFTTrainer( model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset_formatted, eval_dataset=val_dataset_formatted, ) # Using Unsloth's gradient accumulation fix from unsloth import unsloth_train trainer_stats = unsloth_train(trainer) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm fairly new to fine-tuning (about 60% VibeCoding; 40% reading docs) and the results so far aren't great. The model underperforms on my tasks - The 8B one.&lt;/p&gt; &lt;p&gt;So I'm reaching out to folks who've worked with Qwen models: What configs have worked well for you, especially for small datasets and code generation? Any tips on preventing overfitting? Are there must-read docs or guides to get started properly?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dyeusyt"&gt; /u/dyeusyt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfasf/finetuning_qwen_4b_for_niche_code_generation_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfasf/finetuning_qwen_4b_for_niche_code_generation_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfasf/finetuning_qwen_4b_for_niche_code_generation_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T07:13:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbhksy</id>
    <title>Google Open-Sources NPU IP, Synaptics Implements It</title>
    <updated>2026-02-22T09:31:16+00:00</updated>
    <author>
      <name>/u/Dontdoitagain69</name>
      <uri>https://old.reddit.com/user/Dontdoitagain69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.eetimes.com/google-open-sources-npu-ip-synaptics-implements-it/"&gt;Google Open-Sources NPU IP, Synaptics Implements It - EE Times&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dontdoitagain69"&gt; /u/Dontdoitagain69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhksy/google_opensources_npu_ip_synaptics_implements_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhksy/google_opensources_npu_ip_synaptics_implements_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhksy/google_opensources_npu_ip_synaptics_implements_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T09:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawge5</id>
    <title>40,000+ AI Agents Exposed to the Internet with Full System Access</title>
    <updated>2026-02-21T17:07:50+00:00</updated>
    <author>
      <name>/u/Monterey-Jack</name>
      <uri>https://old.reddit.com/user/Monterey-Jack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"&gt; &lt;img alt="40,000+ AI Agents Exposed to the Internet with Full System Access" src="https://external-preview.redd.it/QJge18zM6lp5gsWJUdMOifSYjcNp_r7jcsM3Yu8BUUo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03267a48c2b49bc6f4cb6f80e3fcb85dc7645091" title="40,000+ AI Agents Exposed to the Internet with Full System Access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Monterey-Jack"&gt; /u/Monterey-Jack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://threatroad.substack.com/p/40000-ai-agents-exposed-to-the-internet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbjtyu</id>
    <title>How to run Qwen Code Locally with Qwen3-coder-next on LM Studio on MAC.</title>
    <updated>2026-02-22T11:45:52+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had trouble setting this up while LM Studio as the server. Maybe you already know this, but here it is anyway: you need to create your settings.json using anthropic not openai type. And then it works in LM Studio. All of it!&lt;br /&gt; I'm running it on LM Studio on MAC Ultra 128GB in MLX 8bit across local network with maxed out context. Most of the time is spent in prompt processing. It connects to web, see my files, write things.... Tomorrow I'll trow some code at it and see how well it can understand coding.....&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you have any tips how to make it faster, better, etc, let me know. This is exciting!&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Just as I'm out of Claude code tokens till Thursday...&lt;/p&gt; &lt;p&gt;Note: use ONLY the official &lt;a href="https://github.com/QwenLM/qwen-code"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br /&gt; Some people were posting here some vibecoded repos - DON'T USE THAT. Seriously. Nobody is checking vibecoded code. Soon vibe coding will be a swearword. Mark my words...&lt;/p&gt; &lt;p&gt;Here is my settings.json to get you started if you are as new as me to local coding agent. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;modelProviders&amp;quot;: { &amp;quot;anthropic&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;qwen/qwen3-coder-next&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;qwen/qwen3-coder-next&amp;quot;, &amp;quot;baseUrl&amp;quot;: &amp;quot;http://192.168.1.100:1234&amp;quot;, &amp;quot;envKey&amp;quot;: &amp;quot;OPENAI_API_KEY&amp;quot; } ] }, &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;none&amp;quot; }, &amp;quot;security&amp;quot;: { &amp;quot;auth&amp;quot;: { &amp;quot;selectedType&amp;quot;: &amp;quot;anthropic&amp;quot; } }, &amp;quot;model&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;qwen/qwen3-coder-next&amp;quot; }, &amp;quot;$version&amp;quot;: 3, &amp;quot;telemetry&amp;quot;: { &amp;quot;enabled&amp;quot;: false, &amp;quot;target&amp;quot;: &amp;quot;local&amp;quot; } } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjtyu/how_to_run_qwen_code_locally_with_qwen3codernext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjtyu/how_to_run_qwen_code_locally_with_qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjtyu/how_to_run_qwen_code_locally_with_qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T11:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbhgcv</id>
    <title>smolcluster: Educational library to cluster your everyday devices to train/inference LLMs</title>
    <updated>2026-02-22T09:23:38+00:00</updated>
    <author>
      <name>/u/East-Muffin-6472</name>
      <uri>https://old.reddit.com/user/East-Muffin-6472</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past month, I've been working on something educational for the community on concepts related to distributed systems, particularly for training LLMs!&lt;/p&gt; &lt;p&gt;I was amazed by the work done by people at @/exolabs where they provide amazing software for connecting Mac minis/studios together to run inference on huge models!&lt;/p&gt; &lt;p&gt;I thought of doing the same, but to learn the concepts from the ground up—networking, OS, and distributed systems—I decided to reimplement popular algorithms like Data/Model Parallelism, FSDP, and EDP, all from scratch using only Python's socket library.&lt;/p&gt; &lt;p&gt;So, I made &lt;a href="https://www.smolcluster.com"&gt;smolcluster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;An educational, distributed learning library for training and inference of neural nets on heterogeneous hardware!&lt;/p&gt; &lt;p&gt;This is primarily meant for those who want to understand various distributed training algorithms in a simple manner, as single-page Python files.&lt;/p&gt; &lt;p&gt;Current implementations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Elastic Distributed Parallelism (EDP)&lt;/li&gt; &lt;li&gt;Synchronous Parameter Server (SyncPS)&lt;/li&gt; &lt;li&gt;Fully Sharded Data Parallelism (FSDP)&lt;/li&gt; &lt;li&gt;Standard Data Parallelism (DP)&lt;/li&gt; &lt;li&gt;Model Parallelism (MP)&lt;/li&gt; &lt;li&gt;Pipeline Parallelism (PP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently under development and cleaning up the codebase is being done. &lt;/p&gt; &lt;p&gt;Tested on the a cluster of Mac minis, raspberry 4/5, 4050 GPU and Jetson Orin Nano!&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/YuvrajSingh-mist/smolcluster/tree/master"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Perfect for students, researchers, or anyone curious about how distributed training actually works under the hood!&lt;/p&gt; &lt;p&gt;Would love to get your feedback!&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Muffin-6472"&gt; /u/East-Muffin-6472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhgcv/smolcluster_educational_library_to_cluster_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhgcv/smolcluster_educational_library_to_cluster_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhgcv/smolcluster_educational_library_to_cluster_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T09:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ray0vz</id>
    <title>CXMT has been offering DDR4 chips at about half the prevailing market rate</title>
    <updated>2026-02-21T18:07:33+00:00</updated>
    <author>
      <name>/u/johnnyApplePRNG</name>
      <uri>https://old.reddit.com/user/johnnyApplePRNG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"&gt; &lt;img alt="CXMT has been offering DDR4 chips at about half the prevailing market rate" src="https://external-preview.redd.it/0K-nyzO4raoSh4Q6Gk6oShuWqJIJ5QWuThVMJGt1MKU.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad4ecef7d00fc6d2fefa3cec8972e26294886527" title="CXMT has been offering DDR4 chips at about half the prevailing market rate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnyApplePRNG"&gt; /u/johnnyApplePRNG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.koreaherald.com/article/10679206"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T18:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb61og</id>
    <title>Nanbeige 4.1 is the best small LLM, it crush qwen 4b</title>
    <updated>2026-02-21T23:32:59+00:00</updated>
    <author>
      <name>/u/Individual-Source618</name>
      <uri>https://old.reddit.com/user/Individual-Source618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Self-explenatory, try it its insane if you give him enough room to think. Its my go to local llm now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Source618"&gt; /u/Individual-Source618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T23:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb270r</id>
    <title>PSA on public agentic tools and the speed they are shipping updates: recent Cline release had a package injected</title>
    <updated>2026-02-21T20:52:57+00:00</updated>
    <author>
      <name>/u/bakawolf123</name>
      <uri>https://old.reddit.com/user/bakawolf123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may remember a post about sloppy OpenCode commit a week ago or so, unsurprisingly others are embracing vibe coding speed and sloppiness as well.&lt;/p&gt; &lt;p&gt;I've randomly stumbled upon&lt;br /&gt; &lt;a href="https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/"&gt;https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/&lt;/a&gt; apparently a recent Cline release had OpenClaw installer injected Their plugin in VSCode has some 3M installs, god knows how many standalone CLI. Then you see posts about 40k OpenClaw agents exposed globally. &lt;/p&gt; &lt;p&gt;Really wish there was more scrutiny involved by the teams developing new tools but everyone is just shipping first, then thinking about it. So at the very least make sure your VSCode extensions for are not on auto-update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bakawolf123"&gt; /u/bakawolf123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T20:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdsds</id>
    <title>Best Model for single 3090 in 2026?</title>
    <updated>2026-02-22T05:47:26+00:00</updated>
    <author>
      <name>/u/myusuf3</name>
      <uri>https://old.reddit.com/user/myusuf3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running a single RTX 3090 (24GB VRAM) and looking for the best overall model in 2026 for coding + reasoning.&lt;/p&gt; &lt;p&gt;Main priorities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong code generation (Go/TypeScript)&lt;/li&gt; &lt;li&gt;Good reasoning depth&lt;/li&gt; &lt;li&gt;Runs comfortably in 24GB (quantized is fine)&lt;/li&gt; &lt;li&gt;Decent latency on local inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you all running on a single 3090 right now? Qwen? DeepSeek? Something else? Would love specific model names + quant setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myusuf3"&gt; /u/myusuf3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T05:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbbmcl</id>
    <title>Ouro 2.6B GGUFs are up — Q8_0 and Q4_K_M | Release notes + known limitations inside</title>
    <updated>2026-02-22T03:53:12+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;GGUFs are live on HuggingFace: https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed Q8_0 (2.7GB) and Q4_K_M (1.6GB) — works in LM Studio, Ollama, llama.cpp. --- ## What Ouro actually is (quick recap) Ouro is a looped inference model — instead of running the transformer once per token, it passes the output back into itself for multiple reasoning iterations before committing. The &amp;quot;thinking&amp;quot; you see in the output is real: it's the model working through loops before settling on an answer. Full writeup in the original post. --- ## ⚠️ Release Notes — What the GGUF does and doesn't include **GGUF format is standard Llama architecture.** Ouro has three custom architectural features that llama.cpp doesn't support. Here's exactly what happens to each: ### 1. Early Exit Gate (skipped) Ouro has an `early_exit_gate` (weight + bias) — a learned mechanism that lets the model decide mid-sequence whether it has &amp;quot;thought enough&amp;quot; and can exit the loop early. **In the GGUF:** This tensor is skipped entirely. The model runs all layers every pass — no early exit. This means the GGUF is slightly *more* compute than the original per loop, but also means it won't short-circuit on hard problems. ### 2. TL2 — Second Layer Norms (skipped) Each transformer block in Ouro has two layer norms instead of one: - `input_layernorm` (TL1) — standard, kept ✅ - `input_layernorm_2` (TL2) — Ouro's second norm pass, skipped ❌ - `post_attention_layernorm` (TL1) — standard, kept ✅ - `post_attention_layernorm_2` (TL2) — skipped ❌ These are present across all 48 layers. The TL2 norms appear to act as a &amp;quot;re-centering&amp;quot; step between loop iterations. Skipping them means the GGUF doesn't re-normalize between passes the way the full model does. **Practical effect:** The GGUF reasoning is still good — the base weights carry the learned behavior. But if you notice the thinking chains being slightly less structured than the HuggingFace original, this is why. ### 3. Python Looping / Inference Wrapper (not in any GGUF) The looping itself — passing output back as input for N iterations — is implemented in Python at the inference layer, not baked into the weights. **No GGUF can include this** because it's control flow, not a tensor. The GGUF runs one pass per token like any standard model. What you get is essentially the *distilled reasoning capability* that Ouro developed through loop training — the model learned to think in its weights, even if the runtime loop isn't there. For the full looped experience, use the original safetensors on HuggingFace with the inference script. --- ## What still works great - The thinking style and extended reasoning — very much present - The chattiness and self-correction behavior - Chat template (ChatML / `&amp;lt;|im_start|&amp;gt;` `&amp;lt;|im_end|&amp;gt;`) works out of the box - Q8_0 has minimal quality loss over F16; Q4_K_M is solid for RAM-constrained setups --- ## Files | File | Size | Use case | |------|------|----------| | `ouro-2.6b-q8_0.gguf` | 2.7GB | Best quality, ~3GB VRAM | | `ouro-2.6b-q4_k_m.gguf` | 1.6GB | Fastest, ~2GB VRAM | --- Happy to answer questions about the architecture, the conversion process, or what the looping actually does. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T03:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb4luf</id>
    <title>O-TITANS: Orthogonal LoRAs for Gemma 3 using Google's TITANS memory architecture</title>
    <updated>2026-02-21T22:32:47+00:00</updated>
    <author>
      <name>/u/Polymorphic-X</name>
      <uri>https://old.reddit.com/user/Polymorphic-X</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been working on a project I call &lt;strong&gt;O-TITANS&lt;/strong&gt; (Orthogonal Tensors for Independent Task Alignment). It's an Orthogonal LoRA approach specifically for Gemma 3 that incorporates the Google TITANS memory architecture.&lt;br /&gt; It was inspired by a project by ffurfaro on HF called &amp;quot;TPTT&amp;quot; that I just couldn't get to work.&lt;/p&gt; &lt;p&gt;I'm building this to wrap into my next project: &lt;strong&gt;MoOLE-T (Mixture of Orthogonal LoRA Experts - Titans)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal of MoOLE-T is to use a smaller 8B router to select one or more O-LoRAs to pass inference through simultaneously. The output will then get translated and de-conflicted at an &amp;quot;exit node&amp;quot; (a larger 20B-80B model). Theoretically, this creates a beefed-up MoE with specific skills like a tool belt. This approach should punch way above its weight class while needing only a fraction of the VRAM footprint. The best part? It's scalable to a stupid degree, since O-Loras don't interfere directly and can be multi-slotted. You could train 100+ O-LoRAs on individual skills and have a toolbelt of capabilities without bloating a base model to hundreds of billions of parameters.&lt;/p&gt; &lt;p&gt;Still working on the MoOLE-T polyswarm idea, but I'll do another post whenever that gets finished.&lt;/p&gt; &lt;p&gt;I just finished training an example &lt;code&gt;.pt&lt;/code&gt; file on Open-Platypus using mlabonne's Gemma3-12b-it-abliterated model as a base. It's on my hugginface if you want to test the non-interference claims yourselves.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face (O-TITANS Gemma 3 Adapters):&lt;/strong&gt; &lt;a href="https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/"&gt;https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to feedback and additional ideas. This is all an attempt to try and approach human-esque parallel skill processing and selection without absurd compute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Polymorphic-X"&gt; /u/Polymorphic-X &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T22:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rblce7</id>
    <title>I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful</title>
    <updated>2026-02-22T13:03:49+00:00</updated>
    <author>
      <name>/u/Weird_Search_4723</name>
      <uri>https://old.reddit.com/user/Weird_Search_4723</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt; &lt;img alt="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" src="https://external-preview.redd.it/NWtrYWtuYXZuMWxnMexVgBFEBEtAfoKpFzO1VgJV4m4gRx-YBoBnOCuCCbAU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f4bb616205fb72d1541634b6985338275c23ac3" title="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is Kon telling you about it's own repo, using glm-4.7-flash-q4 running locally on my i7-14700F × 28, 64GB RAM, 24GB VRAM (RTX 3090) – video is sped up 2x&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;github: &lt;a href="https://github.com/kuutsav/kon"&gt;https://github.com/kuutsav/kon&lt;/a&gt;&lt;br /&gt; pypi: &lt;a href="https://pypi.org/project/kon-coding-agent/"&gt;https://pypi.org/project/kon-coding-agent/&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The pitch (in the readme as well):&lt;/p&gt; &lt;p&gt;It has a tiny harness: about &lt;strong&gt;215 tokens&lt;/strong&gt; for the system prompt and around &lt;strong&gt;600 tokens&lt;/strong&gt; for tool definitions – so under 1k tokens before conversation context.&lt;/p&gt; &lt;p&gt;At the time of writing this README (22 Feb 2026), this repo has 112 files and is easy to understand in a weekend. Here’s a rough file-count comparison against a couple of popular OSS coding agents:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ fd . | cut -d/ -f1 | sort | uniq -c | sort -rn 4107 opencode 740 pi-mono 108 kon &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Others are of course more mature, support more models, include broader test coverage, and cover more surfaces. But if you want a truly minimal coding agent with batteries included – something you can understand, fork, and extend quickly – Kon might be interesting.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;It takes lots of inspiration from &lt;a href="https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent"&gt;pi-coding-agent&lt;/a&gt;, see the &lt;a href="https://github.com/kuutsav/kon?tab=readme-ov-file#acknowledgements"&gt;acknowledgements&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit 1: this is a re-post, deleted the last one (missed to select video type when creating the post)&lt;br /&gt; Edit 2: more about the model that was running in the demo and the config: &lt;a href="https://github.com/kuutsav/kon/blob/main/LOCAL.md"&gt;https://github.com/kuutsav/kon/blob/main/LOCAL.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Search_4723"&gt; /u/Weird_Search_4723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jf0xcw9vn1lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T13:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbfh1y</id>
    <title>dyslexia and ADHD in the coding community</title>
    <updated>2026-02-22T07:23:34+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my third post on my first Reddit account. Here's why that took so long.&lt;/p&gt; &lt;p&gt;I have dyslexia and ADHD. I've been lurking in communities like this one for years -- reading everything, learning everything -- but never posting. Not because I had nothing to contribute. Because I was scared of what would happen when people saw how I write.&lt;/p&gt; &lt;p&gt;People with dyslexia and ADHD don't write the way the internet expects. The spelling is off. The punctuation is wrong. The sentences don't flow right. And the internet has never been kind about that. We get called stupid. We get told our ideas don't matter because the package they came in looked messy. So we lurk. We learn. We do real work quietly and never share it because the cost of being mocked is too high.&lt;/p&gt; &lt;p&gt;I use AI to help me write. Not to generate ideas -- the ideas are mine. Not to do the work -- I did the work. To help me communicate in a way that doesn't get me dismissed before anyone reads what I actually built.&lt;/p&gt; &lt;p&gt;Yesterday I shipped the first working GGUF quantization of Ouro -- ByteDance's recurrent thinking model. I figured out the tensor mapping, the layer norm mismatch, the early exit gate skip. That was me. And the first thing someone did was question whether I was human.&lt;/p&gt; &lt;p&gt;I'm posting this because I know I'm not the only one. There are people in this community right now with real knowledge, real skills, real contributions -- who won't post because they're afraid of exactly what happened to me today.&lt;/p&gt; &lt;p&gt;You belong here. Your ideas belong here. How you write doesn't determine what you know.&lt;/p&gt; &lt;p&gt;This was my first post. It won't be my last.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T07:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawoe4</id>
    <title>PSA: The software “Shade” is a fraudulent, plagiarized copy of Heretic</title>
    <updated>2026-02-21T17:16:21+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three days ago, the following repository was published, which its “creator” has been aggressively promoting on various channels since then:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/assemsabry/shade"&gt;https://github.com/assemsabry/shade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire source code in the repository is plagiarized from Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), with only the project name and the copyright notice replaced, claiming “original authorship” of everything. The repository does not acknowledge Heretic as its source, and has erased the commit history and the names of all Heretic contributors.&lt;/p&gt; &lt;p&gt;I and several others have called the repository owner out, but he has deleted all issues and tried to cover up his wrongdoing by adding some bogus “additional features” using an AI agent. A quick look at the source files, however, reveals that they are still 95% identical to Heretic’s code. In some cases, only the copyright notice was replaced.&lt;/p&gt; &lt;p&gt;**I can only assume that the ultimate goal is to push malware of some sort, and strongly advise people to stay clear of this plagiarized repository.**&lt;/p&gt; &lt;p&gt;This is one of several incidents where malicious actors tried to profit from Heretic’s surging popularity during the past days, when it reached #1 on the GitHub trending chart and was posted in various social feeds that cater to scammers.&lt;/p&gt; &lt;p&gt;Please also see &lt;a href="https://github.com/p-e-w/heretic/issues/167"&gt;https://github.com/p-e-w/heretic/issues/167&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m doing everything in my power to keep Heretic clean and available to everyone. Thank you for your encouragement in the past few months, it means the world to me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbafs8</id>
    <title>I Trained a Language Model on CPU for 40 Hours - It Beat the GPU Baseline</title>
    <updated>2026-02-22T02:54:39+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who have been following this project, you may recall FlashLM v3, then v4 &amp;quot;Bolt&amp;quot;, and v5.2 &amp;quot;Nova-Ignition&amp;quot;. I am pleased to announce that FlashLM v5 &amp;quot;Thunderbolt&amp;quot; is now complete.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Final BPC&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Parameters&lt;/td&gt; &lt;td align="left"&gt;29.7M (26.5M ternary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Time&lt;/td&gt; &lt;td align="left"&gt;~40 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7950X3D&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;FlashLM v5 achieves a validation perplexity of 1.36, which beats the TinyStories-1M baseline (PPL 1.59). This represents the first instance of a CPU-trained model beating this baseline.&lt;/p&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;p&gt;FlashLM v5 utilizes ParallelGatedRecurrence, a MatMul-free architecture featuring:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BitLinear with ternary weights {-1, 0, +1}&lt;/li&gt; &lt;li&gt;Parallel gated recurrence with learned decay gates&lt;/li&gt; &lt;li&gt;No matrix multiplications in the forward pass&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Parameters: 29,750,784 Ternary: 26,542,080 (89%) Float: 3,208,704 (11%) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Acknowledgments&lt;/h1&gt; &lt;p&gt;I would like to thank arki05 for providing the AMD Ryzen 7950X3D used for training. Without this contribution, the project would not have been possible.&lt;/p&gt; &lt;h1&gt;Generation Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Version&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;BPC&lt;/th&gt; &lt;th align="left"&gt;Output Quality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;v4 &amp;quot;Bolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;15.05&lt;/td&gt; &lt;td align="left"&gt;0.88&lt;/td&gt; &lt;td align="left"&gt;Short, repetitive&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5.2 &amp;quot;Nova-Ignition&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;10.56&lt;/td&gt; &lt;td align="left"&gt;0.78&lt;/td&gt; &lt;td align="left"&gt;Better coherence&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5 &amp;quot;Thunderbolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;td align="left"&gt;Significantly better&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v5 demonstrates improved cohesive storytelling compared to v4 and v5.2&lt;/li&gt; &lt;li&gt;v5 shows better vocabulary diversity and grammar&lt;/li&gt; &lt;li&gt;BPC improved from 0.88 (v4) to 0.44 (v5), representing a 2x improvement&lt;/li&gt; &lt;li&gt;PPL improved from 15.05 (v4) to 1.36 (v5), representing an 11x improvement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Samples&lt;/h1&gt; &lt;p&gt;Prompt: &amp;quot;Once upon a time, there was a brave girl named Lucy.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time, there was a brave girl named Lucy. her big tiny looked door, and she wanted. Lucy loved to creative things. She would find toy when, while small laughing, when she thought. She would be friends all day.One day, Lucy found her toy saw a little hole. Lucy was very happy. She wanted to see who was mean. The little hole was not alone anymore. When Lucy was done playing, she saw the little...&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/changcheng967/flashlm-v5-demo"&gt;https://huggingface.co/spaces/changcheng967/flashlm-v5-demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model Card: &lt;a href="https://huggingface.co/changcheng967/flashlm-v5-thunderbolt"&gt;https://huggingface.co/changcheng967/flashlm-v5-thunderbolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/changcheng967/FlashLM"&gt;https://github.com/changcheng967/FlashLM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Future Directions&lt;/h1&gt; &lt;p&gt;FlashLM v5 concludes the v5 series. Future work includes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;FlashLM v6 - Continuing to validate the ParallelGatedRecurrence architecture&lt;/li&gt; &lt;li&gt;Nano-Coder (NC series) - Applying FlashLM techniques to code generation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T02:54:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbculq</id>
    <title>Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)</title>
    <updated>2026-02-22T04:56:59+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt; &lt;img alt="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" src="https://external-preview.redd.it/6QqGCIHe3v1WQBe6_gTslJhJpyRq4mX4jqVDYTY6xG0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=504511a0ed53dd20492f3b96504f6d5f1655bc7b" title="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://discrepancyreport.com/lawyer-says-google-shut-down-his-gmail-voice-and-photos-after-notebooklm-upload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T04:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbio4h</id>
    <title>Has anyone else tried IQ2 quantization? I'm genuinely shocked by the quality</title>
    <updated>2026-02-22T10:37:47+00:00</updated>
    <author>
      <name>/u/Any-Chipmunk5480</name>
      <uri>https://old.reddit.com/user/Any-Chipmunk5480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've always used GGUF and never went below Q4_K_M because I assumed anything lower would be garbage. Today I decided to try UD-IQ2_XXS on Qwen3-30B-A3B (10.3 GB) and I'm honestly shocked. First off 100 TPS on my RX 9060 XT 16GB, up from 20 TPS on Q4_K_M. 5x speedup with 20K+ context, fully offloaded to GPU. But the real surprise is the quality. I had Claude Opus 4.6 generate progressively harder questions to test it chemistry, math, physics, relativity, deep academic topics. At high school and university level, I couldn't find any meaningful difference between IQ2 and Q4. The only noticeable quality drop was on really niche academic stuff (Gödel's Incompleteness Theorem level), and even there it scored 81/100 vs Q4's 92. The funniest part on a graph analysis question, my 10GB local IQ2 model got the correct answer while both Claude Opus 4.6 and Sonnet 4.6 misread the graph and got it wrong. Has anyone else had similar experiences with ultra-low quants? Why is this not that hyped? Setup: RX 9060 XT 16GB / llama.cpp / Vulkan / Qwen3-30B-A3B UD-IQ2_XXS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Chipmunk5480"&gt; /u/Any-Chipmunk5480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T10:37:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1raq23i</id>
    <title>they have Karpathy, we are doomed ;)</title>
    <updated>2026-02-21T12:34:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt; &lt;img alt="they have Karpathy, we are doomed ;)" src="https://preview.redd.it/ergzi9d1eukg1.png?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=2005c28094bfd489a487151bba9f5c550c22c55b" title="they have Karpathy, we are doomed ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(added second image for the context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raq23i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T12:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb2j5c</id>
    <title>Favourite niche usecases?</title>
    <updated>2026-02-21T21:06:34+00:00</updated>
    <author>
      <name>/u/Figai</name>
      <uri>https://old.reddit.com/user/Figai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt; &lt;img alt="Favourite niche usecases?" src="https://preview.redd.it/o4l2ankhxwkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7201facadd4e9d14e1aac7efef2133d85d346f7" title="Favourite niche usecases?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Figai"&gt; /u/Figai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4l2ankhxwkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T21:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbjxpv</id>
    <title>I think openclaw is OVERHYPED. Just use skills</title>
    <updated>2026-02-22T11:51:38+00:00</updated>
    <author>
      <name>/u/Deep_Traffic_7873</name>
      <uri>https://old.reddit.com/user/Deep_Traffic_7873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think openclaw is useful, loop, memory, agents, integrations, but after a week a testing, honestly I don't need it much.&lt;/p&gt; &lt;p&gt;- memory, is nice. But I prefere to have &amp;quot;manual memory&amp;quot;. Prompt: Ok, write what yout learnt in &amp;quot;superreporttrending-skill&amp;quot;. Automatic memory often pollute the context of info you don't care.&lt;/p&gt; &lt;p&gt;- cron. Useful but I already use other tools for that and I can always recall a skill whenever i want. I don't need everyday at 8:00AM, i prefere recall it when i want with up to date data&lt;/p&gt; &lt;p&gt;Conclusion: for me &amp;quot;opencode web&amp;quot; is a much superior option, but much of the &amp;quot;intelligence&amp;quot; and value is the skills that you develop or you integrate, not in the runner itself, what do you think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Traffic_7873"&gt; /u/Deep_Traffic_7873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T11:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbkeea</id>
    <title>Which one are you waiting for more: 9B or 35B?</title>
    <updated>2026-02-22T12:15:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt; &lt;img alt="Which one are you waiting for more: 9B or 35B?" src="https://preview.redd.it/jyvany3jf1lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f667e97854acf566b7f6d1d56e9c09e17f5a8ee8" title="Which one are you waiting for more: 9B or 35B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jyvany3jf1lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T12:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
