<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-19T12:53:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ppzof4</id>
    <title>LatitudeGames/Hearthfire-24B ¬∑ Hugging Face</title>
    <updated>2025-12-18T19:24:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt; &lt;img alt="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" src="https://external-preview.redd.it/A3gGg_h4D053EFPLZSslW4oGkfGx4Yyo44cLXCFOpgw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af804b5dc2799b163e1ddae03ccbee1392cf7d39" title="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hearthfire is a narrative longform writing model designed to embrace the quiet moments between the chaos. While most roleplay models are trained to relentlessly drive the plot forward with high-stakes action and constant external pressure, Hearthfire is tuned to appreciate atmosphere, introspection, and the slow burn of a scene.&lt;/p&gt; &lt;p&gt;It prioritizes vibes over velocity. It is comfortable with silence. It will not force a goblin attack just because the conversation lulled.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LatitudeGames/Hearthfire-24B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq45po</id>
    <title>New AI Dungeon Model: Hearthfire 24B</title>
    <updated>2025-12-18T22:24:22+00:00</updated>
    <author>
      <name>/u/NottKolby</name>
      <uri>https://old.reddit.com/user/NottKolby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today AI Dungeon open sourced a new narrative roleplay model!&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LatitudeGames/Hearthfire-24B"&gt;Hearthfire 24B&lt;/a&gt;&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;Hearthfire is our new Mistral Small 3.2 finetune, and it's the lo-fi hip hop beats of AI storytelling. Built for slice-of-life moments, atmospheric scenes, and narratives where the stakes are personal rather than apocalyptic. It won't rush you toward the next plot point. It's happy to linger.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NottKolby"&gt; /u/NottKolby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T22:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppt1xb</id>
    <title>Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction</title>
    <updated>2025-12-18T15:05:22+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt; &lt;img alt="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" src="https://preview.redd.it/go7lager9z7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96b4f63fa1cdd2136e6c82f35c609cc6cc1ead9c" title="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/facebook/map-anything-v1"&gt;https://huggingface.co/facebook/map-anything-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports 12+ tasks like multi-view stereo and SfM in a single feed-forward pass&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go7lager9z7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqd933</id>
    <title>Fine-tuning Gemma3 1B to create 3D objects</title>
    <updated>2025-12-19T05:39:59+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 6 weeks generating synthetic datasets of 3d objects and finetuned Gemma3 1B on it.&lt;/p&gt; &lt;p&gt;Turned out pretty good lol.&lt;/p&gt; &lt;p&gt;Anyway I made web app out of it, lmk what you think!&lt;/p&gt; &lt;p&gt;If anyone is interested, I can write a blog post about it and share.&lt;/p&gt; &lt;p&gt;Good night!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cadmonkey.web.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd933/finetuning_gemma3_1b_to_create_3d_objects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd933/finetuning_gemma3_1b_to_create_3d_objects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T05:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqf27c</id>
    <title>Is gpt oss:120b still the best at its size?</title>
    <updated>2025-12-19T07:27:00+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in math and coding.. is there still no model that is clearly stronger at 120b or less?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T07:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqi0hf</id>
    <title>Installed an AMD Radeon R9700 32GB GPU in our Nexus AI Station and tested local LLMs</title>
    <updated>2025-12-19T10:39:31+00:00</updated>
    <author>
      <name>/u/Expensive_Chest_2224</name>
      <uri>https://old.reddit.com/user/Expensive_Chest_2224</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqi0hf/installed_an_amd_radeon_r9700_32gb_gpu_in_our/"&gt; &lt;img alt="Installed an AMD Radeon R9700 32GB GPU in our Nexus AI Station and tested local LLMs" src="https://a.thumbs.redditmedia.com/P3HDH6qzhoyDT4EONVl-pJsbzcvErLhaROPqfh8CcK0.jpg" title="Installed an AMD Radeon R9700 32GB GPU in our Nexus AI Station and tested local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just got our hands on an AMD Radeon R9700 32GB AI inference GPU, so naturally the first thing we did was drop it into our Nexus AI Station and see how it handles local LLMs.&lt;/p&gt; &lt;p&gt;After installing the card, we set up Ollama + WebUI, configured inference to run on the AMD GPU, and pulled two models:&lt;/p&gt; &lt;p&gt;Qwen3:32B&lt;/p&gt; &lt;p&gt;DeepSeek-R1:32B&lt;/p&gt; &lt;p&gt;We gave both models the same math problem and let them run side by side. The GPU was fully loaded, steady inference, all running locally ‚Äî no cloud involved.&lt;/p&gt; &lt;p&gt;Interesting part: both models took noticeably different reasoning paths. Curious what others think ‚Äî which approach would you prefer?&lt;/p&gt; &lt;p&gt;We‚Äôll keep sharing more local AI tests as we go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive_Chest_2224"&gt; /u/Expensive_Chest_2224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqi0hf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqi0hf/installed_an_amd_radeon_r9700_32gb_gpu_in_our/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqi0hf/installed_an_amd_radeon_r9700_32gb_gpu_in_our/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T10:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqes8u</id>
    <title>Got tired of slow legacy Whisper. Built a custom stack (Faster-Whisper + Pyannote 4.0) on CUDA 12.8. The alignment is now O(N) and flies. üöÄ</title>
    <updated>2025-12-19T07:09:32+00:00</updated>
    <author>
      <name>/u/Key_Mousse_8034</name>
      <uri>https://old.reddit.com/user/Key_Mousse_8034</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqes8u/got_tired_of_slow_legacy_whisper_built_a_custom/"&gt; &lt;img alt="Got tired of slow legacy Whisper. Built a custom stack (Faster-Whisper + Pyannote 4.0) on CUDA 12.8. The alignment is now O(N) and flies. üöÄ" src="https://preview.redd.it/pggn223q148g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3ec98a08b1862c148864dea3affd4e668859f01" title="Got tired of slow legacy Whisper. Built a custom stack (Faster-Whisper + Pyannote 4.0) on CUDA 12.8. The alignment is now O(N) and flies. üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last few days in absolute &amp;quot;Dependency Hell&amp;quot; trying to modernize my legacy ASR pipeline.&lt;/p&gt; &lt;p&gt;I was running an old WhisperX setup, but it was starting to show its age (abandoned repo, old PyTorch, memory leaks). I decided to rebuild it from scratch using &lt;strong&gt;Faster-Whisper&lt;/strong&gt; (CTranslate2) and the new &lt;strong&gt;Pyannote 4.0.3&lt;/strong&gt; for diarization.&lt;/p&gt; &lt;p&gt;It sounded simple. It was not.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Nightmare:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PyTorch 2.8 + cuDNN 9:&lt;/strong&gt; Pip installs cuDNN 9 inside &lt;code&gt;site-packages&lt;/code&gt;, but the Linux system linker has no clue where it is. Result? Constant Segfaults and &lt;code&gt;Exit Code 52&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API Breaking Changes:&lt;/strong&gt; Pyannote 4.0 changed how it returns annotations (containers instead of objects), which broke my entire alignment logic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dependency Conflicts:&lt;/strong&gt; Trying to make &lt;code&gt;lightning&lt;/code&gt; (new) coexist with libraries expecting &lt;code&gt;pytorch-lightning&lt;/code&gt; (old) inside one Docker container is painful.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Solution (The &amp;quot;Nuclear Option&amp;quot;):&lt;/p&gt; &lt;p&gt;I ended up manually building the environment layer by layer in Docker.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Forced Paths:&lt;/strong&gt; I had to explicitly set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to point deep into the python packages so the system could find the NVIDIA libs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Algorithm Rewrite:&lt;/strong&gt; I rewrote the speaker-to-word alignment algorithm. It used to be quadratic O(N*M), which choked on long audio. I optimized it to a linear scan O(N).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The Result:&lt;/p&gt; &lt;p&gt;The service now processes audio fully (transcription + diarization + alignment) in ~30 seconds for test files that used to take much longer.&lt;/p&gt; &lt;p&gt;Hardware: RTX 4000 Ada.&lt;/p&gt; &lt;p&gt;VRAM usage: ~4GB (huge headroom left).&lt;/p&gt; &lt;p&gt;Attached is the screenshot of the final successful build after 50+ failed attempts. Seeing those green checkmarks felt better than coffee.&lt;/p&gt; &lt;p&gt;Has anyone else dealt with PyTorch 2.8 / cuDNN 9 path issues in Docker recently? That was the hardest part to debug.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Mousse_8034"&gt; /u/Key_Mousse_8034 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pggn223q148g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqes8u/got_tired_of_slow_legacy_whisper_built_a_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqes8u/got_tired_of_slow_legacy_whisper_built_a_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T07:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqdig8</id>
    <title>Some local LLMs running as CPU only</title>
    <updated>2025-12-19T05:54:36+00:00</updated>
    <author>
      <name>/u/_malfeasance_</name>
      <uri>https://old.reddit.com/user/_malfeasance_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqdig8/some_local_llms_running_as_cpu_only/"&gt; &lt;img alt="Some local LLMs running as CPU only" src="https://b.thumbs.redditmedia.com/ReY66zq8r6x7AifcgByGWhcXgcu1sye9Xa_A48b_7JU.jpg" title="Some local LLMs running as CPU only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The results show what you may be able to do if you buy a 2nd hand server without a GPU for around $USD1k as I did. It is interesting but not too practical.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n1llbbmim38g1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b357ce429a86a312860bf0257f0786d742d8eb86"&gt;https://preview.redd.it/n1llbbmim38g1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b357ce429a86a312860bf0257f0786d742d8eb86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alibaba-NLP_Tongyi-DeepResearch is quick but it is not very useful as it struggles to stay in English amongst other faults.&lt;/p&gt; &lt;p&gt;Nemotron from Nvidia is excellent which is somewhat ironic given it is designed with Nvidia hardware in mind. Kimi-K2 is excellent. Results can vary quite a bit depending on the query type. For example, the DeepSeek Speciale listed here took 10 hours and 20 minutes at 0.5 tps to answer a c++ boyer-moore std::string_view build question with a google test kind of query (mainly due to much thinking with &amp;gt;20k tokens). Interesting, but not very practical.&lt;/p&gt; &lt;p&gt;Results were with custom client/server app using an embedded llama.cpp. Standard query used after a warm-up query. 131072 context with 65536 output config where supported.&lt;/p&gt; &lt;p&gt;_____&lt;br /&gt; Revision notes: &lt;br /&gt; Alibaba DeepResearch above is a Q4_K_L quant.&lt;br /&gt; Qwen3-30B-A3B-Instruct-2507-Q4-K_XL runs at 15.7 tps.&lt;/p&gt; &lt;p&gt;Processors: 4 √ó Intel Xeon E7-8867 v4 @ 2.40GHz (144 logical CPUs total: 18 cores/socket, 2 threads/core).&lt;br /&gt; RAM: 2.0 TiB total - 64GB DDR4 ECC DIMMS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_malfeasance_"&gt; /u/_malfeasance_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqdig8/some_local_llms_running_as_cpu_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqdig8/some_local_llms_running_as_cpu_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqdig8/some_local_llms_running_as_cpu_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T05:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwqki</id>
    <title>FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!</title>
    <updated>2025-12-18T17:31:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt; &lt;img alt="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" src="https://external-preview.redd.it/MXBiZjQzZTd4ejdnMYei2aDWEA5WccTd6X2Ceg7tONZcTZmqT6GgxYYEX2jv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c19158fb01b0628ef68c006e673dc09cd2cf081" title="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Google released FunctionGemma, a lightweight (270M), open foundation model built for creating specialized function calling models! To test it out, I built a small game where you use natural language to solve physics simulation puzzles. It runs entirely locally in your browser on WebGPU, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Game: &lt;a href="https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground"&gt;https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground&lt;/a&gt;&lt;br /&gt; - FunctionGemma on Hugging Face: &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k33t7zd7xz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjqqy</id>
    <title>Known Pretraining Tokens for LLMs</title>
    <updated>2025-12-19T12:21:06+00:00</updated>
    <author>
      <name>/u/phree_radical</name>
      <uri>https://old.reddit.com/user/phree_radical</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"&gt; &lt;img alt="Known Pretraining Tokens for LLMs" src="https://preview.redd.it/970lzt7sk58g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b260274c8027bbb342abe4985e434e89f2823c51" title="Known Pretraining Tokens for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretraining compute seems like it doesn't get enough attention, compared to Parameters.&lt;/p&gt; &lt;p&gt;I was working on this spreadsheet a few months ago. If a vendor didn't publish anything about how many pretraining tokens, I left them out. But I'm certain I've missed some important models.&lt;/p&gt; &lt;p&gt;What can we add to this spreadsheet?&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1vKOK0UPUcUBIEf7srkbGfwQVJTx854_a3rCmglU9QuY/"&gt;https://docs.google.com/spreadsheets/d/1vKOK0UPUcUBIEf7srkbGfwQVJTx854_a3rCmglU9QuY/&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Family / Vendor&lt;/th&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Parameters (B)&lt;/th&gt; &lt;th&gt;Pretraining Tokens (T)&lt;/th&gt; &lt;th&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 33B&lt;/td&gt; &lt;td&gt;33&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LlaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 13B&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LlaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 3 8B&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 3 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-1.8B&lt;/td&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-72B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-0.5b&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-1.5b&lt;/td&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-7b&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-72b&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-57B-A14B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;11.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 0.5B&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 1.5B&lt;/td&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 3B&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 32B&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 72B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 0.6B&lt;/td&gt; &lt;td&gt;0.6&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 1.7B&lt;/td&gt; &lt;td&gt;1.7&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 4B&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 8B&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 32B&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td&gt;235&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM&lt;/td&gt; &lt;td&gt;GLM-130B&lt;/td&gt; &lt;td&gt;130&lt;/td&gt; &lt;td&gt;23&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chinchilla&lt;/td&gt; &lt;td&gt;Chinchilla-70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI&lt;/td&gt; &lt;td&gt;GPT-3 (175B)&lt;/td&gt; &lt;td&gt;175&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI&lt;/td&gt; &lt;td&gt;GPT-4 (1.8T)&lt;/td&gt; &lt;td&gt;1800&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;PaLM (540B)&lt;/td&gt; &lt;td&gt;540&lt;/td&gt; &lt;td&gt;0.78&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TII&lt;/td&gt; &lt;td&gt;Falcon-180B&lt;/td&gt; &lt;td&gt;180&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 1 2B&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 1 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 2B&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 9B&lt;/td&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 27B&lt;/td&gt; &lt;td&gt;27&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 1B&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 4B&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 12B&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 27B&lt;/td&gt; &lt;td&gt;27&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-Coder 1.3B&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-Coder 33B&lt;/td&gt; &lt;td&gt;33&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-LLM 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-LLM 67B&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V2&lt;/td&gt; &lt;td&gt;236&lt;/td&gt; &lt;td&gt;8.1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V3&lt;/td&gt; &lt;td&gt;671&lt;/td&gt; &lt;td&gt;14.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V3.1&lt;/td&gt; &lt;td&gt;685&lt;/td&gt; &lt;td&gt;15.6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-1&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;0.054&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-1.5&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;0.15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-2&lt;/td&gt; &lt;td&gt;2.7&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-medium&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;4.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-small&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;4.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-mini&lt;/td&gt; &lt;td&gt;3.8&lt;/td&gt; &lt;td&gt;3.3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-MoE-instruct&lt;/td&gt; &lt;td&gt;42&lt;/td&gt; &lt;td&gt;4.9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-mini-instruct&lt;/td&gt; &lt;td&gt;3.82&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-MoE-instruct&lt;/td&gt; &lt;td&gt;42&lt;/td&gt; &lt;td&gt;4.9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Xiaomi&lt;/td&gt; &lt;td&gt;MiMo-7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-3-8B-Base-4k&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;3.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-4-340B&lt;/td&gt; &lt;td&gt;340&lt;/td&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-4-15B&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ByteDance&lt;/td&gt; &lt;td&gt;Seed-oss&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phree_radical"&gt; /u/phree_radical &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/970lzt7sk58g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:21:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq6h6b</id>
    <title>T5 Gemma Text to Speech</title>
    <updated>2025-12-19T00:04:46+00:00</updated>
    <author>
      <name>/u/ObjectiveOctopus2</name>
      <uri>https://old.reddit.com/user/ObjectiveOctopus2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"&gt; &lt;img alt="T5 Gemma Text to Speech" src="https://external-preview.redd.it/OoYCpcn_PwfmbZl7Fy8iEFdYUosTf1a8HGTxpebEBLY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be91b4470a2fc8d77abd575b339b2d41dce4c231" title="T5 Gemma Text to Speech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma-TTS-2b-2b is a multilingual Text-to-Speech (TTS) model. It utilizes an Encoder-Decoder LLM architecture, supporting English, Chinese, and Japanese. And its üî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObjectiveOctopus2"&gt; /u/ObjectiveOctopus2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T00:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqd7sy</id>
    <title>I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it.</title>
    <updated>2025-12-19T05:37:51+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"&gt; &lt;img alt="I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it." src="https://external-preview.redd.it/ejdpemFoZnloMzhnMWGEd-zNYE7e7CLbHm_rOf9Rp-W6GE7TweEbQZSklBMz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=054b952c38361bd74bb25ed593bcaebaa995ebda" title="I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even though it looks simple. This thing has quite the process behind it. I am using Godot Mono, with LLamaSharp (llama.cpp under the hood) for inferencing. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;I start with Phi-3.5 mini. It rewrites the users query into 4 alternative queries&lt;/li&gt; &lt;li&gt;I take those queries and use Qwen 3 embedding model to pull back vector db results for each one&lt;/li&gt; &lt;li&gt;I then dedupe and run a reranking algorithm to limit the results down to around 10 'hits'&lt;/li&gt; &lt;li&gt;Next up is taking the hits and expanding it to include neighboring 'chunks' in the document&lt;/li&gt; &lt;li&gt;Then I format the chunks neatly&lt;/li&gt; &lt;li&gt;Then I pass the context and user's prompt to Qwen 8B with thinking active for it to answer the users question.&lt;/li&gt; &lt;li&gt;Finally the output is sent back to Phi-3.5 mini to 'extract' the answer out of the thinking model's response and format it for the UI. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There's a lot of checks and looping going on in the background too. Lots of juggling with chat history. But by using these small models, it runs very quickly on VRAM. Because the models are small I can just load and unload per request without the load times being crazy. &lt;/p&gt; &lt;p&gt;I won't say this is perfect. And I haven't taken this process and ran it against any benchmarks. But it's honestly gone ALOT better than I ever anticipated. The quality could even improve more when I implement a &amp;quot;Deep Think&amp;quot; mode next. Which will basically just be an agent setup to loop and pull in more relevant context. &lt;/p&gt; &lt;p&gt;But if there's anything I've learned throughout this process...It's that even small language models can answer questions reliably. As long as you give proper context. Context engineering is the most important piece of the pie. We don't need these 300B plus models for most AI needs.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Offloom is just the name I gave my proof of concept. This thing isn't on the market, and probably never will be. It's my own personal playground for proving out concepts. I enjoy making things look nice. Even for POCs.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h85i48fyh38g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T05:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2uvi</id>
    <title>192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA</title>
    <updated>2025-12-18T21:31:29+00:00</updated>
    <author>
      <name>/u/Sero_x</name>
      <uri>https://old.reddit.com/user/Sero_x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt; &lt;img alt="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" src="https://b.thumbs.redditmedia.com/rqYvfP2xSe7ILLKpKsQzha57H6-7i7Cnwe-N3-UA3RM.jpg" title="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab"&gt;https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I bought and built this 3 months ago, I started with 4x 3090s and really loved the process so got another 4x 3090s&lt;/p&gt; &lt;p&gt;Now I‚Äôm convinced I need double the VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sero_x"&gt; /u/Sero_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqeauj</id>
    <title>Meta is developing a new image and video AI model ‚ÄúMango‚Äù, along with a previously reported ‚ÄúAvocado‚Äù according to WSJ.</title>
    <updated>2025-12-19T06:40:33+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"&gt; &lt;img alt="Meta is developing a new image and video AI model ‚ÄúMango‚Äù, along with a previously reported ‚ÄúAvocado‚Äù according to WSJ." src="https://preview.redd.it/yf9939hiw38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25b7cf3ee9d09bdbbcda23002e194ff41e6d07c0" title="Meta is developing a new image and video AI model ‚ÄúMango‚Äù, along with a previously reported ‚ÄúAvocado‚Äù according to WSJ." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://www.wsj.com/tech/ai/meta-developing-new-ai-image-and-video-model-code-named-mango-16e785c7"&gt;https://www.wsj.com/tech/ai/meta-developing-new-ai-image-and-video-model-code-named-mango-16e785c7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yf9939hiw38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2rx7</id>
    <title>Exo 1.0 is finally out</title>
    <updated>2025-12-18T21:28:19+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt; &lt;img alt="Exo 1.0 is finally out" src="https://preview.redd.it/zxmsw724618g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=912f00f6d6f4874ab451714c731bec0bbc5a59be" title="Exo 1.0 is finally out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can download from &lt;a href="https://exolabs.net/"&gt;https://exolabs.net/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zxmsw724618g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqala0</id>
    <title>MBZUAI releases K2-V2 - 70B fully open model.</title>
    <updated>2025-12-19T03:20:39+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holy frijoles. Has anyone given this a look? Fully open like Olmo 3, but a solid 70B of performance. I‚Äôm not sure why I‚Äôm just hearing about it, but, definitely looking forward to seeing how folks receive it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/"&gt;https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(I searched for other posts on this but didn‚Äôt see anything - let me know if I missed a thread!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T03:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzhtq</id>
    <title>T5Gemma 2: The next generation of encoder-decoder models</title>
    <updated>2025-12-18T19:17:53+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt; &lt;img alt="T5Gemma 2: The next generation of encoder-decoder models" src="https://external-preview.redd.it/_rnSBYMvSInq6EN43nG_cTgBC4Jp6XTPNyUPRgnGKn0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9dbe7f224d36b036fe98650042395413b48e5a4" title="T5Gemma 2: The next generation of encoder-decoder models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input and generating text output, with open weights for three pretrained sizes (270M-270M, 1B-1B, and 4B-4B).&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tied embeddings:&lt;/strong&gt; Embeddings are tied between the encoder and decoder. This significantly reduces the overall parameter count and allowing to pack more active capabilities into the same memory footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Merged attention:&lt;/strong&gt; The decoder uses a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodality:&lt;/strong&gt; T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extended long context:&lt;/strong&gt; Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massively multilingual:&lt;/strong&gt; Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Models - &lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;https://huggingface.co/collections/google/t5gemma-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official Blog post - &lt;a href="https://blog.google/technology/developers/t5gemma-2/"&gt;https://blog.google/technology/developers/t5gemma-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjja2</id>
    <title>Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family.</title>
    <updated>2025-12-19T12:09:34+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt; &lt;img alt="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." src="https://b.thumbs.redditmedia.com/wfpfrO9KgU8CdhDynFFqinaA1_4j8ZSos50KzQEDVTA.jpg" title="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma Scope 2: &lt;a href="https://huggingface.co/google/gemma-scope-2"&gt;https://huggingface.co/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/google/gemma-scope-2"&gt;https://huggingface.co/collections/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Google AI Developers on ùïè: &lt;a href="https://x.com/googleaidevs/status/2001986944687804774"&gt;https://x.com/googleaidevs/status/2001986944687804774&lt;/a&gt;&lt;br /&gt; Blog post: Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior: &lt;a href="https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/"&gt;https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqjja2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppun3v</id>
    <title>Google's Gemma models family</title>
    <updated>2025-12-18T16:09:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt; &lt;img alt="Google's Gemma models family" src="https://preview.redd.it/59w0vja4lz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd2d66ee23d4078bf31aba81cdeecc769669af4" title="Google's Gemma models family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59w0vja4lz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq5k6e</id>
    <title>Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios</title>
    <updated>2025-12-18T23:23:44+00:00</updated>
    <author>
      <name>/u/Competitive_Travel16</name>
      <uri>https://old.reddit.com/user/Competitive_Travel16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt; &lt;img alt="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" src="https://external-preview.redd.it/A_KZLQUNhCh0wGe2hwjJCJ470X6QmuVpXZdzOWccb0U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fdf14dca65c42b501a6a7e33b1acf44e71ac72f" title="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Travel16"&gt; /u/Competitive_Travel16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4l4UWZGxvoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T23:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2ry0</id>
    <title>Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster</title>
    <updated>2025-12-18T21:28:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt; &lt;img alt="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" src="https://preview.redd.it/32z50w1s518g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be2781529b5cacb7d7a84c794d37a156e1bdc798" title="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing llama.cpp RPC vs Exo's new RDMA Tensor setting on a cluster of 4x Mac Studios (2x 512GB and 2x 256GB) that Apple loaned me until Februrary.&lt;/p&gt; &lt;p&gt;Would love to do more testing between now and returning it. A lot of the earlier testing was debugging stuff since the RDMA support was very new for the past few weeks... now that it's somewhat stable I can do more.&lt;/p&gt; &lt;p&gt;The annoying thing is there's nothing nice like llama-bench in Exo, so I can't give as direct comparisons with context sizes, prompt processing speeds, etc. (it takes a lot more fuss to do that, at least).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32z50w1s518g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqfmsr</id>
    <title>Meta releases SAM Audio for audio separation</title>
    <updated>2025-12-19T08:03:38+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"&gt; &lt;img alt="Meta releases SAM Audio for audio separation" src="https://external-preview.redd.it/cHEzMGt1a2YzNDhnMXbShRCjAlPQsamMmoIWTAtR2gquYxttgWY9vfB1L3ZP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a42426e742804c90a902cb380ea41038b48a1027" title="Meta releases SAM Audio for audio separation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;SAM Audio separates target and residual sounds from any audio or audiovisual source‚Äîacross general sound, music, and speech.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://ai.meta.com/samaudio/"&gt;https://ai.meta.com/samaudio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/facebook/sam-audio"&gt;https://huggingface.co/collections/facebook/sam-audio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/sam-audio"&gt;https://github.com/facebookresearch/sam-audio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/en7nfnmf348g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T08:03:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqegcr</id>
    <title>Realist meme of the year!</title>
    <updated>2025-12-19T06:49:54+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt; &lt;img alt="Realist meme of the year!" src="https://preview.redd.it/8oge3a2by38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4697e9a87c50f3f170db7e87eccd27363c505dc" title="Realist meme of the year!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8oge3a2by38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We‚Äôll be answering questions live on Thursday, Dec. 18, from 2-3pm PT. Hope to see you there.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
