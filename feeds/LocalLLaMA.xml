<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-14T10:40:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mpgkap</id>
    <title>Why itâ€™s a mistake to ask chatbots about their mistakes</title>
    <updated>2025-08-13T21:02:24+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;The tendency to ask AI bots to explain themselves reveals widespread misconceptions about how they work.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arstechnica.com/ai/2025/08/why-its-a-mistake-to-ask-chatbots-about-their-mistakes/"&gt;https://arstechnica.com/ai/2025/08/why-its-a-mistake-to-ask-chatbots-about-their-mistakes/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T21:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mprb4d</id>
    <title>Devs: Devstral VS Qwen3-30b/GPT-OSS?</title>
    <updated>2025-08-14T05:04:57+00:00</updated>
    <author>
      <name>/u/mitchins-au</name>
      <uri>https://old.reddit.com/user/mitchins-au</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m just reaching out for anyone with first hand experience in real world coding tasks between the dense devstral small and the light MOE.&lt;/p&gt; &lt;p&gt;I know thereâ€™s benchmarks but real world experience tends to be better. If youâ€™ve played both both whatâ€™s your advice? Mainly python and some JS stuff. &lt;/p&gt; &lt;p&gt;Tooling support would be crucial.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mitchins-au"&gt; /u/mitchins-au &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprb4d/devs_devstral_vs_qwen330bgptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprb4d/devs_devstral_vs_qwen330bgptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mprb4d/devs_devstral_vs_qwen330bgptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T05:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp6it6</id>
    <title>now it can turn your PDFs and docs into clean fine tuning datasets</title>
    <updated>2025-08-13T14:48:26+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt; &lt;img alt="now it can turn your PDFs and docs into clean fine tuning datasets" src="https://external-preview.redd.it/3sG_aaHa7N5A_uKldFg_ckXPZRKSagJ4eq_vlsxxQ-g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b05b86b865816d2d239bd9c679d5afbf3fd0461" title="now it can turn your PDFs and docs into clean fine tuning datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4z271b5usif1.png?width=1812&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4d98143bf7d60e382b53787e3ce6eb6272f8c8"&gt;The flow on how it generates datasets using local resources&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mp6it6/video/hhwtavqwusif1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;repo is here &lt;a href="https://github.com/Datalore-ai/datalore-localgen-cli"&gt;https://github.com/Datalore-ai/datalore-localgen-cli&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a while back I posted here about a terminal tool I made during my internship that could generate fine tuning datasets from real world data using deep research.&lt;br /&gt; after that post, I got quite a few dms and some really thoughtful feedback. thank you to everyone who reached out.&lt;/p&gt; &lt;p&gt;also, it got around 15 stars on GitHub which might be small but it was my first project so I am really happy about it. thanks to everyone who checked it out.&lt;/p&gt; &lt;p&gt;one of the most common requests was if it could work on local resources instead of only going online.&lt;br /&gt; so over the weekend I built a separate version that does exactly that.&lt;/p&gt; &lt;p&gt;you point it to a local file like a pdf, docx, jpg or txt and describe the dataset you want. it extracts the text, finds relevant parts with semantic search, applies your instructions through a generated schema, and outputs the dataset.&lt;/p&gt; &lt;p&gt;I am planning to integrate this into the main tool soon so it can handle both online and offline sources in one workflow.&lt;/p&gt; &lt;p&gt;if you want to see some example datasets it generated, feel free to dm me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp92nc</id>
    <title>Flash Attention massively accelerate gpt-oss-120b inference speed on Apple silicon</title>
    <updated>2025-08-13T16:24:53+00:00</updated>
    <author>
      <name>/u/DaniDubin</name>
      <uri>https://old.reddit.com/user/DaniDubin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share my observation and experience with gpt-oss-120b (unsloth/gpt-oss-120b-GGUF, F16).&lt;br /&gt; I am running it via LM Studio (latest v0.3.23), my hardware config is Mac Studio M4 Max (16c/40g) with 128GB of unified memory. &lt;/p&gt; &lt;p&gt;My main complaint against gpt-oss-120b was its inference speed, once the context window get filled up, it was dropping from 35-40 to 10-15 t/s when the context was around 15K only.&lt;/p&gt; &lt;p&gt;Now I noticed that by default Flash Attention is turned off. Once I turn it on via LM Studio model's configuration, I got ~50t/s with the context window at 15K, instead of the usual &amp;lt;15t/s.&lt;/p&gt; &lt;p&gt;Has anyone else tried to run this model with Flash Attention? Is there any trade-offs in model's accuracy? In my *very* limited testing I didn't notice any. I did not know that it can speed up so much the inference speed. I also noticed that Flash Attention is only available with GGUF quants, not on MLX.&lt;/p&gt; &lt;p&gt;Would like to hear your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaniDubin"&gt; /u/DaniDubin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T16:24:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1moz341</id>
    <title>gpt-oss-120B most intelligent model that fits on an H100 in native precision</title>
    <updated>2025-08-13T08:46:18+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt; &lt;img alt="gpt-oss-120B most intelligent model that fits on an H100 in native precision" src="https://preview.redd.it/4okvse7e2rif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=943876a00ac037e2110c919f54e46c6e6d4303b4" title="gpt-oss-120B most intelligent model that fits on an H100 in native precision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting analysis thread: &lt;a href="https://x.com/artificialanlys/status/1952887733803991070"&gt;https://x.com/artificialanlys/status/1952887733803991070&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4okvse7e2rif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T08:46:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mppqtu</id>
    <title>YAMS: Yet Another Memory System for LLM's</title>
    <updated>2025-08-14T03:41:54+00:00</updated>
    <author>
      <name>/u/blkmanta</name>
      <uri>https://old.reddit.com/user/blkmanta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"&gt; &lt;img alt="YAMS: Yet Another Memory System for LLM's" src="https://external-preview.redd.it/Wdl3okAHvOXaOkYdjPCaNhixUPRpyzUYZxAYiri9Ewg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=365c61553d764b17eeff651fffa6fb624ced2120" title="YAMS: Yet Another Memory System for LLM's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;|| || |Built this for my LLM workflows - needed searchable, persistent memory that wouldn't blow up storage costs. I also wanted to use it locally for my research. It's a content-addressed storage system with block-level deduplication (saves 30-40% on typical codebases). I have integrated the CLI tool into most of my workflows in Zed, Claude Code, and Cursor, and I provide the prompt I'm currently using in the repo. The project is in C++ and the build system is rough around the edges but is tested on macOS and Ubuntu 24.04.|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blkmanta"&gt; /u/blkmanta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trvon/yams"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T03:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpee0x</id>
    <title>GPT OSS 120b 34th on Simple bench, roughly on par with Llama 3.3 70b</title>
    <updated>2025-08-13T19:40:46+00:00</updated>
    <author>
      <name>/u/and_human</name>
      <uri>https://old.reddit.com/user/and_human</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/and_human"&gt; /u/and_human &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://simple-bench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpee0x/gpt_oss_120b_34th_on_simple_bench_roughly_on_par/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpee0x/gpt_oss_120b_34th_on_simple_bench_roughly_on_par/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T19:40:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp4gwl</id>
    <title>Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985</title>
    <updated>2025-08-13T13:28:20+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt; &lt;img alt="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" src="https://external-preview.redd.it/Bvw60PvhPgoef0Ng9Djae_QLUotq8vncLfnhqt8cL74.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=218782160b09032a3d5202434bc6cfb1ccd15a36" title="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/beelink-gtr9-pro-mini-pc-launched-140w-amd-ryzen-ai-max-395-128-gb-dual-10gbe-1985-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T13:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpw8s7</id>
    <title>Updated my setup &lt;3</title>
    <updated>2025-08-14T09:56:28+00:00</updated>
    <author>
      <name>/u/Wooden_Yam1924</name>
      <uri>https://old.reddit.com/user/Wooden_Yam1924</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"&gt; &lt;img alt="Updated my setup &amp;lt;3" src="https://b.thumbs.redditmedia.com/Z8qaY02yyXET_0RFo2p9TACB74TtdXHAWrwwKDQEqZY.jpg" title="Updated my setup &amp;lt;3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just got 2x rtx pro 6000, probably eventually I sell A6000 for the next one, I'm so excited that wanted to share it with someone :D what benchmarks/llms should I run?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pxp6orxnjyif1.jpg?width=4284&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e96a0b89446f43b029db8162f50cf2962d5ab051"&gt;https://preview.redd.it/pxp6orxnjyif1.jpg?width=4284&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e96a0b89446f43b029db8162f50cf2962d5ab051&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden_Yam1924"&gt; /u/Wooden_Yam1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T09:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1j7e</id>
    <title>Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp</title>
    <updated>2025-08-13T11:12:30+00:00</updated>
    <author>
      <name>/u/csixtay</name>
      <uri>https://old.reddit.com/user/csixtay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt; &lt;img alt="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" src="https://preview.redd.it/j7hi9xgjrrif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=328e2e9fc9cd738d0907c1394e77c1ec12b827b3" title="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csixtay"&gt; /u/csixtay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7hi9xgjrrif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmeba</id>
    <title>Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude</title>
    <updated>2025-08-14T01:03:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"&gt; &lt;img alt="Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude" src="https://b.thumbs.redditmedia.com/3X645e074wAZwFAUEsvdDQTEHHMP4k46jGqsiJfXi6w.jpg" title="Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;this post&lt;/a&gt; yesterday, here are the updated results using Q8 of the Jan V1 model with Serper search.&lt;/p&gt; &lt;p&gt;Summaries corresponding to each image:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Jan V1 Q8 with brave search: Actually produces an answer. But it gives the result for 2023.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Jan V1 Q8 with serper: Same result as above. It seems to make the mistake in the first thinking step in initiating the search - &amp;quot;Let me phrase the query as &amp;quot;US GDP current value&amp;quot; or something similar. Let me check the parameters: I need to specify a query. Let's go with &amp;quot;US GDP 2023 latest&amp;quot; to get recent data.&amp;quot; It thinks its way to the wrong query... &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3 A3B:30B via OpenRouter (with Msty's inbuilt web search): It had the right answer but then included numbers from 1999 and was far too verbose. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GPT-OSS 20B via OpenRouter (with Msty's inbuilt web search): On the ball but a tad verbose&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Perplexity Pro: nailed it&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Claude Desktop w Sonnet 4: got it as well, but again more info than requested.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I didnt bother trying anything more.. Its harsh to jump to conclusions with just 1 question but its hard for me to see how Jan V1 is actually better than Perplexity or any other LLM+search tool &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpmeba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T01:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpabh1</id>
    <title>Matrix-Game 2.0 â€” first open-source, real-time, long-sequence interactive world model. 25 FPS, minutes-long interaction</title>
    <updated>2025-08-13T17:10:32+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Skywork_ai/status/1955237399912648842?t=hsxnA2t2FyKxRsSRBCJ1kA&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk834</id>
    <title>Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s.</title>
    <updated>2025-08-13T23:27:12+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"&gt; &lt;img alt="Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s." src="https://external-preview.redd.it/dTd1ZDB2NnBldmlmMavIpB9AHSfqY-PSwwptUZpoVAt7ZMaVO0xQLghP-sG0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65f0830ecafdacba42b232e8ce354f02aa9a68f2" title="Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ycrl1u6pevif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp2wq3</id>
    <title>There is a new text-to-image model named nano-banana</title>
    <updated>2025-08-13T12:20:32+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt; &lt;img alt="There is a new text-to-image model named nano-banana" src="https://preview.redd.it/jmw88evj4sif1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53c768eb9781ffe9119d98e2a2e9f3c88c8adab5" title="There is a new text-to-image model named nano-banana" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jmw88evj4sif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T12:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpm8kr</id>
    <title>ERNIE 4.5 21BA3B appreciation post.</title>
    <updated>2025-08-14T00:55:45+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think it's the best model of it's size, outshining gpt-oss 20 and qwen 3 30BA3B.&lt;/p&gt; &lt;p&gt;It's not as good at coding, but it runs without error even at decent context. I find the qwen a3b to be better for code gen, but prefer ernie for everythign else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T00:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpuvok</id>
    <title>Qwen Coder 30bA3B harder... better... faster... stronger...</title>
    <updated>2025-08-14T08:33:51+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt; &lt;img alt="Qwen Coder 30bA3B harder... better... faster... stronger..." src="https://external-preview.redd.it/ZzJlajBibXkzeWlmMSKN9Y-F1uPgmObNpOLYQwn_bi3ofDf3vCkP-ziGE8lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f02def6a5e8cd415b1d862b2482b085e1338926" title="Qwen Coder 30bA3B harder... better... faster... stronger..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playing around with 30b a3b to get tool calling up and running and I was bored in the CLI so I asked it to punch things up and make things more exciting... and this is what it spit out. I thought it was hilarious, so I thought I'd share :). Sorry about the lower quality video, I might upload a cleaner copy in 4k later.&lt;/p&gt; &lt;p&gt;This is all running off a single 24gb vram 4090. Each agent has its own 15,000 token context window independent of the others and can operate and handle tool calling at near 100% effectiveness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mnpg8bmy3yif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T08:33:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpez1p</id>
    <title>Added locally generated dialogue + voice acting to my game!</title>
    <updated>2025-08-13T20:02:24+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt; &lt;img alt="Added locally generated dialogue + voice acting to my game!" src="https://external-preview.redd.it/N2szZGNtMzRldWlmMZmQp7O5BpjYg7UqegAgE9IdgP7TYx8Szh9dJVqIheQu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eccefbf13830c4a641bc7633ab5e9b01c2c86540" title="Added locally generated dialogue + voice acting to my game!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1qgim34euif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T20:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mptvsl</id>
    <title>tencent/Hunyuan-GameCraft-1.0 Â· Hugging Face</title>
    <updated>2025-08-14T07:32:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt; &lt;img alt="tencent/Hunyuan-GameCraft-1.0 Â· Hugging Face" src="https://external-preview.redd.it/aPfnDoE4lStgbUiMQComf1wdLlqoQrdsgG6jn-2D3d8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47b9265890a5ca1272fc550cc59c1e4e8a3a0326" title="tencent/Hunyuan-GameCraft-1.0 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition&lt;/p&gt; &lt;p&gt;ðŸ“œ Requirements An NVIDIA GPU with CUDA support is required. The model is tested on a machine with 8GPUs. Minimum: The minimum GPU memory required is 24GB but very slow. Recommended: We recommend using a GPU with 80GB of memory for better generation quality. Tested operating system: Linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-GameCraft-1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqew3</id>
    <title>Pruned GPT-OSS 6.0B kinda works</title>
    <updated>2025-08-14T04:16:29+00:00</updated>
    <author>
      <name>/u/Quiet-Engineer110</name>
      <uri>https://old.reddit.com/user/Quiet-Engineer110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt; &lt;img alt="Pruned GPT-OSS 6.0B kinda works" src="https://external-preview.redd.it/aaoKLInTgXWvAC3h_YKai0S41TEi4sEQ5dlZR6riJuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47d9c1b49d8d7aed9f6e4058ae49360afadc00f" title="Pruned GPT-OSS 6.0B kinda works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Engineer110"&gt; /u/Quiet-Engineer110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpvije</id>
    <title>Swiss Canton Basel open sourced multiple tools for on-premise hosting of LLM services</title>
    <updated>2025-08-14T09:12:54+00:00</updated>
    <author>
      <name>/u/fabkosta</name>
      <uri>https://old.reddit.com/user/fabkosta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought this is worth sharing: The Swiss Canton of Basel has made available multiple tools they built for on-premise hosting of LLM-based services (text transcription, RAG, document conversion etc.). None of this is totally breaking news, but they did a solid job building an API plus frontend on top of all their services. And it's there entirely for free, using an MIT license, so everyone may re-use or extend the tools as they wish.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DCC-BS"&gt;https://github.com/DCC-BS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most of the services are relying on a combination of vLLM, Qwen3 32b, LlamaIndex, Python (FastAPI), and Whisper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabkosta"&gt; /u/fabkosta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T09:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mprwv9</id>
    <title>Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?</title>
    <updated>2025-08-14T05:38:43+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt; &lt;img alt="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" src="https://preview.redd.it/ydbnycjn8xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8bcb62748563efa5cb1f78789aa2cd8f3b2860a" title="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's rumors that R2 is coming up sometime in the next month. It does feel that the release of the recent proprietary models have been a bit disappointing, given the marginal gains (e.g. on my &lt;a href="https://www.designarena.ai/"&gt;frontend benchmark&lt;/a&gt;, GPT-5, Opus 4, and 4.1 are basically equivalent though there's a small sample size for the new versions. &lt;/p&gt; &lt;p&gt;In terms of recent releases, open source and open weight models have been amazing. DeepSeek R1-0528 and Qwen3 Coder are #5 and #6 respectively, while GLM 4.5 is #9. &lt;/p&gt; &lt;p&gt;I'm am interested to see what happens with R2. My prediction is that it will basically match GPT-5 and Opus 4 (perhaps might even be a bit better) and we might see a moment similar to when DeepSeek R1 came out. &lt;/p&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ydbnycjn8xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T05:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp5bjc</id>
    <title>God I love Qwen and llamacpp so much!</title>
    <updated>2025-08-13T14:01:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt; &lt;img alt="God I love Qwen and llamacpp so much!" src="https://external-preview.redd.it/YWE3eDdxZG5tc2lmMRvVg1psIEfKedgCcU_ySdSE0fdUxqG9M3HUjgrx1S5i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afab7c45ab87f6ac2ce8db445bb27de25840096" title="God I love Qwen and llamacpp so much!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local batch inference with qwen3 30B Instruct on a single RTX3090, 4 requests in parallel &lt;/p&gt; &lt;p&gt;Gonna use it to mass process some data to generate insights about our platform usage&lt;/p&gt; &lt;p&gt;I feel like I'm hitting my limits here and gonna need a multi GPU setup soon ðŸ˜„&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ur3oxzhnmsif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpr0nc</id>
    <title>Who are the 57 million people who downloaded bert last month?</title>
    <updated>2025-08-14T04:49:28+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt; &lt;img alt="Who are the 57 million people who downloaded bert last month?" src="https://preview.redd.it/vk2njmk01xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9a1c88826aae167a25ae0705a428dcb9f502529" title="Who are the 57 million people who downloaded bert last month?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vk2njmk01xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpu8ot</id>
    <title>DeepSeekâ€™s next AI model delayed by attempt to use Chinese chips</title>
    <updated>2025-08-14T07:54:43+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt; &lt;img alt="DeepSeekâ€™s next AI model delayed by attempt to use Chinese chips" src="https://external-preview.redd.it/tZB3bb_nXpUPAppdkT0H9zuzs440GPDTx7LT8wXA6Cc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e14d54f21759775f1711223ee90d6cd8a8c81634" title="DeepSeekâ€™s next AI model delayed by attempt to use Chinese chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
