<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-22T19:40:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rbvbzt</id>
    <title>Best open-source coder model for replacing Claude Code with Qwen locally?</title>
    <updated>2026-02-22T19:40:34+00:00</updated>
    <author>
      <name>/u/pauljeba</name>
      <uri>https://old.reddit.com/user/pauljeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm currently using Claude Code but want to move fully local.&lt;/p&gt; &lt;p&gt;I‚Äôm specifically looking for a strong coding model for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude code like capaiblities - code + bash &lt;/li&gt; &lt;li&gt;Long file capabiliites&lt;/li&gt; &lt;li&gt;Read image, files&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm considering &lt;code&gt;Qwen3-Coder&lt;/code&gt;, but I‚Äôm unsure:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is &lt;code&gt;Qwen3-Coder&lt;/code&gt; the best choice for a 12GB GPU?&lt;/li&gt; &lt;li&gt;Should I instead run a smaller Qwen coder model (7B/14B) quantized?&lt;/li&gt; &lt;li&gt;Are there better alternatives that outperform Qwen for coding in this VRAM range?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would appreciate real-world experience. If there is an hardward upgrade recommendation what would that be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pauljeba"&gt; /u/pauljeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbppew</id>
    <title>Qwen3 next coder q4 via CLI coding assistant</title>
    <updated>2026-02-22T16:10:01+00:00</updated>
    <author>
      <name>/u/Slow-Ability6984</name>
      <uri>https://old.reddit.com/user/Slow-Ability6984</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Next Coder is awesome when single shot, speed is acceptable and results are great. When using ClaudeCode or OpenCode i feel nothing happens and when appens and i would lilke to modify... I loose motivation üòÑ&lt;/p&gt; &lt;p&gt;Llamacpp logs shows an average of 1000 PP and 60 ts.&lt;/p&gt; &lt;p&gt;Is this the same for you? I'm missing something?&lt;/p&gt; &lt;p&gt;Q4_k_m on latest llamacpp build.&lt;/p&gt; &lt;p&gt;Would like to know if it is the same for you or i'm making some mistake.&lt;/p&gt; &lt;p&gt;Last session, I waited 2 hours and the final result was not good enough so i dropped. I'm using a 5090 that I'm still paying üòÖ and i will for next 6 months. 128GB ddr5 RAM.&lt;/p&gt; &lt;p&gt;A RTX 6000 pro (i have no money but just asking) changes things dratically?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slow-Ability6984"&gt; /u/Slow-Ability6984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbppew/qwen3_next_coder_q4_via_cli_coding_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbppew/qwen3_next_coder_q4_via_cli_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbppew/qwen3_next_coder_q4_via_cli_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T16:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb270r</id>
    <title>PSA on public agentic tools and the speed they are shipping updates: recent Cline release had a package injected</title>
    <updated>2026-02-21T20:52:57+00:00</updated>
    <author>
      <name>/u/bakawolf123</name>
      <uri>https://old.reddit.com/user/bakawolf123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may remember a post about sloppy OpenCode commit a week ago or so, unsurprisingly others are embracing vibe coding speed and sloppiness as well.&lt;/p&gt; &lt;p&gt;I've randomly stumbled upon&lt;br /&gt; &lt;a href="https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/"&gt;https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/&lt;/a&gt; apparently a recent Cline release had OpenClaw installer injected Their plugin in VSCode has some 3M installs, god knows how many standalone CLI. Then you see posts about 40k OpenClaw agents exposed globally. &lt;/p&gt; &lt;p&gt;Really wish there was more scrutiny involved by the teams developing new tools but everyone is just shipping first, then thinking about it. So at the very least make sure your VSCode extensions for are not on auto-update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bakawolf123"&gt; /u/bakawolf123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T20:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbbmcl</id>
    <title>Ouro 2.6B GGUFs are up ‚Äî Q8_0 and Q4_K_M | Release notes + known limitations inside</title>
    <updated>2026-02-22T03:53:12+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;GGUFs are live on HuggingFace: https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed Q8_0 (2.7GB) and Q4_K_M (1.6GB) ‚Äî works in LM Studio, Ollama, llama.cpp. --- ## What Ouro actually is (quick recap) Ouro is a looped inference model ‚Äî instead of running the transformer once per token, it passes the output back into itself for multiple reasoning iterations before committing. The &amp;quot;thinking&amp;quot; you see in the output is real: it's the model working through loops before settling on an answer. Full writeup in the original post. --- ## ‚ö†Ô∏è Release Notes ‚Äî What the GGUF does and doesn't include **GGUF format is standard Llama architecture.** Ouro has three custom architectural features that llama.cpp doesn't support. Here's exactly what happens to each: ### 1. Early Exit Gate (skipped) Ouro has an `early_exit_gate` (weight + bias) ‚Äî a learned mechanism that lets the model decide mid-sequence whether it has &amp;quot;thought enough&amp;quot; and can exit the loop early. **In the GGUF:** This tensor is skipped entirely. The model runs all layers every pass ‚Äî no early exit. This means the GGUF is slightly *more* compute than the original per loop, but also means it won't short-circuit on hard problems. ### 2. TL2 ‚Äî Second Layer Norms (skipped) Each transformer block in Ouro has two layer norms instead of one: - `input_layernorm` (TL1) ‚Äî standard, kept ‚úÖ - `input_layernorm_2` (TL2) ‚Äî Ouro's second norm pass, skipped ‚ùå - `post_attention_layernorm` (TL1) ‚Äî standard, kept ‚úÖ - `post_attention_layernorm_2` (TL2) ‚Äî skipped ‚ùå These are present across all 48 layers. The TL2 norms appear to act as a &amp;quot;re-centering&amp;quot; step between loop iterations. Skipping them means the GGUF doesn't re-normalize between passes the way the full model does. **Practical effect:** The GGUF reasoning is still good ‚Äî the base weights carry the learned behavior. But if you notice the thinking chains being slightly less structured than the HuggingFace original, this is why. ### 3. Python Looping / Inference Wrapper (not in any GGUF) The looping itself ‚Äî passing output back as input for N iterations ‚Äî is implemented in Python at the inference layer, not baked into the weights. **No GGUF can include this** because it's control flow, not a tensor. The GGUF runs one pass per token like any standard model. What you get is essentially the *distilled reasoning capability* that Ouro developed through loop training ‚Äî the model learned to think in its weights, even if the runtime loop isn't there. For the full looped experience, use the original safetensors on HuggingFace with the inference script. --- ## What still works great - The thinking style and extended reasoning ‚Äî very much present - The chattiness and self-correction behavior - Chat template (ChatML / `&amp;lt;|im_start|&amp;gt;` `&amp;lt;|im_end|&amp;gt;`) works out of the box - Q8_0 has minimal quality loss over F16; Q4_K_M is solid for RAM-constrained setups --- ## Files | File | Size | Use case | |------|------|----------| | `ouro-2.6b-q8_0.gguf` | 2.7GB | Best quality, ~3GB VRAM | | `ouro-2.6b-q4_k_m.gguf` | 1.6GB | Fastest, ~2GB VRAM | --- Happy to answer questions about the architecture, the conversion process, or what the looping actually does. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T03:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbi0ij</id>
    <title>Are AI coding agents (GPT/Codex, Claude Sonnet/Opus) actually helping you ship real products?</title>
    <updated>2026-02-22T09:58:38+00:00</updated>
    <author>
      <name>/u/darshan_aqua</name>
      <uri>https://old.reddit.com/user/darshan_aqua</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been testing AI coding agents a lot lately and I‚Äôm curious about real-world impact beyond demos.&lt;/p&gt; &lt;p&gt;A few things I keep noticing:&lt;/p&gt; &lt;p&gt;‚Ä¢ They seem great with Python + JavaScript frameworks, but weaker with Java, C++, or more structured systems ‚Äî is that true for others too?&lt;/p&gt; &lt;p&gt;‚Ä¢ Do they genuinely speed up startup/MVP development, or do you still spend a lot of time fixing hallucinations and messy code?&lt;/p&gt; &lt;p&gt;As someone with ~15 years in software, I‚Äôm also wondering how experienced devs are adapting:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ leaning more into architecture/design? ‚Ä¢ using AI mostly for boilerplate? ‚Ä¢ building faster solo? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Some pain points I hit often:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ confident but wrong code ‚Ä¢ fake APIs ‚Ä¢ good at small tasks, shaky at big systems &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And with local/private AI tools:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ search quality can be rough ‚Ä¢ answers don‚Äôt always stick to your actual files ‚Ä¢ weak or missing citations ‚Ä¢ hard to trust memory &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Would love to hear what‚Äôs actually working for you in production ‚Äî and what still feels like hype.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darshan_aqua"&gt; /u/darshan_aqua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T09:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdsds</id>
    <title>Best Model for single 3090 in 2026?</title>
    <updated>2026-02-22T05:47:26+00:00</updated>
    <author>
      <name>/u/myusuf3</name>
      <uri>https://old.reddit.com/user/myusuf3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running a single RTX 3090 (24GB VRAM) and looking for the best overall model in 2026 for coding + reasoning.&lt;/p&gt; &lt;p&gt;Main priorities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong code generation (Go/TypeScript)&lt;/li&gt; &lt;li&gt;Good reasoning depth&lt;/li&gt; &lt;li&gt;Runs comfortably in 24GB (quantized is fine)&lt;/li&gt; &lt;li&gt;Decent latency on local inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you all running on a single 3090 right now? Qwen? DeepSeek? Something else? Would love specific model names + quant setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myusuf3"&gt; /u/myusuf3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T05:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb4luf</id>
    <title>O-TITANS: Orthogonal LoRAs for Gemma 3 using Google's TITANS memory architecture</title>
    <updated>2026-02-21T22:32:47+00:00</updated>
    <author>
      <name>/u/Polymorphic-X</name>
      <uri>https://old.reddit.com/user/Polymorphic-X</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been working on a project I call &lt;strong&gt;O-TITANS&lt;/strong&gt; (Orthogonal Tensors for Independent Task Alignment). It's an Orthogonal LoRA approach specifically for Gemma 3 that incorporates the Google TITANS memory architecture.&lt;br /&gt; It was inspired by a project by ffurfaro on HF called &amp;quot;TPTT&amp;quot; that I just couldn't get to work.&lt;/p&gt; &lt;p&gt;I'm building this to wrap into my next project: &lt;strong&gt;MoOLE-T (Mixture of Orthogonal LoRA Experts - Titans)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal of MoOLE-T is to use a smaller 8B router to select one or more O-LoRAs to pass inference through simultaneously. The output will then get translated and de-conflicted at an &amp;quot;exit node&amp;quot; (a larger 20B-80B model). Theoretically, this creates a beefed-up MoE with specific skills like a tool belt. This approach should punch way above its weight class while needing only a fraction of the VRAM footprint. The best part? It's scalable to a stupid degree, since O-Loras don't interfere directly and can be multi-slotted. You could train 100+ O-LoRAs on individual skills and have a toolbelt of capabilities without bloating a base model to hundreds of billions of parameters.&lt;/p&gt; &lt;p&gt;Still working on the MoOLE-T polyswarm idea, but I'll do another post whenever that gets finished.&lt;/p&gt; &lt;p&gt;I just finished training an example &lt;code&gt;.pt&lt;/code&gt; file on Open-Platypus using mlabonne's Gemma3-12b-it-abliterated model as a base. It's on my hugginface if you want to test the non-interference claims yourselves.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face (O-TITANS Gemma 3 Adapters):&lt;/strong&gt; &lt;a href="https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/"&gt;https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to feedback and additional ideas. This is all an attempt to try and approach human-esque parallel skill processing and selection without absurd compute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Polymorphic-X"&gt; /u/Polymorphic-X &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T22:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbt955</id>
    <title>Built an offline MCP server that stops AI context bloat using local vector search over a locally indexed codebase.</title>
    <updated>2026-02-22T18:22:35+00:00</updated>
    <author>
      <name>/u/Trust_Me_Bro_4sure</name>
      <uri>https://old.reddit.com/user/Trust_Me_Bro_4sure</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbt955/built_an_offline_mcp_server_that_stops_ai_context/"&gt; &lt;img alt="Built an offline MCP server that stops AI context bloat using local vector search over a locally indexed codebase." src="https://external-preview.redd.it/X77HaFAGL7z76XOIEcsb4FeHtMxHhEbcNapdjRBYtjE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d1231a947bd7b78b47c28f4b26206e8fe45985c" title="Built an offline MCP server that stops AI context bloat using local vector search over a locally indexed codebase." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share an open-source tool I‚Äôve been developing called code-memory. It's a MCP server designed to fix how AI coding assistants interact with large codebases.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Right now, the default approach for AI coding assistants is to either brute-force dump your entire repository into the context window, or rely on shallow, keyword-based search. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Context limits &amp;amp; cost:&lt;/strong&gt; Shoving 200k tokens of code into a prompt is slow, expensive, and eats up VRAM if you are running local models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy degradation:&lt;/strong&gt; LLMs suffer from the &amp;quot;Lost in the Middle&amp;quot; phenomenon‚Äîthey hallucinate variables, invent abstractions that don't exist, and lose track of the actual architecture.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;The Solution&lt;/h1&gt; &lt;p&gt;Instead of blindly dumping context, &lt;code&gt;code-memory&lt;/code&gt; forces the LLM to explicitly fetch only what it needs. Everything‚Äîfrom the embedding model to the db index‚Äîruns &amp;amp; stays &lt;strong&gt;100% locally on your machine&lt;/strong&gt;. No code leaves your system.&lt;/p&gt; &lt;h1&gt;How it works under the hood&lt;/h1&gt; &lt;p&gt;I built the stack to be entirely local, fast, and structured:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AST Parsing (&lt;/strong&gt;&lt;code&gt;tree-sitter&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Instead of chunking raw text, it parses 10+ languages (Python, TS, Rust, Go, C++, etc.) into actual structural components (classes, functions, methods).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Vector DB (&lt;/strong&gt;&lt;code&gt;sqlite-vec&lt;/code&gt; &lt;strong&gt;+ SQLite):&lt;/strong&gt; Stores both structural metadata and dense vector embeddings in a lightweight, in-process database.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Embeddings (&lt;/strong&gt;&lt;code&gt;sentence-transformers&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Uses small, fast local embedding models (like &lt;code&gt;jina-code-embeddings&lt;/code&gt;) to generate vector representations of your code on the fly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Retrieval:&lt;/strong&gt; Combines BM25 (exact keyword match) with Dense Vector search &lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example LLM Workflow&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;If you ask the LLM: &amp;quot;Why is the login timing out?&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Default Approach:&lt;/strong&gt; The LLM runs a bunch of &lt;code&gt;grep&lt;/code&gt; commands or keyword file-searches. It gets back noisy, fragmented lines of code (&amp;quot;login&amp;quot; appears 400 times). Since it lacks structural context, it has to blindly load entire files into context just to find the actual method definition.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;With code-memory:&lt;/strong&gt; The LLM calls &lt;code&gt;search_code(query=&amp;quot;login timeout&amp;quot;)&lt;/code&gt;. It gets back the exact &lt;code&gt;AuthService.login&lt;/code&gt; class method quickly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs still in early development, but fully functional. I‚Äôd love to get this community's feedback on the overall architecture, or hear about any bugs you hit while testing it out. &lt;/p&gt; &lt;p&gt;GitHub Repo: &lt;a href="https://github.com/kapillamba4/code-memory"&gt;https://github.com/kapillamba4/code-memory&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trust_Me_Bro_4sure"&gt; /u/Trust_Me_Bro_4sure &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://github.com/kapillamba4/code-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbt955/built_an_offline_mcp_server_that_stops_ai_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbt955/built_an_offline_mcp_server_that_stops_ai_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T18:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbtudq</id>
    <title>Void-Box: Capability-Bound Agent Runtime</title>
    <updated>2026-02-22T18:44:27+00:00</updated>
    <author>
      <name>/u/Wide_Spite5612</name>
      <uri>https://old.reddit.com/user/Wide_Spite5612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We‚Äôve been building &lt;strong&gt;Void-Box&lt;/strong&gt;, a Rust runtime for executing AI agent workflows inside disposable KVM micro-VMs.&lt;/p&gt; &lt;p&gt;The core idea:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VoidBox = Agent(Skill) + Isolation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of running agents inside shared processes or containers, each stage runs inside its own micro-VM that is created on demand and destroyed after execution. Structured output is then passed to the next stage in a pipeline.&lt;/p&gt; &lt;p&gt;Architecture highlights&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Per-stage micro-VM isolation&lt;/strong&gt; (stronger boundary than shared-process/container models)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy-enforced runtime&lt;/strong&gt; ‚Äî command allowlists, resource limits, seccomp-BPF, controlled egress&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Capability-bound skill model&lt;/strong&gt; ‚Äî MCP servers, SKILL files, CLI tools mounted explicitly per Box&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Composable pipeline API&lt;/strong&gt; ‚Äî sequential &lt;code&gt;.pipe()&lt;/code&gt; and parallel &lt;code&gt;.fan_out()&lt;/code&gt; with explicit failure domains&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Code runtime integration&lt;/strong&gt; (Claude by default, Ollama via compatible provider mode)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built-in observability&lt;/strong&gt; ‚Äî OTLP traces, structured logs, stage-level telemetry&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rootless networking&lt;/strong&gt; via usermode SLIRP (smoltcp, no TAP devices)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The design goal is to treat execution boundaries as a first-class primitive:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No shared filesystem state&lt;/li&gt; &lt;li&gt;No cross-run side effects&lt;/li&gt; &lt;li&gt;Deterministic teardown after each stage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Still early, but the KVM sandbox + pipeline engine are functional.&lt;/p&gt; &lt;p&gt;We‚Äôd especially appreciate feedback from folks with experience in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KVM / virtualization from Rust&lt;/li&gt; &lt;li&gt;Capability systems&lt;/li&gt; &lt;li&gt;Sandbox/runtime design&lt;/li&gt; &lt;li&gt;Secure workflow execution&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/the-void-ia/void-box"&gt;https://github.com/the-void-ia/void-box&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide_Spite5612"&gt; /u/Wide_Spite5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtudq/voidbox_capabilitybound_agent_runtime/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtudq/voidbox_capabilitybound_agent_runtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtudq/voidbox_capabilitybound_agent_runtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T18:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawoe4</id>
    <title>PSA: The software ‚ÄúShade‚Äù is a fraudulent, plagiarized copy of Heretic</title>
    <updated>2026-02-21T17:16:21+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three days ago, the following repository was published, which its ‚Äúcreator‚Äù has been aggressively promoting on various channels since then:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/assemsabry/shade"&gt;https://github.com/assemsabry/shade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire source code in the repository is plagiarized from Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), with only the project name and the copyright notice replaced, claiming ‚Äúoriginal authorship‚Äù of everything. The repository does not acknowledge Heretic as its source, and has erased the commit history and the names of all Heretic contributors.&lt;/p&gt; &lt;p&gt;I and several others have called the repository owner out, but he has deleted all issues and tried to cover up his wrongdoing by adding some bogus ‚Äúadditional features‚Äù using an AI agent. A quick look at the source files, however, reveals that they are still 95% identical to Heretic‚Äôs code. In some cases, only the copyright notice was replaced.&lt;/p&gt; &lt;p&gt;**I can only assume that the ultimate goal is to push malware of some sort, and strongly advise people to stay clear of this plagiarized repository.**&lt;/p&gt; &lt;p&gt;This is one of several incidents where malicious actors tried to profit from Heretic‚Äôs surging popularity during the past days, when it reached #1 on the GitHub trending chart and was posted in various social feeds that cater to scammers.&lt;/p&gt; &lt;p&gt;Please also see &lt;a href="https://github.com/p-e-w/heretic/issues/167"&gt;https://github.com/p-e-w/heretic/issues/167&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm doing everything in my power to keep Heretic clean and available to everyone. Thank you for your encouragement in the past few months, it means the world to me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rburpm</id>
    <title>Predictions / Expectations / Wishlist on LLMs by end of 2026? (Realistic)</title>
    <updated>2026-02-22T19:19:06+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here my Wishlist:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;1-4B models with best t/s(Like 20-30) for Mobile &amp;amp; edge devices.(Currently getting only 5 t/s for Qwen3-4B-IQ4XS on my 8GB RAM mobile)&lt;/li&gt; &lt;li&gt;4-10B models with performance of current 30B models&lt;/li&gt; &lt;li&gt;30-50B models with performance of current 100-150B models&lt;/li&gt; &lt;li&gt;100-150B models with performance of current 500+B models&lt;/li&gt; &lt;li&gt;10-20B Coder models with performance of current 30-80B coder models&lt;/li&gt; &lt;li&gt;More Tailored models like STEM, Writer, Designer, etc., (Like how already we have few categories like Coder, Medical) or Tailored models like Math, Science, History, etc.,&lt;/li&gt; &lt;li&gt;Ability to run 30B MOE models(Q4) on CPU-only inference with 40-50 t/s (Currently getting 25 t/s with 32GB DDR5 RAM on llama.cpp. Somebody please let me know what ik_llama.cpp is giving)&lt;/li&gt; &lt;li&gt;I prefer 5 100B models(Model-WorldKnowledge, Model-Coder, Model-Writer, Model-STEM, Model-Misc) to 1 500B model(Model-GiantALLinOne). Good for Consumer hardwares where Q4 comes in 50GB size. Of course it's good to have additional giant models(or like those 5 tailored models).&lt;/li&gt; &lt;li&gt;Really want to see coding models(with good Agentic coding) to run just with my 8GB VRAM + 32GB RAM(Able to run Qwen3-30B-A3B's IQ4_XS at 35-40 t/s. 15-20 t/s with 32K context). Is this possible by this year end? Though I'm getting new rig, still want to use my current laptop (whenever I'm away from home) effectively with small/medium models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So what are your Predictions, Expectations &amp;amp; Wishlist?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rburpm/predictions_expectations_wishlist_on_llms_by_end/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rburpm/predictions_expectations_wishlist_on_llms_by_end/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rburpm/predictions_expectations_wishlist_on_llms_by_end/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbafs8</id>
    <title>I Trained a Language Model on CPU for 40 Hours - It Beat the GPU Baseline</title>
    <updated>2026-02-22T02:54:39+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who have been following this project, you may recall FlashLM v3, then v4 &amp;quot;Bolt&amp;quot;, and v5.2 &amp;quot;Nova-Ignition&amp;quot;. I am pleased to announce that FlashLM v5 &amp;quot;Thunderbolt&amp;quot; is now complete.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Final BPC&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Parameters&lt;/td&gt; &lt;td align="left"&gt;29.7M (26.5M ternary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Time&lt;/td&gt; &lt;td align="left"&gt;~40 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7950X3D&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;FlashLM v5 achieves a validation perplexity of 1.36, which beats the TinyStories-1M baseline (PPL 1.59). This represents the first instance of a CPU-trained model beating this baseline.&lt;/p&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;p&gt;FlashLM v5 utilizes ParallelGatedRecurrence, a MatMul-free architecture featuring:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BitLinear with ternary weights {-1, 0, +1}&lt;/li&gt; &lt;li&gt;Parallel gated recurrence with learned decay gates&lt;/li&gt; &lt;li&gt;No matrix multiplications in the forward pass&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Parameters: 29,750,784 Ternary: 26,542,080 (89%) Float: 3,208,704 (11%) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Acknowledgments&lt;/h1&gt; &lt;p&gt;I would like to thank arki05 for providing the AMD Ryzen 7950X3D used for training. Without this contribution, the project would not have been possible.&lt;/p&gt; &lt;h1&gt;Generation Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Version&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;BPC&lt;/th&gt; &lt;th align="left"&gt;Output Quality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;v4 &amp;quot;Bolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;15.05&lt;/td&gt; &lt;td align="left"&gt;0.88&lt;/td&gt; &lt;td align="left"&gt;Short, repetitive&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5.2 &amp;quot;Nova-Ignition&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;10.56&lt;/td&gt; &lt;td align="left"&gt;0.78&lt;/td&gt; &lt;td align="left"&gt;Better coherence&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5 &amp;quot;Thunderbolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;td align="left"&gt;Significantly better&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v5 demonstrates improved cohesive storytelling compared to v4 and v5.2&lt;/li&gt; &lt;li&gt;v5 shows better vocabulary diversity and grammar&lt;/li&gt; &lt;li&gt;BPC improved from 0.88 (v4) to 0.44 (v5), representing a 2x improvement&lt;/li&gt; &lt;li&gt;PPL improved from 15.05 (v4) to 1.36 (v5), representing an 11x improvement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Samples&lt;/h1&gt; &lt;p&gt;Prompt: &amp;quot;Once upon a time, there was a brave girl named Lucy.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time, there was a brave girl named Lucy. her big tiny looked door, and she wanted. Lucy loved to creative things. She would find toy when, while small laughing, when she thought. She would be friends all day.One day, Lucy found her toy saw a little hole. Lucy was very happy. She wanted to see who was mean. The little hole was not alone anymore. When Lucy was done playing, she saw the little...&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/changcheng967/flashlm-v5-demo"&gt;https://huggingface.co/spaces/changcheng967/flashlm-v5-demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model Card: &lt;a href="https://huggingface.co/changcheng967/flashlm-v5-thunderbolt"&gt;https://huggingface.co/changcheng967/flashlm-v5-thunderbolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/changcheng967/FlashLM"&gt;https://github.com/changcheng967/FlashLM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Future Directions&lt;/h1&gt; &lt;p&gt;FlashLM v5 concludes the v5 series. Future work includes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;FlashLM v6 - Continuing to validate the ParallelGatedRecurrence architecture&lt;/li&gt; &lt;li&gt;Nano-Coder (NC series) - Applying FlashLM techniques to code generation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T02:54:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbfh1y</id>
    <title>dyslexia and ADHD in the coding community</title>
    <updated>2026-02-22T07:23:34+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my third post on my first Reddit account. Here's why that took so long.&lt;/p&gt; &lt;p&gt;I have dyslexia and ADHD. I've been lurking in communities like this one for years -- reading everything, learning everything -- but never posting. Not because I had nothing to contribute. Because I was scared of what would happen when people saw how I write.&lt;/p&gt; &lt;p&gt;People with dyslexia and ADHD don't write the way the internet expects. The spelling is off. The punctuation is wrong. The sentences don't flow right. And the internet has never been kind about that. We get called stupid. We get told our ideas don't matter because the package they came in looked messy. So we lurk. We learn. We do real work quietly and never share it because the cost of being mocked is too high.&lt;/p&gt; &lt;p&gt;I use AI to help me write. Not to generate ideas -- the ideas are mine. Not to do the work -- I did the work. To help me communicate in a way that doesn't get me dismissed before anyone reads what I actually built.&lt;/p&gt; &lt;p&gt;Yesterday I shipped the first working GGUF quantization of Ouro -- ByteDance's recurrent thinking model. I figured out the tensor mapping, the layer norm mismatch, the early exit gate skip. That was me. And the first thing someone did was question whether I was human.&lt;/p&gt; &lt;p&gt;I'm posting this because I know I'm not the only one. There are people in this community right now with real knowledge, real skills, real contributions -- who won't post because they're afraid of exactly what happened to me today.&lt;/p&gt; &lt;p&gt;You belong here. Your ideas belong here. How you write doesn't determine what you know.&lt;/p&gt; &lt;p&gt;This was my first post. It won't be my last.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T07:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1raq23i</id>
    <title>they have Karpathy, we are doomed ;)</title>
    <updated>2026-02-21T12:34:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt; &lt;img alt="they have Karpathy, we are doomed ;)" src="https://preview.redd.it/ergzi9d1eukg1.png?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=2005c28094bfd489a487151bba9f5c550c22c55b" title="they have Karpathy, we are doomed ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(added second image for the context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raq23i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T12:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbtfld</id>
    <title>What Other Subs Do you Read to Keep Up with AI?</title>
    <updated>2026-02-22T18:29:17+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what other subs do you recommend to read to keep up with AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T18:29:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbio4h</id>
    <title>Has anyone else tried IQ2 quantization? I'm genuinely shocked by the quality</title>
    <updated>2026-02-22T10:37:47+00:00</updated>
    <author>
      <name>/u/Any-Chipmunk5480</name>
      <uri>https://old.reddit.com/user/Any-Chipmunk5480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've always used GGUF and never went below Q4_K_M because I assumed anything lower would be garbage. Today I decided to try UD-IQ2_XXS on Qwen3-30B-A3B (10.3 GB) and I'm honestly shocked. First off 100 TPS on my RX 9060 XT 16GB, up from 20 TPS on Q4_K_M. 5x speedup with 20K+ context, fully offloaded to GPU. But the real surprise is the quality. I had Claude Opus 4.6 generate progressively harder questions to test it chemistry, math, physics, relativity, deep academic topics. At high school and university level, I couldn't find any meaningful difference between IQ2 and Q4. The only noticeable quality drop was on really niche academic stuff (G√∂del's Incompleteness Theorem level), and even there it scored 81/100 vs Q4's 92. The funniest part on a graph analysis question, my 10GB local IQ2 model got the correct answer while both Claude Opus 4.6 and Sonnet 4.6 misread the graph and got it wrong. Has anyone else had similar experiences with ultra-low quants? Why is this not that hyped? Setup: RX 9060 XT 16GB / llama.cpp / Vulkan / Qwen3-30B-A3B UD-IQ2_XXS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Chipmunk5480"&gt; /u/Any-Chipmunk5480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbio4h/has_anyone_else_tried_iq2_quantization_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T10:37:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbculq</id>
    <title>Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)</title>
    <updated>2026-02-22T04:56:59+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt; &lt;img alt="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" src="https://external-preview.redd.it/6QqGCIHe3v1WQBe6_gTslJhJpyRq4mX4jqVDYTY6xG0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=504511a0ed53dd20492f3b96504f6d5f1655bc7b" title="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://discrepancyreport.com/lawyer-says-google-shut-down-his-gmail-voice-and-photos-after-notebooklm-upload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T04:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbmnw7</id>
    <title>Is there *any* good coding agent software for use with local models?</title>
    <updated>2026-02-22T14:04:29+00:00</updated>
    <author>
      <name>/u/eapache</name>
      <uri>https://old.reddit.com/user/eapache</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Code seems to be &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;taking steps&lt;/a&gt; to make it more and more difficult to use with local models with things like forcing the context to constantly be recalculated. OpenCode has made the decision to basically not have a permissions model and just &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r8oehn/opencode_arbitrary_code_execution_major_security/"&gt;allow the LLM to execute whatever code it wants&lt;/a&gt;. Cline was &lt;a href="https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/"&gt;made to install OpenClaw on users machines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;All I want is a stable, secure, permission-sensible coding agent, that I trust to run without eighteen layers of sandboxing. So Claude Code, but one that I can easily run against a local model. Does it not exist?&lt;/p&gt; &lt;p&gt;I know there are other competitors in this space (Roo, Pi, ...) but at this point I was hoping for a positive recommendation before I waste more time evaluating garbage.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eapache"&gt; /u/eapache &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T14:04:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb2j5c</id>
    <title>Favourite niche usecases?</title>
    <updated>2026-02-21T21:06:34+00:00</updated>
    <author>
      <name>/u/Figai</name>
      <uri>https://old.reddit.com/user/Figai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt; &lt;img alt="Favourite niche usecases?" src="https://preview.redd.it/o4l2ankhxwkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7201facadd4e9d14e1aac7efef2133d85d346f7" title="Favourite niche usecases?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Figai"&gt; /u/Figai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4l2ankhxwkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T21:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rblce7</id>
    <title>I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful</title>
    <updated>2026-02-22T13:03:49+00:00</updated>
    <author>
      <name>/u/Weird_Search_4723</name>
      <uri>https://old.reddit.com/user/Weird_Search_4723</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt; &lt;img alt="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" src="https://external-preview.redd.it/NWtrYWtuYXZuMWxnMexVgBFEBEtAfoKpFzO1VgJV4m4gRx-YBoBnOCuCCbAU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f4bb616205fb72d1541634b6985338275c23ac3" title="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is Kon telling you about it's own repo, using glm-4.7-flash-q4 running locally on my i7-14700F √ó 28, 64GB RAM, 24GB VRAM (RTX 3090) ‚Äì video is sped up 2x&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;github: &lt;a href="https://github.com/kuutsav/kon"&gt;https://github.com/kuutsav/kon&lt;/a&gt;&lt;br /&gt; pypi: &lt;a href="https://pypi.org/project/kon-coding-agent/"&gt;https://pypi.org/project/kon-coding-agent/&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The pitch (in the readme as well):&lt;/p&gt; &lt;p&gt;It has a tiny harness: about &lt;strong&gt;215 tokens&lt;/strong&gt; for the system prompt and around &lt;strong&gt;600 tokens&lt;/strong&gt; for tool definitions ‚Äì so under 1k tokens before conversation context.&lt;/p&gt; &lt;p&gt;At the time of writing this README (22 Feb 2026), this repo has 112 files and is easy to understand in a weekend. Here‚Äôs a rough file-count comparison against a couple of popular OSS coding agents:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ fd . | cut -d/ -f1 | sort | uniq -c | sort -rn 4107 opencode 740 pi-mono 108 kon &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Others are of course more mature, support more models, include broader test coverage, and cover more surfaces. But if you want a truly minimal coding agent with batteries included ‚Äì something you can understand, fork, and extend quickly ‚Äì Kon might be interesting.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;It takes lots of inspiration from &lt;a href="https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent"&gt;pi-coding-agent&lt;/a&gt;, see the &lt;a href="https://github.com/kuutsav/kon?tab=readme-ov-file#acknowledgements"&gt;acknowledgements&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit 1: this is a re-post, deleted the last one (missed to select video type when creating the post)&lt;br /&gt; Edit 2: more about the model that was running in the demo and the config: &lt;a href="https://github.com/kuutsav/kon/blob/main/LOCAL.md"&gt;https://github.com/kuutsav/kon/blob/main/LOCAL.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Search_4723"&gt; /u/Weird_Search_4723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jf0xcw9vn1lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T13:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbjxpv</id>
    <title>I think openclaw is OVERHYPED. Just use skills</title>
    <updated>2026-02-22T11:51:38+00:00</updated>
    <author>
      <name>/u/Deep_Traffic_7873</name>
      <uri>https://old.reddit.com/user/Deep_Traffic_7873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think openclaw is useful, loop, memory, agents, integrations, but after a week a testing, honestly I don't need it much.&lt;/p&gt; &lt;p&gt;- memory, is nice. But I prefere to have &amp;quot;manual memory&amp;quot;. Prompt: Ok, write what yout learnt in &amp;quot;superreporttrending-skill&amp;quot;. Automatic memory often pollute the context of info you don't care.&lt;/p&gt; &lt;p&gt;- cron. Useful but I already use other tools for that and I can always recall a skill whenever i want. I don't need everyday at 8:00AM, i prefere recall it when i want with up to date data&lt;/p&gt; &lt;p&gt;Conclusion: for me &amp;quot;opencode web&amp;quot; is a much superior option, but much of the &amp;quot;intelligence&amp;quot; and value is the skills that you develop or you integrate, not in the runner itself, what do you think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Traffic_7873"&gt; /u/Deep_Traffic_7873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T11:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbnczy</id>
    <title>The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets.</title>
    <updated>2026-02-22T14:34:36+00:00</updated>
    <author>
      <name>/u/w1nter5n0w</name>
      <uri>https://old.reddit.com/user/w1nter5n0w</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt; &lt;img alt="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." src="https://preview.redd.it/l8duwvse42lg1.png?width=140&amp;amp;height=106&amp;amp;auto=webp&amp;amp;s=2928d1df2289068d0491626609ab2109106409dc" title="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, a friend of mine posted a thread here (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/&lt;/a&gt;) regarding a project he started called &lt;strong&gt;DeepSeek-Overclock&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal was to create an experimental setup designed to theoretically push the model's reasoning capabilities to the absolute limit. However, the &amp;quot;overclocked&amp;quot; DeepSeek model kept failing during the process. After diving deep into the logs, he realized the model wasn't hallucinating. In many instances, it was rigorously deriving answers that were technically correct but contradicted the provided &amp;quot;gold standard&amp;quot; labels.&lt;/p&gt; &lt;p&gt;He ended up writing Python scripts to verify the math line-by-line from first principles. Then he found out that &lt;strong&gt;the data quality in both the GPQA and HLE (Humanity's Last Exam) test sets is seriously flawed.&lt;/strong&gt; (You can check the link above for the specific details of that investigation).&lt;/p&gt; &lt;p&gt;Fast forward to a couple of days ago, and the &lt;strong&gt;Qwen team just released a paper&lt;/strong&gt; that basically confirms exactly what we saw: the data quality in GPQA and HLE is a mess.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20"&gt;https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Attached the screenshot of Fig. 1: Structural composition of HLE-Verified.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arxiv Link:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2602.13964v2"&gt;https://arxiv.org/abs/2602.13964v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper doesn't mince words. Right from the intro, it bluntly points out that a lot of the questions in the HLE test set are fundamentally broken. And in some cases, &amp;quot;standard answers&amp;quot; that are straight-up wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w1nter5n0w"&gt; /u/w1nter5n0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T14:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbkeea</id>
    <title>Which one are you waiting for more: 9B or 35B?</title>
    <updated>2026-02-22T12:15:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt; &lt;img alt="Which one are you waiting for more: 9B or 35B?" src="https://preview.redd.it/jyvany3jf1lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f667e97854acf566b7f6d1d56e9c09e17f5a8ee8" title="Which one are you waiting for more: 9B or 35B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jyvany3jf1lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T12:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
