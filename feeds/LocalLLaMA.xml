<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-09T16:55:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1phtydt</id>
    <title>FYI, looks like Tesla P40s are back down in price!</title>
    <updated>2025-12-09T00:58:16+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just posting so y'all are aware. I previously grabbed a P40 for 165, and I see them going for 190 on eBay now. I would say the price is reasonable and the card is still well supported in Llama.cpp. &lt;/p&gt; &lt;p&gt;The Mi60 32gb has been price inflated. So I would avoid that. &lt;/p&gt; &lt;p&gt;With the dram prices going sky high, getting a few of these in a rig could definitely be a viable option. You can probably grab like 3 of these for under 600 bucks and run Derestricted 120B in VRAM at really high speeds since 120B is quite compute light. You could even run Derestricted GLM 4.5 Air at Q4 as well. And they will destroy DRAM setups in terms of speed. &lt;/p&gt; &lt;p&gt;I know there is talk about cuda dropping support for the newest versions, but this card still works, and will always work. (And I doubt llama.cpp will require new cuda versions for the foreseeable future). And currently the Air and 120B models are very good. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi91e7</id>
    <title>Micron üßêüíÄ</title>
    <updated>2025-12-09T14:38:24+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi91e7/micron/"&gt; &lt;img alt="Micron üßêüíÄ" src="https://preview.redd.it/ol4nyn3sw66g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b67e605d76f5dfcb76784cc4fb94dfc00e60c983" title="Micron üßêüíÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;-&amp;gt; today, what companies that train models are looking for is to look for optimizations and that it is cheap to train, for example when the TPU issue came up, that is, there will not always be a high demand&lt;/p&gt; &lt;p&gt;-&amp;gt; perhaps in 2026 more optimizations will come out of China, which may lead to lower consumption &lt;/p&gt; &lt;p&gt;-&amp;gt; An HBM plant takes approximately 1 year to build, what if optimizations come out within a year? üíÄ&lt;/p&gt; &lt;p&gt;Note: &lt;/p&gt; &lt;p&gt;&lt;a href="https://finance.yahoo.com/news/micron-plans-9-6-billion-125500795.html"&gt;https://finance.yahoo.com/news/micron-plans-9-6-billion-125500795.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ol4nyn3sw66g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi91e7/micron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi91e7/micron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T14:38:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi1tc8</id>
    <title>Native Parallel Reasoner (NPR): Reasoning in Parallelism via Self-Distilled RL, 4.6x Faster, 100% genuine parallelism, fully open source</title>
    <updated>2025-12-09T07:56:06+00:00</updated>
    <author>
      <name>/u/Think_Specific_7241</name>
      <uri>https://old.reddit.com/user/Think_Specific_7241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am excited to share our latest research, &lt;strong&gt;Native Parallel Reasoner (NPR)&lt;/strong&gt;, which introduces a new paradigm to enable LLMs to perform native, internal parallel reasoning.&lt;/p&gt; &lt;p&gt;We know that sequential, token-by-token reasoning can be slow and sometimes inefficient. NPR changes this by training the model to simultaneously generate multiple candidate &amp;quot;thought&amp;quot; branches, execute them in parallel, and reduce them to a final answer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Instead of relying on strong external teachers (like GPT-series distillation) or manual annotation, NPR uses a format-aware self-exploration loop:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Self-Distillation + Parallel SFT:&lt;/strong&gt; The model learns to propose parallel branches.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PAPO (Parallel-Aware Policy Optimization):&lt;/strong&gt; A specialized parallel Reinforcement Learning algorithm we designed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NPR-Engine:&lt;/strong&gt; A verifiable inference engine that validates the format and results of every branch, allowing the model to self-optimize.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Key Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; We achieved up to a &lt;strong&gt;4.6√ó wall-clock speedup&lt;/strong&gt; compared to standard autoregressive methods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Significantly outperforms existing parallel and autoregressive baselines on math and complex reasoning benchmarks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robustness:&lt;/strong&gt; In testing, we saw a &lt;strong&gt;~100% parallel trigger rate&lt;/strong&gt;, meaning the model genuinely internalized the &amp;quot;parallel thinking&amp;quot; strategy and didn't fall back to sequential generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, this offers a reproducible path to go from algorithm to engineering, making &amp;quot;parallel thinking&amp;quot; a trainable, verifiable, and deployable capability rather than just a prompting trick.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;X:&lt;/strong&gt; &lt;a href="https://x.com/ZilongZheng/status/1998252267783516444?s=20"&gt;https://x.com/ZilongZheng/status/1998252267783516444?s=20&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HF:&lt;/strong&gt;&lt;a href="https://huggingface.co/papers/2512.07461"&gt;https://huggingface.co/papers/2512.07461&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Project Page:&lt;/strong&gt;&lt;a href="https://bigai-nlco.github.io/Native-Parallel-Reasoner/"&gt;https://bigai-nlco.github.io/Native-Parallel-Reasoner/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper (ArXiv):&lt;/strong&gt;&lt;a href="https://arxiv.org/abs/2512.07461"&gt;https://arxiv.org/abs/2512.07461&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer any questions about the training pipeline or the architecture!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Think_Specific_7241"&gt; /u/Think_Specific_7241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi1tc8/native_parallel_reasoner_npr_reasoning_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi1tc8/native_parallel_reasoner_npr_reasoning_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi1tc8/native_parallel_reasoner_npr_reasoning_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T07:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph8wel</id>
    <title>RAM prices explained</title>
    <updated>2025-12-08T10:17:09+00:00</updated>
    <author>
      <name>/u/Lopsided_Sentence_18</name>
      <uri>https://old.reddit.com/user/Lopsided_Sentence_18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI bought up 40% of global DRAM production in raw wafers they're not even using - just stockpiling to deny competitors access. Result? Memory prices are skyrocketing. Month before chrismass.&lt;/p&gt; &lt;p&gt;Source: Moore¬¥s law is Dead&lt;br /&gt; Link: &lt;a href="https://www.mooreslawisdead.com/post/sam-altman-s-dirty-dram-deal"&gt;Sam Altman‚Äôs Dirty DRAM Deal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Sentence_18"&gt; /u/Lopsided_Sentence_18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T10:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1phcyvk</id>
    <title>After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM</title>
    <updated>2025-12-08T13:54:31+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt; &lt;img alt="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" src="https://a.thumbs.redditmedia.com/cM8ZY8pfeiL7V-euNxiaRZSPcskPH5ahnCORrr5O-W4.jpg" title="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, it's ugly and frankly embarrassing to look at. I just finished this build last night by adding 2 additional GPUs to go from 6 to 8, where I will stop &amp;amp; call this build complete.&lt;/p&gt; &lt;p&gt;I've built many PCs over the years but this was a whole other level and at this point I'm just happy it works. It runs off daisy chained 1500W and 1000W PSUs (5 cards on the 1500W and 3 on the 1000W), and the system is fed by a 20A dedicated branch circuit.&lt;/p&gt; &lt;p&gt;Cramming the GPUs in a case without having to use long GPU riser cables was the hardest part. If I were to do this again, I'd just use long PCIE 1x cables that give me the freedom to neatly stack the cards and save myself the headache, since this is just an inference system... only time PCIE bandwidth matters is when loading models. But I went down the path of using certified PCIE 4.0 cables that range from 200-250mm, &amp;amp; as you can see, it ain't pretty. One card has to sit outside the rack bc there was simply no space for it among the chonky GPUs &amp;amp; PCIE riser spaghetti.&lt;/p&gt; &lt;p&gt;Good news is that the system has been running stable for it's entire existence as I kept adding parts &amp;amp; just learning as I go. GPU temps never exceed 70ish*C under load since the GPUs are pretty well spread out in an open case, and all in I spent about $8k, as almost every part in the system is used (only the motherboard was bought new - a supermicro supermicro h12ssl-i which was $400 at the time).&lt;br /&gt; The most I paid for a GPU was $700, the lowest was $500, which was just this week. FB Marketplace is great in my area - I had tons of options and I highly recommend local sellers over ebay.&lt;br /&gt; All I've done so far is load GLM 4.5 air Q6_K GGUF using llama.cpp, specifically these settings - &lt;code&gt;llama-server \-m /home/hisma/llama.cpp/models/GLM-4.5-Air.i1-Q6_K/GLM-4.5-Air.i1-Q6_K.gguf -c 131072 -ngl 99 -b 4096 -ub 2048 -fa --temp 0.6 --top-p 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8888&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From the screenshot, you can see it pulled off a respectable ~49 t/s.&lt;br /&gt; My next steps -&lt;/p&gt; &lt;ul&gt; &lt;li&gt;power limit all cards to ~250W (maybe lower depending on how my system responds - confident I shouldn't need to go any lower than 200W which would only be a ~20% perf hit)&lt;/li&gt; &lt;li&gt;test some AWQ models using VLLM with tensor parallelism (specifically MiniMax-M2-AWQ-4bit). &lt;ul&gt; &lt;li&gt;My whole reason for going to 8 GPUs is bc TP requires either 2, 4 or 8 cards. So 8 cards was always my goal to get the most out of this system&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Once I find a solid set of models, start doing some agentic coding with roocode &amp;amp; let this thing rip&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With PC hardware prices going insane lately, I feel lucky to have this thing, even with the janky ass build. It was a good learning experience &amp;amp; certainly would do some things different w/ the lessons I learned, but I forsee future enshittification of cloud models as the big corpos pivot to pleasing shareholders over burning cash, and in the 1 year I've had this system local models have continued to improve and trade blows with frontier models while using less memory, I'm sure the trend will continue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phcyvk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:54:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi4qmg</id>
    <title>nano-trm - Train your own TRM in a few minutes</title>
    <updated>2025-12-09T11:07:05+00:00</updated>
    <author>
      <name>/u/randomwalkin</name>
      <uri>https://old.reddit.com/user/randomwalkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;Tiny Recursive Models reach impressive results on ARC AGI. I implemented a version from scratch, with ease of experimentation in mind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;cleaner config: hydra, uv, lightning&lt;/li&gt; &lt;li&gt;smaller datasets for faster iteration (Sudoku 6x6 and 9x9)&lt;/li&gt; &lt;li&gt;introduction, in-code video&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All important implementation details have been carefully kept. The results of the paper are reproducible (Sudoku Extreme, Maze Hard).&lt;/p&gt; &lt;p&gt;Feedback/contributions welcome.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/olivkoch/nano-trm"&gt;https://github.com/olivkoch/nano-trm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomwalkin"&gt; /u/randomwalkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4qmg/nanotrm_train_your_own_trm_in_a_few_minutes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4qmg/nanotrm_train_your_own_trm_in_a_few_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4qmg/nanotrm_train_your_own_trm_in_a_few_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T11:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi6hbx</id>
    <title>Models that has the least collapse when ctx length grows. Especially using it with tools.</title>
    <updated>2025-12-09T12:44:57+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;local models: what is your experience. Any models you can realiably push to 128k or even past that with consistent success and not getting into retry loops or thinking loops with tools?? My best expereince so far is gpt-oss at 64k but past 64k its starts to get hickups and missaps. what are your experiences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi6hbx/models_that_has_the_least_collapse_when_ctx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi6hbx/models_that_has_the_least_collapse_when_ctx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi6hbx/models_that_has_the_least_collapse_when_ctx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T12:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pib8z9</id>
    <title>New ways to roast people in the AI era</title>
    <updated>2025-12-09T16:04:20+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the AI era, we can update the way we roast people.&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;nerd,&amp;quot; try saying &amp;quot;benchmaxxed.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;brain-dead,&amp;quot; try saying &amp;quot;pruned/quantized.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;no brain,&amp;quot; try saying &amp;quot;low params count.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;didn't study,&amp;quot; try saying &amp;quot;undertrained.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;only knows book knowledge,&amp;quot; try saying &amp;quot;overfitted.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;boring and dull,&amp;quot; try saying &amp;quot;safetymaxxed.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;slow to react,&amp;quot; try saying &amp;quot;slow prompt processing/token generation.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;clumsy,&amp;quot; try saying &amp;quot;poor tool use performance.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;talks nonsense endlessly,&amp;quot; try saying &amp;quot;temperature too high/missing EOS.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;speaks gibberish,&amp;quot; try saying &amp;quot;template config error/topK sampling error.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;disobedient,&amp;quot; try saying &amp;quot;non-instruct base model.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;doesn't think with the brain,&amp;quot; try saying &amp;quot;non-thinking instruct model.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;poor memory,&amp;quot; try saying &amp;quot;low context window.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;easily fooled,&amp;quot; try saying &amp;quot;vulnerable to prompt injection.&amp;quot;&lt;/p&gt; &lt;p&gt;It's normal if you don't understand any of this. If you understand all of these, go outside and touch some grass.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T16:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi28mq</id>
    <title>model: support Rnj-1 by philip-essential ¬∑ Pull Request #17811 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-09T08:24:52+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi28mq/model_support_rnj1_by_philipessential_pull/"&gt; &lt;img alt="model: support Rnj-1 by philip-essential ¬∑ Pull Request #17811 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/HJud6LLFn4jcoTTYFQN2jlM9S8V73LW3HfXmSX8jM3s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0e15d519011eeac1611d0ff185d55ed015a906e" title="model: support Rnj-1 by philip-essential ¬∑ Pull Request #17811 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models. These models perform well across a range of programming languages and boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent), while also excelling at tool-calling. They additionally exhibit strong capabilities in math and science. Herein, &lt;code&gt;rnj-1&lt;/code&gt; refers to the base model, while &lt;code&gt;rnj-1-instruct&lt;/code&gt; refers to the post-trained instruction tuned model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/EssentialAI/rnj-1-instruct"&gt;https://huggingface.co/EssentialAI/rnj-1-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/EssentialAI/rnj-1-instruct-GGUF"&gt;https://huggingface.co/EssentialAI/rnj-1-instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17811"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi28mq/model_support_rnj1_by_philipessential_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi28mq/model_support_rnj1_by_philipessential_pull/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T08:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1phsqix</id>
    <title>Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use</title>
    <updated>2025-12-09T00:04:35+00:00</updated>
    <author>
      <name>/u/0xmaxhax</name>
      <uri>https://old.reddit.com/user/0xmaxhax</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"&gt; &lt;img alt="Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use" src="https://preview.redd.it/s0wx32rvk26g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=497a12c2009280cf7d68db66bd9159fbbb109206" title="Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As of recent swe-bench evaluations, this is where top open weight models stand regarding real-world agentic coding use. My personal experience, though, is different.&lt;/p&gt; &lt;p&gt;Benchmarks are very crude approximations of a models ability to perform in specific use cases (i.e. solving real-world GitHub issues for top Python repositories in this case), but nothing than that - a rough, inherently flawed approximation to be taken with extreme caution. Not to mention they often gloss over the unpredictability of results in real-world usage along with the large margin of error in benchmarking.&lt;/p&gt; &lt;p&gt;Now, in my experience (within Claude Code), Minimax M2 is good for what it is; an efficient, compact, and effective tool-calling agent - but I feel it somewhat lacks the reasoning depth required for planning and executing complex problems without veering off course. It‚Äôs amazingly efficient and capable for local use at Q4 quant, and works well for most use cases. GLM 4.6, in my experience, seems to be like a more reliable choice to daily drive, and can handle more difficult tasks if properly guided - I‚Äôd say it‚Äôs only slightly worse than Sonnet 4.5 in CC (for my particular use case) - the difference is not very noticeable to me. I have not yet had the opportunity to try out Deepseek v3.2 within CC, but I will update this post on my thoughts once I do. From what I‚Äôve heard / read, it is a noticeable step up from v3.2-exp, which means it should land at or very slightly above GLM 4.6 for agentic coding use (matching what swe-bench recently reports).&lt;/p&gt; &lt;p&gt;In many ways, open weight models are growing increasingly more practical for local and professional use in agentic coding applications, especially with the latest releases and architectural / training advancements. I would love to know your thoughts: Which open LLM (for local or API use) is best for agentic coding, whether it be in CC or in other platforms? What is your experience with the provided models, and does Deepseek v3.2 surpass GLM 4.6 and/or Minimax M2 for your use cases? And if anyone has run private, non-polluted evaluations of the aforementioned models as of recently, I‚Äôm interested in your results. Disagreement is welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xmaxhax"&gt; /u/0xmaxhax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s0wx32rvk26g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9fpf</id>
    <title>PaCoRe: The first open-source deep think 8B model beats GPT-5 on HMMT25</title>
    <updated>2025-12-09T14:54:13+00:00</updated>
    <author>
      <name>/u/Fancy_Fanqi77</name>
      <uri>https://old.reddit.com/user/Fancy_Fanqi77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"&gt; &lt;img alt="PaCoRe: The first open-source deep think 8B model beats GPT-5 on HMMT25" src="https://external-preview.redd.it/36cc9Kci4vmEFbiqweyD9wkzOHfQN0s2ACnf4eu8ZUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=583cedaf4f617529e9733518fd2bbaac2593e3a3" title="PaCoRe: The first open-source deep think 8B model beats GPT-5 on HMMT25" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Parallel Coordinated Reasoning (PaCoRe)&lt;/p&gt; &lt;p&gt;An 8B &lt;strong&gt;model beats GPT-5 on&lt;/strong&gt; HMMT25 by unlocking parallel thinking for te&lt;strong&gt;st-time scaling!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The first open-source &lt;strong&gt;deep think:&lt;/strong&gt; data + model + inference code!&lt;/p&gt; &lt;p&gt;MIT-licensed ‚Äî use it however you want&lt;/p&gt; &lt;p&gt;- Github: &lt;a href="https://github.com/stepfun-ai/PaCoRe"&gt;https://github.com/stepfun-ai/PaCoRe&lt;/a&gt;&lt;br /&gt; - Paper: &lt;a href="https://github.com/stepfun-ai/PaCoRe/blob/main/pacore_report.pdf"&gt;https://github.com/stepfun-ai/PaCoRe/blob/main/pacore_report.pdf&lt;/a&gt;&lt;br /&gt; - Model: &lt;a href="https://huggingface.co/stepfun-ai/PaCoRe-8B"&gt;https://huggingface.co/stepfun-ai/PaCoRe-8B&lt;/a&gt;&lt;br /&gt; - Data: &lt;a href="https://huggingface.co/datasets/stepfun-ai/PaCoRe-Train-8k"&gt;https://huggingface.co/datasets/stepfun-ai/PaCoRe-Train-8k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vgqyqe7fy66g1.png?width=814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0db9e68bf4e8750170acb24abb42a1b26eb2764"&gt;https://preview.redd.it/vgqyqe7fy66g1.png?width=814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0db9e68bf4e8750170acb24abb42a1b26eb2764&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7tufs4jhy66g1.png?width=851&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5ce148934a1569d2df62cf271d9bb7d36ad94f3"&gt;https://preview.redd.it/7tufs4jhy66g1.png?width=851&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5ce148934a1569d2df62cf271d9bb7d36ad94f3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fancy_Fanqi77"&gt; /u/Fancy_Fanqi77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T14:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi75j1</id>
    <title>got tired of staring at raw logs while my local agents ran, so I built a "Mission Control" UI that connects to my terminal. Thoughts?</title>
    <updated>2025-12-09T13:16:45+00:00</updated>
    <author>
      <name>/u/Durst123</name>
      <uri>https://old.reddit.com/user/Durst123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi75j1/got_tired_of_staring_at_raw_logs_while_my_local/"&gt; &lt;img alt="got tired of staring at raw logs while my local agents ran, so I built a &amp;quot;Mission Control&amp;quot; UI that connects to my terminal. Thoughts?" src="https://external-preview.redd.it/OTVjcnA0dndoNjZnMWDQvuPx_qAwAWbSlGAjJ1Y3p2Pqxou6uljBUEewC7tF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f80c7c46e04a29b0fb50f1008e527535bbd7e69" title="got tired of staring at raw logs while my local agents ran, so I built a &amp;quot;Mission Control&amp;quot; UI that connects to my terminal. Thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running a lot of long-running agents (Claude Code / Open Interpreter / Codex) on my local machine and VPS.&lt;/p&gt; &lt;p&gt;The problem is: if I step away, I lose visibility. And reading raw matrix-style logs on my phone via SSH is painful. (I even built an Android app for that)&lt;/p&gt; &lt;p&gt;I built this &amp;quot;Control Plane&amp;quot; prototype. It basically pipes stdout from my local terminal to a web dashboard.&lt;/p&gt; &lt;p&gt;Left: Raw terminal stream.&lt;/p&gt; &lt;p&gt;Right: It parses &amp;quot;Thoughts&amp;quot; vs &amp;quot;Logs&amp;quot; into a clean timeline.&lt;/p&gt; &lt;p&gt;Features: I added a &amp;quot;Pause&amp;quot; button that actually sends a signal back to the local process to halt execution if the agent starts hallucinating.&lt;/p&gt; &lt;p&gt;Is this something you'd use? Any features you would like to see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durst123"&gt; /u/Durst123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8rqdptuwh66g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi75j1/got_tired_of_staring_at_raw_logs_while_my_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi75j1/got_tired_of_staring_at_raw_logs_while_my_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T13:16:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi3aah</id>
    <title>ZAI Open Source AutoGLM --A AI Phone Agent</title>
    <updated>2025-12-09T09:36:03+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/AutoGLM-Phone-9B"&gt;https://huggingface.co/zai-org/AutoGLM-Phone-9B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/zai-org/Open-AutoGLM"&gt;https://github.com/zai-org/Open-AutoGLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T09:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1phz8vy</id>
    <title>The Absurdity of the prices of consumer RAM versus ECC RAM</title>
    <updated>2025-12-09T05:21:16+00:00</updated>
    <author>
      <name>/u/Substantial_Cut_9418</name>
      <uri>https://old.reddit.com/user/Substantial_Cut_9418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"&gt; &lt;img alt="The Absurdity of the prices of consumer RAM versus ECC RAM" src="https://b.thumbs.redditmedia.com/qkgwBgpSDc_KLfK_m0RtTj-8qk1N-2lbUTbO4kdymGg.jpg" title="The Absurdity of the prices of consumer RAM versus ECC RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Cut_9418"&gt; /u/Substantial_Cut_9418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phz8vy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T05:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1phzzrq</id>
    <title>Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?</title>
    <updated>2025-12-09T06:04:27+00:00</updated>
    <author>
      <name>/u/__issac</name>
      <uri>https://old.reddit.com/user/__issac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"&gt; &lt;img alt="Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?" src="https://preview.redd.it/5a2im0y2d46g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef5e61c3d9ead02bd600ec3c330ee11fda109956" title="Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(I know Artificial Analysis is suck. But is interesting:)) I think now the hype is almost gone, so I have some question. Benchmark says thier models(even 30b a3b and 4b!) beat gpt4. But what do you think? Please don't tell me &amp;quot;depends on field&amp;quot;. We should compare on overall performance. Because benchmark says it is. Can we now truly replace old flagship closed-source model with a small open model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__issac"&gt; /u/__issac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5a2im0y2d46g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T06:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1piasv8</id>
    <title>MagicQuant - Hybrid Evolution GGUF (TPS boosts, precision gains, full transparency)</title>
    <updated>2025-12-09T15:47:38+00:00</updated>
    <author>
      <name>/u/crossivejoker</name>
      <uri>https://old.reddit.com/user/crossivejoker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a system that evolves &lt;strong&gt;hybrid GGUF quantizations&lt;/strong&gt; to automatically find the best tensor level mix for any model. It‚Äôs called &lt;strong&gt;MagicQuant&lt;/strong&gt;, and the whole idea is simple:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stop guessing quant types. Let the math decide the optimal configuration.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MagicQuant runs survival rounds, epsilon-greedy exploration, precision-loss scoring, TPS benchmarking, and a ton of tensor-group heuristics to evolve better (and sometimes &lt;em&gt;way&lt;/em&gt; better) GGUFs than standard baselines.&lt;/p&gt; &lt;p&gt;And the results so far have been amazing.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Example: Seed-OSS 36B&lt;/h2&gt; &lt;p&gt;This is one of the crazier results I‚Äôve gotten so far.&lt;/p&gt; &lt;p&gt;The best Q4-range baseline was &lt;strong&gt;IQ4_NL&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;19.31 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;27.70 TPS&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.1076% precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MagicQuant evolved a hybrid at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;18.95 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32.00 TPS&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.2709% precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slightly smaller&lt;/li&gt; &lt;li&gt;&lt;strong&gt;+15.5% faster&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~75% LESS precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This hybrid: &lt;a href="https://huggingface.co/magiccodingman/Seed-OSS-36B-Instruct-unsloth-MagicQuant-Hybrid-GGUF"&gt;mxfp4_moe-EHQKOUD-IQ4NL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the kind of thing MagicQuant keeps finding.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;MagicQuant Hybrids for Seed OSS 36B&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model_name&lt;/th&gt; &lt;th&gt;file_size_gb&lt;/th&gt; &lt;th&gt;bench_tps&lt;/th&gt; &lt;th&gt;avg_prec_loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-HK-B16-EO-Q5K-QUD-Q8_0&lt;/td&gt; &lt;td&gt;39.71&lt;/td&gt; &lt;td&gt;17.73&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.0213%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-O-MXFP4-EHQKUD-Q8_0&lt;/td&gt; &lt;td&gt;35.78&lt;/td&gt; &lt;td&gt;18.72&lt;/td&gt; &lt;td&gt;0.0272%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-E-B16-D-IQ4NL-KOU-Q6K-HQ-Q8_0&lt;/td&gt; &lt;td&gt;28.02&lt;/td&gt; &lt;td&gt;24.27&lt;/td&gt; &lt;td&gt;0.1768%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-EHQKOUD-Q6K&lt;/td&gt; &lt;td&gt;27.63&lt;/td&gt; &lt;td&gt;23.34&lt;/td&gt; &lt;td&gt;0.2037%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;mxfp4_moe-EHQKOUD-IQ4NL&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;18.95&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;32.00&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.2709%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-HQKU-IQ4NL-EOD-MXFP4&lt;/td&gt; &lt;td&gt;18.66&lt;/td&gt; &lt;td&gt;26.90&lt;/td&gt; &lt;td&gt;0.7098%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MXFP4_MOE&lt;/td&gt; &lt;td&gt;17.90&lt;/td&gt; &lt;td&gt;20.46&lt;/td&gt; &lt;td&gt;2.7338%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Baseline Reference (for comparison)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model_name&lt;/th&gt; &lt;th&gt;file_size_gb&lt;/th&gt; &lt;th&gt;bench_tps&lt;/th&gt; &lt;th&gt;avg_prec_loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;BF16&lt;/td&gt; &lt;td&gt;67.35&lt;/td&gt; &lt;td&gt;11.48&lt;/td&gt; &lt;td&gt;0.0000%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q8_0&lt;/td&gt; &lt;td&gt;35.78&lt;/td&gt; &lt;td&gt;17.77&lt;/td&gt; &lt;td&gt;0.0272%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q6_K&lt;/td&gt; &lt;td&gt;27.63&lt;/td&gt; &lt;td&gt;22.95&lt;/td&gt; &lt;td&gt;0.2037%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q5_K&lt;/td&gt; &lt;td&gt;23.84&lt;/td&gt; &lt;td&gt;22.04&lt;/td&gt; &lt;td&gt;0.2923%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ4_NL&lt;/td&gt; &lt;td&gt;19.31&lt;/td&gt; &lt;td&gt;27.70&lt;/td&gt; &lt;td&gt;1.1076%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MXFP4_MOE&lt;/td&gt; &lt;td&gt;17.90&lt;/td&gt; &lt;td&gt;20.46&lt;/td&gt; &lt;td&gt;2.7338%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q4_K_M&lt;/td&gt; &lt;td&gt;20.27&lt;/td&gt; &lt;td&gt;26.65&lt;/td&gt; &lt;td&gt;2.9161%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;MagicQuant compares everything against these to determine the ‚Äúwinner.‚Äù&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What MagicQuant keeps discovering&lt;/h2&gt; &lt;p&gt;Different architectures respond to quantization very differently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some &lt;em&gt;love&lt;/em&gt; MXFP4.&lt;/li&gt; &lt;li&gt;Some prefer IQ4_NL.&lt;/li&gt; &lt;li&gt;Some models randomly explode in quality on Q5_K.&lt;/li&gt; &lt;li&gt;Seed-OSS ditched most baselines entirely.&lt;/li&gt; &lt;li&gt;Apriel 1.5-15B? That model is a complete gremlin, it loves &lt;strong&gt;Q5_K&lt;/strong&gt; more than anything else I‚Äôve thrown at it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MagicQuant isn‚Äôt about producing hybrids for the sake of hybrids. &lt;strong&gt;MagicQuant is the verdict, whatever wins stays.&lt;/strong&gt; Sometimes that‚Äôs a hybrid. Sometimes the baseline reigns king. Sometimes Q6_K beats Q8_0 in both TPS and precision. Sometimes Q4_K_M outperforms IQ4_NL on &lt;em&gt;certain models.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Everything depends on the architecture.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Philosophically&lt;/h2&gt; &lt;p&gt;I‚Äôm honestly tired of downloading Q8/Q6/Q5/Q4 files with &lt;strong&gt;no benchmarks&lt;/strong&gt;. If a quant is bigger, slower, &lt;em&gt;and&lt;/em&gt; more precision loss, why use it? If a smaller quant loses 5% precision, I want to &lt;strong&gt;see that number&lt;/strong&gt; before downloading.&lt;/p&gt; &lt;p&gt;MagicQuant is my attempt at making quantization:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;empirical&lt;/li&gt; &lt;li&gt;transparent&lt;/li&gt; &lt;li&gt;repeatable&lt;/li&gt; &lt;li&gt;and actually &lt;em&gt;useful&lt;/em&gt; for the community&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Every model will always include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmark TPS&lt;/li&gt; &lt;li&gt;precision loss scoring&lt;/li&gt; &lt;li&gt;file size&lt;/li&gt; &lt;li&gt;the full hybrid naming breakdown&lt;/li&gt; &lt;li&gt;data sets&lt;/li&gt; &lt;li&gt;methodology&lt;/li&gt; &lt;li&gt;raw results&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything is open and reproducible.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;HuggingFace Collection&lt;/h2&gt; &lt;p&gt;All MagicQuant releases live here: &lt;a href="https://huggingface.co/collections/magiccodingman/magic-quant"&gt;https://huggingface.co/collections/magiccodingman/magic-quant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More hybrids are already in the pipeline.&lt;/p&gt; &lt;p&gt;Right now a dense 4B model takes ~2-3 hours to run. A 30B MOE takes ~24 hours (MOE takes ~double as long due to sensitivity). My prediction engine has to build sample data until confidence is high enough that it can properly predict hybrids. Some models are easier than others. Sine dense models need only 46-55 samples, while others need 120 samples, while some need more or less. The engine figures that out.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Documentation / Wiki&lt;/h2&gt; &lt;p&gt;Full documentation, philosophy, naming scheme, methodology, and technical breakdown: &lt;a href="https://github.com/magiccodingman/MagicQuant-Wiki"&gt;https://github.com/magiccodingman/MagicQuant-Wiki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MagicQuant is still evolving, but the results so far have been extremely promising and the more models I run, the weirder and more interesting the quantization patterns become.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;But if you have any suggestions, requests for MagicQuant models, holes to poke, I'm all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossivejoker"&gt; /u/crossivejoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1phjxca</id>
    <title>I'm calling these people out right now.</title>
    <updated>2025-12-08T18:21:39+00:00</updated>
    <author>
      <name>/u/WeMetOnTheMountain</name>
      <uri>https://old.reddit.com/user/WeMetOnTheMountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For being heroes of the community.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;|Blazing fast fine-tuning + premium GGUF quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mradermacher&lt;/strong&gt;|Quantizes literally EVERYTHING, absolute machine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bartowski&lt;/strong&gt;|High-quality quants, great documentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TheBloke&lt;/strong&gt;|The OG - before he stepped back, he was THE source&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoneStriker&lt;/strong&gt;|Solid AWQ/GPTQ quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nexesenex&lt;/strong&gt;|iMatrix quants, gap hunter and filler&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everyone here owes so much to you folks. Take a bow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WeMetOnTheMountain"&gt; /u/WeMetOnTheMountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi4yr4</id>
    <title>GLM-4.6V Model Now Available in GGUF Format</title>
    <updated>2025-12-09T11:20:33+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"&gt; &lt;img alt="GLM-4.6V Model Now Available in GGUF Format" src="https://external-preview.redd.it/DmQpOneG32bl0j63UZH5xIwLDgq-lgYKNllu4rNGOIU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1491231818dca9afc91a1c8bbef01c666b2238d" title="GLM-4.6V Model Now Available in GGUF Format" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently came across the GGUF version of the popular GLM-4.6V Flash model. I shared this as this will be useful to many who want to try this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T11:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1phn925</id>
    <title>Thoughts?</title>
    <updated>2025-12-08T20:25:29+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt; &lt;img alt="Thoughts?" src="https://preview.redd.it/j6fp9xhsh16g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2979be36927eb9e804221b6247830706ea9e7487" title="Thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting take&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6fp9xhsh16g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T20:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9wpt</id>
    <title>Devstral 2 and Mistral Vibe CLI released.</title>
    <updated>2025-12-09T15:13:18+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9wpt/devstral_2_and_mistral_vibe_cli_released/"&gt; &lt;img alt="Devstral 2 and Mistral Vibe CLI released." src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Devstral 2 and Mistral Vibe CLI released." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9wpt/devstral_2_and_mistral_vibe_cli_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9wpt/devstral_2_and_mistral_vibe_cli_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:13:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1piabn8</id>
    <title>Devstral-Small-2-24B-Instruct-2512 on Hugging Face</title>
    <updated>2025-12-09T15:29:19+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt; &lt;img alt="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" src="https://external-preview.redd.it/9AtiZkI9TGGX4HUjb1yXt2_pTwLgjJScmnM7q3ZVgpw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e98ba88a60b440a866a778f583a15081cf6838a4" title="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1phujwo</id>
    <title>Check on lil bro</title>
    <updated>2025-12-09T01:25:42+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt; &lt;img alt="Check on lil bro" src="https://preview.redd.it/s8rfm29bz26g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99684b39e5571f190bf37b141c34049e9f79cc1" title="Check on lil bro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s8rfm29bz26g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T01:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi8z74</id>
    <title>Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found</title>
    <updated>2025-12-09T14:35:50+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt; &lt;img alt="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" src="https://preview.redd.it/h9d1fvb7w66g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cd2f0eef8f43dfd10528e31cb2b9efd46b87bfb" title="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.&lt;/p&gt; &lt;p&gt;Used GPT-OSS 120B as teacher to generate 10k synthetic training examples per task. Fine-tuned everything with identical settings: LoRA rank 64, 4 epochs, 5e-5 learning rate.&lt;/p&gt; &lt;p&gt;Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #1: Tunability (which models improve most)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smallest models showed the biggest gains from fine-tuning. Llama-3.2-1B ranked #1 for tunability, followed by Llama-3.2-3B and Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;This pattern makes sense - smaller models start weaker but have more room to grow. Fine-tuning closed the gap hard. The 8B models ranked lowest for tunability not because they're bad, but because they started strong and had less room to improve.&lt;/p&gt; &lt;p&gt;If you're stuck with small models due to hardware constraints, this is good news. Fine-tuning can make a 1B model competitive with much larger models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #2: Best fine-tuned performance (can student match teacher?)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 came out on top for final performance. After fine-tuning, it matched or exceeded the 120B teacher on 7 out of 8 benchmarks.&lt;/p&gt; &lt;p&gt;Breakdown: TREC (+3 points), Docs (+2), Ecommerce (+3), HotpotQA (tied), Mental Health (+1), Roman Empire (+5). Only fell short on Banking77 by 3 points.&lt;/p&gt; &lt;p&gt;SQuAD 2.0 was wild - the 4B student scored 0.71 vs teacher's 0.52. That's a 19 point gap favoring the smaller model. A model 30x smaller outperforming the one that trained it.&lt;/p&gt; &lt;p&gt;Before fine-tuning, the 8B models dominated everything. After fine-tuning, model size mattered way less.&lt;/p&gt; &lt;p&gt;If you're running stuff on your own hardware, you can get frontier-level performance from a 4B model on a single consumer GPU. No expensive cloud instances. No API rate limits.&lt;/p&gt; &lt;p&gt;Let us know if there's a specific model you want benchmarked.&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9d1fvb7w66g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T14:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9q3t</id>
    <title>Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI</title>
    <updated>2025-12-09T15:05:54+00:00</updated>
    <author>
      <name>/u/YanderMan</name>
      <uri>https://old.reddit.com/user/YanderMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt; &lt;img alt="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YanderMan"&gt; /u/YanderMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
