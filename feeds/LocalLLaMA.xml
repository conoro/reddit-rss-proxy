<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-03T09:34:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nw6ot2</id>
    <title>Granite-4.0 running on latest Qualcomm NPUs (with benchmarks)</title>
    <updated>2025-10-02T15:19:48+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw6ot2/granite40_running_on_latest_qualcomm_npus_with/"&gt; &lt;img alt="Granite-4.0 running on latest Qualcomm NPUs (with benchmarks)" src="https://external-preview.redd.it/MjFtOWFkMXV0cHNmMdg4lbHLbhrLkzDfVtbcBjXS_Swv3usnXgduLh9snYEo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5daac987a1c61e5c98d567054d66d0a6afc418ab" title="Granite-4.0 running on latest Qualcomm NPUs (with benchmarks)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all â€” Iâ€™m Alan from Nexa AI. Granite-4.0 just dropped, and we got &lt;strong&gt;Granite-4.0-Micro (3B)&lt;/strong&gt; running on NPU from Qualcommâ€™s newest platforms (Day-0 support!)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Snapdragon X2 Elite PCs&lt;/li&gt; &lt;li&gt;Snapdragon 8 Elite Gen 5 smartphones&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It also works on CPU/GPU through the same SDK. Here are some early benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;X2 Elite NPU â€” 36.4 tok/s&lt;/li&gt; &lt;li&gt;8 Elite Gen 5 NPU â€” 28.7 tok/s&lt;/li&gt; &lt;li&gt;X Elite CPU â€” 23.5 tok/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what people think about running Granite on NPU.&lt;br /&gt; Follow along if youâ€™d like to see more models running on NPU â€” and would love your feedback.&lt;br /&gt; ðŸ‘‰ GitHub: &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;github.com/NexaAI/nexa-sdk&lt;/a&gt; If you have a Qualcomm Snapdragon PC, you can run Granite 4 directly on NPU/GPU/CPU using NexaSDK.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a7zdec1utpsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw6ot2/granite40_running_on_latest_qualcomm_npus_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw6ot2/granite40_running_on_latest_qualcomm_npus_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T15:19:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw74ec</id>
    <title>We built this open-source LLM Inference project to boost context generation by up to 15x and now it is being implemented by NVIDIA Dynamo!</title>
    <updated>2025-10-02T15:35:52+00:00</updated>
    <author>
      <name>/u/ExplanationEven9787</name>
      <uri>https://old.reddit.com/user/ExplanationEven9787</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, our team has been working nonstop on our open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and recently it has been implemented by NVIDIA's Inference project Dyanamo.&lt;/p&gt; &lt;p&gt;In LLM serving, often when processing large documents, KV Cache context gets overwhelmed and begins to evict precious context requiring the model to reprocess context resulting in much slower speeds. With LMCache, KV Caches get stored outside of just the high bandwidth memory into places like DRAM, disk, or other storages available. &lt;/p&gt; &lt;p&gt;Ask us anything! We would love it if you check us out, we recently hit 5,000 stars on GitHub and want to continue our growth!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/LMCache/LMCache"&gt;https://github.com/LMCache/LMCache&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Early industry adopters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OSS projects: vLLM production stack, Redhat llm-d, KServe, Nvidia Dynamo.&lt;/li&gt; &lt;li&gt;Commercial: Bloomberg, AWS, Tencent, Redis, BentoML, Weka, FlowGPT, GMI, â€¦&lt;/li&gt; &lt;li&gt;Work in progress: Character AI, GKE, Cohere, Baseten, Novita, â€¦&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full Technical Report:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmcache.ai/tech_report.pdf"&gt;https://lmcache.ai/tech_report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplanationEven9787"&gt; /u/ExplanationEven9787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw74ec/we_built_this_opensource_llm_inference_project_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw74ec/we_built_this_opensource_llm_inference_project_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw74ec/we_built_this_opensource_llm_inference_project_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T15:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwnfpm</id>
    <title>Let's talk about practical implementation and actually doing something useful at scale and or multi-running distributed processes with efficacy</title>
    <updated>2025-10-03T02:34:52+00:00</updated>
    <author>
      <name>/u/Plus_Emphasis_8383</name>
      <uri>https://old.reddit.com/user/Plus_Emphasis_8383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The average AI / LLM user is ad-hoc pasting things into GPT, Claude, etc and doing basic vibe coding, discussion, or surprisingly these days as a conversationalist.&lt;/p&gt; &lt;p&gt;However, we then see big orgs or even startups doing things like generative gaming worlds, minecraft, battling against each other, etc&lt;/p&gt; &lt;p&gt;How are these orgs constructing these at scale ?&lt;/p&gt; &lt;p&gt;To be blunt I can't even get an LLM to write a basic script half the time right without egregious prompting and a lot of hand holding&lt;/p&gt; &lt;p&gt;How are people getting it to write entire books, research vast topics, etcetera&lt;/p&gt; &lt;p&gt;How does this work ? The idea these just run unmitigated for days self resolving and more importantly even remotely staying on task is absurd to me given the prior&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plus_Emphasis_8383"&gt; /u/Plus_Emphasis_8383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnfpm/lets_talk_about_practical_implementation_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnfpm/lets_talk_about_practical_implementation_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnfpm/lets_talk_about_practical_implementation_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T02:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwql4a</id>
    <title>DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder (Delivers 14.8Ã— faster inference than the base model)</title>
    <updated>2025-10-03T05:24:33+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This also seems to work with image diffusion models. Could it be used for LLM diffusion models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hanlab.mit.edu/projects/dc-videogen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwql4a/dcvideogen_efficient_video_generation_with_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwql4a/dcvideogen_efficient_video_generation_with_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T05:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwpikr</id>
    <title>Couldnâ€™t find an app to fix grammar/spelling in a whole bookâ€¦ so I built a local CLI for it</title>
    <updated>2025-10-03T04:23:30+00:00</updated>
    <author>
      <name>/u/PanicTasty</name>
      <uri>https://old.reddit.com/user/PanicTasty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been hunting for a simple app that can take an entire document (webnovel/EPUB), run grammar + spelling correction in one go, and give me a cleaned file. Most tools I found were either interactive (great for a paragraph, not 300 pages) or cloud-only.&lt;/p&gt; &lt;p&gt;With help from ChatGPT, I put together a small command-line tool that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chunks a Markdown file by paragraphs&lt;/li&gt; &lt;li&gt;Sends each chunk to a local LLM (LM Studio; Iâ€™m using Qwen3-4B Instruct for speed)&lt;/li&gt; &lt;li&gt;Corrects grammar and spelling while preserving wording/Markdown&lt;/li&gt; &lt;li&gt;Streams progress, writes partial output/checkpoints, and resumes if interrupted&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Itâ€™s already very useful on webnovels with rough grammar or weak machine translations and massively lowers friction when reading.&lt;/p&gt; &lt;p&gt;Iâ€™m genuinely surprised I had to roll this myself, simple as it is. What deceptively simple programs have you ended up building because you thought, surely someoneâ€™s already made this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PanicTasty"&gt; /u/PanicTasty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpikr/couldnt_find_an_app_to_fix_grammarspelling_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpikr/couldnt_find_an_app_to_fix_grammarspelling_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpikr/couldnt_find_an_app_to_fix_grammarspelling_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T04:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwpxxx</id>
    <title>Performance wise what is the best backend right now?</title>
    <updated>2025-10-03T04:47:32+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently I'm using mostly ollama and sometimes the transformers library, ollama is really nice allowing me to focus on the code instead of configure model and manager memory and gpu load, while transformers takes more work.&lt;/p&gt; &lt;p&gt;Any other frameworks I should test, specially one that offer more performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpxxx/performance_wise_what_is_the_best_backend_right/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpxxx/performance_wise_what_is_the_best_backend_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpxxx/performance_wise_what_is_the_best_backend_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T04:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw60fj</id>
    <title>Open source speech foundation model that runs locally on CPU in real-time</title>
    <updated>2025-10-02T14:54:43+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt; &lt;img alt="Open source speech foundation model that runs locally on CPU in real-time" src="https://external-preview.redd.it/3w13BgLMXQ4-v0J3QSPqnnHAcC8U3HjNheDu4QFAWrk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bfeb812f14c0b82e510265b807d168b2af385bc" title="Open source speech foundation model that runs locally on CPU in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1nw60fj/video/3kh334ujppsf1/player"&gt;https://reddit.com/link/1nw60fj/video/3kh334ujppsf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weâ€™ve just released Neuphonic TTS Air, a lightweight open-source speech foundation model under Apache 2.0.&lt;/p&gt; &lt;p&gt;The main idea: frontier-quality text-to-speech, but small enough to run in realtime on CPU. No GPUs, no cloud APIs, no rate limits.&lt;/p&gt; &lt;p&gt;Why we built this: - Most speech models today live behind paid APIs â†’ privacy tradeoffs, recurring costs, and external dependencies. - With Air, you get full control, privacy, and zero marginal cost. - It enables new use cases where running speech models on-device matters (edge compute, accessibility tools, offline apps).&lt;/p&gt; &lt;p&gt;Git Repo: &lt;a href="https://github.com/neuphonic/neutts-air"&gt;https://github.com/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;https://huggingface.co/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from on performance, applications, and contributions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwc1oc</id>
    <title>Apertus model implementation has been merged into llama.cpp</title>
    <updated>2025-10-02T18:37:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"&gt; &lt;img alt="Apertus model implementation has been merged into llama.cpp" src="https://external-preview.redd.it/WBeE9GPvyJdOEySnojQ_o2A9ys0na0K0XH7uI9iyd_o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23de1c1602b1561cace347dd342baae689fd7c5c" title="Apertus model implementation has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think Piotr can now fully focus on Qwen Next ;)&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;Apertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models. The model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509"&gt;https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509"&gt;https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15852"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T18:37:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvzeuh</id>
    <title>Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance</title>
    <updated>2025-10-02T09:47:49+00:00</updated>
    <author>
      <name>/u/ShinobuYuuki</name>
      <uri>https://old.reddit.com/user/ShinobuYuuki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt; &lt;img alt="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" src="https://external-preview.redd.it/NzRlNXJuc3A2b3NmMe-uhlatbqnQI0WkANIEyFuJlq6CEOqVOtkO0hhCMPfO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=152efa7692053a33bdf74c0824752b907d2becc8" title="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm Yuuki from the Jan team.&lt;/p&gt; &lt;p&gt;Weâ€™ve been working on some updates for a while. We released Jan v0.7.0. I'd like to quickly share what's new:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp improvements&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan now automatically optimizes llama.cpp settings (e.g. context size, gpu layers) based on your hardware. So your models run more efficiently. It's an experimental feature&lt;/li&gt; &lt;li&gt;You can now see some stats (how much context is used, etc.) when the model runs&lt;/li&gt; &lt;li&gt;Projects is live now. You can organize your chats using it - it's pretty similar to ChatGPT&lt;/li&gt; &lt;li&gt;You can rename your models in Settings&lt;/li&gt; &lt;li&gt;Plus, we're also improving Jan's cloud capabilities: Model names update automatically - so no need to manually add cloud models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't seen it yet: Jan is an open-source ChatGPT alternative. It runs AI models locally and lets you add agentic capabilities &lt;a href="https://www.jan.ai/docs/desktop/mcp#configure-and-use-mcps-within-jan"&gt;through MCPs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/menloresearch/jan"&gt;https://github.com/menloresearch/jan&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShinobuYuuki"&gt; /u/ShinobuYuuki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/49h5xlsp6osf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T09:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwnhe6</id>
    <title>On the new test-time compute inference paradigm (Long post but worth it)</title>
    <updated>2025-10-03T02:37:12+00:00</updated>
    <author>
      <name>/u/omagdy7</name>
      <uri>https://old.reddit.com/user/omagdy7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hope this discussion is appropriate for this sub&lt;/p&gt; &lt;p&gt;So while I wouldn't consider my self someone knowledgeable in the field of AI/ML I would just like to share this thought and ask the community here if it holds water.&lt;/p&gt; &lt;p&gt;So the new Test-Time compute paradigm(o1/o3 like models) feels like symbolic AI's combinatorial problem dressed in GPUs. Symbolic AI attempts mostly hit a wall because brute search scales exponentially and pruning the tree of possible answers needed careful hard coding for every domain to get any tangible results. So I feel like we may be just burning billions in AI datacenters to rediscover that law with fancier hardware.&lt;/p&gt; &lt;p&gt;The reason however I think TTC have had a better much success because it has a good prior of pre-training it seems like Symbolic AI with very good general heuristic for most domains. So if your prompt/query is in-distribution which makes pruning unlikely answers very easy because they won't be even top 100 answers, but if you are OOD the heuristic goes flat and you are back to exponential land.&lt;/p&gt; &lt;p&gt;That's why we've seen good improvements for code and math which I think is due to the fact that they are not only easily verifiable but we already have tons of data and even more synthetic data could be generated meaning any query you will ask you will likely be in in-distribution.&lt;/p&gt; &lt;p&gt;If I probably read more about how these kind of models are trained I think I would have probably a better or more deeper insight but this is me just thinking philosophically more than empirically. I think what I said though could be easily empirically tested though maybe someone already did and wrote a paper about it.&lt;/p&gt; &lt;p&gt;In a way also the solution to this problem is kind of like the symbolic AI problem but instead of programmers hand curating clever ways to prune the tree the solution the current frontier labs are probably employing is feeding more data into the domain you want the model to be better at for example I hear a lot about frontier labs hiring professionals to generate more data in their domain of expertise. but if we are just fine-tuning the model with extra data for each domain akin to hand curating ways to prune the tree in symbolic AI it feels like we are re-learning the mistakes of the past with a new paradigm. And it also means that the underlying system isn't general enough.&lt;/p&gt; &lt;p&gt;If my hypothesis is true it means AGI is no where near and what we are getting is a facade of intelligence. that's why I like benchmarks like ARC-AGI because it truly tests actually ways that the model can figure out new abstractions and combine them o3-preview has showed some of that but ARC-AGI-1 was very one dimensional it required you to figure out 1 abstraction/rule and apply it which is a progress but ARC-AGI-2 evolved and you now need to figure out multiple abstractions/rules and combine them and most models today doesn't surpass 17% and at a very high computation cost as well. you may say at least there is progress but I would counter if it needed 200$ per task as o3-preview to figure out only 1 rule and apply it I feel like the compute will grow exponentially if it's 2 or 3 or n rules that needed to solve the task at hand and we are back to some sort of another combinatoric explosion and we really don't know how OpenAI achieved this the creators of the test admitted that some of ARC-AGI-1 tasks are susceptible to brute force so that could mean the OpenAI produced Millions of synthetic data of ARC-1 like tasks trying to predict the test in the private eval but we can't be sure and I won't take it away from them that it was impressive and it signaled that what they are doing is at least different from pure auto regressive LLMs but the questions remains are what they are doing linear-ally scaleable or exponentially scaleable for example in the report that ARC-AGI shared post the breakthrough it showed that a generation of 111M tokens yielded 82.7% accuracy and a generation of 9.5B yes a B as in Billion yielded 91.5% aside from how much that cost which is insane but almost 10X the tokens yielded 8.7% improvement that doesn't look linear to me.&lt;/p&gt; &lt;p&gt;I don't work in a frontier lab but from what I feel they don't have a secret sauce because open source isn't really that far ahead. they just have more compute to try out more experiments than open source could they find a break through they might but I've watched a lot of podcasts from people working and OpenAI and Claude and they are all very convinced that &amp;quot;Scale Scale Scale is all you need&amp;quot; and really betting on emergent behaviors.&lt;/p&gt; &lt;p&gt;And using RL post training is the new Scaling they are trying to max and don't get me wrong it will yield better models for the domains that can benefit from an RL environment which are math and code but if what the labs are make are another domain specific AI and that's what they are marketing fair, but Sam talks about AGI in less than 1000 days like maybe 100 days ago and Dario believes the it's in the end of the Next year.&lt;/p&gt; &lt;p&gt;What makes me bullish even more about the AGI timeline is that I am 100% sure that when GPT-4 came they weren't experimenting with test-time compute because why else would they train the absolute monster of GPT4.5 probably the biggest deep learning model of its kind by their words it was so slow and not at all worth it for coding or math and they tried to market it as more empathetic AI or it's linguistically intelligent. So does Anthropic they were fairly late to the whole thinking paradigm game and I would say they still are behind OpenAI by good margins when it comes to this new paradigm which also means they were also betting on purely scaling LLMs as well, But I am fair enough that this is more speculative than facts so you can dismiss this.&lt;/p&gt; &lt;p&gt;I really hope you don't dismiss my criticism as me being an AI hater I feel like I am asking the questions that matter and I don't think dogma has been any helpful in science specially in AI.&lt;/p&gt; &lt;p&gt;BTW I have no doubt that AI as a tool will keep getting better and maybe even being somewhat economically valuable in the upcoming years but its role will be like that of how excel is very valuable to businesses today which is pretty big don't get me wrong but it's no where near what they promise of AI scientific discovery explosion or curing cancer or proving new math.&lt;/p&gt; &lt;p&gt;What do you think of this hypothesis? am I out of touch and need to learn more about this new paradigm and how they learn and I am sort of steel manning an assumption of how this new paradigm works?&lt;/p&gt; &lt;p&gt;I am really hopeful for a fruitful discussion specially for those who disagree with my narrative&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omagdy7"&gt; /u/omagdy7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnhe6/on_the_new_testtime_compute_inference_paradigm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnhe6/on_the_new_testtime_compute_inference_paradigm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnhe6/on_the_new_testtime_compute_inference_paradigm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T02:37:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwogkl</id>
    <title>Sloppiest model!?</title>
    <updated>2025-10-03T03:27:03+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Odd request, but can anyone share the sloppiest models they have tried? I'm trying to generate data with as much AI slop (it's not thisâ€“its that / shivers-down-spines / emojis / bulleted lists / testaments &amp;amp; tapestries /etc) as possible.&lt;/p&gt; &lt;p&gt;EDIT: Thanks for the input guys! I think I found the model (Original versions of Qwen3 14B / 30BA3B with /no_think seems to do a great job :D)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T03:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw8jbn</id>
    <title>Ring Flash 2.0 104B A6B with Linear Attention released a few days ago</title>
    <updated>2025-10-02T16:28:11+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"&gt; &lt;img alt="Ring Flash 2.0 104B A6B with Linear Attention released a few days ago" src="https://external-preview.redd.it/arTReyF0GyVAVaEDNDlfVvJyFYJ0q7EWSfcCybSgWt0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af2d40f335ae44a32d6d45dc30487a7c16511fa4" title="Ring Flash 2.0 104B A6B with Linear Attention released a few days ago" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2ghd</id>
    <title>GLM 4.6 is nice</title>
    <updated>2025-10-02T12:31:10+00:00</updated>
    <author>
      <name>/u/theodordiaconu</name>
      <uri>https://old.reddit.com/user/theodordiaconu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bit the bullet and sacrificed 3$ (lol) for a &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; subscription as I can't run this behemoth locally. And because I'm a very generous dude I wanted them to keep the full margin instead of going through routers.&lt;/p&gt; &lt;p&gt;For convenience, I created a simple 'glm' bash script that starts claude with env variables (that point to z.ai). I type glm and I'm locked in.&lt;/p&gt; &lt;p&gt;Previously I experimented a lot with OW models with GPT-OSS-120B, GLM 4.5, KIMI K2 0905, Qwen3 Coder 480B (and their latest variant included which is only through 'qwen' I think) honestly they were making silly mistakes on the project or had trouble using agentic tools (many failed edits) and abandoned their use quickly in favor of the king: gpt-5-high. I couldn't even work with Sonnet 4 unless it was frontend.&lt;/p&gt; &lt;p&gt;This specific project I tested it on is an open-source framework I'm working on, and it's not very trivial to work on a framework that wants to adhere to 100% code coverage for every change, every little addition/change has impacts on tests, on documentation on lots of stuff. Before starting any task I have to feed the whole documentation.&lt;/p&gt; &lt;p&gt;GLM 4.6 is in another class for OW models. I felt like it's an equal to GPT-5-high and Claude 4.5 Sonnet. Ofcourse this is an early vibe-based assessment, so take it with a grain of sea salt.&lt;/p&gt; &lt;p&gt;Today I challenged them (Sonnet 4.5, GLM 4.6) to refactor a class that had 600+ lines. And I usually have bad experiences when asking for refactors with all models.&lt;/p&gt; &lt;p&gt;Sonnet 4.5 could not make it reach 100% on its own after refactor, started modifying existing tests and sort-of found a silly excuse for not reaching 100% it stopped at 99.87% and said that it's the testing's fault (lmao).&lt;/p&gt; &lt;p&gt;Now on the other hand, GLM 4.6, it worked for 10 mins I think?, ended up with a perfect result. It understood the assessment. They both had interestingly similar solutions to refactoring, so planning wise, both were good and looked like they really understood the task. I never leave an agent run without reading its plan first.&lt;/p&gt; &lt;p&gt;I'm not saying it's better than Sonnet 4.5 or GPT-5-High, I just tried it today, all I can say for a fact is that it's a different league for open weight, perceived on this particular project.&lt;/p&gt; &lt;p&gt;Congrats &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;&lt;br /&gt; What OW models do you use for coding?&lt;/p&gt; &lt;p&gt;LATER_EDIT: the 'bash' script since a few asked in ~/.local/bin on Mac: &lt;a href="https://pastebin.com/g9a4rtXn"&gt;https://pastebin.com/g9a4rtXn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theodordiaconu"&gt; /u/theodordiaconu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwgma3</id>
    <title>A Summary of Key AI Events from September 2025</title>
    <updated>2025-10-02T21:27:57+00:00</updated>
    <author>
      <name>/u/nh_local</name>
      <uri>https://old.reddit.com/user/nh_local</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;ByteDance released &lt;strong&gt;Seedream 4.0&lt;/strong&gt;, a next-generation image model unifying high-quality text-to-image generation and natural-language image editing.&lt;/li&gt; &lt;li&gt;An advanced Gemini variant, reported as &lt;strong&gt;Gemini 2.5 - Deep Think&lt;/strong&gt;, achieved gold-medal-level performance at the ICPC World Finals programming contest.&lt;/li&gt; &lt;li&gt;OpenAI reported a reasoning and code model achieved a perfect score (12/12) in ICPC testing.&lt;/li&gt; &lt;li&gt;Suno released &lt;strong&gt;Suno v5&lt;/strong&gt;, an upgrade in music generation with studio-grade fidelity and more natural-sounding vocals.&lt;/li&gt; &lt;li&gt;Alibaba unveiled &lt;strong&gt;Qwen-3-Max&lt;/strong&gt;, its flagship model with over a trillion parameters, focusing on long context and agent capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wan 2.5&lt;/strong&gt; was released, a generative video model focused on multi-shot consistency and character animation.&lt;/li&gt; &lt;li&gt;Anthropic announced &lt;strong&gt;Claude Sonnet 4.5&lt;/strong&gt;, a model optimized for coding, agent construction, and improved reasoning.&lt;/li&gt; &lt;li&gt;OpenAI released &lt;strong&gt;Sora 2&lt;/strong&gt;, a flagship video and audio generation model with improved physical modeling and synchronized sound.&lt;/li&gt; &lt;li&gt;DeepSeek released &lt;strong&gt;DeepSeek-V3.2-Exp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;OpenAI and NVIDIA announced a strategic partnership for NVIDIA to supply at least &lt;strong&gt;10 gigawatts&lt;/strong&gt; of AI systems for OpenAI's infrastructure.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nh_local"&gt; /u/nh_local &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T21:27:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwpshs</id>
    <title>Granite 4 H Tiny Q8 in RTX 3090, It's a context king.</title>
    <updated>2025-10-03T04:39:05+00:00</updated>
    <author>
      <name>/u/Plotozoario</name>
      <uri>https://old.reddit.com/user/Plotozoario</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing the Granite 4 H Tiny Q8 in the LM Studio, and holy moly, you can set the context window up to 1M and keep solid 50-60 tokens/s using a single RTX 3090 24Gb + 48GB RAM DDR4 3200mhz with Flash attention enabled. How far we come!! &lt;/p&gt; &lt;p&gt;Unfortunately i didn't tested yet the degradation of the model after the 100k tokens.&lt;/p&gt; &lt;p&gt;What is your vision about this new model and its new context management?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plotozoario"&gt; /u/Plotozoario &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T04:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwibsb</id>
    <title>Ming V2 is out</title>
    <updated>2025-10-02T22:37:08+00:00</updated>
    <author>
      <name>/u/Chance_Camp3720</name>
      <uri>https://old.reddit.com/user/Chance_Camp3720</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ming V2 is already out&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance_Camp3720"&gt; /u/Chance_Camp3720 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T22:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw5kkc</id>
    <title>It's been a long time since Google released a new Gemma model.</title>
    <updated>2025-10-02T14:37:55+00:00</updated>
    <author>
      <name>/u/ArcherAdditional2478</name>
      <uri>https://old.reddit.com/user/ArcherAdditional2478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was here using Gemma 3 4B, a model that I can confidently say has so far been the best of its size, something truly usable: itâ€™s super coherent in Portuguese (not just in English and Chinese) and even gives me solid image recognition. It allowed me to process personal stuff without having to throw it into some obscure cloud. After seeing so many amazing releases, but with little focus on being multilingual, I deeply missed seeing Google release a new Gemma. And judging by the pace of AI evolution, itâ€™s been about 35 years since Google last released a new Gemma, letâ€™s be honest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArcherAdditional2478"&gt; /u/ArcherAdditional2478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwimej</id>
    <title>GLM 4.6 Local Gaming Rig Performance</title>
    <updated>2025-10-02T22:50:00+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"&gt; &lt;img alt="GLM 4.6 Local Gaming Rig Performance" src="https://preview.redd.it/0peifhs11ssf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c59d73d412700f4d4389356e00f3193eb466bc1" title="GLM 4.6 Local Gaming Rig Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sad there is no GLM-4.6-Air (seems unlikely it will be released, but who knows). So instead I cooked the &lt;code&gt;ubergarm/GLM-4.6-GGUF&lt;/code&gt; &lt;code&gt;smol-IQ2_KS&lt;/code&gt; 97.990 GiB (2.359 BPW) quant which is just a little bigger than full Q8_0 Air.&lt;/p&gt; &lt;p&gt;It is running well on my local gaming rig with 96GB RAM + 24 GB VRAM. I can get up to 32k context, or can do some trade-offs between PP and TG speeds and context length. &lt;/p&gt; &lt;p&gt;The graph is &lt;code&gt;llama-sweep-bench&lt;/code&gt; showing how quantizing kv-cache gives a steeper drop off on TG for this architecture which I observed similarly in the older GLM-4.5.&lt;/p&gt; &lt;p&gt;Have fun running quants of these big models at home on your gaming rig! The huggingface repo has some metrics comparing quality vs size trade-offs and folks over on AI Beavers Discord have a lot of KLD metrics comparing various available quants from different quant cookers so pick the right size for your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0peifhs11ssf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T22:50:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw8c6y</id>
    <title>Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration</title>
    <updated>2025-10-02T16:20:56+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt; &lt;img alt="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" src="https://external-preview.redd.it/aG1yZ2k0M3Y0cXNmMTjBkk0zpHe1cUKuUpjTdKuc-czjYGWzckCtqtrm-IdD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd6b6a33eda5421f6a81ae5b65f2f068b49e13c" title="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/14cmif4v4qsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw52ad</id>
    <title>Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP</title>
    <updated>2025-10-02T14:18:05+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt; &lt;img alt="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" src="https://external-preview.redd.it/ODh3bjRsOWJpcHNmMcggpjsEMzF-IE1l8vJahmQmeeToARZwc_P-uEOcis7p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491d98e181b37d6e6d0003c442bfc14ebbed0594" title="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwnlp8</id>
    <title>How has everyone been liking Granite 4?</title>
    <updated>2025-10-03T02:43:24+00:00</updated>
    <author>
      <name>/u/SpicyWangz</name>
      <uri>https://old.reddit.com/user/SpicyWangz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does it compare to similar models for you?&lt;/p&gt; &lt;p&gt;So far I've been testing out the 7b model and it's been performing really well on my benchmarks for a model of that size. I think I've found a new go-to model for that class.&lt;/p&gt; &lt;p&gt;The output looks fairly plaintext without much formatting or markdown. I'd probably like to see a little more structure and variation from it, but I prefer plain to the table hell that I've gotten from gpt-oss-20b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpicyWangz"&gt; /u/SpicyWangz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T02:43:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2wd6</id>
    <title>Granite 4.0 Language Models - a ibm-granite Collection</title>
    <updated>2025-10-02T12:51:10+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt; &lt;img alt="Granite 4.0 Language Models - a ibm-granite Collection" src="https://external-preview.redd.it/dG6nrEEPIkS2YfUpzm-ii0PPK1xkTA3ZMcynqcTCXQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374e83fed526e7600d653259e65b30be13801c21" title="Granite 4.0 Language Models - a ibm-granite Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite 4, &lt;strong&gt;32B-A9B, 7B-A1B, and 3B&lt;/strong&gt; dense models available.&lt;/p&gt; &lt;p&gt;GGUF's are in the same repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-language-models-6811a18b820ef362d9e5a82c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwovv8</id>
    <title>Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements</title>
    <updated>2025-10-03T03:49:33+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt; &lt;img alt="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" src="https://preview.redd.it/q7lat3zxjtsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db27566f8b03ab0a4d8599a0ebfc454ee0ea0790" title="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just looking at some of the new model releases and wanted to share a quick comparison I made that really shows how fast things are moving in the world of open-source LLMs.&lt;/p&gt; &lt;p&gt;I've been tracking and comparing a couple of Mixture of Experts models that have a similar dense and active parameters, in this case a 7B total parameter count with 1B active parameters. With today's Granite release we can compare OLMoE, which came out in January, and the new Granite-4.0-H-Tiny model that just dropped today.&lt;/p&gt; &lt;p&gt;The side-by-side results are pretty wild for just a 10-month difference. The new Granite model is straight-up better on every single metric we can compare. It's not just a small improvement, either. We're talking huge jumps in areas like math, coding, and general knowledge.&lt;/p&gt; &lt;p&gt;Things are advancing really fast, just to give a little more perspective, the new Granite-4.0-H-Tiny has a similar MMLU score to Llama 2 70B that came out on January 2024 but the granite model can run at reasonable speeds even on a potato PC with CPU inference, I still remember the old days when people were happy that Llama 2 70B could run at 2tk/s on their machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q7lat3zxjtsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T03:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwr6sb</id>
    <title>How's granite 4 small 32B going for you?</title>
    <updated>2025-10-03T06:01:18+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice that it's almost twice as fast as my current favorite, SEED OSS 36B. 79 tokens/sec starting from a blank context, but this speed doesn't seem to degrade as you fill up the context. &lt;/p&gt; &lt;p&gt;Accuracy on some hard questions is a little challenging ( less smart than SEED OSS ) but it does good with clarifications.&lt;br /&gt; Output length is short and to the point, doesn't spam you with emojis, fancy formatting or tables ( i like this ) &lt;/p&gt; &lt;p&gt;Memory consumption is extremely low per K of context, I don't understand how i can jack the context up to 512k and run it on a 5090. Memory usage doesn't seem to climb as i fill up the context either.&lt;/p&gt; &lt;p&gt;First impressions are good. There may be something special here. Let me know what your experiences look like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T06:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwkzq7</id>
    <title>Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data</title>
    <updated>2025-10-03T00:36:48+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt; &lt;img alt="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" src="https://external-preview.redd.it/nBFUIJw0Ejvd09O6shC9aA8_DA1taNSIvE_cak2wtlo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac886e4b9ca714a3746fa6d670ba959d7721d3a2" title="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2509.22944"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect â€” The Openâ€‘Source Distributed Training Lab (Thu, Oct 2 â€¢ 10 AM â€“ 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect â€” The Openâ€‘Source Distributed Training Lab (Thu, Oct 2 â€¢ 10 AM â€“ 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect â€” The Openâ€‘Source Distributed Training Lab (Thu, Oct 2 â€¢ 10 AM â€“ 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect â€” Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect â€” Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;Iâ€™m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM â€“ 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
