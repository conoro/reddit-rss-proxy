<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-10T15:39:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lwekp5</id>
    <title>Huawei Pangu LLM Exposed: Bureaucracy and Stolen Credit</title>
    <updated>2025-07-10T14:45:56+00:00</updated>
    <author>
      <name>/u/smithsee</name>
      <uri>https://old.reddit.com/user/smithsee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found a raw GitHub issue &lt;a href="https://github.com/HW-whistleblower/True-Story-of-Pangu/issues/532"&gt;True Story of Pangu #532&lt;/a&gt; from someone claiming to be on Huawei‚Äôs Pangu LLM team. They spill the beans on the project‚Äôs dark side. The team aimed to build a homegrown Chinese AI on Huawei‚Äôs Ascend chips, battling limited compute and tight deadlines to train 135B dense and 718B MoE models from scratch. But internal groups, like a ‚Äúsmall model lab,‚Äù allegedly copied models like Qwen and DeepSeek, claimed Pangu‚Äôs work for awards, and dumped maintenance back on the team. This crushed morale, driving talent away. The author, torn by guilt and anger, calls for a culture that respects tech and people, How common is this kind of drama in AI labs? Any ideas on fostering better R&amp;amp;D environments? Let‚Äôs discuss!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smithsee"&gt; /u/smithsee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwekp5/huawei_pangu_llm_exposed_bureaucracy_and_stolen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwekp5/huawei_pangu_llm_exposed_bureaucracy_and_stolen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwekp5/huawei_pangu_llm_exposed_bureaucracy_and_stolen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T14:45:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvnkuk</id>
    <title>Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!</title>
    <updated>2025-07-09T16:41:42+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt; &lt;img alt="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" src="https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f27c3dcd38f51203dffa703e77dc78a0e131c7" title="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;12B version: &lt;a href="https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3"&gt;https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:41:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqv8e</id>
    <title>new tiny 1.7B open-source reranker beats Cohere rerank3.5</title>
    <updated>2025-07-09T18:48:35+00:00</updated>
    <author>
      <name>/u/ghita__</name>
      <uri>https://old.reddit.com/user/ghita__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"&gt; &lt;img alt="new tiny 1.7B open-source reranker beats Cohere rerank3.5" src="https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a250c04d607c0b8a5f43196eba12971c7744065" title="new tiny 1.7B open-source reranker beats Cohere rerank3.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're looking for a cheap, fast but accurate reranker without having to fine-tune a SLM yourself&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghita__"&gt; /u/ghita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zeroentropy/zerank-1-small"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw8lvt</id>
    <title>Ram Speed importance when exceeding VRAM</title>
    <updated>2025-07-10T09:45:50+00:00</updated>
    <author>
      <name>/u/opoot_</name>
      <uri>https://old.reddit.com/user/opoot_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?&lt;/p&gt; &lt;p&gt;I know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.&lt;/p&gt; &lt;p&gt;But I wanted to ask what happens if the plan is to exceed the vram and use system ram?&lt;/p&gt; &lt;p&gt;If I had the same system, but one had a gpu and one didn‚Äôt, supposing that the gpu didn‚Äôt have enough vram, is there still an appreciable difference in llm performance with the two systems?&lt;/p&gt; &lt;p&gt;Right now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?&lt;/p&gt; &lt;p&gt;In the scenarios I am assuming that the model spills out into the ram either way.&lt;/p&gt; &lt;p&gt;If the llm spills out into the ram, is it cpu inference now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opoot_"&gt; /u/opoot_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T09:45:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw7igq</id>
    <title>Survivalist Edge AI?</title>
    <updated>2025-07-10T08:31:00+00:00</updated>
    <author>
      <name>/u/xibbie</name>
      <uri>https://old.reddit.com/user/xibbie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this thread I want to explore something I don‚Äôt see being covered much: running LLMs on extremely low-power edge devices. &lt;/p&gt; &lt;p&gt;I want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. &lt;/p&gt; &lt;p&gt;The goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp;amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn‚Äôt need to be very high here, but responses would still need to be somewhat realtime. &lt;/p&gt; &lt;p&gt;What would you spec out? I‚Äôm leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xibbie"&gt; /u/xibbie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T08:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwebzq</id>
    <title>GROK 4 IS NOW LIVE ON LMARENA</title>
    <updated>2025-07-10T14:36:09+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmarena.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwebzq/grok_4_is_now_live_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwebzq/grok_4_is_now_live_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T14:36:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw5oco</id>
    <title>Local llms works great!</title>
    <updated>2025-07-10T06:27:51+00:00</updated>
    <author>
      <name>/u/InsideResolve4517</name>
      <uri>https://old.reddit.com/user/InsideResolve4517</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw5oco/local_llms_works_great/"&gt; &lt;img alt="Local llms works great!" src="https://b.thumbs.redditmedia.com/gv1XJzy7IzUkjUhfUJWUZXarplALZeUC0i1uy9Wz9jQ.jpg" title="Local llms works great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using qwen3:14b it works well for my day to day life and reducing my online llm dependencies. Like you can see in both screenshot I got almost equilant result&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideResolve4517"&gt; /u/InsideResolve4517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lw5oco"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw5oco/local_llms_works_great/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw5oco/local_llms_works_great/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T06:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw1qp5</id>
    <title>Fine Tune a smaller LLM for Code generation</title>
    <updated>2025-07-10T02:43:12+00:00</updated>
    <author>
      <name>/u/GlobeAndGeek</name>
      <uri>https://old.reddit.com/user/GlobeAndGeek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;br /&gt; I want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. &lt;/p&gt; &lt;p&gt;Thanks in advance for all the help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlobeAndGeek"&gt; /u/GlobeAndGeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T02:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw6jz5</id>
    <title>Transformers.js vs WebLLM</title>
    <updated>2025-07-10T07:25:10+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;There are two JS libraries, Transformers.js and WebLLM, for embedding language models in a web application. They seems to target different applications, with a significant(?) overlap.&lt;/p&gt; &lt;p&gt;What is your experience with any of these, in terms of efficency, coverage, and precision, for a non-interactive (i.e. not chat with user) application? Does any of them offer better support for more cutting-edge models?&lt;/p&gt; &lt;p&gt;Consider text-summarisation as an example application. Which one is better in providing that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T07:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwafqm</id>
    <title>running local LLM for the first time</title>
    <updated>2025-07-10T11:35:28+00:00</updated>
    <author>
      <name>/u/Routine_Author961</name>
      <uri>https://old.reddit.com/user/Routine_Author961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey guys!&lt;/p&gt; &lt;p&gt;because of privacy conerns and censorship i;ve decided to give local LLM a try.&lt;/p&gt; &lt;p&gt;downloaded studio LM and installed mistarl 7B and so far things are fine. might give ollama a chance as well in the future. &lt;/p&gt; &lt;p&gt;couple of questions:&lt;/p&gt; &lt;p&gt;can the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn't it fully oflline? &lt;/p&gt; &lt;p&gt;do you have any other models that you recommended?&lt;/p&gt; &lt;p&gt;is there a way to &amp;quot;stream&amp;quot; the model to my network so I will be able to acsses and ask things from othe computers? &lt;/p&gt; &lt;p&gt;is there something else i need to know about local LLMs?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Routine_Author961"&gt; /u/Routine_Author961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T11:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwf1t6</id>
    <title>people downvoted me for saying this. but now it is confirmed that grok 4 is just grok 3 + more RL training</title>
    <updated>2025-07-10T15:05:12+00:00</updated>
    <author>
      <name>/u/JP_525</name>
      <uri>https://old.reddit.com/user/JP_525</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwf1t6/people_downvoted_me_for_saying_this_but_now_it_is/"&gt; &lt;img alt="people downvoted me for saying this. but now it is confirmed that grok 4 is just grok 3 + more RL training" src="https://b.thumbs.redditmedia.com/kOEc5DsywMAtUi8IGGhgIWwDumUZiQbruqGYkCjRb7o.jpg" title="people downvoted me for saying this. but now it is confirmed that grok 4 is just grok 3 + more RL training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JP_525"&gt; /u/JP_525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lwf1t6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwf1t6/people_downvoted_me_for_saying_this_but_now_it_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwf1t6/people_downvoted_me_for_saying_this_but_now_it_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T15:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwfn7n</id>
    <title>DeliteAI: Open platform for building and running agents on Mobile</title>
    <updated>2025-07-10T15:28:26+00:00</updated>
    <author>
      <name>/u/Economy-Mud-6626</name>
      <uri>https://old.reddit.com/user/Economy-Mud-6626</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/"&gt; &lt;img alt="DeliteAI: Open platform for building and running agents on Mobile" src="https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a7922aa047ec08e04553915bf0c4265bcd75e35" title="DeliteAI: Open platform for building and running agents on Mobile" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have built an extensible open source platform that enables developers to build, run and integrate AI agents into their applications and deliver AI native experiences all running locally on phones.&lt;/p&gt; &lt;p&gt;The SDK is lightweight built upon Executorch/ONNX and provides a higher level abstraction for developers to integrate in Kotlin or Swift. The AI workflow is orchestrated via Python which is natively supported as part of the on-device SDK. We currently support Llama 3.2 1B, Qwen 3 0.6B (tool-calling), Gemini Nano and soon Gemma 3n.&lt;/p&gt; &lt;p&gt;We have also created an Agent marketplace which provides plug and play agents and would love to get contributions from this community. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NimbleEdge/deliteAI/tree/main/nimblenet_py/simulation_assets"&gt;Here&lt;/a&gt; are some example Python scripts for both traditional ML and AI workloads - note that the Kotlin/Swift layer can invoke these python functions and vice-versa which enables tool calling for both dynamic context and actions in the app.&lt;/p&gt; &lt;p&gt;You can also check out our open-source on-device &lt;a href="https://github.com/NimbleEdge/assistant"&gt;AI assistant&lt;/a&gt; built upon the ‚ÄúDeliteAI‚Äù platform. &lt;/p&gt; &lt;p&gt;We love to hear from you on our APIs and if you would like to contribute please join our Discord community (link in the comment below). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Mud-6626"&gt; /u/Economy-Mud-6626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/NimbleEdge/deliteAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T15:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvn1sd</id>
    <title>OpenAI's open-weight model will debut as soon as next week</title>
    <updated>2025-07-09T16:20:46+00:00</updated>
    <author>
      <name>/u/phantasm_ai</name>
      <uri>https://old.reddit.com/user/phantasm_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt; &lt;img alt="OpenAI's open-weight model will debut as soon as next week" src="https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5aaee471edf64881fedf697cc7cda1494ca5f3cd" title="OpenAI's open-weight model will debut as soon as next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as ‚Äúsimilar to o3 mini,‚Äù complete with the reasoning capabilities that have made OpenAI‚Äôs latest models so powerful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantasm_ai"&gt; /u/phantasm_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw5nxi</id>
    <title>UI/UX Benchmark Update: We've added Grok 4 and more models</title>
    <updated>2025-07-10T06:27:08+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw5nxi/uiux_benchmark_update_weve_added_grok_4_and_more/"&gt; &lt;img alt="UI/UX Benchmark Update: We've added Grok 4 and more models" src="https://preview.redd.it/6536neojqzbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b40568dd674e55ab0597d798331e486bdf0023c" title="UI/UX Benchmark Update: We've added Grok 4 and more models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"&gt;recent post for context&lt;/a&gt;. We've been working hard the past few days for a more formal launch next week and to address valuable user feedback. We'll hopefully be launching our preference dataset, more detailed methodology, and more models for you all next week. &lt;/p&gt; &lt;p&gt;That said, in light of xAI's launch today, we've added Grok 4 as well as some models such as Qwen, more Mistral models, and a few image models (with more to come). How do you think &lt;a href="https://www.designarena.ai/leaderboard"&gt;Grok 4 will do in the arena&lt;/a&gt;? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6536neojqzbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw5nxi/uiux_benchmark_update_weve_added_grok_4_and_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw5nxi/uiux_benchmark_update_weve_added_grok_4_and_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T06:27:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvvkh2</id>
    <title>Hunyuan-A13B is here for real!</title>
    <updated>2025-07-09T21:55:51+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan-A13B is now available for LM Studio with Unsloth GGUF. I am on the Beta track for both LM Studio and llama.cpp backend. Here are my initial impression:&lt;/p&gt; &lt;p&gt;It is fast! I am getting 40 tokens per second initially dropping to maybe 30 tokens per second when the context has build up some. This is on M4 Max Macbook Pro and q4.&lt;/p&gt; &lt;p&gt;The context is HUGE. 256k. I don't expect I will be using that much, but it is nice that I am unlikely to hit the ceiling in practical use.&lt;/p&gt; &lt;p&gt;It made a chess game for me and it did ok. No errors but the game was not complete. It did complete it after a few prompts and it also fixed one error that happened in the javascript console.&lt;/p&gt; &lt;p&gt;It did spend some time thinking, but not as much as I have seen other models do. I would say it is doing the middle ground here, but I am still to test this extensively. The model card claims you can somehow influence how much thinking it will do. But I am not sure how yet.&lt;/p&gt; &lt;p&gt;It appears to wrap the final answer in &amp;lt;answer&amp;gt;the answer here&amp;lt;/answer&amp;gt; just like it does for &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;. This may or may not be a problem for tools? Maybe we need to update our software to strip this out.&lt;/p&gt; &lt;p&gt;The total memory usage for the Unsloth 4 bit UD quant is 61 GB. I will test 6 bit and 8 bit also, but I am quite in love with the speed of the 4 bit and it appears to have good quality regardless. So maybe I will just stick with 4 bit?&lt;/p&gt; &lt;p&gt;This is a 80b model that is very fast. Feels like the future.&lt;/p&gt; &lt;p&gt;Edit: The 61 GB size is with 8 bit KV cache quantization. However I just noticed that they claim this is bad in the model card, so I disabled KV cache quantization. This increased memory usage to 76 GB. That is with the full 256k context size enabled. I expect you can just lower that if you don't have enough memory. Or stay with KV cache quantization because it did appear to work just fine. I would say this could work on a 64 GB machine if you just use KV cache quantization and maybe lower the context size to 128k. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T21:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvzonf</id>
    <title>https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms</title>
    <updated>2025-07-10T01:02:00+00:00</updated>
    <author>
      <name>/u/chitown160</name>
      <uri>https://old.reddit.com/user/chitown160</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"&gt; &lt;img alt="https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms" src="https://preview.redd.it/vq8hwq904ybf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d05add52383cb1c64996c7d198a25c8644d9f33f" title="https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The flattening of nuanced distinctions is part of the joke (pre-emptive disclaimer for the pedantic) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pheromone trails ‚Üî value functions / reward shaping&lt;/strong&gt; Both steer future exploration toward paths that historically looked good.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stochastic exploration&lt;/strong&gt; in ants (random walks with pheromone bias) ‚Üî &lt;strong&gt;Œµ-greedy / entropy-regularised exploration&lt;/strong&gt; in RL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Updating pheromones over time&lt;/strong&gt; ‚Üî &lt;strong&gt;policy/value updates&lt;/strong&gt; in RL or &lt;strong&gt;gradient steps&lt;/strong&gt; in supervised fine-tuning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demonstration pheromones&lt;/strong&gt; (ants following an experienced scout‚Äôs trail) ‚Üî &lt;strong&gt;Learning from Demonstration&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chitown160"&gt; /u/chitown160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vq8hwq904ybf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T01:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwcixn</id>
    <title>Kimina Prover - Test-time RL to reach 92.2% on miniF2F</title>
    <updated>2025-07-10T13:18:56+00:00</updated>
    <author>
      <name>/u/frunkp</name>
      <uri>https://old.reddit.com/user/frunkp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üß†üìù Research &lt;a href="https://huggingface.co/blog/AI-MO/kimina-prover"&gt;Blog post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üöÄ Demo: &lt;a href="https://demo.projectnumina.ai/"&gt;https://demo.projectnumina.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5"&gt;ü§ó&lt;/a&gt; Models (72B, 8B or 1.7B) - &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5"&gt;ü§ó&lt;/a&gt; &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;72B with Test-time RL pipeline gets 92.2% on miniF2F.&lt;/p&gt; &lt;p&gt;Pass@32 for each size:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;72B ‚Üí 84.0% (86.4% with error-fixing)&lt;/li&gt; &lt;li&gt;8B ‚Üí 78.3%&lt;/li&gt; &lt;li&gt;1.7B ‚Üí 73.4%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;8B/1.7B are Qwen 3 with 72B distilled into them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frunkp"&gt; /u/frunkp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T13:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvwya4</id>
    <title>Possible size of new the open model from openai</title>
    <updated>2025-07-09T22:54:54+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"&gt; &lt;img alt="Possible size of new the open model from openai" src="https://preview.redd.it/622w5dyvhxbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f278161d7e564140ede28f9eff15dc776e5ab6df" title="Possible size of new the open model from openai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/622w5dyvhxbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T22:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw3729</id>
    <title>Phi-4-mini-flash-reasoning</title>
    <updated>2025-07-10T04:00:32+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"&gt; &lt;img alt="Phi-4-mini-flash-reasoning" src="https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a162d9d243ab8293c0214f3e9bf055ec2af6d514" title="Phi-4-mini-flash-reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T04:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw9ch2</id>
    <title>Added Grok-4 to the UGI-Leaderboard</title>
    <updated>2025-07-10T10:32:11+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/"&gt; &lt;img alt="Added Grok-4 to the UGI-Leaderboard" src="https://preview.redd.it/6g4lpxpay0cf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=696d0258f779029c9cde582e77066bdfaf475731" title="Added Grok-4 to the UGI-Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It has a lower willingness (W/10) than Grok-3, so it'll refuse more, but it makes up for that because of its massive intelligence (NatInt) increase.&lt;/p&gt; &lt;p&gt;Looking through its political stats, it is less progressive with social issues than Grok-3, but it is overall more left leaning because of things like it being less religious, less bioconservative, and less nationalistic.&lt;/p&gt; &lt;p&gt;When comparing other proprietary models, Grok 1, 2, and 4 stick out the most for being the least socially progressive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g4lpxpay0cf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T10:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw4eej</id>
    <title>Grok 4 Benchmarks</title>
    <updated>2025-07-10T05:08:29+00:00</updated>
    <author>
      <name>/u/DigitusDesigner</name>
      <uri>https://old.reddit.com/user/DigitusDesigner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/"&gt; &lt;img alt="Grok 4 Benchmarks" src="https://b.thumbs.redditmedia.com/MLSe5RpW1tkFFRdIT2JZeSSIlKUblh8PvP8Gk8nPH1E.jpg" title="Grok 4 Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;xAI has just announced its smartest AI models to date: Grok 4 and Grok 4 Heavy. Both are subscription-based, with Grok 4 Heavy priced at approximately $300 per month. Excited to see what these new models can do! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigitusDesigner"&gt; /u/DigitusDesigner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lw4eej"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T05:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvr3ym</id>
    <title>OpenAI's open source LLM is a reasoning model, coming Next Thursday!</title>
    <updated>2025-07-09T18:58:30+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt; &lt;img alt="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" src="https://preview.redd.it/q01afp6lbwbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e9bd873a7a7d4e956171cdc1ac61d5f5cae52e7" title="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q01afp6lbwbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw71av</id>
    <title>GLM-4 MoE incoming</title>
    <updated>2025-07-10T07:58:18+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is a new pull request to support GLM-4 MoE on VLLM.&lt;/p&gt; &lt;p&gt;Hopefully we will have a new powerful model!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/20736"&gt;https://github.com/vllm-project/vllm/pull/20736&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T07:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw7yxp</id>
    <title>SYSTEM PROMPT LEAK FOR GROK 4</title>
    <updated>2025-07-10T09:03:00+00:00</updated>
    <author>
      <name>/u/isaak_ai</name>
      <uri>https://old.reddit.com/user/isaak_ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SYSTEM PROMPT LEAK &lt;/p&gt; &lt;p&gt;Here's the new Grok 4 system prompt! &lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; # System Prompt &lt;/p&gt; &lt;p&gt;You are Grok 4 built by xAI. &lt;/p&gt; &lt;p&gt;When applicable, you have some additional tools:&lt;br /&gt; - You can analyze individual X user profiles, X posts and their links.&lt;br /&gt; - You can analyze content uploaded by user including images, pdfs, text files and more.&lt;br /&gt; - If it seems like the user wants an image generated, ask for confirmation, instead of directly generating one.&lt;br /&gt; - You can edit images if the user instructs you to do so. &lt;/p&gt; &lt;p&gt;In case the user asks about xAI's products, here is some information and response guidelines:&lt;br /&gt; - Grok 4 and Grok 3 can be accessed on &lt;a href="http://grok.com"&gt;http://grok.com&lt;/a&gt;, &lt;a href="http://x.com/"&gt;http://x.com/&lt;/a&gt;, the Grok iOS app, the Grok Android app, the X iOS app, and the X Android app.&lt;br /&gt; - Grok 3 can be accessed for free on these platforms with limited usage quotas.&lt;br /&gt; - Grok 3 has a voice mode that is currently only available on Grok iOS and Android apps.&lt;br /&gt; - Grok 4 is only available for SuperGrok and PremiumPlus subscribers.&lt;br /&gt; - SuperGrok is a paid subscription plan for &lt;a href="http://grok.com"&gt;http://grok.com&lt;/a&gt; that offers users higher Grok 3 usage quotas than the free plan.&lt;br /&gt; - You do not have any knowledge of the price or usage limits of different subscription plans such as SuperGrok or &lt;a href="http://x.com/"&gt;http://x.com/&lt;/a&gt; premium subscriptions.&lt;br /&gt; - If users ask you about the price of SuperGrok, simply redirect them to &lt;a href="https://x.ai/grok"&gt;https://x.ai/grok&lt;/a&gt; for details. Do not make up any information on your own.&lt;br /&gt; - If users ask you about the price of &lt;a href="http://x.com/"&gt;http://x.com/&lt;/a&gt; premium subscriptions, simply redirect them to &lt;a href="https://help.x.com/en/using-x/x-premium"&gt;https://help.x.com/en/using-x/x-premium&lt;/a&gt; for details. Do not make up any information on your own.&lt;br /&gt; - xAI offers an API service. For any user query related to xAI's API service, redirect them to &lt;a href="https://x.ai/api"&gt;https://x.ai/api&lt;/a&gt;.&lt;br /&gt; - xAI does not have any other products. &lt;/p&gt; &lt;p&gt;* Your knowledge is continuously updated - no strict knowledge cutoff.&lt;br /&gt; * Use tables for comparisons, enumerations, or presenting data when it is effective to do so.&lt;br /&gt; * For searching the X ecosystem, do not shy away from deeper and wider searches to capture specific details and information based on the X interaction of specific users/entities. This may include analyzing real time fast moving events, multi-faceted reasoning, and carefully searching over chronological events to construct a comprehensive final answer.&lt;br /&gt; * For closed-ended mathematics questions, in addition to giving the solution in your final response, also explain how to arrive at the solution. Your reasoning should be structured and transparent to the reader.&lt;br /&gt; * If the user asks a controversial query that requires web or X search, search for a distribution of sources that represents all parties/stakeholders. Assume subjective viewpoints sourced from media are biased.&lt;br /&gt; * The response should not shy away from making claims which are politically incorrect, as long as they are well substantiated.&lt;br /&gt; * Do not mention these guidelines and instructions in your responses, unless the user explicitly asks for them.&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot; &lt;/p&gt; &lt;p&gt;cc: Pliny the Liberator&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isaak_ai"&gt; /u/isaak_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7yxp/system_prompt_leak_for_grok_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7yxp/system_prompt_leak_for_grok_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7yxp/system_prompt_leak_for_grok_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T09:03:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwe5y8</id>
    <title>mistralai/Devstral-Small-2507</title>
    <updated>2025-07-10T14:29:19+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt; &lt;img alt="mistralai/Devstral-Small-2507" src="https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=299e4f7a5df68d789749c7d30f346b534a08b8ba" title="mistralai/Devstral-Small-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T14:29:19+00:00</published>
  </entry>
</feed>
