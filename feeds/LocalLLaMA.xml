<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-16T16:39:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ni1uw3</id>
    <title>Anyone else have small models just "forget" MCP tools exist?</title>
    <updated>2025-09-15T23:37:53+00:00</updated>
    <author>
      <name>/u/TheLostWanderer47</name>
      <uri>https://old.reddit.com/user/TheLostWanderer47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to stitch together a lightweight &amp;quot;local research assistant&amp;quot; setup with MCP, but running into weird behavior:&lt;/p&gt; &lt;p&gt;Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/brightdata/brightdata-mcp"&gt;Bright Data MCP&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;Cherry Studio&lt;/a&gt; built-in knowledge graph MCP&lt;/li&gt; &lt;li&gt;Ollama connected w/ Qwen3-4B-Instruct-2507 as the model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most of the time, Qwen doesn’t even seem to know that the MCP tools are there. Paraphrasing the problem here:&lt;/p&gt; &lt;p&gt;Me: &amp;quot;Fetch this URL, then summarize it in 3 bullets, and finally, store it in the knowledge graph with observations.&amp;quot;&lt;br /&gt; Qwen: &amp;quot;Sorry, I don't have any tools that can browse the internet to fetch the contents of that page for you.&amp;quot;&lt;/p&gt; &lt;p&gt;…but maybe 1 out of 3 tries, it does call the Bright Data MCP and returns clean markdown???&lt;/p&gt; &lt;p&gt;Same with Cherry’s knowledge graph. sometimes it builds links between entities, sometimes the model acts like the tool was never registered.&lt;/p&gt; &lt;p&gt;I've tried explicitly reminding the model, &amp;quot;you have these tools available,&amp;quot; but it doesn't stick.&lt;/p&gt; &lt;p&gt;Have I messed up the config somewhere? Has anyone else run into this &amp;quot;tool amnesia&amp;quot; issue with Cherry studio or MCP servers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLostWanderer47"&gt; /u/TheLostWanderer47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni1uw3/anyone_else_have_small_models_just_forget_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni1uw3/anyone_else_have_small_models_just_forget_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni1uw3/anyone_else_have_small_models_just_forget_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T23:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni87hl</id>
    <title>Feedback on trimmed-down AI workstation build (based on a16z specs)</title>
    <updated>2025-09-16T04:44:46+00:00</updated>
    <author>
      <name>/u/cuuuuuooooongg</name>
      <uri>https://old.reddit.com/user/cuuuuuooooongg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m putting together a local AI workstation build inspired by the &lt;a href="https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/"&gt;a16z setup&lt;/a&gt;. The idea is to stop bleeding money on GCP/AWS for GPU hours and finally have a home rig for quick ideation and prototyping. I’ll mainly be using it to train and finetune custom architectures.&lt;/p&gt; &lt;p&gt;I’ve slimmed down the original spec to make it (slightly) more reasonable while keeping room to expand in the future. I’d love feedback from this community before pulling the trigger.&lt;/p&gt; &lt;p&gt;Here are the main changes vs the reference build:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4× GPU → 1× GPU (will expand later if needed)&lt;/li&gt; &lt;li&gt;256GB RAM → 128GB RAM&lt;/li&gt; &lt;li&gt;8TB storage → 2TB storage&lt;/li&gt; &lt;li&gt;Sticking with the same PSU for headroom if I add GPUs later&lt;/li&gt; &lt;li&gt;Unsure if the motherboard swap is the right move (original was GIGABYTE MH53-G40, I picked the ASUS Pro WS WRX90E-SAGE SE — any thoughts here?)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Current parts list:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;NVIDIA RTX PRO 6000 Blackwell Max-Q&lt;/td&gt; &lt;td align="left"&gt;$8,449.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen Threadripper PRO 7975WX 32-core 5.3GHz Computer Processor&lt;/td&gt; &lt;td align="left"&gt;$3,400.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Pro WS WRX90E-SAGE SE&lt;/td&gt; &lt;td align="left"&gt;$1,299.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;OWC DDR5 4×32GB&lt;/td&gt; &lt;td align="left"&gt;$700.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;WD_BLACK 2TB SN8100 NVMe SSD Internal Solid State Drive - Gen 5 PCIe 5.0x4, M.2 2280&lt;/td&gt; &lt;td align="left"&gt;$230.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Thermaltake Toughpower GF3&lt;/td&gt; &lt;td align="left"&gt;$300.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;ARCTIC Liquid Freezer III Pro 420 A-RGB – AIO CPU Cooler, 3 × 140 mm Water Cooling, 38 mm Radiator, PWM Pump, VRM Fan, for AMD/Intel sockets&lt;/td&gt; &lt;td align="left"&gt;$115.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$14,493.00&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Any advice on the component choices or obvious oversights would be super appreciated. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cuuuuuooooongg"&gt; /u/cuuuuuooooongg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni87hl/feedback_on_trimmeddown_ai_workstation_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni87hl/feedback_on_trimmeddown_ai_workstation_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni87hl/feedback_on_trimmeddown_ai_workstation_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T04:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nigq8b</id>
    <title>Z440 with 512GB RAM and a 3090</title>
    <updated>2025-09-16T12:57:00+00:00</updated>
    <author>
      <name>/u/Potential-Leg-639</name>
      <uri>https://old.reddit.com/user/Potential-Leg-639</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;Thinking about to re activate my HP Z440.&lt;/p&gt; &lt;p&gt;I could get 512GB DDR4 2400 for around 400€.&lt;/p&gt; &lt;p&gt;I have a 2690V4 (14 Core) and could throw an RTX 3090 in, much more won't be possible so easy because of the 700W PSU (yes - it could be changed to a normal ATX etc, but want to keep it simple for now).&lt;/p&gt; &lt;p&gt;What performance could I expect - also on bigger models?&lt;/p&gt; &lt;p&gt;Some references out there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Potential-Leg-639"&gt; /u/Potential-Leg-639 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nigq8b/z440_with_512gb_ram_and_a_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nigq8b/z440_with_512gb_ram_and_a_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nigq8b/z440_with_512gb_ram_and_a_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T12:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nii0iy</id>
    <title>What are the current options for running LLMs locally on a laptop?</title>
    <updated>2025-09-16T13:48:50+00:00</updated>
    <author>
      <name>/u/ConSemaforos</name>
      <uri>https://old.reddit.com/user/ConSemaforos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The main ones I’ve seen are MacBook and The ROG Z FLOW. Are there other options? I’m looking for 100+ gb RAM. I guess the 395+ is not good with image generation. Most of my work and hobby involves LLMs but I’d like to be able to use image and audio generation as well. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConSemaforos"&gt; /u/ConSemaforos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nii0iy/what_are_the_current_options_for_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nii0iy/what_are_the_current_options_for_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nii0iy/what_are_the_current_options_for_running_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T13:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1niit47</id>
    <title>Can Domain-Specific Pretraining on Proprietary Data Beat GPT-5 or Gemini in Specialized Fields?</title>
    <updated>2025-09-16T14:19:43+00:00</updated>
    <author>
      <name>/u/hezarfenserden</name>
      <uri>https://old.reddit.com/user/hezarfenserden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working in a domain that relies heavily on large amounts of non-public, human-generated data. This data uses highly specialized jargon and terminology that current state-of-the-art (SOTA) large language models (LLMs) struggle to interpret correctly. Suppose I take one of the leading open-source LLMs and perform continual pretraining on this raw, domain-specific corpus, followed by generating a small set of question–answer pairs for instruction tuning. In this scenario, could the adapted model realistically outperform cutting-edge general-purpose models like GPT-5 or Gemini within this narrow domain?&lt;/p&gt; &lt;p&gt;What are the main challenges and limitations in this approach—for example, risks of catastrophic forgetting during continual pretraining, the limited effectiveness of synthetic QA data for instruction tuning, scaling issues when compared to the massive pretraining of frontier models, or the difficulty of evaluating “outperformance” in terms of accuracy, reasoning, and robustness?&lt;/p&gt; &lt;p&gt;I've checked the previous work but they compare the performances of old models like GPT3.5 GPT-4 and I think LLMs made a long way since and it is difficult to beat them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hezarfenserden"&gt; /u/hezarfenserden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niit47/can_domainspecific_pretraining_on_proprietary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niit47/can_domainspecific_pretraining_on_proprietary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niit47/can_domainspecific_pretraining_on_proprietary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nig0zp</id>
    <title>Hardware and model recommendations for on-prem LLM deployment</title>
    <updated>2025-09-16T12:26:23+00:00</updated>
    <author>
      <name>/u/neenawa</name>
      <uri>https://old.reddit.com/user/neenawa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've delivered a couple of projects using frontier models, but my latest client wants something on-prem for his team of ~10. The application will have a RAG pipeline. Starting with ~100 PDFs. Later I will need to add and some agentic reasoning. &lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Which open-source LLM is a good place to start for RAG? I will experiment a bit, but nice to have some working experience.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Viable hardware: do I need Nvidia? AMD? I've only ever used cloud-based systems, so this is a bit new to me, and the part I feel less sure about.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help would be appreciated, thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neenawa"&gt; /u/neenawa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nig0zp/hardware_and_model_recommendations_for_onprem_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nig0zp/hardware_and_model_recommendations_for_onprem_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nig0zp/hardware_and_model_recommendations_for_onprem_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T12:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni815f</id>
    <title>Voice Assistant Running on a Raspyberry Pi</title>
    <updated>2025-09-16T04:34:43+00:00</updated>
    <author>
      <name>/u/localslm</name>
      <uri>https://old.reddit.com/user/localslm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni815f/voice_assistant_running_on_a_raspyberry_pi/"&gt; &lt;img alt="Voice Assistant Running on a Raspyberry Pi" src="https://external-preview.redd.it/Y3RvYW1ibmRnZ3BmMWGGnl44cMWlAhwBPOVxHgzmQ7jmQEBvcqJECv2kUwPF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=052f93c040b3819082eb24799548218152addf9c" title="Voice Assistant Running on a Raspyberry Pi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I just published a write-up on a project I’ve been working on: pi-assistant — a local, open-source voice assistant that runs fully offline on a Raspberry Pi 5.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://alexfi.dev/blog/raspberry-pi-assistant"&gt;https://alexfi.dev/blog/raspberry-pi-assistant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/alexander-fischer/pi-assistant"&gt;https://github.com/alexander-fischer/pi-assistant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What it is&lt;/p&gt; &lt;p&gt;pi-assistant is a modular, tool-calling voice assistant that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Listens for a wake word (e.g., “Hey Jarvis”)&lt;/li&gt; &lt;li&gt;Transcribes your speech&lt;/li&gt; &lt;li&gt;Uses small LLMs to interpret commands and call tools (weather, Wikipedia, smart home)&lt;/li&gt; &lt;li&gt;Speaks the answer back to you —all without sending data to the cloud.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tech stack&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wake word detection: openWakeWord&lt;/li&gt; &lt;li&gt;ASR: nemo-parakeet-tdt-0.6b-v2 / nvidia/canary-180m-flash&lt;/li&gt; &lt;li&gt;Function calling: Arch-Function 1.5B&lt;/li&gt; &lt;li&gt;Answer generation: Gemma3 1B&lt;/li&gt; &lt;li&gt;TTS: Piper&lt;/li&gt; &lt;li&gt;Hardware: Raspberry Pi 5 (16 GB), Jabra Speak 410&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can easily change the language models for a bigger hardware setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/localslm"&gt; /u/localslm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lh67dy5eggpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni815f/voice_assistant_running_on_a_raspyberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni815f/voice_assistant_running_on_a_raspyberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T04:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni8noo</id>
    <title>LLMs for detailed book summaries?</title>
    <updated>2025-09-16T05:09:50+00:00</updated>
    <author>
      <name>/u/JealousAmoeba</name>
      <uri>https://old.reddit.com/user/JealousAmoeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am picturing a tool that I can throw any arbitrary ePub novel at and get back a SparkNotes-style summary:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.sparknotes.com/lit/pride/"&gt;https://www.sparknotes.com/lit/pride/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(This page has a plot overview but there are other pages that do deeper dives into the material.)&lt;/p&gt; &lt;p&gt;It seems like something an LLM could do in principle if you could avoid hallucinations and maintain coherency. I don’t really think dumping the entire book into context would work, especially since some books are too long to reasonably fit.&lt;/p&gt; &lt;p&gt;Has anyone had success on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JealousAmoeba"&gt; /u/JealousAmoeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni8noo/llms_for_detailed_book_summaries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni8noo/llms_for_detailed_book_summaries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni8noo/llms_for_detailed_book_summaries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T05:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nigiuk</id>
    <title>Small LLM evaluation</title>
    <updated>2025-09-16T12:48:25+00:00</updated>
    <author>
      <name>/u/InformationPretty616</name>
      <uri>https://old.reddit.com/user/InformationPretty616</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have a script for evaluating tiny language models that I'm sharing with the community. I hope it's useful to you. I'm looking for your feedback on what other metrics could be added to measure performance, GPU consumption, answer quality, and more. Thanks! (AMD 1800 32GB RAM GTX 1070). # ======================================================================&lt;/p&gt; &lt;p&gt;# Archivo: llm_evaluation_script.py&lt;/p&gt; &lt;p&gt;# Descripción: Script de evaluación de modelos LLM con métricas de rendimiento y ranking automático.&lt;/p&gt; &lt;p&gt;# ======================================================================&lt;/p&gt; &lt;p&gt;from dotenv import load_dotenv &lt;/p&gt; &lt;p&gt;import os &lt;/p&gt; &lt;p&gt;import sys &lt;/p&gt; &lt;p&gt;import time &lt;/p&gt; &lt;p&gt;import psutil &lt;/p&gt; &lt;p&gt;import json &lt;/p&gt; &lt;p&gt;from openai import OpenAI &lt;/p&gt; &lt;p&gt;from IPython.display import Markdown, display &lt;/p&gt; &lt;p&gt;# Cargar variables de entorno desde el archivo .env&lt;/p&gt; &lt;p&gt;load_dotenv(override=True) &lt;/p&gt; &lt;p&gt;# Inicializar el cliente de OpenAI para interactuar con Ollama&lt;/p&gt; &lt;p&gt;client = OpenAI( &lt;/p&gt; &lt;p&gt;base_url=&amp;quot;&lt;a href="http://192.168.50.253:11434/v1"&gt;http://192.168.50.253:11434/v1&lt;/a&gt;&amp;quot;, &lt;/p&gt; &lt;p&gt;api_key=&amp;quot;ollama&amp;quot;, &lt;/p&gt; &lt;p&gt;timeout=120 &lt;/p&gt; &lt;p&gt;) &lt;/p&gt; &lt;p&gt;# ======================================================================&lt;/p&gt; &lt;p&gt;# Configuración del Benchmarking&lt;/p&gt; &lt;p&gt;# ======================================================================&lt;/p&gt; &lt;p&gt;# Lista de modelos a evaluar&lt;/p&gt; &lt;p&gt;models = [ &lt;/p&gt; &lt;p&gt;&amp;quot;llama3.2:1b&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;llama3.2:3b&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;qwen3:1.7b&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;gemma3n:e4b&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;qwen3:0.6b&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;gemma3:1b&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;cogito:3b&amp;quot; &lt;/p&gt; &lt;p&gt;] &lt;/p&gt; &lt;p&gt;# Tamaños de los modelos en GB para la estimación de energía&lt;/p&gt; &lt;p&gt;model_sizes = { &lt;/p&gt; &lt;p&gt;&amp;quot;llama3.2:1b&amp;quot;: 1.0, &lt;/p&gt; &lt;p&gt;&amp;quot;llama3.2:3b&amp;quot;: 3.0, &lt;/p&gt; &lt;p&gt;&amp;quot;qwen3:1.7b&amp;quot;: 1.3, &lt;/p&gt; &lt;p&gt;&amp;quot;gemma3n:e4b&amp;quot;: 4.0, &lt;/p&gt; &lt;p&gt;&amp;quot;qwen3:0.6b&amp;quot;: 1.0, &lt;/p&gt; &lt;p&gt;&amp;quot;gemma3:1b&amp;quot;: 1.0 &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;# Tareas de evaluación y sus prompts&lt;/p&gt; &lt;p&gt;tasks = { &lt;/p&gt; &lt;p&gt;&amp;quot;Programación&amp;quot;: &amp;quot;Here’s a buggy Python function for the Fibonacci sequence: ```def fib(n): if n &amp;lt;= 1: return n; else: return fib(n-1) + fib(n-2)``` The function is correct for small `n` but inefficient for larger `n`. Suggest an optimized version and explain the bug in 100 words or less.&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;Razonamiento Profundo&amp;quot;: &amp;quot;Three people, A, B, and C, are either knights (always tell the truth) or knaves (always lie). A says, 'B is a knight.' B says, 'C is a knave.' C says, 'A and B are knaves.' Determine who is a knight and who is a knave in 100 words or less.&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;Matemáticas&amp;quot;: &amp;quot;Calculate the integral ∫(0 to 1) x^2 dx and explain the steps in 100 words or less.&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;Física&amp;quot;: &amp;quot;A ball is thrown horizontally at 10 m/s from a 20 m high cliff. How far from the base of the cliff does it land? Ignore air resistance and use g = 9.8 m/s². Answer in 100 words or less.&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;Química&amp;quot;: &amp;quot;Balance the chemical equation: C3H8 + O2 → CO2 + H2O. Provide the balanced equation and a brief explanation in 100 words or less.&amp;quot;, &lt;/p&gt; &lt;p&gt;&amp;quot;Creatividad&amp;quot;: &amp;quot;Write a 100-word story about a robot discovering a hidden forest on Mars.&amp;quot; &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;# Prompt del sistema para guiar a los modelos&lt;/p&gt; &lt;p&gt;system_prompt = &amp;quot;You are an expert AI assistant. Provide accurate, concise, and clear answers to the following task in 100 words or less.&amp;quot; &lt;/p&gt; &lt;p&gt;# Diccionarios para almacenar resultados, rankings y puntajes&lt;/p&gt; &lt;p&gt;results = {task: {model: {&amp;quot;response&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;metrics&amp;quot;: {}} for model in models} for task in tasks} &lt;/p&gt; &lt;p&gt;rankings = {task: {} for task in tasks} &lt;/p&gt; &lt;p&gt;overall_scores = {model: 0 for model in models} &lt;/p&gt; &lt;p&gt;# ======================================================================&lt;/p&gt; &lt;p&gt;# Bucle de Evaluación Principal&lt;/p&gt; &lt;p&gt;# ======================================================================&lt;/p&gt; &lt;p&gt;# Evaluar cada modelo en cada tarea&lt;/p&gt; &lt;p&gt;for task, prompt in tasks.items(): &lt;/p&gt; &lt;p&gt;print(f&amp;quot;\n=== Evaluando tarea: {task} ===\n&amp;quot;) &lt;/p&gt; &lt;p&gt;competitors = [] &lt;/p&gt; &lt;p&gt;answers = [] &lt;/p&gt; &lt;p&gt;for model_name in models: &lt;/p&gt; &lt;p&gt;print(f&amp;quot;\n--- Modelo: {model_name} ---&amp;quot;) &lt;/p&gt; &lt;p&gt;try: &lt;/p&gt; &lt;p&gt;# 1. Medir el rendimiento antes de la llamada&lt;/p&gt; &lt;p&gt;cpu_before = psutil.cpu_percent(interval=None) &lt;/p&gt; &lt;p&gt;mem_before = psutil.virtual_memory().used / 1024**2 &lt;/p&gt; &lt;p&gt;start_time = time.time() &lt;/p&gt; &lt;p&gt;# 2. Llamada a la API de Ollama&lt;/p&gt; &lt;p&gt;response = client.chat.completions.create( &lt;/p&gt; &lt;p&gt;model=model_name, &lt;/p&gt; &lt;p&gt;messages=[ &lt;/p&gt; &lt;p&gt;{&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt}, &lt;/p&gt; &lt;p&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: prompt} &lt;/p&gt; &lt;p&gt;], &lt;/p&gt; &lt;p&gt;max_tokens=200 &lt;/p&gt; &lt;p&gt;) &lt;/p&gt; &lt;p&gt;# 3. Medir el rendimiento después de la llamada&lt;/p&gt; &lt;p&gt;elapsed_time = time.time() - start_time &lt;/p&gt; &lt;p&gt;if elapsed_time &amp;gt; 120: &lt;/p&gt; &lt;p&gt;raise TimeoutError(&amp;quot;La respuesta excedió el límite de 2 minutos.&amp;quot;) &lt;/p&gt; &lt;p&gt;cpu_after = psutil.cpu_percent(interval=None) &lt;/p&gt; &lt;p&gt;mem_after = psutil.virtual_memory().used / 1024**2 &lt;/p&gt; &lt;p&gt;cpu_usage = (cpu_before + cpu_after) / 2 &lt;/p&gt; &lt;p&gt;mem_usage = mem_after - mem_before &lt;/p&gt; &lt;p&gt;energy_estimate = model_sizes.get(model_name, 0) * elapsed_time &lt;/p&gt; &lt;p&gt;# 4. Almacenar la respuesta y las métricas&lt;/p&gt; &lt;p&gt;answer = response.choices[0].message.content &lt;/p&gt; &lt;p&gt;display(Markdown(f&amp;quot;**{model_name}** (Tiempo: {elapsed_time:.2f}s, CPU: {cpu_usage:.1f}%, Mem: {mem_usage:.1f} MB, Energía: {energy_estimate:.1f} GB*s): {answer}&amp;quot;)) &lt;/p&gt; &lt;p&gt;print(f&amp;quot;{model_name} (Tiempo: {elapsed_time:.2f}s, CPU: {cpu_usage:.1f}%, Mem: {mem_usage:.1f} MB, Energía: {energy_estimate:.1f} GB*s): {answer}&amp;quot;) &lt;/p&gt; &lt;p&gt;results[task][model_name] = { &lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: answer, &lt;/p&gt; &lt;p&gt;&amp;quot;metrics&amp;quot;: { &lt;/p&gt; &lt;p&gt;&amp;quot;response_time&amp;quot;: elapsed_time, &lt;/p&gt; &lt;p&gt;&amp;quot;cpu_usage&amp;quot;: cpu_usage, &lt;/p&gt; &lt;p&gt;&amp;quot;mem_usage&amp;quot;: mem_usage, &lt;/p&gt; &lt;p&gt;&amp;quot;energy_estimate&amp;quot;: energy_estimate &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;competitors.append(model_name) &lt;/p&gt; &lt;p&gt;answers.append(answer) &lt;/p&gt; &lt;p&gt;except Exception as e: &lt;/p&gt; &lt;p&gt;print(f&amp;quot;Error con {model_name}: {e}&amp;quot;, file=sys.stderr) &lt;/p&gt; &lt;p&gt;error_msg = f&amp;quot;Error: No response ({str(e)})&amp;quot; &lt;/p&gt; &lt;p&gt;results[task][model_name] = { &lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: error_msg, &lt;/p&gt; &lt;p&gt;&amp;quot;metrics&amp;quot;: { &lt;/p&gt; &lt;p&gt;&amp;quot;response_time&amp;quot;: float(&amp;quot;inf&amp;quot;), &lt;/p&gt; &lt;p&gt;&amp;quot;cpu_usage&amp;quot;: 0, &lt;/p&gt; &lt;p&gt;&amp;quot;mem_usage&amp;quot;: 0, &lt;/p&gt; &lt;p&gt;&amp;quot;energy_estimate&amp;quot;: float(&amp;quot;inf&amp;quot;) &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;competitors.append(model_name) &lt;/p&gt; &lt;p&gt;answers.append(error_msg) &lt;/p&gt; &lt;p&gt;# 4. Juzgar las respuestas y generar un ranking&lt;/p&gt; &lt;p&gt;together = &amp;quot;&amp;quot; &lt;/p&gt; &lt;p&gt;for index, answer in enumerate(answers): &lt;/p&gt; &lt;p&gt;together += f&amp;quot;# Respuesta del competidor {index+1}\n\n{answer}\n\n&amp;quot; &lt;/p&gt; &lt;p&gt;print(f&amp;quot;\n=== Respuestas Combinadas para {task} ===\n&amp;quot;) &lt;/p&gt; &lt;p&gt;print(together) &lt;/p&gt; &lt;p&gt;judge_prompt = f&amp;quot;&amp;quot;&amp;quot;Estás juzgando una competencia entre {len(competitors)} competidores para la tarea: {task}. &lt;/p&gt; &lt;p&gt;Evalúa cada respuesta por precisión, claridad, concisión y relevancia. Clasifícalos del mejor al peor. Si una respuesta es un mensaje de error, clasifícala al final.&lt;/p&gt; &lt;p&gt;Responde solo con JSON: &lt;/p&gt; &lt;p&gt;{{&amp;quot;results&amp;quot;: [&amp;quot;número del mejor competidor&amp;quot;, &amp;quot;número del segundo mejor&amp;quot;, ...]}} &lt;/p&gt; &lt;p&gt;Respuestas: &lt;/p&gt; &lt;p&gt;{together} &lt;/p&gt; &lt;p&gt;Responde solo con el ranking en formato JSON.&amp;quot;&amp;quot;&amp;quot; &lt;/p&gt; &lt;p&gt;try: &lt;/p&gt; &lt;p&gt;response = client.chat.completions.create( &lt;/p&gt; &lt;p&gt;model=&amp;quot;cogito:8b&amp;quot;, &lt;/p&gt; &lt;p&gt;messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: judge_prompt}], &lt;/p&gt; &lt;p&gt;max_tokens=200 &lt;/p&gt; &lt;p&gt;) &lt;/p&gt; &lt;p&gt;judge_result = json.loads(response.choices[0].message.content) &lt;/p&gt; &lt;p&gt;ranks = judge_result[&amp;quot;results&amp;quot;] &lt;/p&gt; &lt;p&gt;print(f&amp;quot;\n=== Rankings para {task} ===\n&amp;quot;) &lt;/p&gt; &lt;p&gt;for index, rank in enumerate(ranks): &lt;/p&gt; &lt;p&gt;competitor = competitors[int(rank) - 1] &lt;/p&gt; &lt;p&gt;rankings[task][competitor] = len(ranks) - index &lt;/p&gt; &lt;p&gt;overall_scores[competitor] += len(ranks) - index &lt;/p&gt; &lt;p&gt;print(f&amp;quot;Rank {index + 1}: {competitor} (Puntaje: {len(ranks) - index})&amp;quot;) &lt;/p&gt; &lt;p&gt;except Exception as e: &lt;/p&gt; &lt;p&gt;print(f&amp;quot;Error al juzgar {task}: {e}&amp;quot;, file=sys.stderr) &lt;/p&gt; &lt;p&gt;# ======================================================================&lt;/p&gt; &lt;p&gt;# Resumen de Resultados&lt;/p&gt; &lt;p&gt;# ======================================================================&lt;/p&gt; &lt;p&gt;# 5. Imprimir el resumen de métricas&lt;/p&gt; &lt;p&gt;print(&amp;quot;\n=== Resumen de Métricas de Rendimiento ===\n&amp;quot;) &lt;/p&gt; &lt;p&gt;for task in tasks: &lt;/p&gt; &lt;p&gt;print(f&amp;quot;\n--- Tarea: {task} ---&amp;quot;) &lt;/p&gt; &lt;p&gt;print(&amp;quot;Modelo\t\t\tTiempo (s)\tCPU (%)\tMem (MB)\tEnergía (GB*s)&amp;quot;) &lt;/p&gt; &lt;p&gt;for model_name in models: &lt;/p&gt; &lt;p&gt;metrics = results[task][model_name][&amp;quot;metrics&amp;quot;] &lt;/p&gt; &lt;p&gt;time_s = metrics[&amp;quot;response_time&amp;quot;] &lt;/p&gt; &lt;p&gt;cpu = metrics[&amp;quot;cpu_usage&amp;quot;] &lt;/p&gt; &lt;p&gt;mem = metrics[&amp;quot;mem_usage&amp;quot;] &lt;/p&gt; &lt;p&gt;energy = metrics[&amp;quot;energy_estimate&amp;quot;] &lt;/p&gt; &lt;p&gt;print(f&amp;quot;{model_name:&amp;lt;20}\t{time_s:.2f}\t\t{cpu:.1f}\t{mem:.1f}\t\t{energy:.1f}&amp;quot;) &lt;/p&gt; &lt;p&gt;# 6. Identificar los modelos más lentos y de mayor consumo&lt;/p&gt; &lt;p&gt;print(&amp;quot;\n=== Modelos Más Lentos y de Mayor Consumo ===\n&amp;quot;) &lt;/p&gt; &lt;p&gt;for task in tasks: &lt;/p&gt; &lt;p&gt;print(f&amp;quot;\n--- Tarea: {task} ---&amp;quot;) &lt;/p&gt; &lt;p&gt;max_time_model = max(models, key=lambda m: results[task][m][&amp;quot;metrics&amp;quot;][&amp;quot;response_time&amp;quot;]) &lt;/p&gt; &lt;p&gt;max_cpu_model = max(models, key=lambda m: results[task][m][&amp;quot;metrics&amp;quot;][&amp;quot;cpu_usage&amp;quot;]) &lt;/p&gt; &lt;p&gt;max_mem_model = max(models, key=lambda m: results[task][m][&amp;quot;metrics&amp;quot;][&amp;quot;mem_usage&amp;quot;]) &lt;/p&gt; &lt;p&gt;max_energy_model = max(models, key=lambda m: results[task][m][&amp;quot;metrics&amp;quot;][&amp;quot;energy_estimate&amp;quot;]) &lt;/p&gt; &lt;p&gt;print(f&amp;quot;Modelo más lento: {max_time_model} ({results[task][max_time_model]['metrics']['response_time']:.2f}s)&amp;quot;) &lt;/p&gt; &lt;p&gt;print(f&amp;quot;Mayor uso de CPU: {max_cpu_model} ({results[task][max_cpu_model]['metrics']['cpu_usage']:.1f}%)&amp;quot;) &lt;/p&gt; &lt;p&gt;print(f&amp;quot;Mayor uso de memoria: {max_mem_model} ({results[task][max_mem_model]['metrics']['mem_usage']:.1f} MB)&amp;quot;) &lt;/p&gt; &lt;p&gt;print(f&amp;quot;Mayor energía estimada: {max_energy_model} ({results[task][max_energy_model]['metrics']['energy_estimate']:.1f} GB*s)&amp;quot;) &lt;/p&gt; &lt;p&gt;# 7. Imprimir el ranking general&lt;/p&gt; &lt;p&gt;print(&amp;quot;\n=== Ranking General de Modelos ===\n&amp;quot;) &lt;/p&gt; &lt;p&gt;sorted_models = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True) &lt;/p&gt; &lt;p&gt;print(&amp;quot;Modelo\t\t\tPuntaje Total&amp;quot;) &lt;/p&gt; &lt;p&gt;for model, score in sorted_models: &lt;/p&gt; &lt;p&gt;print(f&amp;quot;{model:&amp;lt;20}\t{score}&amp;quot;) &lt;/p&gt; &lt;p&gt;# 8. Recomendaciones de optimización (añadidas para mayor valor)&lt;/p&gt; &lt;p&gt;print(&amp;quot;\n=== Recomendaciones de Optimización del Servidor ===\n&amp;quot;) &lt;/p&gt; &lt;p&gt;slowest_model = max(models, key=lambda m: sum(results[task][m][&amp;quot;metrics&amp;quot;][&amp;quot;response_time&amp;quot;] for task in tasks)) &lt;/p&gt; &lt;p&gt;highest_energy_model = max(models, key=lambda m: sum(results[task][m][&amp;quot;metrics&amp;quot;][&amp;quot;energy_estimate&amp;quot;] for task in tasks)) &lt;/p&gt; &lt;p&gt;print(f&amp;quot;1. **Aceleración por GPU**: Modelos grandes como {slowest_model} (el más lento) y {highest_energy_model} (el de mayor consumo) se benefician enormemente de una GPU. Configura Ollama con soporte para GPU: `&lt;a href="https://ollama.com/docs/gpu%5C%60.%22"&gt;https://ollama.com/docs/gpu\`.&amp;quot;&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;print(&amp;quot;2. **Cuantización**: Aplica cuantización a los modelos grandes para reducir la memoria y el tiempo de inferencia. Utiliza `ollama quantize`.&amp;quot;)&lt;/p&gt; &lt;p&gt;print(&amp;quot;3. **Monitoreo de Recursos**: Monitorea la RAM del servidor (`htop` o `nvidia-smi`) para evitar cuellos de botella.&amp;quot;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InformationPretty616"&gt; /u/InformationPretty616 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nigiuk/small_llm_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nigiuk/small_llm_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nigiuk/small_llm_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T12:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nijfv6</id>
    <title>Nvidia Rtx Pro 6000 96gb workstation for fine tuning</title>
    <updated>2025-09-16T14:43:18+00:00</updated>
    <author>
      <name>/u/Psychological_Ad8426</name>
      <uri>https://old.reddit.com/user/Psychological_Ad8426</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to get this for work for training local models. Training data is sensitive so would rather keep it local. I would like a pre-built but would build one if it made sense. I have been looking at OriginPC and the card is significantly cheaper in one of their pre-builds. Anyone have any recommendations on pre-built and/or parts for building? Only one thing I really want is ability to add another GPU later if needed. I'm also open to other ideas for something better. Looking at budget of ~$15K (company money :-) ). Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ad8426"&gt; /u/Psychological_Ad8426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijfv6/nvidia_rtx_pro_6000_96gb_workstation_for_fine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijfv6/nvidia_rtx_pro_6000_96gb_workstation_for_fine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nijfv6/nvidia_rtx_pro_6000_96gb_workstation_for_fine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:43:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhz4dn</id>
    <title>Qwen-next - no gguf yet</title>
    <updated>2025-09-15T21:44:23+00:00</updated>
    <author>
      <name>/u/mgr2019x</name>
      <uri>https://old.reddit.com/user/mgr2019x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone know why llama.cpp has not implemented the new architecture yet?&lt;/p&gt; &lt;p&gt;I am not complaining, i am just wondering what the reason(s) might be. The feature request on github seems quite stuck to me.&lt;/p&gt; &lt;p&gt;Sadly there is no skill on my side, so i am not able to help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mgr2019x"&gt; /u/mgr2019x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T21:44:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1niezv4</id>
    <title>I built a tool to search content in my local files using semantic search</title>
    <updated>2025-09-16T11:37:33+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;A while back I shared an open source tool called DeepDoc that I built to explore local files using a research type workflow. The support and feedback I got here really meant a lot and kept me building more so thank you&lt;/p&gt; &lt;p&gt;The idea is simple. Instead of manually going through pdfs, docs, or notes I wanted a smarter way to search the content of my own files&lt;br /&gt; You just point it to a folder with pdf docx txt or image files. It extracts the text splits it into chunks does semantic search based on your query and builds a structured markdown report step by step&lt;/p&gt; &lt;p&gt;Here is the repo if you want to take a look&lt;br /&gt; &lt;a href="https://github.com/Datalore-ai/deepdoc"&gt;https://github.com/Datalore-ai/deepdoc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It recently reached 95 stars which honestly means a lot to me. Knowing that people actually use it and find it useful really made my day&lt;/p&gt; &lt;p&gt;Many people suggested adding OneDrive Google Drive integrations and support for more file formats which I am planning to add soon. and keep making it better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niezv4/i_built_a_tool_to_search_content_in_my_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niezv4/i_built_a_tool_to_search_content_in_my_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niezv4/i_built_a_tool_to_search_content_in_my_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nikq42</id>
    <title>RTX 6000 Pro Workstation sold out, can I use server edition instead?</title>
    <updated>2025-09-16T15:30:51+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a server for running local LLM. The idea was to get a single RTX 6000 Pro Workstation. But it appears to be completely sold out in my area with uncertain delivery times of at least 1-2 months. The Max Q version is available, but I want the full version. The server edition also appears to be available, but that one has no fans. My server is a rack system, but home build and 100% not with enough airflow to passively cool a card like that. But I am good with a 3D printer and maybe I could design an adapter to fit a 120 fan to cool it? Anyone done this before? Will I get in trouble? What happens if the cooling is insufficient? What about the power connector - is that standard?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nikq42/rtx_6000_pro_workstation_sold_out_can_i_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nikq42/rtx_6000_pro_workstation_sold_out_can_i_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nikq42/rtx_6000_pro_workstation_sold_out_can_i_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T15:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhtv5f</id>
    <title>Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info.</title>
    <updated>2025-09-15T18:27:22+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"&gt; &lt;img alt="Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info." src="https://preview.redd.it/5difgej3fdpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f699faed3067a46c354771c3653e38f77a492e56" title="Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just an small post about some power consumption of those some GPUs if some people are interested.&lt;/p&gt; &lt;p&gt;As extra info, all the cards are both undervolted + power limited, but it shouldn't affect idle power consumption.&lt;/p&gt; &lt;p&gt;Undervolt was done with LACT, and they are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3090s: 1875Mhz max core clock, +150Mhz core clock offset, +1700Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;A6000: 1740Mhz max core clock, +150Mhz core clock offset, +2000 Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;4090 (1): 2850Mhz max core clock, +150Mhz core clock offset, +2700Mhz VRAM.&lt;/li&gt; &lt;li&gt;4090 (2): 2805Mhz max core clock, +180Mhz core clock offset, +1700Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;5090s: 3010Mhz max core clock, +1000Mhz core clock offset, +4400Mhz VRAM offset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If someone wants to know how to use LACT just let me know, but I basically use SDDM (sudo systemctl start sddm), LACT for the GUI, set the values and then run&lt;/p&gt; &lt;p&gt;sudo a (it does nothing, but helps for the next command)&lt;br /&gt; (echo suspend | sudo tee /proc/driver/nvidia/suspend ;echo resume | sudo tee /proc/driver/nvidia/suspend)&amp;amp;&lt;/p&gt; &lt;p&gt;Then run sudo systemctl stop sddm.&lt;/p&gt; &lt;p&gt;This mostly puts the 3090s, A6000 and 4090 (2) at 0.9V. 4090 (1) is at 0.915V, and 5090s are at 0.895V.&lt;/p&gt; &lt;p&gt;Also this offset in VRAM is MT/s basically, so on Windows comparatively, it is half of that (+1700Mhz = +850Mhz on MSI Afterburner, +1800 = +900, +2700 = 1350, +4400 = +2200)&lt;/p&gt; &lt;p&gt;EDIT: Just as an info, maybe (not) surprisingly, the GPUs that idle at the lower power are the most efficient.&lt;/p&gt; &lt;p&gt;I.e. 5090 2 is more efficient than 5090 0, or 4090 6 is more efficient than 4090 1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5difgej3fdpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T18:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nijl9n</id>
    <title>Lightweight chat web UI that supports on-disk storage and can hook to llama.cpp</title>
    <updated>2025-09-16T14:48:55+00:00</updated>
    <author>
      <name>/u/yellow_gravel</name>
      <uri>https://old.reddit.com/user/yellow_gravel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! What options exists for a locally running web UI that is able to integrate with llama.cpp's API to provide a chat interface and store the conversations in a local database. llama.cpp's web UI is nice and simply, but it only stores data in the browser using IndexedDB. I also looked at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chatbox: only works with ollama&lt;/li&gt; &lt;li&gt;Open WebUI: very heavyweight, difficult to maintain and deploy&lt;/li&gt; &lt;li&gt;LibreChat: doesn't seem to support llama.cpp&lt;/li&gt; &lt;li&gt;LMStudio: desktop app, doesn't run a web interface&lt;/li&gt; &lt;li&gt;text-generation-webui (oobabooga): the docs leave a lot to be desired&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any other options I missed? Alternatively, if I were to build one myself, are there any LLM chat interface templates that I could reuse?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yellow_gravel"&gt; /u/yellow_gravel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijl9n/lightweight_chat_web_ui_that_supports_ondisk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijl9n/lightweight_chat_web_ui_that_supports_ondisk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nijl9n/lightweight_chat_web_ui_that_supports_ondisk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nihs0a</id>
    <title>Genuine question about RAG</title>
    <updated>2025-09-16T13:39:22+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, as many have mentioned or pointed out, I’m a bit of a noob at AI and probably coding. I’m a 43yo old techy. Yeah I’m not up on a lot of newer tech, but becoming disabled and having tons of time on my hands because I cant work has lead me to wanting to at least build myself an AI that can help me with daily tasks. I don’t have the hardware to build myself own model so I’m trying to build tools that can help augment any available LLM that I can run. I have limited funds, so I’m building what I can with what I have. But what is all the hype about RAG? I don’t understand it. And a lot of platforms just assume when you’re trying to share your code with an LLM that you want RAG. what is RAG? From what I can limitedly gather, it only looks at say a few excerpts from your code or file you upload and uses that to show the model. If I’m uploading a file I don’t want to have the UI randomly look through the code for whatever I’m saying in the chat I’m sending the code with. I’d rather the model just read my code, and respond to my question. Can someone please explain RAG. In a human readable way please? I’m just getting back into coding and I’m not as into a lot of the terminology as I probably should. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nihs0a/genuine_question_about_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nihs0a/genuine_question_about_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nihs0a/genuine_question_about_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T13:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni5tq3</id>
    <title>AMD Max+ 395 with a 7900xtx as a little helper.</title>
    <updated>2025-09-16T02:41:02+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally got around to hooking up my 7900xtx to my GMK X2. A while back some people were interested in numbers for this so here are some numbers for OSS 120B. The big win is that adding the 7900xtx didn't make it slower and in fact made everything a little faster. My experience going multi-gpu is that there is a speed penalty. In this case adding the 7900xtx is effectively like just having another 24GB added to the 128GB.&lt;/p&gt; &lt;p&gt;I'll start with a baseline run in Vulkan on just the Max+ 395.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 | 473.93 ± 3.64 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 | 51.49 ± 0.03 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 @ d20000 | 261.49 ± 0.58 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 @ d20000 | 41.03 ± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's a run in Vulkan split between the Max+ and the 7900xtx.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_vulkan: Found 2 Vulkan devices: ggml_vulkan: 0 = Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat ggml_vulkan: 1 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | ts | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------ | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | pp512 | 615.07 ± 3.11 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | tg128 | 53.08 ± 0.31 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | pp512 @ d20000 | 343.58 ± 5.11 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | tg128 @ d20000 | 40.53 ± 0.13 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And lastly, here's a split ROCm run for comparison. Vulkan is still king. Particularly as the context grows.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_cuda_init: found 2 ROCm devices: Device 0: Radeon RX 7900 XTX, gfx1100 (0x1100), VMM: no, Wave Size: 32 Device 1: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | main_gpu | fa | ts | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | -: | ------------ | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | pp512 | 566.14 ± 4.61 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | tg128 | 46.88 ± 0.15 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | pp512 @ d20000 | 397.01 ± 0.99 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | tg128 @ d20000 | 18.09 ± 0.06 | &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5tq3/amd_max_395_with_a_7900xtx_as_a_little_helper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5tq3/amd_max_395_with_a_7900xtx_as_a_little_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5tq3/amd_max_395_with_a_7900xtx_as_a_little_helper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T02:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1niin71</id>
    <title>Qwen Next vLLM fail @ 48GB</title>
    <updated>2025-09-16T14:13:14+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I cannot seem to squeeze the 4 bit ones into vram but I don't see any 3 bit ones anywhere? Is this an AWQ thing? Maybe it's just not possible?&lt;/p&gt; &lt;p&gt;If it is possible, does anyone feel like making one? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niin71/qwen_next_vllm_fail_48gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niin71/qwen_next_vllm_fail_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niin71/qwen_next_vllm_fail_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1niktfz</id>
    <title>VoxCPM-0.5B</title>
    <updated>2025-09-16T15:34:15+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"&gt; &lt;img alt="VoxCPM-0.5B" src="https://external-preview.redd.it/r3qnehuYhIo41bAc9p8n4efqIezTbTJqzszutOT9598.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c21b8a7fb420d7443519db438480fdc9bd7c71a4" title="VoxCPM-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Supports both Regular text and Phoneme input. Seems promising!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T15:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni5bao</id>
    <title>Fully local data analysis assistant (plus new Model)</title>
    <updated>2025-09-16T02:16:04+00:00</updated>
    <author>
      <name>/u/mshintaro777</name>
      <uri>https://old.reddit.com/user/mshintaro777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5bao/fully_local_data_analysis_assistant_plus_new_model/"&gt; &lt;img alt="Fully local data analysis assistant (plus new Model)" src="https://preview.redd.it/ifula3tiqfpf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=e334cd9bdf8f0d006bb34767d9f02bebb39206c1" title="Fully local data analysis assistant (plus new Model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community! Today I’m releasing an open-source, fully local data analysis assistant along with a lightweight LLM trained for it, called &lt;a href="https://quelmap.com"&gt;&lt;strong&gt;quelmap&lt;/strong&gt;&lt;/a&gt; and &lt;strong&gt;Lightning-4b&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;LLMs are amazing, but handing over all your data to a major LLM provider isn’t how it should be. Nowadays, data analysis has relied on huge context windows and very large models. Instead, we tried to see if we could cover most common analysis tasks with an efficient XML-based output format and GRPO training.&lt;/p&gt; &lt;p&gt;It even works smoothly on my &lt;strong&gt;M4 MacBook Air (16GB)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Basic Features&lt;/strong&gt;&lt;br /&gt; 📊 Data visualization&lt;br /&gt; 🚀 Table joins&lt;br /&gt; 📈 Run statistical tests&lt;br /&gt; 📂 Unlimited rows, analyze 30+ tables at once&lt;br /&gt; 🐍 Built-in Python sandbox&lt;br /&gt; 🦙 Ollama or LM Studio API integration&lt;/p&gt; &lt;p&gt;Lightning-4b is trained specifically for quelmap, and it’s been accurate and stable in generating structured outputs and Python code—more consistent than gpt-oss-120b or even Qwen3-235B in simple analysis tasks on quelmap. You can check the training details and performance here:&lt;br /&gt; 👉 &lt;a href="https://www.quelmap.com/lightning-4b/"&gt;https://www.quelmap.com/lightning-4b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s not meant for writing complex research reports or high-level business advice like Gemini-DeepResearch. But I hope it can be a helpful tool for privacy-conscious analysts and beginners who just want to explore or analyze their data safely.&lt;/p&gt; &lt;p&gt;All details, installation instructions, and source code are here:&lt;br /&gt; 🔗 Github: &lt;a href="https://github.com/quelmap-inc/quelmap"&gt;https://github.com/quelmap-inc/quelmap&lt;/a&gt;&lt;br /&gt; 🔗 HuggingFace: &lt;a href="https://huggingface.co/quelmap/Lightning-4b"&gt;https://huggingface.co/quelmap/Lightning-4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If people find this useful, I’d love to keep working on this project (agent mode, new models and more). Let me know what you think—I’d love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshintaro777"&gt; /u/mshintaro777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ifula3tiqfpf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5bao/fully_local_data_analysis_assistant_plus_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5bao/fully_local_data_analysis_assistant_plus_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T02:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni2chb</id>
    <title>Qwen3-Next 80b MLX (Mac) runs on latest LM Studio</title>
    <updated>2025-09-15T23:59:55+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was excited to see this work. About 35 tps on my M1 Mac Studio 64 gb. Takes about 42 gb. Edit: &lt;a href="https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T23:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nijikb</id>
    <title>Inference will win ultimately</title>
    <updated>2025-09-16T14:46:07+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt; &lt;img alt="Inference will win ultimately" src="https://preview.redd.it/jp7ada3lhjpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2651ab9359b4d75a0e7c1c55003fec8ea92f4fdb" title="Inference will win ultimately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;inference is where the real value shows up. it’s where models are actually used at scale.&lt;/p&gt; &lt;p&gt;A few reasons why I think this is where the winners will be: •Hardware is shifting. Morgan Stanley recently noted that more chips will be dedicated to inference than training in the years ahead. The market is already preparing for this transition. •Open-source is exploding. Meta’s Llama models alone have crossed over a billion downloads. That’s a massive long tail of developers and companies who need efficient ways to serve all kinds of models. •Agents mean real usage. Training is abstract , inference is what everyday people experience when they use agents, apps, and platforms. That’s where latency, cost, and availability matter. •Inefficiency is the opportunity. Right now GPUs are underutilized, cold starts are painful, and costs are high. Whoever cracks this at scale , making inference efficient, reliable, and accessible , will capture enormous value.&lt;/p&gt; &lt;p&gt;In short, inference isn’t just a technical detail. It’s where AI meets reality. And that’s why inference will win.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jp7ada3lhjpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nidixx</id>
    <title>Think twice before spending on GPU?</title>
    <updated>2025-09-16T10:16:07+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen team is shifting paradigm. Qwen Next is probably first big step of many that Qwen (and other chinese labs) are taking towards sparse models, because they do not have the required GPUs to train on.&lt;/p&gt; &lt;p&gt;10% of the training cost, 10x inference throughout, 512 experts, ultra long context (though not good enough yet).&lt;/p&gt; &lt;p&gt;They have a huge incentive to train this model further (on 36T tokens instead of 15T). They will probably release the final checkpoint in coming months or even weeks. Think of the electricity savings running (and on idle) a pretty capable model. We might be able to run a qwen 235B equivalent locally on a hardware under $1500. 128GB of RAM could be enough for the models this year and it's easily upgradable to 256GB for the next.&lt;/p&gt; &lt;p&gt;Wdyt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T10:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nif778</id>
    <title>Unofficial VibeVoice finetuning code released!</title>
    <updated>2025-09-16T11:47:48+00:00</updated>
    <author>
      <name>/u/Downtown-Accident-87</name>
      <uri>https://old.reddit.com/user/Downtown-Accident-87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this on discord: &lt;a href="https://github.com/voicepowered-ai/VibeVoice-finetuning"&gt;https://github.com/voicepowered-ai/VibeVoice-finetuning&lt;/a&gt;&lt;br /&gt; I will try training a lora soon, I hope it works :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Downtown-Accident-87"&gt; /u/Downtown-Accident-87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nifajh</id>
    <title>I bought a modded 4090 48GB in Shenzhen. This is my story.</title>
    <updated>2025-09-16T11:52:20+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt; &lt;img alt="I bought a modded 4090 48GB in Shenzhen. This is my story." src="https://external-preview.redd.it/1vD_R63iqu4vnM_qQf7pZNwXb9dy_UDc_Gl2j3LnTpU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5102c5612db16c04c26877a1e72e86700648e25" title="I bought a modded 4090 48GB in Shenzhen. This is my story." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe"&gt;https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few years ago, before ChatGPT became popular, I managed to score a Tesla P40 on eBay for around $150 shipped. With a few tweaks, I installed it in a Supermicro chassis. At the time, I was mostly working on video compression and simulation. It worked, but the card consistently climbed to 85°C.&lt;/p&gt; &lt;p&gt;When DeepSeek was released, I was impressed and installed Ollama in a container. With 24GB of VRAM, it worked—but slowly. After trying Stable Diffusion, it became clear that an upgrade was necessary.&lt;/p&gt; &lt;p&gt;The main issue was finding a modern GPU that could actually &lt;strong&gt;fit&lt;/strong&gt; in the server chassis. Standard 4090/5090 cards are designed for desktops: they're too large, and the power plug is inconveniently placed on top. After watching the LTT video featuring a modded 4090 with 48GB (and a follow-up from Gamers Nexus), I started searching the only place I knew might have one: Alibaba.com.&lt;/p&gt; &lt;p&gt;I contacted a seller and got a quote: &lt;strong&gt;CNY 22,900&lt;/strong&gt;. Pricey, but cheaper than expected. However, Alibaba enforces VAT collection, and I’ve had bad experiences with DHL—there was a non-zero chance I’d be charged twice for taxes. I was already over €700 in taxes and fees.&lt;/p&gt; &lt;p&gt;Just for fun, I checked &lt;a href="http://Trip.com"&gt;Trip.com&lt;/a&gt; and realized that for the same amount of money, I could fly to Hong Kong and back, with a few days to explore. After confirming with the seller that they’d meet me at their business location, I booked a flight and an Airbnb in Hong Kong.&lt;/p&gt; &lt;p&gt;For context, I don’t speak Chinese at all. Finding the place using a Chinese address was tricky. Google Maps is useless in China, Apple Maps gave some clues, and Baidu Maps was beyond my skill level. With a little help from DeepSeek, I decoded the address and located the place in an industrial estate outside the city center. Thanks to Shenzhen’s extensive metro network, I didn’t need a taxi.&lt;/p&gt; &lt;p&gt;After arriving, the manager congratulated me for being the first foreigner to find them unassisted. I was given the card from a large batch—they’re clearly producing these in volume at a factory elsewhere in town (I was proudly shown videos of the assembly line). I asked them to retest the card so I could verify its authenticity.&lt;/p&gt; &lt;p&gt;During the office tour, it was clear that their next frontier is repurposing old mining cards. I saw a large collection of NVIDIA Ampere mining GPUs. I was also told that modded 5090s with over 96GB of VRAM are in development.&lt;/p&gt; &lt;p&gt;After the test was completed, I paid in cash (a &lt;em&gt;lot&lt;/em&gt; of banknotes!) and returned to Hong Kong with my new purchase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
