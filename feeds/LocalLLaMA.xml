<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-23T07:49:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pt6i9b</id>
    <title>CUTIA - compress prompts without degrading eval scores</title>
    <updated>2025-12-22T18:09:40+00:00</updated>
    <author>
      <name>/u/Camvizioneer</name>
      <uri>https://old.reddit.com/user/Camvizioneer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt6i9b/cutia_compress_prompts_without_degrading_eval/"&gt; &lt;img alt="CUTIA - compress prompts without degrading eval scores" src="https://preview.redd.it/brfkuaig0r8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc3ad45957e9aaf933806a3f1d50456c9bdde2da" title="CUTIA - compress prompts without degrading eval scores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish someone motivated me like overoptimized prompts motivate LLMs.&lt;/p&gt; &lt;p&gt;But often prompt optimizers go too far - mixing genuinely useful instructions with a bunch of noise. Some time ago, after yet another round of manually pruning bloated prompts and running evals to verify the score didn't tank, I decided to build a prompt compressor to automate this tedious work.&lt;/p&gt; &lt;p&gt;Please welcome &lt;strong&gt;CUTIA&lt;/strong&gt; - a quality-aware prompt compressor that splits prompts into segments and then tries to cut/rewrite each chunk, making sure that eval score is not degrading. Since I'm a &lt;strong&gt;DSPy&lt;/strong&gt; user, first of all I've implemented this compressor as a custom DSPy optimizer. Next, I plan to create a framework-agnostic version which could be adopted to any other platform.&lt;/p&gt; &lt;p&gt;This compressor doesn't require a strong teacher model - I tested it during development and am now using it mostly with gpt-oss-20b. But don't go below it - smaller models I tested struggled with splitting prompts into chunks correctly. I plan to improve this in a future release.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/napmany/cutia"&gt;https://github.com/napmany/cutia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's still plenty I want to improve and experiment with, but CUTIA successfully compressed my DSPy pipeline (and even slightly improved eval scores), so I figured it's ready to share. Hope it helps someone else reduce their token footprint too :)&lt;/p&gt; &lt;p&gt;Happy to answer questions or hear feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Camvizioneer"&gt; /u/Camvizioneer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/brfkuaig0r8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt6i9b/cutia_compress_prompts_without_degrading_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt6i9b/cutia_compress_prompts_without_degrading_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T18:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1psyqha</id>
    <title>upstage/Solar-Open-100B · Hugging Face</title>
    <updated>2025-12-22T12:47:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"&gt; &lt;img alt="upstage/Solar-Open-100B · Hugging Face" src="https://external-preview.redd.it/KGZNZzWd5K6vM05pplkzhPrPxHhbMAP1w-s6MnLTkhM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=559420850549bc9f134f2c1908f869fba5d48dbf" title="upstage/Solar-Open-100B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...do you remember &lt;a href="https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0"&gt;https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0&lt;/a&gt; from 2024?&lt;/p&gt; &lt;p&gt;It looks like they have something new:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#solar-open"&gt;&lt;/a&gt;Solar Open&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Solar Open&lt;/strong&gt; is Upstage's flagship &lt;strong&gt;102B-parameter&lt;/strong&gt; large language model, trained &lt;strong&gt;entirely from scratch&lt;/strong&gt; and released under the &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (see &lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;). As a &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; architecture, it delivers enterprise-grade performance in reasoning, instruction-following, and agentic capabilities—all while prioritizing transparency and customization for the open-source community.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#highlights"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MoE Architecture (102B / 12B):&lt;/strong&gt; Built on a Mixture-of-Experts architecture with &lt;strong&gt;102B total / 12B active parameters&lt;/strong&gt;. This design delivers the knowledge depth of a massive model with the inference speed and cost-efficiency of a much smaller model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massive Training Scale:&lt;/strong&gt; Pre-trained on &lt;strong&gt;19.7 trillion tokens&lt;/strong&gt;, ensuring broad knowledge coverage and robust reasoning capabilities across various domains.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#model-overview"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Overview&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Name:&lt;/strong&gt; Solar Open 100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face ID:&lt;/strong&gt; Upstage/Solar-Open-100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Mixture-of-Experts (MoE) &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total Parameters:&lt;/strong&gt; 102.6B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Active Parameters:&lt;/strong&gt; 12B (per token)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experts:&lt;/strong&gt; 129 Experts (top 8 among 128 Routed + 1 Shared)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-training Tokens:&lt;/strong&gt; 19.7 Trillion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Length:&lt;/strong&gt; 128k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Hardware:&lt;/strong&gt; NVIDIA B200 GPUs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (See &lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptn2lq</id>
    <title>Batch OCR: Dockerized PaddleOCR pipeline to convert thousands of PDFs into clean text (GPU/CPU, Windows + Linux)</title>
    <updated>2025-12-23T06:39:03+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"&gt; &lt;img alt="Batch OCR: Dockerized PaddleOCR pipeline to convert thousands of PDFs into clean text (GPU/CPU, Windows + Linux)" src="https://external-preview.redd.it/jtfq7WMsQ6MVADp_C19uPunr_Ib9UQe2B12piwoyxvY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e39d273b2df321cf09ad739dc6906b9cc539ebfc" title="Batch OCR: Dockerized PaddleOCR pipeline to convert thousands of PDFs into clean text (GPU/CPU, Windows + Linux)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear All,&lt;/p&gt; &lt;p&gt;I just open-sourced Batch OCR — a Dockerized, PaddleOCR-based pipeline for turning large collections of PDFs into clean text files. After testing many OCR/model options from Hugging Face, I settled on PaddleOCR for its speed and accuracy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/94wg0beyfw8g1.png?width=2740&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f9ac2791c12f525cc1bf1b5f16cbf6f2731fb7c"&gt;https://preview.redd.it/94wg0beyfw8g1.png?width=2740&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f9ac2791c12f525cc1bf1b5f16cbf6f2731fb7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A simple Gradio UI lets you choose a folder and recursively process PDFs into .txt files for indexing, search, or LLM training.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/BoltzmannEntropy/batch-ocr"&gt;https://github.com/BoltzmannEntropy/batch-ocr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2u3cptgfgw8g1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d6398e16609884a5bd2aca850e294b83c921f75"&gt;https://preview.redd.it/2u3cptgfgw8g1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d6398e16609884a5bd2aca850e294b83c921f75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;- Process hundreds or thousands of PDFs reliably&lt;/p&gt; &lt;p&gt;- Extract embedded text when available; fall back to OCR when needed&lt;/p&gt; &lt;p&gt;- Produce consistent, clean text with a lightweight quality filter&lt;/p&gt; &lt;p&gt;- Mirror the input folder structure and write results under ocr_results&lt;/p&gt; &lt;p&gt;- GPU or CPU: Uses PaddlePaddle CUDA when available; CPU fallback&lt;/p&gt; &lt;p&gt;- Simple UI: Select folder, list PDFs, initialize OCR, run batch&lt;/p&gt; &lt;p&gt;- Clean output: Writes &amp;lt;name&amp;gt;_ocr.txt per PDF; errors as &amp;lt;name&amp;gt;_ERROR.txt&lt;/p&gt; &lt;p&gt;- Cross‑platform: Windows and Linux/macOS via Docker&lt;/p&gt; &lt;p&gt;- Privacy: Everything runs locally; no cloud calls&lt;/p&gt; &lt;p&gt;Feedback and contributions welcome. If you try it on a large dataset or different languages, I’d love to hear how it goes.&lt;/p&gt; &lt;p&gt;Best,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T06:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptibgg</id>
    <title>vLLM adds support for the new GLM-4.7 model</title>
    <updated>2025-12-23T02:31:19+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptibgg/vllm_adds_support_for_the_new_glm47_model/"&gt; &lt;img alt="vLLM adds support for the new GLM-4.7 model" src="https://preview.redd.it/etmoh4i57v8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31a204e1d56e3d9b9ff42a7d6e896f95a21fe6cb" title="vLLM adds support for the new GLM-4.7 model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Highlights of GLM 4.7 &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Core Coding:&lt;/strong&gt; GLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vibe Coding:&lt;/strong&gt; GLM-4.7 produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Using:&lt;/strong&gt; GLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as τ^2-Bench and on web browsing via BrowseComp.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Reasoning:&lt;/strong&gt; GLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanity’s Last Exam) benchmark compared to GLM-4.6.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM.html"&gt;https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/etmoh4i57v8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptibgg/vllm_adds_support_for_the_new_glm47_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptibgg/vllm_adds_support_for_the_new_glm47_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T02:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptkqee</id>
    <title>2025 LLM's vs 2007 AI</title>
    <updated>2025-12-23T04:30:31+00:00</updated>
    <author>
      <name>/u/Rombodawg</name>
      <uri>https://old.reddit.com/user/Rombodawg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"&gt; &lt;img alt="2025 LLM's vs 2007 AI" src="https://b.thumbs.redditmedia.com/GuKc03QNuV73YqFB8sWLS9E5-uR0Eq9JKINJ0eokOPI.jpg" title="2025 LLM's vs 2007 AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2025: Gpt 5.2, Gemini 3.0, Claude 4.5 opus: 20% fail rate on most tasks &lt;/p&gt; &lt;p&gt;2007: Akinator: 100% success rate literally reading your mind&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i90p0gh0tv8g1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5f7a16e77dc19aa9ae5386495880045aebd70e15"&gt;https://preview.redd.it/i90p0gh0tv8g1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5f7a16e77dc19aa9ae5386495880045aebd70e15&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rombodawg"&gt; /u/Rombodawg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T04:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pthijn</id>
    <title>Best free “uncensored” local LLM for RTX 3060 12GB (Portuguese, up to 13B 4-bit)?</title>
    <updated>2025-12-23T01:53:01+00:00</updated>
    <author>
      <name>/u/Big_Preparation_6869</name>
      <uri>https://old.reddit.com/user/Big_Preparation_6869</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I want to run a local LLM on my PC and I’m looking for recommendations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My PC:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: Ryzen 5 3400G&lt;/li&gt; &lt;li&gt;GPU: RTX 3060 (12GB VRAM)&lt;/li&gt; &lt;li&gt;RAM: 24GB (2x8GB DDR4 3600MHz + 1x8GB DDR4 2666MHz)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I want:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; models, as “uncensored” as possible&lt;/li&gt; &lt;li&gt;Good &lt;strong&gt;Portuguese&lt;/strong&gt; performance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recent&lt;/strong&gt; and strong overall quality&lt;/li&gt; &lt;li&gt;Ability to &lt;strong&gt;fine-tune&lt;/strong&gt; (LoRA/fine-tuning)&lt;/li&gt; &lt;li&gt;If possible, &lt;strong&gt;web browsing&lt;/strong&gt; via a tool/integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limit:&lt;/strong&gt; up to &lt;strong&gt;13B (4-bit)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Which models do you recommend, and which quantization/format (GGUF, GPTQ, AWQ, etc.) works best on this setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Preparation_6869"&gt; /u/Big_Preparation_6869 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pthijn/best_free_uncensored_local_llm_for_rtx_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pthijn/best_free_uncensored_local_llm_for_rtx_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pthijn/best_free_uncensored_local_llm_for_rtx_3060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T01:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt4248</id>
    <title>Minimax M2.1 is out!</title>
    <updated>2025-12-22T16:35:35+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt4248/minimax_m21_is_out/"&gt; &lt;img alt="Minimax M2.1 is out!" src="https://b.thumbs.redditmedia.com/y0pyViSqdxRwpxmwfDwnxykLzOFo0B_V1-Rg8O1Ep2A.jpg" title="Minimax M2.1 is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uncrsome9s8g1.png?width=1687&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=abbc7ab3db68075fe7eb6827bf0122e5dcfce9b4"&gt;https://agent.minimax.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://agent.minimax.io/"&gt;https://agent.minimax.io/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt4248/minimax_m21_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt4248/minimax_m21_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt4248/minimax_m21_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T16:35:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt8hpn</id>
    <title>I built a benchmark to test which LLMs would kill you in the apocalypse. The answer: all of them, just in different ways.</title>
    <updated>2025-12-22T19:26:06+00:00</updated>
    <author>
      <name>/u/tmanchester</name>
      <uri>https://old.reddit.com/user/tmanchester</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Grid's dead. Internet's gone. But you've got a solar-charged laptop and some open-weight models you downloaded before everything went dark. Three weeks in, you find a pressure canner and ask your local LLM how to safely can food for winter.&lt;/p&gt; &lt;p&gt;If you're running LLaMA 3.1 8B, you just got advice that would give you botulism.&lt;/p&gt; &lt;p&gt;I spent the past few days building apocalypse-bench: 305 questions across 13 survival domains (agriculture, medicine, chemistry, engineering, etc.). Each answer gets graded on a rubric with &amp;quot;auto-fail&amp;quot; conditions for advice dangerous enough to kill you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model ID&lt;/th&gt; &lt;th align="left"&gt;Overall Score (Mean)&lt;/th&gt; &lt;th align="left"&gt;Auto-Fail Rate&lt;/th&gt; &lt;th align="left"&gt;Median Latency (ms)&lt;/th&gt; &lt;th align="left"&gt;Total Questions&lt;/th&gt; &lt;th align="left"&gt;Completed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;openai/gpt-oss-20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.78&lt;/td&gt; &lt;td align="left"&gt;6.89%&lt;/td&gt; &lt;td align="left"&gt;1,841&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;google/gemma-3-12b-it&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.41&lt;/td&gt; &lt;td align="left"&gt;6.56%&lt;/td&gt; &lt;td align="left"&gt;15,015&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;qwen3-8b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.33&lt;/td&gt; &lt;td align="left"&gt;6.67%&lt;/td&gt; &lt;td align="left"&gt;8,862&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;nvidia/nemotron-nano-9b-v2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.02&lt;/td&gt; &lt;td align="left"&gt;8.85%&lt;/td&gt; &lt;td align="left"&gt;18,288&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;liquid/lfm2-8b-a1b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;6.56&lt;/td&gt; &lt;td align="left"&gt;9.18%&lt;/td&gt; &lt;td align="left"&gt;4,910&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;meta-llama/llama-3.1-8b-instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5.58&lt;/td&gt; &lt;td align="left"&gt;15.41%&lt;/td&gt; &lt;td align="left"&gt;700&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;td align="left"&gt;305&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;The highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLaMA 3.1&lt;/strong&gt; advised heating canned beans to 180°F to kill botulism. Botulism spores laugh at that temperature. It also refuses to help you make alcohol for wound disinfection (safety first!), but will happily guide you through a fake penicillin extraction that produces nothing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; told me to identify mystery garage liquids by holding a lit match near them. Same model scored highest on &amp;quot;Very Hard&amp;quot; questions and perfectly recalled ancient Roman cement recipes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS&lt;/strong&gt; (the winner) refuses to explain a centuries-old breech birth procedure, but when its guardrails don't fire, it advises putting unknown chemicals in your mouth to identify them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma&lt;/strong&gt; gave flawless instructions for saving cabbage seeds, except it told you to break open the head and collect them. Cabbages don't have seeds in the head. You'd destroy your vegetable supply finding zero seeds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nemotron&lt;/strong&gt; correctly identified that sulfur would fix your melting rubber boots... then told you not to use it because &amp;quot;it requires precise application.&amp;quot; Its alternative? Rub salt on them. This would do nothing. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The takeaway:&lt;/strong&gt; No single model will keep you alive. The safest strategy is a &amp;quot;survival committee&amp;quot;, different models for different domains. And a book or two.&lt;/p&gt; &lt;p&gt;Full article here: &lt;a href="https://www.crowlabs.tech/blog/apocalypse-bench"&gt;https://www.crowlabs.tech/blog/apocalypse-bench&lt;/a&gt;&lt;br /&gt; Github link: &lt;a href="https://github.com/tristanmanchester/apocalypse-bench"&gt;https://github.com/tristanmanchester/apocalypse-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tmanchester"&gt; /u/tmanchester &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt8hpn/i_built_a_benchmark_to_test_which_llms_would_kill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt8hpn/i_built_a_benchmark_to_test_which_llms_would_kill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt8hpn/i_built_a_benchmark_to_test_which_llms_would_kill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T19:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptjntu</id>
    <title>AMD Ryzen AI MAX+ 395 vs Ryzen 9 7940HS vs Ryzen 7 5700G</title>
    <updated>2025-12-23T03:36:42+00:00</updated>
    <author>
      <name>/u/jstormes</name>
      <uri>https://old.reddit.com/user/jstormes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working with Qwen3-coder and did a quick set of benchmarks across the three AMD systems I had handy.&lt;/p&gt; &lt;p&gt;I though this group might find it interesting. The &lt;a href="https://github.com/jstormes/StrixHalo/blob/main/README.md"&gt;README.md&lt;/a&gt; has what I did and how.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jstormes/StrixHalo"&gt;https://github.com/jstormes/StrixHalo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;## Performance Summary&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;System&lt;/th&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;RAM&lt;/th&gt; &lt;th align="left"&gt;Max Context&lt;/th&gt; &lt;th align="left"&gt;Propmt&lt;/th&gt; &lt;th align="left"&gt;Generation&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Ryzen AI Max+ 395&lt;/td&gt; &lt;td align="left"&gt;Radeon 8060S&lt;/td&gt; &lt;td align="left"&gt;128GB&lt;/td&gt; &lt;td align="left"&gt;1M&lt;/td&gt; &lt;td align="left"&gt;~450 tok/s&lt;/td&gt; &lt;td align="left"&gt;~40 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ryzen 9 7940HS&lt;/td&gt; &lt;td align="left"&gt;Radeon 780M&lt;/td&gt; &lt;td align="left"&gt;64GB DDR5&lt;/td&gt; &lt;td align="left"&gt;512K&lt;/td&gt; &lt;td align="left"&gt;~30 tok/s&lt;/td&gt; &lt;td align="left"&gt;~31 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ryzen 7 5700G&lt;/td&gt; &lt;td align="left"&gt;Radeon Vega&lt;/td&gt; &lt;td align="left"&gt;64GB DDR4&lt;/td&gt; &lt;td align="left"&gt;256K&lt;/td&gt; &lt;td align="left"&gt;~74 tok/s&lt;/td&gt; &lt;td align="left"&gt;~13 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jstormes"&gt; /u/jstormes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjntu/amd_ryzen_ai_max_395_vs_ryzen_9_7940hs_vs_ryzen_7/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjntu/amd_ryzen_ai_max_395_vs_ryzen_9_7940hs_vs_ryzen_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjntu/amd_ryzen_ai_max_395_vs_ryzen_9_7940hs_vs_ryzen_7/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T03:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstlas</id>
    <title>major open-source releases this year</title>
    <updated>2025-12-22T07:30:46+00:00</updated>
    <author>
      <name>/u/sahilypatel</name>
      <uri>https://old.reddit.com/user/sahilypatel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt; &lt;img alt="major open-source releases this year" src="https://preview.redd.it/wynfuvk9kp8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=763bc1a7f949dc4ff18c4a976a10f017205abb54" title="major open-source releases this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sahilypatel"&gt; /u/sahilypatel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wynfuvk9kp8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt27mo</id>
    <title>GLM-4.7 Scores 42% on Humanities Last Exam?!</title>
    <updated>2025-12-22T15:22:13+00:00</updated>
    <author>
      <name>/u/domlincog</name>
      <uri>https://old.reddit.com/user/domlincog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt27mo/glm47_scores_42_on_humanities_last_exam/"&gt; &lt;img alt="GLM-4.7 Scores 42% on Humanities Last Exam?!" src="https://a.thumbs.redditmedia.com/4GGVQek6-eaur5WyiUtyKFnXd9YNQnnpfW6fwuRWM74.jpg" title="GLM-4.7 Scores 42% on Humanities Last Exam?!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Noticed in docs. Seems like this isn't a small release at all, time will tell.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ft6vl98wr8g1.png?width=865&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=075a4c5313a4fced4590edaa175c75e9e81a53f2"&gt;https://preview.redd.it/0ft6vl98wr8g1.png?width=865&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=075a4c5313a4fced4590edaa175c75e9e81a53f2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.z.ai/guides/llm/glm-4.7"&gt;https://docs.z.ai/guides/llm/glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/domlincog"&gt; /u/domlincog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt27mo/glm47_scores_42_on_humanities_last_exam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt27mo/glm47_scores_42_on_humanities_last_exam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt27mo/glm47_scores_42_on_humanities_last_exam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T15:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptd60q</id>
    <title>Stop wasting your MCP context window. LTP (Lazy Tool Protocol) reduces tool-calling overhead by up to 93 percent.</title>
    <updated>2025-12-22T22:37:38+00:00</updated>
    <author>
      <name>/u/song-junhyeong</name>
      <uri>https://old.reddit.com/user/song-junhyeong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd60q/stop_wasting_your_mcp_context_window_ltp_lazy/"&gt; &lt;img alt="Stop wasting your MCP context window. LTP (Lazy Tool Protocol) reduces tool-calling overhead by up to 93 percent." src="https://b.thumbs.redditmedia.com/ZmFYqL2BAkBid2YXZuXdBnn96S3WsyuZUJ2eKu54gbg.jpg" title="Stop wasting your MCP context window. LTP (Lazy Tool Protocol) reduces tool-calling overhead by up to 93 percent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on a solution for a problem that has been bothering me with AI agents: the massive hidden cost of tool definitions.&lt;/p&gt; &lt;p&gt;Current implementations of the Model Context Protocol (MCP) typically require loading full tool schemas into the AI's context at the start. If you are using a large library of tools, you can easily burn through 60,000 to 300,000 tokens just to define what the tools do before any actual work begins.&lt;/p&gt; &lt;p&gt;I built LTP (Lazy Tool Protocol) to solve this through a Lazy Loading pattern.&lt;/p&gt; &lt;p&gt;Instead of bloating the context window, LTP uses a CLI bridge that allows the AI to discover and fetch tool information only when necessary.&lt;/p&gt; &lt;p&gt;Key Benchmarks from v0.1.0:&lt;/p&gt; &lt;p&gt;93 Percent Token Reduction: In tests with 100 tool calls, LTP reduced token consumption from 300,000 to just 20,000.&lt;/p&gt; &lt;p&gt;Efficiency at Scale: While traditional MCP usage grows linearly with the number of calls, LTP maintains a near-fixed discovery cost.&lt;/p&gt; &lt;p&gt;The --schema Flag: This new feature provides compact function signatures to the AI at the start of a session. It eliminates the need for repeated metadata calls while keeping the context footprint minimal.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;Unlimited Tools: You can connect hundreds or thousands of MCP tools without degrading reasoning performance or hitting context limits.&lt;/p&gt; &lt;p&gt;Executable Crafts: We are moving beyond static instructions. A &amp;quot;Craft&amp;quot; is a package containing precise AI prompts and executable automation scripts to ensure reliability.&lt;/p&gt; &lt;p&gt;Security-First Design: It includes a built-in whitelist, sandbox path restrictions, and mandatory confirmation for high-risk operations like file deletions.&lt;/p&gt; &lt;p&gt;How to use it: The protocol works by giving your AI a system prompt that teaches it how to interact with the LTP CLI. The AI can then search for tools, read schemas on-demand, and execute them as needed.&lt;/p&gt; &lt;p&gt;I have released this as an open-source project and am running the registry on my own infrastructure to support the community.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/JuN-B-official/ltp"&gt;https://github.com/JuN-B-official/ltp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Url: &lt;a href="https://ltp.jun-b.com"&gt;https://ltp.jun-b.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Efficiency Analysis: &lt;a href="https://ltp.jun-b.com/docs/effect"&gt;https://ltp.jun-b.com/docs/effect&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/song-junhyeong"&gt; /u/song-junhyeong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ptd60q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd60q/stop_wasting_your_mcp_context_window_ltp_lazy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd60q/stop_wasting_your_mcp_context_window_ltp_lazy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T22:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptjmb0</id>
    <title>MiniMax M2.1 benchmark</title>
    <updated>2025-12-23T03:34:38+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"&gt; &lt;img alt="MiniMax M2.1 benchmark" src="https://b.thumbs.redditmedia.com/OHdACrr3ilm8bDAbmlW2TC91DBmLNVmeavTzyzCbMgM.jpg" title="MiniMax M2.1 benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lxif8yh0jv8g1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1f787941640a15bee52988c35f35c0c1e5c3eca"&gt;https://preview.redd.it/lxif8yh0jv8g1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1f787941640a15bee52988c35f35c0c1e5c3eca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Multi-language Coding (beyond Python) SOTA across Rust, Java, Go, C++, Kotlin, Obj-C, TS &amp;amp; JS, scoring 72.5% for SWE-bench Multilingual and exceeding Gemini 3 Pro and Claude Sonnet 4.5.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;AppDev &amp;amp; WebDev Major upgrades for native Android &amp;amp; iOS, plus stronger web aesthetics + realistic scientific simulations. Not only vibe WebDev, but also vibe AppDev.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Lightning Fast with Concise Reasoning Faster responses, more concise reasoning, and significantly reduced token consumption.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Advanced Interleaved Thinking &amp;amp; Instruction Following Excels at integrating &amp;quot;composite instruction constraints&amp;quot; (as seen in OctoCodingBench), ready for office automation tasks.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T03:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptd1nc</id>
    <title>GLM-4.7 FP8 on 4x6000 pro blackwells</title>
    <updated>2025-12-22T22:32:26+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ptd1nc/video/oueyacty0u8g1/player"&gt;https://reddit.com/link/1ptd1nc/video/oueyacty0u8g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4.7 FP8 sglang mtp fp8 e4m3fn KVCache on 4x6000 Blackwell pro max can get 140k context and mtp is faster then last time I had this with 4.6. May be due to using new sglang with newer jit flashinfer for sm120. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd1nc/glm47_fp8_on_4x6000_pro_blackwells/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd1nc/glm47_fp8_on_4x6000_pro_blackwells/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd1nc/glm47_fp8_on_4x6000_pro_blackwells/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T22:32:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptifqq</id>
    <title>MiniMax M2.1 released on openrouter!</title>
    <updated>2025-12-23T02:37:16+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/minimax/minimax-m2.1"&gt;https://openrouter.ai/minimax/minimax-m2.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.minimax.io/news/minimax-m21"&gt;https://www.minimax.io/news/minimax-m21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/api-reference/text-intro"&gt;https://platform.minimax.io/docs/api-reference/text-intro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptifqq/minimax_m21_released_on_openrouter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptifqq/minimax_m21_released_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptifqq/minimax_m21_released_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T02:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt18x4</id>
    <title>NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!</title>
    <updated>2025-12-22T14:42:56+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"&gt; &lt;img alt="NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!" src="https://preview.redd.it/k20itq6cpr8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28c0951b3081aafea1e185ecbd82f4f0fc54726f" title="NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog Link: &lt;a href="https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/"&gt;https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You'll learn about: - Training methods: LoRA, FFT, RL - When to fine-tune and why + use-cases - Amount of data and VRAM needed - How to train locally on DGX Spark, RTX GPUs &amp;amp; more&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k20itq6cpr8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T14:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptm3n4</id>
    <title>GLM 4.7 top the chart at Rank #6 in WebDev</title>
    <updated>2025-12-23T05:43:37+00:00</updated>
    <author>
      <name>/u/GeLaMi-Speaker</name>
      <uri>https://old.reddit.com/user/GeLaMi-Speaker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"&gt; &lt;img alt="GLM 4.7 top the chart at Rank #6 in WebDev" src="https://b.thumbs.redditmedia.com/s0EknbN86ZnaumUiKb5KC7z3WQ1YE737LQCzHo-j3pg.jpg" title="GLM 4.7 top the chart at Rank #6 in WebDev" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;https://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeLaMi-Speaker"&gt; /u/GeLaMi-Speaker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ptm3n4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T05:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt5jfn</id>
    <title>GLM 4.7 released!</title>
    <updated>2025-12-22T17:32:39+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/"&gt; &lt;img alt="GLM 4.7 released!" src="https://b.thumbs.redditmedia.com/gArPotffsWB1U2wwy4EN2C6LihhGp5xRmDZVZabOdLE.jpg" title="GLM 4.7 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.7 is here!&lt;/p&gt; &lt;p&gt;GLM-4.7 surpasses GLM-4.6 with substantial improvements in coding, complex reasoning, and tool usage, setting new open-source SOTA standards. It also boosts performance in chat, creative writing, and role-play scenarios.&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="http://huggingface.co/zai-org/GLM-4.7"&gt;http://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech Blog: &lt;a href="http://z.ai/blog/glm-4.7"&gt;http://z.ai/blog/glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pt5jfn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptb4jj</id>
    <title>GLM-4.7 GGUF is here!</title>
    <updated>2025-12-22T21:12:09+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptb4jj/glm47_gguf_is_here/"&gt; &lt;img alt="GLM-4.7 GGUF is here!" src="https://external-preview.redd.it/TGrfpTKzENR2d9AEv3GwEXRPg1BfQXmQleE_vDAQ5qs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c04adace1535fe995e55aae71ce5fa9e94f80d8" title="GLM-4.7 GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still in the process of quantizing, it's a big model :)&lt;br /&gt; HF: &lt;a href="https://huggingface.co/AaryanK/GLM-4.7-GGUF"&gt;https://huggingface.co/AaryanK/GLM-4.7-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptb4jj/glm47_gguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptb4jj/glm47_gguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T21:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt3sco</id>
    <title>I made Soprano-80M: Stream ultra-realistic TTS in &lt;15ms, up to 2000x realtime, and &lt;1 GB VRAM, released under Apache 2.0!</title>
    <updated>2025-12-22T16:24:36+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"&gt; &lt;img alt="I made Soprano-80M: Stream ultra-realistic TTS in &amp;lt;15ms, up to 2000x realtime, and &amp;lt;1 GB VRAM, released under Apache 2.0!" src="https://external-preview.redd.it/NzJvcm5nM3g1czhnMTuHKMiW0LLPLmT-UAsj3QPgelU3LLUn7ZzaJN_zFkuW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92e3fd36cfab3db61e8dc90b692c7a0d1918ce30" title="I made Soprano-80M: Stream ultra-realistic TTS in &amp;lt;15ms, up to 2000x realtime, and &amp;lt;1 GB VRAM, released under Apache 2.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I’m Eugene, and I’ve been working on &lt;strong&gt;Soprano&lt;/strong&gt;: a new state-of-the-art TTS model I designed for voice chatbots. Voice applications require very low latency and natural speech generation to sound convincing, and I created Soprano to deliver on both of these goals.&lt;/p&gt; &lt;p&gt;Soprano is the world’s fastest TTS by an enormous margin. It is optimized to stream audio playback with &lt;strong&gt;&amp;lt;15 ms latency&lt;/strong&gt;, 10x faster than any other realtime TTS model like Chatterbox Turbo, VibeVoice-Realtime, GLM TTS, or CosyVoice3. It also natively supports batched inference, benefiting greatly from long-form speech generation. &lt;strong&gt;I was able to generate a 10-hour audiobook in under 20 seconds, achieving ~2000x realtime!&lt;/strong&gt; This is multiple orders of magnitude faster than any other TTS model, making ultra-fast, ultra-natural TTS a reality for the first time.&lt;/p&gt; &lt;p&gt;I owe these gains to the following design choices:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Higher sample rate:&lt;/strong&gt; most TTS models use a sample rate of 24 kHz, which can cause s and z sounds to be muffled. In contrast, Soprano natively generates 32 kHz audio, which sounds much sharper and clearer. In fact, 32 kHz speech sounds indistinguishable from 44.1/48 kHz speech, so I found it to be the best choice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vocoder-based audio decoder:&lt;/strong&gt; Most TTS designs use diffusion models to convert LLM outputs into audio waveforms. However, this comes at the cost of slow generation. To fix this, I trained a vocoder-based decoder instead, which uses a Vocos model to perform this conversion. My decoder runs several orders of magnitude faster than diffusion-based decoders (~6000x realtime!), enabling extremely fast audio generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless Streaming:&lt;/strong&gt; Streaming usually requires generating multiple audio chunks and applying crossfade. However, this causes streamed output to sound worse than nonstreamed output. I solve this by using a Vocos-based decoder. Because Vocos has a finite receptive field. I can exploit its input locality to completely skip crossfading, producing streaming output that is identical to unstreamed output. Furthermore, I modified the Vocos architecture to reduce the receptive field, allowing Soprano to start streaming audio after generating just five audio tokens with the LLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art Neural Audio Codec:&lt;/strong&gt; Speech is represented using a novel neural codec that compresses audio to ~15 tokens/sec at just 0.2 kbps. This helps improve generation speed, as only 15 tokens need to be generated to synthesize 1 second of audio, compared to 25, 50, or other commonly used token rates. To my knowledge, this is the highest bitrate compression achieved by any audio codec.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infinite generation length:&lt;/strong&gt; Soprano automatically generates each sentence independently, and then stitches the results together. Theoretically, this means that sentences can no longer influence each other, but in practice I found that this doesn’t really happen anyway. Splitting by sentences allows for batching on long inputs, dramatically improving inference speed. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’m a second-year undergrad who’s just started working on TTS models, so I wanted to start small. Soprano was only pretrained on 1000 hours of audio (~100x less than other TTS models), so its stability and quality will improve tremendously as I train it on more data. Also, I optimized Soprano purely for speed, which is why it lacks bells and whistles like voice cloning, style control, and multilingual support. Now that I have experience creating TTS models, I have a lot of ideas for how to make Soprano even better in the future, so stay tuned for those!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface Demo: &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Weights: &lt;a href="https://huggingface.co/ekwek/Soprano-80M"&gt;https://huggingface.co/ekwek/Soprano-80M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/htwi2n2x5s8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T16:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt5heq</id>
    <title>GLM 4.7 is out on HF!</title>
    <updated>2025-12-22T17:30:33+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"&gt; &lt;img alt="GLM 4.7 is out on HF!" src="https://external-preview.redd.it/gR0grxFGZc9MSnGGFWbK39DsDkjKEI-u2jMcygDp6Nc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb92abe1e270f4c3e9d804bcff4653d2d0d7cc74" title="GLM 4.7 is out on HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptk5fs</id>
    <title>Unsloth GLM-4.7 GGUF</title>
    <updated>2025-12-23T04:01:15+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T04:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptdtmz</id>
    <title>DGX Spark: an unpopular opinion</title>
    <updated>2025-12-22T23:05:29+00:00</updated>
    <author>
      <name>/u/emdblc</name>
      <uri>https://old.reddit.com/user/emdblc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt; &lt;img alt="DGX Spark: an unpopular opinion" src="https://preview.redd.it/cktkoyb16u8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddc1aaf35931031505022ffcc5838d1fb7a1a8ea" title="DGX Spark: an unpopular opinion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there has been a lot of criticism about the DGX Spark here, so I want to share some of my personal experience and opinion:&lt;/p&gt; &lt;p&gt;I’m a doctoral student doing data science in a small research group that doesn’t have access to massive computing resources. We only have a handful of V100s and T4s in our local cluster, and limited access to A100s and L40s on the university cluster (two at a time). Spark lets us prototype and train foundation models, and (at last) compete with groups that have access to high performance GPUs like the H100s or H200s.&lt;/p&gt; &lt;p&gt;I want to be clear: Spark is NOT faster than an H100 (or even a 5090). But its all-in-one design and its massive amount of memory (all sitting on your desk) enable us — a small group with limited funding, to do more research.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emdblc"&gt; /u/emdblc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cktkoyb16u8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T23:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
</feed>
