<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-05T01:11:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1peh1bg</id>
    <title>RTX 5090: Designed for the Age of Neural Rendering (Hot Chips 2025 slides)</title>
    <updated>2025-12-05T00:42:47+00:00</updated>
    <author>
      <name>/u/Primary_Olive_5444</name>
      <uri>https://old.reddit.com/user/Primary_Olive_5444</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peh1bg/rtx_5090_designed_for_the_age_of_neural_rendering/"&gt; &lt;img alt="RTX 5090: Designed for the Age of Neural Rendering (Hot Chips 2025 slides)" src="https://b.thumbs.redditmedia.com/fVrJOrywdqj1FxoEEoAB0wo49Hhm4HvD0Gc5nkE1Unw.jpg" title="RTX 5090: Designed for the Age of Neural Rendering (Hot Chips 2025 slides)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g0nlmmwb4a5g1.png?width=1537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a9c6d552cd8db59bb4e051a601b77f6733ecd3c"&gt;Neural Shading&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k8g4e52w5a5g1.png?width=1732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7add904a858b7fa1042b2949ed69689b38010612"&gt;RT-Core + CUDA + Tensor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From hot chips 2025&lt;/p&gt; &lt;p&gt;&lt;a href="https://hc2025.hotchips.org/assets/program/conference/day1/33_nvidia_blackstein_final.pdf"&gt;https://hc2025.hotchips.org/assets/program/conference/day1/33_nvidia_blackstein_final.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ADCsiXLbUcY&amp;amp;t=2025s"&gt;https://www.youtube.com/watch?v=ADCsiXLbUcY&amp;amp;t=2025s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(video link to that presentation) &lt;/p&gt; &lt;p&gt;For Visual+Physical form of AI (not referring to LLMs), Robotics training and Video Generation stuff, the compute performance is much better if there are &lt;strong&gt;RT Core (&amp;quot;Ray-Tracing&amp;quot;)&lt;/strong&gt; involved in the pipeline?&lt;/p&gt; &lt;p&gt;Does Google TPU (their latest Ironwood version) have any of Google's own implementation of their RT-Cores? Or current TPU core design are mostly just consisting of Tensor-Core like architecture?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Primary_Olive_5444"&gt; /u/Primary_Olive_5444 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peh1bg/rtx_5090_designed_for_the_age_of_neural_rendering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peh1bg/rtx_5090_designed_for_the_age_of_neural_rendering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peh1bg/rtx_5090_designed_for_the_age_of_neural_rendering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T00:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1peh4ps</id>
    <title>Best llm for a simple laptop?</title>
    <updated>2025-12-05T00:47:00+00:00</updated>
    <author>
      <name>/u/Bubbly-Click718</name>
      <uri>https://old.reddit.com/user/Bubbly-Click718</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a laptop for uni with 16 gb ram i5 11gen and the inter with ntel(R) Iris(R) Xe Graphics&amp;quot;. and i want the best model for those specs for times without wifi now i use the Phi-3.5 but is there anythink better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bubbly-Click718"&gt; /u/Bubbly-Click718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peh4ps/best_llm_for_a_simple_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peh4ps/best_llm_for_a_simple_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peh4ps/best_llm_for_a_simple_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T00:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pehe3h</id>
    <title>[Project] sub-tools: Generate multilingual subtitles using WhisperX + Gemini AI</title>
    <updated>2025-12-05T00:58:58+00:00</updated>
    <author>
      <name>/u/dohyeon_kim</name>
      <uri>https://old.reddit.com/user/dohyeon_kim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pehe3h/project_subtools_generate_multilingual_subtitles/"&gt; &lt;img alt="[Project] sub-tools: Generate multilingual subtitles using WhisperX + Gemini AI" src="https://external-preview.redd.it/qRMYxyi5C_3LuXxBqRU342GSYwnODwMtG00_F3pQLzU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bf02a6c78d4915880cc547e7ec6ee48271e190f" title="[Project] sub-tools: Generate multilingual subtitles using WhisperX + Gemini AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a Python CLI tool that converts video/audio into high-quality multilingual subtitles using WhisperX + Gemini.&lt;/p&gt; &lt;p&gt;I needed to create subtitles for video content in multiple languages. Manual transcription services were expensive, and basic speech-to-text made too many errors.&lt;/p&gt; &lt;p&gt;The breakthrough came when ChatGPT became good enough to translate English subtitles to other languages with acceptable quality (about 2 years ago). Then, the second breakthrough was when Gemini became good enough to take audio as input and produce translated subtitles directly (about this time last year, with Gemini 2.5).&lt;/p&gt; &lt;p&gt;Since then, I've been tweaking the pipeline to make it faster and more accurate—splitting the audio into smaller chunks and combining them, validating SRT output and retrying when it's not valid. It was working okay, but never well enough.&lt;/p&gt; &lt;p&gt;Recently, I reconsidered the whole approach and came up with a different solution: use WhisperX to get SRT output with accurate timestamps but not-so-accurate subtitles, then ask Gemini to &amp;quot;proofread&amp;quot; the subtitles using both the SRT and the audio as input. I couldn't believe the accuracy of both the timestamps and the content, and I feel like this is worth sharing broadly.&lt;/p&gt; &lt;p&gt;I'd love to hear what you think about this tool and would like to make it even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dohyeon_kim"&gt; /u/dohyeon_kim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/dohyeondk/sub-tools"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pehe3h/project_subtools_generate_multilingual_subtitles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pehe3h/project_subtools_generate_multilingual_subtitles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T00:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1peern1</id>
    <title>OSS alternatives to Claude Code as an agent foundation?</title>
    <updated>2025-12-04T23:04:36+00:00</updated>
    <author>
      <name>/u/ggaowp</name>
      <uri>https://old.reddit.com/user/ggaowp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Claude Code and really like how it works - the tool use, MCP integration, skills system. But it's closed source and tied to Claude models.&lt;/p&gt; &lt;p&gt;I'm looking for something I can build on top of, customize, and not worry about vendor lock-in. Ideally:&lt;/p&gt; &lt;p&gt;- Multi-provider LLM support, or at least swappable&lt;br /&gt; - Tool/MCP &amp;amp; domain-knowledge/skills integration that actually works reliably&lt;br /&gt; - Something production-grade, not just a demo&lt;/p&gt; &lt;p&gt;For those who've gone down this path:&lt;/p&gt; &lt;p&gt;- What are you using as your agent foundation?&lt;br /&gt; - Did you end up building your own? If so, what was harder than expected?&lt;br /&gt; - How do you handle the skills/knowledge injection part? Just prompt stuffing or something more structured?&lt;/p&gt; &lt;p&gt;UPDATES:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/anthropics/claude-code"&gt;https://github.com/anthropics/claude-code&lt;/a&gt; is only a wrapper of it mainly plugins and extensions, the actual Claude Code CLI implementation is close-sourced afaict like built-in tools, skill system, context management etc.&lt;/p&gt; &lt;p&gt;and by agent foundation, I was thinking of sth I can built an agent on top of, for example, an agent to do my daily ML ops chore. it could be a workflow or more than that when things pop up. in other words, the usage scenario feels like LangChain and LangGraph, but the abstractions are similar to Claude Code CLI do: base model, built-in tools like file access &amp;amp; bash etc. MCP system and Skill system.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ggaowp"&gt; /u/ggaowp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peern1/oss_alternatives_to_claude_code_as_an_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peern1/oss_alternatives_to_claude_code_as_an_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peern1/oss_alternatives_to_claude_code_as_an_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T23:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe59ud</id>
    <title>smallevals - Tiny 0.6B Evaluation Models and a Local LLM Evaluation Framework</title>
    <updated>2025-12-04T17:00:00+00:00</updated>
    <author>
      <name>/u/mburaksayici</name>
      <uri>https://old.reddit.com/user/mburaksayici</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; , you may know me from the latest blogs I've shared on &lt;a href="http://mburaksayici.com/"&gt;mburaksayici.com/&lt;/a&gt; , discussing LLM and RAG systems, and RAG Boilerplates.&lt;/p&gt; &lt;p&gt;When I study evaluation frameworks on LLMs, I've seen they require lots of API calls to generate golden datasets, open-ended and subjective. I thought at least in the retrieval stage, I can come up with a tiny 0.6B models and a framework that uses those models to evaluate vectorDB(for now) and RAG pipelines (in the near future).&lt;/p&gt; &lt;p&gt;I’m releasing smallevals, a lightweight evaluation suite built to evaluate RAG / retrieval systems fast and free — powered by tiny 0.6B models trained on Google Natural Questions and TriviaQA to generate golden evaluation datasets.&lt;/p&gt; &lt;p&gt;smallevals is designed to run extremely fast even on CPU and fully offline — with no API calls, no costs, and no external dependencies.&lt;/p&gt; &lt;p&gt;smallevals generates one question per chunk and then measures whether your vector database can retrieve the correct chunk back using that question.&lt;/p&gt; &lt;p&gt;This directly evaluates retrieval quality using precision, recall, MRR and hit-rate at the chunk level.&lt;/p&gt; &lt;p&gt;SmallEvals includes a built-in local dashboard to visualize rank distributions, failing chunks, retrieval performance, and dataset statistics on your machine.&lt;/p&gt; &lt;p&gt;The first released model is QAG-0.6B, a tiny question-generation model that creates evaluation questions directly from your documents.&lt;/p&gt; &lt;p&gt;This lets you evaluate retrieval quality independently from generation quality, which is exactly where most RAG systems fail silently.&lt;/p&gt; &lt;p&gt;Following QAG-0.6B, upcoming models will evaluate context relevance, faithfulness / groundedness, and answer correctness — closing the gap for a fully local, end-to-end evaluation pipeline.&lt;/p&gt; &lt;p&gt;Source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mburaksayici/smallevals"&gt;https://github.com/mburaksayici/smallevals&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Install:&lt;/p&gt; &lt;p&gt;pip install smallevals&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mburaksayici/golden_generate_qwen_0.6b_v3_gguf"&gt;https://huggingface.co/mburaksayici/golden_generate_qwen_0.6b_v3_gguf&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mburaksayici"&gt; /u/mburaksayici &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe59ud/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe59ud/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe59ud/smallevals_tiny_06b_evaluation_models_and_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T17:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ped5x6</id>
    <title>Gemma-2-MoE: Frankenstein MoE Builder for Gemma 2</title>
    <updated>2025-12-04T21:58:28+00:00</updated>
    <author>
      <name>/u/suayptalha</name>
      <uri>https://old.reddit.com/user/suayptalha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5x6/gemma2moe_frankenstein_moe_builder_for_gemma_2/"&gt; &lt;img alt="Gemma-2-MoE: Frankenstein MoE Builder for Gemma 2" src="https://external-preview.redd.it/uAXlGuYE4DXLBfPsZ5l_7qlc0Ltv8OCdeN30wd7W4tA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=334c8f64d28a90157c5bc345541484fab4604acc" title="Gemma-2-MoE: Frankenstein MoE Builder for Gemma 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve built Gemma-2-MoE, a lightweight toolkit that lets you turn Gemma 2 checkpoints into a Frankenstein-style Mixture of Experts model.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automatic MoE builder with YAML configs&lt;/li&gt; &lt;li&gt;Combine multiple Gemma 2 models as experts&lt;/li&gt; &lt;li&gt;Fully HuggingFace AutoClass–compatible modeling + config&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to experiment with MoE variants of Gemma 2, this makes it simple.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suayptalha"&gt; /u/suayptalha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/suayptalha/Gemma-2-MoE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5x6/gemma2moe_frankenstein_moe_builder_for_gemma_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5x6/gemma2moe_frankenstein_moe_builder_for_gemma_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:58:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdh0sm</id>
    <title>8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich</title>
    <updated>2025-12-03T21:26:43+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt; &lt;img alt="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" src="https://external-preview.redd.it/ZzlmajZ6b3gzMjVnMYyMXOA9G9iEfbHd4uR1YsqLbApEsnv66h0V49mXIA5l.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f3aaa8ebe40ec05d81117c0df8ce6a792a1bd" title="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o8n25oox325g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1peabyw</id>
    <title>HalluBench: LLM Hallucination Rate Benchmark</title>
    <updated>2025-12-04T20:08:07+00:00</updated>
    <author>
      <name>/u/muayyadalsadi</name>
      <uri>https://old.reddit.com/user/muayyadalsadi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peabyw/hallubench_llm_hallucination_rate_benchmark/"&gt; &lt;img alt="HalluBench: LLM Hallucination Rate Benchmark" src="https://external-preview.redd.it/VTTnZIA1Y6ibMILERW5MD4kUFJe24-ufISi4Wxr0BSA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=768cabe438a9ad77a91129641a0d28707b1f2594" title="HalluBench: LLM Hallucination Rate Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zero-knowledge benchmark that measures llm hallucination rate&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muayyadalsadi"&gt; /u/muayyadalsadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/muayyad-alsadi/HalluBench"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peabyw/hallubench_llm_hallucination_rate_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peabyw/hallubench_llm_hallucination_rate_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T20:08:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqxbw</id>
    <title>Chinese CXMT unveils DDR5-8000 RAM</title>
    <updated>2025-12-04T04:43:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory"&gt;https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chinese RAM might be the way to buck the trend of rising prices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe1bd4</id>
    <title>The "Confident Idiot" Problem: Why LLM-as-a-Judge fails in production.</title>
    <updated>2025-12-04T14:25:15+00:00</updated>
    <author>
      <name>/u/Proud-Employ5627</name>
      <uri>https://old.reddit.com/user/Proud-Employ5627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;EDIT: A lot of you agree that we need deterministic checks. I actually open-sourced the library I built to do exactly this (Python decorators for JSON/PII/Logic). Repo: &lt;a href="https://github.com/imtt-dev/steer"&gt;https://github.com/imtt-dev/steer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've been struggling with agent reliability lately. I noticed that the industry standard for fixing hallucinations is &amp;quot;LLM-as-a-Judge&amp;quot; (asking a larger model to grade the output).&lt;/p&gt; &lt;p&gt;But I'm finding this creates a circular dependency. If the underlying models suffer from sycophancy or hallucination, the Judge often hallucinates a passing grade. We are trying to fix probability with more probability.&lt;/p&gt; &lt;p&gt;I wrote up a deep dive on why I think we need to re-introduce &lt;strong&gt;Deterministic Assertions&lt;/strong&gt; (running actual code/regex/SQL parsing) into the agent loop instead of just relying on &amp;quot;Vibe Checks.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Argument:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a URL is valid. Run &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a SQL query is safe. Parse the AST.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If the code says &amp;quot;No&amp;quot;, the agent stops. No matter how confident the LLM is.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full analysis here: &lt;a href="https://steerlabs.substack.com/p/confident-idiot-problem"&gt;https://steerlabs.substack.com/p/confident-idiot-problem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how others are handling this? Are you using LLM-as-a-Judge successfully, or do you rely on hard constraints?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud-Employ5627"&gt; /u/Proud-Employ5627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T14:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdvupp</id>
    <title>Cruxy: Train 1.5B models on 4GB VRAM - new optimiser just released</title>
    <updated>2025-12-04T09:37:58+00:00</updated>
    <author>
      <name>/u/National_Control4101</name>
      <uri>https://old.reddit.com/user/National_Control4101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've just released Cruxy - an adaptive optimiser that lets you fine-tune billion-parameter models on consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; - Drop-in replacement for AdamW - Meta-Lion mode uses 1/3 the memory of AdamW - Automatic stability control - no scheduler tuning needed - Verified on TinyLlama 1.1B and Qwen 2.5 1.5B on a GTX 1650 (4GB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (Shakespeare GPT):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Optimiser&lt;/th&gt; &lt;th&gt;Final Loss&lt;/th&gt; &lt;th&gt;Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AdamW&lt;/td&gt; &lt;td&gt;1.6843&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta3&lt;/td&gt; &lt;td&gt;1.6413&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta-Lion&lt;/td&gt; &lt;td&gt;1.6633&lt;/td&gt; &lt;td&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pip install Cruxy&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/christophergardner-star/Crux1"&gt;https://github.com/christophergardner-star/Crux1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions. Built this on evenings and weekends because cloud GPUs are expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National_Control4101"&gt; /u/National_Control4101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T09:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1peaoyn</id>
    <title>With current trends, is 256GB of system RAM a good idea?</title>
    <updated>2025-12-04T20:22:00+00:00</updated>
    <author>
      <name>/u/Ra1den</name>
      <uri>https://old.reddit.com/user/Ra1den</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just built a system with a 9950x3d and a 5090, along with 64gb of RAM (2*32). I have the Gigabyte B850 AI TOP motherboard. I thought 64 was enough since VRAM has always seemed most important, but it seems like the MOE popularity means system RAM is now also very important. I have the opportunity to get 128 GB of 5600 mhz RAM by Crucial (2*64) for around $950, which is a steal at today's prices. Will I wish I had 128GB or even 256GB in the coming years? My 2*32=64 is still unopened.&lt;/p&gt; &lt;p&gt;My use case is running LLMs locally for inference and data analysis, content creation is not a priority. As you can tell, I'm not a professional, just a hobbyist you could say, but I have a lot of data I would not want to put into the cloud.&lt;/p&gt; &lt;p&gt;Thank you and pardon my ignorance, so much has changed in the last few months in this landscape and most of what I find on this topic is outdated.&lt;/p&gt; &lt;p&gt;Edit: I appreciate the responses. It is sounding like 64 is indeed lacking and 128GB may be the sweet spot. I was mainly wondering if the jump from 128 to 256 was something that seemed like a no-brainer, but it is sounding like I don't really get much else from going from 128 to 256, with the popular models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ra1den"&gt; /u/Ra1den &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peaoyn/with_current_trends_is_256gb_of_system_ram_a_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peaoyn/with_current_trends_is_256gb_of_system_ram_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peaoyn/with_current_trends_is_256gb_of_system_ram_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T20:22:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pefrht</id>
    <title>is the new Deepseek v3.2 that bad?</title>
    <updated>2025-12-04T23:47:23+00:00</updated>
    <author>
      <name>/u/Caffdy</name>
      <uri>https://old.reddit.com/user/Caffdy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"&gt; &lt;img alt="is the new Deepseek v3.2 that bad?" src="https://preview.redd.it/vwvxerd4y95g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3656af4dd7e0fe4438bebcc3c39d130546066e3" title="is the new Deepseek v3.2 that bad?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Caffdy"&gt; /u/Caffdy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vwvxerd4y95g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T23:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu5pe</id>
    <title>WTF are these AI companies doing where they supposedly are the cause of the ram price spike?</title>
    <updated>2025-12-04T07:46:40+00:00</updated>
    <author>
      <name>/u/Red_Redditor_Reddit</name>
      <uri>https://old.reddit.com/user/Red_Redditor_Reddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand what could justify that much investment. Maybe I'm way out of the loop, but what huge application are they expecting that would have this kind of payout? Why is there all of the sudden this spike instead of a slower increase in demand? Like I kinda get the overall GPU demand, but this sudden dramatic change in RAM demand doesn't make sense to me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Redditor_Reddit"&gt; /u/Red_Redditor_Reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdupdg</id>
    <title>Deepseek's progress</title>
    <updated>2025-12-04T08:21:16+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt; &lt;img alt="Deepseek's progress" src="https://preview.redd.it/zpkzyrrxc55g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88e4dbc74aac37f16270f4775ec470f375eab2f5" title="Deepseek's progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's fascinating that DeepSeek has been able to make all this progress with the same pre-trained model since the start of the year, and has just improved post-training and attention mechanisms. It makes you wonder if other labs are misusing their resources by training new base models so often.&lt;/p&gt; &lt;p&gt;Also, what is going on with the Mistral Large 3 benchmarks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zpkzyrrxc55g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T08:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4xm4</id>
    <title>VLLM v0.12.0 supports NVFP4 for SM120 (RTX 50xx and RTX PRO 6000 Blackwell)</title>
    <updated>2025-12-04T16:47:16+00:00</updated>
    <author>
      <name>/u/Rascazzione</name>
      <uri>https://old.reddit.com/user/Rascazzione</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My kudos for the VLLM team that has release the v0.12.0 with support for NVFP4 for the SM120 family!&lt;/p&gt; &lt;h1&gt;Quantization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;W4A8&lt;/strong&gt;: Marlin kernel support (&lt;a href="https://github.com/vllm-project/vllm/pull/24722"&gt;#24722&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVFP4&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;MoE CUTLASS support for SM120 (&lt;a href="https://github.com/vllm-project/vllm/pull/29242"&gt;#29242&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;TRTLLM MoE NVFP4 kernel (&lt;a href="https://github.com/vllm-project/vllm/pull/28892"&gt;#28892&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;CuteDSL MoE with NVFP4 DeepEP dispatch (&lt;a href="https://github.com/vllm-project/vllm/pull/27141"&gt;#27141&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Non-gated activations support in modelopt path (&lt;a href="https://github.com/vllm-project/vllm/pull/29004"&gt;#29004&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AWQ&lt;/strong&gt;: Compressed-tensors AWQ support for Turing GPUs (&lt;a href="https://github.com/vllm-project/vllm/pull/29732"&gt;#29732&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt;: FusedMoE LoRA Triton kernel for MXFP4 (&lt;a href="https://github.com/vllm-project/vllm/pull/29708"&gt;#29708&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Online quantization&lt;/strong&gt;: Moved to &lt;code&gt;model.load_weights&lt;/code&gt; (&lt;a href="https://github.com/vllm-project/vllm/pull/26327"&gt;#26327&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/releases"&gt;https://github.com/vllm-project/vllm/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT (removed the test presented before, because is not NVFP4, see comments). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rascazzione"&gt; /u/Rascazzione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu46s</id>
    <title>New model, microsoft/VibeVoice-Realtime-0.5B</title>
    <updated>2025-12-04T07:43:58+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt; &lt;img alt="New model, microsoft/VibeVoice-Realtime-0.5B" src="https://external-preview.redd.it/yC3RHTaiptQZaDONKxzLP6lQoJh8pT8uDk6mruPADNY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b568bf1e3f993edb57eab9f43241d593fd7c1c2" title="New model, microsoft/VibeVoice-Realtime-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VibeVoice: A Frontier Open-Source Text-to-Speech Model&lt;/p&gt; &lt;p&gt;VibeVoice-Realtime is a lightweight real‑time text-to-speech model supporting streaming text input. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long before a full answer is generated. It produces initial audible speech in ~300 ms (hardware dependent).&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;Parameter size: 0.5B (deployment-friendly) Realtime TTS (~300 ms first audible latency) Streaming text input Robust long-form speech generation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ped5p2</id>
    <title>At What Point Does Owning GPUs Become Cheaper Than LLM APIs ? I</title>
    <updated>2025-12-04T21:58:14+00:00</updated>
    <author>
      <name>/u/Chimchimai</name>
      <uri>https://old.reddit.com/user/Chimchimai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I often see people say that using APIs is always cheaper and that running models locally is mainly for other reasons like privacy or control.&lt;/p&gt; &lt;p&gt;I am choosing infrastructure for my company with LLM features and I am trying to decide between frontier model APIs, AWS GPU rentals, or buying and self hosting GPUs.&lt;/p&gt; &lt;p&gt;My expected load is a few thousand users with peak concurrency around 256 requests per minute, plus heavy use of tool calls and multi step agents with steady daily traffic.&lt;/p&gt; &lt;p&gt;Based on my estimates, API token costs grow very fast at this scale, and AWS rentals seem to reach the full hardware price in about a year. For a long term 24/7 product, buying GPUs looks cheaper to me.&lt;/p&gt; &lt;p&gt;For those with real production experience, at what scale or workload does API or cloud rental still make more financial sense than owning the hardware? What costs am I likely underestimating ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chimchimai"&gt; /u/Chimchimai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4iev</id>
    <title>We Got Claude to Fine-Tune an Open Source LLM</title>
    <updated>2025-12-04T16:31:07+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/blog/hf-skills-training"&gt;https://huggingface.co/blog/hf-skills-training&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5l30</id>
    <title>Tell us a task and we'll help you solve it with Granite</title>
    <updated>2025-12-04T17:11:09+00:00</updated>
    <author>
      <name>/u/ibm</name>
      <uri>https://old.reddit.com/user/ibm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share a task, workflow, or challenge you’d like one of our Granite 4.0 models to help with, and we’ll select a few and show you — step by step — how to choose the right model and get it done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibm"&gt; /u/ibm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T17:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pebwh6</id>
    <title>[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style.</title>
    <updated>2025-12-04T21:08:58+00:00</updated>
    <author>
      <name>/u/Robert-treboR</name>
      <uri>https://old.reddit.com/user/Robert-treboR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt; &lt;img alt="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." src="https://external-preview.redd.it/cnp4eTBhb3U1OTVnMfuPSbsqUMLpJROMWbsiBCXZtzJPCMmpR4Hze4lcXzSH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adfad9a694bdb5ac7bebdf3c924e2842afcf2999" title="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I keep all of my notes as files in cursor&lt;br /&gt; It took me 20min to finetune/RL my personal DeepSeek model on them&lt;br /&gt; I used tinker API &amp;amp; Lora with Gemini to create train dataset&lt;br /&gt; Now I have a model that literally &lt;strong&gt;THINKS&lt;/strong&gt; like me. made it open source repo + tutorial&lt;/p&gt; &lt;p&gt;Github repo :&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OneInterface/Finetune-your-notes"&gt;https://github.com/OneInterface/Finetune-your-notes&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I like playing around with data and models. I see some interesting use cases in the industry. Who wants to bounce idea's?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert-treboR"&gt; /u/Robert-treboR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rnc81tnu595g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pedmsi</id>
    <title>State of AI | OpenRouter | Paper</title>
    <updated>2025-12-04T22:16:54+00:00</updated>
    <author>
      <name>/u/adumdumonreddit</name>
      <uri>https://old.reddit.com/user/adumdumonreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"&gt; &lt;img alt="State of AI | OpenRouter | Paper" src="https://external-preview.redd.it/I8s1kreihjYxvww-6N97nYVDeyvOSua5e5pQ0I02dIM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee04018db47f903b764c2104ae1d522683a51250" title="State of AI | OpenRouter | Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New paper/blog/thing from OpenRouter in collaboration with a16z on token/model usage on OpenRouter. Some interesting insights like how medium sized open source models are the new small, and Chinese vs. Rest of World releases&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adumdumonreddit"&gt; /u/adumdumonreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/state-of-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T22:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pec8hz</id>
    <title>speed optimizations for Qwen Next on CUDA have been merged into llama.cpp</title>
    <updated>2025-12-04T21:22:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzn2n</id>
    <title>legends</title>
    <updated>2025-12-04T13:11:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt; &lt;img alt="legends" src="https://preview.redd.it/vu26lxrns65g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a61d2260347cccaa67517ffc3812c121edcd5d0" title="legends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vu26lxrns65g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
