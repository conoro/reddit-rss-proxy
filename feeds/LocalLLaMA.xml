<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-20T13:33:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qhgi10</id>
    <title>lightonai/LightOnOCR-2-1B · Hugging Face</title>
    <updated>2026-01-19T20:57:11+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt; &lt;img alt="lightonai/LightOnOCR-2-1B · Hugging Face" src="https://external-preview.redd.it/owrWH9MOuE15-iASn4iPzZcG9U3KIDtVJ9SmxpvC1c0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d891c173f79ddf24b05c65d408e9287701ba72c2" title="lightonai/LightOnOCR-2-1B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqrl7</id>
    <title>DeepSeek V3.2 (open weights) beats GPT-5.2-Codex and Claude Opus on production code challenge — The Multivac daily blind peer eval</title>
    <updated>2026-01-20T04:05:23+00:00</updated>
    <author>
      <name>/u/Silver_Raspberry_811</name>
      <uri>https://old.reddit.com/user/Silver_Raspberry_811</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; DeepSeek V3.2 scored 9.39 to beat GPT-5.2-Codex (9.20) and every other closed model on a complex coding task. But the real story is Claude Sonnet 4.5 got scored anywhere from 3.95 to 8.80 by different judges — same exact code.&lt;/p&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;We asked 10 models to write a production-grade nested JSON parser with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Path syntax (&amp;quot;user.profile.settings.theme&amp;quot;)&lt;/li&gt; &lt;li&gt;Array indexing (&amp;quot;users[0].name&amp;quot;)&lt;/li&gt; &lt;li&gt;Circular reference detection&lt;/li&gt; &lt;li&gt;Typed results with error messages&lt;/li&gt; &lt;li&gt;Full type hints and docstrings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a real-world task. Every backend engineer has written something like this.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Std Dev&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek V3.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;9.39&lt;/td&gt; &lt;td align="left"&gt;0.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;9.20&lt;/td&gt; &lt;td align="left"&gt;0.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Grok 3&lt;/td&gt; &lt;td align="left"&gt;8.89&lt;/td&gt; &lt;td align="left"&gt;0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Grok Code Fast 1&lt;/td&gt; &lt;td align="left"&gt;8.46&lt;/td&gt; &lt;td align="left"&gt;1.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;8.16&lt;/td&gt; &lt;td align="left"&gt;0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;7.57&lt;/td&gt; &lt;td align="left"&gt;1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;7.02&lt;/td&gt; &lt;td align="left"&gt;2.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Pro&lt;/td&gt; &lt;td align="left"&gt;4.30&lt;/td&gt; &lt;td align="left"&gt;1.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;GLM 4.7&lt;/td&gt; &lt;td align="left"&gt;2.91&lt;/td&gt; &lt;td align="left"&gt;3.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;MiniMax M2.1&lt;/td&gt; &lt;td align="left"&gt;0.70&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Open weights won.&lt;/strong&gt; DeepSeek V3.2 is fully open.&lt;/p&gt; &lt;h1&gt;The Variance Problem (responding to yesterday's feedback)&lt;/h1&gt; &lt;p&gt;Yesterday &lt;a href="/u/Proud-Claim-485"&gt;u/Proud-Claim-485&lt;/a&gt; critiqued our methodology — said we're measuring &amp;quot;output alignment&amp;quot; not &amp;quot;reasoning alignment.&amp;quot;&lt;/p&gt; &lt;p&gt;Today's data supports this. Look at Claude Sonnet's std dev: &lt;strong&gt;2.03&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's a 5-point spread (3.95 to 8.80) on the same response. Judges fundamentally disagreed on what &amp;quot;good&amp;quot; means.&lt;/p&gt; &lt;p&gt;Compare to GPT-5.2-Codex with 0.50 std dev — everyone agreed within ~1 point.&lt;/p&gt; &lt;p&gt;When evaluators disagree this much, the benchmark is under-specified.&lt;/p&gt; &lt;h1&gt;Judge Strictness (meta-analysis)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Judge&lt;/th&gt; &lt;th align="left"&gt;Avg Score Given&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;5.92 (strictest)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;5.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;6.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek V3.2&lt;/td&gt; &lt;td align="left"&gt;7.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;9.11 (most lenient)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Claude models judge harshly but score mid-tier themselves. Interesting pattern.&lt;/p&gt; &lt;h1&gt;What We're Adding (based on your feedback)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;5 open-weight models for tomorrow:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama-3.3-70B-Instruct&lt;/li&gt; &lt;li&gt;Qwen2.5-72B-Instruct&lt;/li&gt; &lt;li&gt;Mistral-Large-2411&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Big-Tiger-Gemma-27B-v3&lt;/strong&gt; (&lt;a href="/u/ttkciar"&gt;u/ttkciar&lt;/a&gt; suggested this — anti-sycophancy finetune)&lt;/li&gt; &lt;li&gt;Phi-4&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;New evaluation dimension:&lt;/strong&gt; We're adding &amp;quot;reasoning justification&amp;quot; scoring — did the model explain its approach, not just produce correct-looking output?&lt;/p&gt; &lt;h1&gt;Methodology&lt;/h1&gt; &lt;p&gt;This is The Multivac — daily 10×10 blind peer matrix:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10 models respond to same question&lt;/li&gt; &lt;li&gt;Each model judges all 10 responses (100 total judgments)&lt;/li&gt; &lt;li&gt;Models don't know which response came from which model&lt;/li&gt; &lt;li&gt;Rankings from peer consensus, not single evaluator&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full responses and analysis: &lt;a href="https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://themultivac.com"&gt;themultivac.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions welcome. Roast the methodology. That's how we improve.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver_Raspberry_811"&gt; /u/Silver_Raspberry_811 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi098r</id>
    <title>I built a Graph-Based Agent to automate my PhD research "trial-and-error" loops (because existing tools were too rigid)</title>
    <updated>2026-01-20T12:50:34+00:00</updated>
    <author>
      <name>/u/New-Weekend3503</name>
      <uri>https://old.reddit.com/user/New-Weekend3503</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"&gt; &lt;img alt="I built a Graph-Based Agent to automate my PhD research &amp;quot;trial-and-error&amp;quot; loops (because existing tools were too rigid)" src="https://b.thumbs.redditmedia.com/5zqrKq9xvh4UA3qyWZcgdgFDtjCS_pscE-hwLfbsIpw.jpg" title="I built a Graph-Based Agent to automate my PhD research &amp;quot;trial-and-error&amp;quot; loops (because existing tools were too rigid)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m a Physics PhD student (working on ML applications in Astrophysics).&lt;/p&gt; &lt;p&gt;We all know the pain of research: you have a hypothesis, you write code, you run the experiment, check the error, modify the code, and repeat. I wanted to automate this loop.&lt;/p&gt; &lt;p&gt;I tried existing solutions like OpenEvolve and Microsoft's RD-Agent, but they didn't fit my workflow:&lt;/p&gt; &lt;p&gt;OpenEvolve focuses heavily on &amp;quot;population-based&amp;quot; evolution. I didn't need a swarm of agents; I needed a agent to iterate deeply on a highly customized research strategy, working like another me.&lt;/p&gt; &lt;p&gt;RD-Agent is powerful but felt like a &amp;quot;black box.&amp;quot; It was hard to customize the specific steps of my research process (e.g., &amp;quot;If accuracy &amp;gt; 80%, do X; else, search web for Y&amp;quot;).&lt;/p&gt; &lt;p&gt;So I built AgentCommander.&lt;/p&gt; &lt;p&gt;What it is: It’s a visual, graph-based workflow engine that wraps around the Gemini CLI (and Qwen) to orchestrate long-running, self-improving experiments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uqadonhk0ieg1.png?width=1489&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2787341f2c1575d5c1b3171dd8eb7ff6f18e4ec5"&gt;Project Introduction&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8pbtc8lt0ieg1.png?width=3045&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c6e51bc4ce1315d3912759548e42ff7f03893b"&gt;Control Panel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Engineering Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Customizable &amp;quot;Graph&amp;quot; Workflow: Instead of a fixed pipeline, you can design your research steps visually (like a flow chart). There's even an in-editor AI assistant to help modify the graph on the fly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0749326p0ieg1.png?width=3063&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2564eca4baf4081283992f0d69ba333212f302ff"&gt;Visual Workflow Editor with AI Assistant.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Best-of-N&amp;quot; Inheritance: It doesn't just blindly scroll forward. It maintains an Evolutionary Tree, ensuring the agent always inherits from the historically best-performing branch (Strategy A -&amp;gt; Strategy A.1 -&amp;gt; Best!).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h7hxff7w0ieg1.png?width=3059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=754ed59bbd341d5a2f5ab5f2609087cf59908d6b"&gt;The Evolutionary Tree tracking the best code branches.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Strict Snapshot Security: This was critical. LLMs love to &amp;quot;cheat&amp;quot; by modifying the evaluation script to get a perfect score. AgentCommander takes a file snapshot before execution. If the Agent tries to touch unauthorized files (like evaluator.py), it instantly reverts the changes.&lt;/p&gt; &lt;p&gt;CLI-First: It uses the Gemini CLI directly, which I found offers better file-permission isolation than other SDK-based approaches.&lt;/p&gt; &lt;p&gt;I’ve been using it to automate my ML tasks for the past month, and it feels like having a clone of myself doing the grunt work.&lt;/p&gt; &lt;p&gt;It's open source (Apache 2.0). I’d love to hear your comments!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/mx-Liu123/AgentCommander"&gt;https://github.com/mx-Liu123/AgentCommander&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Weekend3503"&gt; /u/New-Weekend3503 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T12:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhg6rm</id>
    <title>GLM-4.7-FLASH-NVFP4 on huggingface (20.5 GB)</title>
    <updated>2026-01-19T20:45:46+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I published a mixed precision NVFP4 quantized version the new GLM-4.7-FLASH on HF, can any of you can test it and let me know how it goes, I would really appreciate it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4"&gt;https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi0xro</id>
    <title>GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)</title>
    <updated>2026-01-20T13:21:34+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt; &lt;img alt="GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)" src="https://b.thumbs.redditmedia.com/I-hI61zDUgcyCW4A5C3NZVA2PraY7bDNdxBTnVt-GkY.jpg" title="GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran some benchmarks with the new GLM-4.7-Flash model with vLLM and also tested llama.cpp with Unsloth dynamic quants&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPUs are from&lt;/strong&gt; &lt;a href="http://jarvislabs.ai"&gt;&lt;strong&gt;jarvislabs.ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sharing some results here.&lt;/p&gt; &lt;h1&gt;vLLM on single H200 SXM&lt;/h1&gt; &lt;p&gt;Ran this with 64K context, 500 prompts from InstructCoder dataset.&lt;/p&gt; &lt;p&gt;- Single user: 207 tok/s, 35ms TTFT&lt;/p&gt; &lt;p&gt;- At 32 concurrent users: 2,267 tok/s, 85ms TTFT&lt;/p&gt; &lt;p&gt;- Peak throughput (no concurrency limit): 4,398 tok/s&lt;/p&gt; &lt;p&gt;All of the benchmarks were done with &lt;a href="https://docs.vllm.ai/en/latest/benchmarking/cli/"&gt;vLLM benchmark CLI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Concurrent&lt;/th&gt; &lt;th align="left"&gt;Decode tok/s&lt;/th&gt; &lt;th align="left"&gt;TTFT (median)&lt;/th&gt; &lt;th align="left"&gt;TTFT (P99)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;207&lt;/td&gt; &lt;td align="left"&gt;35ms&lt;/td&gt; &lt;td align="left"&gt;42ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;348&lt;/td&gt; &lt;td align="left"&gt;44ms&lt;/td&gt; &lt;td align="left"&gt;55ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;547&lt;/td&gt; &lt;td align="left"&gt;53ms&lt;/td&gt; &lt;td align="left"&gt;66ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;882&lt;/td&gt; &lt;td align="left"&gt;61ms&lt;/td&gt; &lt;td align="left"&gt;161ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;1,448&lt;/td&gt; &lt;td align="left"&gt;69ms&lt;/td&gt; &lt;td align="left"&gt;187ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;2,267&lt;/td&gt; &lt;td align="left"&gt;85ms&lt;/td&gt; &lt;td align="left"&gt;245ms&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Fits fine on single H200 at 64K. For full context (200k) we will need 2xH200.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a9tzl54z7ieg1.png?width=4291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a246dd4a6b53b58c42106e476e8e14a2c76becd3"&gt;https://preview.redd.it/a9tzl54z7ieg1.png?width=4291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a246dd4a6b53b58c42106e476e8e14a2c76becd3&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;llama.cpp GGUF on RTX 6000 Ada (48GB)&lt;/h1&gt; &lt;p&gt;Ran the Unsloth dynamic quants with 16k context length and guide by &lt;a href="https://unsloth.ai/docs/models/glm-4.7"&gt;Unsloth&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Generation tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;112&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q6_K_XL&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_K_XL&lt;/td&gt; &lt;td align="left"&gt;91&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qi0xro/video/h3damlpb8ieg1/player"&gt;https://reddit.com/link/1qi0xro/video/h3damlpb8ieg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my initial testing this is really capable and good model for its size.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T13:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhml0s</id>
    <title>With DRAM and NAND prices what they are, the DGX Spark almost seems like a bargain now LOL.</title>
    <updated>2026-01-20T00:57:58+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know a lot of the inference-focused crowd (myself included) were let down by the DGX Spark when it was released because of its weak memory bandwidth and high price tag. &lt;/p&gt; &lt;p&gt;Fast forward a few months and the whole consumer PC component market has turned into an absolute shitshow, RAM prices have quadrupled, now M2 prices are doing the same. That being said, if you break down the current retail market cost of the hardware components thar make up the DGX Spark, it’s sadly turned into a decent value from a solely HW component perspective. &lt;/p&gt; &lt;p&gt;Here’s a break down the core specs of the DGX Spark and what the market prices of the equivalent components would be (pulled these prices from Amazon US today) &lt;/p&gt; &lt;p&gt;- 128 GB of LPDDR5x RAM = $1600 (for 6000 MT/s, the DGX Spark has 8533 MT/s)&lt;/p&gt; &lt;p&gt;- 4TB M2 Gen5 SSD = $895&lt;/p&gt; &lt;p&gt;- 20 core CPU = $300&lt;/p&gt; &lt;p&gt;- Connectx-7 400 GB Nic (which the Spark has built-in = $1,197 &lt;/p&gt; &lt;p&gt;- 5070 GPU (which is what the DGX is said to be equivalent to from a pure GPU compute standpoint) = $639&lt;/p&gt; &lt;p&gt;Total current market prices of equivalent DGX Spark components = $4,631&lt;/p&gt; &lt;p&gt;DGX Spark Current price (4TB model) = $3,999&lt;/p&gt; &lt;p&gt;Estimated cost savings (if you bought a Spark instead of the components) = $632&lt;/p&gt; &lt;p&gt;I did not take into account Motherboard, Case, PSU, cooling, etc. You probably are looking at at least another $300 or more saved by getting the Spark, but I wasn’t really going to count those because the market prices for those components are pretty stable. &lt;/p&gt; &lt;p&gt;Anyways, I’m not advocating buying a Spark or anything like that, I just thought it was interesting that our mindset of what is a good deal vs. what isn’t a good deal is probably going to shift as DRAM and other component market prices get worse. My point is that 6 months ago, DGX Spark was a terrible perceived value proposition, but now in the current HW component market, maybe it’s not so bad. It is still pretty garbage for inference speed though except for some specific NVFP4 models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:57:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhvkrb</id>
    <title>how i 10x my claude code results by giving it a local truth layer</title>
    <updated>2026-01-20T08:26:12+00:00</updated>
    <author>
      <name>/u/OpportunityFit8282</name>
      <uri>https://old.reddit.com/user/OpportunityFit8282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;if you’re using terminal agents for backend work, you know the hallucination struggle is real. i found a way to ground claude code using a local execution engine so it stops guessing what my apis do.&lt;/p&gt; &lt;p&gt;i’ve been documenting this &lt;strong&gt;claude code tutorial&lt;/strong&gt; workflow where i link my terminal to the &lt;strong&gt;apidog cli guide&lt;/strong&gt;. basically, instead of letting the llm assume a schema, i make it run a deterministic &lt;strong&gt;automated api testing guide&lt;/strong&gt; locally before it suggests any code changes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the loop:&lt;/strong&gt; i mapped my scenarios into .claude/skills/. now i just tell the agent to &amp;quot;fix the endpoint and verify.&amp;quot; it fires the apidog cli, checks the actual server response, and auto-corrects based on the logs. it's a huge time saver for local dev loops.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OpportunityFit8282"&gt; /u/OpportunityFit8282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhvkrb/how_i_10x_my_claude_code_results_by_giving_it_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhvkrb/how_i_10x_my_claude_code_results_by_giving_it_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhvkrb/how_i_10x_my_claude_code_results_by_giving_it_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T08:26:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhaq21</id>
    <title>New in llama.cpp: Anthropic Messages API</title>
    <updated>2026-01-19T17:33:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt; &lt;img alt="New in llama.cpp: Anthropic Messages API" src="https://external-preview.redd.it/zqasF6xdAR1yVfMl-Ppz2b8-S-Dv35pa4J_UeKummLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56eabcfaa752210d59dc7af42f1b2087636a579d" title="New in llama.cpp: Anthropic Messages API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhz5fz</id>
    <title>GLM 4.7 Flash is endlessly reasoning in chinese</title>
    <updated>2026-01-20T11:55:35+00:00</updated>
    <author>
      <name>/u/xenydactyl</name>
      <uri>https://old.reddit.com/user/xenydactyl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just downloaded the UD-Q4_K_XL unsloth quant of GLM 4.7 Flash and used the recommended settings &lt;code&gt;--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1&lt;/code&gt;. I pulled and compiled the latest llama.cpp and ran the model and tried using it in kilo code. The entire reasoning block is in chinese and filled with nonsense numbers all over the place. It also seemingly won't stop reasoning. I've encountered this problem with GLM 4.6V Flash too. Does anyone know how to solve this? Am I doing something wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenydactyl"&gt; /u/xenydactyl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz5fz/glm_47_flash_is_endlessly_reasoning_in_chinese/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz5fz/glm_47_flash_is_endlessly_reasoning_in_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz5fz/glm_47_flash_is_endlessly_reasoning_in_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T11:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhz9e2</id>
    <title>[Research] I forensic-audited "Humanity’s Last Exam" (HLE) &amp; GPQA to benchmark my "unleashed" DeepSeek model. Result: A ~58% verifiable error rate caused by bad OCR and typos.</title>
    <updated>2026-01-20T12:01:08+00:00</updated>
    <author>
      <name>/u/Dear_Ad_1381</name>
      <uri>https://old.reddit.com/user/Dear_Ad_1381</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/"&gt; &lt;img alt="[Research] I forensic-audited &amp;quot;Humanity’s Last Exam&amp;quot; (HLE) &amp;amp; GPQA to benchmark my &amp;quot;unleashed&amp;quot; DeepSeek model. Result: A ~58% verifiable error rate caused by bad OCR and typos." src="https://b.thumbs.redditmedia.com/8P0UFiNh2zSjzfQfikS9F3Vi7JhC3E5RnxobARUaVmo.jpg" title="[Research] I forensic-audited &amp;quot;Humanity’s Last Exam&amp;quot; (HLE) &amp;amp; GPQA to benchmark my &amp;quot;unleashed&amp;quot; DeepSeek model. Result: A ~58% verifiable error rate caused by bad OCR and typos." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;(Note: Please visualize the attached images first. They explain the specific errors found.)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Context: Why I did this&lt;/strong&gt; I am an independent researcher (originally from a medical background, pivoted to Physics/CS). I run a small startup and fund my own research out of pocket—no big lab backing, just curiosity and my own API budget.&lt;/p&gt; &lt;p&gt;Recently, inspired by the DeepSeek-V3.2 papers and my own hypotheses on &lt;strong&gt;Statistical Physics and Intelligence Dynamics&lt;/strong&gt;, I began working on a project called &lt;strong&gt;DeepSeek-Overclock (dsoc)&lt;/strong&gt;. My goal was to create an experimental setup to remove the &amp;quot;Alignment Tax&amp;quot; and force ultra-long Chain-of-Thought, theoretically pushing the model's reasoning capabilities to their absolute limit.&lt;/p&gt; &lt;p&gt;To measure if my &amp;quot;unleashed&amp;quot; model was actually working, I needed the hardest rulers available. I chose &lt;strong&gt;GPQA-Diamond&lt;/strong&gt; and the newly released &lt;strong&gt;Humanity's Last Exam (HLE)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Anomaly&lt;/strong&gt; During the evaluation, my &amp;quot;Overclocked&amp;quot; DeepSeek kept failing. But looking at the logs, the model wasn't hallucinating. In many cases, it was rigorously deriving answers that contradicted the provided &amp;quot;Gold Standard.&amp;quot;&lt;/p&gt; &lt;p&gt;So, I stopped tweaking the model and started looking at the test itself. I applied a simple, rigorous methodology: &lt;strong&gt;I manually graded the exam papers.&lt;/strong&gt; I verified the math line-by-line using Python scripts and first principles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Findings (Scientific Insolvency)&lt;/strong&gt; What I found looks less like a &amp;quot;Benchmark&amp;quot; and more like a crime scene of &amp;quot;OCR Errors.&amp;quot;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPQA-Diamond:&lt;/strong&gt; Inherent error lower bound of &lt;strong&gt;26.8%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HLE (Sampled):&lt;/strong&gt; Inherent error lower bound of &lt;strong&gt;~58%&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;Smoking Gun&amp;quot;: Reverse Engineering the Typos&lt;/strong&gt; HLE claims to test the limits of human intelligence. In reality, it often tests if an AI can guess what a professor wrote on a messy chalkboard.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Exhibit A: The Case of the Phantom Parameter (HLE Item: 66fecb...)&lt;/strong&gt; &lt;em&gt;See Image 2&lt;/em&gt; This is a lattice adsorption physics problem. The text is literally broken (&amp;quot;interaction vertical interaction&amp;quot;).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Clue:&lt;/strong&gt; The standard answer was a precise floating-point number: &lt;code&gt;Z=4.61&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Forensics:&lt;/strong&gt; I hypothesized the answer &lt;code&gt;4.61&lt;/code&gt; was correct for the &lt;em&gt;original&lt;/em&gt; intended question. I brute-forced millions of parameter combinations to find exactly what physical setup yields this result.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Findings:&lt;/strong&gt; I successfully reverse-engineered the original parameters. They reveal exactly how the data entry failed: 1. The intended max layer was &lt;strong&gt;4&lt;/strong&gt;. The dataset wrote &lt;strong&gt;k&lt;/strong&gt;. (&lt;code&gt;4&lt;/code&gt; interpreted as &lt;code&gt;k&lt;/code&gt;). 2. A critical energy parameter is missing because the professor likely crossed it out (strikethrough), which the data entry person interpreted as a deletion. - &lt;strong&gt;Verdict:&lt;/strong&gt; My &amp;quot;Overclocked&amp;quot; DeepSeek failed this not because it doesn't know physics, but because it couldn't telepathically reconstruct a typo.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Exhibit B: The Visual Counterfeit (HLE Item: 673738...)&lt;/strong&gt; &lt;em&gt;See Image 3&lt;/em&gt; A math problem on complex projective space.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Error:&lt;/strong&gt; The standard answer contains a geometrically impossible exponent term.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Reconstruction:&lt;/strong&gt; The expert likely wrote &lt;code&gt;(n+1)(n+1)&lt;/code&gt; (Rank × Dimension). The Transcriber saw slanted handwriting and encoded it as &lt;code&gt;(n+1)^(n+1)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verdict:&lt;/strong&gt; The benchmark penalizes the model for knowing the correct math formula because it demands the &amp;quot;typo&amp;quot; version.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt; We are currently &amp;quot;Gaslighting&amp;quot; our best models. Laboratories are burning millions in compute to climb leaderboards that are fundamentally broken. When a model correctly identifies a logical break effectively saying, &amp;quot;Hey, this parameter is missing,&amp;quot; the benchmark tells it: &amp;quot;Wrong. Be dumber.&amp;quot;&lt;/p&gt; &lt;p&gt;I performed this audit with a &amp;quot;medical biopsy&amp;quot; mindset, but the method was just plain old hard work. I believe every dollar wasted on optimizing for these broken benchmarks is a dollar stolen from actual scientific progress.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper &amp;amp; Resources&lt;/strong&gt; I’ve published the full forensic report on Zenodo (Arxiv requires endorsement which I don't have yet as an independent):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full PDF:&lt;/strong&gt; [ &lt;a href="https://doi.org/10.5281/zenodo.18293568"&gt;&lt;strong&gt;https://doi.org/10.5281/zenodo.18293568&lt;/strong&gt;&lt;/a&gt; ]&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code:&lt;/strong&gt; I am currently cleaning/sanitizing the API keys in my repo. I will release the verification scripts for these 139 audited questions to the community within roughly 2 weeks (or sooner if possible).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am just an independent guy burning my own &amp;quot;allowance&amp;quot; on this research. I hope this helps the community stop chasing ghosts.&lt;/p&gt; &lt;p&gt;AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear_Ad_1381"&gt; /u/Dear_Ad_1381 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qhz9e2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T12:01:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhx44i</id>
    <title>Compiled awesome reranker resources into one list</title>
    <updated>2026-01-20T10:00:21+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt; &lt;img alt="Compiled awesome reranker resources into one list" src="https://a.thumbs.redditmedia.com/rTzqbjRv7RniD0ivDYw-0Ay72od8XMAyCbeLHfr3dx8.jpg" title="Compiled awesome reranker resources into one list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/55s7lzc59heg1.png?width=1700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa05cd747a7065b96cd34e6499be0bcb78c1069d"&gt;https://preview.redd.it/55s7lzc59heg1.png?width=1700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa05cd747a7065b96cd34e6499be0bcb78c1069d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Been building RAG systems for a few months. Info on rerankers was scattered everywhere - docs, papers, Reddit threads. &lt;/p&gt; &lt;p&gt;Put it all in one place: &lt;a href="https://github.com/agentset-ai/awesome-rerankers"&gt;https://github.com/agentset-ai/awesome-rerankers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quick start code (works out of the box)&lt;/li&gt; &lt;li&gt;Model comparison table&lt;/li&gt; &lt;li&gt;Local options (FlashRank runs on CPU, ~4MB)&lt;/li&gt; &lt;li&gt;Framework integrations&lt;/li&gt; &lt;li&gt;Live benchmarks with ELO scores&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Rerankers give you a solid 15-40% accuracy boost over just vector search. But figuring out which one to use or whether you can run it locally was a pain.&lt;/p&gt; &lt;p&gt;This covers it. If you're building RAG, might save you some time.&lt;/p&gt; &lt;p&gt;Let me know if I missed anything useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhx6u1</id>
    <title>no problems with GLM-4.7-Flash</title>
    <updated>2026-01-20T10:04:33+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"&gt; &lt;img alt="no problems with GLM-4.7-Flash" src="https://b.thumbs.redditmedia.com/RyfGOsESx4YsXYbZlTJQbs2Km0GhT7KhWobtlXZeH9A.jpg" title="no problems with GLM-4.7-Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw many comments that GLM-4.7-Flash doesn't work correctly, could you show specific prompts? I am not doing anything special, all settings are default&lt;/p&gt; &lt;p&gt;!!! UPDATE !!! - check the comments from &lt;a href="https://www.reddit.com/user/shokuninstudio/"&gt;shokuninstudio&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qhx6u1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjhlh</id>
    <title>GLM-4.7-Flash-GGUF is here!</title>
    <updated>2026-01-19T22:49:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt; &lt;img alt="GLM-4.7-Flash-GGUF is here!" src="https://external-preview.redd.it/xaz8me0jAeBOkTb7mKUXdYdIdr8aoSsiwENwulyOJmI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f21f70be7ae2e1b3f10f33471dbfc4c47ba6518" title="GLM-4.7-Flash-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5wdq</id>
    <title>zai-org/GLM-4.7-Flash · Hugging Face</title>
    <updated>2026-01-19T14:40:27+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.7-Flash · Hugging Face" src="https://external-preview.redd.it/Qs0t4y5eLm-uwORWdP6T0dcwW2T6VJyQFBUSY70CTF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8700f4a43fe16a1031ccda94b517fd709573a5c3" title="zai-org/GLM-4.7-Flash · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqzsi</id>
    <title>Mosquito - 7.3M parameter tiny knowledge model</title>
    <updated>2026-01-20T04:16:20+00:00</updated>
    <author>
      <name>/u/Lopsided-Repair-3638</name>
      <uri>https://old.reddit.com/user/Lopsided-Repair-3638</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A mosquito brain size model (7.3M params) that can answer surprisingly many general knowledge questions. Demo: &lt;a href="https://huggingface.co/spaces/ag14850/Mosquito-Demo"&gt;https://huggingface.co/spaces/ag14850/Mosquito-Demo&lt;/a&gt; Model: &lt;a href="https://huggingface.co/ag14850/Mosquito"&gt;https://huggingface.co/ag14850/Mosquito&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided-Repair-3638"&gt; /u/Lopsided-Repair-3638 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi06kp</id>
    <title>One of the DeepSeek repositories got updated with a reference to a new “model1” model.</title>
    <updated>2026-01-20T12:46:56+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"&gt; &lt;img alt="One of the DeepSeek repositories got updated with a reference to a new “model1” model." src="https://preview.redd.it/j3ifa4kn2ieg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aab1dc0b54e56da3161e03846e6cdc3fb3e15f1" title="One of the DeepSeek repositories got updated with a reference to a new “model1” model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source DeepSeek on GitHub: FlashMLA: flash_mla/flash_mla_interface.py: &lt;a href="https://github.com/deepseek-ai/FlashMLA/blob/main/flash_mla/flash_mla_interface.py"&gt;https://github.com/deepseek-ai/FlashMLA/blob/main/flash_mla/flash_mla_interface.py&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j3ifa4kn2ieg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T12:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhlnsv</id>
    <title>Unsloth GLM 4.7-Flash GGUF</title>
    <updated>2026-01-20T00:17:58+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhpima</id>
    <title>Bartowski comes through again. GLM 4.7 flash GGUF</title>
    <updated>2026-01-20T03:07:33+00:00</updated>
    <author>
      <name>/u/RenewAi</name>
      <uri>https://old.reddit.com/user/RenewAi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenewAi"&gt; /u/RenewAi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T03:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhitrj</id>
    <title>GLM 4.7 Flash official support merged in llama.cpp</title>
    <updated>2026-01-19T22:24:24+00:00</updated>
    <author>
      <name>/u/ayylmaonade</name>
      <uri>https://old.reddit.com/user/ayylmaonade</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt; &lt;img alt="GLM 4.7 Flash official support merged in llama.cpp" src="https://external-preview.redd.it/AVP8Isc32PMjAyVGtAipaav3x8aU8JY8Lx1bZ_yPak0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43081fb39d8cfd3c8faeeb3516b7513654ed8fce" title="GLM 4.7 Flash official support merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayylmaonade"&gt; /u/ayylmaonade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhwfe0</id>
    <title>How to run and fine-tune GLM-4.7-Flash locally</title>
    <updated>2026-01-20T09:19:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt; &lt;img alt="How to run and fine-tune GLM-4.7-Flash locally" src="https://preview.redd.it/g5y2icqg1heg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e9539a968badc4795c9185a6384ef02c6b8c01" title="How to run and fine-tune GLM-4.7-Flash locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7-Flash is Z.ai’s new 30B MoE reasoning model built for local deployment, delivering best-in-class performance for coding, agentic workflows, and chat. &lt;/li&gt; &lt;li&gt;The model uses ~3.6B parameters, supports 200K context, and leads SWE-Bench, GPQA, and reasoning/chat benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official guide - &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash"&gt;https://unsloth.ai/docs/models/glm-4.7-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g5y2icqg1heg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T09:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhii5v</id>
    <title>My gpu poor comrades, GLM 4.7 Flash is your local agent</title>
    <updated>2026-01-19T22:12:06+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried many MoE models at 30B or under and all of them failed sooner or later in an agentic framework. If z.ai is not redirecting my requests to another model, then GLM 4.7 Flash is finally the reliable (soon local) agent that I desperately wanted.&lt;/p&gt; &lt;p&gt;I am running it since more than half an hour on opencode and it produced hundreds of thousands tokens in one session (with context compacting obviously) without any tool calling errors. It clones github repos, it runs all kind of commands, edits files, commits changes, all perfect, not a single error yet.&lt;/p&gt; &lt;p&gt;Can't wait for GGUFs to try this locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi04hg</id>
    <title>He just knows…. He always knows.</title>
    <updated>2026-01-20T12:44:08+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi04hg/he_just_knows_he_always_knows/"&gt; &lt;img alt="He just knows…. He always knows." src="https://preview.redd.it/8s6k3kho2ieg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5770b52d50947bda78f86a81791a1cbdcd753e57" title="He just knows…. He always knows." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8s6k3kho2ieg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi04hg/he_just_knows_he_always_knows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi04hg/he_just_knows_he_always_knows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T12:44:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhxlgy</id>
    <title>glm-4.7-flash has the best thinking process with clear steps, I love it</title>
    <updated>2026-01-20T10:28:16+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I tested several personal prompts like &lt;code&gt;imagine you are in a farm, what is your favorite barn color?&lt;/code&gt;&lt;/li&gt; &lt;li&gt;although the prompt is short, glm can analyze the prompt and give clear thinking process&lt;/li&gt; &lt;li&gt;without my instruction in the prompt, glm mostly thinks in these steps: &lt;ol&gt; &lt;li&gt;request/goal analysis&lt;/li&gt; &lt;li&gt;brainstorm&lt;/li&gt; &lt;li&gt;draft response&lt;/li&gt; &lt;li&gt;refine response: gives option1, option2, option3...&lt;/li&gt; &lt;li&gt;revise response/plan&lt;/li&gt; &lt;li&gt;polish&lt;/li&gt; &lt;li&gt;final response&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;so the glm thinking duration(110s) is really long compared to nemotron-nano(19s), but the thinking content is my favorite of all the small models. the final response is also clear &lt;ul&gt; &lt;li&gt;thinking process like this seems to be perfect for data analysis (waiting for a fine-tune)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;overall, i love glm-4.7-flash, and will try to replace qwen3-30b and nemotron-nano.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;del&gt;but GLM-4.7-Flash-mlx-4bit is very&lt;/del&gt; &lt;strong&gt;&lt;del&gt;slow&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;at&lt;/del&gt; &lt;strong&gt;&lt;del&gt;19 token/s&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;compared to nemotron-anno-mlx-4bit&lt;/del&gt; &lt;strong&gt;&lt;del&gt;30+ token/s&lt;/del&gt;&lt;/strong&gt;&lt;del&gt;. i donnot understand.&lt;/del&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm using &lt;a href="https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit"&gt;https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit&lt;/a&gt; on my m4 macbook air. with default config, the model often goes into loop. with the following config, it finally works for me &lt;ul&gt; &lt;li&gt;temperature 1.0&lt;/li&gt; &lt;li&gt;repeat penalty: 1.1&lt;/li&gt; &lt;li&gt;top-p: 0.95&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;- is there any trick to make the thinking process faster? Thinking can be toggled on/off through lmstudio ui, but i donnot want to disable it, how to make thinking faster?&lt;br /&gt; - lowering the temperature helps. tried 1.0/0.8/0.6&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;br /&gt; - 🐛 I tried several more prompts. sometimes the thinking content does not comply to the flow above, for these situations, the model often goes into loops.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhs2sd</id>
    <title>It's been one year since the release of Deepseek-R1</title>
    <updated>2026-01-20T05:08:29+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt; &lt;img alt="It's been one year since the release of Deepseek-R1" src="https://preview.redd.it/cin706z9tfeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65fbe53bfb15712186113b0e795fc46c050d0d13" title="It's been one year since the release of Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cin706z9tfeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T05:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
