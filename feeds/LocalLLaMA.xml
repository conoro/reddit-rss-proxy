<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-09T12:30:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1phy0s8</id>
    <title>Phone Agent -- A mobile intelligent assistant framework built on AutoGLM [Open Source/Model]</title>
    <updated>2025-12-09T04:10:23+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;src: &lt;a href="https://github.com/zai-org/Open-AutoGLM/"&gt;https://github.com/zai-org/Open-AutoGLM/&lt;/a&gt;&lt;br /&gt; model: &lt;a href="https://huggingface.co/zai-org/AutoGLM-Phone-9B"&gt;https://huggingface.co/zai-org/AutoGLM-Phone-9B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phy0s8/phone_agent_a_mobile_intelligent_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phy0s8/phone_agent_a_mobile_intelligent_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phy0s8/phone_agent_a_mobile_intelligent_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T04:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph7njc</id>
    <title>Vector db comparison</title>
    <updated>2025-12-08T08:55:16+00:00</updated>
    <author>
      <name>/u/Kaneki_Sana</name>
      <uri>https://old.reddit.com/user/Kaneki_Sana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"&gt; &lt;img alt="Vector db comparison" src="https://b.thumbs.redditmedia.com/EQXPxZZvz2rHdMZgPyJSqdIDNHw4hQWeTCCI6KvOvdQ.jpg" title="Vector db comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for the best vector for our RAG product, and went down a rabbit hole to compare all of them. Key findings:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;RAG systems under ~10M vectors, standard HNSW is fine.&lt;/strong&gt; Above that, you'll need to choose a different index. &lt;/p&gt; &lt;p&gt;- Large dataset + cost-sensitive&lt;em&gt;:&lt;/em&gt; &lt;strong&gt;Turbopuffer.&lt;/strong&gt; Object storage makes it cheap at scale.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;pgvector&lt;/strong&gt; is good for small scale and local experiments. Specialized vector dbs perform better at scale.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Chroma&lt;/strong&gt; - Lightweight, good for running in notebooks or small servers&lt;/p&gt; &lt;p&gt;Here's the full breakdown: &lt;a href="https://agentset.ai/blog/best-vector-db-for-rag"&gt;https://agentset.ai/blog/best-vector-db-for-rag&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kaneki_Sana"&gt; /u/Kaneki_Sana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ph7njc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T08:55:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1phjz8s</id>
    <title>Aquif-AI HuggingFace page throws 404 after community found evidence of aquif-ai republishing work of others as their own without attribution.</title>
    <updated>2025-12-08T18:23:39+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aquif is a Brazil-based organization that was publishing some open weight models on HF, mainly LLMs.&lt;/p&gt; &lt;p&gt;Community found evidence of aquif-Image-14B model being a &lt;a href="https://huggingface.co/wikeeyang/Magic-Wan-Image-v1.0/discussions/3"&gt;republished finetune with matching hashes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the 800M LLM models also apparently matches corresponding Granite model 1:1 but I didn't confirm that, and further discovery of the scale of their deception will be harder to do now since their models are no longer public in their original repos, and mainly quants are available.&lt;/p&gt; &lt;p&gt;It's not clear if Aquif genuinely trained any models that they published. Their benchmark results shouldn't be blindly trusted.&lt;/p&gt; &lt;p&gt;I think you should be wary with models from them from now on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjz8s/aquifai_huggingface_page_throws_404_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjz8s/aquifai_huggingface_page_throws_404_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phjz8s/aquifai_huggingface_page_throws_404_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1phzpfq</id>
    <title>Support for rnj-1 now in llama.cpp</title>
    <updated>2025-12-09T05:48:15+00:00</updated>
    <author>
      <name>/u/Amazing_Athlete_2265</name>
      <uri>https://old.reddit.com/user/Amazing_Athlete_2265</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Athlete_2265"&gt; /u/Amazing_Athlete_2265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7328"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phzpfq/support_for_rnj1_now_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phzpfq/support_for_rnj1_now_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T05:48:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1phmt95</id>
    <title>Upcoming models from llama.cpp support queue (This month or Jan possibly)</title>
    <updated>2025-12-08T20:09:02+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Added only PR items with enough progress.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17811"&gt;EssentialAI/Rnj-1&lt;/a&gt; (Stats look better for its size) - &lt;strong&gt;Update&lt;/strong&gt; : PR merged, GGUF soon probably.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17592"&gt;moonshotai/Kimi-Linear-48B-A3B&lt;/a&gt; (Q4 of Qwen3-Next gave me 10+ t/s on my 8GB VRAM + 32GB RAM so this one could be better)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17454"&gt;inclusionAI/LLaDA2.0-mini &amp;amp; inclusionAI/LLaDA2.0-flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17400"&gt;deepseek-ai/DeepSeek-OCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17141"&gt;Infinigence/Megrez2-3x7B-A3B&lt;/a&gt; (Glad they're in progress with this one after 2nd ticket)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Below one went stale &amp;amp; got closed. Really wanted to have this model(&lt;strong&gt;s&lt;/strong&gt;) earlier.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/15585"&gt;allenai/FlexOlmo-7x7B-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; : BTW Above links navigates to llama.cpp PRs to see progress.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phmt95/upcoming_models_from_llamacpp_support_queue_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phmt95/upcoming_models_from_llamacpp_support_queue_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phmt95/upcoming_models_from_llamacpp_support_queue_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T20:09:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1phk59c</id>
    <title>Tiny-A2D: An Open Recipe to Turn Any AR LM into a Diffusion LM</title>
    <updated>2025-12-08T18:29:49+00:00</updated>
    <author>
      <name>/u/Individual-Ninja-141</name>
      <uri>https://old.reddit.com/user/Individual-Ninja-141</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phk59c/tinya2d_an_open_recipe_to_turn_any_ar_lm_into_a/"&gt; &lt;img alt="Tiny-A2D: An Open Recipe to Turn Any AR LM into a Diffusion LM" src="https://external-preview.redd.it/amg0MWxsaDN4MDZnMeq0L6tFOFYIbTBJmZOjfxjCUMUH4cuuibIVjJcy3j27.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1cc9fcede925a2c03b7ee166735f08a86de3e97" title="Tiny-A2D: An Open Recipe to Turn Any AR LM into a Diffusion LM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;https://github.com/ZHZisZZ/dllm&lt;/a&gt;&lt;br /&gt; Checkpoints: &lt;a href="https://huggingface.co/collections/dllm-collection/tiny-a2d"&gt;https://huggingface.co/collections/dllm-collection/tiny-a2d&lt;/a&gt;&lt;br /&gt; Twitter: &lt;a href="https://x.com/asapzzhou/status/1998098118827770210"&gt;https://x.com/asapzzhou/status/1998098118827770210&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: You can now turn &lt;strong&gt;ANY&lt;/strong&gt; autoregressive LM into a diffusion LM (parallel generation + infilling) with minimal compute. Using this recipe, we built a collection of the smallest diffusion LMs that work well in practice (e.g., &lt;a href="https://huggingface.co/dllm-collection/Qwen3-0.6B-diffusion-bd3lm-v0.1"&gt;Qwen3-0.6B-diffusion-bd3lm-v0.1&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ZHZisZZ/dllm"&gt;&lt;strong&gt;dLLM&lt;/strong&gt;&lt;/a&gt;: The Tiny-A2D series is &lt;em&gt;trained, evaluated and visualized&lt;/em&gt; with &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;dLLM&lt;/a&gt; — a unified library for training and evaluating diffusion language models. It brings transparency, reproducibility, and simplicity to the entire pipeline, &lt;strong&gt;serving as an all-in-one, tutorial-style resource.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Ninja-141"&gt; /u/Individual-Ninja-141 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzyejih3x06g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phk59c/tinya2d_an_open_recipe_to_turn_any_ar_lm_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phk59c/tinya2d_an_open_recipe_to_turn_any_ar_lm_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1phsyag</id>
    <title>The Universal Weight Subspace Hypothesis</title>
    <updated>2025-12-09T00:14:11+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2512.05117"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsyag/the_universal_weight_subspace_hypothesis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phsyag/the_universal_weight_subspace_hypothesis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1phaaon</id>
    <title>GLM-4.6V (108B) has been released</title>
    <updated>2025-12-08T11:41:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt; &lt;img alt="GLM-4.6V (108B) has been released" src="https://b.thumbs.redditmedia.com/3UlSBmijpC7kl1f1Bcn1r5q-4r_S7Xxg1tTNLZ4W9ms.jpg" title="GLM-4.6V (108B) has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dyfhb6nhwy5g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d03177e251a72b04491b10634e66bdde1a9544c5"&gt;https://preview.redd.it/dyfhb6nhwy5g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d03177e251a72b04491b10634e66bdde1a9544c5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4.6V series model includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance cluster scenarios, and GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications. GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding among models of similar parameter scales. Crucially, we integrate native Function Calling capabilities for the first time. This effectively bridges the gap between &amp;quot;visual perception&amp;quot; and &amp;quot;executable action&amp;quot; providing a unified technical foundation for multimodal agents in real-world business scenarios.&lt;/p&gt; &lt;p&gt;Beyond achieves SoTA performance across major multimodal benchmarks at comparable model scales. GLM-4.6V introduces several key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Native Multimodal Function Calling&lt;/strong&gt; Enables native vision-driven tool use. Images, screenshots, and document pages can be passed directly as tool inputs without text conversion, while visual outputs (charts, search images, rendered pages) are interpreted and integrated into the reasoning chain. This closes the loop from perception to understanding to execution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interleaved Image-Text Content Generation&lt;/strong&gt; Supports high-quality mixed media creation from complex multimodal inputs. GLM-4.6V takes a multimodal context—spanning documents, user inputs, and tool-retrieved images—and synthesizes coherent, interleaved image-text content tailored to the task. During generation it can actively call search and retrieval tools to gather and curate additional text and visuals, producing rich, visually grounded content.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Document Understanding&lt;/strong&gt; GLM-4.6V can process up to 128K tokens of multi-document or long-document input, directly interpreting richly formatted pages as images. It understands text, layout, charts, tables, and figures jointly, enabling accurate comprehension of complex, image-heavy documents without requiring prior conversion to plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend Replication &amp;amp; Visual Editing&lt;/strong&gt; Reconstructs pixel-accurate HTML/CSS from UI screenshots and supports natural-language-driven edits. It detects layout, components, and styles visually, generates clean code, and applies iterative visual modifications through simple user instructions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V"&gt;https://huggingface.co/zai-org/GLM-4.6V&lt;/a&gt;&lt;/p&gt; &lt;p&gt;please notice that llama.cpp support for GLM 4.5V is still draft&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16600"&gt;https://github.com/ggml-org/llama.cpp/pull/16600&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi1tc8</id>
    <title>Native Parallel Reasoner (NPR): Reasoning in Parallelism via Self-Distilled RL, 4.6x Faster, 100% genuine parallelism, fully open source</title>
    <updated>2025-12-09T07:56:06+00:00</updated>
    <author>
      <name>/u/Think_Specific_7241</name>
      <uri>https://old.reddit.com/user/Think_Specific_7241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am excited to share our latest research, &lt;strong&gt;Native Parallel Reasoner (NPR)&lt;/strong&gt;, which introduces a new paradigm to enable LLMs to perform native, internal parallel reasoning.&lt;/p&gt; &lt;p&gt;We know that sequential, token-by-token reasoning can be slow and sometimes inefficient. NPR changes this by training the model to simultaneously generate multiple candidate &amp;quot;thought&amp;quot; branches, execute them in parallel, and reduce them to a final answer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Instead of relying on strong external teachers (like GPT-series distillation) or manual annotation, NPR uses a format-aware self-exploration loop:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Self-Distillation + Parallel SFT:&lt;/strong&gt; The model learns to propose parallel branches.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PAPO (Parallel-Aware Policy Optimization):&lt;/strong&gt; A specialized parallel Reinforcement Learning algorithm we designed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NPR-Engine:&lt;/strong&gt; A verifiable inference engine that validates the format and results of every branch, allowing the model to self-optimize.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Key Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; We achieved up to a &lt;strong&gt;4.6× wall-clock speedup&lt;/strong&gt; compared to standard autoregressive methods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Significantly outperforms existing parallel and autoregressive baselines on math and complex reasoning benchmarks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robustness:&lt;/strong&gt; In testing, we saw a &lt;strong&gt;~100% parallel trigger rate&lt;/strong&gt;, meaning the model genuinely internalized the &amp;quot;parallel thinking&amp;quot; strategy and didn't fall back to sequential generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, this offers a reproducible path to go from algorithm to engineering, making &amp;quot;parallel thinking&amp;quot; a trainable, verifiable, and deployable capability rather than just a prompting trick.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;X:&lt;/strong&gt; &lt;a href="https://x.com/ZilongZheng/status/1998252267783516444?s=20"&gt;https://x.com/ZilongZheng/status/1998252267783516444?s=20&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HF:&lt;/strong&gt;&lt;a href="https://huggingface.co/papers/2512.07461"&gt;https://huggingface.co/papers/2512.07461&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Project Page:&lt;/strong&gt;&lt;a href="https://bigai-nlco.github.io/Native-Parallel-Reasoner/"&gt;https://bigai-nlco.github.io/Native-Parallel-Reasoner/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper (ArXiv):&lt;/strong&gt;&lt;a href="https://arxiv.org/abs/2512.07461"&gt;https://arxiv.org/abs/2512.07461&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer any questions about the training pipeline or the architecture!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Think_Specific_7241"&gt; /u/Think_Specific_7241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi1tc8/native_parallel_reasoner_npr_reasoning_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi1tc8/native_parallel_reasoner_npr_reasoning_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi1tc8/native_parallel_reasoner_npr_reasoning_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T07:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pha7l1</id>
    <title>zai-org/GLM-4.6V-Flash (9B) is here</title>
    <updated>2025-12-08T11:36:39+00:00</updated>
    <author>
      <name>/u/Cute-Sprinkles4911</name>
      <uri>https://old.reddit.com/user/Cute-Sprinkles4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks incredible for your own machine. &lt;/p&gt; &lt;p&gt;GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications. GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding among models of similar parameter scales. Crucially, we integrate native Function Calling capabilities for the first time. This effectively bridges the gap between &amp;quot;visual perception&amp;quot; and &amp;quot;executable action&amp;quot; providing a unified technical foundation for multimodal agents in real-world business scenarios.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V-Flash"&gt;https://huggingface.co/zai-org/GLM-4.6V-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Sprinkles4911"&gt; /u/Cute-Sprinkles4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi4qmg</id>
    <title>nano-trm - Train your own TRM in a few minutes</title>
    <updated>2025-12-09T11:07:05+00:00</updated>
    <author>
      <name>/u/randomwalkin</name>
      <uri>https://old.reddit.com/user/randomwalkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;Tiny Recursive Models reach impressive results on ARC AGI. I implemented a version from scratch, with ease of experimentation in mind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;cleaner config: hydra, uv, lightning&lt;/li&gt; &lt;li&gt;smaller datasets for faster iteration (Sudoku 6x6 and 9x9)&lt;/li&gt; &lt;li&gt;introduction, in-code video&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All important implementation details have been carefully kept. The results of the paper are reproducible (Sudoku Extreme, Maze Hard).&lt;/p&gt; &lt;p&gt;Feedback/contributions welcome.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/olivkoch/nano-trm"&gt;https://github.com/olivkoch/nano-trm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomwalkin"&gt; /u/randomwalkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4qmg/nanotrm_train_your_own_trm_in_a_few_minutes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4qmg/nanotrm_train_your_own_trm_in_a_few_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4qmg/nanotrm_train_your_own_trm_in_a_few_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T11:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi3aah</id>
    <title>ZAI Open Source AutoGLM --A AI Phone Agent</title>
    <updated>2025-12-09T09:36:03+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/AutoGLM-Phone-9B"&gt;https://huggingface.co/zai-org/AutoGLM-Phone-9B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/zai-org/Open-AutoGLM"&gt;https://github.com/zai-org/Open-AutoGLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T09:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1phox68</id>
    <title>GLM-4.6V AWQ is released</title>
    <updated>2025-12-08T21:28:37+00:00</updated>
    <author>
      <name>/u/YellowTree11</name>
      <uri>https://old.reddit.com/user/YellowTree11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-4bit"&gt;cyankiwi/GLM-4.6V-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-8bit"&gt;cyankiwi/GLM-4.6V-AWQ-8bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-4bit"&gt;cyankiwi/GLM-4.6V-Flash-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-8bit"&gt;cyankiwi/GLM-4.6V-Flash-AWQ-8bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YellowTree11"&gt; /u/YellowTree11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phox68/glm46v_awq_is_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phox68/glm46v_awq_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phox68/glm46v_awq_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T21:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1phtydt</id>
    <title>FYI, looks like Tesla P40s are back down in price!</title>
    <updated>2025-12-09T00:58:16+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just posting so y'all are aware. I previously grabbed a P40 for 165, and I see them going for 190 on eBay now. I would say the price is reasonable and the card is still well supported in Llama.cpp. &lt;/p&gt; &lt;p&gt;The Mi60 32gb has been price inflated. So I would avoid that. &lt;/p&gt; &lt;p&gt;With the dram prices going sky high, getting a few of these in a rig could definitely be a viable option. You can probably grab like 3 of these for under 600 bucks and run Derestricted 120B in VRAM at really high speeds since 120B is quite compute light. You could even run Derestricted GLM 4.5 Air at Q4 as well. And they will destroy DRAM setups in terms of speed. &lt;/p&gt; &lt;p&gt;I know there is talk about cuda dropping support for the newest versions, but this card still works, and will always work. (And I doubt llama.cpp will require new cuda versions for the foreseeable future). And currently the Air and 120B models are very good. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph8wel</id>
    <title>RAM prices explained</title>
    <updated>2025-12-08T10:17:09+00:00</updated>
    <author>
      <name>/u/Lopsided_Sentence_18</name>
      <uri>https://old.reddit.com/user/Lopsided_Sentence_18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI bought up 40% of global DRAM production in raw wafers they're not even using - just stockpiling to deny competitors access. Result? Memory prices are skyrocketing. Month before chrismass.&lt;/p&gt; &lt;p&gt;Source: Moore´s law is Dead&lt;br /&gt; Link: &lt;a href="https://www.mooreslawisdead.com/post/sam-altman-s-dirty-dram-deal"&gt;Sam Altman’s Dirty DRAM Deal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Sentence_18"&gt; /u/Lopsided_Sentence_18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T10:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi28mq</id>
    <title>model: support Rnj-1 by philip-essential · Pull Request #17811 · ggml-org/llama.cpp</title>
    <updated>2025-12-09T08:24:52+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi28mq/model_support_rnj1_by_philipessential_pull/"&gt; &lt;img alt="model: support Rnj-1 by philip-essential · Pull Request #17811 · ggml-org/llama.cpp" src="https://external-preview.redd.it/HJud6LLFn4jcoTTYFQN2jlM9S8V73LW3HfXmSX8jM3s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0e15d519011eeac1611d0ff185d55ed015a906e" title="model: support Rnj-1 by philip-essential · Pull Request #17811 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models. These models perform well across a range of programming languages and boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent), while also excelling at tool-calling. They additionally exhibit strong capabilities in math and science. Herein, &lt;code&gt;rnj-1&lt;/code&gt; refers to the base model, while &lt;code&gt;rnj-1-instruct&lt;/code&gt; refers to the post-trained instruction tuned model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/EssentialAI/rnj-1-instruct"&gt;https://huggingface.co/EssentialAI/rnj-1-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/EssentialAI/rnj-1-instruct-GGUF"&gt;https://huggingface.co/EssentialAI/rnj-1-instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17811"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi28mq/model_support_rnj1_by_philipessential_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi28mq/model_support_rnj1_by_philipessential_pull/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T08:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1phcyvk</id>
    <title>After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM</title>
    <updated>2025-12-08T13:54:31+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt; &lt;img alt="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" src="https://a.thumbs.redditmedia.com/cM8ZY8pfeiL7V-euNxiaRZSPcskPH5ahnCORrr5O-W4.jpg" title="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, it's ugly and frankly embarrassing to look at. I just finished this build last night by adding 2 additional GPUs to go from 6 to 8, where I will stop &amp;amp; call this build complete.&lt;/p&gt; &lt;p&gt;I've built many PCs over the years but this was a whole other level and at this point I'm just happy it works. It runs off daisy chained 1500W and 1000W PSUs (5 cards on the 1500W and 3 on the 1000W), and the system is fed by a 20A dedicated branch circuit.&lt;/p&gt; &lt;p&gt;Cramming the GPUs in a case without having to use long GPU riser cables was the hardest part. If I were to do this again, I'd just use long PCIE 1x cables that give me the freedom to neatly stack the cards and save myself the headache, since this is just an inference system... only time PCIE bandwidth matters is when loading models. But I went down the path of using certified PCIE 4.0 cables that range from 200-250mm, &amp;amp; as you can see, it ain't pretty. One card has to sit outside the rack bc there was simply no space for it among the chonky GPUs &amp;amp; PCIE riser spaghetti.&lt;/p&gt; &lt;p&gt;Good news is that the system has been running stable for it's entire existence as I kept adding parts &amp;amp; just learning as I go. GPU temps never exceed 70ish*C under load since the GPUs are pretty well spread out in an open case, and all in I spent about $8k, as almost every part in the system is used (only the motherboard was bought new - a supermicro supermicro h12ssl-i which was $400 at the time).&lt;br /&gt; The most I paid for a GPU was $700, the lowest was $500, which was just this week. FB Marketplace is great in my area - I had tons of options and I highly recommend local sellers over ebay.&lt;br /&gt; All I've done so far is load GLM 4.5 air Q6_K GGUF using llama.cpp, specifically these settings - &lt;code&gt;llama-server \-m /home/hisma/llama.cpp/models/GLM-4.5-Air.i1-Q6_K/GLM-4.5-Air.i1-Q6_K.gguf -c 131072 -ngl 99 -b 4096 -ub 2048 -fa --temp 0.6 --top-p 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8888&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From the screenshot, you can see it pulled off a respectable ~49 t/s.&lt;br /&gt; My next steps -&lt;/p&gt; &lt;ul&gt; &lt;li&gt;power limit all cards to ~250W (maybe lower depending on how my system responds - confident I shouldn't need to go any lower than 200W which would only be a ~20% perf hit)&lt;/li&gt; &lt;li&gt;test some AWQ models using VLLM with tensor parallelism (specifically MiniMax-M2-AWQ-4bit). &lt;ul&gt; &lt;li&gt;My whole reason for going to 8 GPUs is bc TP requires either 2, 4 or 8 cards. So 8 cards was always my goal to get the most out of this system&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Once I find a solid set of models, start doing some agentic coding with roocode &amp;amp; let this thing rip&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With PC hardware prices going insane lately, I feel lucky to have this thing, even with the janky ass build. It was a good learning experience &amp;amp; certainly would do some things different w/ the lessons I learned, but I forsee future enshittification of cloud models as the big corpos pivot to pleasing shareholders over burning cash, and in the 1 year I've had this system local models have continued to improve and trade blows with frontier models while using less memory, I'm sure the trend will continue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phcyvk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:54:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1phsqix</id>
    <title>Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use</title>
    <updated>2025-12-09T00:04:35+00:00</updated>
    <author>
      <name>/u/0xmaxhax</name>
      <uri>https://old.reddit.com/user/0xmaxhax</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"&gt; &lt;img alt="Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use" src="https://preview.redd.it/s0wx32rvk26g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=497a12c2009280cf7d68db66bd9159fbbb109206" title="Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As of recent swe-bench evaluations, this is where top open weight models stand regarding real-world agentic coding use. My personal experience, though, is different.&lt;/p&gt; &lt;p&gt;Benchmarks are very crude approximations of a models ability to perform in specific use cases (i.e. solving real-world GitHub issues for top Python repositories in this case), but nothing than that - a rough, inherently flawed approximation to be taken with extreme caution. Not to mention they often gloss over the unpredictability of results in real-world usage along with the large margin of error in benchmarking.&lt;/p&gt; &lt;p&gt;Now, in my experience (within Claude Code), Minimax M2 is good for what it is; an efficient, compact, and effective tool-calling agent - but I feel it somewhat lacks the reasoning depth required for planning and executing complex problems without veering off course. It’s amazingly efficient and capable for local use at Q4 quant, and works well for most use cases. GLM 4.6, in my experience, seems to be like a more reliable choice to daily drive, and can handle more difficult tasks if properly guided - I’d say it’s only slightly worse than Sonnet 4.5 in CC (for my particular use case) - the difference is not very noticeable to me. I have not yet had the opportunity to try out Deepseek v3.2 within CC, but I will update this post on my thoughts once I do. From what I’ve heard / read, it is a noticeable step up from v3.2-exp, which means it should land at or very slightly above GLM 4.6 for agentic coding use (matching what swe-bench recently reports).&lt;/p&gt; &lt;p&gt;In many ways, open weight models are growing increasingly more practical for local and professional use in agentic coding applications, especially with the latest releases and architectural / training advancements. I would love to know your thoughts: Which open LLM (for local or API use) is best for agentic coding, whether it be in CC or in other platforms? What is your experience with the provided models, and does Deepseek v3.2 surpass GLM 4.6 and/or Minimax M2 for your use cases? And if anyone has run private, non-polluted evaluations of the aforementioned models as of recently, I’m interested in your results. Disagreement is welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xmaxhax"&gt; /u/0xmaxhax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s0wx32rvk26g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1phz8vy</id>
    <title>The Absurdity of the prices of consumer RAM versus ECC RAM</title>
    <updated>2025-12-09T05:21:16+00:00</updated>
    <author>
      <name>/u/Substantial_Cut_9418</name>
      <uri>https://old.reddit.com/user/Substantial_Cut_9418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"&gt; &lt;img alt="The Absurdity of the prices of consumer RAM versus ECC RAM" src="https://b.thumbs.redditmedia.com/qkgwBgpSDc_KLfK_m0RtTj-8qk1N-2lbUTbO4kdymGg.jpg" title="The Absurdity of the prices of consumer RAM versus ECC RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Cut_9418"&gt; /u/Substantial_Cut_9418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phz8vy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T05:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi4yr4</id>
    <title>GLM-4.6V Model Now Available in GGUF Format</title>
    <updated>2025-12-09T11:20:33+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"&gt; &lt;img alt="GLM-4.6V Model Now Available in GGUF Format" src="https://external-preview.redd.it/DmQpOneG32bl0j63UZH5xIwLDgq-lgYKNllu4rNGOIU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1491231818dca9afc91a1c8bbef01c666b2238d" title="GLM-4.6V Model Now Available in GGUF Format" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently came across the GGUF version of the popular GLM-4.6V Flash model. I shared this as this will be useful to many who want to try this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T11:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1phzzrq</id>
    <title>Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?</title>
    <updated>2025-12-09T06:04:27+00:00</updated>
    <author>
      <name>/u/__issac</name>
      <uri>https://old.reddit.com/user/__issac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"&gt; &lt;img alt="Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?" src="https://preview.redd.it/5a2im0y2d46g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef5e61c3d9ead02bd600ec3c330ee11fda109956" title="Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(I know Artificial Analysis is suck. But is interesting:)) I think now the hype is almost gone, so I have some question. Benchmark says thier models(even 30b a3b and 4b!) beat gpt4. But what do you think? Please don't tell me &amp;quot;depends on field&amp;quot;. We should compare on overall performance. Because benchmark says it is. Can we now truly replace old flagship closed-source model with a small open model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__issac"&gt; /u/__issac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5a2im0y2d46g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T06:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1phjxca</id>
    <title>I'm calling these people out right now.</title>
    <updated>2025-12-08T18:21:39+00:00</updated>
    <author>
      <name>/u/WeMetOnTheMountain</name>
      <uri>https://old.reddit.com/user/WeMetOnTheMountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For being heroes of the community.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;|Blazing fast fine-tuning + premium GGUF quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mradermacher&lt;/strong&gt;|Quantizes literally EVERYTHING, absolute machine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bartowski&lt;/strong&gt;|High-quality quants, great documentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TheBloke&lt;/strong&gt;|The OG - before he stepped back, he was THE source&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoneStriker&lt;/strong&gt;|Solid AWQ/GPTQ quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nexesenex&lt;/strong&gt;|iMatrix quants, gap hunter and filler&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everyone here owes so much to you folks. Take a bow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WeMetOnTheMountain"&gt; /u/WeMetOnTheMountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1phn925</id>
    <title>Thoughts?</title>
    <updated>2025-12-08T20:25:29+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt; &lt;img alt="Thoughts?" src="https://preview.redd.it/j6fp9xhsh16g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2979be36927eb9e804221b6247830706ea9e7487" title="Thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting take&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6fp9xhsh16g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T20:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1phujwo</id>
    <title>Check on lil bro</title>
    <updated>2025-12-09T01:25:42+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt; &lt;img alt="Check on lil bro" src="https://preview.redd.it/s8rfm29bz26g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99684b39e5571f190bf37b141c34049e9f79cc1" title="Check on lil bro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s8rfm29bz26g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T01:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
