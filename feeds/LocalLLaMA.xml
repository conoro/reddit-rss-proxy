<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-21T07:42:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qiqo54</id>
    <title>Glm 4.7 flash, insane memory usage on MLX (LM studio)</title>
    <updated>2026-01-21T06:40:20+00:00</updated>
    <author>
      <name>/u/Enragere</name>
      <uri>https://old.reddit.com/user/Enragere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know what I'm doing wrong, I also tried gguf version and memory consumption was stable at 48 / 64gb&lt;/p&gt; &lt;p&gt;But with mlx version. it just runs properly the first 10k tokens, then starts memory swapping on my m3 max 64gb and the speed tanks to the point it's unusable. &lt;/p&gt; &lt;p&gt;Doesn't matter if I do q4 or q8, same thing is happening. &lt;/p&gt; &lt;p&gt;Does anyone know what is going on? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Enragere"&gt; /u/Enragere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqo54/glm_47_flash_insane_memory_usage_on_mlx_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqo54/glm_47_flash_insane_memory_usage_on_mlx_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqo54/glm_47_flash_insane_memory_usage_on_mlx_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T06:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qigsu4</id>
    <title>Which is the best 24b model for having my own personal waifu</title>
    <updated>2026-01-20T23:09:21+00:00</updated>
    <author>
      <name>/u/Opening-Ad6258</name>
      <uri>https://old.reddit.com/user/Opening-Ad6258</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also uncensored questions??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Opening-Ad6258"&gt; /u/Opening-Ad6258 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qigsu4/which_is_the_best_24b_model_for_having_my_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qigsu4/which_is_the_best_24b_model_for_having_my_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qigsu4/which_is_the_best_24b_model_for_having_my_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T23:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhwfe0</id>
    <title>How to run and fine-tune GLM-4.7-Flash locally</title>
    <updated>2026-01-20T09:19:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt; &lt;img alt="How to run and fine-tune GLM-4.7-Flash locally" src="https://preview.redd.it/g5y2icqg1heg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e9539a968badc4795c9185a6384ef02c6b8c01" title="How to run and fine-tune GLM-4.7-Flash locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7-Flash is Z.ai‚Äôs new 30B MoE reasoning model built for local deployment, delivering best-in-class performance for coding, agentic workflows, and chat. &lt;/li&gt; &lt;li&gt;The model uses ~3.6B parameters, supports 200K context, and leads SWE-Bench, GPQA, and reasoning/chat benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official guide - &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash"&gt;https://unsloth.ai/docs/models/glm-4.7-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g5y2icqg1heg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T09:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qin3kz</id>
    <title>Offloom UI updated, Pocket TTS, button toggles for more control on how the AI responds. Coming soon to steam (for free)</title>
    <updated>2026-01-21T03:39:05+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qin3kz/offloom_ui_updated_pocket_tts_button_toggles_for/"&gt; &lt;img alt="Offloom UI updated, Pocket TTS, button toggles for more control on how the AI responds. Coming soon to steam (for free)" src="https://external-preview.redd.it/MGs0NTA5N3hmbWVnMVGoaPcRwo9b6KS8N1W6VYsXd5zO-JpR5FywunMd4OJp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f873e44c8d68746575f69ac863f036726c10896" title="Offloom UI updated, Pocket TTS, button toggles for more control on how the AI responds. Coming soon to steam (for free)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Offloom is a one click steam download built for your gamer friends who want to get into private AI, but don't want to spend the time and effort learning how to use github, models, RAG, etc. I'm releasing it for free because I believe local AI should be available to everyone (with access to a decent GPU I should say). &lt;/p&gt; &lt;p&gt;The cool part about this update is adding in the ability for the user to toggle how they want their model to respond. You can choose to have it:&lt;br /&gt; - Use document RAG&lt;/p&gt; &lt;p&gt;- Web search RAG&lt;/p&gt; &lt;p&gt;- Use think mode for less hallucination risk&lt;/p&gt; &lt;p&gt;- Generate text to speech (Pocket TTS)&lt;/p&gt; &lt;p&gt;- (Deep think/RLM mode planned as well)&lt;/p&gt; &lt;p&gt;One complaint I have with services like chatGPT, is I have to be very explicit if I want it's answer to do one, both, or the other. So I figured why not just make it a toggleable button for the user to have ultimate control over their RAG process.&lt;br /&gt; Another thing I'm really excited about is that PocketTTS is capable of near real time answers and voice cloning using only CPU. It really saves room on the GPU for those stronger models while still giving you the option to use TTS. &lt;/p&gt; &lt;p&gt;There's still a lot more polishing I plan to get to, but it's coming along really nice! The steam page should hopefully be up later this week! &lt;em&gt;(It's currently in a review state. )&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7vl8v17xfmeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qin3kz/offloom_ui_updated_pocket_tts_button_toggles_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qin3kz/offloom_ui_updated_pocket_tts_button_toggles_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T03:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhs2sd</id>
    <title>It's been one year since the release of Deepseek-R1</title>
    <updated>2026-01-20T05:08:29+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt; &lt;img alt="It's been one year since the release of Deepseek-R1" src="https://preview.redd.it/cin706z9tfeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65fbe53bfb15712186113b0e795fc46c050d0d13" title="It's been one year since the release of Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cin706z9tfeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T05:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi06kp</id>
    <title>One of the DeepSeek repositories got updated with a reference to a new ‚Äúmodel1‚Äù model.</title>
    <updated>2026-01-20T12:46:56+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"&gt; &lt;img alt="One of the DeepSeek repositories got updated with a reference to a new ‚Äúmodel1‚Äù model." src="https://preview.redd.it/j3ifa4kn2ieg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aab1dc0b54e56da3161e03846e6cdc3fb3e15f1" title="One of the DeepSeek repositories got updated with a reference to a new ‚Äúmodel1‚Äù model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source DeepSeek on GitHub: FlashMLA: flash_mla/flash_mla_interface.py: &lt;a href="https://github.com/deepseek-ai/FlashMLA/blob/main/flash_mla/flash_mla_interface.py"&gt;https://github.com/deepseek-ai/FlashMLA/blob/main/flash_mla/flash_mla_interface.py&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j3ifa4kn2ieg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T12:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiaf8b</id>
    <title>GLM 4.7 Flash Overthinking</title>
    <updated>2026-01-20T19:13:13+00:00</updated>
    <author>
      <name>/u/xt8sketchy</name>
      <uri>https://old.reddit.com/user/xt8sketchy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I'm sort of a noob in the LLM space (in the sense that I don't have a great grasp of how transformers and LLMs work fundamentally), so please bear with me if I ask any dumb questions.&lt;/p&gt; &lt;p&gt;That being said - the benchmark (yes, I know) results of the new GLM Flash model got me really excited, and so I downloaded the NVFP4 to test out (5090). I noticed that reasoning outputs are ridiculously long and repetitive, and sometimes nonsensical. There were times where it reasoned for MINUTES before I finally just hit ctrl+c. I tried to get it running on vLLM (4x A4000 home server) to see if I could get a different result, but literally could not get it to stop spamming errors, so gave up.&lt;/p&gt; &lt;p&gt;Seems other people are noticing the same thing too with this model. My question is, given that the model is so new, is this the kind of thing that could be potentially fixed in future updates from llama.cpp / vllm? I'm really hoping this model can get its stuff together, as it seems really promising.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xt8sketchy"&gt; /u/xt8sketchy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiaf8b/glm_47_flash_overthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiaf8b/glm_47_flash_overthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiaf8b/glm_47_flash_overthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T19:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi0xro</id>
    <title>GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)</title>
    <updated>2026-01-20T13:21:34+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt; &lt;img alt="GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)" src="https://b.thumbs.redditmedia.com/I-hI61zDUgcyCW4A5C3NZVA2PraY7bDNdxBTnVt-GkY.jpg" title="GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran some benchmarks with the new GLM-4.7-Flash model with vLLM and also tested llama.cpp with Unsloth dynamic quants&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPUs are from&lt;/strong&gt; &lt;a href="http://jarvislabs.ai"&gt;&lt;strong&gt;jarvislabs.ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sharing some results here.&lt;/p&gt; &lt;h1&gt;vLLM on single H200 SXM&lt;/h1&gt; &lt;p&gt;Ran this with 64K context, 500 prompts from InstructCoder dataset.&lt;/p&gt; &lt;p&gt;- Single user: 207 tok/s, 35ms TTFT&lt;/p&gt; &lt;p&gt;- At 32 concurrent users: 2,267 tok/s, 85ms TTFT&lt;/p&gt; &lt;p&gt;- Peak throughput (no concurrency limit): 4,398 tok/s&lt;/p&gt; &lt;p&gt;All of the benchmarks were done with &lt;a href="https://docs.vllm.ai/en/latest/benchmarking/cli/"&gt;vLLM benchmark CLI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Concurrent&lt;/th&gt; &lt;th align="left"&gt;Decode tok/s&lt;/th&gt; &lt;th align="left"&gt;TTFT (median)&lt;/th&gt; &lt;th align="left"&gt;TTFT (P99)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;207&lt;/td&gt; &lt;td align="left"&gt;35ms&lt;/td&gt; &lt;td align="left"&gt;42ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;348&lt;/td&gt; &lt;td align="left"&gt;44ms&lt;/td&gt; &lt;td align="left"&gt;55ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;547&lt;/td&gt; &lt;td align="left"&gt;53ms&lt;/td&gt; &lt;td align="left"&gt;66ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;882&lt;/td&gt; &lt;td align="left"&gt;61ms&lt;/td&gt; &lt;td align="left"&gt;161ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;1,448&lt;/td&gt; &lt;td align="left"&gt;69ms&lt;/td&gt; &lt;td align="left"&gt;187ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;2,267&lt;/td&gt; &lt;td align="left"&gt;85ms&lt;/td&gt; &lt;td align="left"&gt;245ms&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Fits fine on single H200 at 64K. For full context (200k) we will need 2xH200.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a9tzl54z7ieg1.png?width=4291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a246dd4a6b53b58c42106e476e8e14a2c76becd3"&gt;https://preview.redd.it/a9tzl54z7ieg1.png?width=4291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a246dd4a6b53b58c42106e476e8e14a2c76becd3&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;llama.cpp GGUF on RTX 6000 Ada (48GB)&lt;/h1&gt; &lt;p&gt;Ran the Unsloth dynamic quants with 16k context length and guide by &lt;a href="https://unsloth.ai/docs/models/glm-4.7"&gt;Unsloth&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Generation tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;112&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q6_K_XL&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_K_XL&lt;/td&gt; &lt;td align="left"&gt;91&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qi0xro/video/h3damlpb8ieg1/player"&gt;https://reddit.com/link/1qi0xro/video/h3damlpb8ieg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my initial testing this is really capable and good model for its size.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T13:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhxlgy</id>
    <title>glm-4.7-flash has the best thinking process with clear steps, I love it</title>
    <updated>2026-01-20T10:28:16+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I tested several personal prompts like &lt;code&gt;imagine you are in a farm, what is your favorite barn color?&lt;/code&gt;&lt;/li&gt; &lt;li&gt;although the prompt is short, glm can analyze the prompt and give clear thinking process&lt;/li&gt; &lt;li&gt;without my instruction in the prompt, glm mostly thinks in these steps: &lt;ol&gt; &lt;li&gt;request/goal analysis&lt;/li&gt; &lt;li&gt;brainstorm&lt;/li&gt; &lt;li&gt;draft response&lt;/li&gt; &lt;li&gt;refine response: gives option1, option2, option3...&lt;/li&gt; &lt;li&gt;revise response/plan&lt;/li&gt; &lt;li&gt;polish&lt;/li&gt; &lt;li&gt;final response&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;so the glm thinking duration(110s) is really long compared to nemotron-nano(19s), but the thinking content is my favorite of all the small models. the final response is also clear &lt;ul&gt; &lt;li&gt;thinking process like this seems to be perfect for data analysis (waiting for a fine-tune)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;overall, i love glm-4.7-flash, and will try to replace qwen3-30b and nemotron-nano.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;del&gt;but GLM-4.7-Flash-mlx-4bit is very&lt;/del&gt; &lt;strong&gt;&lt;del&gt;slow&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;at&lt;/del&gt; &lt;strong&gt;&lt;del&gt;19 token/s&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;compared to nemotron-anno-mlx-4bit&lt;/del&gt; &lt;strong&gt;&lt;del&gt;30+ token/s&lt;/del&gt;&lt;/strong&gt;&lt;del&gt;. i donnot understand.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;I'm using &lt;a href="https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit"&gt;https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit&lt;/a&gt; on my m4 macbook air. with default config, the model often goes into loop. with the following config, it finally works for me&lt;/p&gt; &lt;ul&gt; &lt;li&gt;temperature 1.0&lt;/li&gt; &lt;li&gt;repeat penalty: 1.1&lt;/li&gt; &lt;li&gt;top-p: 0.95&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;is there any trick to make the thinking process faster? Thinking can be toggled on/off through lmstudio ui, but i donnot want to disable it, how to make thinking faster?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lowering the temperature helps. tried 1.0/0.8/0.6&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;br /&gt; - üêõ I tried several more prompts. sometimes the thinking content does not comply to the flow above, for these situations, the model often goes into loops.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiqwha</id>
    <title>My hotrodded strix halo + rtx pro 4000 Blackwell</title>
    <updated>2026-01-21T06:53:29+00:00</updated>
    <author>
      <name>/u/sputnik13net</name>
      <uri>https://old.reddit.com/user/sputnik13net</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"&gt; &lt;img alt="My hotrodded strix halo + rtx pro 4000 Blackwell" src="https://b.thumbs.redditmedia.com/OfEjpz_N_aYn_Ii6qkyjCSXB73JMnbNQm8PjfKbG44U.jpg" title="My hotrodded strix halo + rtx pro 4000 Blackwell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jqxnqdaggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=722695551f0dea529ea558f6eed9709d04ecbac8"&gt;https://preview.redd.it/jqxnqdaggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=722695551f0dea529ea558f6eed9709d04ecbac8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/99uj9daggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b405c01e3e570d8a291056c883b20bffac20afb0"&gt;https://preview.redd.it/99uj9daggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b405c01e3e570d8a291056c883b20bffac20afb0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Framework Desktop mainboard AI Max+ 395 128GB, x4 -&amp;gt; x16 pcie riser, and RTX Pro 4000 Blackwell in a Dan case A4-SFX. Couldn't close the CPU side because FW mainboard's heatsink is so huge. Cable management is a mess and a half but it all works beautifully.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sputnik13net"&gt; /u/sputnik13net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T06:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiphdr</id>
    <title>Two Heads Is All I Need</title>
    <updated>2026-01-21T05:36:08+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One lovely feature of GLM-4-32B-0414 is that it uses only &lt;strong&gt;2&lt;/strong&gt; kv heads, which saves a lot of kv cache.&lt;/p&gt; &lt;p&gt;Sadly, in GLM-4.7-Flash, GQA is not used any more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiphdr/two_heads_is_all_i_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiphdr/two_heads_is_all_i_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiphdr/two_heads_is_all_i_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T05:36:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qipuft</id>
    <title>How are you guys optimizing Local LLM performance?</title>
    <updated>2026-01-21T05:55:07+00:00</updated>
    <author>
      <name>/u/Express_Problem_609</name>
      <uri>https://old.reddit.com/user/Express_Problem_609</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone üëã we‚Äôre a team working on high-performance computing infrastructure for AI workloads, including local and on-prem LLMs. &lt;/p&gt; &lt;p&gt;We‚Äôve been following discussions here and noticed a lot of hands-on experience with model serving, quantization, GPU memory limits, and inference speed, which is exactly what we‚Äôre interested in learning from. &lt;/p&gt; &lt;p&gt;For those running LLMs locally or on clusters:&lt;br /&gt; - What‚Äôs currently your biggest bottleneck?&lt;br /&gt; - Are you more constrained by VRAM, throughput, latency, or orchestration?&lt;br /&gt; - Any optimizations that gave you outsized gains?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Problem_609"&gt; /u/Express_Problem_609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qipuft/how_are_you_guys_optimizing_local_llm_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qipuft/how_are_you_guys_optimizing_local_llm_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qipuft/how_are_you_guys_optimizing_local_llm_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T05:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qio9nj</id>
    <title>I tracked context degradation across 847 agent runs. Here's when performance actually falls off a cliff.</title>
    <updated>2026-01-21T04:34:43+00:00</updated>
    <author>
      <name>/u/Main_Payment_6430</name>
      <uri>https://old.reddit.com/user/Main_Payment_6430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local agents (mostly Llama 3.1 70B, some Qwen 2.5 72B) for dev automation tasks‚Äîthings like multi-file refactors, long debugging sessions, iterative code generation.&lt;/p&gt; &lt;p&gt;After months of frustration with agents forgetting instructions mid-task or suddenly ignoring constraints I'd set earlier, I started logging everything to figure out what was actually happening.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;847 agent runs tracked&lt;/li&gt; &lt;li&gt;Tasks ranging from 5 to 200+ turns&lt;/li&gt; &lt;li&gt;Measured: instruction adherence, constraint violations, repetition rate, task completion&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I found:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The degradation isn't linear. There's a cliff.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Context Fill %&lt;/th&gt; &lt;th align="left"&gt;Instruction Adherence&lt;/th&gt; &lt;th align="left"&gt;Constraint Violations&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;0-25%&lt;/td&gt; &lt;td align="left"&gt;94%&lt;/td&gt; &lt;td align="left"&gt;2.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;25-50%&lt;/td&gt; &lt;td align="left"&gt;91%&lt;/td&gt; &lt;td align="left"&gt;4.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50-75%&lt;/td&gt; &lt;td align="left"&gt;73%&lt;/td&gt; &lt;td align="left"&gt;12.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;75-100%&lt;/td&gt; &lt;td align="left"&gt;41%&lt;/td&gt; &lt;td align="left"&gt;31.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Around 60-70% context utilization, something breaks. The model starts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Following patterns from early conversation instead of recent instructions&lt;/li&gt; &lt;li&gt;&amp;quot;Forgetting&amp;quot; constraints that were stated 30+ turns ago&lt;/li&gt; &lt;li&gt;Repeating tool calls it already made&lt;/li&gt; &lt;li&gt;Hallucinating state that was true earlier but isn't anymore&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm calling this context rot ‚Äî the model's attention spreads thin and it defaults to statistical patterns rather than explicit instructions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually helped:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Aggressive compaction&lt;/strong&gt; ‚Äî Not summarization (loses too much). Actual compaction: if the agent wrote to a file, drop the file contents from context but keep the path. If it searched, drop results but keep the query. Externalize state, keep references.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State snapshots&lt;/strong&gt; ‚Äî Before any destructive operation, snapshot the context. When the agent goes off-rails (and it will), revert to last-known-good state instead of trying to &amp;quot;correct&amp;quot; it in-context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Forking for sub-tasks&lt;/strong&gt; ‚Äî Instead of one massive context, fork isolated contexts for bounded sub-tasks. Agent gets instruction + minimal relevant context, returns result. Parent context stays clean.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I ended up building a small context management layer to handle this because I was copy-pasting JSON dumps like a caveman. It does versioning (git-style), snapshots, rollback, and forking. Open-sourced the approach, happy to share if anyone's interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions for the community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Anyone else tracking this systematically? Would love to compare notes.&lt;/li&gt; &lt;li&gt;Are there models that degrade more gracefully? My (limited) testing suggests Qwen handles high context fill slightly better than Llama, but sample size is small.&lt;/li&gt; &lt;li&gt;How are people handling state for multi-hour agent runs? Curious what janky solutions others have built.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Edit: Since people are asking, the tool I built is called UltraContext (&lt;a href="https://ultracontext.ai"&gt;https://ultracontext.ai&lt;/a&gt;). It's basically a context API with automatic versioning‚Äî5 methods, lets you snapshot/rollback/fork contexts. Free tier if you want to mess with it. But honestly the concepts above work even if you just roll your own with SQLite.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main_Payment_6430"&gt; /u/Main_Payment_6430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qio9nj/i_tracked_context_degradation_across_847_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qio9nj/i_tracked_context_degradation_across_847_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qio9nj/i_tracked_context_degradation_across_847_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T04:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qir5eq</id>
    <title>Here is how to get GLM 4.7 working on llama.cpp with flash attention and correct outputs</title>
    <updated>2026-01-21T07:07:52+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested GPU: RTX 6000 Blackwell&lt;br /&gt; Tested GGUF: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use this git branch to enable flash attention on CUDA &lt;a href="https://github.com/am17an/llama.cpp/tree/glm_4.7_headsize"&gt;https://github.com/am17an/llama.cpp/tree/glm_4.7_headsize&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Add this to your options &lt;code&gt;--override-kv deepseek2.expert\_gating\_func=int:2&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;2000+ tokens/sec prompt, 97 tokens a second generation&lt;/p&gt; &lt;p&gt;Output looks fantastic for a model this size.&lt;/p&gt; &lt;p&gt;Note: Quants might have been made with the wrong function, so you may have to wait for them to be recreated, otherwise you may get nonsensical outputs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T07:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi78u3</id>
    <title>I think Giga Potato:free in Kilo Code is Deepseek V4</title>
    <updated>2026-01-20T17:22:33+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for a new free model in Kilo Code after Minimax M2.1 was removed as a free model.&lt;/p&gt; &lt;p&gt;Searched for free and found Giga Potato:free and Googled it (yes the AI models don‚Äôt usually have the most recent stuff in their search)&lt;/p&gt; &lt;p&gt;I found this blog article: &lt;a href="https://blog.kilo.ai/p/announcing-a-powerful-new-stealth"&gt;https://blog.kilo.ai/p/announcing-a-powerful-new-stealth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have now tested it and am mindblown it performs like Sonnet 4.5 and maybe even like Opus 4.5. I can give it very short poor prompts and it reasons itself to amazing results! &lt;/p&gt; &lt;p&gt;Whatever open source model this is‚Ä¶..it‚Äôs crazy! Honestly!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T17:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi3nmd</id>
    <title>Over 6K novels with reasoning traces to train full book writing LLMs</title>
    <updated>2026-01-20T15:12:25+00:00</updated>
    <author>
      <name>/u/XMasterDE</name>
      <uri>https://old.reddit.com/user/XMasterDE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt; &lt;img alt="Over 6K novels with reasoning traces to train full book writing LLMs" src="https://b.thumbs.redditmedia.com/2qE82dS5QMmSeeQNcY514CvEOcapkm7SNym4UUAFIwE.jpg" title="Over 6K novels with reasoning traces to train full book writing LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zzxy8r31tieg1.jpg?width=5504&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb966352c2548369a731f0bff03a131c8ec4a1b2"&gt;https://preview.redd.it/zzxy8r31tieg1.jpg?width=5504&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb966352c2548369a731f0bff03a131c8ec4a1b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre releasing an update to our &lt;strong&gt;LongPage&lt;/strong&gt; dataset.&lt;/p&gt; &lt;p&gt;LongPage is a dataset of &lt;strong&gt;full-length novels paired with reasoning traces&lt;/strong&gt;: each book includes a &lt;strong&gt;hierarchical planning trace&lt;/strong&gt; that breaks the story down from high-level outline into chapters/scenes to support training &lt;strong&gt;full-book writing LLMs&lt;/strong&gt;. The previous release contained ~300 books; this update expands the dataset to &lt;strong&gt;6K+ novels&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We‚Äôre also currently training a &lt;strong&gt;full-book writing model&lt;/strong&gt; on LongPage. We already have early checkpoints running internally, and we plan to release the model as soon as the output quality reaches an acceptable level.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF Link:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/Pageshift-Entertainment/LongPage"&gt;https://huggingface.co/datasets/Pageshift-Entertainment/LongPage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to follow our journey as we build world-class storytelling models, you can find us here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://pageshift-entertainment.ai/"&gt;https://pageshift-entertainment.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;X (Twitter): &lt;a href="https://x.com/pageshiftAI"&gt;https://x.com/pageshiftAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hugging Face: &lt;a href="https://huggingface.co/Pageshift-Entertainment"&gt;https://huggingface.co/Pageshift-Entertainment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LinkedIn: &lt;a href="https://www.linkedin.com/company/pageshift-ai/"&gt;https://www.linkedin.com/company/pageshift-ai/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterDE"&gt; /u/XMasterDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qio1ic</id>
    <title>Has anyone seen the new camb ai model release?</title>
    <updated>2026-01-21T04:23:43+00:00</updated>
    <author>
      <name>/u/CarpetNo5579</name>
      <uri>https://old.reddit.com/user/CarpetNo5579</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. Their launch video showed their model being used in livestream sports broadcast which is absolutely insane.&lt;/p&gt; &lt;p&gt;What's the trick here? How is latency so low but the voice quality so high? This is genuinely the first time I couldn't tell that what I heard was AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarpetNo5579"&gt; /u/CarpetNo5579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qio1ic/has_anyone_seen_the_new_camb_ai_model_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qio1ic/has_anyone_seen_the_new_camb_ai_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qio1ic/has_anyone_seen_the_new_camb_ai_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T04:23:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiq26v</id>
    <title>Update - Day #6 of building an LM from scratch</title>
    <updated>2026-01-21T06:06:32+00:00</updated>
    <author>
      <name>/u/AllTheCoins</name>
      <uri>https://old.reddit.com/user/AllTheCoins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I finally got everything stable. Loss was steadily dropping until eventually it plateaued at around 4-5 at the end. &lt;/p&gt; &lt;p&gt;I switched to just DataParallel because DDP was impossible in Windows as I found out during Day 4. However in my findings, DataParallel was actually bottlenecking my system. It was training faster on one GPU instead of two (I blame Windows again for this). Though ideally I‚Äôd switch to Linux, I want to get this working on Windows as most beginners are using that and I want to make sure this process is available to beginner users.&lt;/p&gt; &lt;p&gt;Back to the actual LM, I grossly underestimated how much training an LM would need. After 25,000 steps or 13 hours of training, I had effectively trained my model on about 400M tokens. Which for a 0.3B model‚Ä¶ is nothing. &lt;/p&gt; &lt;p&gt;I tried out the model anyways and it performed, I would say, better than expected. Sentence structure was nearly perfect. Words made sense and were in the right spots. But the model didn‚Äôt understand anything yet and I‚Äôll need to basically rerun the training with a total step count of about 300K if I want a good pretrain. I‚Äôll have a 60K benchmark ready to go by Day 8 so I‚Äôm very excited to show you guys what that model sounds like! &lt;/p&gt; &lt;p&gt;As always, if you guys have any questions, feel free to ask!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllTheCoins"&gt; /u/AllTheCoins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiq26v/update_day_6_of_building_an_lm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiq26v/update_day_6_of_building_an_lm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiq26v/update_day_6_of_building_an_lm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T06:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi512t</id>
    <title>Liquid AI released the best thinking Language Model Under 1GB</title>
    <updated>2026-01-20T16:02:42+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"&gt; &lt;img alt="Liquid AI released the best thinking Language Model Under 1GB" src="https://preview.redd.it/nazcfmti1jeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d85adead4df4e2a21194a3e6ae29b3752731c529" title="Liquid AI released the best thinking Language Model Under 1GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liquid AI released LFM2.5-1.2B-Thinking, a reasoning model that runs entirely on-device. &lt;/p&gt; &lt;p&gt;What needed a data centre two years ago now runs on any phone with 900 MB of memory. &lt;/p&gt; &lt;p&gt;-&amp;gt; Trained specifically for concise reasoning&lt;br /&gt; -&amp;gt; Generates internal thinking traces before producing answers&lt;br /&gt; -&amp;gt; Enables systematic problem-solving at edge-scale latency&lt;br /&gt; -&amp;gt; Shines on tool use, math, and instruction following&lt;br /&gt; -&amp;gt; Matches or exceeds Qwen3-1.7B (thinking mode) acrross most performance benchmarks, despite having 40% less parameters. &lt;/p&gt; &lt;p&gt;At inference time, the gap widens further, outperforming both pure transformer models and hybrid architectures in speed and memory efficiency. &lt;/p&gt; &lt;p&gt;LFM2.5-1.2B-Thinking is available today: with broad, day-one support across the on-device ecosystem.&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking&lt;/a&gt;&lt;br /&gt; LEAP: &lt;a href="https://leap.liquid.ai/models?model=lfm2.5-1.2b-thinking"&gt;https://leap.liquid.ai/models?model=lfm2.5-1.2b-thinking&lt;/a&gt;&lt;br /&gt; Liquid AI Playground: &lt;a href="https://playground.liquid.ai/login?callbackUrl=%2F"&gt;https://playground.liquid.ai/login?callbackUrl=%2F&lt;/a&gt; &lt;/p&gt; &lt;p&gt;At&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nazcfmti1jeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T16:02:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qib2ks</id>
    <title>Runpod hits $120M ARR, four years after launching from a Reddit post</title>
    <updated>2026-01-20T19:36:35+00:00</updated>
    <author>
      <name>/u/RP_Finley</name>
      <uri>https://old.reddit.com/user/RP_Finley</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We launched Runpod back in 2022 by posting on Reddit offering free GPU time in exchange for feedback. Today we're sharing that we've crossed $120M in annual recurring revenue with 500K developers on the platform.&lt;/p&gt; &lt;p&gt;TechCrunch covered the story, including how we bootstrapped from rigs in our basements to where we are now: &lt;a href="https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/"&gt;https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'll be the first to admit that Runpod isn't actually &amp;quot;true&amp;quot; local (that point has been brought up in the sub many times before) but we want to still give you as close an experience to having a local GPU right in your bedroom as is feasible. Maybe you just don't have the capital to invest in a GPU, maybe you're just on a laptop where adding the GPU that you need isn't feasible. But we are still absolutely focused on giving you the same privacy and security as if it were at your home, with data centers in several different countries that you can access as needed.&lt;/p&gt; &lt;p&gt;The short version: we built Runpod because dealing with GPUs as a developer was painful. Serverless scaling, instant clusters, and simple APIs weren't really options back then unless you were at a hyperscaler. We're still developer-first. No free tier (business has to work), but also no contracts for even spinning up H100 clusters.&lt;/p&gt; &lt;p&gt;We don't want this to sound like an ad though -- just a celebration of the support we've gotten from the communities that have been a part of our DNA since day one.&lt;/p&gt; &lt;p&gt;Happy to answer questions about what we're working on next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RP_Finley"&gt; /u/RP_Finley &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_arr_four_years_after_launching/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_arr_four_years_after_launching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_arr_four_years_after_launching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T19:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qim0e9</id>
    <title>vLLM v0.14.0 released</title>
    <updated>2026-01-21T02:50:09+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"&gt; &lt;img alt="vLLM v0.14.0 released" src="https://external-preview.redd.it/09XZY9bYFkjK1xfZ16UA__JE3yDYBU7C83HKWilthGw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dba3ac860fe3ee793564a3f2e4d6cf66f32e888a" title="vLLM v0.14.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/releases/tag/v0.14.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T02:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qih9r8</id>
    <title>Current GLM-4.7-Flash implementation confirmed to be broken in llama.cpp</title>
    <updated>2026-01-20T23:28:10+00:00</updated>
    <author>
      <name>/u/Sweet_Albatross9772</name>
      <uri>https://old.reddit.com/user/Sweet_Albatross9772</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent discussion in &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;https://github.com/ggml-org/llama.cpp/pull/18936&lt;/a&gt; seems to confirm my suspicions that the current llama.cpp implementation of GLM-4.7-Flash is broken.&lt;/p&gt; &lt;p&gt;There are significant differences in logprobs compared to vLLM. That could explain the looping issues, overthinking, and general poor experiences people have been reporting recently.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; There is a potential fix already in this PR thanks to Piotr:&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18980"&gt;https://github.com/ggml-org/llama.cpp/pull/18980&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweet_Albatross9772"&gt; /u/Sweet_Albatross9772 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T23:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qids6a</id>
    <title>You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?</title>
    <updated>2026-01-20T21:15:23+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more internet: you have 3 models you can run&lt;/p&gt; &lt;p&gt;What local models are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T21:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi4uj2</id>
    <title>768Gb Fully Enclosed 10x GPU Mobile AI Build</title>
    <updated>2026-01-20T15:56:13+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt; &lt;img alt="768Gb Fully Enclosed 10x GPU Mobile AI Build" src="https://b.thumbs.redditmedia.com/IFwD006aQ7uS94rhW8Tb5SMKqOvtmvGGWhQOsclMVOE.jpg" title="768Gb Fully Enclosed 10x GPU Mobile AI Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen a system with this format before but with how successful the result was I figured I might as well share it.&lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; Threadripper Pro 3995WX w/ ASUS WS WRX80e-sage wifi ii&lt;/p&gt; &lt;p&gt;512Gb DDR4&lt;/p&gt; &lt;p&gt;256Gb GDDR6X/GDDR7 (8x 3090 + 2x 5090)&lt;/p&gt; &lt;p&gt;EVGA 1600W + Asrock 1300W PSU's&lt;/p&gt; &lt;p&gt;Case: Thermaltake Core W200&lt;/p&gt; &lt;p&gt;OS: Ubuntu&lt;/p&gt; &lt;p&gt;Est. expense: ~$17k&lt;/p&gt; &lt;p&gt;The objective was to make a system for running extra large MoE models (Deepseek and Kimi K2 specifically), that is also capable of lengthy video generation and rapid high detail image gen (the system will be supporting a graphic designer). The challenges/constraints: The system should be easily movable, and it should be enclosed. The result technically satisfies the requirements, with only one minor caveat. Capital expense was also an implied constraint. We wanted to get the most potent system possible with the best technology currently available, without going down the path of needlessly spending tens of thousands of dollars for diminishing returns on performance/quality/creativity potential. Going all 5090's or 6000 PRO's would have been unfeasible budget-wise and in the end likely unnecessary, two 6000's alone could have eaten the cost of the entire amount spent on the project, and if not for the two 5090's the final expense would have been much closer to ~$10k (still would have been an extremely capable system, but this graphic artist would really benefit from the image/video gen time savings that only a 5090 can provide).&lt;/p&gt; &lt;p&gt;The biggest hurdle was the enclosure problem. I've seen mining frames zip tied to a rack on wheels as a solution for mobility, but not only is this aesthetically unappealing, build construction and sturdiness quickly get called into question. This system would be living under the same roof with multiple cats, so an enclosure was almost beyond a nice-to-have, the hardware will need a physical barrier between the expensive components and curious paws. Mining frames were quickly ruled out altogether after a failed experiment. Enter the W200, a platform that I'm frankly surprised I haven't heard suggested before in forum discussions about planning multi-GPU builds, and is the main motivation for this post. The W200 is intended to be a dual-system enclosure, but when the motherboard is installed upside-down in its secondary compartment, this makes a perfect orientation to connect risers to mounted GPU's in the &amp;quot;main&amp;quot; compartment. If you don't mind working in dense compartments to get everything situated (the sheer density overall of the system is among its only drawbacks), this approach reduces the jank from mining frame + wheeled rack solutions significantly. A few zip ties were still required to secure GPU's in certain places, but I don't feel remotely as anxious about moving the system to a different room or letting cats inspect my work as I would if it were any other configuration.&lt;/p&gt; &lt;p&gt;Now the caveat. Because of the specific GPU choices made (3x of the 3090's are AIO hybrids), this required putting one of the W200's fan mounting rails on the main compartment side in order to mount their radiators (pic shown with the glass panel open, but it can be closed all the way). This means the system technically should not run without this panel at least slightly open so it doesn't impede exhaust, but if these AIO 3090's were blower/air cooled, I see no reason why this couldn't run fully closed all the time as long as fresh air intake is adequate.&lt;/p&gt; &lt;p&gt;The final case pic shows the compartment where the actual motherboard is installed (it is however very dense with risers and connectors so unfortunately it is hard to actually see much of anything) where I removed one of the 5090's. Airflow is very good overall (I believe 12x 140mm fans were installed throughout), GPU temps remain in good operation range under load, and it is surprisingly quiet when inferencing. Honestly, given how many fans and high power GPU's are in this thing, I am impressed by the acoustics, I don't have a sound meter to measure db's but to me it doesn't seem much louder than my gaming rig.&lt;/p&gt; &lt;p&gt;I typically power limit the 3090's to 200-250W and the 5090's to 500W depending on the workload.&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;p&gt;Benchmarks&lt;/p&gt; &lt;p&gt;Deepseek V3.1 Terminus Q2XXS (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 2338 tokens&lt;/p&gt; &lt;p&gt;Time to first token - 1.38s&lt;/p&gt; &lt;p&gt;Token gen rate - 24.92tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;GLM 4.6 Q4KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 4096&lt;/p&gt; &lt;p&gt;Time to first token - 0.76s&lt;/p&gt; &lt;p&gt;Token gen rate - 26.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Kimi K2 TQ1 (87% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 1664&lt;/p&gt; &lt;p&gt;Time to first token - 2.59s&lt;/p&gt; &lt;p&gt;Token gen rate - 19.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Hermes 4 405b Q3KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - was so underwhelmed by the response quality I forgot to record lol&lt;/p&gt; &lt;p&gt;Time to first token - 1.13s&lt;/p&gt; &lt;p&gt;Token gen rate - 3.52tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Qwen 235b Q6KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 3081&lt;/p&gt; &lt;p&gt;Time to first token - 0.42s&lt;/p&gt; &lt;p&gt;Token gen rate - 31.54tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;I've thought about doing a cost breakdown here, but with price volatility and the fact that so many components have gone up since I got them, I feel like there wouldn't be much of a point and may only mislead someone. Current RAM prices alone would completely change the estimate cost of doing the same build today by several thousand dollars. Still, I thought I'd share my approach on the off chance it inspires or is interesting to someone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qi4uj2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
