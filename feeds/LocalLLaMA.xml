<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-02T13:50:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n6ce6q</id>
    <title>Building IndieGPU: A software dev's approach to GPU cost optimization(Self-Promo)</title>
    <updated>2025-09-02T06:43:00+00:00</updated>
    <author>
      <name>/u/rakii6</name>
      <uri>https://old.reddit.com/user/rakii6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;A Software dev (with 2YOE) here who got tired of watching startup friends complain about AWS GPU costs. So I built &lt;a href="https://www.indiegpu.com/"&gt;IndieGPU&lt;/a&gt; - simple GPU rental for ML training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I discovered about GPU costs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AWS P3.2xlarge (1x V100): $3.06/hour&lt;/li&gt; &lt;li&gt;For a typical model training session (12-24 hours), that's $36-72 per run&lt;/li&gt; &lt;li&gt;Small teams training 2-3 models per week ‚Üí $300-900/month just for compute&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My approach:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 4070s with 12GB VRAM&lt;/li&gt; &lt;li&gt;Transparent hourly pricing&lt;/li&gt; &lt;li&gt;Docker containers with Jupyter/PyTorch ready in 60 seconds&lt;/li&gt; &lt;li&gt;Focus on training workloads, not production inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt; What are the biggest GPU cost pain points you see for small ML/AI teams? Is it the hourly rate, minimum commitments, or something else?&lt;/p&gt; &lt;p&gt;Right now I am trying to find users who could use the platform for their ML/AI training, free for a month, no strings attached.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rakii6"&gt; /u/rakii6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ce6q/building_indiegpu_a_software_devs_approach_to_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ce6q/building_indiegpu_a_software_devs_approach_to_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ce6q/building_indiegpu_a_software_devs_approach_to_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T06:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6dp35</id>
    <title>How do make GLM stop making up stuff?</title>
    <updated>2025-09-02T08:08:50+00:00</updated>
    <author>
      <name>/u/AxelFooley</name>
      <uri>https://old.reddit.com/user/AxelFooley</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using GLM 4.5 lately via Openrouter and i must say it's a very good model. It works pretty well with tool calling and i like how it structure its output when it comes to web search (like quotes from the pages and reference link printed out by default).&lt;/p&gt; &lt;p&gt;The issue i'm having is that it has a very high tendency at making up stuff when it comes to summarising content.&lt;/p&gt; &lt;p&gt;For example yesterday i asked to make comparison between two products, focusing on real world reviews from trusted sources, and it did a brilliant job at searching relevant sources, collecting the most relevant data and quoting the result.&lt;/p&gt; &lt;p&gt;At the end though, when it was the time for generating a summarised overview with a recommendation, it threw into it an unbelievably realistic, highly credible yet completely invented review. I realised that because for one of the product he mentioned that &amp;quot;trusted reviews praised XYZ feature&amp;quot; but that specific product doesn't even have such feature.&lt;/p&gt; &lt;p&gt;Another example: i asked what are the most profitable passive income businesses in GTA Online for my son, and i asked to refer to reddit as source. Again brilliant job at collecting the sources, and then it produced a detailed list of businesses and a comprehensive action plan on how to achieve the desired result, it even included step by step instructions on what to buy, in what order, etc etc..&lt;/p&gt; &lt;p&gt;Problem is that the steps provided cannot be done because it was referencing stuff that doesn't even exist in the game.&lt;/p&gt; &lt;p&gt;As a last experiment i asked to generate the performance review for one of my employees, i have all peer reviews and performance data stored in a ChromaDB, and it generated one of the best performance reviews i have ever seen....by using peer reviews for other people and completely inventing metrics and results achieved for projects that we never did.&lt;/p&gt; &lt;p&gt;I guess you get the gist by now, it's brilliant at searching and extracting sources, but when it comes to summarising it goes absolutely bananas and makes up a lot of stuff. Which is a bit of a shame because it's still a good model imo, but having to double check every single word from the output kinda invalidates the whole point of using it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelFooley"&gt; /u/AxelFooley &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6dp35/how_do_make_glm_stop_making_up_stuff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6dp35/how_do_make_glm_stop_making_up_stuff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6dp35/how_do_make_glm_stop_making_up_stuff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T08:08:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6hnc4</id>
    <title>Fully Annotated Guide to "What are Diffusion Models?"</title>
    <updated>2025-09-02T12:05:24+00:00</updated>
    <author>
      <name>/u/song-sc</name>
      <uri>https://old.reddit.com/user/song-sc</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/song-sc"&gt; /u/song-sc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ki-seki.github.io/posts/250902-diffusion-annotated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6hnc4/fully_annotated_guide_to_what_are_diffusion_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6hnc4/fully_annotated_guide_to_what_are_diffusion_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T12:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6j9te</id>
    <title>Python script to summarize and talk about a YouTube video</title>
    <updated>2025-09-02T13:19:56+00:00</updated>
    <author>
      <name>/u/autoencoder</name>
      <uri>https://old.reddit.com/user/autoencoder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6j9te/python_script_to_summarize_and_talk_about_a/"&gt; &lt;img alt="Python script to summarize and talk about a YouTube video" src="https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4" title="Python script to summarize and talk about a YouTube video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autoencoder"&gt; /u/autoencoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/danuker/81cb7136f6e45528550d4a5cde9d045f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6j9te/python_script_to_summarize_and_talk_about_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6j9te/python_script_to_summarize_and_talk_about_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T13:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n68l86</id>
    <title>vLLM vs MLIR - TTS Performance</title>
    <updated>2025-09-02T03:05:00+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l86/vllm_vs_mlir_tts_performance/"&gt; &lt;img alt="vLLM vs MLIR - TTS Performance" src="https://preview.redd.it/ytt747xm2omf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ee5db23ef9cbac3ea9a0f9c6c43c67d1c06c0c7" title="vLLM vs MLIR - TTS Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;vLLM leverages nvcc toolchain, MLIR (https://mlir.llvm.org/) transforms IR (Intermediate Representation) to PTX directly for nvidia. MLIR's IR could be transformed to other GPU/CPU instructions via dialects. From the TTS-1 Technical Report (https://arxiv.org/html/2507.21138v1) of Inworld.ai, &amp;quot;The inference stack leverages a graph compiler (MAX pipeline) for optimizations like kernel fusion and memory planning, complemented by custom kernels for critical operations like attention and matrix-vector multiplication, which were also developed in Mojo to outperform standard library implementations.&amp;quot; and &amp;quot;As a result of these combined optimizations, the streaming API delivers the first two seconds of synthesized audio on average 70% faster than a vanilla vLLM-based implementation&amp;quot; MAX/Mojo uses MLIR. This looks to be a purpose speicific optimization to squeeze more throughput from GPUs. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ytt747xm2omf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l86/vllm_vs_mlir_tts_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l86/vllm_vs_mlir_tts_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T03:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6f5q2</id>
    <title>Hi everyone, This is my first attempt at fine-tuning a LLAMA 3.1 8B model for roleplay.</title>
    <updated>2025-09-02T09:45:45+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still new to the whole fine-tuning process, so I'm not 100% sure what I did and is everything correctly works.&lt;/p&gt; &lt;p&gt;I'd really appreciate it if anyone could test it out and share their feedback what works, what doesn't, and where I can improve. Thanks in advance!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/samunder12/llama-3.1-8b-roleplay-jio-gguf"&gt;https://huggingface.co/samunder12/llama-3.1-8b-roleplay-jio-gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5q2/hi_everyone_this_is_my_first_attempt_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5q2/hi_everyone_this_is_my_first_attempt_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5q2/hi_everyone_this_is_my_first_attempt_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:45:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5rhbd</id>
    <title>I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use</title>
    <updated>2025-09-01T15:17:56+00:00</updated>
    <author>
      <name>/u/Avienir</name>
      <uri>https://old.reddit.com/user/Avienir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"&gt; &lt;img alt="I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use" src="https://external-preview.redd.it/cXB3bmh1bGZsa21mMfbflv5Di1j64vZv4v6FbqGgackbIUKWjlzVaYUu9HIx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da4116b03f8236881e7202c319fe08d1d76dec5" title="I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of overengineered and bloated AI libraries and needed something to prototype local RAG apps quickly so I decided to make my own library,&lt;br /&gt; Features:&lt;br /&gt; ‚û°Ô∏è Get to prototyping local RAG applications in seconds: uvx rocketrag prepare &amp;amp; uv rocketrag ask is all you need&lt;br /&gt; ‚û°Ô∏è CLI first interface, you can even visualize embeddings in your terminal&lt;br /&gt; ‚û°Ô∏è Native llama.cpp bindings - no Ollama bullshit&lt;br /&gt; ‚û°Ô∏è Ready to use minimalistic web app with chat, vectors visualization and browsing documents‚û°Ô∏è Minimal footprint: milvus-lite, llama.cpp, kreuzberg, simple html web app&lt;br /&gt; ‚û°Ô∏è Tiny but powerful - use any chucking method from chonkie, any LLM with .gguf provided and any embedding model from sentence-transformers&lt;br /&gt; ‚û°Ô∏è Easily extendible - implement your own document loaders, chunkers and BDs, contributions welcome!&lt;br /&gt; Link to repo: &lt;a href="https://github.com/TheLion-ai/RocketRAG"&gt;https://github.com/TheLion-ai/RocketRAG&lt;/a&gt;&lt;br /&gt; Let me know what you think. If anybody wants to collaborate and contribute DM me or just open a PR!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Avienir"&gt; /u/Avienir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tqnduvlflkmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T15:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5u32n</id>
    <title>AMD 6x7900xtx 24GB + 2xR9700 32GB VLLM QUESTIONS</title>
    <updated>2025-09-01T16:55:26+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5u32n/amd_6x7900xtx_24gb_2xr9700_32gb_vllm_questions/"&gt; &lt;img alt="AMD 6x7900xtx 24GB + 2xR9700 32GB VLLM QUESTIONS" src="https://preview.redd.it/txo8g9us0lmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a9855934c099881ce9400ed4488f4043ebbec7e" title="AMD 6x7900xtx 24GB + 2xR9700 32GB VLLM QUESTIONS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear reddit community, last two years from time to time our pc with one 7900xtx growed into this machine.&lt;/p&gt; &lt;p&gt;I am try to find solution to utilize it for 2-3 parallel queries at high speed for the qwen3-coder-flash model or for the quantized version of qwen3-235b-instruct.&lt;/p&gt; &lt;p&gt;I test different ways to launch VLLM with different cards, but it stay on Cuda graph (i also disabled with enforce_eager).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version: '3.8' services: vllm: pull_policy: always tty: true restart: unless-stopped ports: - 8000:8000 image: rocm/vllm-dev:nightly_main_20250817 shm_size: '128g' volumes: - /mnt/tb_disk/llm:/app/models devices: - /dev/kfd:/dev/kfd - /dev/dri:/dev/dri - /dev/mem:/dev/mem environment: - ROCM_VISIBLE_DEVICES=1,2,3,4,5,7,0,6 - HIP_VISIBLE_DEVICES=1,2,3,4,5,7,0,6 - VLLM_USE_V1=0 - VLLM_ATTENTION_BACKEND=ROCM_FLASH - ROCM_USE_FLASH_ATTN_V2_TRITON=True - VLLM_USE_TRITON_FLASH_ATTN=1 - VLLM_CUSTOM_OPS=all - NCCL_DEBUG=ERROR - PYTORCH_HIP_ALLOC_CONF=expandable_segments:True command: | sh -c ' vllm serve /app/models/models/vllm/Qwen3-Coder-30B-A3B-Instruct \ --served-model-name qwen3-coder-flash \ --max-model-len 131072 \ --gpu-memory-utilization 0.97 \ --tensor-parallel-size 4 \ --enable-auto-tool-choice \ --disable-log-requests \ --tool-call-parser qwen3_coder \ --enable-chunked-prefill \ --max-num-batched-tokens 4096 \ --max-num-seqs 8 ' volumes: {} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;This work ok for -tp 4, but for -tp 8 always stack.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;i know about llama-cpp, but it's very slow if we look at same utilization in vllm, maybe someone here have successful launch tensor parallelism in TGI?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interesting thing: R9700 does not loose speed inference in case when model distributed between two cards or one.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Feel free to ask any question about this machine.&lt;/p&gt; &lt;p&gt;also some GPTQ models work and some don't, maybe it's due to the quantization format,&lt;/p&gt; &lt;p&gt;Other helpful info: MB: MZ32-AR0 3200MT/s x8 32gb, 2x PSU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/txo8g9us0lmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5u32n/amd_6x7900xtx_24gb_2xr9700_32gb_vllm_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5u32n/amd_6x7900xtx_24gb_2xr9700_32gb_vllm_questions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T16:55:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5j783</id>
    <title>I built, pre-trained, and fine-tuned a small language model and it is truly open-source.</title>
    <updated>2025-09-01T08:26:34+00:00</updated>
    <author>
      <name>/u/itsnikity</name>
      <uri>https://old.reddit.com/user/itsnikity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"&gt; &lt;img alt="I built, pre-trained, and fine-tuned a small language model and it is truly open-source." src="https://preview.redd.it/cwyoa0f6kimf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=147ff4faaa129cc07cda0d4d53d824668e625f35" title="I built, pre-trained, and fine-tuned a small language model and it is truly open-source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, most of the time we all read open-source and in reality it is just open-weights. This time it is truly open-source.&lt;/p&gt; &lt;p&gt;Lille is a 130M parameter model trained from scratch and every part of the stack is open. Dataset, Model weights, Training code, Tokenizer, Optimizer, Evaluation framework...&lt;/p&gt; &lt;p&gt;Two versions are available: a base model trained on billions of tokens, and an instruction-tuned version fine-tuned on a curated instruction dataset.&lt;/p&gt; &lt;p&gt;Fun fact: it was trained locally on a single RTX 4070-TI.&lt;/p&gt; &lt;p&gt;I‚Äôd love feedback, suggestions, or contributions - whether it‚Äôs fine-tuning ideas, evaluation improvements, or even architectural tweaks.&lt;/p&gt; &lt;p&gt;Thanks! Check it out: &lt;a href="https://huggingface.co/Nikity/lille-130m-instruct"&gt;Lille 130M Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnikity"&gt; /u/itsnikity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwyoa0f6kimf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T08:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n63lz8</id>
    <title>Top small LLM as of September '25</title>
    <updated>2025-09-01T23:06:29+00:00</updated>
    <author>
      <name>/u/_-inside-_</name>
      <uri>https://old.reddit.com/user/_-inside-_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've been away for the last couple of months, and suddenly I don't seem to see references to new small models around here. Is there any novelty o the topic of small models since the releases of Qwen 3 and Gemma 3n? Something I could run with 4GB VRAM? thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_-inside-_"&gt; /u/_-inside-_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n63lz8/top_small_llm_as_of_september_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n63lz8/top_small_llm_as_of_september_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n63lz8/top_small_llm_as_of_september_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T23:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6hk90</id>
    <title>Image 'editing' app with Qwen Image Edit and an iOS client</title>
    <updated>2025-09-02T12:01:15+00:00</updated>
    <author>
      <name>/u/baliord</name>
      <uri>https://old.reddit.com/user/baliord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6hk90/image_editing_app_with_qwen_image_edit_and_an_ios/"&gt; &lt;img alt="Image 'editing' app with Qwen Image Edit and an iOS client" src="https://external-preview.redd.it/_phhztHOXP1EaSigwjpmdclnYlgiIY_nMfXGtHApDV0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9e4ac0aed522efe5c46173ca8aa2cff20f9af4" title="Image 'editing' app with Qwen Image Edit and an iOS client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I commented on a recent '&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n6862l/uncensored_image_editing_and_generation/"&gt;Uncensored Image Editing&lt;/a&gt;' post that I'd built something similar for fun with &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;Qwen Image Edit&lt;/a&gt;, and &lt;a href="/u/GreenGreasyGreasels"&gt;u/GreenGreasyGreasels&lt;/a&gt; called me out (politely) for not actually making at least the architecture public.&lt;/p&gt; &lt;p&gt;I figured I might as well put up the whole thing; it's not a lot of work, but I want to emphasize that it is NOT safe to run on a public endpoint.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cyberfox/image-edit"&gt;https://github.com/cyberfox/image-edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Set up an environment, &lt;code&gt;pip install -r server/requirements.txt&lt;/code&gt; and then&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uvicorn image-edit-server:app --host 0.0.0.0 --port 8000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll need a developer license and to have your iPhone in developer mode, to build and install the iOS client. I don't have an icon for it, so it'll just look janky. In fact I should have named it Janky Image Editor. ü§£&lt;/p&gt; &lt;p&gt;In the client, you enter your IP address and port in the settings, and it should basically work.&lt;/p&gt; &lt;p&gt;You can read the docs, the server source, and the Swift source. Again, it's janky, and I coded it really quick. Here's some images to whet the appetite.&lt;/p&gt; &lt;p&gt;I looked for an image with free usage, and &lt;a href="https://pixabay.com/photos/woman-young-casual-sitting-fence-8788000/"&gt;found this image&lt;/a&gt; to be useful for this purpose. Hopefully it is free as they say it is.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lae5tho1rqmf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f2f24a7fd0d6ac9dca43cea7a7c328cf5f3d51"&gt;https://preview.redd.it/lae5tho1rqmf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f2f24a7fd0d6ac9dca43cea7a7c328cf5f3d51&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vstq6rp7qqmf1.jpg?width=853&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9b1084616d8f973758773a13b03ada685e922a19"&gt;Before editing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ulxqbrnpqqmf1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3809aba7d482a0b7b2244b07b2e17173b4209b23"&gt;After Editing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope that's interesting to folks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baliord"&gt; /u/baliord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6hk90/image_editing_app_with_qwen_image_edit_and_an_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6hk90/image_editing_app_with_qwen_image_edit_and_an_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6hk90/image_editing_app_with_qwen_image_edit_and_an_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T12:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6h3rk</id>
    <title>rStar2-Agent</title>
    <updated>2025-09-02T11:37:39+00:00</updated>
    <author>
      <name>/u/thatusernsmeis</name>
      <uri>https://old.reddit.com/user/thatusernsmeis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This MS model was realeased some days ago and I haven‚Äôt see any posts talking about it on here but from the benchmarks looks promising for a 14B model. &lt;/p&gt; &lt;p&gt;Has anyone tried it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thatusernsmeis"&gt; /u/thatusernsmeis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2508.20722"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6h3rk/rstar2agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6h3rk/rstar2agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T11:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5w9yy</id>
    <title>I fine-tuned Llama 3.2 3B for transcript analysis and it outperformed bigger models with ease</title>
    <updated>2025-09-01T18:15:43+00:00</updated>
    <author>
      <name>/u/CartographerFun4221</name>
      <uri>https://old.reddit.com/user/CartographerFun4221</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5w9yy/i_finetuned_llama_32_3b_for_transcript_analysis/"&gt; &lt;img alt="I fine-tuned Llama 3.2 3B for transcript analysis and it outperformed bigger models with ease" src="https://b.thumbs.redditmedia.com/XI_Ubzun5wNt_4EOP4JZv-BnoPQ4ncNVDSzs6HtcQ9M.jpg" title="I fine-tuned Llama 3.2 3B for transcript analysis and it outperformed bigger models with ease" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently wrote a &lt;a href="https://github.com/bilawalriaz/lazy-notes"&gt;small local tool &lt;/a&gt;to transcribe my local audio notes to text using Whisper/Parakeet.&lt;br /&gt; I wanted to process the raw transcripts locally without needing OpenRouter so i tried Llama 3.2 3B and got surprisingly decent yet ultimately mediocre results. I decided to see how i could improve this using SFT.&lt;/p&gt; &lt;p&gt;I fine-tuned Llama 3.2 3B to clean and analyze raw dictation transcripts locally, outputting a structured JSON object (title, tags, entities, dates, actions).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data: 13 real voice memos ‚Üí teacher (Kimi K2) for gold JSON ‚Üí ~40k synthetic transcripts + gold. Keys are canonicalized to stabilize JSON supervision. &lt;a href="http://Chutes.ai"&gt;Chutes.ai&lt;/a&gt; was used, giving 5000 reqs/day.&lt;/li&gt; &lt;li&gt;Training: RTX 4090 24GB, ~4 hours, LoRA (r=128, alpha=128, dropout=0.05), max seq length of 2048 tokens, batch size 16, lr=5e-5, cosine scheduler, Unsloth. Could've done it without all this VRAM but would've taken slower (8 hours on my RTX 2070 Super 8GB).&lt;/li&gt; &lt;li&gt;Inference: merged to GGUF, quantized Q4_K_M using llama.cpp, runs locally via LM Studio.&lt;/li&gt; &lt;li&gt;Evals (100-sample sanity check, scored by GLM 4.5 FP8): &lt;strong&gt;overall score 5.35 (base 3B)&lt;/strong&gt; ‚Üí &lt;strong&gt;8.55 (fine-tuned)&lt;/strong&gt;. Completeness 4.12 ‚Üí 7.62, factual accuracy 5.24 ‚Üí 8.57.&lt;/li&gt; &lt;li&gt;Head-to-head (10 samples): specialized 3B averaged ~8.40 vs Hermes-70B 8.18, Mistral-Small-24B 7.90, Gemma-3-12B 7.76, Qwen3-14B 7.62. Teacher Kimi K2 ~8.82.&lt;/li&gt; &lt;li&gt;Why it works: task specialization + JSON canonicalization reduce output variance and help the model learn the exact structure and fields.&lt;/li&gt; &lt;li&gt;Lessons learned: important to train on completions only, synthetic datasets are okay for specialised fine-tunes, Llama is surprisingly easy to train&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yw48krzwllmf1.jpg?width=3600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=985ed3cbb09fcd77e470060dda382dadd6325c7f"&gt;https://preview.redd.it/yw48krzwllmf1.jpg?width=3600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=985ed3cbb09fcd77e470060dda382dadd6325c7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/exgsrqzwllmf1.jpg?width=6000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38c3ca44b377f4cee7808b2ccf6f1d57b0fb87e8"&gt;https://preview.redd.it/exgsrqzwllmf1.jpg?width=6000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38c3ca44b377f4cee7808b2ccf6f1d57b0fb87e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code, dataset pipeline, hyperparams, eval details, and a 4-bit GGUF download are in the post: &lt;a href="https://bilawal.net/post/finetuning-llama32-3b-for-transcripts/"&gt;https://bilawal.net/post/finetuning-llama32-3b-for-transcripts/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to discuss training setup, eval rubric, or deployment details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CartographerFun4221"&gt; /u/CartographerFun4221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bilawal.net/post/finetuning-llama32-3b-for-transcripts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5w9yy/i_finetuned_llama_32_3b_for_transcript_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5w9yy/i_finetuned_llama_32_3b_for_transcript_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T18:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6g0rt</id>
    <title>about 300 pages: Global Fix Map for local LLMs (upgrade from the Problem Map) + looking for your feedback</title>
    <updated>2025-09-02T10:38:07+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6g0rt/about_300_pages_global_fix_map_for_local_llms/"&gt; &lt;img alt="about 300 pages: Global Fix Map for local LLMs (upgrade from the Problem Map) + looking for your feedback" src="https://preview.redd.it/9lvmz0lkcqmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dbe81f1db581025cd1aa4f66b0092d904609484" title="about 300 pages: Global Fix Map for local LLMs (upgrade from the Problem Map) + looking for your feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;thanks for the support on my Problem Map earlier. &lt;/p&gt; &lt;p&gt;quick update: &lt;/p&gt; &lt;p&gt;i‚Äôve turned it into a Global Fix Map. it‚Äôs a one-stop index that routes real bugs to the right repair page, with copy-paste fixes and acceptance targets so you can verify the change, not just hope it worked. if you run local models (llama.cpp, Ollama, textgen-webui, TGI, koboldcpp, gpt4all, exllama‚Ä¶), jump to the LocalDeploy_Inference section inside. &lt;/p&gt; &lt;p&gt;(70 days 800 stars repo) &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;what‚Äôs inside&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;about 300 pages across stacks: Providers &amp;amp; Agents, Data &amp;amp; Retrieval, Input &amp;amp; Parsing, Reasoning &amp;amp; Memory, Eval &amp;amp; Governance&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;each page ends with acceptance targets so fixes are testable: ŒîS(question, context) ‚â§ 0.45, coverage ‚â• 0.70, Œª stays convergent on 3 paraphrases&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;local folks can start at LocalDeploy_Inference, then branch to VectorDBs_and_Stores, Embeddings, Safety/Prompt-Integrity, and OpsDeploy&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;what we think is happening ‚Üí what‚Äôs really happening (local edition)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;‚Äúcontext got weird after 8k, rope is broken‚Äù ‚Üí tokenizer or chat-template mismatch, not rope. check template, system/user roles, and casing rules. often maps to No.8 Retrieval Traceability + Prompt Assembly.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚Äúmodel changed after quant, bad weights?‚Äù ‚Üí quantization variance + loader flags. gguf/awq/gptq differ in samplers and kv-cache behavior. pin flags, compare against fp16 baseline. often OpsDeploy + Eval issue, not the model.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚Äújson mode keeps breaking mid stream‚Äù ‚Üí schema or tool contract drift. enforce cite-then-explain or tool-first, clamp max tokens for tool calls, and add a fail-fast. maps to Safety / Prompt Integrity.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚Äúsame prompt, different answer every run‚Äù ‚Üí pre-deploy collapse on first warmup, or sampler drift. verify secrets, env, and warm-up fences, then lock temperature/top-p. relates to No.16 Pre-Deploy Collapse.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚Äúsimilarity high but meaning off‚Äù ‚Üí metric mismatch / normalization in the store. rebuild with the right distance and scaling, then re-eval. see Embeddings + VectorDBs_and_Stores.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚Äúserver is fine, but long outputs cut off‚Äù ‚Üí streaming body cutoffs / proxy timeouts. move to chunked streaming, raise body size, add retry with idempotency on the writer. shows up under Cloud/Serverless + OpsDeploy.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚Äúhybrid underperforms single retriever‚Äù ‚Üí query-parsing split and missing reranker weights. either fix the weights or move reranking out of chain. that‚Äôs Retrieval + RAG_VectorDB territory.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;what i‚Äôm asking from LocalLLaMA&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;tell me which checklists you want first: llama.cpp flags, Ollama model cards, textgen-webui prompt templates, TGI json/tool notes, kv-cache + sampler guardrails, rope-scaling caveats, gguf vs awq vs gptq comparison&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;point to any page that feels unclear and i‚Äôll rewrite it into a minimal recipe with before/after checks&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;got a reproducible? drop a short trace: loader + version, model + quant, key flags, prompt template, smallest failing prompt, expected vs observed, and a one-line log snippet&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;goal stays simple: make local pipelines stable without changing your infra. if you missed the original Problem Map, this is the upgraded version you can jump into directly.&lt;/p&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;Closing note&lt;/p&gt; &lt;p&gt;i also want to thank this community , a lot of you gave me encouragement and shared practical tips when i first posted the Problem Map. i kept notes on that feedback. this new Global Fix Map is my way of turning those notes into something concrete.&lt;/p&gt; &lt;p&gt;i‚Äôd love more input this round: which parts are most urgent for you? do you want to see checklists first, or code recipes, or more worked examples? if there‚Äôs a tool or workflow you want prioritized, let me know. i‚Äôll keep adding based on what the community actually needs. ü´°&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9lvmz0lkcqmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6g0rt/about_300_pages_global_fix_map_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6g0rt/about_300_pages_global_fix_map_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T10:38:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6dduv</id>
    <title>silly-v0.2 - an RL-heavy, chat-style roleplay model</title>
    <updated>2025-09-02T07:48:17+00:00</updated>
    <author>
      <name>/u/Abject-Huckleberry13</name>
      <uri>https://old.reddit.com/user/Abject-Huckleberry13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6dduv/sillyv02_an_rlheavy_chatstyle_roleplay_model/"&gt; &lt;img alt="silly-v0.2 - an RL-heavy, chat-style roleplay model" src="https://external-preview.redd.it/XQq2vPgZp4sFzlDHMJKDpZat86LD2LJxT0U4e9ahf3s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7645db8dc5720cd92c94af1c490139178868579" title="silly-v0.2 - an RL-heavy, chat-style roleplay model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;has a unique tone. &amp;quot;pretty good for a 12b&amp;quot; say most people&lt;/p&gt; &lt;p&gt;This is mostly a proof-of-concept, showcasing that POLAR reward models can be very useful for &amp;quot;out of distribution&amp;quot; tasks like roleplaying. If you're working on your own roleplay finetunes, please consider using POLAR!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject-Huckleberry13"&gt; /u/Abject-Huckleberry13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/wave-on-discord/silly-v0.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6dduv/sillyv02_an_rlheavy_chatstyle_roleplay_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6dduv/sillyv02_an_rlheavy_chatstyle_roleplay_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T07:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6699f</id>
    <title>GPU credits for students, tinkerers, solopreneurs</title>
    <updated>2025-09-02T01:10:20+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We recognize that GPU grants are often biased. Funded startups, prominent researchers, or other successful individuals are swimming in credits. At the same time, it can be challenging to obtain a GPU if you're just getting started and when you need it the most. We're working to address this issue through our GPU credits program, which is &lt;strong&gt;available to everyone&lt;/strong&gt; (also, we're a poor early-stage startup, so we can't offer generous sponsorship programs).&lt;/p&gt; &lt;p&gt;- Get from $100 to $1000 for your project. Note that our prices are one-quarter of those of Hyperscalers, and we offer consumer GPUs like RTX 4090 / 5090 / Pro 6000 for rent, so you really get $500-$10,000 of GPU value.&lt;/p&gt; &lt;p&gt;- We pool applications and make decisions every two weeks. We've allocated a $3,000 monthly budget for this program. We will increase it if it proves successful.&lt;/p&gt; &lt;p&gt;- We're looking for projects that address pressing community problems. It doesn't have to be a significant issue. If you're working on one, please don't forget to refer to the Reddit thread that describes the problem. It helps us refine the product to meet community needs.&lt;/p&gt; &lt;p&gt;- We'd like to ask you to mention us in your social media post, article, or blog. Having an active social media profile, published articles, or blog posts is a plus. Ultimately, we're a business and aim to promote our product.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/ai-grant"&gt;https://www.cloudrift.ai/ai-grant&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6699f/gpu_credits_for_students_tinkerers_solopreneurs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6699f/gpu_credits_for_students_tinkerers_solopreneurs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6699f/gpu_credits_for_students_tinkerers_solopreneurs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T01:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6827e</id>
    <title>After deepseekv3 I feel like other MoE architectures are old or outdated. Why did Qwen chose a simple MoE architecture with softmax routing and aux loss for their Qwen3 models when there‚Äôs been better architectures for a while?</title>
    <updated>2025-09-02T02:38:32+00:00</updated>
    <author>
      <name>/u/Euphoric_Ad9500</name>
      <uri>https://old.reddit.com/user/Euphoric_Ad9500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseekv3, R1, and deepseekv3.1 use sigmoid based routing with aux free bias gating and shared experts whereas Qwen3 MoE models use standard soft max routing with aux loss balancing. The Deepseekv3 architecture is better because it applies a bias to the raw affinity score for balancing. Qwen3 uses aux loss which can compete with other rewards. There are a couple other features that make the Deepseekv3 architecture better. This honestly makes me wary about even using Qwen3 MoE models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Ad9500"&gt; /u/Euphoric_Ad9500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6827e/after_deepseekv3_i_feel_like_other_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6827e/after_deepseekv3_i_feel_like_other_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6827e/after_deepseekv3_i_feel_like_other_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T02:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6cvgt</id>
    <title>I built a free Structured Prompt Builder (with local library + Gemini optimization) because other tools are bloated &amp; paywalled</title>
    <updated>2025-09-02T07:14:42+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I want to share something I‚Äôve been building out of frustration with the current ‚Äúprompt builder‚Äù tools floating around. Most of them are either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Locked behind paywalls&lt;/li&gt; &lt;li&gt;Bloated with features I don‚Äôt need&lt;/li&gt; &lt;li&gt;Or just plain confusing to use&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I made my own: &lt;strong&gt;Structured Prompt Builder&lt;/strong&gt;. It‚Äôs &lt;strong&gt;100% free, runs entirely in your browser, no sign‚Äëup, no backend, no tracking&lt;/strong&gt;. Everything is stored locally (in &lt;code&gt;localStorage&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;Link :: &lt;a href="https://structured-prompt-builder.vercel.app/"&gt;structured-prompt-builder.vercel.app&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why I built it&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I needed a clean, lightweight way to design prompts without ‚ÄúAI SaaS subscriptions‚Äù.&lt;/li&gt; &lt;li&gt;I wanted to save prompts, reuse them, and share them easily.&lt;/li&gt; &lt;li&gt;I wanted &lt;strong&gt;Gemini&lt;/strong&gt; to &lt;em&gt;polish&lt;/em&gt; my prompts (fix spelling/grammar/clarity) while keeping the exact structure intact ‚Äî not generate random extra stuff.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Structured fields&lt;/strong&gt; ‚Üí Role, Task, Audience, Style, Tone&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add sections&lt;/strong&gt; ‚Üí Constraints, Steps, Inputs (name:value), Few‚Äëshot examples&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Preview instantly&lt;/strong&gt; in &lt;strong&gt;Markdown, JSON, YAML&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Copy / Download&lt;/strong&gt; any format in one click&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Import from JSON&lt;/strong&gt; to keep your workflow portable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adjust parameters&lt;/strong&gt; ‚Üí Temperature, Top‚Äëp, Max Tokens, Presence &amp;amp; Frequency penalties&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Library&lt;/strong&gt; ‚Üí Save, Load, Duplicate, Delete prompts right in the browser&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini Optimizer&lt;/strong&gt; ‚Üí Paste your Gemini API key, hit ‚ÄúGenerate with Gemini,‚Äù and it will: &lt;ul&gt; &lt;li&gt;Clean up your text&lt;/li&gt; &lt;li&gt;Preserve the schema/keys&lt;/li&gt; &lt;li&gt;Return only the format you asked for (Markdown/JSON/YAML)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What makes it different&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Free. No hidden tiers.&lt;/li&gt; &lt;li&gt;Offline‚Äëfirst. Runs in the browser, nothing sent to my server.&lt;/li&gt; &lt;li&gt;Open &amp;amp; hackable (MIT License).&lt;/li&gt; &lt;li&gt;Built for &lt;em&gt;practical&lt;/em&gt; prompt design, not flashy dashboards.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Sponsor / Support&lt;/h1&gt; &lt;p&gt;If you like this project and want it to keep growing (template gallery, cloud sync, maybe integrations), I‚Äôd really appreciate sponsorships or any kind of support. Even small help means I can keep it 100% free.&lt;/p&gt; &lt;p&gt;üëâ Repo: &lt;a href="https://github.com/Siddhesh2377/structured-prompt-builder"&gt;github.com/Siddhesh2377/structured-prompt-builder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading, and let me know if you try it out or have ideas for improvements!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6cvgt/i_built_a_free_structured_prompt_builder_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6cvgt/i_built_a_free_structured_prompt_builder_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6cvgt/i_built_a_free_structured_prompt_builder_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T07:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5zed0</id>
    <title>I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large</title>
    <updated>2025-09-01T20:13:23+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"&gt; &lt;img alt="I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large" src="https://external-preview.redd.it/ywyWexAkeoEnV3YXP8YcOUkQeZuDP2-5umUBtdqKkZ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebad8a84758c0785d9bbd8dcbf0a28e687394e9e" title="I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks from LocalLLama sub! I am really thankful for amazing people in this sub for sharing useful things which helped me to learn lots of things about pretraing , post training and evaluation etc for your context I don't have professional ML background!&lt;/p&gt; &lt;p&gt;Today I am super excited to share that I pretrained and post trained 150M parameter model from scratch which outperforms Google BERT model and I also built embedding model which works on par with Jina-embedings-v2-base model in MTEB benchmarks &lt;/p&gt; &lt;p&gt;In this article I shared how I did this model along with links to weights of model&lt;br /&gt; thanks again&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@harishhacker3010/pretraining-a-llm-with-less-than-50-budget-which-outperforms-google-bert-dbe541b7b14b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T20:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n68afq</id>
    <title>Why are all AI "Success" posts terrible?</title>
    <updated>2025-09-02T02:49:54+00:00</updated>
    <author>
      <name>/u/Bimbam_tm</name>
      <uri>https://old.reddit.com/user/Bimbam_tm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Wow look at this!&amp;quot; someone cries, and includes a screenshot/gif from a single-line AI prompt magically producing a working product.&lt;/p&gt; &lt;p&gt;Great, and completely unsurprising given that one-line prompts work exactly like horoscopes - so vague they can't help but satisfy whatever slop gets generated. But whatever, as long as it looks gifable right?&lt;/p&gt; &lt;p&gt;&amp;quot;Build me a todo app that looks nice!&amp;quot;&lt;br /&gt; Congratulations, you just wrote the AI equivalent of &amp;quot;you will face challenges this week.&amp;quot; The AI spits out literally anything 'todo adjacent' and you're amazed because technically it worked. Just like horoscopes, it's response is written so broadly that the reader finds it somehow fits their expectations..&lt;/p&gt; &lt;p&gt;A real horoscope would say &amp;quot;On Tuesday at 3:47 PM, you will receive a text from someone whose name starts with J about a blue object.&amp;quot;&lt;/p&gt; &lt;p&gt;With that in mind, how about someone show me a real workflow:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your original concept art/design docs/sketches&lt;/li&gt; &lt;li&gt;How close you actually got to achieve your original concept/idea&lt;/li&gt; &lt;li&gt;How many iterations it took&lt;/li&gt; &lt;li&gt;What didn't work&lt;/li&gt; &lt;li&gt;The actual prompts you used (all of them)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unless that AI output was almost EXACTLY what you had in mind from prompt #1 (which seems highly unlikely), all your &amp;quot;amazing&amp;quot; result proves was your prompt was horoscope-level vague, and you're apparently ok with mediocrity .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bimbam_tm"&gt; /u/Bimbam_tm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T02:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6fta4</id>
    <title>ETH Zurich Open LLM "Apertus" has been released</title>
    <updated>2025-09-02T10:25:45+00:00</updated>
    <author>
      <name>/u/kisamoto</name>
      <uri>https://old.reddit.com/user/kisamoto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"&gt; &lt;img alt="ETH Zurich Open LLM &amp;quot;Apertus&amp;quot; has been released" src="https://external-preview.redd.it/3xCYbgdmDkf0KukpAo-RYTjRTShJKNSz9uOuaVJW_jI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5da84c50675f066ea42c6cb75049480ff32b8ed5" title="ETH Zurich Open LLM &amp;quot;Apertus&amp;quot; has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kisamoto"&gt; /u/kisamoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/swiss-ai/apertus-llm-68b699e65415c231ace3b059"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T10:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ewmu</id>
    <title>Apertus: a fully open, transparent, multilingual language model</title>
    <updated>2025-09-02T09:29:24+00:00</updated>
    <author>
      <name>/u/Stock-Variation-2237</name>
      <uri>https://old.reddit.com/user/Stock-Variation-2237</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"&gt; &lt;img alt="Apertus: a fully open, transparent, multilingual language model" src="https://external-preview.redd.it/bTft7kJCnEeOJxWtFZSRa954qWiZB1xs4iZXNvShGsI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afccde1fc69d131b9cf94781d34b916dcaf90c23" title="Apertus: a fully open, transparent, multilingual language model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Variation-2237"&gt; /u/Stock-Variation-2237 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://actu.epfl.ch/news/apertus-a-fully-open-transparent-multilingual-lang/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6f5xl</id>
    <title>I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more.</title>
    <updated>2025-09-02T09:46:05+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt; &lt;img alt="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." src="https://b.thumbs.redditmedia.com/UxxcnGYx3ztd3aYOh5G5hmtgSX12gsIZtQoWGJsTZJs.jpg" title="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working hard on a big update for my open-source project, MAESTRO, and I'm excited to share v0.1.5-alpha with you all. MAESTRO is an autonomous research agent that turns any question into a fully-cited report.&lt;/p&gt; &lt;p&gt;A huge focus of this release was improving performance and compatibility with local models. I've refined the core agent workflows and prompts to make sure it works well with most reasonably intelligent locally hosted models.&lt;/p&gt; &lt;p&gt;I also launched a completely new documentation site to help users setup and start using MAESTRO. The best part is the new &lt;a href="https://murtaza-nasir.github.io/maestro/example-reports/"&gt;Example Reports Section&lt;/a&gt; that shows many reports generated with Local LLMs.&lt;/p&gt; &lt;p&gt;I've done extensive testing and shared the resulting reports so you can see what it's capable of. There are examples from a bunch of self-hosted models, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Large Models:&lt;/strong&gt; Qwen 2.5 72B, GPT-OSS 120B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Medium Models:&lt;/strong&gt; Qwen 3 32B, Gemma 3 27B, GPT-OSS 20B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a great way to see how different models handle complex topics and various writing styles before you commit to running them. I've also included performance notes on things like KV cache usage during these runs.&lt;/p&gt; &lt;p&gt;Under the hood, I improved some UI features and added parallel processing for more operations, so it‚Äôs a little faster and more responsive.&lt;/p&gt; &lt;p&gt;If you're interested in AI assisted research or just want to see what's possible with the latest open models, I'd love for you to check it out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/murtaza-nasir/maestro"&gt;&lt;strong&gt;GitHub Release&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://murtaza-nasir.github.io/maestro"&gt;&lt;strong&gt;New Docs Site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful. Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6f5xl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6eimy</id>
    <title>New Open LLM from Switzerland "Apertus", 40%+ training data is non English</title>
    <updated>2025-09-02T09:03:53+00:00</updated>
    <author>
      <name>/u/EnnioEvo</name>
      <uri>https://old.reddit.com/user/EnnioEvo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html"&gt;https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnnioEvo"&gt; /u/EnnioEvo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6epwv</id>
    <title>My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench üòÖ</title>
    <updated>2025-09-02T09:17:01+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt; &lt;img alt="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench üòÖ" src="https://b.thumbs.redditmedia.com/pYjWCv-fYbHaF7KTK2GkiEx7BRL3zHSwgEFLLf7Zn0M.jpg" title="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench üòÖ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Hitting a million brick walls with multi-turn RL training isn't fun, so I thought I would try something new to climb Stanford's leaderboard for now! So this weekend I was just tinkering with multi-agent systems and... somehow ended up beating Claude Code on Stanford's TerminalBench leaderboard (#12)! Genuinely didn't expect this - started as a fun experiment and ended up with something that works surprisingly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a multi-agent AI system with three specialised agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;: The brain - never touches code, just delegates and coordinates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explorer agents&lt;/strong&gt;: Read &amp;amp; run only investigators that gather intel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coder agents&lt;/strong&gt;: The ones who actually implement stuff&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Created a &amp;quot;Context Store&amp;quot; which can be thought of as persistent memory that lets agents share their discoveries.&lt;/p&gt; &lt;p&gt;Tested on TerminalBench with both Claude Sonnet-4 and Qwen3-Coder-480B. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Orchestrator + Sonnet-4: &lt;strong&gt;36.0% success rate&lt;/strong&gt; (#12 on leaderboard, ahead of Claude Code!)&lt;/li&gt; &lt;li&gt;Orchestrator + Qwen-3-Coder: 19.25% success rate&lt;/li&gt; &lt;li&gt;Sonnet-4 consumed 93.2M tokens vs Qwen's 14.7M tokens to compete all tasks!&lt;/li&gt; &lt;li&gt;The orchestrator's explicit task delegation + intelligent context sharing between subagents seems to be the secret sauce&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;(Kind of) Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The orchestrator can't read/write code directly - this forces proper delegation patterns and strategic planning&lt;/li&gt; &lt;li&gt;Each agent gets precise instructions about what &amp;quot;knowledge artifacts&amp;quot; to return, these artifacts are then stored, and can be provided to future subagents upon launch.&lt;/li&gt; &lt;li&gt;Adaptive trust calibration: simple tasks = high autonomy, complex tasks = iterative decomposition&lt;/li&gt; &lt;li&gt;Each agent has its own set of tools it can use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repo has all the code, system messages, and way more technical details if you're interested!&lt;/p&gt; &lt;p&gt;‚≠êÔ∏è &lt;a href="https://github.com/Danau5tin/multi-agent-coding-system"&gt;&lt;strong&gt;Orchestrator repo - all code open sourced!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent &lt;a href="https://www.tbench.ai/"&gt;TerminalBench&lt;/a&gt; benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6epwv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
