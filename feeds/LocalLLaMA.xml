<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-13T17:20:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r3rhru</id>
    <title>Nvidia Tesla P40 + RTX 5060 TI 16GB (FRANKENSTEIN IS ALIVE!)</title>
    <updated>2026-02-13T15:03:50+00:00</updated>
    <author>
      <name>/u/iampoorandsad</name>
      <uri>https://old.reddit.com/user/iampoorandsad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3rhru/nvidia_tesla_p40_rtx_5060_ti_16gb_frankenstein_is/"&gt; &lt;img alt="Nvidia Tesla P40 + RTX 5060 TI 16GB (FRANKENSTEIN IS ALIVE!)" src="https://preview.redd.it/h6jt10wpy9jg1.png?width=140&amp;amp;height=106&amp;amp;auto=webp&amp;amp;s=3ee1aadc75c4fb5dc28f9b30a405b6c04ff2fb6c" title="Nvidia Tesla P40 + RTX 5060 TI 16GB (FRANKENSTEIN IS ALIVE!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h6jt10wpy9jg1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3e14c19da7d95637c18e559583d1b55573ae960a"&gt;nvidia-smi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wanted to share how I finally made it to work (no testing was performed so far but drivers and nvidia-smi recognizes them together, finally).&lt;/p&gt; &lt;p&gt;I am running this setup on a Lenovo P920, Dual Xeon 8168, 256gb ram. I believe I could still squeeze another 5060 16gb there.&lt;/p&gt; &lt;p&gt;I struggled to find a driver that would work with both of the 5060 and P40 and gave up after trying many possible scenarios. Until yesterday when I saw a post where the user said he tricked the driver to believe it was a Quadro and began digging again, after all, the quadros of this generation are still supported by Nvidia. &lt;/p&gt; &lt;p&gt;So, it works! The driver that I used was: 581.57-desktop-win10-win11-64bit-international-nsd-dch-whql.exe and I basically just followed the tutorial here: &lt;a href="https://linustechtips.com/topic/1496913-can-i-enable-wddm-on-a-tesla-p40/"&gt;Can I enable WDDM on a tesla P40? - Graphics Cards - Linus Tech Tips&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yes, it is on windows. I will try a similar approach on Linux later on, but for now, I am happy it worked. &lt;/p&gt; &lt;p&gt;Happy Friday the 13th!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iampoorandsad"&gt; /u/iampoorandsad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3rhru/nvidia_tesla_p40_rtx_5060_ti_16gb_frankenstein_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3rhru/nvidia_tesla_p40_rtx_5060_ti_16gb_frankenstein_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3rhru/nvidia_tesla_p40_rtx_5060_ti_16gb_frankenstein_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T15:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3qkl7</id>
    <title>Minimax M2.5!</title>
    <updated>2026-02-13T14:27:28+00:00</updated>
    <author>
      <name>/u/BroQuant</name>
      <uri>https://old.reddit.com/user/BroQuant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BroQuant"&gt; /u/BroQuant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qkl7/minimax_m25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qkl7/minimax_m25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qkl7/minimax_m25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:27:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3gjx5</id>
    <title>ZwZ 8B/7B/4B</title>
    <updated>2026-02-13T05:17:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt; &lt;img alt="ZwZ 8B/7B/4B" src="https://preview.redd.it/0qvadyln47jg1.png?width=140&amp;amp;height=72&amp;amp;auto=webp&amp;amp;s=05efe6fee87739b20abf6a144df17c59c6612f79" title="ZwZ 8B/7B/4B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#model-summary"&gt;&lt;/a&gt;Model Summary&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ZwZ-8B&lt;/strong&gt; is a fine-grained multimodal perception model built upon &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B"&gt;Qwen3-VL-8B&lt;/a&gt;. It is trained using &lt;strong&gt;Region-to-Image Distillation (R2I)&lt;/strong&gt; combined with reinforcement learning, enabling superior fine-grained visual understanding in a single forward pass ‚Äî no inference-time zooming or tool calling required.&lt;/p&gt; &lt;p&gt;ZwZ-8B achieves state-of-the-art performance on fine-grained perception benchmarks among open-source models of comparable size, while also demonstrating strong out-of-distribution generalization on visual reasoning, GUI agent, and AIGC detection tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0qvadyln47jg1.png?width=3461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b9d12949205d2c9015be9a120643d5298548e6b"&gt;https://preview.redd.it/0qvadyln47jg1.png?width=3461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b9d12949205d2c9015be9a120643d5298548e6b&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#key-features"&gt;&lt;/a&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚ö° Single-Pass Efficiency&lt;/strong&gt;: Achieves fine-grained perception in one forward pass, eliminating inference-time tool-calling overhead&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üéØ Superior Accuracy&lt;/strong&gt;: State-of-the-art on perception benchmarks among open-source models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üìà Broad Improvements&lt;/strong&gt;: Enhances not only perception benchmarks but also out-of-distribution generalization on visual reasoning, GUI agent, and AIGC detection&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#how-it-works"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;p&gt;Traditional &amp;quot;Thinking-with-Images&amp;quot; methods zoom into regions of interest during inference, incurring high latency from repeated tool calls and visual re-encoding. &lt;strong&gt;ZwZ&lt;/strong&gt; transforms zooming from an inference-time tool into a training-time primitive:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Zoom in&lt;/strong&gt; to micro-cropped regions and let strong teacher models (Qwen3-VL-235B, GLM-4.5V) generate high-quality VQA data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distill&lt;/strong&gt; this region-grounded supervision back to the full image with explicit bounding-box overlays&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reinforce&lt;/strong&gt; via RL training to enable single-glance fine-grained perception without tool use&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B"&gt;https://huggingface.co/inclusionAI/ZwZ-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-7B"&gt;https://huggingface.co/inclusionAI/ZwZ-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-4B"&gt;https://huggingface.co/inclusionAI/ZwZ-4B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T05:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3toe1</id>
    <title>Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2</title>
    <updated>2026-02-13T16:25:08+00:00</updated>
    <author>
      <name>/u/Rascazzione</name>
      <uri>https://old.reddit.com/user/Rascazzione</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt; &lt;img alt="Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2" src="https://preview.redd.it/ps0fnwi7fajg1.png?width=140&amp;amp;height=38&amp;amp;auto=webp&amp;amp;s=e14c464f298d56218853942eb3cb6fdf095ae035" title="Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ps0fnwi7fajg1.png?width=1462&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1209b5ed071f67d465b5ab243fcbc309a676c17"&gt;Coding Index 13/02/2026 Artificial Analisys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fepkt4hffajg1.png?width=1468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c457992a63fd80a590b2c3296b1ce95843c7f8f8"&gt;General Index Intelligence 13/02/2026 Artificial Analisys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seems Minimax-M2.5 is on par with GLM-4.7 and DeepSeek-3.2, let's see if the Agent capabilities makes differences. &lt;/p&gt; &lt;p&gt;Stats from &lt;a href="https://artificialanalysis.ai/"&gt;https://artificialanalysis.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rascazzione"&gt; /u/Rascazzione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ukr7</id>
    <title>MiniMax-M2.5 MLX Q3/Q4 uploaded</title>
    <updated>2026-02-13T16:58:56+00:00</updated>
    <author>
      <name>/u/ToastFetish</name>
      <uri>https://old.reddit.com/user/ToastFetish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The weights for MiniMax just dropped M2.5 today (229B MoE, 10B active params, 80.2% SWE-Bench Verified) and I quantized it to MLX within an hour of release. First time doing this!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My quants:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 4-bit MLX - &lt;a href="https://huggingface.co/ahoybrotherbear/MiniMax-M2.5-4bit-MLX"&gt;https://huggingface.co/ahoybrotherbear/MiniMax-M2.5-4bit-MLX&lt;/a&gt; &lt;em&gt;(~129GB, needs 256GB+ RAM variant)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- 3-bit MLX - &lt;a href="https://huggingface.co/ahoybrotherbear/MiniMax-M2.5-3bit-MLX"&gt;https://huggingface.co/ahoybrotherbear/MiniMax-M2.5-3bit-MLX&lt;/a&gt; &lt;em&gt;(~86GB, would run tight on 96GB machines)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other MLX quants out there:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- mlx-community/MiniMax-M2.5-4bit - &lt;a href="https://huggingface.co/mlx-community/MiniMax-M2.5-4bit"&gt;https://huggingface.co/mlx-community/MiniMax-M2.5-4bit&lt;/a&gt; (the bot got there a few minutes before me lol)&lt;/p&gt; &lt;p&gt;- inferencerlabs 6.5-bit and 9-bit - &lt;a href="https://huggingface.co/inferencerlabs/MiniMax-M2.5-MLX-9bit"&gt;https://huggingface.co/inferencerlabs/MiniMax-M2.5-MLX-9bit&lt;/a&gt; (higher quality quants using modified MLX)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance on M3 Ultra 512GB:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- ~53 tokens/sec generation (4-bit)&lt;/p&gt; &lt;p&gt;- ~54 tokens/sec generation (3-bit)&lt;/p&gt; &lt;p&gt;- ~128GB peak memory (4-bit), ~100GB peak memory (3-bit)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quality note on the 3-bit:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;inferencerlabs' testing shows significant quality degradation below 4 bits for this model (43% token accuracy at q3.5 vs 91%+ at q4.5). I tested my 3-bit on coding and reasoning tasks on a few tasks and it produced coherent, correct output, but it's definitely not as sharp as 4-bit. Think of it as the smallest viable quant for people who can't fit the 4-bit version. 2-bit was completely unusable (infinite repetition loops).&lt;/p&gt; &lt;p&gt;Converted with mlx-lm v0.30.7. Happy to answer questions if anyone else wants to try running this locally - cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ToastFetish"&gt; /u/ToastFetish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ukr7/minimaxm25_mlx_q3q4_uploaded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ukr7/minimaxm25_mlx_q3q4_uploaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ukr7/minimaxm25_mlx_q3q4_uploaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:58:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2xotu</id>
    <title>Minimax M2.5 Officially Out</title>
    <updated>2026-02-12T16:17:13+00:00</updated>
    <author>
      <name>/u/Which_Slice1600</name>
      <uri>https://old.reddit.com/user/Which_Slice1600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt; &lt;img alt="Minimax M2.5 Officially Out" src="https://preview.redd.it/75rjx62d93jg1.png?width=140&amp;amp;height=67&amp;amp;auto=webp&amp;amp;s=bc1913b92a211d48c5d2979574442a87148c17cf" title="Minimax M2.5 Officially Out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only official webpages released now. But the bench looks very promising:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-Bench Verified 80.2%&lt;/li&gt; &lt;li&gt;Multi-SWE-Bench 51.3%&lt;/li&gt; &lt;li&gt;BrowseComp 76.3%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Edit: replaced with the en page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.minimax.io/news/minimax-m25"&gt;https://www.minimax.io/news/minimax-m25&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Which_Slice1600"&gt; /u/Which_Slice1600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r2xotu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3toe5</id>
    <title>MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000</title>
    <updated>2026-02-13T16:25:08+00:00</updated>
    <author>
      <name>/u/cyysky</name>
      <uri>https://old.reddit.com/user/cyysky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt; &lt;img alt="MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000" src="https://preview.redd.it/o66j8wb57ajg1.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=470df23fa8a32ba9ccb885ddda7d09d8607191ca" title="MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000 &lt;/p&gt; &lt;p&gt;Hosting it is easier then I thought, it just reuse the same script for M2.1.&lt;br /&gt; Time to do the vibe coding test! &lt;/p&gt; &lt;p&gt;Generation: 70 tokens-per-sec and 122 tokens-per-sec for two conneciton&lt;br /&gt; Peak Memory: 728GB&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o66j8wb57ajg1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddf90e73b3792510afd31f58604a8ccd0ab18246"&gt;https://preview.redd.it/o66j8wb57ajg1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddf90e73b3792510afd31f58604a8ccd0ab18246&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/99vp2ub57ajg1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=40fe8e0b643735c6fc10b5d6e47bb5fa279b45f2"&gt;https://preview.redd.it/99vp2ub57ajg1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=40fe8e0b643735c6fc10b5d6e47bb5fa279b45f2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyysky"&gt; /u/cyysky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3jadj</id>
    <title>Improving LLM's coding ability through a new edit format</title>
    <updated>2026-02-13T07:53:36+00:00</updated>
    <author>
      <name>/u/Mushoz</name>
      <uri>https://old.reddit.com/user/Mushoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"&gt; &lt;img alt="Improving LLM's coding ability through a new edit format" src="https://external-preview.redd.it/_HtanEVWgmWOk8SpjQcvTfNBYkpegEjBayvVrK7UD5E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e44168f7cbe68411e3b5b140f2f465f516bb44d0" title="Improving LLM's coding ability through a new edit format" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mushoz"&gt; /u/Mushoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.can.ac/2026/02/12/the-harness-problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T07:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3r3bt</id>
    <title>Make a SVG of a Pelican riding a bicycle - Small MoE edition.</title>
    <updated>2026-02-13T14:48:21+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"&gt; &lt;img alt="Make a SVG of a Pelican riding a bicycle - Small MoE edition." src="https://preview.redd.it/hrah141zm9jg1.png?width=140&amp;amp;height=65&amp;amp;auto=webp&amp;amp;s=1be312b3eb2454d314cc7c1a60371b3afb3f0f8d" title="Make a SVG of a Pelican riding a bicycle - Small MoE edition." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r3r3bt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:48:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3q1h7</id>
    <title>https://huggingface.co/MiniMaxAI/MiniMax-M2.5/tree/main</title>
    <updated>2026-02-13T14:05:44+00:00</updated>
    <author>
      <name>/u/Remarkable_Jicama775</name>
      <uri>https://old.reddit.com/user/Remarkable_Jicama775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5/tree/main"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.5/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quants are here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Jicama775"&gt; /u/Remarkable_Jicama775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q1h7/httpshuggingfacecominimaxaiminimaxm25treemain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q1h7/httpshuggingfacecominimaxaiminimaxm25treemain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q1h7/httpshuggingfacecominimaxaiminimaxm25treemain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35d2x</id>
    <title>MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters</title>
    <updated>2026-02-12T21:02:15+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt; &lt;img alt="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" src="https://external-preview.redd.it/_kcNQarR05LXfQqSjI9sCiHSj5IycOpRZaI00SHW4k8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96ababa53bad9198147827e5856fa3e99fbda827" title="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenHands reveals the model size in their announcement.&lt;/p&gt; &lt;p&gt;Still waiting for the model to appear on HF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openhands.dev/blog/minimax-m2-5-open-weights-models-catch-up-to-claude"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T21:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3qwyi</id>
    <title>oMLX - open-source MLX inference server with paged SSD caching for Apple Silicon</title>
    <updated>2026-02-13T14:41:23+00:00</updated>
    <author>
      <name>/u/cryingneko</name>
      <uri>https://old.reddit.com/user/cryingneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qwyi/omlx_opensource_mlx_inference_server_with_paged/"&gt; &lt;img alt="oMLX - open-source MLX inference server with paged SSD caching for Apple Silicon" src="https://preview.redd.it/gy8epdcex9jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63f504c27de7ef48c0affbd687b587bbf921db42" title="oMLX - open-source MLX inference server with paged SSD caching for Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I know things are buzzing with the MiniMax and GLM releases right now, so I'm not sure if today is the best day to post this - but I wanted to share something I've been working on and I'm genuinely proud of.&lt;/p&gt; &lt;p&gt;Whether you love or hate Ollama, we all know what it is. Setting aside the technical debates, I think Ollama absolutely nailed the concept of making LLMs accessible to everyday users. But it always bugged me that there wasn't a really easy-to-use, open-source app built on MLX. So I built one.&lt;/p&gt; &lt;h2&gt;What is oMLX?&lt;/h2&gt; &lt;p&gt;An LLM inference server for Apple Silicon with a native macOS menubar app. Download the DMG, drag to Applications, done. No terminal, no config files to start.&lt;/p&gt; &lt;h2&gt;Why I built this&lt;/h2&gt; &lt;p&gt;I don't need VLM or TTS/STT right now. What I needed was:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A server I can easily spin up on my Mac&lt;/li&gt; &lt;li&gt;An LLM backend for my Obsidian Copilot&lt;/li&gt; &lt;li&gt;Embedding + Reranking for my notes - all in one app, not three separate tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And then there's the coding agent use case. I personally wanted to use local LLMs with Claude Code. But here's the thing we all know - prefix caching in existing apps is... rough. Coding agents send requests where the prefix keeps shifting, invalidating the cache. A few turns later, the agent circles back to a previous prefix, and now your Mac has to re-prefill that entire context from scratch. Painfully slow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;oMLX solves this with paged SSD caching.&lt;/strong&gt; Every KV cache block gets persisted to SSD. When a previous prefix comes back, it's restored instantly instead of being recomputed. This is a game-changer for long coding sessions.&lt;/p&gt; &lt;p&gt;OK, enough rambling. Here's what's under the hood:&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; - Continuous batching via mlx-lm - handles multiple concurrent requests - Multi-model serving - load LLM + Embedding + Reranker simultaneously, with LRU eviction - Reasoning model support - automatic &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tag handling (DeepSeek, MiniMax, etc.) - Harmony protocol - native support for gpt-oss models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caching&lt;/strong&gt; - Paged KV Cache - block-based with prefix sharing and copy-on-write (vLLM&amp;amp;vllm-mlx project inspired) - SSD tiered caching - automatic GPU-to-SSD offloading for virtually unlimited context caching - Hybrid cache - mixed KVCache + RotatingKVCache for complex architectures (Gemma3, etc.) - Persistent cache - KV cache blocks survive server restarts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;API&lt;/strong&gt; - OpenAI compatible - &lt;code&gt;/v1/chat/completions&lt;/code&gt;, &lt;code&gt;/v1/completions&lt;/code&gt;, &lt;code&gt;/v1/models&lt;/code&gt;, &lt;code&gt;/v1/embeddings&lt;/code&gt; - Anthropic compatible - &lt;code&gt;/v1/messages&lt;/code&gt; - Tool calling - JSON, Qwen, Gemma, MiniMax, GLM formats + MCP - Structured output - JSON mode and JSON Schema&lt;/p&gt; &lt;p&gt;&lt;strong&gt;macOS App&lt;/strong&gt; - Native menubar app (PyObjC, not Electron) - Admin dashboard with built-in chat and real-time monitoring - HuggingFace model downloader built into the dashboard - Signed &amp;amp; notarized DMG&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Apple Silicon (M1+), macOS 14.0+&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/jundot/omlx"&gt;github.com/jundot/omlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href="https://github.com/jundot/omlx/releases"&gt;github.com/jundot/omlx/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm genuinely happy with what I've built. If you have similar needs, I hope oMLX makes your workflow better too. It's 100% open source - if my hobby project can help someone out there, that's even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cryingneko"&gt; /u/cryingneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gy8epdcex9jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qwyi/omlx_opensource_mlx_inference_server_with_paged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qwyi/omlx_opensource_mlx_inference_server_with_paged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:41:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ob0r</id>
    <title>GLM 5 has a regression in international language writing according to NCBench</title>
    <updated>2026-02-13T12:49:51+00:00</updated>
    <author>
      <name>/u/jugalator</name>
      <uri>https://old.reddit.com/user/jugalator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This surprised me but also confirmed my poor first impression of it, since I happened to use it for text generation in a less common language and it performed very poorly, barely even like the aging Gemini 2.5 Flash and more like a good 70B Llama 3.x model.&lt;/p&gt; &lt;p&gt;At NCBench - Language Writing, it trails GLM 4.5-4.7 by quite a distance when tested for European languages and Hindi. GLM 4.5 is the clear, superior release in this regard according to NCBench.&lt;/p&gt; &lt;p&gt;Interestingly, Language &lt;em&gt;Comprehension&lt;/em&gt; didn't seem to regress much at all!&lt;/p&gt; &lt;p&gt;GLM 5 may be great and all, but just a heads up if you use it for this particular scenario since I think it's been flying below the radar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jugalator"&gt; /u/jugalator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nc-bench.com/tests/language-writing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ob0r/glm_5_has_a_regression_in_international_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ob0r/glm_5_has_a_regression_in_international_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3l572</id>
    <title>MiniMax onX: Weights dropping REALLY, REALLY, SOON</title>
    <updated>2026-02-13T09:51:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt; &lt;img alt="MiniMax onX: Weights dropping REALLY, REALLY, SOON" src="https://preview.redd.it/jrgpe9krh8jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30a1ae2be695a2a4f2dee2ca962e2fa76614dcc1" title="MiniMax onX: Weights dropping REALLY, REALLY, SOON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jrgpe9krh8jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3q0qb</id>
    <title>llama.cpp llama-server running SSM models VRAM fix merged</title>
    <updated>2026-02-13T14:04:54+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;During my time fixing the Kimi Linear server bug reported by &lt;a href="/u/Lord_Pazzu"&gt;u/Lord_Pazzu&lt;/a&gt;, I discovered that running llama-server running SSM hybrid models in general uses KV cache that is multiple of the number of parallel threads (--parallel), so for example, if you run Nemotron 3 Nano at 1M context and --parallel 8, then it would use 48GB VRAM KV cache instead of 6GB even though each server instance can only serve 128K context. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/19552"&gt;https://github.com/ggml-org/llama.cpp/issues/19552&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this fix, you will only use 6GB just like the transformer models. That means with 48GB VRAM to spare, you can now serve 8 users simultaneously with 1M context each.&lt;/p&gt; &lt;p&gt;Merged PR:&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/19559"&gt;https://github.com/ggml-org/llama.cpp/pull/19559&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This applies to all SSM hybrid models like Qwen3Next, Kimi Linear, Nemotron 3 Nano, etc.&lt;/p&gt; &lt;p&gt;So if u r a llama-server user with these new models, then it will be a great news to you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t8ro</id>
    <title>Nvidia‚Äôs new technique cuts LLM reasoning costs by 8x without losing accuracy</title>
    <updated>2026-02-13T16:09:31+00:00</updated>
    <author>
      <name>/u/Mission-Street4214</name>
      <uri>https://old.reddit.com/user/Mission-Street4214</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia developed a new technique called Dynamic Memory Sparsification (DMS) that vastly improves how LLMs manage their KV cache during inference. It accomplishes this by retrofitting existing models so that the attention layers output a &lt;strong&gt;learned keep or evict&lt;/strong&gt; signal for each token in the KV cache. &lt;/p&gt; &lt;p&gt;In addition, they've added a &amp;quot;delayed eviction&amp;quot; that marks a token as low-importance, but doesn't delete it immediately. Instead, it remains accessible for a short time and allows the model to extract any useful information into newer tokens before it's discarded.&lt;/p&gt; &lt;p&gt;These advancements reduce KV memory usage by up to &lt;strong&gt;8x&lt;/strong&gt;, allowing the model to think longer, run faster and handle more concurrent requests.&lt;/p&gt; &lt;p&gt;Definitely recommend reading the full article. Looking forward to seeing this on self hosted hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://venturebeat.com/orchestration/nvidias-new-technique-cuts-llm-reasoning-costs-by-8x-without-losing-accuracy"&gt;VentureBeat Article&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mission-Street4214"&gt; /u/Mission-Street4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ntgi</id>
    <title>Deepseek announced they are testing a new model.</title>
    <updated>2026-02-13T12:25:39+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt; &lt;img alt="Deepseek announced they are testing a new model." src="https://preview.redd.it/y5kcxf8699jg1.jpg?width=140&amp;amp;height=58&amp;amp;auto=webp&amp;amp;s=7a1927f261bcd4f8544cc048c08f30ee495de504" title="Deepseek announced they are testing a new model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/y5kcxf8699jg1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a7fad6dc2630447bffd04ec671cfb62edc9187f"&gt;https://preview.redd.it/y5kcxf8699jg1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a7fad6dc2630447bffd04ec671cfb62edc9187f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Chinese group&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rutggcjgf9jg1.png?width=342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e393275ade2c1baff47b0855457c817e87bca3e3"&gt;https://preview.redd.it/rutggcjgf9jg1.png?width=342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e393275ade2c1baff47b0855457c817e87bca3e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the benchmark model name is fake (placeholder) . This is just for distinguishing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The benchmark is test reading skills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Testing completed, OpenAI MRCR 8-pin&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;Index | Target | Tokens | Prefix | Score | Result&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;78 | 128000 | 130660 | BfohKPVF96 | 0.9821 | üÜó 0.98&lt;/p&gt; &lt;p&gt;71 | 128000 | 132425 | NPnuBb2ccE | 0.0575 | üÜó 0.06&lt;/p&gt; &lt;p&gt;32 | 128000 | 132828 | dlUj2XS3iz | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;45 | 128000 | 135258 | VAUPEFeyUy | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;56 | 128000 | 136965 | kZWrPWyo2z | 0.0276 | üÜó 0.03&lt;/p&gt; &lt;p&gt;7 | 128000 | 136974 | kej4Qdr9Mf | 0.0101 | üÜó 0.01&lt;/p&gt; &lt;p&gt;57 | 128000 | 137211 | HdvXqxVvwQ | 0.0420 | üÜó 0.04&lt;/p&gt; &lt;p&gt;87 | 128000 | 138158 | 4KJuJvpDKt | 0.1123 | üÜó 0.11&lt;/p&gt; &lt;p&gt;64 | 128000 | 138512 | piNIebm2Zr | 0.0560 | üÜó 0.06&lt;/p&gt; &lt;p&gt;88 | 128000 | 138628 | 9W0rMIR3gM | 0.0963 | üÜó 0.10&lt;/p&gt; &lt;p&gt;69 | 256000 | 255410 | BdPq3nqqWy | 0.0307 | üÜó 0.03&lt;/p&gt; &lt;p&gt;40 | 256000 | 255073 | mlzCS98ySY | 0.0221 | üÜó 0.02&lt;/p&gt; &lt;p&gt;58 | 256000 | 254750 | 7ABmnzg5oI | 0.9830 | üÜó 0.98&lt;/p&gt; &lt;p&gt;61 | 256000 | 254317 | gkaLloQvjH | 0.1098 | üÜó 0.11&lt;/p&gt; &lt;p&gt;97 | 256000 | 253819 | 9RNBXn2Gh5 | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;51 | 256000 | 251993 | E4c3w7oF2w | 0.0703 | üÜó 0.07&lt;/p&gt; &lt;p&gt;23 | 256000 | 251766 | SNtG1BhaDM | 0.9952 | üÜó 1.00&lt;/p&gt; &lt;p&gt;280 | 256000 | 261742 | RsuyJ8tkrC | 0.0681 | üÜó 0.07&lt;/p&gt; &lt;p&gt;278 | 256000 | 263214 | D7Ndj9vdKm | 0.1613 | üÜó 0.16&lt;/p&gt; &lt;p&gt;224 | 256000 | 265550 | 1YZYhQtMCW | 0.1545 | üÜó 0.15&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;üèÜ Total Marks: 0.3489&lt;/p&gt; &lt;p&gt;üìè Length statistics:&lt;/p&gt; &lt;p&gt;- 128000 Tokens: 0.3384 (n=10)&lt;/p&gt; &lt;p&gt;- 256000 Tokens: 0.3595 (n=10)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3mnm3</id>
    <title>ByteDance Releases Protenix-v1</title>
    <updated>2026-02-13T11:22:41+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;ByteDance Releases Protenix-v1: A New Open-Source Model Achieving AF3-Level Performance in Biomolecular Structure Prediction&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/bytedance/Protenix"&gt;https://github.com/bytedance/Protenix&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T11:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3hlfq</id>
    <title>UG student launches Dhi-5B (Trained from Scratch)</title>
    <updated>2026-02-13T06:13:29+00:00</updated>
    <author>
      <name>/u/gradNorm</name>
      <uri>https://old.reddit.com/user/gradNorm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt; &lt;img alt="UG student launches Dhi-5B (Trained from Scratch)" src="https://preview.redd.it/5tsgquvue7jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fac59fbf4b00df28aabae2f993f4d65bb88169c" title="UG student launches Dhi-5B (Trained from Scratch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii everyone,&lt;/p&gt; &lt;p&gt;I present Dhi-5B: A 5 billion parameter Multimodal Language Model trained compute optimally with just ‚Çπ1.1 lakh ($1200).&lt;/p&gt; &lt;p&gt;I incorporate the latest architecture design and training methodologies in this. And I also use a custom built codebase for training these models.&lt;/p&gt; &lt;p&gt;I train the Dhi-5B in 5 stages:-&lt;/p&gt; &lt;p&gt;üìö Pre-Training: The most compute heavy phase, where the core is built. (Gives the Base varient.)&lt;/p&gt; &lt;p&gt;üìú Context-Length-Extension: The model learns to handle 16k context from the 4k learned during PT.&lt;/p&gt; &lt;p&gt;üìñ Mid-Training: Annealing on very high quality datasets.&lt;/p&gt; &lt;p&gt;üí¨ Supervised-Fine-Tuning: Model learns to handle conversations. (Gives the Instruct model.)&lt;/p&gt; &lt;p&gt;üëÄ Vision-Extension: The model learns to see. (Results in The Dhi-5B.)&lt;/p&gt; &lt;p&gt;I'll be dropping it in 3 phases:-&lt;/p&gt; &lt;p&gt;i. Dhi-5B-Base (available now)&lt;/p&gt; &lt;p&gt;ii. Dhi-5B-Instruct (coming soon)&lt;/p&gt; &lt;p&gt;iii. The Dhi-5B (coming soon)&lt;/p&gt; &lt;p&gt;Some details about the Dhi-5B-Base model:-&lt;/p&gt; &lt;p&gt;The base varient is of 4 billion parameters. It is trained on 40 billion natural language tokens mostly in english from FineWeb-Edu dataset.&lt;/p&gt; &lt;p&gt;I use the new Muon optimizer for optimising the Matrix Layers, and rest are optimized by AdamW.&lt;/p&gt; &lt;p&gt;The model has 32 layers, with 3072 width, SwiGLU MLPs, the full MHA attention with FlashAttention-3, 4096 context length, 64k vocab and 2 million batch size during training.&lt;/p&gt; &lt;p&gt;Attached are some evaluations of the base model, the compared models are about 10x more expensive than ours.&lt;/p&gt; &lt;p&gt;Thank you, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gradNorm"&gt; /u/gradNorm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tsgquvue7jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T06:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3o6je</id>
    <title>New DeepSeek update: "DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window."</title>
    <updated>2026-02-13T12:43:50+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt; &lt;img alt="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" src="https://preview.redd.it/dg94ujw1c9jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55de58cf8a3e4a397d81184a2473b94f7a31aa33" title="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AiBattle on ùïè: &lt;a href="https://x.com/AiBattle_/status/2022280288643039235"&gt;https://x.com/AiBattle_/status/2022280288643039235&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dg94ujw1c9jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3kzce</id>
    <title>MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours</title>
    <updated>2026-02-13T09:41:01+00:00</updated>
    <author>
      <name>/u/Own_Forever_5997</name>
      <uri>https://old.reddit.com/user/Own_Forever_5997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt; &lt;img alt="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" src="https://preview.redd.it/p94fz9gsf8jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=920f76b1a80dd8b1b58e34745f143966274a40a4" title="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Forever_5997"&gt; /u/Own_Forever_5997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p94fz9gsf8jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3pxy7</id>
    <title>MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face</title>
    <updated>2026-02-13T14:01:52+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face" src="https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de0bab4be78008336f973196f0ed98e2bbe49764" title="MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can monitor quants begin to appear with this search: &lt;a href="https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3tqox</id>
    <title>RAM shortage problem solved</title>
    <updated>2026-02-13T16:27:35+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3tqox/ram_shortage_problem_solved/"&gt; &lt;img alt="RAM shortage problem solved" src="https://preview.redd.it/p23ri3acgajg1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35411e042a18e57db564abff805694de6498657d" title="RAM shortage problem solved" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p23ri3acgajg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3tqox/ram_shortage_problem_solved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3tqox/ram_shortage_problem_solved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3csbk</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)</title>
    <updated>2026-02-13T02:12:47+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)" src="https://preview.redd.it/orcqu1oq76jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=617bc649889e0dd0343008568d8aa957dde229c5" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Friday's guests: &lt;strong&gt;The Core Team of MiniMax Lab and The Lab‚Äôs Founder!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Friday, Feb. 13th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/orcqu1oq76jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T02:12:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax ‚Äî Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax ‚Äî Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî &lt;strong&gt;Founder and CEO of MiniMax&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; ‚Äî Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
