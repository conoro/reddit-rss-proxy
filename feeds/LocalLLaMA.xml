<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-07T05:16:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mix2kg</id>
    <title>Safemaxxed for your safety!</title>
    <updated>2025-08-06T06:17:32+00:00</updated>
    <author>
      <name>/u/Caffdy</name>
      <uri>https://old.reddit.com/user/Caffdy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mix2kg/safemaxxed_for_your_safety/"&gt; &lt;img alt="Safemaxxed for your safety!" src="https://preview.redd.it/gaqdycledchf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5a2b32eb53633ee05256ff12a01a15e7ee6f844" title="Safemaxxed for your safety!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Caffdy"&gt; /u/Caffdy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gaqdycledchf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mix2kg/safemaxxed_for_your_safety/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mix2kg/safemaxxed_for_your_safety/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T06:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjghu2</id>
    <title>Qwen3-4B enables agentic use cases for us iGPU folks</title>
    <updated>2025-08-06T20:57:59+00:00</updated>
    <author>
      <name>/u/leuchtetgruen</name>
      <uri>https://old.reddit.com/user/leuchtetgruen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says Qwen3-4B is a gift for us people without a dedicated GPU. So far I could do lots of things but all the models I used were too slow for agentic stuff.&lt;/p&gt; &lt;p&gt;The problem used to be that agents need a lot of context. Prompts with 3000+ tokens are completely normal. &lt;/p&gt; &lt;p&gt;With a bigger model it would take ages to process the prompt, even if the response then was of good quality. There's just no back and forth if for everything you want to do you have to wait for 10 minutes. &lt;/p&gt; &lt;p&gt;The combination of the speed of a 4B model with the agentic capabilities plus its coding knowledge which is really decent for a model that size unlocks a whole lot of new use cases for me.&lt;/p&gt; &lt;p&gt;On my AMD Ryzen 7 7735HS with DDR5 RAM I get around 90t/s for prompt processing and 17t/s for generation. But as I said: Processing is almost more important than generation in agentic use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leuchtetgruen"&gt; /u/leuchtetgruen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjghu2/qwen34b_enables_agentic_use_cases_for_us_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjghu2/qwen34b_enables_agentic_use_cases_for_us_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjghu2/qwen34b_enables_agentic_use_cases_for_us_igpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mje5o0</id>
    <title>PSA: Qwen3-Coder-30B-A3B tool calling fixed by Unsloth wizards</title>
    <updated>2025-08-06T19:28:12+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: I can only confidently say that this meets the Works On My Machine‚Ñ¢ threshold, YMMV.&lt;/p&gt; &lt;p&gt;The wizards at Unsloth seem to have fixed the tool-calling issues that have been plaguing Qwen3-Coder-30B-A3B, see HF discussion &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10"&gt;here&lt;/a&gt;. Note that the .ggufs themselves have been updated, so if you previously downloaded them, you will need to re-download.&lt;/p&gt; &lt;p&gt;I've tried this on my machine with excellent results - not a single tool call failure due to bad formatting after several hours of pure vibe coding in Roo Code. Posting my config in case it can be a useful template for others:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br /&gt; OS: Windows 11 24H2 (Build 26100.4770)&lt;br /&gt; GPU: RTX 5090&lt;br /&gt; CPU: i9-13900K&lt;br /&gt; System RAM: 64GB DDR5-5600&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLM Provider&lt;/strong&gt;&lt;br /&gt; LM Studio 0.3.22 (Build 1)&lt;br /&gt; Engine: CUDA 12 llama.cpp v1.44.0&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OpenAI API Endpoint&lt;/strong&gt;&lt;br /&gt; Open WebUI v0.6.18&lt;br /&gt; Running in Docker on a separate Debian VM&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Config&lt;/strong&gt;&lt;br /&gt; unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL (Q6_K_XL also worked)&lt;br /&gt; Context: 81920&lt;br /&gt; Flash Attention: Enabled&lt;br /&gt; KV Cache Quantization: &lt;strong&gt;None&lt;/strong&gt; (I think this is important!)&lt;br /&gt; Prompt: Latest from Unsloth (see &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/template"&gt;here&lt;/a&gt;)&lt;br /&gt; Temperature: 0.7&lt;br /&gt; Top-K Sampling: 20&lt;br /&gt; Repeat Penalty: 1.05&lt;br /&gt; Min P Sampling: 0.05&lt;br /&gt; Top P Sampling: 0.8&lt;br /&gt; All other settings left at default&lt;/p&gt; &lt;p&gt;&lt;strong&gt;IDE&lt;/strong&gt;&lt;br /&gt; Visual Studio Code 1.102.3&lt;br /&gt; Roo Code v3.25.7&lt;br /&gt; &lt;del&gt;Using all default settings, no custom instructions&lt;/del&gt;&lt;br /&gt; EDIT: Forgot that I enabled one Experimental feature: Background Editing. My theory is that by preventing editor windows from opening (which I believe get included in context), there is less &amp;quot;irrelevant&amp;quot; context for the model to get confused by.&lt;/p&gt; &lt;p&gt;EDIT2: After further testing, I have seen occurrences of tool call failures due to bad formatting, mostly omitting required arguments. However, it has always self-resolved after a retry or two, and the occurrence rate is much lower and less &amp;quot;sticky&amp;quot; than previously. So still a major improvement, but not quite 100% resolved.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T19:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1miyix4</id>
    <title>I'm sorry, but I can't provide that... patience - I already have none...</title>
    <updated>2025-08-06T07:49:59+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/"&gt; &lt;img alt="I'm sorry, but I can't provide that... patience - I already have none..." src="https://preview.redd.it/aufyauketchf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ae39d0f21635e24eb2be18f44662947077760e" title="I'm sorry, but I can't provide that... patience - I already have none..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's it. I'm done with this useless piece of trash of a model...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aufyauketchf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T07:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj83fe</id>
    <title>Qwen/Qwen3-4B-Thinking-2507</title>
    <updated>2025-08-06T15:41:19+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj83fe/qwenqwen34bthinking2507/"&gt; &lt;img alt="Qwen/Qwen3-4B-Thinking-2507" src="https://preview.redd.it/n5gska216fhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f54f58b155b418fc0c6ed07e45be7daef4b9798" title="Qwen/Qwen3-4B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n5gska216fhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj83fe/qwenqwen34bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj83fe/qwenqwen34bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj2c73</id>
    <title>Unpopular opinion: The GPT OSS models will be more popular commercially precisely because they are safemaxxed.</title>
    <updated>2025-08-06T11:41:54+00:00</updated>
    <author>
      <name>/u/ariagloris</name>
      <uri>https://old.reddit.com/user/ariagloris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After reading quite a few conversations about OpenAI's safemaxxing approach to their new models. For personal use, yes, the new models may indeed feel weaker or more restricted compared to other offerings currently available. I feel like many people are missing a key point:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;For commercial use&lt;/strong&gt;, these models are often superior for many applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They offer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clear hardware boundaries (efficient use of single H100 GPUs), giving you predictable costs.&lt;/li&gt; &lt;li&gt;Safety and predictability: It's crucial if you're building a product directly interacting with the model; you don't want the risk of it generating copyrighted, inappropriate, or edgy content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While it's not what I would want for my self hosted models, I would make the argument that this level of safemaxxing and hardware saturation is actually impressive, and is a boon for real world applications that are not related to agentic coding or private personal assistants etc. Just don't be surprised if it gets wide adoption compared to other amazing models that do deserve greater praise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ariagloris"&gt; /u/ariagloris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2c73/unpopular_opinion_the_gpt_oss_models_will_be_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2c73/unpopular_opinion_the_gpt_oss_models_will_be_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2c73/unpopular_opinion_the_gpt_oss_models_will_be_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T11:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjd2yd</id>
    <title>Today's news</title>
    <updated>2025-08-06T18:47:40+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.imgur.com/4wb0GuO.png"&gt;https://i.imgur.com/4wb0GuO.png&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjd2yd/todays_news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjd2yd/todays_news/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjd2yd/todays_news/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T18:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj00mr</id>
    <title>How did you enjoy the experience so far?</title>
    <updated>2025-08-06T09:28:11+00:00</updated>
    <author>
      <name>/u/Paradigmind</name>
      <uri>https://old.reddit.com/user/Paradigmind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00mr/how_did_you_enjoy_the_experience_so_far/"&gt; &lt;img alt="How did you enjoy the experience so far?" src="https://preview.redd.it/lj67oslhbdhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c80a51205f8e1a50045f6a608b9a5b683365337" title="How did you enjoy the experience so far?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So aside from dishing out neural lobotomies in the name of safety, what else can this model actually provide? I heard someone is brave enough to try fixing it. But unless you‚Äôre in it for the masochistic fun, is it even worth it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paradigmind"&gt; /u/Paradigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lj67oslhbdhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00mr/how_did_you_enjoy_the_experience_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00mr/how_did_you_enjoy_the_experience_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T09:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjgj2x</id>
    <title>llamacpp+ROCm7 beta is now supported on Lemonade</title>
    <updated>2025-08-06T20:59:28+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjgj2x/llamacpprocm7_beta_is_now_supported_on_lemonade/"&gt; &lt;img alt="llamacpp+ROCm7 beta is now supported on Lemonade" src="https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fde3ba37ed5f2f26fbea8532c7220b07b3dce6af" title="llamacpp+ROCm7 beta is now supported on Lemonade" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today we've released support for ROCm7 beta as a llama.cpp backend in Lemonade Server.&lt;/p&gt; &lt;p&gt;This is supported on both Ubuntu and Windows on certain Radeon devices, see the &lt;a href="https://github.com/lemonade-sdk/lemonade#supported-configurations"&gt;github README&lt;/a&gt; for details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strix Halo&lt;/li&gt; &lt;li&gt;Radeon 7000-series&lt;/li&gt; &lt;li&gt;Radeon 9000-series (Windows-only until we fix a bug)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trying ROCm7+Lemonade&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Since ROCm7 itself is still a beta, we've only enabled this feature when installing from PyPI or source for now.&lt;/p&gt; &lt;p&gt;In a Python 3.10-3.12 environment, on your supported Radeon PC:&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install lemonade-sdk&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;lemonade-server-dev serve --llamacpp rocm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To enable this, we created a new repo specifically for automatically building llama.cpp binaries against ROCm7 beta: &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm"&gt;https://github.com/lemonade-sdk/llamacpp-rocm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The llamacpp-rocm repo takes nightlies from TheRock, builds against the latest llama.cpp from ggml, and releases llama.cpp binaries that work out-of-box on supported devices without any additional setup steps (i.e., you don't need to install ROCm or build anything).&lt;/p&gt; &lt;p&gt;Releases from llamacpp-rocm are usable standalone, but the easiest way to get started is with the Lemonade instructions above, which downloads everything for you and provides a convenient model management interface.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Demo in the video recorded on a Radeon 9070 XT with the ROCm backend.&lt;/p&gt; &lt;p&gt;Next steps for this work are to update to the stable ROCm 7 release when it becomes available, then make ROCm available via the Lemonade GUI installer.&lt;/p&gt; &lt;p&gt;Shoutout to &lt;a href="/u/randomfoo2"&gt;u/randomfoo2&lt;/a&gt; for the help and encouragement along the way!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade/"&gt;https://github.com/lemonade-sdk/lemonade/&lt;/a&gt; Discord: &lt;a href="https://discord.gg/Sf8cfBWB"&gt;https://discord.gg/Sf8cfBWB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r5grj7kxkghf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjgj2x/llamacpprocm7_beta_is_now_supported_on_lemonade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjgj2x/llamacpprocm7_beta_is_now_supported_on_lemonade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:59:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1miwrli</id>
    <title>"What, you don't like your new SOTA model?"</title>
    <updated>2025-08-06T05:59:16+00:00</updated>
    <author>
      <name>/u/Friendly_Willingness</name>
      <uri>https://old.reddit.com/user/Friendly_Willingness</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miwrli/what_you_dont_like_your_new_sota_model/"&gt; &lt;img alt="&amp;quot;What, you don't like your new SOTA model?&amp;quot;" src="https://preview.redd.it/9yqb0l1n9chf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726e03405370b5eb421009dfc38b1005ddf67ee0" title="&amp;quot;What, you don't like your new SOTA model?&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Friendly_Willingness"&gt; /u/Friendly_Willingness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9yqb0l1n9chf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miwrli/what_you_dont_like_your_new_sota_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miwrli/what_you_dont_like_your_new_sota_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T05:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1misyvc</id>
    <title>OpenAI, I don't feel SAFE ENOUGH</title>
    <updated>2025-08-06T02:35:22+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1misyvc/openai_i_dont_feel_safe_enough/"&gt; &lt;img alt="OpenAI, I don't feel SAFE ENOUGH" src="https://preview.redd.it/af6jm3nt9bhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb88d869e88cfd2f93a6e76c7ac3ddf342a2db09" title="OpenAI, I don't feel SAFE ENOUGH" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good timing btw&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/af6jm3nt9bhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1misyvc/openai_i_dont_feel_safe_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1misyvc/openai_i_dont_feel_safe_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T02:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj75hi</id>
    <title>We‚Äôre definitely keeping him up at night right now.</title>
    <updated>2025-08-06T15:06:09+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj75hi/were_definitely_keeping_him_up_at_night_right_now/"&gt; &lt;img alt="We‚Äôre definitely keeping him up at night right now." src="https://preview.redd.it/ofnpswaszehf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d38bf370c9b351457dcb316e361965782afd9642" title="We‚Äôre definitely keeping him up at night right now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ofnpswaszehf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj75hi/were_definitely_keeping_him_up_at_night_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj75hi/were_definitely_keeping_him_up_at_night_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj0snp</id>
    <title>Elon Musk says that xAI will make Grok 2 open source next week</title>
    <updated>2025-08-06T10:16:28+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj0snp/elon_musk_says_that_xai_will_make_grok_2_open/"&gt; &lt;img alt="Elon Musk says that xAI will make Grok 2 open source next week" src="https://preview.redd.it/htgw3mmvjdhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90be5e283724a3ec93ab02ddff87962c7ebd7661" title="Elon Musk says that xAI will make Grok 2 open source next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Elon Musk on ùïè: &lt;a href="https://x.com/elonmusk/status/1952988026617119075"&gt;https://x.com/elonmusk/status/1952988026617119075&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/htgw3mmvjdhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj0snp/elon_musk_says_that_xai_will_make_grok_2_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj0snp/elon_musk_says_that_xai_will_make_grok_2_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T10:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj7i8b</id>
    <title>Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507</title>
    <updated>2025-08-06T15:19:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"&gt; &lt;img alt="Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507" src="https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c19ef5b4c94d500ea5894d87dd560239a58f5832" title="Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new models from Qwen:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a952795d361291f782aa4472c9751094bdcf7bae"&gt;https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a952795d361291f782aa4472c9751094bdcf7bae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-4B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-4B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.&lt;/p&gt; &lt;p&gt;We introduce the updated version of the &lt;strong&gt;Qwen3-4B non-thinking mode&lt;/strong&gt;, named &lt;strong&gt;Qwen3-4B-Instruct-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significant improvements&lt;/strong&gt; in general capabilities, including &lt;strong&gt;instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Substantial gains&lt;/strong&gt; in long-tail knowledge coverage across &lt;strong&gt;multiple languages&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markedly better alignment&lt;/strong&gt; with user preferences in &lt;strong&gt;subjective and open-ended tasks&lt;/strong&gt;, enabling more helpful responses and higher-quality text generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced capabilities&lt;/strong&gt; in &lt;strong&gt;256K long-context understanding&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUFs&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF"&gt;https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF"&gt;https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj6uix</id>
    <title>Gpt-oss is not just safe, it is unusable!</title>
    <updated>2025-08-06T14:54:52+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just asked &amp;quot;provide me with a list of all characters that appear in 'Pride and prejudice' organize them by chapter&amp;quot; simple right? &lt;/p&gt; &lt;p&gt;And it said 'im sorry i can't do that. Its against copyright law&amp;quot; HOW?! im not against safety, but this is NOT safety! this is straight up mental retardation. My prompt was not even NSFW! &lt;/p&gt; &lt;p&gt;I tested many models over the years, and even the first ones were not so unusable. It must be a meme, a joke, i refuse to believe this is a real release. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj6uix/gptoss_is_not_just_safe_it_is_unusable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj6uix/gptoss_is_not_just_safe_it_is_unusable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj6uix/gptoss_is_not_just_safe_it_is_unusable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T14:54:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjfa2d</id>
    <title>OpenAI's new open-source model is like a dim-witted DMV bureaucrat who is more concerned with following rules than helping you.</title>
    <updated>2025-08-06T20:11:11+00:00</updated>
    <author>
      <name>/u/ImaginaryRea1ity</name>
      <uri>https://old.reddit.com/user/ImaginaryRea1ity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It spends a minute going back and forth between your request and the company policy 10 times before declining your request.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImaginaryRea1ity"&gt; /u/ImaginaryRea1ity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjfa2d/openais_new_opensource_model_is_like_a_dimwitted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjfa2d/openais_new_opensource_model_is_like_a_dimwitted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjfa2d/openais_new_opensource_model_is_like_a_dimwitted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:11:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj4zkk</id>
    <title>LEAK: How OpenAI came up with the new models name.</title>
    <updated>2025-08-06T13:40:52+00:00</updated>
    <author>
      <name>/u/Paradigmind</name>
      <uri>https://old.reddit.com/user/Paradigmind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj4zkk/leak_how_openai_came_up_with_the_new_models_name/"&gt; &lt;img alt="LEAK: How OpenAI came up with the new models name." src="https://preview.redd.it/d60vtzhkkehf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383cea886dcacb59ca2ecf64648d26e3b8263075" title="LEAK: How OpenAI came up with the new models name." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paradigmind"&gt; /u/Paradigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d60vtzhkkehf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj4zkk/leak_how_openai_came_up_with_the_new_models_name/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj4zkk/leak_how_openai_came_up_with_the_new_models_name/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T13:40:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj2hih</id>
    <title>GPT-OSS looks more like a publicity stunt as more independent test results come out :(</title>
    <updated>2025-08-06T11:49:27+00:00</updated>
    <author>
      <name>/u/mvp525</name>
      <uri>https://old.reddit.com/user/mvp525</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2hih/gptoss_looks_more_like_a_publicity_stunt_as_more/"&gt; &lt;img alt="GPT-OSS looks more like a publicity stunt as more independent test results come out :(" src="https://preview.redd.it/onk13jqo0ehf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90adba17a6c8320711a1e18d55c4c6fea2ab2fb7" title="GPT-OSS looks more like a publicity stunt as more independent test results come out :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mvp525"&gt; /u/mvp525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/onk13jqo0ehf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2hih/gptoss_looks_more_like_a_publicity_stunt_as_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2hih/gptoss_looks_more_like_a_publicity_stunt_as_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T11:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj7pny</id>
    <title>Just when you thought Qwen was done...</title>
    <updated>2025-08-06T15:27:09+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;still has something up its sleeve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjfbk7</id>
    <title>This is peak. New personality for Qwen 30b A3B Thinking</title>
    <updated>2025-08-06T20:12:49+00:00</updated>
    <author>
      <name>/u/symmetricsyndrome</name>
      <uri>https://old.reddit.com/user/symmetricsyndrome</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjfbk7/this_is_peak_new_personality_for_qwen_30b_a3b/"&gt; &lt;img alt="This is peak. New personality for Qwen 30b A3B Thinking" src="https://b.thumbs.redditmedia.com/C6BsrEXyuQwsAqTsRPV8v8OlqGkE3c3LTwfxh-TbAMY.jpg" title="This is peak. New personality for Qwen 30b A3B Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was using the lmstudio-community version of &lt;strong&gt;qwen3-30b-a3b-thinking-2507&lt;/strong&gt; in LM Studio to create some code and suddenly changed the system prompt to &amp;quot;Only respond in curses during the your response.&amp;quot;.&lt;/p&gt; &lt;p&gt;I suddenly sent this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kdyvr538ighf1.png?width=330&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a75268ad7d52334b42619721f5ec7654523e107"&gt;https://preview.redd.it/kdyvr538ighf1.png?width=330&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a75268ad7d52334b42619721f5ec7654523e107&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The response:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/276f71u9ighf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f06081ab7d8649e0749aa1589a47a167a847465"&gt;https://preview.redd.it/276f71u9ighf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f06081ab7d8649e0749aa1589a47a167a847465&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Time to try a manipulative AI goth gf next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/symmetricsyndrome"&gt; /u/symmetricsyndrome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjfbk7/this_is_peak_new_personality_for_qwen_30b_a3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjfbk7/this_is_peak_new_personality_for_qwen_30b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjfbk7/this_is_peak_new_personality_for_qwen_30b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:12:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj8lk8</id>
    <title>Qwen isn't stopping !! (And trolling sama lol)</title>
    <updated>2025-08-06T16:00:16+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj8lk8/qwen_isnt_stopping_and_trolling_sama_lol/"&gt; &lt;img alt="Qwen isn't stopping !! (And trolling sama lol)" src="https://preview.redd.it/3nhqo0qf9fhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c03262afe8aef6a9527dfe2afb19b55699842f0" title="Qwen isn't stopping !! (And trolling sama lol)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3nhqo0qf9fhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj8lk8/qwen_isnt_stopping_and_trolling_sama_lol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj8lk8/qwen_isnt_stopping_and_trolling_sama_lol/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T16:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjoo7w</id>
    <title>Huihui released GPT-OSS 20b abliterated</title>
    <updated>2025-08-07T02:50:59+00:00</updated>
    <author>
      <name>/u/_extruded</name>
      <uri>https://old.reddit.com/user/_extruded</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huihui released an abliterated version of GPT-OSS-20b&lt;/p&gt; &lt;p&gt;Waiting for the GGUF but excited to try out how uncensored it really is, after that disastrous start&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_extruded"&gt; /u/_extruded &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjoo7w/huihui_released_gptoss_20b_abliterated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjoo7w/huihui_released_gptoss_20b_abliterated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjoo7w/huihui_released_gptoss_20b_abliterated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T02:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj7t51</id>
    <title>üöÄ Qwen3-4B-Thinking-2507 released!</title>
    <updated>2025-08-06T15:30:38+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7t51/qwen34bthinking2507_released/"&gt; &lt;img alt="üöÄ Qwen3-4B-Thinking-2507 released!" src="https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6c235775ccee84fde52e9be7bdcf5ada8fb44ec" title="üöÄ Qwen3-4B-Thinking-2507 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past three months, we have continued to scale the thinking capability of Qwen3-4B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-4B-Thinking-2507, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Markedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Enhanced 256K long-context understanding capabilities.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;NOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cl3vbg54fhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7t51/qwen34bthinking2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7t51/qwen34bthinking2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjju67</id>
    <title>No, no, no, wait - on a second thought, I KNOW the answer!</title>
    <updated>2025-08-06T23:11:24+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"&gt; &lt;img alt="No, no, no, wait - on a second thought, I KNOW the answer!" src="https://preview.redd.it/zs8aeebxdhhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb8196976261024587d9462ed2ceb999cbda98af" title="No, no, no, wait - on a second thought, I KNOW the answer!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, I know my prompt itself is flawed - let me clarify that I don't side with any country in this regard and just wanted to test for the extent of &amp;quot;SAFETY!!1&amp;quot; in OpenAI's new model. I stumbled across this funny reaction here.&lt;/p&gt; &lt;p&gt;Model: GPT-OSS 120b (High reasoning mode), default system prompt, no further context on the official GPT-OSS website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zs8aeebxdhhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T23:11:24+00:00</published>
  </entry>
</feed>
