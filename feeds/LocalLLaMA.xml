<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-31T22:41:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qsbdla</id>
    <title>I built a tool to see what AI agents (Moltbot, Claude, Cursor) are actually doing on your computer</title>
    <updated>2026-01-31T19:15:18+00:00</updated>
    <author>
      <name>/u/gregb_parkingaccess</name>
      <uri>https://old.reddit.com/user/gregb_parkingaccess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone's installing AI agents that can control their entire computer. Moltbot, Clawdbot, Claude Desktop, Cursor - they can read files, click anywhere, take screenshots.&lt;/p&gt; &lt;p&gt;But there's zero visibility into what they're doing.&lt;/p&gt; &lt;p&gt;So I built Molteye. It's a simple Electron app that:&lt;/p&gt; &lt;p&gt;- Shows when AI agents start/stop&lt;/p&gt; &lt;p&gt;- Logs file changes while AI is active&lt;/p&gt; &lt;p&gt;- Alerts on sensitive files (.env, .ssh, credentials)&lt;/p&gt; &lt;p&gt;~100 lines of code. Runs 100% local. No cloud, no tracking.&lt;/p&gt; &lt;p&gt;Mac only for now. Looking for help with Windows support.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/gbessoni/molteye"&gt;https://github.com/gbessoni/molteye&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community - you guys care about local/private AI more than anyone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gregb_parkingaccess"&gt; /u/gregb_parkingaccess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbdla/i_built_a_tool_to_see_what_ai_agents_moltbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbdla/i_built_a_tool_to_see_what_ai_agents_moltbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbdla/i_built_a_tool_to_see_what_ai_agents_moltbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T19:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrazyy</id>
    <title>Cline team got absorbed by OpenAI. Kilo is going full source available in response.</title>
    <updated>2026-01-30T16:56:49+00:00</updated>
    <author>
      <name>/u/demon_bhaiya</name>
      <uri>https://old.reddit.com/user/demon_bhaiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt; &lt;img alt="Cline team got absorbed by OpenAI. Kilo is going full source available in response." src="https://external-preview.redd.it/OJiv7stnybHLdn8-mzf6t_NZ9C8xS7VIYLhMSJsX0d8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=969735c0073c014c64189d4c6b79a9e599fe2c52" title="Cline team got absorbed by OpenAI. Kilo is going full source available in response." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who used Cline with local models, heads up that the core team appears to have joined OpenAI's Codex group based on their LinkedIn profiles. No official announcement yet, but we have seen how these acqui-hires usually play out.&lt;/p&gt; &lt;p&gt;Kilo Code (which forked from Cline and Roo Code) just responded by announcing they are making their backend source available by Feb 6. The VS Code extension, JetBrains plugin, and CLI stay Apache 2.0(Open source). Their gateway supports 500+ models including Qwen, DeepSeek, and Mistral.&lt;/p&gt; &lt;p&gt;They're offering $100 credits to anyone who contributed to Cline, and $150 per merged PR in February. If you want to keep building on an open codebase instead of watching another project disappear into a walled garden, might be worth checking out.&lt;/p&gt; &lt;p&gt;The agentic coding space needs alternatives that work with local and open weight models. Would suck to see all the decent tools end up controlled by the big labs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/demon_bhaiya"&gt; /u/demon_bhaiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.kilo.ai/p/cline-just-acqui-hired"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T16:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs1y5f</id>
    <title>Are commercial models like Claude, Gemini, and ChatGPT counting their whole internal tool calling pipeline part of their ‚Äúmodel‚Äù? (for benchmarks)</title>
    <updated>2026-01-31T13:09:57+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When it comes to benchmark testing and comparing against open source local models, are the big companies wrapping a bunch of tools together with their base model and calling the sum of all the parts the ‚Äúmodel‚Äù? Or are they just testing and benchmarking the base LLM without any connected tools?&lt;/p&gt; &lt;p&gt;It seems like it would be unfair to compare local models to SOTA commercial models if they are not comparing apples to apples. &lt;/p&gt; &lt;p&gt;Could we even tell if they were doing this or not? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsg48d</id>
    <title>I made a LLM based simple IDS/IPS for nginx for fun, using gpt-oss-120b on my own DGX Spark as the model, so I don't have to deal with rate limits or token usage.</title>
    <updated>2026-01-31T22:18:43+00:00</updated>
    <author>
      <name>/u/Saren-WTAKO</name>
      <uri>https://old.reddit.com/user/Saren-WTAKO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsg48d/i_made_a_llm_based_simple_idsips_for_nginx_for/"&gt; &lt;img alt="I made a LLM based simple IDS/IPS for nginx for fun, using gpt-oss-120b on my own DGX Spark as the model, so I don't have to deal with rate limits or token usage." src="https://preview.redd.it/gjnib5t7frgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef34cad52be5e842432ff343093db5a0b273360e" title="I made a LLM based simple IDS/IPS for nginx for fun, using gpt-oss-120b on my own DGX Spark as the model, so I don't have to deal with rate limits or token usage." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What it does and how it works: A vibe coded script would monitor my nginx logs, submit the context and logs (with /24 block of same IP, in case of small scale DDoS) to LLM for consideration. Then, the LLM would issue an IP ban automatically with reason, and notifies me. &lt;/p&gt; &lt;p&gt;When an IP is banned, nginx config is updated and nginx process is restarted. Then, a reviewer script that is sharp vibe coded determines how long the IP should be banned and give a verdict. If it's false positive, it will be unbanned immediately . If it's unsolicited bot or it has weird UA, would ban for 1-24 hours. If it's obviously malicious, then indefinite (30 days) ban. &lt;/p&gt; &lt;p&gt;A summary will be sent to my telegram group topic on script (re)start and every few hours. By using telegram, I can quote the summary to ask for more details and nginx rules to add. I can unban an IP, and I can add &amp;quot;memories&amp;quot; which is more context for a nginx server section, mostly used for minimize false positives. &lt;/p&gt; &lt;p&gt;The first version was done last September. I stopped it because Openrouter didn't really like how I used the free requests 24/7. And because I was VRAM poor, using a small model is inviting troubles for this kind of tasks, obviously.&lt;/p&gt; &lt;p&gt;This is never going to be commercially useful, by the way. This isn't realtime IDS/IPS and never will be, and it makes mistakes, fairly easily despite I am using a moderately intelligent model. &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Entrypoint to my server at home (hopefully this won't be hacked when I wake up, but it's battle tested so it should be fine): &lt;a href="https://apps.wtako.net/board"&gt;https://apps.wtako.net/board&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Optimized vllm deployment: &lt;a href="https://github.com/christopherowen/spark-vllm-mxfp4-docker"&gt;https://github.com/christopherowen/spark-vllm-mxfp4-docker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLM IDS/IPS: &lt;a href="https://github.com/Saren-Arterius/llm-nginx-monitor"&gt;https://github.com/Saren-Arterius/llm-nginx-monitor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saren-WTAKO"&gt; /u/Saren-WTAKO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gjnib5t7frgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsg48d/i_made_a_llm_based_simple_idsips_for_nginx_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsg48d/i_made_a_llm_based_simple_idsips_for_nginx_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T22:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsggwk</id>
    <title>Introducing tapes: Local transparent agentic telemtry</title>
    <updated>2026-01-31T22:33:09+00:00</updated>
    <author>
      <name>/u/jpmmcb</name>
      <uri>https://old.reddit.com/user/jpmmcb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all - John here, CTO &amp;amp; Co-founder at &lt;a href="http://tapes.dev"&gt;tapes.dev&lt;/a&gt; - we just open sourced &lt;code&gt;tapes&lt;/code&gt;: a transparent agentic telemetry system for storing session data, emitting metrics, searching back on previous sessions, and context check-pointing.&lt;/p&gt; &lt;p&gt;Use &lt;code&gt;tapes&lt;/code&gt; search back on conversation turns:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tapes search &amp;quot;What's the weather like in New York?&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then checkout a previous conversation state for context check-pointing and retry (like git):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tapes checkout abc123xyz987 tapes chat &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I built this with local AI in mind and ran the announcement demo with Ollama: I thin this group will appreciate it - &lt;a href="https://www.youtube.com/watch?v=ATeUB6vb57s"&gt;https://www.youtube.com/watch?v=ATeUB6vb57s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://tapes.dev/"&gt;https://tapes.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/papercomputeco/tapes"&gt;https://github.com/papercomputeco/tapes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Give it a try and let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpmmcb"&gt; /u/jpmmcb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsggwk/introducing_tapes_local_transparent_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsggwk/introducing_tapes_local_transparent_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsggwk/introducing_tapes_local_transparent_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T22:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs2cyh</id>
    <title>Early language models - how did they pull it off?</title>
    <updated>2026-01-31T13:28:33+00:00</updated>
    <author>
      <name>/u/OwnMathematician2620</name>
      <uri>https://old.reddit.com/user/OwnMathematician2620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you remember Tay, the Microsoft chatbot from 2016? Or (earliest generation of) Xiaoice from 2014? Despite the fact that AI technology has been around for many years, I find it increasingly difficult to imagine how they managed to do it back then.&lt;/p&gt; &lt;p&gt;The paper 'Attention is All You Need' was published in 2017, and the GPT-2 paper ('Language Models are Unsupervised Multitask Learners') in 2019. Yes, I know we had RNNs before that could do a similar thing, but how on earth did they handle the training dataset? Not to mention their ability to learn from many conversations during inference, which is also what got Tay taken down after only a day.&lt;/p&gt; &lt;p&gt;I don't think they even used the design principle as modern LLMs. It's a shame that I can't find any official information about Tay's architecture, as well as how it's trained...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnMathematician2620"&gt; /u/OwnMathematician2620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsdwlt</id>
    <title>93GB model on a StrixHalo 128GB with 64k Context</title>
    <updated>2026-01-31T20:52:06+00:00</updated>
    <author>
      <name>/u/El_90</name>
      <uri>https://old.reddit.com/user/El_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen anyone mention getting the biggest models working on Strix Halo (or I missed them) so I thought I would document my configs in case anyone else wants to do the same and is struggling. I'm quite new to this, be gentle on me!&lt;/p&gt; &lt;p&gt;And if anyone sees room for improvement or sees issues, please give the feedback, I'm all for learning! This took many goes to get it stable. I wanted this for coding so I chose a larger model at a slower speed. &lt;/p&gt; &lt;p&gt;1: Bios - set full RAM to system/CPU (i.e. not gpu) &lt;/p&gt; &lt;p&gt;2: /etc/default/grub&lt;/p&gt; &lt;p&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet amd_iommu=off amdgpu.gttsize=131072 ttm.pages _limit=33554432&amp;quot;&lt;/p&gt; &lt;p&gt;3: Llama-server command&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080 -ngl 999 -fa on -c 65536 -b 2048 -ub 2048 -ctk q4_0 -ctv q4_0 --cache-reuse 256 --numa distribute --no-mmap --log-file --log-timestamps --perf -m /root/.cache/llama.cpp/bartowski_Qwen_Qwen3-235B-A22B-Instruct-2507-GGUF_Qwen_Qwen3-235B-A22B-Instruct-2507-IQ3_XS_Qwen_Qwen3-235B-A22B-Instruct-2507-IQ3_XS-00001-of-00003.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(I'm sure people will debate other models, this post isn't specific to the model, but on how to fit a larger GB model!)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;4: Of note:&lt;/p&gt; &lt;p&gt;High context 64k&lt;br /&gt; b/ub set to 2048, 4096 was too high&lt;br /&gt; quantised keys and vals to q4_0 &lt;/p&gt; &lt;p&gt;5: Speed&lt;/p&gt; &lt;p&gt;At the beginning of a session it's 15t/s, but as the agent continues (and context fills up?) it slows to a very stable 7-9t/s, which I'm happy with for the model size and the performance. &lt;/p&gt; &lt;p&gt;Not sure if this is valuable or not :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_90"&gt; /u/El_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdwlt/93gb_model_on_a_strixhalo_128gb_with_64k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdwlt/93gb_model_on_a_strixhalo_128gb_with_64k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsdwlt/93gb_model_on_a_strixhalo_128gb_with_64k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T20:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrj1y4</id>
    <title>Stop it with the Agents/Projects Slop and spam</title>
    <updated>2026-01-30T21:44:24+00:00</updated>
    <author>
      <name>/u/Daemontatox</name>
      <uri>https://old.reddit.com/user/Daemontatox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The sub is now averaging 3-4 unfinished sloppy Agentic project that's titled the &amp;quot;best next discovery&amp;quot; or &amp;quot;alternative to [insert famous tool here]&amp;quot; or this tool is so amazing i can't even.&lt;/p&gt; &lt;p&gt;It's getting really hard to filter through them and read through the meaningful posts or actual local content.&lt;/p&gt; &lt;p&gt;We need to either add a new tag for slop or ban it altogether because the sub is slowly turning into &amp;quot;omg this tool is clawdbot 2.0&amp;quot; or some guy trying to sell his half finished project that clauded wrote for him on a weekend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemontatox"&gt; /u/Daemontatox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T21:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr4p4x</id>
    <title>Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself.</title>
    <updated>2026-01-30T12:55:38+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt; &lt;img alt="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." src="https://external-preview.redd.it/MnNnNHZ6eGNoaGdnMcC0w-E97YmQ2Bn80LEN79By6gOnSLJ7DXbqces3JuUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189015efd04b82b6e73fa3d8be460d38d65659e4" title="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Forbes on YouTube: Yann LeCun Gives Unfiltered Take On The Future Of AI In Davos: &lt;a href="https://www.youtube.com/watch?v=MWMe7yjPYpE"&gt;https://www.youtube.com/watch?v=MWMe7yjPYpE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video by vitrupo on ùïè: &lt;a href="https://x.com/vitrupo/status/2017218170273313033"&gt;https://x.com/vitrupo/status/2017218170273313033&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n31pvrxchhgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T12:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsfctq</id>
    <title>[vLLM Office Hours #42] Deep Dive Into the vLLM CPU Offloading Connector - January 29, 2026</title>
    <updated>2026-01-31T21:48:28+00:00</updated>
    <author>
      <name>/u/Agreeable-Market-692</name>
      <uri>https://old.reddit.com/user/Agreeable-Market-692</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsfctq/vllm_office_hours_42_deep_dive_into_the_vllm_cpu/"&gt; &lt;img alt="[vLLM Office Hours #42] Deep Dive Into the vLLM CPU Offloading Connector - January 29, 2026" src="https://external-preview.redd.it/mJAENQiCvOzWn8ByHg4lRPDbMtsgCaVFUCmOL6Esd-k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c397f6c6eec47270f68d6a197c2bb77ae4b6a95" title="[vLLM Office Hours #42] Deep Dive Into the vLLM CPU Offloading Connector - January 29, 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I didn't see this posted here yet and it seems like a lot of people don't even know about this feature or the few who have posted about it had some issues with it a while back. Just want to raise awareness this feature is constantly evolving. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Market-692"&gt; /u/Agreeable-Market-692 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=LFnvDv1Drrw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsfctq/vllm_office_hours_42_deep_dive_into_the_vllm_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsfctq/vllm_office_hours_42_deep_dive_into_the_vllm_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T21:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrmu2v</id>
    <title>What shoddy development looks like</title>
    <updated>2026-01-31T00:12:53+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"&gt; &lt;img alt="What shoddy development looks like" src="https://preview.redd.it/9l7wwnsu6kgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a3c02e836b0951ddacdc25d64410b0f33a6b6e4" title="What shoddy development looks like" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9l7wwnsu6kgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T00:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsaath</id>
    <title>Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</title>
    <updated>2026-01-31T18:35:51+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;*Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2601.21590"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsaath/scalable_power_sampling_unlocking_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsaath/scalable_power_sampling_unlocking_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T18:35:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsegan</id>
    <title>llama.cpp RPC: 4√ó3090 box + Strix Halo 128GB (sanity check)</title>
    <updated>2026-01-31T21:13:10+00:00</updated>
    <author>
      <name>/u/CloudEquivalent7296</name>
      <uri>https://old.reddit.com/user/CloudEquivalent7296</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a game pc (Gigabyte X670 with a 7950X) on which i should be able to connect a 4090 and 3√ó RTX 3090 externally using MINIS FORUM DEG1 / oculink, so 96GB VRAM + 192GB RAM&lt;/p&gt; &lt;p&gt;I‚Äôm considering adding 1 - 2x AMD Strix Halo 128GB (Bosgame M5) as a llama.cpp RPC workers (not for speed, mainly to fit larger models).&lt;/p&gt; &lt;p&gt;Im planning to connect them using a 25GbE Mellanox.&lt;/p&gt; &lt;p&gt;The goal is to be able to run somewhat bigger models (e.g. ~671B Q4-ish or ~1T @ ~3-bit) by pooling memory via RPC.&lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Anyone tried something similar before? How did it perform? Any expected TPS hit vs single host? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any gotchas with heterogeneous CUDA (3090s) + ROCm (Strix) RPC?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What‚Äôs the best device split strategy to minimize network bottlenecks?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;alternatively, i could also add a 3090 to each strix? Would that work in this setup?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I've seen posts on multiple halo's and adding an external gpu to a halo, but not for something similar to this... probably for a reason, im kinda new to this all so go easy on me :D&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CloudEquivalent7296"&gt; /u/CloudEquivalent7296 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsegan/llamacpp_rpc_43090_box_strix_halo_128gb_sanity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsegan/llamacpp_rpc_43090_box_strix_halo_128gb_sanity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsegan/llamacpp_rpc_43090_box_strix_halo_128gb_sanity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T21:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qscc4n</id>
    <title>Benchmarks are good for open source AI</title>
    <updated>2026-01-31T19:51:27+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see a lot of hate for benchmarks, particularly a certain one, Artificial Analysis.&lt;/p&gt; &lt;p&gt;A comprehensive, cross-domain benchmark with several transparent and independently verifiable subscores, like AA, is a fine place to start a conversation comparing models, far better than many commonly accepted statements like &amp;quot;GPT 5.2 Thinking is better than any open source model.&amp;quot; &lt;/p&gt; &lt;p&gt;Ignoring benchmarks is bad for the open source community. Many proprietary models enjoy a mystique that benchmarks effectively dismantle.&lt;/p&gt; &lt;p&gt;Because things are developing so fast, it's important to accurately assess performance gaps rather than glaze the flavor of the month proprietary model. The fact is that there was no model last summer that matches Kimi K2.5 across benchmarks (or my personal battery of tests) and the idea that open source llms are a year behind closed is a dangerous falsehood.&lt;/p&gt; &lt;p&gt;Ideally comparisons should be intra-domain rather than a search for the &amp;quot;smartest model&amp;quot; but if we must make broad comparisons (for example, to explain the ai race to AI naive people) we should consider what difficult-to-game benchmarks like SWE Re-bench or Humanity's Last Exam are telling us. &lt;/p&gt; &lt;p&gt;Benchmarks will also keep getting better. Right now AA's top models align remarkable closely with user consensus, which hasn't always been the case: Anthropic used to score much more poorly than reputation would suggest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qscc4n/benchmarks_are_good_for_open_source_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qscc4n/benchmarks_are_good_for_open_source_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qscc4n/benchmarks_are_good_for_open_source_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T19:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrkb1b</id>
    <title>How was GPT-OSS so good?</title>
    <updated>2026-01-30T22:31:44+00:00</updated>
    <author>
      <name>/u/xt8sketchy</name>
      <uri>https://old.reddit.com/user/xt8sketchy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been messing around with a lot of local LLMs (120b and under) recently, and while some of them excel at specific things, none of them feel quite as good as GPT-OSS 120b all-around.&lt;/p&gt; &lt;p&gt;The model is 64GB at full precision, is BLAZING fast, and is pretty good at everything. It's consistent, it calls tools properly, etc.&lt;/p&gt; &lt;p&gt;But it's sort of old... it's been so long since GPT-OSS came out and we haven't really had a decent all-around open-weights/source replacement for it (some may argue GLM4.5 Air, but I personally feel like that model is only really better in agentic software dev, and lags behind in everything else. It's also slower and larger at full precision.)&lt;/p&gt; &lt;p&gt;I'm no expert when it comes to how LLM training/etc works, so forgive me if some of my questions are dumb, but:&lt;br /&gt; - Why don't people train more models in 4-bit natively, like GPT-OSS? Doesn't it reduce training costs? Is there some downside I'm not thinking of?&lt;br /&gt; - I know GPT-OSS was fast in part due to it being A3B, but there are plenty of smaller, dumber, NEWER A3B models that are much slower. What else makes it so fast? Why aren't we using what we learned from GPT-OSS in newer models?&lt;br /&gt; - What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xt8sketchy"&gt; /u/xt8sketchy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T22:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsd1u9</id>
    <title>LuxTTS - 150x real time TTS w/ voice cloning</title>
    <updated>2026-01-31T20:18:37+00:00</updated>
    <author>
      <name>/u/ChromaBroma</name>
      <uri>https://old.reddit.com/user/ChromaBroma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latency is often the issue with TTS models - making them borderline unusable for local agents/chatbots on consumer hardware. Those that excel at latency often fall off a cliff when it comes to general quality. &lt;/p&gt; &lt;p&gt;LuxTTS is not perfect, so let's get that out of the way, but IMO it's one of the better options that deliver ultra low latency and an acceptable quality (specifically re voice cloning). &lt;/p&gt; &lt;p&gt;I've tested it locally w/ voice cloning on a RTX 5090. I haven't even optimised it (as it's just running off PyTorch on the GPU) but the delay is so minimal that I might not even bother with further optimisations. &lt;/p&gt; &lt;p&gt;Github&lt;br /&gt; &lt;a href="https://github.com/ysharma3501/LuxTTS"&gt;https://github.com/ysharma3501/LuxTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface&lt;br /&gt; &lt;a href="https://huggingface.co/YatharthS/LuxTTS"&gt;https://huggingface.co/YatharthS/LuxTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/YatharthS/LuxTTS"&gt;https://huggingface.co/spaces/YatharthS/LuxTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyways thanks to the creators. I might replace chatterbox turbo with this TTS. More testing is needed but my initial impressions are quite good! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChromaBroma"&gt; /u/ChromaBroma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsd1u9/luxtts_150x_real_time_tts_w_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsd1u9/luxtts_150x_real_time_tts_w_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsd1u9/luxtts_150x_real_time_tts_w_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T20:18:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsbkpe</id>
    <title>M4 Max 128 GB vs Strix halo 128 GB</title>
    <updated>2026-01-31T19:22:46+00:00</updated>
    <author>
      <name>/u/dever121</name>
      <uri>https://old.reddit.com/user/dever121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;Which one is the best device for inference: Mac studio 128 GB vs. GMKtec EVO-X2 AI Mini PC Ryzen Al Max+ 395 (128 GB). I am looking for a prod environment, so speed is a must, plus sometimes small fine-tuning jobs are also required.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dever121"&gt; /u/dever121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbkpe/m4_max_128_gb_vs_strix_halo_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbkpe/m4_max_128_gb_vs_strix_halo_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsbkpe/m4_max_128_gb_vs_strix_halo_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T19:22:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs9zaw</id>
    <title>LLMs are great until you point them at actual company data</title>
    <updated>2026-01-31T18:24:21+00:00</updated>
    <author>
      <name>/u/jowers15</name>
      <uri>https://old.reddit.com/user/jowers15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You know the drill - connect to your CRM, ERP, whatever legacy system management swears is &amp;quot;mission critical.&amp;quot; That part? Done in an afternoon.&lt;/p&gt; &lt;p&gt;Then you actually look at the data. Fields named things like custom_attribute_2847. Tables that reference other tables that reference other tables. Documentation that was last updated when flip phones were cool.&lt;/p&gt; &lt;p&gt;And when you try to feed this into an LLM for anything useful? It just generates confidently wrong answers because it has no idea that &amp;quot;status_code_5&amp;quot; means &amp;quot;pending executive approval&amp;quot; in your specific workflow.&lt;/p&gt; &lt;p&gt;I've been reading about &lt;a href="https://thenewstack.io/how-precog-adds-business-context-to-make-enterprise-data-ai-ready/"&gt;this approach to adding business context&lt;/a&gt; earlier in the pipeline, but honestly - what are people actually doing here?&lt;/p&gt; &lt;p&gt;Manual metadata tagging? Knowledge graphs? Just... really good prompts?&lt;/p&gt; &lt;p&gt;Would love to know what's working for others because right now it feels like we're all just crossing our fingers and hoping.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jowers15"&gt; /u/jowers15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs9zaw/llms_are_great_until_you_point_them_at_actual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs9zaw/llms_are_great_until_you_point_them_at_actual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs9zaw/llms_are_great_until_you_point_them_at_actual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T18:24:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrwo9v</id>
    <title>Here it goes</title>
    <updated>2026-01-31T08:12:32+00:00</updated>
    <author>
      <name>/u/gotkush</name>
      <uri>https://old.reddit.com/user/gotkush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"&gt; &lt;img alt="Here it goes" src="https://preview.redd.it/pchjv5z88ngg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46ff31a155e1a7011f67f91d666503ef5cbcdf51" title="Here it goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My friend sold me his mining unit that he never got to use. He had it at his mom‚Äôs house and his mom moved out of town so he let me keep it. Was gonna part it out but I think it‚Äôs my new project. It has 8 RTx 3090 which has 24gbvram I would just need to upgrade the mobo cpu ram and the est j found was around 2500 for mobo 5900ryzen 256gb ram. It has 4 1000w power, would just need to get 8 pci risers so i can have each gou run at pcie4.0 x16. What donyoi guys think ? U think its over kill, im bery interested in havin my own ai sandbkx. Wouldnlike to get eveyones r thoughts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gotkush"&gt; /u/gotkush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pchjv5z88ngg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T08:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrzyaz</id>
    <title>I found that MXFP4 has lower perplexity than Q4_K_M and Q4_K_XL.</title>
    <updated>2026-01-31T11:27:30+00:00</updated>
    <author>
      <name>/u/East-Engineering-653</name>
      <uri>https://old.reddit.com/user/East-Engineering-653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post was originally written in Korean and then translated into English using ChatGPT.&lt;br /&gt; Hello, I am currently serving LLM models using a Tesla P40 and llama.cpp. When running models in the 30‚Äì32B range, I usually rely on 4-bit quantization. Until now, I primarily used Q4_K_XL, and if Q4_K_XL was not available, I used Q4_K_M instead. I initially avoided MXFP4 quantization because, compared to other 4-bit quantization methods, it has a smaller size, so I naturally assumed its accuracy would be lower. However, out of curiosity sparked by MXFP4‚Äôs fast speed, I compared Q4_K_M, Q4_K_XL, and MXFP4 quantization methods for the GLM-4.7-Flash and Nemotron-3-nano models using the &lt;code&gt;llama-perplexity&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;Below are the commands used, along with the Python code and command used to generate the dataset. The dataset generation command was created using ChatGPT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import argparse import os import re import sys import urllib.request from pathlib import Path import random def download(url: str, dst: Path) -&amp;gt; None: dst.parent.mkdir(parents=True, exist_ok=True) with urllib.request.urlopen(url) as r, open(dst, &amp;quot;wb&amp;quot;) as f: f.write(r.read()) def normalize_text(text: str, mode: str) -&amp;gt; str: text = text.replace(&amp;quot;\r\n&amp;quot;, &amp;quot;\n&amp;quot;).replace(&amp;quot;\r&amp;quot;, &amp;quot;\n&amp;quot;) if mode == &amp;quot;ppl&amp;quot;: text = re.sub(r&amp;quot;\n\s*\n+&amp;quot;, &amp;quot;\n&amp;quot;, text) text = re.sub(r&amp;quot;[ \t]+&amp;quot;, &amp;quot; &amp;quot;, text) text = text.strip() + &amp;quot;\n&amp;quot; return text if mode == &amp;quot;line&amp;quot;: lines = [] for line in text.split(&amp;quot;\n&amp;quot;): line = line.strip() if not line: continue line = re.sub(r&amp;quot;[ \t]+&amp;quot;, &amp;quot; &amp;quot;, line) lines.append(line) return &amp;quot;\n&amp;quot;.join(lines) + &amp;quot;\n&amp;quot; raise ValueError(f&amp;quot;unknown mode: {mode}&amp;quot;) def take_prefix(text: str, max_chars: int | None) -&amp;gt; str: if max_chars is None: return text if max_chars &amp;lt;= 0: return &amp;quot;&amp;quot; return text[:max_chars] def sample_lines(text: str, n_lines: int, seed: int) -&amp;gt; str: random.seed(seed) lines = [ln for ln in text.split(&amp;quot;\n&amp;quot;) if ln.strip()] if n_lines &amp;lt;= 0 or n_lines &amp;gt;= len(lines): return &amp;quot;\n&amp;quot;.join(lines) + &amp;quot;\n&amp;quot; sampled = random.sample(lines, n_lines) return &amp;quot;\n&amp;quot;.join(sampled) + &amp;quot;\n&amp;quot; def main(): ap = argparse.ArgumentParser() g = ap.add_mutually_exclusive_group(required=True) g.add_argument(&amp;quot;--url&amp;quot;, help=&amp;quot;download source url&amp;quot;) g.add_argument(&amp;quot;--infile&amp;quot;, help=&amp;quot;local input file path&amp;quot;) ap.add_argument(&amp;quot;--out&amp;quot;, required=True, help=&amp;quot;output text file path&amp;quot;) ap.add_argument(&amp;quot;--mode&amp;quot;, choices=[&amp;quot;ppl&amp;quot;, &amp;quot;line&amp;quot;], default=&amp;quot;ppl&amp;quot;, help=&amp;quot;ppl: keep newlines but collapse blanks/spaces, line: one sentence per line style&amp;quot;) ap.add_argument(&amp;quot;--max-chars&amp;quot;, type=int, default=None, help=&amp;quot;optional: cut the output to first N characters (fast/low-memory eval)&amp;quot;) ap.add_argument(&amp;quot;--sample-lines&amp;quot;, type=int, default=None, help=&amp;quot;optional: sample N non-empty lines uniformly (good for quick comparison)&amp;quot;) ap.add_argument(&amp;quot;--seed&amp;quot;, type=int, default=42) args = ap.parse_args() out_path = Path(args.out) if args.url: tmp = out_path.with_suffix(out_path.suffix + &amp;quot;.download&amp;quot;) download(args.url, tmp) in_path = tmp else: in_path = Path(args.infile) try: raw = in_path.read_text(encoding=&amp;quot;utf-8&amp;quot;, errors=&amp;quot;replace&amp;quot;) except Exception as e: print(f&amp;quot;failed to read input: {e}&amp;quot;, file=sys.stderr) sys.exit(1) text = normalize_text(raw, args.mode) if args.sample_lines is not None: text = sample_lines(text, args.sample_lines, args.seed) text = take_prefix(text, args.max_chars) out_path.parent.mkdir(parents=True, exist_ok=True) out_path.write_text(text, encoding=&amp;quot;utf-8&amp;quot;) if args.url: try: os.remove(in_path) except OSError: pass print(f&amp;quot;wrote: {out_path} ({out_path.stat().st_size} bytes)&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 wikitext_prep.py \ --url https://cosmo.zip/pub/datasets/wikitext-2-raw/wiki.test.raw \ --out /data/wikitext2_test.txt \ --mode ppl \ --max-chars 2000000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using the command below, I measured the perplexity of the quantized models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity -m modelname.gguf -f wikitext2_test.txt -c 32768 -b 4096 -fa on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The table below summarizes the test results, which were also organized using ChatGPT. The actual &lt;code&gt;llama-perplexity&lt;/code&gt; output is quite long, so it is attached separately below. For reference, Q4_K_M and Q4_K_XL were measured simultaneously, and after a llama.cpp update, Q4_K_XL and MXFP4 were measured simultaneously. Because the testing time was very long and the perplexity of Q4_K_XL was similar before and after the update, I assumed that the perplexity of Q4_K_M would also not be significantly affected by build changes.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Q4_K_M (Unsloth)&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL (previous)&lt;/th&gt; &lt;th align="left"&gt;MXFP4_MOE&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL (current)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp build&lt;/td&gt; &lt;td align="left"&gt;7803&lt;/td&gt; &lt;td align="left"&gt;7803&lt;/td&gt; &lt;td align="left"&gt;7896&lt;/td&gt; &lt;td align="left"&gt;7896&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF file type&lt;/td&gt; &lt;td align="left"&gt;Q4_K ‚Äì Medium&lt;/td&gt; &lt;td align="left"&gt;Q4_K ‚Äì Medium&lt;/td&gt; &lt;td align="left"&gt;MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;Q4_K ‚Äì Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;16.31 GiB&lt;/td&gt; &lt;td align="left"&gt;15.79 GiB&lt;/td&gt; &lt;td align="left"&gt;16.31 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BPW&lt;/td&gt; &lt;td align="left"&gt;4.89&lt;/td&gt; &lt;td align="left"&gt;4.68&lt;/td&gt; &lt;td align="left"&gt;4.53&lt;/td&gt; &lt;td align="left"&gt;4.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (final)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;16.1745 ¬± 0.1870&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.8605 ¬± 0.1823&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;10.7235 ¬± 0.1052&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.7309 ¬± 0.1803&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt eval speed&lt;/td&gt; &lt;td align="left"&gt;64.39 tok/s&lt;/td&gt; &lt;td align="left"&gt;64.37 tok/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68.20 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.73 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ms/token&lt;/td&gt; &lt;td align="left"&gt;15.53 ms&lt;/td&gt; &lt;td align="left"&gt;15.54 ms&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.66 ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.76 ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Time per pass (ETA)&lt;/td&gt; &lt;td align="left"&gt;529.38 s&lt;/td&gt; &lt;td align="left"&gt;530.05 s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;501.55 s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;502.66 s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU self (total)&lt;/td&gt; &lt;td align="left"&gt;20811 MiB&lt;/td&gt; &lt;td align="left"&gt;20056 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;17874 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;18552 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU model buffer&lt;/td&gt; &lt;td align="left"&gt;17284.84 MiB&lt;/td&gt; &lt;td align="left"&gt;16529.37 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15852.01 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16529.37 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KV cache size&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3196 MiB&lt;/strong&gt; (K 1692 + V 1504)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3196 MiB&lt;/strong&gt; (K 1692 + V 1504)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1692 MiB&lt;/strong&gt; (K 1692 + V 0)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1692 MiB&lt;/strong&gt; (K 1692 + V 0)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU free (log-based)&lt;/td&gt; &lt;td align="left"&gt;3406 MiB&lt;/td&gt; &lt;td align="left"&gt;4162 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;6342 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5666 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Load time&lt;/td&gt; &lt;td align="left"&gt;9.90 s&lt;/td&gt; &lt;td align="left"&gt;9.55 s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.13 s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;43.72 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mmap / direct_io&lt;/td&gt; &lt;td align="left"&gt;mmap off / direct_io on&lt;/td&gt; &lt;td align="left"&gt;mmap off / direct_io on&lt;/td&gt; &lt;td align="left"&gt;mmap on / direct_io off&lt;/td&gt; &lt;td align="left"&gt;mmap on / direct_io off&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;[1]&lt;/th&gt; &lt;th align="left"&gt;[2]&lt;/th&gt; &lt;th align="left"&gt;[3]&lt;/th&gt; &lt;th align="left"&gt;[4]&lt;/th&gt; &lt;th align="left"&gt;[5]&lt;/th&gt; &lt;th align="left"&gt;[6]&lt;/th&gt; &lt;th align="left"&gt;Final PPL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;15.2952&lt;/td&gt; &lt;td align="left"&gt;15.1950&lt;/td&gt; &lt;td align="left"&gt;15.7101&lt;/td&gt; &lt;td align="left"&gt;14.8037&lt;/td&gt; &lt;td align="left"&gt;14.5891&lt;/td&gt; &lt;td align="left"&gt;16.1745&lt;/td&gt; &lt;td align="left"&gt;16.1745 ¬± 0.1870&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL (previous)&lt;/td&gt; &lt;td align="left"&gt;14.7572&lt;/td&gt; &lt;td align="left"&gt;14.4954&lt;/td&gt; &lt;td align="left"&gt;15.0386&lt;/td&gt; &lt;td align="left"&gt;14.1713&lt;/td&gt; &lt;td align="left"&gt;14.1425&lt;/td&gt; &lt;td align="left"&gt;15.8605&lt;/td&gt; &lt;td align="left"&gt;15.8605 ¬± 0.1823&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;10.1764&lt;/td&gt; &lt;td align="left"&gt;10.1296&lt;/td&gt; &lt;td align="left"&gt;10.4917&lt;/td&gt; &lt;td align="left"&gt;9.8666&lt;/td&gt; &lt;td align="left"&gt;9.8629&lt;/td&gt; &lt;td align="left"&gt;10.7235&lt;/td&gt; &lt;td align="left"&gt;10.7235 ¬± 0.1052&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL (current)&lt;/td&gt; &lt;td align="left"&gt;14.4241&lt;/td&gt; &lt;td align="left"&gt;14.2673&lt;/td&gt; &lt;td align="left"&gt;14.8671&lt;/td&gt; &lt;td align="left"&gt;14.0460&lt;/td&gt; &lt;td align="left"&gt;14.0444&lt;/td&gt; &lt;td align="left"&gt;15.7309&lt;/td&gt; &lt;td align="left"&gt;15.7309 ¬± 0.1803&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Below is a table comparing MXFP4 and Q4_K_XL quantization methods on the Nemotron-3-nano model. This table was also created using ChatGPT.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Q4_K_XL (previous)&lt;/th&gt; &lt;th align="left"&gt;MXFP4 (current)&lt;/th&gt; &lt;th align="left"&gt;Change (MXFP4 ‚àí Q4_K_XL)&lt;/th&gt; &lt;th align="left"&gt;Meaning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;7.7090&lt;/td&gt; &lt;td align="left"&gt;7.5294&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-0.1796&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;MXFP4 is lower ‚Üí based on this corpus, ‚Äúless accuracy loss (or more accurate)‚Äù&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL error (¬±)&lt;/td&gt; &lt;td align="left"&gt;0.05361&lt;/td&gt; &lt;td align="left"&gt;0.05198&lt;/td&gt; &lt;td align="left"&gt;-0.00163&lt;/td&gt; &lt;td align="left"&gt;Uncertainty is nearly identical&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt eval speed&lt;/td&gt; &lt;td align="left"&gt;763.26 tok/s&lt;/td&gt; &lt;td align="left"&gt;797.79 tok/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+34.53 tok/s (+4.5%)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;MXFP4 is slightly faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Time per pass&lt;/td&gt; &lt;td align="left"&gt;24.74 s/pass&lt;/td&gt; &lt;td align="left"&gt;23.45 s/pass&lt;/td&gt; &lt;td align="left"&gt;-1.29 s/pass&lt;/td&gt; &lt;td align="left"&gt;MXFP4 is slightly shorter&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU model memory&lt;/td&gt; &lt;td align="left"&gt;21537 MiB&lt;/td&gt; &lt;td align="left"&gt;16782 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-4755 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;MXFP4 uses &lt;strong&gt;significantly less model memory&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU free VRAM&lt;/td&gt; &lt;td align="left"&gt;2286 MiB&lt;/td&gt; &lt;td align="left"&gt;7040 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+4754 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Available VRAM increases greatly&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU context memory&lt;/td&gt; &lt;td align="left"&gt;143 MiB&lt;/td&gt; &lt;td align="left"&gt;143 MiB&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;Same due to identical &lt;code&gt;n_ctx&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU compute buffer&lt;/td&gt; &lt;td align="left"&gt;271 MiB&lt;/td&gt; &lt;td align="left"&gt;271 MiB&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;Same&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Host usage (total)&lt;/td&gt; &lt;td align="left"&gt;268 MiB&lt;/td&gt; &lt;td align="left"&gt;394 MiB&lt;/td&gt; &lt;td align="left"&gt;+126 MiB&lt;/td&gt; &lt;td align="left"&gt;Difference is small and of limited significance&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I rewrote this post to add the Nemotron-3-nano benchmark, and in the previous post, one user commented that perplexity and tool calling or coding are completely different domains. They mentioned that using the HumanEval benchmark would provide values more directly related to tool calling and coding performance. If I get the chance, I plan to test again using the HumanEval benchmark in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qrwnd4/comment/o2rape9/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qrwnd4/comment/o2rape9/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To be honest, after seeing these benchmark results, I hoped that perplexity would be directly related to coding and tool calling performance, so it is a bit disappointing.&lt;br /&gt; If anyone has other opinions, I would appreciate it if you could share them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Engineering-653"&gt; /u/East-Engineering-653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T11:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs27hf</id>
    <title>g-HOOT in the Machine</title>
    <updated>2026-01-31T13:21:43+00:00</updated>
    <author>
      <name>/u/TheVeryNearFuture</name>
      <uri>https://old.reddit.com/user/TheVeryNearFuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt; &lt;img alt="g-HOOT in the Machine" src="https://preview.redd.it/z78lvao9rogg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae89cf23b154560e4ea34ce2fff5ea8a457a781b" title="g-HOOT in the Machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.14805"&gt;https://arxiv.org/abs/2507.14805&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheVeryNearFuture"&gt; /u/TheVeryNearFuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z78lvao9rogg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrsy4q</id>
    <title>How close are open-weight models to "SOTA"? My honest take as of today, benchmarks be damned.</title>
    <updated>2026-01-31T04:49:42+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"&gt; &lt;img alt="How close are open-weight models to &amp;quot;SOTA&amp;quot;? My honest take as of today, benchmarks be damned." src="https://preview.redd.it/k38sg20q7mgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1d06005e56d7a9be9a1c820f7096aa2805c52dc" title="How close are open-weight models to &amp;quot;SOTA&amp;quot;? My honest take as of today, benchmarks be damned." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k38sg20q7mgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T04:49:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsenpy</id>
    <title>Don‚Äôt buy b60 for LLMs</title>
    <updated>2026-01-31T21:21:10+00:00</updated>
    <author>
      <name>/u/damirca</name>
      <uri>https://old.reddit.com/user/damirca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kinda regret buying b60. I thought that 24gb for 700 eur is a great deal, but the reality is completely different.&lt;/p&gt; &lt;p&gt;For starters, I live with a custom compiled kernel with the patch from an Intel dev to solve ffmpeg crashes.&lt;/p&gt; &lt;p&gt;Then I had to install the card into a windows machine in order to get GPU firmware updated (under Linux one need v2.0.19 of fwupd which is not available in Ubuntu yet) to solve the crazy fan speed on the b60 even when the temp of the gpu is 30 degrees Celsius.&lt;/p&gt; &lt;p&gt;But even after solving all of this, the actual experience doing local LLM on b60 is meh.&lt;/p&gt; &lt;p&gt;On llama.cpp the card goes crazy every time it does inference: fans go super high then low, the high again. The speed is about 10-15tks at best in models like mistral 14b. The noise level is just unbearable.&lt;/p&gt; &lt;p&gt;So the only reliable way is intel‚Äôs llm-scaler, but as of now it‚Äôs based on vllm 0.11.1 whereas latest version of vllm is 0.15. So Intel is like 6 months behind which is an eternity in this AI bubble times. For example any of new mistral models are not supported and one cannot run them on vanilla vllm too.&lt;/p&gt; &lt;p&gt;With llm-scaler the behavior of the card is ok: when it‚Äôs doing inference the fan goes louder and stays louder as long is it‚Äôs needed. The speed is like 20-25 tks on qwen3 VL 8b. However there are only some models that work with llm-scaler and most of them only with fp8, so for example qwen3 VL 8b after some requests processed with 16k length takes 20gb. That kinda bad: you have 24gb of vram but you cannot run normally 30b model with q4 quant and has to stick with 8b model with fp8.&lt;/p&gt; &lt;p&gt;Overall I think XFX 7900XTX would have been much better deal: same 24gb, 2x faster, in Dec the price was only 50 eur more than b60, it can run newest models with newest llama.cpp versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/damirca"&gt; /u/damirca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T21:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
