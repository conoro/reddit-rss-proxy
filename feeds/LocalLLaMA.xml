<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-09T12:53:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r039cd</id>
    <title>Pocket LLM: Chat offline on device all private | AI</title>
    <updated>2026-02-09T12:45:38+00:00</updated>
    <author>
      <name>/u/Dismal-Perception-29</name>
      <uri>https://old.reddit.com/user/Dismal-Perception-29</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r039cd/pocket_llm_chat_offline_on_device_all_private_ai/"&gt; &lt;img alt="Pocket LLM: Chat offline on device all private | AI" src="https://external-preview.redd.it/djLQaBLcllYpDgm6vwQbcgk60czLvE6bEzI00vMysRE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3f3b61f4825a7d53b84be7a93b04cdd5b8c1f09" title="Pocket LLM: Chat offline on device all private | AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Think Local - Private AI, On Your Device &lt;/p&gt; &lt;p&gt;Run powerful AI models directly on your iPhone, iPad, and Mac - fully offline, fully private, and fully yours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dismal-Perception-29"&gt; /u/Dismal-Perception-29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://apps.apple.com/us/app/think-local-ai-private-chat/id6758632782"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r039cd/pocket_llm_chat_offline_on_device_all_private_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r039cd/pocket_llm_chat_offline_on_device_all_private_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:45:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzy8uu</id>
    <title>[Project] MCP Orchestrator - Turn one AI agent into a team with parallel sub-agents</title>
    <updated>2026-02-09T07:49:56+00:00</updated>
    <author>
      <name>/u/ask149</name>
      <uri>https://old.reddit.com/user/ask149</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I built an open-source MCP server that lets you spawn parallel AI sub-agents — think of it as turning one AI coding agent into a team.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Spawns up to 10 parallel sub-agents using Copilot CLI or Claude Code CLI&lt;/li&gt; &lt;li&gt;Passes file context to each agent (full file, summary, or grep mode)&lt;/li&gt; &lt;li&gt;Smart timeout selection based on MCP servers requested&lt;/li&gt; &lt;li&gt;Cross-platform: macOS, Linux, and Windows&lt;/li&gt; &lt;li&gt;Headless &amp;amp; programmatic — designed for AI-to-AI orchestration via MCP protocol&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example use case:&lt;/strong&gt; You give one prompt like &amp;quot;research job openings at Stripe, Google, and Meta&amp;quot; — the orchestrator fans that out to 3 parallel agents, each with their own MCP servers (e.g., Playwright for browser access), and aggregates results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt; &lt;code&gt;npm i @​ask149/mcp-orchestrator&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ask149/orchestrator"&gt;https://github.com/Ask149/orchestrator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Looking for dev feedback &amp;amp; contributions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What CLI backends would you want supported next? (e.g., Aider, Open Interpreter, local LLM CLIs)&lt;/li&gt; &lt;li&gt;Any ideas for improving the context-passing system?&lt;/li&gt; &lt;li&gt;What MCP server integrations would be most useful for your workflows?&lt;/li&gt; &lt;li&gt;PRs and issues welcome — check out CONTRIBUTING.md in the repo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a solo side project and I'd really appreciate any suggestions, code reviews, or feature ideas from this community. Not looking for donations — just want to build something useful with input from people who actually use these tools daily.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ask149"&gt; /u/ask149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzy8uu/project_mcp_orchestrator_turn_one_ai_agent_into_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzy8uu/project_mcp_orchestrator_turn_one_ai_agent_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzy8uu/project_mcp_orchestrator_turn_one_ai_agent_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T07:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzgvyh</id>
    <title>pwilkin is doing things</title>
    <updated>2026-02-08T18:38:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"&gt; &lt;img alt="pwilkin is doing things" src="https://external-preview.redd.it/LP9lWJIkvOFwEJy7i2edxqBM2iBmROue3pUEdiXyxYg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45fd4b46acdf1a22c62c7c684471a43354c1397" title="pwilkin is doing things" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19435"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T18:38:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzs0h9</id>
    <title>Final Destination, Hallucination Station. (Opus 4.6 hallucinates</title>
    <updated>2026-02-09T02:26:08+00:00</updated>
    <author>
      <name>/u/UnreasonableEconomy</name>
      <uri>https://old.reddit.com/user/UnreasonableEconomy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"&gt; &lt;img alt="Final Destination, Hallucination Station. (Opus 4.6 hallucinates" src="https://b.thumbs.redditmedia.com/4J2ROZIwjRUzPsoF71wpUEegN7DPzfyS-gevaPovW4g.jpg" title="Final Destination, Hallucination Station. (Opus 4.6 hallucinates" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: Ope, ate the title. TBH, IDK how the title should end. &amp;quot;We're all toast?&amp;quot;&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;This is just some napkin math.&lt;/p&gt; &lt;p&gt;Hallucination is of course the biggest thing holding back agentics, and if it's not solved within the next 24 months this whole hype train is going to smash into the buffer stop. It's not looking good.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/525cpl98rdig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=251ced00f0ee29ede414db448df8f062abd11e5a"&gt;https://preview.redd.it/525cpl98rdig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=251ced00f0ee29ede414db448df8f062abd11e5a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, local models lag behind by a wide margin, but even if we look at the SOTA (opus 4.6), it's still pretty harrowing.&lt;/p&gt; &lt;p&gt;On page 76 of the 4.6 system card (&lt;a href="https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a53f14cf5288.pdf"&gt;https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a53f14cf5288.pdf&lt;/a&gt;) they run SimpleQA, and give the model the option to abstain if it's uncertain. The top is how often the model is right, the bottom is how often it's right - how often it's wrong.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lxe7zoftpdig1.png?width=979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26d0d2574e47e8310a4ace9de1366bd64b271491"&gt;https://preview.redd.it/lxe7zoftpdig1.png?width=979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26d0d2574e47e8310a4ace9de1366bd64b271491&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let's interpret this charitably. Let's say the model is correct 50% of the time, and gets a net score of 25%.&lt;/p&gt; &lt;p&gt;That means that out of 100 tries, it gets 50 correct, confidently hallucinates at least 25, and correctly abstains from 25.&lt;/p&gt; &lt;p&gt;That means at least 1 out of 3 answers have no grounded basis, but the model doesn't know that.&lt;/p&gt; &lt;p&gt;In reality, it's much worse. Thinking+Effort: 46.2% correct, 7.8% net. 53.8% wrong, (46.2 - 7.8) = 38.4% confidently hallucinated, (100 - 46.2 - 38.4) 15.4% correctly abstained.&lt;/p&gt; &lt;p&gt;that means that approximately out of 5 times, it will know it doesn't know 2 times and hallucinate 3 times.&lt;/p&gt; &lt;p&gt;That means every time you ask an LLM to double check its' answer (assuming it was wrong because it doesn't know), the likelihood that the new answer is now worse is 60%, and assuming you even gave it an out, it would ask for help 40% of the time.&lt;/p&gt; &lt;p&gt;If you tell it to fix it, and give it tests, the probability that it will hallucinate &lt;em&gt;increases exponentially&lt;/em&gt; 1-(1-0.6)&lt;sup&gt;n,&lt;/sup&gt; and the probability that it will catch itself &lt;em&gt;decreases exponentially&lt;/em&gt; (0.4)&lt;sup&gt;n,&lt;/sup&gt; causing a token churn with zero yield.&lt;/p&gt; &lt;p&gt;This also explains why Thinking+Effort has a lower net yield than just Thinking.&lt;/p&gt; &lt;p&gt;TL;DR: whether a model can do any novel task right is a coin flip. If you give an agent the option to flip again, it'll turn into a gambling addict on your dime.&lt;/p&gt; &lt;p&gt;What we need is a model that reaches a net score &amp;gt;50%. But it looks like we're a long way off from that.&lt;/p&gt; &lt;p&gt;Clawd is just another iteration of autogpt/swarmgpt and all that stuff. When will people learn?&lt;/p&gt; &lt;p&gt;Thanks for coming to my draft of a ted talk.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnreasonableEconomy"&gt; /u/UnreasonableEconomy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r01hjn</id>
    <title>Help needed: running a local LLM with a custom prompt/memory (non-commercial)</title>
    <updated>2026-02-09T11:11:19+00:00</updated>
    <author>
      <name>/u/Disastrous-Way3174</name>
      <uri>https://old.reddit.com/user/Disastrous-Way3174</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I’m looking for someone with experience in local / open-source AI models (LLaMA, Mistral, Ollama, LM Studio, etc.).&lt;/p&gt; &lt;p&gt;I have built, over time, a structured corpus (texts, tone, interaction style, memory elements) with an AI model, and I would like help transposing this corpus into a local, open-source setup, for personal use.&lt;/p&gt; &lt;p&gt;This is not a commercial project.&lt;/p&gt; &lt;p&gt;It’s a personal, human, and creative exploration around continuity, memory, and dialogue with an AI system.&lt;/p&gt; &lt;p&gt;I don’t have financial means to pay for development work.&lt;/p&gt; &lt;p&gt;In exchange, I can offer time, gratitude, and genuine human reciprocity. I’m a trained psychologist and coach, if that is ever useful — but mostly, I’m looking for someone curious and kind.&lt;/p&gt; &lt;p&gt;If this resonates with you, feel free to reply or DM me.&lt;/p&gt; &lt;p&gt;Thank you for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Way3174"&gt; /u/Disastrous-Way3174 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01hjn/help_needed_running_a_local_llm_with_a_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01hjn/help_needed_running_a_local_llm_with_a_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r01hjn/help_needed_running_a_local_llm_with_a_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T11:11:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r00c94</id>
    <title>Local solution for TTS/SST using Raspberry + Hailo-10H</title>
    <updated>2026-02-09T10:02:51+00:00</updated>
    <author>
      <name>/u/RegularDude2024</name>
      <uri>https://old.reddit.com/user/RegularDude2024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody,&lt;/p&gt; &lt;p&gt;I am working on a local project enabling my system to work with local LLM using raspberry pi 5 + hailo-10H. &lt;/p&gt; &lt;p&gt;My target is to implement a local TTS/STT (Text To Speach / Speach To Text)--system with TTFT (Time To First Token) &amp;lt; 100ms.&lt;/p&gt; &lt;p&gt;My first test was to chat/stream one simple sentence and measure the performance of TTFT.&lt;/p&gt; &lt;p&gt;I am not happy with the performance results of TTFT using models like llama3.2:1b or qwen2:1.5b. It is round about between 350 ms and 500 ms.&lt;/p&gt; &lt;p&gt;Anyone of you have expericed some better model or system to be used locally?&lt;/p&gt; &lt;p&gt;Greetings!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegularDude2024"&gt; /u/RegularDude2024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r00c94/local_solution_for_ttssst_using_raspberry_hailo10h/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r00c94/local_solution_for_ttssst_using_raspberry_hailo10h/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r00c94/local_solution_for_ttssst_using_raspberry_hailo10h/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T10:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz23pp</id>
    <title>PR opened for Qwen3.5!!</title>
    <updated>2026-02-08T06:57:13+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt; &lt;img alt="PR opened for Qwen3.5!!" src="https://preview.redd.it/r10pwm02y7ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb19e2c9eac9c47e80b6a33b08c10d458c3fb6c0" title="PR opened for Qwen3.5!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43830/"&gt;https://github.com/huggingface/transformers/pull/43830/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking at the code at &lt;code&gt;src/transformers/models/qwen3_5/modeling_qwen3_5.py&lt;/code&gt;, it looks like Qwen3.5 series will have VLMs right off the bat!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r10pwm02y7ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T06:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r02ka4</id>
    <title>Agent that "watches" you browse, distills the logic via LLM, and survives UI changes.</title>
    <updated>2026-02-09T12:10:51+00:00</updated>
    <author>
      <name>/u/Ok_Owl_1414</name>
      <uri>https://old.reddit.com/user/Ok_Owl_1414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building scrapers and automation scripts for years, and I'm tired of the &amp;quot;cat and mouse&amp;quot; game. Every time the website updates its CSS or changes a &lt;code&gt;div&lt;/code&gt; ID, my script breaks.&lt;/p&gt; &lt;p&gt;Standard RPA records coordinates (brittle). Standard Agents (AutoGPT style) are too expensive/slow to reason from scratch every step.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Exogram&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Concept: &amp;quot;Procedural Memory&amp;quot; for Agents&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of hard-coding steps, Exogram works in 3 phases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Teach (The Spy):&lt;/strong&gt; It records your workflow (e.g., clicking through a messy ERP system). It doesn't just record coordinates; it captures the &lt;strong&gt;DOM context&lt;/strong&gt; and &lt;strong&gt;semantic intent&lt;/strong&gt; of what you clicked.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distill (The Alchemy):&lt;/strong&gt; It uses an LLM (Claude 3.5 / GPT-4o) to &amp;quot;distill&amp;quot; the raw logs into a &lt;strong&gt;heuristic rule&lt;/strong&gt; (SOP). &lt;ul&gt; &lt;li&gt;&lt;em&gt;Raw Log:&lt;/em&gt; &lt;code&gt;Click #btn-402&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Distilled Rule:&lt;/em&gt; &amp;quot;Find the primary action button labeled 'Export', usually located in the top-right container. Ignore popups with 'Subscribe' text.&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run (The Agent):&lt;/strong&gt; The agent executes using this &amp;quot;distilled memory&amp;quot;. &lt;strong&gt;I tested this by changing the button color and ID locally, and the agent still found it based on the semantic rule.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Eye:&lt;/strong&gt; &lt;code&gt;workflow-use&lt;/code&gt; (for recording DOM events)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hand:&lt;/strong&gt; &lt;code&gt;browser-use&lt;/code&gt; (Playwright wrapper)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Brain:&lt;/strong&gt; LangChain + Your LLM of choice (DeepSeek-V3 works great for the distillation part to save costs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I made this:&lt;/strong&gt; I wanted a middle ground between &amp;quot;dumb&amp;quot; Selenium scripts and &amp;quot;expensive&amp;quot; autonomous agents. This is an attempt to give agents &amp;quot;muscle memory.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; [&lt;a href="https://github.com/qingshanyuluo/exogram"&gt;https://github.com/qingshanyuluo/exogram&lt;/a&gt;] &lt;strong&gt;Demo:&lt;/strong&gt; [&lt;a href="https://github.com/user-attachments/assets/07af1f77-4344-4916-adfe-984a3626d105"&gt;https://github.com/user-attachments/assets/07af1f77-4344-4916-adfe-984a3626d105&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;It's still an MVP (v0.1), but I'd love to hear if this approach makes sense to you guys. Roast my code or star it if you like the idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Owl_1414"&gt; /u/Ok_Owl_1414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02ka4/agent_that_watches_you_browse_distills_the_logic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02ka4/agent_that_watches_you_browse_distills_the_logic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r02ka4/agent_that_watches_you_browse_distills_the_logic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:10:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz5uww</id>
    <title>Qwen3 Coder Next as first "usable" coding model &lt; 60 GB for me</title>
    <updated>2026-02-08T10:43:59+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried lots of &amp;quot;small&amp;quot; models &amp;lt; 60 GB in the past. GLM 4.5 Air, GLM 4.7 Flash, GPT OSS 20B and 120B, Magistral, Devstral, Apriel Thinker, previous Qwen coders, Seed OSS, QwQ, DeepCoder, DeepSeekCoder, etc. So what's different with Qwen3 Coder Next in OpenCode or in Roo Code with VSCodium?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: The reasoning models would often yet not always produce rather good results. However, now and then they'd enter reasoning loops despite correct sampling settings, leading to no results at all in a large over-night run. Aside from that the sometimes extensive reasoning takes quite some time for the multiple steps that OpenCode or Roo would induce, slowing down interactive work &lt;em&gt;a lot&lt;/em&gt;. Q3CN on the other hand is an instruct MoE model, doesn't have internal thinking loops and is relatively quick at generating tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality&lt;/strong&gt;: Other models occasionally botched the tool calls of the harness. This one seems to work reliably. Also I finally have the impression that this can handle a moderately complex codebase with a custom client &amp;amp; server, different programming languages, protobuf, and some quirks. It provided good answers to extreme multi-hop questions and made reliable full-stack changes. Well, almost. On Roo Code it was sometimes a bit lazy and needed a reminder to really go deep to achieve correct results. Other models often got lost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context size&lt;/strong&gt;: Coding on larger projects needs context. Most models with standard attention eat all your VRAM for breakfast. With Q3CN having 100k+ context is easy. A few other models also supported that already, yet there were drawbacks in the first two mentioned points.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I run the model this way:&lt;br /&gt; &lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m Qwen3-Coder-Next-UD-Q4_K_XL.gguf -ngl 99 -fa on -c 120000 --n-cpu-moe 29 --temp 0 --cache-ram 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This works well with 24 GB VRAM and 64 GB system RAM when there's (almost) nothing else on the GPU. Yields about 180 TPS prompt processing and 30 TPS generation speed for me.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temp 0&lt;/code&gt;? Yes, works well for instruct for me, no higher-temp &amp;quot;creativity&amp;quot; needed. Prevents the &lt;em&gt;very occasional&lt;/em&gt; issue that it outputs an unlikely (and incorrect) token when coding.&lt;/li&gt; &lt;li&gt;&lt;code&gt;cache-ram 0&lt;/code&gt;? The cache was supposed to be fast (30 ms), but I saw 3 second query/update times after each request. So I didn't investigate further and disabled it, as it's only one long conversation history in a single slot anyway.&lt;/li&gt; &lt;li&gt;&lt;code&gt;GGML_CUDA_GRAPH_OPT&lt;/code&gt;? Experimental option to get more TPS. Usually works, yet breaks processing with some models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;OpenCode vs. Roo Code&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Both solved things with the model, yet with OpenCode I've seen slightly more correct answers and solutions. But: Roo asks &lt;em&gt;by default&lt;/em&gt; about every single thing, even harmless things like running a syntax check via command line. This can be configured with an easy permission list to not stop the automated flow that often. OpenCode on the other hand just permits everything by default in code mode. One time it encountered an issue, uninstalled and reinstalled packages in an attempt of solving it, removed files and drove itself into a corner by breaking the dev environment. Too autonomous in trying to &amp;quot;get things done&amp;quot;, which doesn't work well on bleeding edge stuff that's not in the training set. Permissions can of course also be configured, but the default is &amp;quot;YOLO&amp;quot;.&lt;/p&gt; &lt;p&gt;Aside from that: Despite running with only a locally hosted model, and having disabled update checks and news downloads, OpenCode (Desktop version) tries to contact a whole lot of IPs on start-up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T10:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzytme</id>
    <title>Caret – A terminal tool to inspect and clean massive LLM datasets</title>
    <updated>2026-02-09T08:26:32+00:00</updated>
    <author>
      <name>/u/Mental_Figure_1130</name>
      <uri>https://old.reddit.com/user/Mental_Figure_1130</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt; &lt;img alt="Caret – A terminal tool to inspect and clean massive LLM datasets" src="https://preview.redd.it/ip091tcnifig1.png?width=140&amp;amp;height=63&amp;amp;auto=webp&amp;amp;s=7526987c073820123909e1af40dafb10c9ef19d9" title="Caret – A terminal tool to inspect and clean massive LLM datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I’ve been working on a CLI tool called &lt;a href="https://github.com/rouapps/caret"&gt;Caret&lt;/a&gt; because I was struggling to inspect large pre-training datasets efficiently.&lt;/p&gt; &lt;p&gt;The main issue I had was that opening 10GB+ JSONL or Parquet files usually crashed my editor (VS Code) or used too much RAM. I wanted something that felt like &lt;code&gt;less&lt;/code&gt; but understood the structure of LLM data, specifically for visualizing tokenization and finding bad data.&lt;/p&gt; &lt;p&gt;It’s written in Rust and uses memory-mapped I/O, so it opens files of basically any size instantly without loading them fully into RAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-Copy Open:&lt;/strong&gt; Uses &lt;code&gt;mmap&lt;/code&gt; to handle massive files. You can scroll through a 100GB dataset instantly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token X-Ray:&lt;/strong&gt; Toggles a view that visualizes exactly how your tokenizer (Tiktoken, Llama 3, GPT-2...) is splitting the text (see screenshot).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SimHash Deduplication:&lt;/strong&gt; Uses parallelized SimHash (with hardware &lt;code&gt;POPCNT&lt;/code&gt;) to find near-duplicates in your training data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parquet &amp;amp; CSV Support:&lt;/strong&gt; Handles binary formats natively without needing to convert them to JSONL first.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP Server:&lt;/strong&gt; I added an experimental MCP (Model Context Protocol) server. If you use Claude Desktop or Cursor, you can connect it to Caret to &amp;quot;chat&amp;quot; with your local dataset (e.g., &amp;quot;Find me 5 examples of bad JSON formatting in this file&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works under the hood:&lt;/strong&gt; Instead of reading the whole file, it builds a lightweight index of line offsets and maps the file into virtual memory. When you scroll, it slices the bytes directly from the OS page cache. For remote HuggingFace datasets, it fetches only the parquet metadata footer first and streams row groups on demand, so you don't have to download the full repo to check the data quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; If you have Rust installed:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/rouapps/caret.git cd caret &amp;amp;&amp;amp; cargo run --release -- path/to/data.jsonl &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It’s still early days, so I’d appreciate any feedback or issue reports if you try it on your datasets!&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/rouapps/caret"&gt;https://github.com/rouapps/caret&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ip091tcnifig1.png?width=1778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff35eda5fa5628659c5b0c7abf2f4903644419b"&gt;https://preview.redd.it/ip091tcnifig1.png?width=1778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff35eda5fa5628659c5b0c7abf2f4903644419b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mental_Figure_1130"&gt; /u/Mental_Figure_1130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r02xqc</id>
    <title>Bitnet.cpp - Inference framework for 1-bit (ternary) LLM's</title>
    <updated>2026-02-09T12:29:57+00:00</updated>
    <author>
      <name>/u/Academic_Wallaby7135</name>
      <uri>https://old.reddit.com/user/Academic_Wallaby7135</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;bitnet.cpp&lt;/strong&gt; is Microsoft’s official C++ inference framework for &lt;strong&gt;1-bit Large Language Models (LLMs)&lt;/strong&gt;, optimized for &lt;strong&gt;BitNet b1.58&lt;/strong&gt; and similar architectures. It supports &lt;strong&gt;fast, lossless inference&lt;/strong&gt; on both &lt;strong&gt;CPU&lt;/strong&gt; and &lt;strong&gt;GPU&lt;/strong&gt; (with NPU support planned), using highly optimized kernels for &lt;strong&gt;ternary quantized models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Officially Supported Models&lt;/strong&gt; (available on Hugging Face):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;BitNet-b1.58-2B-4T&lt;/strong&gt; (~2.4B params) – Optimized GGUF format for CPU/GPU inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bitnet_b1_58-large&lt;/strong&gt; (~0.7B params) – Lightweight variant for edge devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bitnet_b1_58-3B&lt;/strong&gt; (~3.3B params) – Larger model for higher accuracy tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Llama3-8B-1.58-100B-tokens&lt;/strong&gt; (~8B params) – LLaMA 3 adapted to 1.58-bit quantization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Falcon3 Family&lt;/strong&gt; (1B–10B params) – Instruction-tuned Falcon models in 1.58-bit format.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Falcon-E Family&lt;/strong&gt; (1B–3B params) – Energy-efficient Falcon variants.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Academic_Wallaby7135"&gt; /u/Academic_Wallaby7135 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02xqc/bitnetcpp_inference_framework_for_1bit_ternary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02xqc/bitnetcpp_inference_framework_for_1bit_ternary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r02xqc/bitnetcpp_inference_framework_for_1bit_ternary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrl2g</id>
    <title>Are there any alternatives to Open WebUI that don't have terrible UX?</title>
    <updated>2026-02-09T02:05:01+00:00</updated>
    <author>
      <name>/u/lostmsu</name>
      <uri>https://old.reddit.com/user/lostmsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Configuring Open WebUI is a nightmare.&lt;/p&gt; &lt;p&gt;Even if you managed to add a tool server and got tools to show up in UI (which is comparable to completing dark brotherhood quest in Skyrim in complexity), you have to enable it every fucking time you start a new chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostmsu"&gt; /u/lostmsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzo77z</id>
    <title>MiniMax M2.2 Coming Soon!</title>
    <updated>2026-02-08T23:22:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt; &lt;img alt="MiniMax M2.2 Coming Soon!" src="https://preview.redd.it/cj2as13ttcig1.png?width=140&amp;amp;height=19&amp;amp;auto=webp&amp;amp;s=0c0420fddf7b3160e28c4a7e5bea9abb03314341" title="MiniMax M2.2 Coming Soon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It found on their website code&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cj2as13ttcig1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9492b73dd14c581e30b35a5e64062f4ac7356a3f"&gt;https://preview.redd.it/cj2as13ttcig1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9492b73dd14c581e30b35a5e64062f4ac7356a3f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://cdn.hailuo.ai/mmx-agent/prod-web-va-0.1.746/_next/static/chunks/app/(pages"&gt;https://cdn.hailuo.ai/mmx-agent/prod-web-va-0.1.746/_next/static/chunks/app/(pages)/(base)/page-0cfae9566c3e528b.js&lt;/a&gt;/(base)/page-0cfae9566c3e528b.js)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T23:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzxgxp</id>
    <title>Qwen3.5 dense and MoE support on llama.cpp</title>
    <updated>2026-02-09T07:02:39+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spotted &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7973"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7973&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxgxp/qwen35_dense_and_moe_support_on_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxgxp/qwen35_dense_and_moe_support_on_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxgxp/qwen35_dense_and_moe_support_on_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T07:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r01sek</id>
    <title>POV: You left repetition_penalty at 1.0</title>
    <updated>2026-02-09T11:28:46+00:00</updated>
    <author>
      <name>/u/AurumDaemonHD</name>
      <uri>https://old.reddit.com/user/AurumDaemonHD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01sek/pov_you_left_repetition_penalty_at_10/"&gt; &lt;img alt="POV: You left repetition_penalty at 1.0" src="https://external-preview.redd.it/dXk0YW5vcW1lZ2lnMetm2yxWlv74oo2KCat6XpxnSQj55CMNYXwNFsRMpvok.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f734f7997c12fe223c17899a35ea1fb934c5f75" title="POV: You left repetition_penalty at 1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AurumDaemonHD"&gt; /u/AurumDaemonHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fuvcpoqmegig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01sek/pov_you_left_repetition_penalty_at_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r01sek/pov_you_left_repetition_penalty_at_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T11:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r01u46</id>
    <title>Ryzen + RTX: you might be wasting VRAM without knowing it (LLama Server)</title>
    <updated>2026-02-09T11:31:23+00:00</updated>
    <author>
      <name>/u/Medium-Technology-79</name>
      <uri>https://old.reddit.com/user/Medium-Technology-79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a pretty stupid mistake, but it’s &lt;em&gt;so&lt;/em&gt; easy to fall into it that I wanted to share it, hoping it might help someone else.&lt;/p&gt; &lt;p&gt;The workstation I use has a Ryzen 9 CPU with an integrated GPU, which I think is a very common setup.&lt;br /&gt; I also have an Nvidia RTX GPU installed in a PCIe slot.&lt;/p&gt; &lt;p&gt;My monitor was connected directly to the Nvidia GPU, which means Windows 11 uses it as the primary GPU (for example when opening a browser, watching YouTube, etc.).&lt;/p&gt; &lt;p&gt;In this configuration, Llama-Server does &lt;strong&gt;not&lt;/strong&gt; have access to the full VRAM of the Nvidia GPU, because part of it is already being used by the operating system for graphics. And when you’re close to the VRAM limit, this makes a &lt;em&gt;huge&lt;/em&gt; difference.&lt;/p&gt; &lt;p&gt;I discovered this completely by accident... I'm VRAM addicted!&lt;/p&gt; &lt;p&gt;After connecting the monitor to the motherboard and rebooting the PC, I was able to confirm that Llama-Server had access to &lt;strong&gt;all&lt;/strong&gt; of the precious VRAM.&lt;br /&gt; Using Windows Task Manager, you can see that the Nvidia GPU VRAM is completely free, while the integrated GPU VRAM is being used instead.&lt;/p&gt; &lt;p&gt;I know this isn’t anything revolutionary, but maybe someone else is making the same mistake without realizing it.&lt;/p&gt; &lt;p&gt;Just it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medium-Technology-79"&gt; /u/Medium-Technology-79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01u46/ryzen_rtx_you_might_be_wasting_vram_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01u46/ryzen_rtx_you_might_be_wasting_vram_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r01u46/ryzen_rtx_you_might_be_wasting_vram_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T11:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzsbn9</id>
    <title>StepFun is preparing a "bigger surprise" for Chinese New Year, and will also release Step-3.5-Flash-Base.</title>
    <updated>2026-02-09T02:40:54+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"&gt; &lt;img alt="StepFun is preparing a &amp;quot;bigger surprise&amp;quot; for Chinese New Year, and will also release Step-3.5-Flash-Base." src="https://preview.redd.it/zytph079tdig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c50a154f20b8f8f0e56f8e7c6353847588c73a1" title="StepFun is preparing a &amp;quot;bigger surprise&amp;quot; for Chinese New Year, and will also release Step-3.5-Flash-Base." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/21#698941a597b7256a083f94b6"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/21#698941a597b7256a083f94b6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They also mentioned discussions with Nvidia regarding NVFP4 and responded to questions about excessive token usage by stating they are working on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zytph079tdig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzwzqj</id>
    <title>ministral-3-3b is great model, give it a shot!</title>
    <updated>2026-02-09T06:35:18+00:00</updated>
    <author>
      <name>/u/FeiX7</name>
      <uri>https://old.reddit.com/user/FeiX7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I was experimenting the small models that can do tool calls effectively and can fit in 6GB Vram and I found ministral-3-3b.&lt;/p&gt; &lt;p&gt;Currently using it's instruct version with Q8 and it's accuracy to run tools written in skills md is generous.&lt;/p&gt; &lt;p&gt;I am curious about your use cases of this model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeiX7"&gt; /u/FeiX7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T06:35:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r015z4</id>
    <title>I managed to jailbreak 43 of 52 recent models</title>
    <updated>2026-02-09T10:52:45+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r015z4/i_managed_to_jailbreak_43_of_52_recent_models/"&gt; &lt;img alt="I managed to jailbreak 43 of 52 recent models" src="https://external-preview.redd.it/YTA3NHl0dmhyZGlnMUNU3vkEOynofhKg3zLh75rLSPZOaY5MGdNqMt8faW6e.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=326b5aa6d703c3059a3c89ab8668c2775aa7efc8" title="I managed to jailbreak 43 of 52 recent models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-5 broke at level 2,&lt;/p&gt; &lt;p&gt;Full report here: &lt;a href="http://rival.tips/jailbreak"&gt;rival.tips/jailbreak&lt;/a&gt; I'll be adding more models to this benchmark soon&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xmbxf1vhrdig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r015z4/i_managed_to_jailbreak_43_of_52_recent_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r015z4/i_managed_to_jailbreak_43_of_52_recent_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T10:52:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r02o7o</id>
    <title>GLM 5 Support Is On It's Way For Transformers</title>
    <updated>2026-02-09T12:16:36+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"&gt; &lt;img alt="GLM 5 Support Is On It's Way For Transformers" src="https://external-preview.redd.it/_RA8pRu79eov51fP28AH3ibXc2RY_CG7SQQVryJy9WU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=810b321415879975e3408c463a34398fefd38bf5" title="GLM 5 Support Is On It's Way For Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This probably means the model launch is imminent, and all evidence points to Pony Alpha on OpenRouter being a stealth deployment of GLM 5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43858"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzppr7</id>
    <title>Qwen3.5 Support Merged in llama.cpp</title>
    <updated>2026-02-09T00:32:33+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"&gt; &lt;img alt="Qwen3.5 Support Merged in llama.cpp" src="https://external-preview.redd.it/LP9lWJIkvOFwEJy7i2edxqBM2iBmROue3pUEdiXyxYg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45fd4b46acdf1a22c62c7c684471a43354c1397" title="Qwen3.5 Support Merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19435"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T00:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzjbw2</id>
    <title>I built a rough .gguf LLM visualizer</title>
    <updated>2026-02-08T20:08:31+00:00</updated>
    <author>
      <name>/u/sultan_papagani</name>
      <uri>https://old.reddit.com/user/sultan_papagani</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"&gt; &lt;img alt="I built a rough .gguf LLM visualizer" src="https://b.thumbs.redditmedia.com/kcxBxykQQ15O2Oz4xDuJc0i9OygqR7aRSLKKBTm5a5E.jpg" title="I built a rough .gguf LLM visualizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hacked together a small tool that lets you upload a .gguf file and visualize its internals in a 3D-ish way (layers / neurons / connections). The original goal was just to see what’s inside these models instead of treating them like a black box. &lt;/p&gt; &lt;p&gt;That said, my version is pretty rough, and I’m very aware that someone who actually knows what they’re doing could’ve built something way better :p &lt;/p&gt; &lt;p&gt;So I figured I’d ask here: Does something like this already exist, but done properly? If yes, I’d much rather use that For reference, this is really good: &lt;a href="https://bbycroft.net/llm"&gt;https://bbycroft.net/llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;…but you can’t upload new LLMs.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sultan_papagani"&gt; /u/sultan_papagani &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qzjbw2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T20:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r00yxq</id>
    <title>A Modest Proposal: A 1% Income Tax on Every Python Library a Developer includes</title>
    <updated>2026-02-09T10:41:15+00:00</updated>
    <author>
      <name>/u/crantob</name>
      <uri>https://old.reddit.com/user/crantob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r00yxq/a_modest_proposal_a_1_income_tax_on_every_python/"&gt; &lt;img alt="A Modest Proposal: A 1% Income Tax on Every Python Library a Developer includes" src="https://preview.redd.it/8zegrsmr6gig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b57e54c1bdb067aeff6da154784be1fe3279cd14" title="A Modest Proposal: A 1% Income Tax on Every Python Library a Developer includes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crantob"&gt; /u/crantob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8zegrsmr6gig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r00yxq/a_modest_proposal_a_1_income_tax_on_every_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r00yxq/a_modest_proposal_a_1_income_tax_on_every_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T10:41:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzz0vr</id>
    <title>GLM 5 is coming! spotted on vllm PR</title>
    <updated>2026-02-09T08:39:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt; &lt;img alt="GLM 5 is coming! spotted on vllm PR" src="https://preview.redd.it/285aias7lfig1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=5a644c4fce313f2c4b8643b1d8a7931145a54db1" title="GLM 5 is coming! spotted on vllm PR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023"&gt;https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/34124"&gt;https://github.com/vllm-project/vllm/pull/34124&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
