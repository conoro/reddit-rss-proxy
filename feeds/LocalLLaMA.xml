<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-14T14:07:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o69wtr</id>
    <title>Best CPU/RAM Combo for AI: EPYC (8-Channel DDR4) vs. Ryzen (Dual-Channel DDR5) with Blackwell PRO 6000 Max Q</title>
    <updated>2025-10-14T08:30:22+00:00</updated>
    <author>
      <name>/u/MustafaMahat</name>
      <uri>https://old.reddit.com/user/MustafaMahat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm planning a new build for hosting and running AI models, and I'm trying to decide on the best platform strategy.&lt;/p&gt; &lt;p&gt;I currently have 256 GB of DDR4 ECC RAM (as 8 x 32GB sticks @ 2400MHz) and I'm looking to buy a Blackwell PRO 6000 Max Q and possibly multiple in the future. This leads me to two very different build options:&lt;/p&gt; &lt;p&gt;Option 1: The EPYC Server Build. I could get an older-generation CPU like an AMD EPYC 7532 (32-core/64-thread). The major benefit here would be to fully utilize my RAM across 8 memory channels, which should provide massive memory bandwidth. There are also more PCI lanes for multi gpus later on, if that is ever required.&lt;/p&gt; &lt;p&gt;Option 2: The Modern Ryzen Build. Alternatively, I could sell the DDR4 and build a modern system around a high-clocked AMD Ryzen CPU with new, faster DDR5 RAM, but I'd be limited to only 2 memory channels.&lt;/p&gt; &lt;p&gt;Now my questions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bandwidth vs. Speed:&lt;/strong&gt; For AI workloads like running Large Language Models (LLMs), what's more important? The massive memory bandwidth of an 8-channel EPYC setup or the higher core clock speeds and faster RAM of a modern dual-channel Ryzen system?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System RAM vs. VRAM:&lt;/strong&gt; How useful is having a large amount of system RAM (256 GB) when a GPU with fast VRAM is doing most of the heavy lifting? Is there a point of diminishing returns?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Efficient RAM Offloading:&lt;/strong&gt; I know it's possible to offload model layers from VRAM to system RAM to run larger models. Are there effective strategies or software settings that allow this to happen without a major hit to generation speed? I want the system RAM to be a useful complement to the VRAM, not a bottleneck.&lt;/p&gt; &lt;p&gt;I'm trying to determine if it's smart to build around this large kit of DDR4 RAM to maximize bandwidth or if I'm better off starting fresh with the latest consumer hardware.&lt;/p&gt; &lt;p&gt;Thanks in advance for any advice or resources!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustafaMahat"&gt; /u/MustafaMahat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69wtr/best_cpuram_combo_for_ai_epyc_8channel_ddr4_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69wtr/best_cpuram_combo_for_ai_epyc_8channel_ddr4_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69wtr/best_cpuram_combo_for_ai_epyc_8channel_ddr4_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5n4fu</id>
    <title>Fully functional native FP4 training finally released</title>
    <updated>2025-10-13T15:37:51+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been eagerly watching the development of FP4 training, as it would enable anyone with a Blackwell device to train models with 2x the parameters that we can currently fit with FP8, and 4x BF16, which most people are still training in (get with the times people).&lt;/p&gt; &lt;p&gt;There have been many papers previously showing that FP4 is effective:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.19115"&gt;https://arxiv.org/abs/2505.19115&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2501.17116"&gt;https://arxiv.org/abs/2501.17116&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.14669"&gt;https://arxiv.org/abs/2505.14669&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.20586"&gt;https://arxiv.org/abs/2502.20586&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And one of them has also been working on public versions of the training kernels... but they have only released the forward pass kernels: &lt;a href="https://github.com/huggingface/transformers/pull/38696"&gt;https://github.com/huggingface/transformers/pull/38696&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a comparison of the 4 papers by Gemini, if you're interested in the details: &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565"&gt;https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT-OSS was also trained in FP4, but released no code, though I would bet that NVidia's in house solution was used.&lt;/p&gt; &lt;p&gt;Now, finally, NVidia has published their own FP4 training recipe. It's not well documented or tested yet, and apparently one of the techniques required for stable quantization (stochastic rounding) &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/2255#issuecomment-3387759788"&gt;simply doesn't work on the consumer RTX 50 series&lt;/a&gt;, only the datacenter cards, but still, it's here and we can use it. The use of Hadamard transforms should still allow consumer cards to train with some stability.&lt;/p&gt; &lt;p&gt;Here's some documentation which touches on their FP4 recipe: &lt;a href="https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb"&gt;https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and here's their paper which goes into detail: &lt;a href="https://arxiv.org/abs/2509.25149v1"&gt;https://arxiv.org/abs/2509.25149v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6dm18</id>
    <title>How would you price this GPU workstation?</title>
    <updated>2025-10-14T12:03:30+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the opportunity to get the following system to a price I would say is really good. The machine is used but was tested by independent people I trust.&lt;/p&gt; &lt;p&gt;The specs:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HP Z8 G4&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;192GB ECC RAM (DDR4 3200 MHz)&lt;/li&gt; &lt;li&gt;2x Intel Xeon Gold 6234 CPU @ 3.30GHz&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2x RTX A6000 48GB&lt;/strong&gt; (GA102GL) (there's an option to get a 3rd one)&lt;/li&gt; &lt;li&gt;2TB NVMe SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I would really love the hear your feedback on this machine, especially for LLM inference.&lt;/p&gt; &lt;p&gt;(the price is not finalized yet but I can post it once it is. However I know a price range in which similar machines were sold)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6dm18/how_would_you_price_this_gpu_workstation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6dm18/how_would_you_price_this_gpu_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6dm18/how_would_you_price_this_gpu_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T12:03:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5o8z1</id>
    <title>Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!</title>
    <updated>2025-10-13T16:18:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt; &lt;img alt="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" src="https://external-preview.redd.it/TovPswR4pl93bt0GT2q9uuik1XMY41ZSblXtMnDzdsU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed07f9c93dd6fdbb3800e12511f49c15a31923c3" title="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hot Take: Many models today are 'too smart' in a creative sense - trying too hard to be sensible and end up limiting their imagination to the user's prompt. Rerolls don't usually lead to different outcomes, and every gen seems catered to the user's expectations. Worst of all, there's an assistant bias that focuses on serving you (the user) instead of the story. All of these stifle their ability to express characters in a lively way. (inb4 skill issue)&lt;/p&gt; &lt;p&gt;Given the success of 22B and 123B ReduX v1.0, I revisited the old models and brought out a flavorful fusion of creativity and smarts through my latest tuning. 22B may not be as smart and sensible as the newer 24B, but ReduX makes it (more than) serviceable for users hoping for broader imagination and better immersion in their creative uses.&lt;/p&gt; &lt;h1&gt;Cydonia ReduX 22B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Behemoth ReduX 123B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1"&gt;https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;Enjoy! (Please note that this is a dual release: 123B and 22B. Notice the two links in this post.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o67m80</id>
    <title>How would you rate this 2x RTX 5090 build ?</title>
    <updated>2025-10-14T06:01:02+00:00</updated>
    <author>
      <name>/u/icybergenome</name>
      <uri>https://old.reddit.com/user/icybergenome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering I am expecting it to run following tasks comfortably:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stable Diffusion XL,&lt;/li&gt; &lt;li&gt;InstantMesh,&lt;/li&gt; &lt;li&gt;ComfyUI Workflows,&lt;/li&gt; &lt;li&gt;LLM Inference (70B, Quant 4, 60-80 token/s, 32K Context),&lt;/li&gt; &lt;li&gt;Fine Tuning 30B using LoRA. 70B using QLoRA&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Component&lt;/th&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Price&lt;/th&gt; &lt;th&gt;Key Specs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;2x NVIDIA RTX 5090 32GB&lt;/td&gt; &lt;td&gt;$4,800&lt;/td&gt; &lt;td&gt;64GB VRAM total • Blackwell FP8/FP4 • 1,792 GB/s each&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;AMD Ryzen 9 7950X&lt;/td&gt; &lt;td&gt;$420&lt;/td&gt; &lt;td&gt;16C/32T • 5.7GHz boost • PCIe 5.0 • 170W TDP&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;ASRock X870E Taichi&lt;/td&gt; &lt;td&gt;$480&lt;/td&gt; &lt;td&gt;2x PCIe 5.0 x16 • 4x DDR5 slots • 5x M.2 • WiFi 7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;256GB DDR5 6000MHz CL30&lt;/td&gt; &lt;td&gt;$700&lt;/td&gt; &lt;td&gt;4x64GB • G.SKILL • EXPO certified • 1.35V&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Storage (OS)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Samsung 990 PRO 2TB&lt;/td&gt; &lt;td&gt;$170&lt;/td&gt; &lt;td&gt;PCIe 4.0 • 7,450 MB/s read • 5yr warranty&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Storage (Data)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Silicon Power UD90 8TB&lt;/td&gt; &lt;td&gt;$310&lt;/td&gt; &lt;td&gt;PCIe 4.0 • 5,000 MB/s • Models + datasets&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Corsair HX1500i 1500W&lt;/td&gt; &lt;td&gt;$400&lt;/td&gt; &lt;td&gt;80+ Platinum • 4x 12VHPWR • 10yr warranty&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Fractal Meshify 2 Compact&lt;/td&gt; &lt;td&gt;$110&lt;/td&gt; &lt;td&gt;ATX • Mesh front • 315mm GPU clearance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Cooling&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Arctic Liquid Freezer III 360&lt;/td&gt; &lt;td&gt;$130&lt;/td&gt; &lt;td&gt;360mm AIO • 350W TDP • 6yr warranty&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Fans&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;3x Noctua NF-A14 PWM&lt;/td&gt; &lt;td&gt;$90&lt;/td&gt; &lt;td&gt;140mm • 1,500 RPM • Ultra-quiet&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Option&lt;/th&gt; &lt;th&gt;Cost&lt;/th&gt; &lt;th&gt;VRAM&lt;/th&gt; &lt;th&gt;Training Speed&lt;/th&gt; &lt;th&gt;Decision&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4x RTX 3090 (used)&lt;/td&gt; &lt;td&gt;$2,800&lt;/td&gt; &lt;td&gt;96GB&lt;/td&gt; &lt;td&gt;Baseline (no FP8)&lt;/td&gt; &lt;td&gt;❌ Outdated architecture&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;2x RTX 5090&lt;/strong&gt; ⭐&lt;/td&gt; &lt;td&gt;$4,800&lt;/td&gt; &lt;td&gt;64GB&lt;/td&gt; &lt;td&gt;&lt;strong&gt;2.5x faster&lt;/strong&gt; (FP8)&lt;/td&gt; &lt;td&gt;✅ &lt;strong&gt;BEST VALUE&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1x RTX 6000 Pro&lt;/td&gt; &lt;td&gt;$7,200&lt;/td&gt; &lt;td&gt;96GB&lt;/td&gt; &lt;td&gt;2x faster&lt;/td&gt; &lt;td&gt;⚠️ Better as 2nd card later&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3x RTX 5090&lt;/td&gt; &lt;td&gt;$7,200&lt;/td&gt; &lt;td&gt;96GB&lt;/td&gt; &lt;td&gt;3x faster&lt;/td&gt; &lt;td&gt;✅ Ideal upgrade path&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What's more valuable: More VRAM (96GB) or modern architecture (64GB)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icybergenome"&gt; /u/icybergenome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o67m80/how_would_you_rate_this_2x_rtx_5090_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o67m80/how_would_you_rate_this_2x_rtx_5090_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o67m80/how_would_you_rate_this_2x_rtx_5090_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T06:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5qx6p</id>
    <title>4x4090 build running gpt-oss:20b locally - full specs</title>
    <updated>2025-10-13T17:53:32+00:00</updated>
    <author>
      <name>/u/RentEquivalent1671</name>
      <uri>https://old.reddit.com/user/RentEquivalent1671</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt; &lt;img alt="4x4090 build running gpt-oss:20b locally - full specs" src="https://external-preview.redd.it/oLekl_ORR7Cm_gsrJon__vT598RBB5Hxp4VkS8gKBSU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c8926f5bd6382bf4684a66eaf41cb4337b53990" title="4x4090 build running gpt-oss:20b locally - full specs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4j5t70ot0xuf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fce49b840afd6f046d783920b7425c7627c7cbe8"&gt;https://preview.redd.it/4j5t70ot0xuf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fce49b840afd6f046d783920b7425c7627c7cbe8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made this monster by myself. &lt;/p&gt; &lt;p&gt;Configuration: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Processor:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; AMD Threadripper PRO 5975WX&lt;/p&gt; &lt;p&gt; -32 cores / 64 threads&lt;/p&gt; &lt;p&gt; -Base/Boost clock: varies by workload&lt;/p&gt; &lt;p&gt; -Av temp: 44°C&lt;/p&gt; &lt;p&gt; -Power draw: 116-117W at 7% load&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Motherboard:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; ASUS Pro WS WRX80E-SAGE SE WIFI&lt;/p&gt; &lt;p&gt; -Chipset: WRX80E&lt;/p&gt; &lt;p&gt; -Form factor: E-ATX workstation&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Memory:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Total: 256GB DDR4-3200 ECC&lt;/p&gt; &lt;p&gt; Configuration: 8x 32GB Samsung modules&lt;/p&gt; &lt;p&gt; Type: Multi-bit ECC registered&lt;/p&gt; &lt;p&gt; Av Temperature: 32-41°C across modules&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Graphics Cards:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 4x NVIDIA GeForce RTX 4090&lt;/p&gt; &lt;p&gt; VRAM: 24GB per card (96GB total)&lt;/p&gt; &lt;p&gt; Power: 318W per card (450W limit each)&lt;/p&gt; &lt;p&gt; Temperature: 29-37°C under load&lt;/p&gt; &lt;p&gt; Utilization: 81-99%&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Storage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Samsung SSD 990 PRO 2TB NVMe&lt;/p&gt; &lt;p&gt; -Temperature: 32-37°C&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Power Supply:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 2x XPG Fusion 1600W Platinum&lt;/p&gt; &lt;p&gt; Total capacity: 3200W&lt;/p&gt; &lt;p&gt; Configuration: Dual PSU redundant&lt;/p&gt; &lt;p&gt; Current load: 1693W (53% utilization)&lt;/p&gt; &lt;p&gt; Headroom: 1507W available &lt;/p&gt; &lt;p&gt;I run &lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;gptoss-20b&lt;/a&gt; on each GPU and have on average 107 tokens per second. So, in total, I have like 430 t/s with 4 threads. &lt;/p&gt; &lt;p&gt;Disadvantage is, 4090 is quite old, and I would recommend to use 5090. This is my first build, this is why mistakes can happen :) &lt;/p&gt; &lt;p&gt;Advantage is, the amount of T/S. And quite good model. Of course It is not ideal and you have to make additional requests to have certain format, but my personal opinion is that gptoss-20b is the real balance between quality and quantity. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RentEquivalent1671"&gt; /u/RentEquivalent1671 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:53:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o695bm</id>
    <title>WhatsApp food ordering AI Agent example with source code</title>
    <updated>2025-10-14T07:39:59+00:00</updated>
    <author>
      <name>/u/necati-ozmen</name>
      <uri>https://old.reddit.com/user/necati-ozmen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We’ve been making minimal AI agent examples with full source code.&lt;/p&gt; &lt;p&gt;Here’s one that lets you order food on WhatsApp, it shows a menu, takes your order, and checks the status through chat. Using Supabase, Whatsapp cloud API, OpenAI and Voltagent.&lt;/p&gt; &lt;p&gt;It uses tools and memory to keep context and handle actions.&lt;/p&gt; &lt;p&gt;The project is simple on purpose and feel free to fork it and build your own version. Feedback and PRs are welcome:)&lt;/p&gt; &lt;p&gt;Disclaimer: I’m one of the maintainers of VoltAgent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/necati-ozmen"&gt; /u/necati-ozmen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/VoltAgent/voltagent/tree/main/examples/with-whatsapp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o695bm/whatsapp_food_ordering_ai_agent_example_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o695bm/whatsapp_food_ordering_ai_agent_example_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T07:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6e2gg</id>
    <title>hey Karpathy! we started a nanochat students group on hugging face</title>
    <updated>2025-10-14T12:25:37+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6e2gg/hey_karpathy_we_started_a_nanochat_students_group/"&gt; &lt;img alt="hey Karpathy! we started a nanochat students group on hugging face" src="https://b.thumbs.redditmedia.com/b3JOkiUPoMTTfPSxgoXT3-DGwYUDyvKzp7vyNgdcjxw.jpg" title="hey Karpathy! we started a nanochat students group on hugging face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;We set up this organization on the hub for people to discuss and share their work on Andrej Karpathy's nanochat. &lt;/p&gt; &lt;p&gt;We'll share checkpoints, articles, and just discuss what we're learning. We already have a tokenizer trained and pretraining running.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mk4g3emwl2vf1.png?width=1594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96b984ebe15a0d20dec76319cfedfaf7bef95360"&gt;https://preview.redd.it/mk4g3emwl2vf1.png?width=1594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96b984ebe15a0d20dec76319cfedfaf7bef95360&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6e2gg/hey_karpathy_we_started_a_nanochat_students_group/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6e2gg/hey_karpathy_we_started_a_nanochat_students_group/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6e2gg/hey_karpathy_we_started_a_nanochat_students_group/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T12:25:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6870y</id>
    <title>GitHub - RagView/RagView : Validate RAG route on your dataset</title>
    <updated>2025-10-14T06:37:21+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6870y/github_ragviewragview_validate_rag_route_on_your/"&gt; &lt;img alt="GitHub - RagView/RagView : Validate RAG route on your dataset" src="https://external-preview.redd.it/CoNyHXb1E7NtiQZE7vuXWMXL6Jkf78hPpk0RKlCSlZ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e774eb1e33f85868b8ceca67abc63be011e9ecd6" title="GitHub - RagView/RagView : Validate RAG route on your dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/RagView/RagView"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6870y/github_ragviewragview_validate_rag_route_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6870y/github_ragviewragview_validate_rag_route_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T06:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5xp19</id>
    <title>What's a good uncensored model for general use?</title>
    <updated>2025-10-13T22:01:21+00:00</updated>
    <author>
      <name>/u/BankbusterMagic</name>
      <uri>https://old.reddit.com/user/BankbusterMagic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried the Satyr model and it can't output anything except porn. If I ask it to write a story about three people going grocery shopping the wind up doing it in the dairy aisle. What's an uncensored model good for general use? 16GB RAM and 16GB VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BankbusterMagic"&gt; /u/BankbusterMagic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xp19/whats_a_good_uncensored_model_for_general_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xp19/whats_a_good_uncensored_model_for_general_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xp19/whats_a_good_uncensored_model_for_general_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T22:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o68jt0</id>
    <title>qwen3 coder 4b and 8b, please</title>
    <updated>2025-10-14T07:00:21+00:00</updated>
    <author>
      <name>/u/madaradess007</name>
      <uri>https://old.reddit.com/user/madaradess007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why did qwen stop releasing small models?&lt;br /&gt; can we do it on our own? i'm on 8gb macbook air, so 8b is max for me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madaradess007"&gt; /u/madaradess007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o68jt0/qwen3_coder_4b_and_8b_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o68jt0/qwen3_coder_4b_and_8b_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o68jt0/qwen3_coder_4b_and_8b_please/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T07:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5qo0r</id>
    <title>It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase</title>
    <updated>2025-10-13T17:44:28+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"&gt; &lt;img alt="It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase" src="https://external-preview.redd.it/gbZbO_XMemMwlSTTx1mACM7p7UtdxxgpOKoIWv4akso.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5312d01181eaa8e15816fcf71e260612f9b1af" title="It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/karpathy/nanochat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5nlli</id>
    <title>Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp; More</title>
    <updated>2025-10-13T15:55:32+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt; &lt;img alt="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" src="https://b.thumbs.redditmedia.com/53soxOPlO99qPaIqPE4FAHhMXBhvv7QgMGAh0UoqSHU.jpg" title="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to share &lt;strong&gt;Nanonets-OCR2&lt;/strong&gt;, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).&lt;/p&gt; &lt;p&gt;🔍 &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LaTeX Equation Recognition:&lt;/strong&gt; Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (&lt;code&gt;$...$&lt;/code&gt;) and display (&lt;code&gt;$$...$$&lt;/code&gt;) equations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Image Description:&lt;/strong&gt; Describes images within documents using structured &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Signature Detection &amp;amp; Isolation:&lt;/strong&gt; Identifies and isolates signatures from other text, outputting them within a &lt;code&gt;&amp;lt;signature&amp;gt;&lt;/code&gt; tag. This is crucial for processing legal and business documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watermark Extraction:&lt;/strong&gt; Detects and extracts watermark text from documents, placing it within a &lt;code&gt;&amp;lt;watermark&amp;gt;&lt;/code&gt; tag.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Checkbox Handling:&lt;/strong&gt; Converts form checkboxes and radio buttons into standardized Unicode symbols (&lt;code&gt;☐&lt;/code&gt;, &lt;code&gt;☑&lt;/code&gt;, &lt;code&gt;☒&lt;/code&gt;) for consistent and reliable processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Table Extraction:&lt;/strong&gt; Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flow charts &amp;amp; Organisational charts:&lt;/strong&gt; Extracts flow charts and organisational as &lt;a href="https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org"&gt;mermaid&lt;/a&gt; code.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handwritten Documents:&lt;/strong&gt; The model is trained on handwritten documents across multiple languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Question Answering (VQA):&lt;/strong&gt; The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with &amp;quot;Not mentioned.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://docstrange.nanonets.com/"&gt;🖥️ Live Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://nanonets.com/research/nanonets-ocr-2"&gt;📢 Blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NanoNets/docstrange"&gt;⌨️ GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 &lt;a href="https://huggingface.co/collections/nanonets/nanonets-ocr2-68ed207f17ee6c31d226319e"&gt;Huggingface models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea00f9623db4529514533820223b2fb53be4767d"&gt;Document with equation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4a1316e250f7f244f6e253d66c8ebf1ba105313"&gt;Document with complex checkboxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8bcc88b138a553c7760d6e46319b864802339913"&gt;Quarterly Report (Please use the Markdown(Financial Docs) for best result in docstrange demo)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9"&gt;Signatures&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=008fce272c2979b00e0033c34ffcd2b0d69cb24c"&gt;mermaid code for flowchart&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jytsym6eiwuf1.png?width=2462&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c"&gt;Visual Question Answering&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try it out and share your feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6c9vs</id>
    <title>I built a fully automated AI podcast generator that connects to ollama</title>
    <updated>2025-10-14T10:54:44+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’ve been working on a fun side project — an &lt;strong&gt;AI-powered podcast generator&lt;/strong&gt; built entirely with &lt;strong&gt;Ollama (for the LLM)&lt;/strong&gt; and &lt;strong&gt;Piper (for TTS)&lt;/strong&gt;. 🎙️&lt;/p&gt; &lt;p&gt;The system takes any topic and automatically:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Write a complete script&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generates the audio&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’ve &lt;strong&gt;open-sourced the full project&lt;/strong&gt; on GitHub so anyone can explore, use, or contribute to it. If you’re into AI, audio, or automation, I’d love your feedback and ideas!&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Laszlobeer/AI-podcast"&gt;https://github.com/Laszlobeer/AI-podcast&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6c9vs/i_built_a_fully_automated_ai_podcast_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6c9vs/i_built_a_fully_automated_ai_podcast_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6c9vs/i_built_a_fully_automated_ai_podcast_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T10:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o65di4</id>
    <title>Nvidia DGX Spark reviews started</title>
    <updated>2025-10-14T03:54:37+00:00</updated>
    <author>
      <name>/u/raphaelamorim</name>
      <uri>https://old.reddit.com/user/raphaelamorim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"&gt; &lt;img alt="Nvidia DGX Spark reviews started" src="https://external-preview.redd.it/uAv19XEYpnCDKkb7y0-bzGB8la4s2d7ck6vl-XJBRac.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=576e994d52db2badc52382731998578b9ff595e5" title="Nvidia DGX Spark reviews started" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably start selling on October 15th&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raphaelamorim"&gt; /u/raphaelamorim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/zs-J9sKxvoM?si=237f_mBVyLH7QBOE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T03:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5ptit</id>
    <title>Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture.</title>
    <updated>2025-10-13T17:14:18+00:00</updated>
    <author>
      <name>/u/Dentuam</name>
      <uri>https://old.reddit.com/user/Dentuam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"&gt; &lt;img alt="Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture." src="https://external-preview.redd.it/IjppR-RE-RkBB_gQduyqs52uBDc0W1Hhz7wl-iWhgJ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bde18ad695deb84f2f7185a79fef6c32828efb7f" title="Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture.&lt;/p&gt; &lt;p&gt;Ring-1T achieves silver-level IMO reasoning through pure natural language reasoning.&lt;/p&gt; &lt;p&gt;→ 1 T total / 50 B active params · 128 K context window → Reinforced by Icepop RL + ASystem (Trillion-Scale RL Engine) → Open-source SOTA in natural language reasoning — AIME 25 / HMMT 25 / ARC-AGI-1 / CodeForce&lt;/p&gt; &lt;p&gt;Deep thinking · Open weights · FP8 version available&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AntLingAGI/status/1977767599657345027?t=jx-D236A8RTnQyzLh-sC6g&amp;amp;s=19"&gt;https://x.com/AntLingAGI/status/1977767599657345027?t=jx-D236A8RTnQyzLh-sC6g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dentuam"&gt; /u/Dentuam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6d6xh</id>
    <title>SHAI – (yet another) open-source Terminal AI coding assistant</title>
    <updated>2025-10-14T11:42:29+00:00</updated>
    <author>
      <name>/u/Fit_Temperature7246</name>
      <uri>https://old.reddit.com/user/Fit_Temperature7246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;At OVHcloud, we built SHAI for our internal needs as a coding assistant that wouldn’t rely on proprietary models or closed services. We’ve now open-sourced it (Apache 2.0) so the community can use and improve it too, including for local use.&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;What is SHAI? 🔎&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A terminal-based AI assistant to help you:&lt;br /&gt; • Build &amp;amp; edit code&lt;br /&gt; • Run shell commands&lt;br /&gt; • Automate workflows&lt;br /&gt; • Or even run headless as part of your stack&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it’s cool ?&lt;/strong&gt; 😎&lt;/p&gt; &lt;p&gt;• Fully Open Source + developer-first design&lt;br /&gt; • No vendor lock-in (configure any LLM endpoint)&lt;br /&gt; • Works out of the box with pre-configured OVHCloud AI Endpoints (free tier with low rate limiting - you can add your API key later)&lt;br /&gt; • Supports Function Calling + MCP&lt;br /&gt; Also → SHAI is part of &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hacktoberfest&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This year! If you want to contribute &amp;amp; grab some swag, it’s a great time: &lt;a href="https://github.com/ovh/shai"&gt;https://github.com/ovh/shai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Temperature7246"&gt; /u/Fit_Temperature7246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d6xh/shai_yet_another_opensource_terminal_ai_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d6xh/shai_yet_another_opensource_terminal_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d6xh/shai_yet_another_opensource_terminal_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T11:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6d4a6</id>
    <title>CPU Only OSS 120</title>
    <updated>2025-10-14T11:38:44+00:00</updated>
    <author>
      <name>/u/Wisepunter</name>
      <uri>https://old.reddit.com/user/Wisepunter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive sold my 3090 and im selling my 4090 as we speak, mostly because the stuff I really need LLMs for I need huge models and the other stuff I only need really small models 4B or less. Also I tend to game on my PS5 as work at my PC all day.&lt;/p&gt; &lt;p&gt;So I used to run OSS120 partially in GPU with the rest offloaded to CPU and it used to fly. Also it was a pretty good model IMO for logic etc for its speed.&lt;/p&gt; &lt;p&gt;So decided to just try it on CPU only (gulp) on my home lab server and actually it's more than usable at a fraction of the power cost too. This is also running in a VM with only half cores given.&lt;/p&gt; &lt;p&gt;prompt eval time = 260.39 ms / 13 tokens ( 20.03 ms per token, 49.92 tokens per second)eval time = 51470.09 ms / 911 tokens ( 56.50 ms per token, 17.70 tokens per second)total time = 51730.48 ms / 924 tokens&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wisepunter"&gt; /u/Wisepunter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d4a6/cpu_only_oss_120/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d4a6/cpu_only_oss_120/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d4a6/cpu_only_oss_120/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T11:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o69jfe</id>
    <title>Still no qwen3 next 80b gguf?</title>
    <updated>2025-10-14T08:05:39+00:00</updated>
    <author>
      <name>/u/LebiaseD</name>
      <uri>https://old.reddit.com/user/LebiaseD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it coming will it come?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LebiaseD"&gt; /u/LebiaseD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6163l</id>
    <title>DGX Spark review with benchmark</title>
    <updated>2025-10-14T00:33:01+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"&gt; &lt;img alt="DGX Spark review with benchmark" src="https://external-preview.redd.it/WNdw4kTz_uFbrszyWcTmBGBzFo8R71Bs5ZxJc5c0h-o.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6226efb1a534fbfbdcc59966a365bcdb316c259" title="DGX Spark review with benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As expected, not the best performer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/-3r2woTQjec?si=PruuNNLJVTwCYvC7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o623qi</id>
    <title>I tested if tiny LLMs can self-improve through memory: Qwen3-1.7B gained +8% accuracy on MATH problems</title>
    <updated>2025-10-14T01:16:29+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;TL;DR&lt;/h2&gt; &lt;p&gt;Implemented Google's ReasoningBank paper on small models (1.7B params). Built a memory system that extracts reasoning strategies from successful solutions and retrieves them for similar problems. &lt;strong&gt;Result: 1.7B model went from 40% → 48% accuracy on MATH Level 3-4 problems (+20% relative improvement).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smaller models benefited MORE than larger ones.&lt;/strong&gt; Afer phase 1 is finished tuning phase 2 will attempt to answer, &amp;quot;can the model recursively improve by fine-tuning on its own successful traces?&amp;quot;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What I Built&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;reasoning-bank-slm&lt;/strong&gt; - Testing if small language models can bootstrap their reasoning ability through: 1. &lt;strong&gt;Memory extraction&lt;/strong&gt;: When the model solves a problem, extract generalizable strategies 2. &lt;strong&gt;Semantic retrieval&lt;/strong&gt;: For new problems, retrieve relevant strategies from memory 3. &lt;strong&gt;Guided solving&lt;/strong&gt;: Inject retrieved strategies as hints into the prompt 4. &lt;strong&gt;Recursive loop&lt;/strong&gt; (Phase 2): Fine-tune the model on successful reasoning traces, repeat&lt;/p&gt; &lt;p&gt;Full code on GitHub: &lt;a href="https://github.com/Lanerra/reasoning-bank-slm"&gt;https://github.com/Lanerra/reasoning-bank-slm&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Experimental Setup&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; - Ryzen 9 7950X, 128GB RAM - RTX 4090 + RTX 3090 - Running llama-server locally&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models tested:&lt;/strong&gt; - Qwen3-1.7B-Instruct (primary) - Qwen3-4B-Instruct (comparison) - Qwen3-Embedding-0.6B (retrieval)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; MATH Level 3-4 (harder than GSM8K) - 100 training problems → build memory bank - 100 test problems → baseline vs memory-augmented&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Design features:&lt;/strong&gt; - Answer leak prevention (filters memories containing expected answer) - Wilson confidence intervals for statistical rigor - Deterministic seeding for reproducibility&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Phase 1 Results (Qwen3-1.7B)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;With Memory&lt;/th&gt; &lt;th&gt;Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Accuracy&lt;/td&gt; &lt;td&gt;40.0%&lt;/td&gt; &lt;td&gt;48.0%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;+8.0%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Problems solved&lt;/td&gt; &lt;td&gt;40/100&lt;/td&gt; &lt;td&gt;48/100&lt;/td&gt; &lt;td&gt;+8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Improvements&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Regressions&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Net effect: +8 problems (2:1 improvement ratio)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Memory bank: 223 strategies extracted from training set&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What Actually Improved&lt;/h2&gt; &lt;p&gt;Sample problems where memory helped:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Complex plane geometry:&lt;/strong&gt; - Baseline: Failed (wrong format) - Retrieved: &amp;quot;Vector Magnitude Method&amp;quot; - Result: ✓ Correct (25π)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Polynomial analysis:&lt;/strong&gt; - Baseline: Failed (no answer) - Retrieved: &amp;quot;Equate Target Value to Function&amp;quot; - Result: ✓ Correct (5)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Fibonacci series summation:&lt;/strong&gt; - Baseline: Failed - Retrieved: &amp;quot;Coefficient Multiplication and Summation&amp;quot; - Result: ✓ Correct (1)&lt;/p&gt; &lt;p&gt;These aren't edge cases - the retrieved strategies were genuinely applicable.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Regressions (The Honest Part)&lt;/h2&gt; &lt;p&gt;8 problems got worse with memory. All showed the same pattern: model failed to produce an answer (not wrong answer, but no answer at all).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; 223 memories is too many. Retrieval pulls less-relevant strategies → context bloat → model confusion.&lt;/p&gt; &lt;p&gt;Supporting evidence: Runs with fewer memories (10, 40) had zero regressions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix for Phase 2:&lt;/strong&gt; Better retrieval filtering, quality thresholds, or reduce k.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Comparison: Model Size Matters&lt;/h2&gt; &lt;p&gt;Tested both 1.7B and 4B on same problems:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;With Memory&lt;/th&gt; &lt;th&gt;Improvement&lt;/th&gt; &lt;th&gt;Regressions&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4B&lt;/td&gt; &lt;td&gt;76%&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;+4%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.7B&lt;/td&gt; &lt;td&gt;40%&lt;/td&gt; &lt;td&gt;48%&lt;/td&gt; &lt;td&gt;+8%&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Key insight:&lt;/strong&gt; Smaller models benefit more from memory but are more fragile. The 4B already knows most strategies; the 1.7B needs the hints.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Why This Might Matter&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Small models can punch above their weight&lt;/strong&gt; with the right scaffolding&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory &amp;gt; parameters&lt;/strong&gt; for certain reasoning tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opens path to recursive self-improvement&lt;/strong&gt;: If Phase 2 works (fine-tuning on successful traces), models could bootstrap capability without human supervision&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;Phase 2 Preview&lt;/h2&gt; &lt;p&gt;Next up: Can the model improve by learning from its own successes?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Loop:&lt;/strong&gt; 1. Harvest successful reasoning traces from memory bank 2. Fine-tune via LoRA on these traces 3. Test on problems the original model failed 4. Measure differential improvement 5. Hot-swap improved model, repeat&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; The 16 improvements from Phase 1 suggest the model can apply better strategies. If we fine-tune on those successful traces, can we bake the improvements in?&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Reproducibility&lt;/h2&gt; &lt;p&gt;Everything is open source. The repo includes: - Full code with fixes and improvements - Dataset preparation scripts (GSM8K and MATH) - Statistical analysis tools - Diagnostic scripts for debugging - Instructions for running locally&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware requirements (All models used for testing are quantized to Q8):&lt;/strong&gt; - 4.3GB+ VRAM for 4B model - 1.7GB+ VRAM for 1.7B model&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Limitations &amp;amp; Honesty&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Not statistically significant&lt;/strong&gt; (95% CI overlap) - need larger n&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Regressions exist&lt;/strong&gt; - memory can confuse small models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extraction variance&lt;/strong&gt; - same training set produces 29-223 memories depending on run&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset ceiling&lt;/strong&gt; - 4B at 76% baseline doesn't have much room to improve&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phase 2 unproven&lt;/strong&gt; - recursive loop might amplify errors instead of improvements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is early research. I'm sharing to get feedback and replication attempts.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Why I'm Posting&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Validation&lt;/strong&gt;: Want others to check my work&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;: Ideas for improving retrieval/extraction?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Curiosity&lt;/strong&gt;: Has anyone else tried this with small models?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt;: This could fail spectacularly in Phase 2 - documenting either way&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you replicate this and get different results, please let me know. Science requires replication.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Lanerra/reasoning-bank-slm"&gt;https://github.com/Lanerra/reasoning-bank-slm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback, criticisms, and replication attempts welcome. Especially interested if anyone has ideas for: - Better memory extraction methods - Smarter retrieval filtering - Handling the regression problem - Phase 2 design approaches&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T01:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6g8se</id>
    <title>Nvidia DGX Spark vs. Others (6000/5090/Mac)</title>
    <updated>2025-10-14T13:57:46+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6g8se/nvidia_dgx_spark_vs_others_60005090mac/"&gt; &lt;img alt="Nvidia DGX Spark vs. Others (6000/5090/Mac)" src="https://preview.redd.it/g7gao5ec23vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be02471f67e88770b1458795aaf8f94c2246c6e" title="Nvidia DGX Spark vs. Others (6000/5090/Mac)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initial benchmarks are coming in: &lt;a href="https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/"&gt;https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7gao5ec23vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6g8se/nvidia_dgx_spark_vs_others_60005090mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6g8se/nvidia_dgx_spark_vs_others_60005090mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T13:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o69vm5</id>
    <title>What’s the point of a DGX Spark for inference if a Mac Studio M1 Ultra beats it at TG and equals it at PP at half the price?</title>
    <updated>2025-10-14T08:28:09+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I might be missing something here, but with the results I’ve seen, the DGX does what Apple did 3 years ago (actually worse token generation). &lt;/p&gt; &lt;p&gt;Is the DGX as bad as it seems for inference? We all knew that TG would have been shit with that bandwidth, but even prompt processing doesn’t seem great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5v78n</id>
    <title>The top open models on are now all by Chinese companies</title>
    <updated>2025-10-13T20:27:10+00:00</updated>
    <author>
      <name>/u/k_schaul</name>
      <uri>https://old.reddit.com/user/k_schaul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt; &lt;img alt="The top open models on are now all by Chinese companies" src="https://preview.redd.it/xhsv9ilkuxuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17f3ce5e0a0548bdb8546f46e0f43b1b008af719" title="The top open models on are now all by Chinese companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full analysis here (🎁 gift link): &lt;a href="https://wapo.st/4nPUBud"&gt;wapo.st/4nPUBud&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_schaul"&gt; /u/k_schaul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhsv9ilkuxuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T20:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o61gzs</id>
    <title>Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8</title>
    <updated>2025-10-14T00:47:06+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt; &lt;img alt="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" src="https://preview.redd.it/fjr53w0m4zuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1986eb8662405e67e0522e5d8d37f03ea577ffc" title="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;-NVFP4 is a way to store numbers for training large models using just 4 bits instead of 8 or 16. This makes training faster and use less memory&lt;/p&gt; &lt;p&gt;-NVFP4 shows 4-bit pretraining of a 12B Mamba Transformer on 10T tokens can match FP8 accuracy while cutting compute and memory.&lt;/p&gt; &lt;p&gt;-The validation loss stays within 1% of FP8 for most of training and grows to about 1.5% late during learning rate decay. &lt;/p&gt; &lt;p&gt;-Task scores stay close, for example MMLU Pro 62.58% vs 62.62%, while coding dips a bit like MBPP+ 55.91% vs 59.11%.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/godofprompt/status/1977678347879714912"&gt;X thread&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2509.25149"&gt;Arxiv paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjr53w0m4zuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
