<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-30T11:06:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pzdw3h</id>
    <title>reko ‚Äì Local-first YouTube-to-Markdown summarizer with small local LLMs</title>
    <updated>2025-12-30T09:35:48+00:00</updated>
    <author>
      <name>/u/Rikifire</name>
      <uri>https://old.reddit.com/user/Rikifire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a local-first tool to summarize YouTube videos into clean Markdown using transcripts + LLMs.&lt;/p&gt; &lt;p&gt;The default setup uses Ollama with small local models. Other local or cloud providers are supported too if preferred.&lt;/p&gt; &lt;p&gt;The Python app is the core engine. The main interface is a CLI (useful for scripting and automation), and I also added a small localhost web UI to speed up session-based workflows (paste a link, tweak settings, get rendered Markdown). Everything runs locally.&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate feedback from this community on model choice, output quality improvement, and anything that could improve the local-first workflow.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/riccardoruspoli/reko"&gt;https://github.com/riccardoruspoli/reko&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rikifire"&gt; /u/Rikifire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzdw3h/reko_localfirst_youtubetomarkdown_summarizer_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzdw3h/reko_localfirst_youtubetomarkdown_summarizer_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzdw3h/reko_localfirst_youtubetomarkdown_summarizer_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T09:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz3643</id>
    <title>So any rumours about llama?</title>
    <updated>2025-12-30T00:28:24+00:00</updated>
    <author>
      <name>/u/AdventurousFly4909</name>
      <uri>https://old.reddit.com/user/AdventurousFly4909</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While others have been cooking, the llama team had been radio silent. Has any interesting news about llama surfaced?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousFly4909"&gt; /u/AdventurousFly4909 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz3643/so_any_rumours_about_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz3643/so_any_rumours_about_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz3643/so_any_rumours_about_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T00:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjjbw</id>
    <title>Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together</title>
    <updated>2025-12-29T11:02:29+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt; &lt;img alt="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" src="https://a.thumbs.redditmedia.com/bB4zUj7vleOqJdnXmwlZ9s4tewjkzGrf2kPk8Dbebv4.jpg" title="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HyperCLOVA X SEED 32B Think: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HyperCLOVA X SEED 8B Omni: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed"&gt;https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Artificial Analysis on ùïè: &lt;a href="https://x.com/ArtificialAnlys/status/2005429176615174207"&gt;https://x.com/ArtificialAnlys/status/2005429176615174207&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyjjbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrjke</id>
    <title>Benchmarks for Quantized Models? (for users locally running Q8/Q6/Q2 precision)</title>
    <updated>2025-12-29T17:00:09+00:00</updated>
    <author>
      <name>/u/No-Grapefruit-1358</name>
      <uri>https://old.reddit.com/user/No-Grapefruit-1358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;Many of us use the quantized Q8/Q6/Q2 model instead of fp16 for obvious reasons. Is there a collection of benchmarks which show SWE, HLE etc on Q8/Q6/Q2 quantized models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Grapefruit-1358"&gt; /u/No-Grapefruit-1358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pywgsb</id>
    <title>Best LLM Related Open Source Tools - 2025?</title>
    <updated>2025-12-29T20:00:49+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think 2025 is good year &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;LLM wise&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now please share the tools you're using with LLMs. I know that half of us here involves with coding by using tools such as Cline, RooCode, KiloCode, QwenCode, MistralVibe, etc.,&lt;/p&gt; &lt;p&gt;Similarly some of us here involves with writing by using Finetuned Writing models. Of course we need tools for writing too. I came across Mikupad, Writingway2, Arrows(p-e-w), WritingTools(theJayTea)&lt;/p&gt; &lt;p&gt;Coding &amp;amp; Writing are just 2 categories I mentioned. Also I mentioned only few tools here(from my bookmarks) &amp;amp; Of course there are so many more other tools exist online which everyone yet to catch. I'm sure around 50 tools available for each category, lets bring those here.&lt;/p&gt; &lt;p&gt;So what other tools are you using? (Please mention category or concise use case)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Just mentioning some categories below to get quick &amp;amp; more replies&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt&lt;/li&gt; &lt;li&gt;RAG, &lt;/li&gt; &lt;li&gt;Brainstorm&lt;/li&gt; &lt;li&gt;AudioBook Maker&lt;/li&gt; &lt;li&gt;Ebook Maker&lt;/li&gt; &lt;li&gt;Second brain&lt;/li&gt; &lt;li&gt;Benchmarks&lt;/li&gt; &lt;li&gt;AI Assistant&lt;/li&gt; &lt;li&gt;Agents&lt;/li&gt; &lt;li&gt;Notebook&lt;/li&gt; &lt;li&gt;NoCode&lt;/li&gt; &lt;li&gt;Wiki&lt;/li&gt; &lt;li&gt;Storytelling/Worldbuilding&lt;/li&gt; &lt;li&gt;Image processing&lt;/li&gt; &lt;li&gt;Game creation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mentioned tools are from github only, I can share link if you need. The reason I didn't include links in this thread because sometime reddit filters remove threads automatically if multiple links present.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So far got mostly coding related tools. Though good to have more tools on coding, lets have more tools on all other categories. I'm thinking of sharing my bookmarks(List of LLM related tools' github repos) later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T20:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzbw4k</id>
    <title>I built a fully offline text to speech Mac app because cloud TTS annoyed me</title>
    <updated>2025-12-30T07:33:06+00:00</updated>
    <author>
      <name>/u/tarunyadav9761</name>
      <uri>https://old.reddit.com/user/tarunyadav9761</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzbw4k/i_built_a_fully_offline_text_to_speech_mac_app/"&gt; &lt;img alt="I built a fully offline text to speech Mac app because cloud TTS annoyed me" src="https://external-preview.redd.it/ODd5Z2hya3duYWFnMT5ICs7kDawfo1Fgfnw0GJpF94UbYpqrosjPAEkkD-iX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe6d2d2d85378994415d31998572ce471e9c6a93" title="I built a fully offline text to speech Mac app because cloud TTS annoyed me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm an indie maker, and I shipped a native macOS app to solve a problem I personally had.&lt;/p&gt; &lt;p&gt;I deal with a lot of long text (articles, drafts, AI outputs), and I wanted to listen to it while working without sending my content to the cloud or paying subscriptions.&lt;/p&gt; &lt;p&gt;So I built Murmur:&lt;/p&gt; &lt;p&gt;‚Ä¢ Runs entirely offline on Apple Silicon&lt;/p&gt; &lt;p&gt;‚Ä¢ No accounts, no quotas, no subscriptions&lt;/p&gt; &lt;p&gt;‚Ä¢ High-quality AI voices&lt;/p&gt; &lt;p&gt;‚Ä¢ One-time purchase&lt;/p&gt; &lt;p&gt;I mainly use it to:&lt;/p&gt; &lt;p&gt;‚Ä¢ Listen to long articles while doing admin work&lt;/p&gt; &lt;p&gt;‚Ä¢ Review AI outputs hands-free&lt;/p&gt; &lt;p&gt;‚Ä¢ Catch mistakes in drafts by listening instead of rereading&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Le Giveaway&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;To get early feedback on UX and improve the product, I‚Äôm giving away 100% off codes to 10 people!&lt;/p&gt; &lt;p&gt;Just drop a comment on what they would actually use it for and I‚Äôll generate and publish a randomized list of winners by the &lt;strong&gt;end of this week.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'll DM the codes to the most interesting use cases in 48 hours&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1pzbkqk"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarunyadav9761"&gt; /u/tarunyadav9761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fgdnzrkwnaag1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzbw4k/i_built_a_fully_offline_text_to_speech_mac_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzbw4k/i_built_a_fully_offline_text_to_speech_mac_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T07:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz6vju</id>
    <title>An open source implementation of that refusal steering paper</title>
    <updated>2025-12-30T03:12:53+00:00</updated>
    <author>
      <name>/u/Remarkable_Threes</name>
      <uri>https://old.reddit.com/user/Remarkable_Threes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone - I just released the code for the refusal steering paper that uses LLM-Refusal-Evaluation. TLDR: Surgical refusal removal with statistical validation instead of vibes-based steering. Main features:&lt;/p&gt; &lt;p&gt;Judge scores validate your training data&lt;/p&gt; &lt;p&gt;Correlation analysis picks best layers automatically&lt;/p&gt; &lt;p&gt;Confidence-weighted steering vectors (WRMD from the paper)&lt;/p&gt; &lt;p&gt;Auto alpha optimization with early stopping&lt;/p&gt; &lt;p&gt;Can merge permanently into weights&lt;/p&gt; &lt;p&gt;It's more setup than simpler steering repos (multi-stage pipeline, needs the eval framework), but you get actual statistical validation at each step instead of guessing.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ElSnacko/llm-steering"&gt;https://github.com/ElSnacko/llm-steering&lt;/a&gt; Paper: &lt;a href="https://arxiv.org/abs/2512.16602"&gt;https://arxiv.org/abs/2512.16602&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from anyone who tries it! Especially curious how it stacks up against abliteration in practice.I will be testing and benchmarking this implementation and so likely more posts to come. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Threes"&gt; /u/Remarkable_Threes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6vju/an_open_source_implementation_of_that_refusal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6vju/an_open_source_implementation_of_that_refusal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6vju/an_open_source_implementation_of_that_refusal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyg4yt</id>
    <title>Tencent just released WeDLM 8B Instruct on Hugging Face</title>
    <updated>2025-12-29T07:38:43+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt; &lt;img alt="Tencent just released WeDLM 8B Instruct on Hugging Face" src="https://b.thumbs.redditmedia.com/C56gntSOSvM_cfj95m0peqGLfh8p1Tnt02oONhKPwFM.jpg" title="Tencent just released WeDLM 8B Instruct on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/tencent/WeDLM-8B-Instruct"&gt;https://huggingface.co/tencent/WeDLM-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A diffusion language model that runs 3-6√ó faster than vLLM-optimized Qwen3-8B on math reasoning tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyg4yt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T07:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz9x3u</id>
    <title>One answer to "what do you use local LLMs for?": a hyper-personalized multimodal event crawler</title>
    <updated>2025-12-30T05:42:32+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see the &amp;quot;what do you use local LLMs for?&amp;quot; question come up every month, so here's one example: a multimodal agent that crawls local websites to find events happening around me.&lt;/p&gt; &lt;h1&gt;Why local instead of API?&lt;/h1&gt; &lt;p&gt;People ask me this a lot. Cloud providers are cheap, until you're generating millions of tokens. I'm crawling dozens of event sources, processing images, deduplicating across sites. That adds up fast.&lt;/p&gt; &lt;p&gt;Local is also faster for my use case. Claude and GPT grind to a halt during peak loads. &lt;a href="https://www.ovidiudan.com/2025/12/25/dual-rtx-pro-6000-llm-guide.html"&gt;My home server&lt;/a&gt; gives me consistent throughput whenever I need it.&lt;/p&gt; &lt;h1&gt;The setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Dual RTX Pro 6000 (96GB VRAM each)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V"&gt;GLM-4.6V&lt;/a&gt; (106B parameter multimodal model) running on vLLM&lt;/li&gt; &lt;li&gt;The crawler, backend, and mobile app were all vibe coded with Claude Opus&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What GLM-4.6V actually does&lt;/h1&gt; &lt;p&gt;The crawler uses the model for five tasks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Extracting info from event flyers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where multimodal models shine. &lt;a href="https://whidbeycamanoislands.com/event/the-dead-guise-new-years-eve/"&gt;Here's an event&lt;/a&gt; where the text description doesn't mention the price, but the flyer image does. The LLM reads the flyer and extracts &amp;quot;$25&amp;quot; into a structured field.&lt;/p&gt; &lt;p&gt;OCR can read text from an image, but it can't understand that &amp;quot;$25&amp;quot; on a psychedelic Grateful Dead flyer is the ticket price and not a date or an address. That requires a model that actually understands what it's looking at.&lt;/p&gt; &lt;p&gt;The model also extracts venue names, performer lineups, age restrictions, and registration requirements from a combination of the raw HTML and the accompanying image.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Rewriting messy descriptions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Scraped event descriptions are a mess: HTML artifacts, escaped characters, inconsistent formatting. The LLM rewrites these into clean paragraphs while preserving the essential info.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Link classification&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rather than fragile regex to find ticket links, the LLM analyzes all links on a page and identifies the primary registration URL (not the &amp;quot;Buy Tickets&amp;quot; link for a different event in the sidebar).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Cross-source deduplication&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The same event appears on multiple websites. The LLM compares new events against existing ones and determines if it's a duplicate. It understands that &amp;quot;NYE Party at The Clyde&amp;quot; and &amp;quot;New Year's Eve Celebration - Clyde Theatre&amp;quot; are the same event.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Multi-event extraction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some sources publish newsletter images containing multiple events. The LLM extracts each event separately from a single composite image.&lt;/p&gt; &lt;h1&gt;The point&lt;/h1&gt; &lt;p&gt;A few years ago, some of this would have been practically impossible. Not just expensive or slow, but actually impossible. Multimodal understanding of unstructured visual data wasn't something you could just spin up.&lt;/p&gt; &lt;p&gt;Now I can throw together a custom tool over a weekend that does exactly what I need. Tools built for an audience of one, running on hardware I control.&lt;/p&gt; &lt;p&gt;Full writeup with more details on the Firebase backend and Flutter app: &lt;a href="https://www.ovidiudan.com/2025/12/30/age-customized-software.html"&gt;The age of hyper-personalized software&lt;/a&gt; (I am not selling or promoting anything, I do this for fun.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T05:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrn9v</id>
    <title>I Finished a Fully Local Agentic RAG Tutorial</title>
    <updated>2025-12-29T17:03:44+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôve just finished a &lt;strong&gt;complete Agentic RAG tutorial + repository&lt;/strong&gt; that shows how to build a fully local, end-to-end system.&lt;/p&gt; &lt;p&gt;No APIs, no cloud, no hidden costs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üí° What‚Äôs inside&lt;/h3&gt; &lt;p&gt;The tutorial covers the full pipeline, including the parts most examples skip:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF ‚Üí Markdown ingestion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hierarchical chunking (parent / child)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hybrid retrieval (dense + sparse)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Vector store with &lt;strong&gt;Qdrant&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Query rewriting + &lt;strong&gt;human-in-the-loop&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Context summarization&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-agent map-reduce&lt;/strong&gt; with &lt;strong&gt;LangGraph&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Local inference with &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Simple &lt;strong&gt;Gradio&lt;/strong&gt; UI&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üéØ Who it‚Äôs for&lt;/h3&gt; &lt;p&gt;If you want to &lt;strong&gt;understand Agentic RAG by building it&lt;/strong&gt;, not just reading theory, this might help.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üîó Repo&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz4x0v</id>
    <title>RAG Paper 25.12.24</title>
    <updated>2025-12-30T01:45:44+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.21280v1"&gt;SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20916v1"&gt;MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20884v1"&gt;The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T01:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzezr3</id>
    <title>Are humans like LLMs?</title>
    <updated>2025-12-30T10:43:03+00:00</updated>
    <author>
      <name>/u/mouseofcatofschrodi</name>
      <uri>https://old.reddit.com/user/mouseofcatofschrodi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else thought of how much humans are like LLMs (or the other way around)?&lt;/p&gt; &lt;p&gt;Prompt techniques work great to humans. I didn't run tests on humans, nor do I have benchmarks to support this intuition, but even in school we have been using the classical promt techniques, like asking the students to think step by step (math problems), giving few examples, and so on.&lt;/p&gt; &lt;p&gt;Humans learn the meaning of words statistically (whatever the word &amp;quot;meaning&amp;quot; means). As babies/toddlers we listen noise and after many repetitions we kind of get patterns.&lt;/p&gt; &lt;p&gt;If you follow the philosophy line of Wittgenstein, we use words within a contextual game. Kind of like LLMs.&lt;/p&gt; &lt;p&gt;We don't know what we know. At least as a student, I was always surprised when writing exams of how much (or less) I knew. I would only discover how much I knew only while answering the exams. This unconsiousness of how much we know, where our knowledge is located, reminds me a lot of LLMs.&lt;/p&gt; &lt;p&gt;We have also &amp;quot;instruct&amp;quot; and &amp;quot;thinking&amp;quot; modes. When having a live discussion with another human, I may discover my opinions by listening what comes out of my mouth. I don't plan what to say before hand. I don't even know what I will say, nor even what I am saying, till I say it. Most of live conversations are in instruct mode within humans. We have no idea which combination of words we will use. Funnily enough, many of us would defend we DO KNOW what we were about to say, and we will give a convincing defense, that we only created after someone told us we didn't know what we are saying. Like LLMs.&lt;/p&gt; &lt;p&gt;For writing this text I'm trying to use the reasoning mode (thinking about this topic since a very long ago before writing about it).&lt;/p&gt; &lt;p&gt;Our abstract logic, that we got from the greeks, was an extraction out of language: Aristotle checked many typical arguments and extracted the words to have a pure philosophical device. For example:&lt;/p&gt; &lt;p&gt;&amp;quot;If I eat a lot I get fat&amp;quot; is a concrete sentence but with a structure that happens in language time and time again. You can represent that structure like: &amp;quot;a --&amp;gt; b&amp;quot;. If I understand how LLMs are able to &amp;quot;reason&amp;quot;, is because they are trained on a huge amount of human texts. The human texts are full of sentences with structures. So their answers use those structures too. Their &amp;quot;smartness&amp;quot; is just the abstract juice of many probable structures usually found in the human corpus.&lt;/p&gt; &lt;p&gt;So their way to using logic reminds me a lot of how we also got there.&lt;/p&gt; &lt;p&gt;I would say the biggest difference is emotions. We relate to words and events emotionally. For example, as kids we may get a very strong impression from some event, and in the future we will relate our new learnings and doings with that impression. I guess that's how writers/creators get their motivations to create outputs about certain topics, or to relate certain stuff to other certain stuff.&lt;/p&gt; &lt;p&gt;Do you think we are indeed similar to LLMs in some ways? &lt;strong&gt;Maybe in some other ways I didn't mention?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Do you think creating AI is a way of discovering how the human mind works?&lt;/strong&gt; I mean, we try to create intelligence, and since we are the ones deciding the definition of words, and we seem to be quite narcissistic, we call &amp;quot;intelligence&amp;quot; that, what we have. We create stuff to our image.&lt;/p&gt; &lt;p&gt;I think creating AI is interesting to understand ourselves, because it offers a way of thinking about us from an external point of view. We are all biased when thinking about ourselves. That's why it is super easy to find defects from other fellow humans, but it is hard to observe them in ourselves. Science Fiction stories usually take common social troubles and put them in a very far away / future civilization, thus making it easier for people to read about it without getting as emotional and biased as if the story would be based in the present time, where we have strong believes, strong polarized opinions and so on.&lt;/p&gt; &lt;p&gt;Also, since the human mind has other modules beside our LLM, &lt;strong&gt;do you think it would be wise to try to avoid using our LLM (words) for many tasks, and only use it for tasks where reasoning with words gives an advantage&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;We humans can get very verbose (just check the length of this post), and many times it is just noise coming out of the black box of our mind. Words can be poisonous: they prevent us from being in the NOW. They put our minds in virtual worlds (like religion) instead of being present.&lt;/p&gt; &lt;p&gt;With all this said, I leave you to enjoy your now. Cheers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mouseofcatofschrodi"&gt; /u/mouseofcatofschrodi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzezr3/are_humans_like_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzezr3/are_humans_like_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzezr3/are_humans_like_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T10:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzd0j7</id>
    <title>Has anyone built a RAG on WikiLeaks?</title>
    <updated>2025-12-30T08:41:14+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because that would be a useful application. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyyp59</id>
    <title>What is the best way to allocated $15k right now for local LLMs?</title>
    <updated>2025-12-29T21:26:38+00:00</updated>
    <author>
      <name>/u/LargelyInnocuous</name>
      <uri>https://old.reddit.com/user/LargelyInnocuous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best bang for $15k right now? Would like to be able to run DeepSeek, Kimi K2 and GLM 4.5+. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LargelyInnocuous"&gt; /u/LargelyInnocuous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T21:26:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pze13o</id>
    <title>Exploring a 1.58-bit / ternary LLM core inspired by BitNet (CUDA attention, GTX 1050 tests)</title>
    <updated>2025-12-30T09:44:36+00:00</updated>
    <author>
      <name>/u/HuseyinKama</name>
      <uri>https://old.reddit.com/user/HuseyinKama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with extreme low-bit LLM inference inspired by the BitNet 1.58-bit paper,&lt;/p&gt; &lt;p&gt;and wanted to share a research-style project I‚Äôve been working on over the last few weeks.&lt;/p&gt; &lt;p&gt;This is NOT a production-ready model, but rather an exploration of how far ternary / sparse logic&lt;/p&gt; &lt;p&gt;can be pushed on consumer GPUs.&lt;/p&gt; &lt;p&gt;What this project explores:&lt;/p&gt; &lt;p&gt;- A custom LLM core using ternary weights {-1, 0, +1}&lt;/p&gt; &lt;p&gt;- Trainable via Straight-Through Estimator (STE)&lt;/p&gt; &lt;p&gt;- Custom CUDA attention kernel (thresholded / shifted-ReLU instead of softmax)&lt;/p&gt; &lt;p&gt;- Designed for local inference (tested on GTX 1050)&lt;/p&gt; &lt;p&gt;Core ideas:&lt;/p&gt; &lt;p&gt;- Replace FP16-heavy matmul layers with ternary linear layers&lt;/p&gt; &lt;p&gt;- Abs-mean scaling (BitNet-style quantization)&lt;/p&gt; &lt;p&gt;- Focus on reducing interference via sparsity rather than magnitude precision&lt;/p&gt; &lt;p&gt;- Attention without softmax to reduce compute and improve stability in low-bit regimes&lt;/p&gt; &lt;p&gt;Current results:&lt;/p&gt; &lt;p&gt;- End-to-end training works&lt;/p&gt; &lt;p&gt;- Overfitting tests succeed (Python training ‚Üí CUDA inference consistency)&lt;/p&gt; &lt;p&gt;- Character-level Shakespeare training produces coherent output&lt;/p&gt; &lt;p&gt;- Memory footprint is significantly reduced compared to FP16 baselines&lt;/p&gt; &lt;p&gt;Limitations / open problems:&lt;/p&gt; &lt;p&gt;- Not competitive with large FP16/INT8 models (expected)&lt;/p&gt; &lt;p&gt;- Sensitive to threshold and temperature tuning&lt;/p&gt; &lt;p&gt;- No advanced optimizations like FlashAttention&lt;/p&gt; &lt;p&gt;- Very much a research prototype&lt;/p&gt; &lt;p&gt;I‚Äôm mainly sharing this to get feedback from people who:&lt;/p&gt; &lt;p&gt;- Have worked with BitNet / ternary networks&lt;/p&gt; &lt;p&gt;- Experiment with custom CUDA kernels&lt;/p&gt; &lt;p&gt;- Care about local / low-power LLM inference&lt;/p&gt; &lt;p&gt;Code and CUDA kernels are available here for anyone curious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QKV-Core/Trion"&gt;https://github.com/QKV-Core/Trion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer technical questions or discuss design tradeoffs.&lt;/p&gt; &lt;p&gt;EDIT / Clarification:&lt;/p&gt; &lt;p&gt;This is not a commercial project, not a startup pitch, and not a benchmark claim.&lt;/p&gt; &lt;p&gt;I‚Äôm sharing this as an experimental research / engineering exploration inspired by the BitNet 1.58-bit paper.&lt;/p&gt; &lt;p&gt;The goal is to understand how far ternary + sparse computation can go on consumer hardware.&lt;/p&gt; &lt;p&gt;No paid product, no token, no API, no funding.&lt;/p&gt; &lt;p&gt;Just code, CUDA kernels, and learning.&lt;/p&gt; &lt;p&gt;Feedback and criticism are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HuseyinKama"&gt; /u/HuseyinKama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T09:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz67hm</id>
    <title>5 new korean models will be released in 2 hours</title>
    <updated>2025-12-30T02:42:37+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura"&gt;https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Naver, LG, SK, NC, Upstage&lt;/p&gt; &lt;p&gt;All 5 models will be released in 2 to 3 hours. Follow with the YouTube link&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzaz73</id>
    <title>[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the "Thinking Process" (Hidden States).</title>
    <updated>2025-12-30T06:40:32+00:00</updated>
    <author>
      <name>/u/JB_King1919</name>
      <uri>https://old.reddit.com/user/JB_King1919</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt; &lt;img alt="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." src="https://b.thumbs.redditmedia.com/Bpxf3FjdwheVRpbXrcXJxisJg0Hd6sGLSEAe6P2x7Fg.jpg" title="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm a PhD student in &lt;strong&gt;Electromagnetics&lt;/strong&gt;. In my daily work, I deal with fields, waves, and trajectories. When I started playing with Local LLMs, I felt something was missing: we usually look at the &lt;em&gt;output&lt;/em&gt; text or the &lt;em&gt;loss curves&lt;/em&gt;, but we rarely see &lt;strong&gt;how&lt;/strong&gt; the model gets from A to B.&lt;/p&gt; &lt;p&gt;To an RF engineer, reasoning isn't just a probability distribution‚Äîit's a &lt;strong&gt;dynamic flow&lt;/strong&gt; through a high-dimensional space.&lt;/p&gt; &lt;p&gt;So, I built a lightweight Python toolkit to extract hidden states layer-by-layer and visualize them as continuous &lt;strong&gt;2D/3D trajectories&lt;/strong&gt;. I wanted to see if &amp;quot;thoughts&amp;quot; have a geometric shape.&lt;/p&gt; &lt;p&gt;The results were surprisingly consistent. I‚Äôm sharing the tool so you can run it on your own models (Llama, Qwen, Mistral, etc.).&lt;/p&gt; &lt;h1&gt;1. The &amp;quot;Confidence Funnel&amp;quot; (Convergence)&lt;/h1&gt; &lt;p&gt;I found that if you feed the model slightly different prompts about the same concept (e.g., &amp;quot;Define Justice&amp;quot;, &amp;quot;What is Fairness&amp;quot;), the internal states start far apart but &lt;strong&gt;physically collapse&lt;/strong&gt; into a single &amp;quot;attractor basin&amp;quot; as the layers get deeper.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c"&gt;https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; You can use this to test &lt;strong&gt;Prompt Stability&lt;/strong&gt;. If the funnel is tight, the model is sure. If it sprays out at the end, the model is confused or hallucinating.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Llama-3 vs. Qwen-2.5: Different &amp;quot;Thinking Styles&amp;quot;&lt;/h1&gt; &lt;p&gt;This was the coolest find. When I ran the same prompts through different architectures, the &amp;quot;shape&amp;quot; of their thinking was totally different.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8"&gt;https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama-3 (Left):&lt;/strong&gt; Seems to &amp;quot;decide&amp;quot; on the semantics very early (Layers 5-10). The trajectory is direct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen-2.5 (Right):&lt;/strong&gt; Keeps the trajectory expanded (in superposition?) until the very last layers (Layer 20+). It seems to &amp;quot;hold&amp;quot; the ambiguity much longer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; This might give us a geometric way to profile model behaviors beyond just benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Visualizing &amp;quot;Refusal&amp;quot; (The Safety Spike)&lt;/h1&gt; &lt;p&gt;I was curious what RLHF looks like geometrically. I visualized the trajectory when the model refuses a jailbreak versus when it follows a safe instruction.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca"&gt;https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hard Refusal(Red):&lt;/strong&gt; Looks like a particle hitting a brick wall‚Äîa sharp, high-curvature spike.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Soft Steering(Green):&lt;/strong&gt; Looks like a smooth turn. And an obvious &amp;quot;U-turn&amp;quot; at the end of its trajectory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; A visual &amp;quot;Geiger Counter&amp;quot; for safety tuning. You can see if your system prompt is creating a hard wall or a soft guide.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì• The Toolkit&lt;/h1&gt; &lt;p&gt;I packaged this into a Python library with example scripts. It works with local HuggingFace weights (no API needed).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/JBKing514/map_llm_toolkit"&gt;LLM Toolkit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß† The Theory (Optional)&lt;/h1&gt; &lt;p&gt;I‚Äôm not an AI researcher, but I wrote up some notes on the &lt;strong&gt;manifold dynamics&lt;/strong&gt; perspective behind this tool (treating inference as a Langevin flow). If you are interested in the math/physics intuition behind these visualizations or need more info about my experiment setup, I put up a page and my notes here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Project Page &amp;amp; Math:&lt;/strong&gt; &lt;a href="https://jbking514.github.io/map_blog/"&gt;Project GitHub Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Foundational Notes:&lt;/strong&gt; &lt;a href="https://zenodo.org/records/17900444"&gt;Manifold Alignment Protocol (MAP)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what &lt;strong&gt;Mistral&lt;/strong&gt; or &lt;strong&gt;Gemma&lt;/strong&gt; trajectories look like if anyone runs this. Let me know what you find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JB_King1919"&gt; /u/JB_King1919 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T06:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcdu1</id>
    <title>LG K EXAONE 236b</title>
    <updated>2025-12-30T08:02:08+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt; &lt;img alt="LG K EXAONE 236b" src="https://preview.redd.it/1wirc918taag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fcab270f79f71dba1d330db0ee8e85422de763b" title="LG K EXAONE 236b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will be released in few days &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wirc918taag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcrtb</id>
    <title>Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model</title>
    <updated>2025-12-30T08:26:06+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt; &lt;img alt="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" src="https://preview.redd.it/yq8uriwhxaag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d990cf5383783b3e2aa22351ddeb29ebac5eb2b2" title="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to open-source Tencent HY-Motion 1.0, a billion-parameter text-to-motion model built on the Diffusion Transformer (DiT) architecture and flow matching. Tencent HY-Motion 1.0 empowers developers and individual creators alike by transforming natural language into high-fidelity, fluid, and diverse 3D character animations, delivering exceptional instruction-following capabilities across a broad range of categories. The generated 3D animation assets can be seamlessly integrated into typical 3D animation pipelines.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;üîπBillion-Scale DiT: Successfully scaled flow-matching DiT to 1B+ parameters, setting a new ceiling for instruction-following capability and generated motion quality.&lt;/p&gt; &lt;p&gt;üîπFull-Stage Training Strategy: The industry‚Äôs first motion generation model featuring a complete Pre-training ‚Üí SFT ‚Üí RL loop to optimize physical plausibility and semantic accuracy.&lt;/p&gt; &lt;p&gt;üîπComprehensive Category Coverage: Features 200+ motion categories across 6 major classes‚Äîthe most comprehensive in the industry, curated via a meticulous data pipeline.&lt;/p&gt; &lt;p&gt;üåêProject Page: &lt;a href="https://hunyuan.tencent.com/motion"&gt;https://hunyuan.tencent.com/motion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóGithub: &lt;a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0"&gt;https://github.com/Tencent-Hunyuan/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§óHugging Face: &lt;a href="https://huggingface.co/tencent/HY-Motion-1.0"&gt;https://huggingface.co/tencent/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑTechnical report: &lt;a href="https://arxiv.org/pdf/2512.23464"&gt;https://arxiv.org/pdf/2512.23464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yq8uriwhxaag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcj1q</id>
    <title>Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy</title>
    <updated>2025-12-30T08:11:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt; &lt;img alt="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" src="https://b.thumbs.redditmedia.com/d_jApTNkEXlNvJcoJA6qryuDnUo0ni-DFWBY6RTdAfg.jpg" title="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/collections/tencent/hy-mt15"&gt;https://huggingface.co/collections/tencent/hy-mt15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights: üîπ 1.8B On-Device Power: Optimized for consumer hardware with a 1GB memory footprint. Using on-policy distillation to align with larger models, it delivers 0.18s latency (50 tokens), outperforming mainstream commercial APIs. üîπ 7B SOTA Performance: An upgraded version of our WMT25 champion, surpassing mid-sized open-source models and rivaling the 90th percentile of closed-source giants like Gemini-3.0-Pro. üîπ 33+ Languages &amp;amp; Dialects: High-fidelity translation across 33 languages and 5 Chinese dialects. üîπ Production-Ready: Native support for custom terminology, long-dialogue context, and maintaining document formatting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzcj1q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7mxr</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:49:11+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if this is real, but the author provides a fascinating story behind its acquisition. I would like for it to be real!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bartowski GGUFs: &lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz68fz</id>
    <title>Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.</title>
    <updated>2025-12-30T02:43:48+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt; &lt;img alt="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." src="https://preview.redd.it/ocq43c2a79ag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e9f7477bee69f806ab9bab82c73557ea1345393" title="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ocq43c2a79ag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7bmv</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:34:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt; &lt;img alt="Llama-3.3-8B-Instruct" src="https://external-preview.redd.it/F-RvVhAB2x8ac9OzOxDw905YUEWDIOQBeDMa2ZyMwo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a109a6917bc66847cb36c61990f58523049b666" title="Llama-3.3-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;allura-forge&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct#llama-33-8b-instruct"&gt;&lt;/a&gt;&lt;strong&gt;Llama 3.3 8B Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yes, this is official, and yes, this is, to my knowledge, a real version of Llama 3.3 8B. (I think, anyways)&lt;/p&gt; &lt;p&gt;Facebook has a &lt;a href="https://llama.developer.meta.com"&gt;Llama API&lt;/a&gt; available that allows for inference of the other Llama models (L3.3 70B, L4 Scout and Maverick), but &lt;em&gt;also&lt;/em&gt; includes a special, new (according to the original press release) &amp;quot;Llama 3.3 8B&amp;quot; that didn't exist anywhere else and was stuck behind the Facebook API!&lt;/p&gt; &lt;p&gt;However. The Llama API supports finetuning L3.3... &lt;em&gt;and downloading the final model in HF format.&lt;/em&gt; Problem solved, right?&lt;/p&gt; &lt;p&gt;Wellllllllllllllll. Not really. The finetuning API was hidden behind layers of support tickets. I tried when the original API dropped in April, and was just told &amp;quot;We'll think about it and send you any updates&amp;quot; (there never were any updates).&lt;/p&gt; &lt;p&gt;Flash forward to December, on a whim I decide to look at the API again. And... by god... the finetuning tab was there. I could click on it and start a job (please ignore that I have no idea how it works, and in fact the finetuning tab actually disappeared after the first time I clicked on it, though I could still manually go to the page).&lt;/p&gt; &lt;p&gt;Apparently, this was not very well tested, as there were a good few bugs, the UI was janky, and the download model function did not actually work due to CORS (I had to manually curl things to get the CDN link).&lt;/p&gt; &lt;p&gt;But... by god... the zip file downloaded, and I had my slightly finetuned model.&lt;/p&gt; &lt;p&gt;To my shock and delight, however, they also provide the adapter that they merged into the model. That means I can &lt;em&gt;subtract&lt;/em&gt; that adapter and get the original model. And... here we are!&lt;/p&gt; &lt;p&gt;(actually, it should be ‚Äúnew model,‚Äù but I used ‚Äúother‚Äù to avoid triggering people)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
