<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-25T21:06:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qmswmc</id>
    <title>[Project] I built a process supervisor for local agents (CrewAI/AutoGen) to prevent infinite loops and runaway costs.</title>
    <updated>2026-01-25T19:34:41+00:00</updated>
    <author>
      <name>/u/Bubbly_Gap6378</name>
      <uri>https://old.reddit.com/user/Bubbly_Gap6378</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few days ago, I asked this sub how everyone handles &amp;quot;kill switches&amp;quot; for local agents. The consensus was mostly &amp;quot;manual monitoring&amp;quot; or &amp;quot;just pull the plug.&amp;quot;&lt;/p&gt; &lt;p&gt;I wasn't super comfortable leaving Llama 3.3 running unattended with that strategy (I‚Äôve had agents get stuck in retry loops that burn serious compute/API credits overnight).&lt;/p&gt; &lt;p&gt;So I spent the weekend building a small CLI tool to solve this specific &amp;quot;supervisor&amp;quot; problem. It‚Äôs called &lt;strong&gt;Vallignus&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Frameworks like CrewAI often swallow exceptions or get stuck in &lt;code&gt;while&lt;/code&gt; loops when the LLM hallucinates a tool call. If you aren't watching the terminal, they spin forever.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution (How this works):&lt;/strong&gt; It wraps your Python execution command and monitors the process group from the &lt;em&gt;outside&lt;/em&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Command:&lt;/strong&gt; &lt;code&gt;vallignus run --max-runtime 300 -- python&lt;/code&gt; &lt;a href="http://agent.py"&gt;&lt;code&gt;agent.py&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enforcement:&lt;/strong&gt; It tracks Wall Time and Output Size. If the agent exceeds limits, it sends &lt;code&gt;SIGTERM&lt;/code&gt; (and then &lt;code&gt;SIGKILL&lt;/code&gt; if it hangs) to the entire process group.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Forensics:&lt;/strong&gt; It captures &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt; to a &lt;code&gt;.jsonl&lt;/code&gt; file, so you can replay the logs and see exactly &lt;em&gt;why&lt;/em&gt; the model started looping.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo (MIT License):&lt;/strong&gt; &lt;a href="https://github.com/jacobgadek/vallignus"&gt;https://github.com/jacobgadek/vallignus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs a simple utility, but it makes running local swarms feel a lot safer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bubbly_Gap6378"&gt; /u/Bubbly_Gap6378 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmswmc/project_i_built_a_process_supervisor_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmswmc/project_i_built_a_process_supervisor_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmswmc/project_i_built_a_process_supervisor_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T19:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmev6q</id>
    <title>[R] Open-sourcing an unfinished research project: A Self-Organizing, Graph-Based Alternative to Transformers (Looking for feedback or continuation</title>
    <updated>2026-01-25T09:40:12+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm sharing a research project I worked on over a long period but had to pause due to personal reasons. Rather than letting it sit idle, I wanted to open it up to the community either for technical feedback, critique, or for anyone interested in continuing or experimenting with it.&lt;/p&gt; &lt;p&gt;The main project is called Self-Organizing State Model (SOSM): &lt;a href="https://github.com/PlanetDestroyyer/Self-Organizing-State-Model"&gt;https://github.com/PlanetDestroyyer/Self-Organizing-State-Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At a high level, the goal was to explore an alternative to standard Transformer attention by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Using graph-based routing instead of dense attention&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Separating semantic representation and temporal pattern learning&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Introducing a hierarchical credit/attribution mechanism for better interpretability&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The core system is modular and depends on a few supporting components: Semantic representation module (MU) &lt;a href="https://github.com/PlanetDestroyyer/MU"&gt;https://github.com/PlanetDestroyyer/MU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Temporal pattern learner (TEMPORAL) &lt;a href="https://github.com/PlanetDestroyyer/TEMPORAL"&gt;https://github.com/PlanetDestroyyer/TEMPORAL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hierarchical / K-1 self-learning mechanism &lt;a href="https://github.com/PlanetDestroyyer/self-learning-k-1"&gt;https://github.com/PlanetDestroyyer/self-learning-k-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm honestly not sure how valuable or novel this work is that‚Äôs exactly why I‚Äôm posting it here. If nothing else, I‚Äôd really appreciate constructive criticism, architectural feedback, or pointers to related work that overlaps with these ideas. If someone finds parts of it useful (or wants to take it further, refactor it, or formalize it into a paper), they‚Äôre more than welcome to do so. The project is open-source, and I‚Äôm happy to answer questions or clarify intent where needed.&lt;/p&gt; &lt;p&gt;Thanks for taking a look.&lt;/p&gt; &lt;p&gt;Summary:&lt;/p&gt; &lt;p&gt;This work explores a language model architecture based on structured semantics rather than unstructured embeddings. Instead of positional encodings, a temporal learning module is used to model sequence progression and context flow. A K-1 hierarchical system is introduced to provide interpretability, enabling analysis of how a token is predicted and which components, states, or nodes contribute to that prediction. Most importantly, rather than comparing every token with all others (as in full self-attention), the model uses a graph-based connection mechanism that restricts computation to only the most relevant or necessary tokens, enabling selective reasoning and improved efficiency.&lt;/p&gt; &lt;p&gt;(Have used claude code to code )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjyxl</id>
    <title>Understanding Multi-Head Latent Attention (MLA)</title>
    <updated>2026-01-25T14:05:43+00:00</updated>
    <author>
      <name>/u/shreyansh26</name>
      <uri>https://old.reddit.com/user/shreyansh26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A short deep-dive on Multi-Head Latent Attention (MLA) (from DeepSeek): intuition + math, then a walk from MHA ‚Üí GQA ‚Üí MQA ‚Üí MLA, with PyTorch code and the fusion/absorption optimizations for KV-cache efficiency.&lt;/p&gt; &lt;p&gt;&lt;a href="http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/"&gt;http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shreyansh26"&gt; /u/shreyansh26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmevh7</id>
    <title>Blazing fast JSON extraction with very small LLMs-3B: LSTM to LLM</title>
    <updated>2026-01-25T09:40:42+00:00</updated>
    <author>
      <name>/u/memphet</name>
      <uri>https://old.reddit.com/user/memphet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've learned a lot from this sub, so I wanted to give back by sharing my experience on a recent project.&lt;/p&gt; &lt;p&gt;My goal was to migrate a text extraction pipeline from LSTM to an LLM. The task involves extracting specific data into JSON format from small text inputs (‚âà1024 tokens). I used in-house data to fine-tune it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Constraints &amp;amp; Achievements (running on an L4 GPU):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Very low end2end latency:&lt;/strong&gt; &amp;lt;500ms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High throughput:&lt;/strong&gt; ‚âà30 RPM (requests per minute)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability:&lt;/strong&gt; 0.99 accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Model:&lt;/strong&gt;&lt;br /&gt; I tested quite a few models for this task.&lt;br /&gt; Ultimately, HuggingFaceTB/SmolLM3-3B was the best fit for our needs.&lt;br /&gt; I also had very strong results with Qwen/Qwen3-4B-Instruct and Ministral&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning parameters matter less than I thought:&lt;/strong&gt; I didn't see huge gains from strictly tweaking hyperparameters. I ran extensive hyperparameter optimization only to find that simply increasing the number of epochs yielded the best (slight) improvements.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data is king:&lt;/strong&gt; Poor labeling logic and bad data quality hurt me the most. If I had to redo it, I would spend much more time cleaning and validating the dataset upfront.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small LLMs struggle with Proper Nouns:&lt;/strong&gt; I noticed about a 10% error rate on names! A significant performance boost came from adding a simple post-processing step using Levenshtein distance to correct names extracted by the LLM against the input text (correcting &amp;quot;Jammes&amp;quot; -&amp;gt; &amp;quot;James&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Efficiency Gains:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Obviously the best bang for your buck. I recommend &lt;strong&gt;FP8&lt;/strong&gt; using llm-compressor if you have a Lovelace GPU or newer. Otherwise, &lt;strong&gt;AWQ&lt;/strong&gt; is solid. &lt;ul&gt; &lt;li&gt;Gain: ~50% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Formatting:&lt;/strong&gt; You want to generate as few tokens as possible. Instead of fine-tuning for a verbose JSON output like {&amp;quot;key1&amp;quot;: &amp;quot;value1&amp;quot;, &amp;quot;key2&amp;quot;: &amp;quot;value2&amp;quot;}, I fine-tuned the model to output just the values: value1,value2. &lt;ul&gt; &lt;li&gt;Gain: ~30% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't work (for me):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I really tried to make &lt;strong&gt;Speculative Decoding&lt;/strong&gt; work with vLLM. In theory, I expected gains even with just n-gram speculative decoding, but I didn't observe any improvement. I did see some speedup using Qwen 0.7B draft model, but since I ultimately chose a different base model architecture, I couldn't use them effectively. Plus, maintaining a base model + a draft model is a pain, which is also why I didn't go with Eagle.&lt;/p&gt; &lt;p&gt;If you have suggestions to squeeze out more performance or thoughts on the setup, I'm all ears!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/memphet"&gt; /u/memphet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmqug5</id>
    <title>Qwen3 vl 8b instruct samplers</title>
    <updated>2026-01-25T18:21:17+00:00</updated>
    <author>
      <name>/u/Aril_1</name>
      <uri>https://old.reddit.com/user/Aril_1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I'm trying to use Qwen vl instruct with koboldcpp using the samplers suggested in the qwen repo and by Unsloth:&lt;/p&gt; &lt;p&gt;temp= 0.7&lt;/p&gt; &lt;p&gt;top_p=0.8&lt;/p&gt; &lt;p&gt;top_k= 20&lt;/p&gt; &lt;p&gt;presence_penalty=1.5&lt;/p&gt; &lt;p&gt;The problem is that for any kind of use, from general assistant, to coding, or for agentic tool calling use, it has fairly poor performance, often even using incorrect json syntax.&lt;/p&gt; &lt;p&gt;Should I change something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aril_1"&gt; /u/Aril_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmqug5/qwen3_vl_8b_instruct_samplers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmqug5/qwen3_vl_8b_instruct_samplers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmqug5/qwen3_vl_8b_instruct_samplers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmutct</id>
    <title>anyone running local llm on iphone for meeting summaries? heres what im using</title>
    <updated>2026-01-25T20:43:51+00:00</updated>
    <author>
      <name>/u/xerdink</name>
      <uri>https://old.reddit.com/user/xerdink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;been messing around with local inference on ios for a meeting notes app im building. wanted to share what works and what doesnt&lt;/p&gt; &lt;p&gt;setup: - whisper for transcription (the small model runs surprisingly well on neural engine) - tried a few different llms for summaries&lt;/p&gt; &lt;p&gt;what i learned: - quantized models are basically required, anything bigger than 2-3B params is too slow - coreml conversion is a pain but worth it for speed - battery drain is real lol, gotta be careful with inference frequency&lt;/p&gt; &lt;p&gt;the whole thing runs offline which was the main goal. didnt want any cloud nonsense after reading about all those &lt;a href="http://otter.ai"&gt;otter.ai&lt;/a&gt; privacy issues&lt;/p&gt; &lt;p&gt;curious what models you guys are using for on device stuff? esp interested in anything good for summarization thats small enough for mobile&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xerdink"&gt; /u/xerdink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmutct/anyone_running_local_llm_on_iphone_for_meeting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmutct/anyone_running_local_llm_on_iphone_for_meeting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmutct/anyone_running_local_llm_on_iphone_for_meeting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmh3si</id>
    <title>What happened to moondream3?</title>
    <updated>2026-01-25T11:49:01+00:00</updated>
    <author>
      <name>/u/StableDiffer</name>
      <uri>https://old.reddit.com/user/StableDiffer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So last year the moondream 3 preview came out. It was a nice performing visual model that could do some cool stuff other VL models couldn't. One month ago a MLX version appeared &lt;a href="https://huggingface.co/moondream/md3p-int4"&gt;https://huggingface.co/moondream/md3p-int4&lt;/a&gt; but until now there is no llama.cpp implementation and no public activity I could find.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableDiffer"&gt; /u/StableDiffer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T11:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmmpwz</id>
    <title>DGX spark performance falls short</title>
    <updated>2026-01-25T15:53:00+00:00</updated>
    <author>
      <name>/u/dereksodo</name>
      <uri>https://old.reddit.com/user/dereksodo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;using cutlass-profiler, gemm, here is the performance:&lt;/p&gt; &lt;p&gt;peak int4: 157 TFLOP&lt;/p&gt; &lt;p&gt;peak int8: 200 TFLOP&lt;/p&gt; &lt;p&gt;peak fp16: 97 TFLOP&lt;/p&gt; &lt;p&gt;anyone knows why performance of int4 is not around 350-450( which i expect)?&lt;/p&gt; &lt;p&gt;env: docker (pytorch:25.12-py3)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dereksodo"&gt; /u/dereksodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlyhn</id>
    <title>GLM-4.7-flash on RTX 6000 pro</title>
    <updated>2026-01-25T15:24:37+00:00</updated>
    <author>
      <name>/u/gittb</name>
      <uri>https://old.reddit.com/user/gittb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I‚Äôm getting horrible throughput considering the models size with vLLM.&lt;/p&gt; &lt;p&gt;Currently with 2x cards and DP 2 @ FP16 I‚Äôm getting around 370 gen TPS with 10x requests.&lt;/p&gt; &lt;p&gt;Anyone have a fix or a ‚Äúworking‚Äù config for 1 or two cards?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gittb"&gt; /u/gittb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmttek</id>
    <title>Built a Clickable 3D Solar System Explorer in 15 Minutes with MiniMax Agent</title>
    <updated>2026-01-25T20:07:21+00:00</updated>
    <author>
      <name>/u/Grand_Excuse1776</name>
      <uri>https://old.reddit.com/user/Grand_Excuse1776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been playing around with MiniMax Agent a lot lately, and I just built this interactive Solar System Explorer web app purely from prompts‚Äîno manual coding or hosting needed. Live link: &lt;a href="https://fdgzvcv7zkki.space.minimax.io/"&gt;https://fdgzvcv7zkki.space.minimax.io/&lt;/a&gt; What I actually use MiniMax Agent for the most (why I keep coming back): Honestly, I use it a ton for quick personal productivity and creative side stuff. Like: ‚Ä¢ Automating repetitive reports/tasks (e.g., turning notes into formatted summaries or site inspection outlines if I‚Äôm brainstorming workflows). ‚Ä¢ Building mini web tools/dashboards fast like this explorer for learning/teaching, or simple calculators/trackers. ‚Ä¢ Prototyping business ideas without dev time (e.g., mocked-up landing pages, forms, or even basic apps I can share/test). ‚Ä¢ Fun educational/exploratory things like simulations, games, or visuals from prompts (their multimodal stuff shines here). It‚Äôs agentic (plans multi-step, codes, deploys), cheaper/faster than some alternatives for what I need, and the instant deployment is addictive. No more ‚ÄúI‚Äôll build this later‚Äù excuses. This solar system one is just a fun example to show how accessible it is for non-coders too. If you‚Äôre into AI agents or no-code-ish building, definitely worth trying with new users get starter credits, and shares like this earn more #minimaxagent #minimax&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grand_Excuse1776"&gt; /u/Grand_Excuse1776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmttek/built_a_clickable_3d_solar_system_explorer_in_15/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmttek/built_a_clickable_3d_solar_system_explorer_in_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmttek/built_a_clickable_3d_solar_system_explorer_in_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmp02r</id>
    <title>Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework.</title>
    <updated>2026-01-25T17:15:06+00:00</updated>
    <author>
      <name>/u/switchdoor1</name>
      <uri>https://old.reddit.com/user/switchdoor1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"&gt; &lt;img alt="Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework." src="https://preview.redd.it/9jgcnzd03jfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b78448eca7d34ac66b090e11ac5facec8f13ce8" title="Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Ultimate goal with this project was to build an agentic testing framework that can automatically stress-test chatbots across multiple dimensions - off-topic handling, safety concerns, hallucination detection, system prompt extraction attempts, and more. The system uses AI agents to generate diverse test personalities and scenarios, then runs them against your chatbot and evaluates the responses. Set it up and you can start stacking up test data for continuous improvement.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Prajwal-Nagaraj/Chatbot-Simulation-Workflow"&gt;https://github.com/Prajwal-Nagaraj/Chatbot-Simulation-Workflow&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Stack:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: GPT-OSS 20B running via llama.cpp server (local, no API keys needed)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Engine&lt;/strong&gt;: Agno framework for orchestrating multi-agent workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: FastAPI with async support for long-running test suites&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Modern but basic web ui using js and html&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Features:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Testing&lt;/strong&gt;: LLM generates realistic user personalities and test scenarios and also communicates with the chatbot endpoint&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-a-Judge Evaluation&lt;/strong&gt;: Automated scoring of chatbot responses using LLM as a judge&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Test Types&lt;/strong&gt;: off topic, safety, hallucination, system prompt testing, financial advice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Configuration&lt;/strong&gt;: CLI, YAML configs, or web UI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Execution&lt;/strong&gt;: Long test suites run in background&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database Persistence&lt;/strong&gt;: All test sessions, personalities, scenarios, and results stored in a sqlite binary The workflow is pretty wild - it generates personalities, creates scenarios for each, runs conversations, and uses an LLM judge to evaluate everything automatically. You just point it at your Openai compatible chatbot endpoint and let it rip.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/switchdoor1"&gt; /u/switchdoor1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9jgcnzd03jfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T17:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbhh</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:21:50+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt; &lt;img alt="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" src="https://b.thumbs.redditmedia.com/tY-PA8qRCq6_itenx-ibWJJ7urdsbE45bXySDC1FH4s.jpg" title="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The Qwen team just dropped &lt;strong&gt;Qwen3-TTS&lt;/strong&gt;, and it‚Äôs a significant step forward for local speech synthesis. If you‚Äôve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAI‚Äôs TTS that you can actually run on your own hardware, this is it.&lt;/p&gt; &lt;p&gt;We‚Äôve put together a repository that provides an &lt;strong&gt;OpenAI-compatible FastAPI server&lt;/strong&gt;, meaning you can use it as a drop-in replacement for any app already using OpenAI‚Äôs TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.&lt;/p&gt; &lt;h1&gt;Why this is a big deal:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Insane Speed:&lt;/strong&gt; It features a dual-track hybrid architecture that hits ~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Voice Control:&lt;/strong&gt; You don't just send text; you can give it natural language instructions like &lt;em&gt;&amp;quot;Say this in an incredibly angry tone&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;A shaky, nervous 17-year-old voice&amp;quot;&lt;/em&gt; and it actually follows through.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Voice Cloning:&lt;/strong&gt; Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Drop-in:&lt;/strong&gt; Works natively with the OpenAI Python client. Just change your &lt;code&gt;base_url&lt;/code&gt; to localhost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started (The Quick Way)&lt;/h1&gt; &lt;p&gt;If you have Docker and a GPU, you can get this running in seconds:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi docker build -t qwen3-tts-api . docker run --gpus all -p 8880:8880 qwen3-tts-api &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Python Usage (OpenAI Style)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(base_url=&amp;quot;http://localhost:8880/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;) response = client.audio.speech.create( model=&amp;quot;qwen3-tts&amp;quot;, voice=&amp;quot;Vivian&amp;quot;, # 9 premium voices included input=&amp;quot;This sounds way too human for a local model.&amp;quot;, speed=1.0 ) response.stream_to_file(&amp;quot;output.mp3&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It uses the new &lt;strong&gt;Qwen3-TTS-Tokenizer-12Hz&lt;/strong&gt; for acoustic compression. It skips the traditional &amp;quot;LM + DiT&amp;quot; bottleneck, which is why the latency is so low.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Sizes:&lt;/strong&gt; Available in &lt;strong&gt;0.6B&lt;/strong&gt; (super fast/light) and &lt;strong&gt;1.7B&lt;/strong&gt; (high fidelity) versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Friendly:&lt;/strong&gt; Supports FlashAttention 2 to keep memory usage down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links to dive deeper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;ü§ó Hugging Face Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.15621"&gt;üìÑ Research Paper on arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;üíª Github Repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.&lt;/p&gt; &lt;p&gt;Let me know if you run into any issues setting it up!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d"&gt;https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T21:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmhvuz</id>
    <title>Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.</title>
    <updated>2026-01-25T12:30:24+00:00</updated>
    <author>
      <name>/u/Charming_Group_2950</name>
      <uri>https://old.reddit.com/user/Charming_Group_2950</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; You build a RAG system. It gives an answer. It sounds right.&lt;br /&gt; But is it actually grounded in your data, or just hallucinating with confidence?&lt;br /&gt; A single &amp;quot;correctness&amp;quot; or &amp;quot;relevance&amp;quot; score doesn‚Äôt cut it anymore, especially in enterprise, regulated, or governance-heavy environments. We need to know why it failed. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;My solution:&lt;/strong&gt;&lt;br /&gt; Introducing &lt;strong&gt;TrustifAI&lt;/strong&gt; ‚Äì a framework designed to quantify, explain, and debug the trustworthiness of AI responses. &lt;/p&gt; &lt;p&gt;Instead of pass/fail, it computes a multi-dimensional Trust Score using signals like:&lt;br /&gt; * Evidence Coverage: Is the answer actually supported by retrieved documents?&lt;br /&gt; * Epistemic Consistency: Does the model stay stable across repeated generations?&lt;br /&gt; * Semantic Drift: Did the response drift away from the given context?&lt;br /&gt; * Source Diversity: Is the answer overly dependent on a single document?&lt;br /&gt; * Generation Confidence: Uses token-level log probabilities at inference time to quantify how confident the model was while generating the answer (not after judging it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;br /&gt; TrustifAI doesn‚Äôt just give you a number - it gives you traceability.&lt;br /&gt; It builds &lt;strong&gt;Reasoning Graphs (DAGs)&lt;/strong&gt; and &lt;strong&gt;Mermaid visualizations&lt;/strong&gt; that show why a response was flagged as reliable or suspicious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How is this different from LLM Evaluation frameworks:&lt;/strong&gt;&lt;br /&gt; All popular Eval frameworks measure how good your RAG system is, but&lt;br /&gt; TrustifAI tells you why you should (or shouldn‚Äôt) trust a specific answer - with explainability in mind.&lt;/p&gt; &lt;p&gt;Since the library is in its early stages, I‚Äôd genuinely love community feedback.&lt;br /&gt; ‚≠ê the repo if it helps üòÑ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;code&gt;pip install trustifai&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github link:&lt;/strong&gt; &lt;a href="https://github.com/Aaryanverma/trustifai"&gt;https://github.com/Aaryanverma/trustifai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Group_2950"&gt; /u/Charming_Group_2950 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmhvuz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T12:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmo3do</id>
    <title>LM Studio - Why does my system RAM fill up and go OOM if the model says Full GPU Offload Possible?</title>
    <updated>2026-01-25T16:42:16+00:00</updated>
    <author>
      <name>/u/Nytse</name>
      <uri>https://old.reddit.com/user/Nytse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Windows, RTX 3090 (24 GB VRAM) upgraded recently from a GTX 1080 (8 GB), 32 GB RAM&lt;/p&gt; &lt;p&gt;With Firefox open with many tabs I use ~18 GB RAM. GPU stays at ~3 GB.&lt;/p&gt; &lt;p&gt;Then, in LM Studio, loading the OpenAI GPT‚ÄëOSS 20B model shows ‚ÄúFull GPU Offload Possible‚Äù. After load, VRAM jumps to ~14 GB and system RAM climbs to 32 GB, then the program crashes with OOM.&lt;/p&gt; &lt;p&gt;I have Strict Guardrails enabled, swap is on.&lt;/p&gt; &lt;p&gt;How can I avoid high RAM usage and the OOM when loading this model while using by browser? How do I know how much allocated RAM the model will have?&lt;/p&gt; &lt;p&gt;I thought that the gguf file size is similar to the VRAM allocation and only like 1 GB RAM is reserved if the model fits in the GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nytse"&gt; /u/Nytse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T16:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmt8ei</id>
    <title>[Rust/AVX-512] I built a Zero-Copy 1.58-bit LLM Engine hitting 117 Tokens/s on a single CPU core. I need help fixing the final Activation layer.</title>
    <updated>2026-01-25T19:46:39+00:00</updated>
    <author>
      <name>/u/dhilip-siva</name>
      <uri>https://old.reddit.com/user/dhilip-siva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The Project:&lt;/strong&gt; I am building &lt;strong&gt;R3-Engine&lt;/strong&gt;, a from-scratch, local AI inference engine for Microsoft's &lt;code&gt;bitnet-b1.58-2B-4T&lt;/code&gt;. It is written in 100% Safe Rust, natively cross-compiles to Wasm SIMD128, and uses Zero heap allocations in the execution loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Physics:&lt;/strong&gt; By mapping a 64-byte aligned &lt;code&gt;.r3&lt;/code&gt; file directly from NVMe to CPU L3 Cache (Zero-Copy) and using AVX-512 &lt;code&gt;VPOPCNTDQ&lt;/code&gt; for branchless math, the Ryzen 9950X3D achieves &lt;strong&gt;117 Tokens/Second&lt;/strong&gt; latency.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem: The AI is mute (Outputting&lt;/strong&gt; &lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; The matrix multiplication pipeline is mathematically complete, but the output is stuck at Token ID 0 (&lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt;). The issue lies in the transition between the quantized weights and the float-based non-linear activations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Where I need expert input:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Weight Tying in BitNet:&lt;/strong&gt; Microsoft's 2B model ties Embeddings with the LM Head. I am cloning the embedding matrix for the output projection, but I suspect a scaling factor is missing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RMSNorm &amp;amp; SiLU in 1.58-bit:&lt;/strong&gt; How should the raw integer accumulators (from the VPOPCNTDQ loop) be scaled before entering the SiLU activation and the subsequent layer?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt;: &lt;a href="https://github.com/r3-engine/r3-engine"&gt;https://github.com/r3-engine/r3-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you know the physics of LLM Logit Sampling or ternary activation math, I would love your eyes on the codebase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dhilip-siva"&gt; /u/dhilip-siva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmt8ei/rustavx512_i_built_a_zerocopy_158bit_llm_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmt8ei/rustavx512_i_built_a_zerocopy_158bit_llm_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmt8ei/rustavx512_i_built_a_zerocopy_158bit_llm_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T19:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmsk9w</id>
    <title>LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens</title>
    <updated>2026-01-25T19:22:14+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt; &lt;img alt="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" src="https://preview.redd.it/gai51kz2pjfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f44d218e6b2a5dc8982ffb434c4c01e0cf195277" title="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Generated from lineage-128 and lineage-192 &lt;a href="http://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; &lt;a href="https://github.com/fairydreaming/lineage-bench-results/tree/main/lineage-8_64_128_192"&gt;benchmark results&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Sorry for overlapping labels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gai51kz2pjfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T19:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmdf2a</id>
    <title>Has anyone got GLM 4.7 flash to not be shit?</title>
    <updated>2026-01-25T08:14:08+00:00</updated>
    <author>
      <name>/u/synth_mania</name>
      <uri>https://old.reddit.com/user/synth_mania</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Real talk. I feel like everyday I'm downloading a new quant and trying it out and not once have I got it to consistently work without looping.&lt;/p&gt; &lt;p&gt;I've tried with and without the suggested settings from unsloth, &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;, and others, to no avail.&lt;/p&gt; &lt;p&gt;Additionally, this has to be the slowest inference I've ever seen from a 30B A3B model. In all fairness, my only point of reference is Qwen3 Coder, but compared to that at least, the token generation speed feels positively lethargic.&lt;/p&gt; &lt;p&gt;If anybody has any tips, please let me know because I feel like I'm going in circles here. I don't think I've ever seen a modern release that had this many issues right off the bat, with no apparent improvement after a few supposed fixes.&lt;/p&gt; &lt;p&gt;It's really unfortunate because I can see the potential this model has. The chain of thought in particular seems uniquely coherent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synth_mania"&gt; /u/synth_mania &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T08:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmrrr4</id>
    <title>ClaraVerse | Local AI workspace (4 months ago) -&gt; Your feedback -&gt; Back with improvements.</title>
    <updated>2026-01-25T18:54:07+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"&gt; &lt;img alt="ClaraVerse | Local AI workspace (4 months ago) -&amp;gt; Your feedback -&amp;gt; Back with improvements." src="https://a.thumbs.redditmedia.com/7F-eZ7FXWPk0GBmRwW4IWCGKcKvAemtDakgsLWI_f-8.jpg" title="ClaraVerse | Local AI workspace (4 months ago) -&amp;gt; Your feedback -&amp;gt; Back with improvements." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;We built an AI workspace that actually gets things done locally (not just another chatbot or AI slope)&lt;/h1&gt; &lt;p&gt;I've been grinding on ClaraVerse for the past few months, and we just dropped a major update. If you're tired of AI tools that just... talk at you, this might be your vibe.&lt;/p&gt; &lt;h1&gt;The TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Run it anywhere&lt;/strong&gt;: CLI tool that works on your laptop, VPS, cloud, whatever. No platform lock-in BS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;50+ integrations&lt;/strong&gt;: Gmail, Sheets, Discord, Slack, you name it. Want more? Just ask.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Actual automation&lt;/strong&gt;: Build agents that DO things, not just answer questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat-first workflow builder&lt;/strong&gt;: Like n8n/Zapier but for AI. Chat your way through creating workflows ask, create, iterate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Everything becomes an API&lt;/strong&gt;: Seriously, every workflow you build = instant API endpoint or schedule it daily, hourly your choice.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;One-liner:&lt;/strong&gt; It's an all-in-one platform (chat, image gen, agents, docs, search). Every tool is part of the package.&lt;/p&gt; &lt;p&gt;What's actually new (beyond UI polish)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built-in tools that agents and chats need:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PPT, PDF, XLSX readers and creators&lt;/li&gt; &lt;li&gt;Isolated code execution with dependency management&lt;/li&gt; &lt;li&gt;Interactive chat so local LLMs can ask clarifying questions mid-prompt&lt;/li&gt; &lt;li&gt;Search, scrape, image search, API tools, and memory all default&lt;/li&gt; &lt;li&gt;Tool router if you have too many tools&lt;/li&gt; &lt;li&gt;Memories that can remember and forget based on your usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;50+ integrations ready to go:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gmail, Sheets, Discord, Slack, and more&lt;/li&gt; &lt;li&gt;Build agents that trigger actual actions, not just suggestions&lt;/li&gt; &lt;li&gt;Schedule workflows and forget about them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For n8n lovers who hate boilerplate:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto-generate workflows from prompts&lt;/li&gt; &lt;li&gt;Chain multiple AI models together&lt;/li&gt; &lt;li&gt;Structured outputs, multi-tool agents, the works&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Better chat UX:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Interactive prompts that ask clarifying questions&lt;/li&gt; &lt;li&gt;Generate images, PDFs, slides, charts in-chat&lt;/li&gt; &lt;li&gt;All integrations work in both chat AND workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Admin and Model Manger:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Manage models and provider in one place&lt;/li&gt; &lt;li&gt;Assign models based on their abilities (tools, text, code, vision, image)&lt;/li&gt; &lt;li&gt;Create alias, check usage and so on with multiple user in same instance&lt;/li&gt; &lt;li&gt;Simple UI works on phone responsive as hell&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it and let us know&lt;/h1&gt; &lt;ul&gt; &lt;li&gt; GitHub: &lt;a href="https://github.com/claraverse-space/ClaraVerse"&gt;github.com/claraverse-space/ClaraVerse&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're open source and privacy-first (chat and data stored in browser or DB, even when self-hosted - user's choice). &lt;/p&gt; &lt;p&gt;I use this myself every day. Honestly, I've seen worse tools raise fund and then lock everything behind subscriptions. This community helped build this with feedback, so it's staying free and open-source.&lt;/p&gt; &lt;p&gt;Happy to answer questions, take feature requests, or hear about how it crashes on your machine so we can fix and improve. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmrrr4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmrjlh</id>
    <title>Do you power off your LLM/AI/SV PC when not using it to save on electricity, or keep it on 24/7? MultiGPU adds a lot of power!</title>
    <updated>2026-01-25T18:45:59+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hoping you're fine.&lt;/p&gt; &lt;p&gt;Wondering here, as electricity is about 0.28USD per kWh on Chile, so I'm kinda forced to have it off most of the time.&lt;/p&gt; &lt;p&gt;My idle power is about 270W with multiple GPUs (7) and no PCIe switches (5090x3,4090x2,A40x1,A6000x1, 9900X), but with a Gen 5 100 lanes switch and a Gen 4 96 lanes switch, I idle at about 370W.&lt;/p&gt; &lt;p&gt;At load it goes it ranges from 900W to 2500W, depending of the backend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:45:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmljeb</id>
    <title>What are the best open source coding ideas you can share?</title>
    <updated>2026-01-25T15:08:39+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt; &lt;img alt="What are the best open source coding ideas you can share?" src="https://preview.redd.it/zcf9q42bgifg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8d5b9a2f9d2a8bbb940fa1ae8f1c616ca45968f" title="What are the best open source coding ideas you can share?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build a place for my friends so they can try and learn ai assisted engineering/vibe coding. Some of them are 50 yrs experienced devs familiar with enterprise standards, some 16 yrs old vibe coders that want to build their first scripts.&lt;/p&gt; &lt;p&gt;How would you structure guide for newcomers? Any favourite tools I should add/replace?&lt;/p&gt; &lt;p&gt;What would you choose for 24h hackathon and what is more suitable for weeks/months project?&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/dontriskit/awesome-ai-software-engineering"&gt;https://github.com/dontriskit/awesome-ai-software-engineering&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zcf9q42bgifg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmu1a1</id>
    <title>GLM-4.7-Flash context slowdown</title>
    <updated>2026-01-25T20:15:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt; &lt;img alt="GLM-4.7-Flash context slowdown" src="https://b.thumbs.redditmedia.com/1XaF-XYj4di3wcMyTbjLQ43nM77xN3jdiZdZndwBcDM.jpg" title="GLM-4.7-Flash context slowdown" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;to check on your setup, run:&lt;br /&gt; (you can use higher -p and -n and modify -d to your needs)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;jacek@AI-SuperComputer:~$ CUDA_VISIBLE_DEVICES=0,1,2 llama-bench -m /mnt/models1/GLM/GLM-4.7-Flash-Q8_0.gguf -d 0,5000,10000,15000,20000,25000,30000,35000,40000,45000,50000 -p 200 -n 200 -fa 1 ggml_cuda_init: found 3 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 | 1985.41 ¬± 11.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 | 95.65 ¬± 0.44 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d5000 | 1392.15 ¬± 12.63 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d5000 | 81.83 ¬± 0.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d10000 | 1027.56 ¬± 13.50 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d10000 | 72.60 ¬± 0.07 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d15000 | 824.05 ¬± 8.08 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d15000 | 64.24 ¬± 0.46 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d20000 | 637.06 ¬± 79.79 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d20000 | 58.46 ¬± 0.14 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d25000 | 596.69 ¬± 11.13 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d25000 | 53.31 ¬± 0.18 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d30000 | 518.71 ¬± 5.25 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d30000 | 49.41 ¬± 0.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d35000 | 465.65 ¬± 2.69 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d35000 | 45.80 ¬± 0.04 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d40000 | 417.97 ¬± 1.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d40000 | 42.65 ¬± 0.05 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d45000 | 385.33 ¬± 1.80 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d45000 | 40.01 ¬± 0.03 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d50000 | 350.91 ¬± 2.17 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d50000 | 37.63 ¬± 0.02 | build: 8f91ca54e (7822) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;real usage of opencode (with 200000 context):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 0 | task 2495 | processing task, is_child = 0 slot update_slots: id 0 | task 2495 | new prompt, n_ctx_slot = 200192, n_keep = 0, task.n_tokens = 66276 slot update_slots: id 0 | task 2495 | n_tokens = 63140, memory_seq_rm [63140, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 65188, batch.n_tokens = 2048, progress = 0.983584 slot update_slots: id 0 | task 2495 | n_tokens = 65188, memory_seq_rm [65188, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 66276, batch.n_tokens = 1088, progress = 1.000000 slot update_slots: id 0 | task 2495 | prompt done, n_tokens = 66276, batch.n_tokens = 1088 slot init_sampler: id 0 | task 2495 | init sampler, took 8.09 ms, tokens: text = 66276, total = 66276 slot print_timing: id 0 | task 2495 | prompt eval time = 10238.44 ms / 3136 tokens ( 3.26 ms per token, 306.30 tokens per second) eval time = 11570.90 ms / 355 tokens ( 32.59 ms per token, 30.68 tokens per second) total time = 21809.34 ms / 3491 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;n_tokens = 66276, 306.30t/s, 30.68t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmu1a1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlpjp</id>
    <title>Internet blackout and Local LLMs</title>
    <updated>2026-01-25T15:15:05+00:00</updated>
    <author>
      <name>/u/DunderSunder</name>
      <uri>https://old.reddit.com/user/DunderSunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).&lt;/p&gt; &lt;p&gt;Meanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunderSunder"&gt; /u/DunderSunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmir5d</id>
    <title>What do you actually want from a private AI chat on your phone?</title>
    <updated>2026-01-25T13:12:00+00:00</updated>
    <author>
      <name>/u/AppDeveloperAsdf</name>
      <uri>https://old.reddit.com/user/AppDeveloperAsdf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt; &lt;img alt="What do you actually want from a private AI chat on your phone?" src="https://external-preview.redd.it/b2k1d3JkaHV0aGZnMbzKSbNJeiRdJL3Vv6uz8BgUY-ES1g_l6yTqUuzYy_d7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f45f222f3ee31c2716f286b9cf0998d79f80e6f" title="What do you actually want from a private AI chat on your phone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends. We are building zerotap - an Android app where AI can control your phone like a human (taps, scrolls, reads screen). It supports Ollama, proxies like OpenRouter and Straico and models directly such as OpenAI, Claude, Gemini and DeepSeek.&lt;/p&gt; &lt;p&gt;Recently we added a chat interface, so now it works like a regular AI chat that can take over your device when needed.&lt;/p&gt; &lt;p&gt;Now we are planning what to focus on next and we'd love your input. Some options we're considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; - connect your chat to external tools and services&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep research&lt;/strong&gt; - letting the AI browse and gather information for you&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modality&lt;/strong&gt; ‚Äî image read &amp;amp; write (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-device models&lt;/strong&gt; ‚Äî we are working on Gemma 3n and Qwen support, but small context windows are hurting performance so much&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speaking of which - for those of you running Ollama: do you expose your instance to the internet or keep it local network only?&lt;/p&gt; &lt;p&gt;Honest question: what would make an AI chat on your phone actually useful for you on a daily basis? Not as a toy, but as something you would rely on - what's missing from current mobile AI apps (that supports ollama) that annoys you the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppDeveloperAsdf"&gt; /u/AppDeveloperAsdf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/em3174huthfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjzx1</id>
    <title>KV cache fix for GLM 4.7 Flash</title>
    <updated>2026-01-25T14:06:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt; &lt;img alt="KV cache fix for GLM 4.7 Flash" src="https://external-preview.redd.it/Yd6yP0tYXhTq7c3g8_wDa0Z1Zijr0IAXDTPXGjQc7ts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=697845700fcf489c797d62fb0a23359703d41821" title="KV cache fix for GLM 4.7 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: remove Air from GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;KV cache uses a lot of VRAM. GLM 4.7 Flash doesn‚Äôt even use V in the KV cache. With long contexts, this means gigabytes of VRAM saved, so you can run much longer context on the same setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
