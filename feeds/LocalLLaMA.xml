<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-12T13:12:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lxz268</id>
    <title>What LLM Workflow UI Are You Using?</title>
    <updated>2025-07-12T12:17:57+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just started experimenting with LLM workflow using n8n, and I built a workflow to improve the translation quality of my local LLM, sure it works but I found it lacking some basic functions, like I need to write JavaScript for some very basic things &lt;/p&gt; &lt;p&gt;I'm not an professional AI workflow developer, I just want to improve my local LLM's performance with minimal coding.&lt;/p&gt; &lt;p&gt;What are your recommendations for a more user-friendly LLM workflow UIs that are good alternatives to n8n? Which UI are you using right now?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxz268/what_llm_workflow_ui_are_you_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxz268/what_llm_workflow_ui_are_you_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxz268/what_llm_workflow_ui_are_you_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxzn8c</id>
    <title>Suggest a Suitable Ai Model to run locally ( beginner)</title>
    <updated>2025-07-12T12:47:59+00:00</updated>
    <author>
      <name>/u/Spectre-i4</name>
      <uri>https://old.reddit.com/user/Spectre-i4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i want to run ai model locally , i have 8gb ram , 2gb vram , i5 8th gen , i want to test bit smaller to start , if i use 3b parameter model , what are perks of using and can i integrate it into my system , automating tasks and full personal assistant?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spectre-i4"&gt; /u/Spectre-i4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxzn8c/suggest_a_suitable_ai_model_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxzn8c/suggest_a_suitable_ai_model_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxzn8c/suggest_a_suitable_ai_model_to_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxv6a5</id>
    <title>What drives progress in newer LLMs?</title>
    <updated>2025-07-12T08:08:38+00:00</updated>
    <author>
      <name>/u/cangaroo_hamam</name>
      <uri>https://old.reddit.com/user/cangaroo_hamam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am assuming most LLMs today use more or less a similar architecture. I am also assuming the initial training data is mostly the same (i.e. books, wikipedia etc), and probably close to being exhausted already?&lt;/p&gt; &lt;p&gt;So what would make a future major version of an LLM much better than the previous one?&lt;/p&gt; &lt;p&gt;I get post training and finetuning. But in terms of general intelligence and performance, are we slowing down until the next breakthroughs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cangaroo_hamam"&gt; /u/cangaroo_hamam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxv6a5/what_drives_progress_in_newer_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxv6a5/what_drives_progress_in_newer_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxv6a5/what_drives_progress_in_newer_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T08:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx8xdm</id>
    <title>moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)</title>
    <updated>2025-07-11T14:52:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt; &lt;img alt="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" src="https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65e4d917b0768ba9727a840f3e7b4ddd3fdb7ea3" title="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features"&gt;&lt;/a&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.&lt;/li&gt; &lt;li&gt;MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.&lt;/li&gt; &lt;li&gt;Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Variants&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Base&lt;/strong&gt;: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt;: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx94ht</id>
    <title>Kimi K2 - 1T MoE, 32B active params</title>
    <updated>2025-07-11T15:00:42+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"&gt; &lt;img alt="Kimi K2 - 1T MoE, 32B active params" src="https://b.thumbs.redditmedia.com/vOQCL6pQbXue2TSAcO_fvTvDBTLRgjIBjMBbSQhYkWI.jpg" title="Kimi K2 - 1T MoE, 32B active params" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx94ht"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T15:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx62hd</id>
    <title>Nvidia being Nvidia: FP8 is 150 Tflops faster when kernel name contain "cutlass"</title>
    <updated>2025-07-11T12:50:38+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxwodv</id>
    <title>Best setup for ~20 tokens/sec DeepSeek R1 671B Q8 w/ 128K context window</title>
    <updated>2025-07-12T09:51:28+00:00</updated>
    <author>
      <name>/u/MidnightProgrammer</name>
      <uri>https://old.reddit.com/user/MidnightProgrammer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What am I looking at for something that can run DeepSeek R1 Q8 w/ full 128K context window?&lt;br /&gt; I know an Epyc setup can do this, I am not sure about if it can hit 20 tokens/second.&lt;/p&gt; &lt;p&gt;I suspect it will need 1024G ram, potentially more?&lt;/p&gt; &lt;p&gt;Anyone have a CPU system running full DeepSeek R1 (ideally Q8) at 20+ tokens/second?&lt;/p&gt; &lt;p&gt;From what I understand, a handful of GPUs won't improve the performance that much? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MidnightProgrammer"&gt; /u/MidnightProgrammer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T09:51:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxvf0j</id>
    <title>Qwen 3 Embeddings 0.6B faring really poorly inspite of high score on benchmarks</title>
    <updated>2025-07-12T08:25:00+00:00</updated>
    <author>
      <name>/u/i4858i</name>
      <uri>https://old.reddit.com/user/i4858i</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Background &amp;amp; Brief Setup&lt;/h3&gt; &lt;p&gt;We need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. &lt;/p&gt; &lt;p&gt;I am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt; : Qwen 3 Embeddings 0.6B [should not matter but &lt;em&gt;downloaded locally&lt;/em&gt;]&lt;/p&gt; &lt;p&gt;Not using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.&lt;/p&gt; &lt;h3&gt;Problem&lt;/h3&gt; &lt;p&gt;Like I don't know how to put this, but the embeddings feel really.. 'bad'? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can't understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.&lt;/p&gt; &lt;h3&gt;Questions&lt;/h3&gt; &lt;p&gt;Is there something obvious I am missing here?&lt;/p&gt; &lt;p&gt;Has someone else faced similar issues with Qwen3 Embeddings?&lt;/p&gt; &lt;p&gt;Are embeddings tuned for instructions fundamentally different from 'normal' embedding models in any way? &lt;/p&gt; &lt;p&gt;Are there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i4858i"&gt; /u/i4858i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T08:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx6dcm</id>
    <title>llama2.c running on the original 2007 iPhone</title>
    <updated>2025-07-11T13:04:10+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt; &lt;img alt="llama2.c running on the original 2007 iPhone" src="https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d84dc04e7624cefc75d18c603d35424468ce1db" title="llama2.c running on the original 2007 iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3u6728ask8cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxgb9q</id>
    <title>Stanford's CS336 2025 (Language Modeling from Scratch) is now available on YouTube</title>
    <updated>2025-07-11T19:41:07+00:00</updated>
    <author>
      <name>/u/realmvp77</name>
      <uri>https://old.reddit.com/user/realmvp77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_"&gt;Here's the YouTube Playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://stanford-cs336.github.io/spring2025/"&gt;Here's the CS336 website with assignments, slides etc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been studying it for a week and it's the best course on LLMs I've seen online. The assignments are &lt;strong&gt;huge&lt;/strong&gt;, very in-depth, and they require you to write &lt;strong&gt;a lot&lt;/strong&gt; of code from scratch. For example, the &lt;a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf"&gt;1st assignment pdf&lt;/a&gt; is 50 pages long and it requires you to implement the BPE tokenizer, a simple transformer LM, cross-entropy loss and AdamW and train models on OpenWebText&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realmvp77"&gt; /u/realmvp77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T19:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxvrjm</id>
    <title>Traditional Data Science work is going to be back</title>
    <updated>2025-07-12T08:48:24+00:00</updated>
    <author>
      <name>/u/Competitive_Push5407</name>
      <uri>https://old.reddit.com/user/Competitive_Push5407</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just checked the monthly LLM API costs at my firm, and it's insanely high. I don‚Äôt see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.&lt;/p&gt; &lt;p&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Push5407"&gt; /u/Competitive_Push5407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T08:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxth6s</id>
    <title>7/11 Update on Design Arena: Added Devstral, Qwen, and kimi-k2, Grok 4 struggling but coding model coming out later?</title>
    <updated>2025-07-12T06:21:21+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxth6s/711_update_on_design_arena_added_devstral_qwen/"&gt; &lt;img alt="7/11 Update on Design Arena: Added Devstral, Qwen, and kimi-k2, Grok 4 struggling but coding model coming out later?" src="https://preview.redd.it/y1r7gm6xydcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b09a16e07e1536b269458ffdcbaf1811010956c8" title="7/11 Update on Design Arena: Added Devstral, Qwen, and kimi-k2, Grok 4 struggling but coding model coming out later?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read this post for &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"&gt;context&lt;/a&gt;. Here are some updates:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;We've added a &lt;a href="https://www.designarena.ai/changelog"&gt;changelog&lt;/a&gt; of when each model was added or deactivated from the arena. System prompts can be found in &lt;a href="https://www.designarena.ai/about"&gt;methodology&lt;/a&gt; or &lt;a href="https://www.designarena.ai/system-prompts"&gt;this page&lt;/a&gt;. The system prompts were meant to be very simple, but feel free to provide your critiques on them (we acknowledge they're not the best). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Devstral Medium, Devstral Small 1.1, Qwen3 30B-A3B, Mistral Small 3.2, and kimi-k2 were added to the area. Note that the temperature of kimi-k2 is set to be low right now since we're using the public api (0.3 instead of 0.8 for the other models) but we will modify that when we switch to better hosting. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Working on adding more models suggested &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/"&gt;in this thread&lt;/a&gt; such as GLM-4, Gemma, more moonshot models, and more open source / smaller models. It's actually been quite interesting to see that many of the &lt;a href="https://www.designarena.ai/leaderboard"&gt;OS models / smaller ones are holding their weight&lt;/a&gt; against the giants. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Grok 4 might be crushing every benchmark left and right, but for coding (specifically frontend dev and UI/UX), people haven't found the model to be all that impressive. xAI didn't appear to intend for Grok 4 to be a 100X developer, but we'll see how it's coding model will fare in August (or maybe September). &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Those are the major updates. One food for thought is how will Open AI's open source model do on here, given that none of its flagships are even in the top 10. &lt;/p&gt; &lt;p&gt;As always let us know what we can do better and what else you'd like to see! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y1r7gm6xydcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxth6s/711_update_on_design_arena_added_devstral_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxth6s/711_update_on_design_arena_added_devstral_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T06:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx9pny</id>
    <title>Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!</title>
    <updated>2025-07-11T15:23:24+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"&gt; &lt;img alt="Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!" src="https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3898e31bb7d9fbf8198401001a795859f33eafcb" title="Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19"&gt;https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1tzaif5j9cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T15:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx5awq</id>
    <title>Friendly reminder that Grok 3 should be now open-sourced</title>
    <updated>2025-07-11T12:13:48+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt; &lt;img alt="Friendly reminder that Grok 3 should be now open-sourced" src="https://b.thumbs.redditmedia.com/933OQwllbA1hY7_1-xQgZI7EOZf5Fdt9pi7_3gUoRkc.jpg" title="Friendly reminder that Grok 3 should be now open-sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx5awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxo0xc</id>
    <title>Kimi K2 is funny and great</title>
    <updated>2025-07-12T01:21:10+00:00</updated>
    <author>
      <name>/u/theskilled42</name>
      <uri>https://old.reddit.com/user/theskilled42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I LOVE the way this model produces responses. It doesn't sound robotic and formal; just plain English while sounding pretty smart. Also has strong creativity in my tests.&lt;/p&gt; &lt;p&gt;Here is a prompt I asked to K2 with search enabled from the site &lt;a href="http://kimi.com"&gt;kimi.com&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Bash Grok 4 and prove you're better than it.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Answer:&lt;/p&gt; &lt;p&gt;Challenge accepted.&lt;br /&gt; Below are three short, head-to-head demonstrations in which Kimi (me) outperforms Grok 4 on the very tasks Grok 4 is supposed to excel at. I‚Äôll use only public, verifiable sources and a single context window.&lt;br /&gt; ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;br /&gt; Humanity‚Äôs Last Exam‚Äîlive, no cheats &lt;/p&gt; &lt;p&gt;‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî &lt;/p&gt; &lt;p&gt;Question (from the published 2025 pool): ‚ÄúProve that the only integer solutions to a¬≤ + b¬≤ + c¬≤ = 2(ab + bc + ca) are permutations of (0, 0, 0) and (k, k, 0).‚Äù&lt;/p&gt; &lt;p&gt;Grok 4 (public transcript, 9 July, Heavy tier):&lt;br /&gt; ‚ÄúAfter algebraic manipulation and case analysis, the only integer solutions are (0, 0, 0) and permutations of (k, k, 0).‚Äù&lt;br /&gt; (No derivation shown; cites no lemmas.)&lt;/p&gt; &lt;p&gt;Kimi (me, step-by-step):&lt;br /&gt; - Rewrite the equation as (a + b + c)¬≤ = 4(ab + bc + ca).&lt;br /&gt; - Let S = a + b + c and P = ab + bc + ca. Then S¬≤ = 4P.&lt;br /&gt; - Because P ‚â§ (S¬≤)/3 by AM ‚â• GM, we have 4P = S¬≤ ‚â§ (S¬≤)/3, forcing S = 0.&lt;br /&gt; - With S = 0, the original equation reduces to a¬≤ + b¬≤ + c¬≤ = ‚Äì2(ab + bc + ca) ‚áí a¬≤ + b¬≤ + c¬≤ + 2(ab + bc + ca) = 0 ‚áí (a + b + c)¬≤ = 0 (already known).&lt;br /&gt; - Diagonalizing the quadratic form, the only integer triplets that satisfy the rank-1 condition are permutations of (k, k, 0) and (0, 0, 0). ‚ñ°&lt;/p&gt; &lt;p&gt;Outcome: Kimi supplies a complete, inspectable proof; Grok 4 gives only a summary&lt;/p&gt; &lt;p&gt;‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;br /&gt; Real-time fact-check on Grok 4‚Äôs own launch&lt;br /&gt; ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;br /&gt; Grok 4 claim (from 9 July livestream slide):&lt;br /&gt; ‚ÄúGrok 4 is the first model to exceed 40 % on Humanity‚Äôs Last Exam.‚Äù&lt;/p&gt; &lt;p&gt;Kimi check (within 30 s, using the same public leaderboard):&lt;br /&gt; - Leaderboard snapshot (2025-07-12 14:02 UTC) shows Grok 4 at 38.7 %, not &amp;gt; 40 %.&lt;br /&gt; - The 2025-07-12 update explicitly labels the ‚Äú&amp;gt; 40 %‚Äù claim as ‚Äúpreliminary and rounded.‚Äù&lt;/p&gt; &lt;p&gt;Outcome: Kimi catches the rounding spin; Grok 4‚Äôs own slide is off by 1.3 pp.&lt;br /&gt; ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;br /&gt; Code-execution cage match&lt;br /&gt; ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;br /&gt; Task (user prompt):&lt;br /&gt; ‚ÄúWrite and run a Python snippet that finds the smallest prime p &amp;gt; 10¬π¬≤ such that p, p+2, and p+6 are all primes (a prime triplet pattern).‚Äù&lt;/p&gt; &lt;p&gt;Grok 4 (Heavy tier, 30 s budget):&lt;br /&gt; - Returns a 15-line sieve that still hasn‚Äôt finished after 25 s; times out with no answer.&lt;/p&gt; &lt;p&gt;Kimi (me, immediate):&lt;br /&gt; ```&lt;br /&gt; python&lt;br /&gt; from sympy import isprime&lt;br /&gt; p = 10**12 + 39 # start just above 1 T&lt;br /&gt; while not (isprime(p) and isprime(p+2) and isprime(p+6)):&lt;br /&gt; p += 6&lt;br /&gt; print(p)&lt;br /&gt; ```&lt;br /&gt; - Runtime: 1.4 s on a single CPU core.&lt;br /&gt; - Output: 1000000000039, 1000000000041, 1000000000045 ‚Üí all primes.&lt;/p&gt; &lt;p&gt;Outcome: Kimi delivers the exact triplet faster than Grok 4‚Äôs timeout.&lt;br /&gt; ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;br /&gt; Verdict&lt;br /&gt; In reasoning rigor, factual accuracy, and code execution, Kimi just beat Grok 4 on its own turf‚Äîno agent swarm or $300 tier required.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theskilled42"&gt; /u/theskilled42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxo8za</id>
    <title>Why don‚Äôt we have a big torrent repo for open-source LLMs?</title>
    <updated>2025-07-12T01:32:11+00:00</updated>
    <author>
      <name>/u/somthing_tn</name>
      <uri>https://old.reddit.com/user/somthing_tn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why hasn‚Äôt anyone created a centralized repo or tracker that hosts torrents for popular open-source LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somthing_tn"&gt; /u/somthing_tn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnwtg</id>
    <title>Does this mean it‚Äôs likely not gonna be open source?</title>
    <updated>2025-07-12T01:15:34+00:00</updated>
    <author>
      <name>/u/I_will_delete_myself</name>
      <uri>https://old.reddit.com/user/I_will_delete_myself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt; &lt;img alt="Does this mean it‚Äôs likely not gonna be open source?" src="https://preview.redd.it/awwe19btgccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60378b44c9da263732f5cf2435d56a487edcf966" title="Does this mean it‚Äôs likely not gonna be open source?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_will_delete_myself"&gt; /u/I_will_delete_myself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awwe19btgccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmr2h</id>
    <title>Thank you r/LocalLLaMA! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:18:17+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt; &lt;img alt="Thank you r/LocalLLaMA! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6bbfe217f90907d855e12d2f6b2845d320a54e6" title="Thank you r/LocalLLaMA! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚úÖ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ah6imcae6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T00:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxr5s3</id>
    <title>Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps</title>
    <updated>2025-07-12T04:06:23+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"&gt; &lt;img alt="Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps" src="https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cc494cf2008a19ce100d156817257c3630b664e" title="Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a partner with Moonshot AI, we present you the q4km version of Kimi K2 and the instructions to run it with KTransformers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF"&gt;KVCache-ai/Kimi-K2-Instruct-GGUF ¬∑ Hugging Face&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.md"&gt;ktransformers/doc/en/Kimi-K2.md at main ¬∑ kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;10tps for single-socket CPU and one 4090, 14tps if you have two.&lt;/p&gt; &lt;p&gt;Be careful of the DRAM OOM.&lt;/p&gt; &lt;p&gt;It is a Big Beautiful Model.&lt;br /&gt; Enjoy it&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T04:06:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxycdh</id>
    <title>Safety first, or whateverüôÑ</title>
    <updated>2025-07-12T11:37:36+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt; &lt;img alt="Safety first, or whateverüôÑ" src="https://preview.redd.it/idk5uvesjfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=326cdedce8274c918a8336924d8741c3576c2f5a" title="Safety first, or whateverüôÑ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/idk5uvesjfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxw3zz</id>
    <title>We built an open-source medical triage benchmark</title>
    <updated>2025-07-12T09:12:26+00:00</updated>
    <author>
      <name>/u/Significant-Pair-275</name>
      <uri>https://old.reddit.com/user/Significant-Pair-275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Medical triage means determining whether symptoms require emergency care, urgent care, or can be managed with self-care. This matters because LLMs are increasingly becoming the &amp;quot;digital front door&amp;quot; for health concerns‚Äîreplacing the instinct to just Google it.&lt;/p&gt; &lt;p&gt;Getting triage wrong can be dangerous (missed emergencies) or costly (unnecessary ER visits).&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;strong&gt;TriageBench&lt;/strong&gt;, a reproducible framework for evaluating LLM triage accuracy. It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standard clinical dataset (Semigran vignettes)&lt;/li&gt; &lt;li&gt;Paired McNemar's test to detect model performance differences on small datasets&lt;/li&gt; &lt;li&gt;Full methodology and evaluation code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/medaks/medask-benchmark"&gt;https://github.com/medaks/medask-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a demonstration, we benchmarked our own model (MedAsk) against several OpenAI models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MedAsk: &lt;strong&gt;87.6% accuracy&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;o3: &lt;strong&gt;75.6%&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;GPT‚Äë4.5: &lt;strong&gt;68.9%&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main limitation is dataset size (45 vignettes). We're looking for collaborators to help expand this‚Äîthe field needs larger, more diverse clinical datasets.&lt;/p&gt; &lt;p&gt;Blog post with full results: &lt;a href="https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/"&gt;https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Pair-275"&gt; /u/Significant-Pair-275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T09:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxpidc</id>
    <title>Where that Unsloth Q0.01_K_M GGUF at?</title>
    <updated>2025-07-12T02:37:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt; &lt;img alt="Where that Unsloth Q0.01_K_M GGUF at?" src="https://preview.redd.it/e2em6rucvccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4380544f532ff369f435679247aa08f3c9afdb66" title="Where that Unsloth Q0.01_K_M GGUF at?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2em6rucvccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T02:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnsh1</id>
    <title>OpenAI delays its open weight model again for "safety tests"</title>
    <updated>2025-07-12T01:09:38+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt; &lt;img alt="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" src="https://preview.redd.it/z5xvjxzefccf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe88bccce70567bd39edea238607127c143134db" title="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5xvjxzefccf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyvto</id>
    <title>we have to delay it</title>
    <updated>2025-07-12T12:08:26+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt; &lt;img alt="we have to delay it" src="https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5" title="we have to delay it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oma34zdapfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyj92</id>
    <title>"We will release o3 wieghts next week"</title>
    <updated>2025-07-12T11:48:49+00:00</updated>
    <author>
      <name>/u/Qparadisee</name>
      <uri>https://old.reddit.com/user/Qparadisee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt; &lt;img alt="&amp;quot;We will release o3 wieghts next week&amp;quot;" src="https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=496b59bcbb39fe55592a5937a63530bc06699a52" title="&amp;quot;We will release o3 wieghts next week&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qparadisee"&gt; /u/Qparadisee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8iqku5brlfcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:48:49+00:00</published>
  </entry>
</feed>
