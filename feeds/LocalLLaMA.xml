<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-04T12:22:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qvkc1n</id>
    <title>Qwen 3 Coder Next tool calling bugs on mxfp4 and official gguf Q4</title>
    <updated>2026-02-04T09:44:53+00:00</updated>
    <author>
      <name>/u/ScoreUnique</name>
      <uri>https://old.reddit.com/user/ScoreUnique</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvkc1n/qwen_3_coder_next_tool_calling_bugs_on_mxfp4_and/"&gt; &lt;img alt="Qwen 3 Coder Next tool calling bugs on mxfp4 and official gguf Q4" src="https://b.thumbs.redditmedia.com/uu2Gp1FoTyOpqJWiTZ-2xSmPZ_DAJM_pD1kB0L6iMSc.jpg" title="Qwen 3 Coder Next tool calling bugs on mxfp4 and official gguf Q4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sa7yciw68ghg1.png?width=1518&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aea588f1c21716125e657df0eba54f3cbde0c060"&gt;https://preview.redd.it/sa7yciw68ghg1.png?width=1518&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aea588f1c21716125e657df0eba54f3cbde0c060&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone having a well working gguf with correct template etc.? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScoreUnique"&gt; /u/ScoreUnique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvkc1n/qwen_3_coder_next_tool_calling_bugs_on_mxfp4_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvkc1n/qwen_3_coder_next_tool_calling_bugs_on_mxfp4_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvkc1n/qwen_3_coder_next_tool_calling_bugs_on_mxfp4_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T09:44:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv0p7u</id>
    <title>MiniCPM-o-4_5 : Full duplex, multimodal with vision and speech at ONLY 9B PARAMETERS??</title>
    <updated>2026-02-03T18:55:33+00:00</updated>
    <author>
      <name>/u/Uncle___Marty</name>
      <uri>https://old.reddit.com/user/Uncle___Marty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-o-4_5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Couldnt find an existing post for this and was surprised, so heres a post about this. Or something. This seems pretty amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uncle___Marty"&gt; /u/Uncle___Marty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0p7u/minicpmo4_5_full_duplex_multimodal_with_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0p7u/minicpmo4_5_full_duplex_multimodal_with_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0p7u/minicpmo4_5_full_duplex_multimodal_with_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T18:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv7lo6</id>
    <title>Insights from Kimi k2.5 Report</title>
    <updated>2026-02-03T23:13:55+00:00</updated>
    <author>
      <name>/u/Cold_Discussion_9570</name>
      <uri>https://old.reddit.com/user/Cold_Discussion_9570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I have been reading the kimi k2.5 report, &lt;a href="https://arxiv.org/pdf/2602.02276"&gt;https://arxiv.org/pdf/2602.02276&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Its really packed with lots of details on training frontier models. I wanted to share some of the insights I got from it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multimodal Pretraining&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;An open question for me has been if training on text + vision is better or worse than text training alone. DeepSeek so far seems to have settled on text only, they did play with DeepSeek VL but havent released a new one since. In Kimi, they showed the vision + text (10% vision, 90% text) actually improves the performance of both modalities, this is really cool.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zero Vision SFT&lt;/strong&gt;&lt;br /&gt; Unlike in pretraining, for SFT, they did only text training, and any vision task is handled via tools. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multimodal RL&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unlike the SFT, the RL is multimodal, and they designed lots of tasks that explicitly require reasoning over visual content to force the model to improve on vision.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Swarm RL&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is the key highlight for me, they really trained this to be a multi agent orchestrator. During the RL training, the model is given tools to spin up and manage sub agents. The sub agents themselves have fixed weights, their trajectories are not included in training, so effectively on the orchestrators actions are trained, while rewards are obtained from the result of the work of the sub-agents, effectively treating the subagents as parts of the environment. &lt;/p&gt; &lt;p&gt;The data for the RL training is constructed to include tasks that are best executed in parallel rather than explicitly prompting the model to do tasks in parallel.&lt;/p&gt; &lt;p&gt;You can read more on the technical report. &lt;a href="https://arxiv.org/abs/2602.02276"&gt;https://arxiv.org/abs/2602.02276&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cold_Discussion_9570"&gt; /u/Cold_Discussion_9570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv7lo6/insights_from_kimi_k25_report/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv7lo6/insights_from_kimi_k25_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv7lo6/insights_from_kimi_k25_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T23:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv9hy5</id>
    <title>MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers</title>
    <updated>2026-02-04T00:31:38+00:00</updated>
    <author>
      <name>/u/Late-Bank7790</name>
      <uri>https://old.reddit.com/user/Late-Bank7790</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv9hy5/memoryllm_plugnplay_interpretable_feedforward/"&gt; &lt;img alt="MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers" src="https://preview.redd.it/3dgsib3lhdhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f9c6360165295088a8bf33d815c1a7c65c25a5" title="MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper Link: &lt;a href="https://www.arxiv.org/abs/2602.00398"&gt;https://www.arxiv.org/abs/2602.00398&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Question:&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;What if FFNs were actually human-interpretable, token-indexed memory?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;This work investigate the role of FFNs through a novel lens of token-indexed neural retrieval memory and present a &lt;em&gt;TKV (token-key-value) framework&lt;/em&gt; to investigate how FFNs construct a persistent context-free memory over the model’s vocabulary.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It explores the spatial perspective of token-indexed memory and found that lexically and semantically similar query tokens tend to access similar memory location within FFNs for retrieval.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;FFNs in MemoryLLM play a dominant role in retrieval-based tasks in comparison to inferential or logical thinking tasks.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;With static token embedding-based training directly from embedding layer, FFN modules in MemoryLLM can be pre-computed and offloaded to storage devices.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It introduces &lt;em&gt;Flex-MemoryLLM&lt;/em&gt;, positioning it between a conventional transformer design and MemoryLLM to bridge the performance gap caused by training FFNs with context-free token-wise embeddings.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Late-Bank7790"&gt; /u/Late-Bank7790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dgsib3lhdhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv9hy5/memoryllm_plugnplay_interpretable_feedforward/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv9hy5/memoryllm_plugnplay_interpretable_feedforward/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T00:31:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvgu2j</id>
    <title>MCP + Ghidra for AI-powered binary analysis — 110 tools, cross-version function matching via normalized hashing</title>
    <updated>2026-02-04T06:13:28+00:00</updated>
    <author>
      <name>/u/XerzesX</name>
      <uri>https://old.reddit.com/user/XerzesX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built an MCP server that gives LLMs deep access to Ghidra's reverse engineering engine. 110 tools covering decompilation, disassembly, annotation, cross-referencing, and automated analysis.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The interesting ML angle: normalized function hashing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm using a technique to create a registry of 154K+ function signatures. The hash captures the logical structure of compiled code (mnemonics + operand categories + control flow) while ignoring address rebase. This enables:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Cross-version documentation transfer&lt;/strong&gt; — annotate once, apply everywhere&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Known-function detection&lt;/strong&gt; in new binaries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Building function similarity datasets&lt;/strong&gt; for training&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It's a simpler alternative to full ML-based binary similarity (like Ghidra's BSim or neural approaches) that works surprisingly well for versioned software.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works with LLMs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The MCP protocol means any LLM client can drive the analysis — Claude Desktop, Claude Code, local models via any MCP-compatible client, or custom pipelines.&lt;/p&gt; &lt;p&gt;The batch operation system reduces API overhead by 93%, which matters a lot when you're running analysis loops that would otherwise make dozens of individual calls per function.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker support&lt;/strong&gt; enables headless batch analysis — feed binaries through analysis pipelines without the GUI.&lt;/p&gt; &lt;p&gt;Validated against Diablo II across 20+ game patches. The normalized hashing correctly matched 1,300+ functions across versions where all addresses had shifted.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; - GitHub: &lt;a href="https://github.com/bethington/ghidra-mcp"&gt;https://github.com/bethington/ghidra-mcp&lt;/a&gt; - Release: &lt;a href="https://github.com/bethington/ghidra-mcp/releases/tag/v2.0.0"&gt;https://github.com/bethington/ghidra-mcp/releases/tag/v2.0.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The hashing approach is deliberately simple — SHA-256 of normalized instruction sequences. No embeddings, no neural networks. I'm curious if anyone has combined similar structural hashing with learned representations for binary similarity. Would love to hear thoughts on the approach.&lt;/p&gt; &lt;p&gt;Also pairs with &lt;a href="https://github.com/bethington/cheat-engine-server-python"&gt;cheat-engine-server-python&lt;/a&gt; for dynamic analysis and &lt;a href="https://github.com/bethington/re-universe"&gt;re-universe&lt;/a&gt; for BSim-powered binary similarity at scale.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XerzesX"&gt; /u/XerzesX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgu2j/mcp_ghidra_for_aipowered_binary_analysis_110/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgu2j/mcp_ghidra_for_aipowered_binary_analysis_110/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgu2j/mcp_ghidra_for_aipowered_binary_analysis_110/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T06:13:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvg844</id>
    <title>RAG accuracy plateau - anyone else stuck around 70-75%?</title>
    <updated>2026-02-04T05:40:36+00:00</updated>
    <author>
      <name>/u/GlitteringWay7289</name>
      <uri>https://old.reddit.com/user/GlitteringWay7289</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been iterating on a RAG setup for internal docs for about 3 months now. Tried different chunking sizes, overlap strategies, switched from ada-002 to text-embedding-3-large. Still hovering around 70-75% on our eval set.&lt;/p&gt; &lt;p&gt;Starting to think vector similarity alone just has a ceiling. The retrieved chunks are &amp;quot;related&amp;quot; but not always what actually answers the question.&lt;/p&gt; &lt;p&gt;Anyone break through this? What actually moved the needle for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlitteringWay7289"&gt; /u/GlitteringWay7289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg844/rag_accuracy_plateau_anyone_else_stuck_around_7075/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg844/rag_accuracy_plateau_anyone_else_stuck_around_7075/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg844/rag_accuracy_plateau_anyone_else_stuck_around_7075/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvm0ft</id>
    <title>Mixture-of-Models routing beats single LLMs on SWE-Bench via task specialization</title>
    <updated>2026-02-04T11:24:04+00:00</updated>
    <author>
      <name>/u/botirkhaltaev</name>
      <uri>https://old.reddit.com/user/botirkhaltaev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been looking at per-task results on SWE-Bench Verified and noticed something that leaderboard averages hide: different models consistently solve &lt;em&gt;different&lt;/em&gt; subsets of tasks.&lt;/p&gt; &lt;p&gt;Even the top overall model on the leaderboard fails a non-trivial number of tasks that other models reliably solve, and the reverse is also true. This suggests strong task-level specialization rather than one model being strictly better.&lt;/p&gt; &lt;p&gt;To test this, I built a &lt;strong&gt;Mixture-of-Models architecture&lt;/strong&gt;, which is different from traditional routing that just defaults to the strongest aggregate model most of the time. The goal isn’t to route to a single model as often as possible, but to exploit complementary strengths between models.&lt;/p&gt; &lt;p&gt;Concretely:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The problem description is embedded&lt;/li&gt; &lt;li&gt;It’s assigned to a semantic cluster (learned from general coding data, not SWE-Bench)&lt;/li&gt; &lt;li&gt;Each cluster has learned per-model success statistics&lt;/li&gt; &lt;li&gt;The task is routed to the historically strongest model for that &lt;em&gt;type&lt;/em&gt; of problem&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Importantly, this does &lt;strong&gt;not&lt;/strong&gt; route the top aggregate model for the majority of tasks. Several clusters consistently route to other models where they outperform it, even though it has the highest overall score.&lt;/p&gt; &lt;p&gt;There’s no new foundation model, no test-time search, and no repo execution, just a lightweight gating mechanism over multiple models.&lt;/p&gt; &lt;p&gt;Using this Mixture-of-Models setup, the system reaches 75.6% on SWE-Bench, exceeding single-model baselines (~74%). The takeaway isn’t the absolute number, but the mechanism: leaderboard aggregates hide complementary strengths, and mixture architectures can capture a higher ceiling than any single model.&lt;/p&gt; &lt;p&gt;Blog with details and methodology here: &lt;a href="https://nordlyslabs.com/blog/hypernova"&gt;https://nordlyslabs.com/blog/hypernova&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: the framework is open source ! &lt;a href="https://github.com/Nordlys-Labs/nordlys"&gt;https://github.com/Nordlys-Labs/nordlys&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/botirkhaltaev"&gt; /u/botirkhaltaev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm0ft/mixtureofmodels_routing_beats_single_llms_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm0ft/mixtureofmodels_routing_beats_single_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm0ft/mixtureofmodels_routing_beats_single_llms_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T11:24:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvg14v</id>
    <title>GGML implementation of Qwen3-ASR</title>
    <updated>2026-02-04T05:30:22+00:00</updated>
    <author>
      <name>/u/redditgivingmeshit</name>
      <uri>https://old.reddit.com/user/redditgivingmeshit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg14v/ggml_implementation_of_qwen3asr/"&gt; &lt;img alt="GGML implementation of Qwen3-ASR" src="https://external-preview.redd.it/sbrvrkybCIXzPoOhitnG--AeU0R3tNgehTBWYD1GgTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ea5fcdd7dc654a047c01962678ce8793e450c54" title="GGML implementation of Qwen3-ASR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently been experimenting with agent loops, and I got it to work somewhat reliably with minimal guidance from me.&lt;/p&gt; &lt;p&gt;As I have a side project that needs high ASR accuracy, I thought &lt;strong&gt;implementing Qwen3-ASR-0.6B in pure ggml&lt;/strong&gt; would be the perfect real-world test, and surprisingly, it worked!&lt;/p&gt; &lt;p&gt;Anyways, I hope this will be of help to anyone who wanted to use the Qwen3-ASR-0.6B model with forced alignment on their devices.&lt;/p&gt; &lt;p&gt;It supports Q8 quantization for now, which lowers the ram usage under 2 gigs, even including the forced aligner model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditgivingmeshit"&gt; /u/redditgivingmeshit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/predict-woo/qwen3-asr.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg14v/ggml_implementation_of_qwen3asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg14v/ggml_implementation_of_qwen3asr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvganp</id>
    <title>Step 3.5 Flash is janky af</title>
    <updated>2026-02-04T05:44:24+00:00</updated>
    <author>
      <name>/u/tharsalys</name>
      <uri>https://old.reddit.com/user/tharsalys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using it in Opencode since yesterday. When it works, it's excellent. It's like a much much faster GLM 4.7. But after a few turns, it starts to hallucinate tool calls. &lt;/p&gt; &lt;p&gt;At this point not sure if its a harness issue or a model issue but looking at the reasoning traces which are also full of repetitive lines and jank, it's probably LLM.&lt;/p&gt; &lt;p&gt;Anyone else tried it? Any way to get it working well because I'm really enjoying the speed here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tharsalys"&gt; /u/tharsalys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvganp/step_35_flash_is_janky_af/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvganp/step_35_flash_is_janky_af/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvganp/step_35_flash_is_janky_af/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvlh5n</id>
    <title>Qwen Coders Visual Benchmark</title>
    <updated>2026-02-04T10:53:25+00:00</updated>
    <author>
      <name>/u/loadsamuny</name>
      <uri>https://old.reddit.com/user/loadsamuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to compare the new Qwen Coders so I ran various gguf (IQ1 vs Q3 vs Q4) quants of Qwen Coder Next, along with Coder 30B and VL 32B just to compare vs non coder.&lt;/p&gt; &lt;p&gt;The lightshow test is the one most fail and only the 30B passed it. &lt;/p&gt; &lt;p&gt;All code and prompts are up at&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/electricazimuth/LocalLLM%5C_VisualCodeTest"&gt;https://github.com/electricazimuth/LocalLLM\_VisualCodeTest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/loadsamuny"&gt; /u/loadsamuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://electricazimuth.github.io/LocalLLM_VisualCodeTest/results/2026.02.04/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvlh5n/qwen_coders_visual_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvlh5n/qwen_coders_visual_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T10:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvdcz4</id>
    <title>Why is GPT-OSS extremely restrictive</title>
    <updated>2026-02-04T03:21:56+00:00</updated>
    <author>
      <name>/u/sayamss</name>
      <uri>https://old.reddit.com/user/sayamss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the response it returns when trying to make home automation work: &lt;/p&gt; &lt;p&gt;**Security &amp;amp; Privacy** – The script would need to log into your camera and send data over the local network. Running that from this chat would mean I’d be accessing your private devices, which isn’t allowed. 2. **Policy** – The OpenAI policy says the assistant must not act as a tool that can directly control a user’s device or network.&lt;/p&gt; &lt;p&gt;Why would they censor the model to this extent? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sayamss"&gt; /u/sayamss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvdcz4/why_is_gptoss_extremely_restrictive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvdcz4/why_is_gptoss_extremely_restrictive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvdcz4/why_is_gptoss_extremely_restrictive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T03:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvvtv</id>
    <title>Qwen3-Coder-Next</title>
    <updated>2026-02-03T16:03:56+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt; &lt;img alt="Qwen3-Coder-Next" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next is out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:03:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvhc3o</id>
    <title>Yuan 3.0 Flash 40B - 3.7b parameter multimodal foundation model. Does anyone know these or have tried the model?</title>
    <updated>2026-02-04T06:41:33+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/YuanLabAI/Yuan3.0-Flash-4bit"&gt;https://huggingface.co/YuanLabAI/Yuan3.0-Flash-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://yuanlab.ai"&gt;https://yuanlab.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was looking for optimized models for RAG data retrieval and found this. I've never heard of it. I wonder if the architecture is supported by llama.cpp (it's probably something derived from existing models).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T06:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvm388</id>
    <title>Qwen3-Coder-Next is available on HuggingChat</title>
    <updated>2026-02-04T11:28:26+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"&gt; &lt;img alt="Qwen3-Coder-Next is available on HuggingChat" src="https://external-preview.redd.it/ts3qmqwhhBSKiMfaD-GP4qTCSy4zry7pFJqkPo5wT7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b618deb783becca7cdc00ba44e4ab3a6dfaf36bd" title="Qwen3-Coder-Next is available on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/models/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T11:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1quxtkj</id>
    <title>The open-source version of Suno is finally here: ACE-Step 1.5</title>
    <updated>2026-02-03T17:13:53+00:00</updated>
    <author>
      <name>/u/AppropriateGuava6262</name>
      <uri>https://old.reddit.com/user/AppropriateGuava6262</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt; &lt;img alt="The open-source version of Suno is finally here: ACE-Step 1.5" src="https://b.thumbs.redditmedia.com/Wc9W0Y1pM9FgVsLGL_nSRSARq7eVx7wAjk_hkOIqGfE.jpg" title="The open-source version of Suno is finally here: ACE-Step 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ACE-Step 1.5 is an open-source music model that can generate a full song in about 2 seconds on an A100, runs locally on a typical PC (around 4GB VRAM), and beats Suno on common evaluation scores.&lt;/p&gt; &lt;p&gt;Key traits of ACE-Step 1.5:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quality: beats Suno on common eval scores&lt;/li&gt; &lt;li&gt;Speed: full song under 2s on A100&lt;/li&gt; &lt;li&gt;Local: ~4GB VRAM, under 10s on RTX 3090&lt;/li&gt; &lt;li&gt;LoRA: train your own style with a few songs&lt;/li&gt; &lt;li&gt;License: MIT, free for commercial use&lt;/li&gt; &lt;li&gt;Data: fully authorized plus synthetic&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ace-step/ACE-Step-1.5"&gt;https://github.com/ace-step/ACE-Step-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights/Training code/LoRA code/Paper are all open.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppropriateGuava6262"&gt; /u/AppropriateGuava6262 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1quxtkj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv5d1k</id>
    <title>Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge</title>
    <updated>2026-02-03T21:47:26+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt; &lt;img alt="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" src="https://external-preview.redd.it/3bIaBnDXu08CXhELxk4__N-qsOVuqLC1ZUdzCxFB0Fo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00daa4c0505c069dbac679c0b3ae6151aa6f7543" title="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-Coder tech report is super interesting on a number of items:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They specifically tested on various tool chat templates to make sure the model stays flexible no matter where you use it. From their own data, only DeepSeek-v3.2 is close - even a bit better - (which suggests they do the same) and they're both quite a bit ahead of other models.&lt;/li&gt; &lt;li&gt;As the model gets smarter and smarter, it gets better and better at finding loopholes in the test environment to find the solution by cheating (&lt;a href="https://github.com/SWE-bench/SWE-bench/pull/471"&gt;https://github.com/SWE-bench/SWE-bench/pull/471&lt;/a&gt;), which they have to combat.&lt;/li&gt; &lt;li&gt;They trained several specialized submodels (UI dev, webdev, software engineering, ...) and the final model is a distillation of those.&lt;/li&gt; &lt;li&gt;It's similar in performance to the base (non-Coder) model on general benchmarks, and quite a bit better at math.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T21:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qva5gk</id>
    <title>How to get more tok/s?</title>
    <updated>2026-02-04T00:59:54+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt; &lt;img alt="How to get more tok/s?" src="https://external-preview.redd.it/ZnpvY2wyN3BtZGhnMX3C4bhSrcOBtwpO2ghilluKqvqoK5kABDx37kIjqzIp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fd51fdcf7f7597f9ade8d8e5db8eb03b2cb2d80" title="How to get more tok/s?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not OC! [Source](&lt;a href="https://x.com/climate%5C_ben/status/2000636466117193866?s=61"&gt;https://x.com/climate\_ben/status/2000636466117193866?s=61&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l8lk0xapmdhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T00:59:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv65ed</id>
    <title>Got Qwen-Coder-Next running on ROCm on my Strix Halo!</title>
    <updated>2026-02-03T22:17:18+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt; &lt;img alt="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" src="https://external-preview.redd.it/dzdscnFjbDZ0Y2hnMarG5pOoEfpz9JksRMChe8rZdrijqwmTF4wbigP7RjX-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8e501a14c67b4224883973e411e9b24a9f6bcf8" title="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to see the new model, 80B with 3B active seems perfect for Strix Halo. Video is running on &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm/releases/tag/b1170"&gt;llamacpp-rocm b1170&lt;/a&gt; with context size 16k and &lt;code&gt;--flash-attn on --no-mmap&lt;/code&gt;. Let me know what you want me to try and I'll run it later tonight!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hnso57l6tchg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T22:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvax2n</id>
    <title>Qwen3-Coder-Next-NVFP4 quantization is up, 45GB</title>
    <updated>2026-02-04T01:33:48+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/Qwen3-Coder-Next-NVFP4"&gt;GadflyII/Qwen3-Coder-Next-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All experts were calibrated with ultrachat_200k dataset, 1.63% accuracy loss in MMLU Pro+, 149GB to 45GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T01:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvqs9</id>
    <title>Qwen/Qwen3-Coder-Next · Hugging Face</title>
    <updated>2026-02-03T15:58:52+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-Coder-Next · Hugging Face" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen/Qwen3-Coder-Next · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T15:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1quzwjf</id>
    <title>ACE-Step-1.5 has just been released. It’s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno</title>
    <updated>2026-02-03T18:26:58+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt; &lt;img alt="ACE-Step-1.5 has just been released. It’s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" src="https://external-preview.redd.it/ZDNiNm9lcXduYmhnMXNUFTz1lD2uwrlR8i5n8_uV8Hgq6zjqVqa04fhxxOUs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b143554ca8c67bc465c8e39d15ee68486eaeef36" title="ACE-Step-1.5 has just been released. It’s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://xcancel.com/acemusicAI/status/2018731205546684678"&gt;https://xcancel.com/acemusicAI/status/2018731205546684678&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ace-step.github.io/ace-step-v1.5.github.io/"&gt;https://ace-step.github.io/ace-step-v1.5.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s already supported in Comfy. MIT license. HuggingFace Demo is also available! Pretty much the whole package - LoRAs are supported, multiple different models to tailor to different needs, cover and repainting features. This is the closest open-source has gotten to Suno and similar top-slop platforms. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r7v6v6qwnbhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T18:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvjonm</id>
    <title>First Qwen3-Coder-Next REAP is out</title>
    <updated>2026-02-04T09:04:09+00:00</updated>
    <author>
      <name>/u/Dany0</name>
      <uri>https://old.reddit.com/user/Dany0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt; &lt;img alt="First Qwen3-Coder-Next REAP is out" src="https://external-preview.redd.it/j98XKqoJ3UOGeW66Etg0lVtFqPsaabyeyZuH8PQVb-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=234ec5f7ffcda5d2272c5b48c2652755e36ad2b9" title="First Qwen3-Coder-Next REAP is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;40% REAP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dany0"&gt; /u/Dany0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lovedheart/Qwen3-Coder-Next-REAP-48B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T09:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvgbhs</id>
    <title>Context rot is killing my agent - how are you handling long conversations?</title>
    <updated>2026-02-04T05:45:40+00:00</updated>
    <author>
      <name>/u/i_m_dead_</name>
      <uri>https://old.reddit.com/user/i_m_dead_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a support agent that needs to maintain context across a full customer session (sometimes 20+ turns). Model starts contradicting itself or forgetting key details around turn 15.&lt;/p&gt; &lt;p&gt;Using GPT-4o with a sliding window but that throws away potentially important early context. Tried summarization but it loses nuance.&lt;/p&gt; &lt;p&gt;Anyone found a practical solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_m_dead_"&gt; /u/i_m_dead_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgbhs/context_rot_is_killing_my_agent_how_are_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgbhs/context_rot_is_killing_my_agent_how_are_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgbhs/context_rot_is_killing_my_agent_how_are_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
