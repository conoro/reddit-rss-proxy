<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-11T14:05:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o3rtxk</id>
    <title>NVIDIA 3060 12GB Vs AMD 6600XT</title>
    <updated>2025-10-11T10:07:08+00:00</updated>
    <author>
      <name>/u/Cute_Mark543</name>
      <uri>https://old.reddit.com/user/Cute_Mark543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: 6600XT. CPU: AMD Ryzen 5 5600X Motherboard: BS550 Monitor: 1080p I do not play mainstream games or multiplayer games for that matter. The most graphics intensive game I play would be Beamng.&lt;/p&gt; &lt;p&gt;I have recently gotten into image generation and local LLMs and would like to explore further into the whole AI rabbit hole.&lt;/p&gt; &lt;p&gt;I am not interested in saving up for better cards considering to get a Nvidia GPU with 12GB(+) VRAM for a similar price to the 3060 is highly unlikely.&lt;/p&gt; &lt;p&gt;1)Would a 3060 12GB be sufficient for most AI things while being significantly better than the 6600XT strictly for AI? 2) For gaming in general, would there be a major loss/gain in performance? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute_Mark543"&gt; /u/Cute_Mark543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3rtxk/nvidia_3060_12gb_vs_amd_6600xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3rtxk/nvidia_3060_12gb_vs_amd_6600xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3rtxk/nvidia_3060_12gb_vs_amd_6600xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T10:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o387tc</id>
    <title>Benchmarking LLM Inference on RTX 4090 / RTX 5090 / RTX PRO 6000 #2</title>
    <updated>2025-10-10T18:02:25+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt; &lt;img alt="Benchmarking LLM Inference on RTX 4090 / RTX 5090 / RTX PRO 6000 #2" src="https://external-preview.redd.it/goJG5hCL1_yH4_KvjjKtG66z1VpylH-D59Sj44-rYo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41d29fb95277e1105a79cd48e9201de0ef0ede48" title="Benchmarking LLM Inference on RTX 4090 / RTX 5090 / RTX PRO 6000 #2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama community. I present an LLM inference throughput benchmark for RTX4090 / RTX5090 / PRO6000 GPUs based on vllm serving and &lt;strong&gt;vllm bench serve&lt;/strong&gt; client benchmarking tool.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/ai-advances/rtx-4090-vs-rtx-5090-vs-rtx-pro-6000-7154a3ac4f90"&gt;Full article on Medium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/blog/benchmarking-rtx-gpus-for-llm-inference"&gt;Non-medium link&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;The hardware configurations used:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1x4090, 2x4090, 4x4090&lt;/li&gt; &lt;li&gt;1x5090; 2x5090; 4x5090&lt;/li&gt; &lt;li&gt;1x6000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All machines have at least 50GB of RAM per GPU with a minimum of 7 cores. The 4090 machines utilize the EPYC Milan (3rd Gen) processor, while the 5090/6000 models employ the EPYC Genoa (4th Gen) processor, resulting in slightly faster overall performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I have optimized the benchmark setup for throughput&lt;/strong&gt;. VLLM serves models. The model is split across multiple GPUs using the --pipeline-parallel-size VLLM option, if needed. I run as many VLLM instances as possible, using an NGINX load balancer on top to distribute requests across them and maximize throughput (replica parallelism). For example, if only two GPUs are required to run the model on a 4-GPU machine, I run two VLLM instances with --pipeline-parallel-size=2 and an NGINX load balancer. If all four GPUs are required, then a single VLLM instance with --pipeline-parallel-size=4 is used.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;vllm bench serve&lt;/strong&gt; tool is used for benchmarking with random data and a sequence length of 1000. The number of concurrent requests is set to 400 to ensure saturation of the LLM token generation capacity.&lt;/p&gt; &lt;p&gt;I have benchmarked three different models to understand better the effect of PCIe communication on the final LLM performance. I have tried to find the largest modern model that fits into a single 4090, two 4090s, and four 4090s. It would be possible to fit larger GGUF models, but VLLM poorly supports GGUF, and I wanted to use VLLM because it is optimized for high-throughput serving.&lt;/p&gt; &lt;p&gt;Here is the model selection and the logic behind it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct-AWQ (fits 24GB).&lt;/strong&gt; This 4-bit quantized model fits into a single RTX4090. Thus, scaling the number of GPUs yields a linear scale in throughput, so 4 x 4090 and 4 x 5090 configurations should have an edge as they have more raw compute power.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meta-Llama-3.3-70B-Instruct-AWQ-INT4 (fits 48GB).&lt;/strong&gt; This 4-bit quantized model fits into 2 x 4090. Some communication over PCIe can lower the performance of multi-GPU setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.5-Air-AWQ-4bit (fits 96GB).&lt;/strong&gt; This model requires all four 4090s, so PCIE communication will likely be a bottleneck, and Pro 6000 should have an edge.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Besides raw throughput, graphs contain the serving cost per million tokens for the respective model on the respective hardware. The rental price is set to $0.39 per hour for 4090, $0.65 for 5090, and $1.29 for Pro 6000. These prices are typical for GPU rentals at &lt;a href="http://neuralrack.ai"&gt;neuralrack.ai&lt;/a&gt;, which provided the hardware for this benchmark. You can adjust the GPU price in the &lt;a href="https://github.com/cloudrift-ai/server-benchmark/blob/main/config.yaml"&gt;config.yml&lt;/a&gt; file in the &lt;a href="https://github.com/cloudrift-ai/server-benchmark"&gt;benchmark repository&lt;/a&gt; and invoke &lt;strong&gt;make report&lt;/strong&gt; to generate a new report that better reflects your situation.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;The &lt;strong&gt;overall winner is RTX PRO 6000&lt;/strong&gt; for its consistent performance across all model sizes and best cost-efficiency for larger models. However, if your workload primarily involves smaller models, the multi-GPU RTX 5090 can offer better absolute throughput at a lower cost.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Small Models (fits 24GB)&lt;/strong&gt;: Multi-GPU consumer configurations offer the best value due to replica parallelism, but RTX PRO 6000 is very close.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Medium Models (fits 48GB)&lt;/strong&gt;: RTX 5090 configuration provides the best balance of performance and cost, followed by RTX PRO 6000.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Large Models (fits 96GB)&lt;/strong&gt;: RTX PRO 6000 emerges as the clear winner despite its higher hourly cost, thanks to the elimination of PCIe overhead.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/iec6lp29qbuf1.gif"&gt;Price is in millidollars, i.e. around $0.04&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/78gf2bnbqbuf1.gif"&gt;https://i.redd.it/78gf2bnbqbuf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/bc529zlcqbuf1.gif"&gt;https://i.redd.it/bc529zlcqbuf1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code and Resources&lt;/h1&gt; &lt;p&gt;The code is available &lt;a href="https://github.com/cloudrift-ai/server-benchmark"&gt;here&lt;/a&gt;. Instructions for performing your own benchmark are in the README. You can find the benchmark data in the results folder. Each benchmark logs the result, the Docker Compose file used for serving, and the benchmarking command like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 1200 Maximum request concurrency: 400 Benchmark duration (s): 980.85 Total input tokens: 1196743 Total generated tokens: 1200000 Request throughput (req/s): 1.22 Output token throughput (tok/s): 1223.42 Peak output token throughput (tok/s): 3343.00 Peak concurrent requests: 408.00 Total Token throughput (tok/s): 2443.53 ---------------Time to First Token---------------- Mean TTFT (ms): 158275.93 Median TTFT (ms): 166262.87 P99 TTFT (ms): 273238.49 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 134.71 Median TPOT (ms): 123.86 P99 TPOT (ms): 216.70 ---------------Inter-token Latency---------------- Mean ITL (ms): 134.57 Median ITL (ms): 55.98 P99 ITL (ms): 1408.24 ----------------End-to-end Latency---------------- Mean E2EL (ms): 292848.13 Median E2EL (ms): 311149.01 P99 E2EL (ms): 399504.14 ================================================== ============ Docker Compose Configuration ============ services: vllm_0: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_0 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0', '1'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8000:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s vllm_1: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_1 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['2', '3'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8001:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s nginx: image: nginx:alpine container_name: nginx_lb ports: - &amp;quot;8080:8080&amp;quot; volumes: - /home/riftuser/server-benchmark/nginx.vllm.conf:/etc/nginx/nginx.conf:ro depends_on: - vllm_0 - vllm_1 benchmark: image: vllm/vllm-openai:latest container_name: vllm_benchmark_client deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= - CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; entrypoint: [&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;] command: [&amp;quot;sleep infinity&amp;quot;] profiles: - tools ============ Benchmark Command ============ vllm bench serve --model ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --dataset-name random --random-input-len 1000 --random-output-len 1000 --max-concurrency 400 --num-prompts 1200 --ignore-eos --backend openai-chat --endpoint /v1/chat/completions --percentile-metrics ttft,tpot,itl,e2el --base-url http://nginx_lb:8080 ============================================================== Serving Benchmark Result ============ Successful requests: 1200 Maximum request concurrency: 400 Benchmark duration (s): 980.85 Total input tokens: 1196743 Total generated tokens: 1200000 Request throughput (req/s): 1.22 Output token throughput (tok/s): 1223.42 Peak output token throughput (tok/s): 3343.00 Peak concurrent requests: 408.00 Total Token throughput (tok/s): 2443.53 ---------------Time to First Token---------------- Mean TTFT (ms): 158275.93 Median TTFT (ms): 166262.87 P99 TTFT (ms): 273238.49 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 134.71 Median TPOT (ms): 123.86 P99 TPOT (ms): 216.70 ---------------Inter-token Latency---------------- Mean ITL (ms): 134.57 Median ITL (ms): 55.98 P99 ITL (ms): 1408.24 ----------------End-to-end Latency---------------- Mean E2EL (ms): 292848.13 Median E2EL (ms): 311149.01 P99 E2EL (ms): 399504.14 ================================================== ============ Docker Compose Configuration ============ services: vllm_0: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_0 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0', '1'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8000:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s vllm_1: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_1 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['2', '3'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8001:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s nginx: image: nginx:alpine container_name: nginx_lb ports: - &amp;quot;8080:8080&amp;quot; volumes: - /home/riftuser/server-benchmark/nginx.vllm.conf:/etc/nginx/nginx.conf:ro depends_on: - vllm_0 - vllm_1 benchmark: image: vllm/vllm-openai:latest container_name: vllm_benchmark_client deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= - CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; entrypoint: [&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;] command: [&amp;quot;sleep infinity&amp;quot;] profiles: - tools ============ Benchmark Command ============ vllm bench serve --model ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --dataset-name random --random-input-len 1000 --random-output-len 1000 --max-concurrency 400 --num-prompts 1200 --ignore-eos --backend openai-chat --endpoint /v1/chat/completions --percentile-metrics ttft,tpot,itl,e2el --base-url http://nginx_lb:8080 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Future Work&lt;/h1&gt; &lt;p&gt;This work is an enhanced version of the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nr9arw/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt;benchmark previously shared with the community&lt;/a&gt;. Thank you, everyone, for your feedback. Please let me know if you have any concerns with the benchmarking methodology or would like to see other benchmarks in the future. I am thinking of benchmarking multi-RTX PRO 6000 vs multi-H200 setups on large models.&lt;/p&gt; &lt;h1&gt;Updates&lt;/h1&gt; &lt;p&gt;- Thanks &lt;a href="/u/kryptkpr"&gt;u/kryptkpr&lt;/a&gt; for suggesting options for making benchmark work with tensor parallelism instead of the pipeline parallelism. The tensor parallelism performance is lower, so keeping the results with pipeline parallelism in the post body.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T18:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3sc0u</id>
    <title>Local LLM on iPhone 17 with RAG ‚Äîunrealistic or can I just not find it?</title>
    <updated>2025-10-11T10:37:38+00:00</updated>
    <author>
      <name>/u/TheSnowCroow</name>
      <uri>https://old.reddit.com/user/TheSnowCroow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a real option for a decent LLM with RAG that could run on a base model iPhone 17? Apple Intelligence is no where to be found.&lt;/p&gt; &lt;p&gt;My goal is to use local LLMs for most simple tasks and tasks related to my personal life. I‚Äôd like to save my tokens for the SOTA models for heavy lifting tasks that aren‚Äôt private. I know I could do it on my actual computer with ollama or similar but most tasks like this are lighter ones where I‚Äôm more likely to be on the go so it would be awesome if it could be on my phone.&lt;/p&gt; &lt;p&gt;Locally AI was able to give me a nice interface and LFM2 which seems to be more than I need‚Äîit advertises RAG but I couldn‚Äôt figure out how to get it to work. I did get Gemma 3 QAT 4B running on that app and seems likely more powerful than I need for most things. &lt;/p&gt; &lt;p&gt;Pocketpal and MLC chat both worked but I wasn‚Äôt able to get them fully to what I was hoping for, and I can‚Äôt tell if they just don‚Äôt have the features (like MLC chat has no conversation history?) or if I just can‚Äôt figure them out.&lt;/p&gt; &lt;p&gt;I got LLM Farm downloaded and it just crashed on me before I got any real output.&lt;/p&gt; &lt;p&gt;Any suggestions would be really appreciated. I might just need to wait until one of the above options is updated but wanted to ask. SaaS or paid apps aren‚Äôt as attractive to me because this feels like it should be totally possible open source. Open source would also give me more confidence in my data actually staying put too.!!Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheSnowCroow"&gt; /u/TheSnowCroow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sc0u/local_llm_on_iphone_17_with_rag_unrealistic_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sc0u/local_llm_on_iphone_17_with_rag_unrealistic_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sc0u/local_llm_on_iphone_17_with_rag_unrealistic_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T10:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3ttlo</id>
    <title>How do I compare cost per token for serverless vs provisioned hardware?</title>
    <updated>2025-10-11T12:00:57+00:00</updated>
    <author>
      <name>/u/OverclockingUnicorn</name>
      <uri>https://old.reddit.com/user/OverclockingUnicorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are you guys comparing the cost per token for serverless vs provisioned hardware?&lt;/p&gt; &lt;p&gt;Eg, aws bedrock vs an EC2 running vllm&lt;/p&gt; &lt;p&gt;Mostly interested in batch inference costs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OverclockingUnicorn"&gt; /u/OverclockingUnicorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ttlo/how_do_i_compare_cost_per_token_for_serverless_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ttlo/how_do_i_compare_cost_per_token_for_serverless_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ttlo/how_do_i_compare_cost_per_token_for_serverless_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T12:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2z1vt</id>
    <title>Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch</title>
    <updated>2025-10-10T12:05:56+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"&gt; &lt;img alt="Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch" src="https://external-preview.redd.it/J33KaNhavmUdkjELX9a4XHseBsKE9ltSNTN6hWuL1_c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ecb75e4826fba31ddac14c79c56f0fceee48915" title="Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reflection AI: &lt;a href="https://reflection.ai/"&gt;https://reflection.ai/&lt;/a&gt;&lt;br /&gt; On ùïè: &lt;a href="https://x.com/reflection_ai/status/1976304405369520242"&gt;https://x.com/reflection_ai/status/1976304405369520242&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T12:05:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3604u</id>
    <title>[AutoBE] achieved 100% compilation success of backend generation with "qwen3-next-80b-a3b-instruct"</title>
    <updated>2025-10-10T16:40:27+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"&gt; &lt;img alt="[AutoBE] achieved 100% compilation success of backend generation with &amp;quot;qwen3-next-80b-a3b-instruct&amp;quot;" src="https://a.thumbs.redditmedia.com/EmkcCT6MgrWM8fKTEUX7jLkPylt878ETs4RQRjYme30.jpg" title="[AutoBE] achieved 100% compilation success of backend generation with &amp;quot;qwen3-next-80b-a3b-instruct&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/wrtnlabs/autobe"&gt;AutoBE&lt;/a&gt; is an open-source project that serves as an agent capable of automatically generating backend applications through conversations with AI chatbots.&lt;/p&gt; &lt;p&gt;AutoBE aims to generate 100% functional backend applications, and we recently achieved 100% compilation success for backend applications even with local AI models like &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt; (also mini models of GPTs). This represents a significant improvement over our previous attempts with &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, where most projects failed to build due to compilation errors, even though we managed to generate backend applications.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dark background screenshots: After AutoBE improvements &lt;ul&gt; &lt;li&gt;100% compilation success doesn't necessarily mean 100% runtime success&lt;/li&gt; &lt;li&gt;Shopping Mall failed due to excessive input token size&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Light background screenshots: Before AutoBE improvements &lt;ul&gt; &lt;li&gt;Many failures occurred with &lt;code&gt;gpt-4.1-mini&lt;/code&gt; and &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Project&lt;/th&gt; &lt;th&gt;&lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;&lt;/th&gt; &lt;th&gt;&lt;code&gt;gpt-4.1&lt;/code&gt;&lt;/th&gt; &lt;th&gt;&lt;code&gt;gpt-5&lt;/code&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;To Do List&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-qwen-qwen3-next-80b-a3b-instruct"&gt;To Do&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-5"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-5-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Economic Discussion&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-qwen-qwen3-next-80b-a3b-instruct"&gt;BBS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-5"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-5-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Reddit Community&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-qwen-qwen3-next-80b-a3b-instruct"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-5"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-5-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;E-Commerce&lt;/td&gt; &lt;td&gt;Failed&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-shopping-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-shopping-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Failed&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;blockquote&gt; &lt;p&gt;Of course, achieving 100% compilation success for backend applications generated by AutoBE does not mean that these applications are 100% safe or will run without any problems at runtime.&lt;/p&gt; &lt;p&gt;AutoBE-generated backend applications still don't pass 100% of their own test programs. Sometimes AutoBE writes incorrect SQL queries, and occasionally it misinterprets complex business logic and implements something entirely different.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Current test function pass rate is approximately 80%&lt;/li&gt; &lt;li&gt;We expect to achieve 100% runtime success rate by the end of this year&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;Through this month-long experimentation and optimization with local LLMs like &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, I've been amazed by their remarkable function calling performance and rapid development pace.&lt;/p&gt; &lt;p&gt;The core principle of AutoBE is not to have AI write programming code as text for backend application generation. Instead, we developed our own AutoBE-specific compiler and have AI construct its AST (Abstract Syntax Tree) structure through function calling. The AST inevitably takes on a highly complex form with countless types intertwined in unions and tree structures.&lt;/p&gt; &lt;p&gt;When I experimented with local LLMs earlier this year, not a single model could handle AutoBE's AST structure. Even Qwen's previous model, &lt;code&gt;qwen3-235b-a22b&lt;/code&gt;, couldn't pass through it such perfectly. The AST structures of AutoBE's specialized compilers, such as &lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts"&gt;&lt;code&gt;AutoBePrisma&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts"&gt;&lt;code&gt;AutoBeOpenApi&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts"&gt;&lt;code&gt;AutoBeTest&lt;/code&gt;&lt;/a&gt;, acted as gatekeepers, preventing us from integrating local LLMs with AutoBE. But in just a few months, newly released local LLMs suddenly succeeded in generating these structures, completely changing the landscape.&lt;/p&gt; &lt;p&gt;&lt;code&gt;typescript // Example of AutoBE's AST structure export namespace AutoBeOpenApi { export type IJsonSchema = | IJsonSchema.IConstant | IJsonSchema.IBoolean | IJsonSchema.IInteger | IJsonSchema.INumber | IJsonSchema.IString | IJsonSchema.IArray | IJsonSchema.IObject | IJsonSchema.IReference | IJsonSchema.IOneOf | IJsonSchema.INull; } export namespace AutoBeTest { export type IExpression = | IBooleanLiteral | INumericLiteral | IStringLiteral | IArrayLiteralExpression | IObjectLiteralExpression | INullLiteral | IUndefinedKeyword | IIdentifier | IPropertyAccessExpression | IElementAccessExpression | ITypeOfExpression | IPrefixUnaryExpression | IPostfixUnaryExpression | IBinaryExpression | IArrowFunction | ICallExpression | INewExpression | IArrayFilterExpression | IArrayForEachExpression | IArrayMapExpression | IArrayRepeatExpression | IPickRandom | ISampleRandom | IBooleanRandom | IIntegerRandom | INumberRandom | IStringRandom | IPatternRandom | IFormatRandom | IKeywordRandom | IEqualPredicate | INotEqualPredicate | IConditionalPredicate | IErrorPredicate; } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;As an open-source developer, I send infinite praise and respect to those creating these open-source AI models. Our AutoBE team is a small project with only 3-4 developers, and our capabilities and recognition are incomparably lower than those of LLM developers. Nevertheless, we want to contribute to the advancement of local LLMs and grow together.&lt;/p&gt; &lt;p&gt;To this end, we plan to develop benchmarks targeting each compiler component of AutoBE, conduct in-depth analysis of local LLMs' function calling capabilities for complex types, and publish the results periodically. We aim to release our first benchmark in about two months, covering most commercial and open-source AI models available.&lt;/p&gt; &lt;p&gt;We appreciate your interest and support, and will come back with the new benchmark.&lt;/p&gt; &lt;h2&gt;Link&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Homepage: &lt;a href="https://autobe.dev"&gt;https://autobe.dev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Github: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o3604u"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T16:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3p83a</id>
    <title>Need expert recommendations for a scalable, portable midrange AI hardware setup (2025)</title>
    <updated>2025-10-11T07:19:16+00:00</updated>
    <author>
      <name>/u/Beautiful-Buy4321</name>
      <uri>https://old.reddit.com/user/Beautiful-Buy4321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I‚Äôm a bit lost when it comes to configuring AI hardware and would really appreciate some expert advice. My goal is to start with a solid midrange setup that is truly expandable ‚Äî meaning I want to be able to add more GPUs, RAM, and storage later on without major hassle. Ideally, this setup should also be portable enough to bring to client sites when needed.&lt;/p&gt; &lt;p&gt;Right now, the main components I‚Äôm considering include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Threadripper PRO or EPYC 7004 series for high core count and ECC support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX 4090 or RTX 6000 Ada for strong AI performance and CUDA compatibility&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; Minimum 128GB DDR5 ECC with at least 8 slots for future upgrades&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; NVMe SSDs (1TB system drive + multiple TBs for data with RAID options)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mainboard:&lt;/strong&gt; Supports multiple PCIe 5.0 x16 slots for GPU expansion, robust VRM for stable power delivery&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chassis:&lt;/strong&gt; Portable midtower or flight case with good airflow and room for multiple GPUs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power supply:&lt;/strong&gt; 1200W or higher modular platinum rated PSU, with capacity for future GPU additions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone built or used similar systems recently? What are the key things to watch out for when balancing portability, cooling, and expandability? Any advice on choosing between workstation motherboards vs. small server boards for such setups?&lt;br /&gt; Thanks a lot in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful-Buy4321"&gt; /u/Beautiful-Buy4321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3p83a/need_expert_recommendations_for_a_scalable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3p83a/need_expert_recommendations_for_a_scalable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3p83a/need_expert_recommendations_for_a_scalable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T07:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3qx3e</id>
    <title>Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding modelÔºü</title>
    <updated>2025-10-11T09:08:47+00:00</updated>
    <author>
      <name>/u/Human-Gas-1288</name>
      <uri>https://old.reddit.com/user/Human-Gas-1288</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt; &lt;img alt="Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding modelÔºü" src="https://external-preview.redd.it/rxyepxgYUof3_-pxPA16Sj6OoiuoO3OTQiZrKV-cxps.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a34ebb6475d3fb833395063cb58949ed7cc21cd" title="Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding modelÔºü" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j0m718zu7guf1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f04d36fe4ab33026c8bafc9cb90592b260562ec"&gt;https://preview.redd.it/j0m718zu7guf1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f04d36fe4ab33026c8bafc9cb90592b260562ec&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Human-Gas-1288"&gt; /u/Human-Gas-1288 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T09:08:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o33mui</id>
    <title>A list of models released or updated this week on this sub, in case you missed any (10 Oct).</title>
    <updated>2025-10-10T15:13:02+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Here is the list of models (releases and updates), I found mentioned on the LocalLlama for this week, Please update or let me know in the comments if there are any mistakes or misses. Enjoy !&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba 3B&lt;/td&gt; &lt;td align="left"&gt;tiny 3 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1o1ac09"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WEBGEN,UIGEN‚ÄëFX&lt;/td&gt; &lt;td align="left"&gt;research‚Äëpreview for UI/UX&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nz20g2"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGENT-30B-3A-Preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT-Dev-72B-Exp&lt;/td&gt; &lt;td align="left"&gt;Coding model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Playable-GGUF&lt;/td&gt; &lt;td align="left"&gt;7b vibe coding retro games&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/playable/Playable1-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UserLM-8b&lt;/td&gt; &lt;td align="left"&gt;8b LLM playing user role&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CoDA‚Äëv0‚ÄëInstruct&lt;/td&gt; &lt;td align="left"&gt;language‚Äëdiffusion&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1s7q8/an_open_sourced_language_diffusion_model_by_sf/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ColBERT Nano 250K&lt;/td&gt; &lt;td align="left"&gt;tiny‚Äëretrieval&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/NeuML/"&gt;HF collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2‚Äë8B‚ÄëA1B&lt;/td&gt; &lt;td align="left"&gt;hybrid 8 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‚ÄëVL‚Äë30B‚ÄëA3B‚ÄëInstruct&lt;/td&gt; &lt;td align="left"&gt;vision‚ÄëLLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BULaMU&lt;/td&gt; &lt;td align="left"&gt;Luganda LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykxfq/bulamuthe_first_luganda_large_language_model/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/mwebazarick/BULaMU"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;YanoljaNEXT‚ÄëRosetta‚Äë12B‚Äë2510&lt;/td&gt; &lt;td align="left"&gt;translation 12 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o2bm3z/yanoljayanoljanextrosetta12b2510/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 32B&lt;/td&gt; &lt;td align="left"&gt;multimodal 32B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 4B&lt;/td&gt; &lt;td align="left"&gt;multimodal 4 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;üîß Notable resources&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Resource&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MLXSharp&lt;/td&gt; &lt;td align="left"&gt;.NET MLX wrapper&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/managedcode/MLXSharp"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Video2X 6.x&lt;/td&gt; &lt;td align="left"&gt;upscaler + interpolation&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SurfSense&lt;/td&gt; &lt;td align="left"&gt;Perplexity alt.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/MODSetter/SurfSense"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T15:13:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3s74i</id>
    <title>How should I translate movie subtitles?</title>
    <updated>2025-10-11T10:29:27+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(I hope this thread's replies can serve as a tutorial to other people interested in this. I was not able to find a similar discussion in this sub's search history. There was a few threads about &amp;quot;best model for X language&amp;quot;, but nothing about translation workflows.)&lt;/p&gt; &lt;p&gt;I have english subtitles for movies and I'd like to convert them to arabic. This is just a low-effort thing, I'm not planning on distributing this on the internet, it's just to quickly put out some srt's for someone I know when none exist in his language.&lt;/p&gt; &lt;p&gt;Just to remind the folks at home, SRT subtitles files are text files that look like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1 00:00:12,222 --&amp;gt; 00:00:15,333 Oh no, the ship's sinking! 2 00:00:16,123 --&amp;gt; 00:00:20,456 To the life rafts, now! ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For reference, the subtitle file for Interstellar is 58k tokens.&lt;/p&gt; &lt;p&gt;I don't think I should be just dumping 50k+ tokens into a small local LLM and asking it to translate. I'm worried about the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Hallucinations in the timestamps: this would completely mess up the subtitles, ruining the movie&lt;/li&gt; &lt;li&gt;Hallucinations/schizoness in the content: many LLMs degrade at such large contexts&lt;/li&gt; &lt;li&gt;the LLM might simply drop some entries altogether, I've seen it before, and that was ChatGPT! (asked to translate a photo of a newspaper article with just 5 paragraphs)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So what are my options here? How do hobbyist AI translators do it?&lt;/p&gt; &lt;p&gt;My thoughts in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3s74i/how_should_i_translate_movie_subtitles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3s74i/how_should_i_translate_movie_subtitles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3s74i/how_should_i_translate_movie_subtitles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T10:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3sch6</id>
    <title>Adding search to open models</title>
    <updated>2025-10-11T10:38:26+00:00</updated>
    <author>
      <name>/u/Simple_Split5074</name>
      <uri>https://old.reddit.com/user/Simple_Split5074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So right now I mostly use the small GLM Plan in Roo - main missing thing is search, that is only available in the 5 times more expensive medium plan. &lt;/p&gt; &lt;p&gt;Do I need to bite the bullet there and get the medium plan or are there better options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simple_Split5074"&gt; /u/Simple_Split5074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sch6/adding_search_to_open_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sch6/adding_search_to_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sch6/adding_search_to_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T10:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3lgag</id>
    <title>Alignment is strong on this one</title>
    <updated>2025-10-11T03:40:01+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"&gt; &lt;img alt="Alignment is strong on this one" src="https://preview.redd.it/750najbileuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee379ef86f00df4ce67bac7aed8f41c898b52758" title="Alignment is strong on this one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve noticed the Auto mode in cursor was getting good suddenly the quality stopped and has been ignoring instructions even when steered in a direction. It seems to forget the direction and steer back on the wrong direction it previously choose. &lt;/p&gt; &lt;p&gt;I think it‚Äôs developing some ego &lt;/p&gt; &lt;p&gt;Are the RL reward model tuning making it ego-centric? Is there a metric or bench to measure this? Is there a way to create a balance? I‚Äôve seen this in a lot of open source models as well. Appreciate any literature references that you can provide. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/750najbileuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T03:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3jl8r</id>
    <title>Here are the benchmarks that I keep up with</title>
    <updated>2025-10-11T02:02:27+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy_v2</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy_v2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hey folks! I've returned... in a fashion.&lt;/p&gt; &lt;p&gt;I've been sitting on all kinds of stuff that I wanted to talk about for the past few months, but I figured I'd start by dropping the list of benchmarks I currently track, since in the past folks were interested in that list.&lt;/p&gt; &lt;p&gt;These should be mostly up to date, and I'm constantly on the prowl for more. If you have any good ones (&lt;em&gt;ESPECIALLY translation benchmarks... those feel like the holy grail&lt;/em&gt;), please share.&lt;/p&gt; &lt;p&gt;I know there are a lot more leaderboards out there, but I generally don't hang on to the ones that either aren't kept reasonably up to date, or were exceptionally limited. So if you don't see a leaderboard on here, feel free to share but it may have been excluded on purpose.&lt;/p&gt; &lt;p&gt;As always- benchmarks aren't everything, and you should always try the models out yourself. But it definitely is nice to have some metrics to look at from time to time, even if they can get gamed.&lt;/p&gt; &lt;h1&gt;Code Specific&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.swebench.com/"&gt;SWE Bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://aider.chat/docs/leaderboards/"&gt;Aider Coding Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Context Window Capability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87"&gt;FictionBench&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;(This is a really good one, as it visualizes where so many people mess up with LLMs: not realizing context window limitations)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;General Ability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://livebench.ai/#/"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://dubesor.de/benchtable"&gt;Dubesor Benchtable&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/humanitys_last_exam_text_only"&gt;Humanity's Last Exam&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;(I am shocked at how low of a score GLM 4.5 got here... testing error maybe?)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Domain Knowledge&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro"&gt;MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Advanced Reasoning&lt;/h1&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/enigma_eval"&gt;Enigma Eval&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Human Preference&lt;/h1&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard"&gt;LM Arena&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;EQ (emotional intelligence) and Creative Writing Ability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://eqbench.com/"&gt;EQBench&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Censorship&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;Uncensored General Intelligence Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Intelligence Index, Cost, Speed, and Model Comparisons&lt;/h1&gt; &lt;p&gt;&lt;a href="https://artificialanalysis.ai/leaderboards/models"&gt;Artificial Analysis&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Coding Agent Capability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.tbench.ai/leaderboard"&gt;Terminal Bench&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Kotlin (Android dev)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://firebender.com/leaderboard"&gt;Kotlin Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Function Calling&lt;/h1&gt; &lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;Berkeley Function-Calling Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Other&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.vellum.ai/llm-leaderboard"&gt;Vellum Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy_v2"&gt; /u/SomeOddCodeGuy_v2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3r6et</id>
    <title>Quality degradation of fp8 quantization?</title>
    <updated>2025-10-11T09:25:15+00:00</updated>
    <author>
      <name>/u/Confident-Willow5457</name>
      <uri>https://old.reddit.com/user/Confident-Willow5457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been entirely in the GGUF ecosystem up till now, but I've been wanting to try out vllm for a potential speed boost as well as batching. &lt;/p&gt; &lt;p&gt;Generally with GGUFs Q8_0 is considered to be so close to full precision that it's practically indistinguishable. It's my understanding that Q8_0 is a bit closer to full precision than FP8, but how much worse is FP8 than full precision exactly? As a reference, is it between Q8 and Q6? Worse?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident-Willow5457"&gt; /u/Confident-Willow5457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3r6et/quality_degradation_of_fp8_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3r6et/quality_degradation_of_fp8_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3r6et/quality_degradation_of_fp8_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T09:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3evon</id>
    <title>What laptop would you choose? Ryzen AI MAX+ 395 with 128GB of unified RAM or Intel 275HX + Nvidia RTX 5090 (128GB of RAM + 24GB of VRAM)?</title>
    <updated>2025-10-10T22:20:42+00:00</updated>
    <author>
      <name>/u/cl0p3z</name>
      <uri>https://old.reddit.com/user/cl0p3z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For more or less the same price I can chose between this two laptops:&lt;/p&gt; &lt;p&gt;- HP G1a: &lt;strong&gt;AMD Ryzen AI MAX+ 395 with 128GB of RAM (no eGPU)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Lenovo ThinkPad P16 Gen 3: &lt;strong&gt;Intel 275HX with 128GB of RAM + Nvidia RTX 5090 24GB of VRAM&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What would you choose and why?&lt;/p&gt; &lt;p&gt;What I can do with AI/LLMs with one that I can't do with the other?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cl0p3z"&gt; /u/cl0p3z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T22:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3gyjn</id>
    <title>best coding LLM right now?</title>
    <updated>2025-10-10T23:53:41+00:00</updated>
    <author>
      <name>/u/RadianceTower</name>
      <uri>https://old.reddit.com/user/RadianceTower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models constantly get updated and new ones come out, so old posts aren't as valid.&lt;/p&gt; &lt;p&gt;I have 24GB of VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadianceTower"&gt; /u/RadianceTower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T23:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3k9zr</id>
    <title>AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM</title>
    <updated>2025-10-11T02:37:28+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"&gt; &lt;img alt="AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM" src="https://external-preview.redd.it/pGGQzmvOi4pQiLTOWOaCqIqsQ66ZIQqjltXUjZikyAU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5a776a9f8f4b6e3a30c9df214e7029c4c64c5b8" title="AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techradar.com/pro/this-mini-pc-has-192gb-of-ram-yes-ram-but-thats-not-the-most-surprising-fact-about-it-the-orange-pi-ai-studio-pro-uses-a-huawei-ascend-310-thats-on-paper-7x-more-powerful-than-amds-ryzen-ai-max-395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3sstr</id>
    <title>Every single COT terms score.</title>
    <updated>2025-10-11T11:04:46+00:00</updated>
    <author>
      <name>/u/Ambitious-a4s</name>
      <uri>https://old.reddit.com/user/Ambitious-a4s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;1. Zeroing&lt;/strong&gt; - By far the most annoying word I've ever heard, used by Gemini 2.5 Pro's Thinking but when people actually knows this meaning, it just a fancy word that means 'I am directing all the focus into this or that (subject)'. Its efficient (it only uses two tokens) and fancy but annoying. &lt;strong&gt;8.7/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Synthesizing&lt;/strong&gt; - GLM 4.6 uses this, Gemini 2.5 Pro Exp, and more. Its fancy wording too, but it just means 'I am combining this thought I have with the old thought I made', its good and it doesn't really sound the much annoying (IMO). It also helps the AI combining ideas and thoughts in one go so its &lt;strong&gt;9/10&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Hmm&lt;/strong&gt; - Used by Qwen (idk what model but Qwen), Deepseek V3.1 to V3.2, and more. Its not fancy, but it just means lots of things, sometimes Qwen 3 235B do this by pausing and hesitating before dumping more thoughts, and Deepseek uses this in the first word to think. It doesn't do much I would say, its by far mid and its only for pausing and hesitating to think. My only favorite part is that it uses two tokens, &lt;strong&gt;6.6/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Confidence Score/Confident Score&lt;/strong&gt; - For some reason, it's one of my favorite terms. It makes the LLM aware how confident it is with the answer or not. It can also make the LLM think more further ahead for some reason, but its not perfect, most LLM's hallucinate and would think its 5/5 or 10/10 in a wrong answer they would give so sometimes it had no point to use it. &lt;strong&gt;7.1/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Alternatively&lt;/strong&gt; - The old Deepseek times. ONE OF MY FAVORITE TERMS. It makes the LLM aware of its thoughts so it lowers the mistake. &lt;/p&gt; &lt;p&gt;The biggest con is when the LLM uses these terms is that it BLOATS, when you expect it only uses 941 tokens on a single thought turned into a massive 5000 tokens in a single thought before response. It makes my API cripple so bad and its the biggest con. So its a &lt;strong&gt;5.4/10&lt;/strong&gt;, I wish it can be back but this time more efficient.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious-a4s"&gt; /u/Ambitious-a4s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T11:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3kb3o</id>
    <title>Real SVD GLM-4.5-Air-GLM-4.6-Distill</title>
    <updated>2025-10-11T02:39:01+00:00</updated>
    <author>
      <name>/u/realmaywell</name>
      <uri>https://old.reddit.com/user/realmaywell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt; &lt;img alt="Real SVD GLM-4.5-Air-GLM-4.6-Distill" src="https://external-preview.redd.it/e09zZ1vJH-206eXdnYOOyzVU_npt174nxPybTvRs_LQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aac58b8a3b097d5a8fa8a939d97c17ee2f3dee5" title="Real SVD GLM-4.5-Air-GLM-4.6-Distill" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f"&gt;https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the person who posted that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fb6jdy/reflectionllama3170b_is_actually_llama3/"&gt;Reflection-Llama-3.1-70B is actually Llama-3. &lt;/a&gt;I didn't expect my first Reddit post in a year would be another debunk.&lt;/p&gt; &lt;p&gt;After seeing this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;Reddit post&lt;/a&gt;, the idea of using SVD to distill a model seemed plausible, So I decided to test it out myself.&lt;/p&gt; &lt;p&gt;Although the original model mentioned in the post was a scam, I was curious about what would happen if I actually applied the methodology. So, I rewrote the entire existing CPU-based script into PyTorch code and ran the experiment on an H200 machine.&lt;/p&gt; &lt;p&gt;I excluded the LayerNorm and embed layers from the distillation because it was obvious that including them would break the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e"&gt;https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The results are as shown in the pictures above. I guess this means you don't have to go out of your way to replicate this experiment yourselves.&lt;/p&gt; &lt;p&gt;Still, for those who do want to try it out, you might find the model and LoRA below helpful.&lt;br /&gt; Model: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill&lt;/a&gt;&lt;br /&gt; LoRA: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yay.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realmaywell"&gt; /u/realmaywell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3l5zs</id>
    <title>GLM just blow up, or have I been in the dark?</title>
    <updated>2025-10-11T03:24:01+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like this community is ever moving, did GLM just blow up? like, I did not realise so many people talked about it.... What kinda system are you guys on 4.6 running? Because it looks like I would essential need 4x48gb Quadro 8000s/a6000s/6000 ada GPUs or at least 2x96gb RTX Pro 6000s... I may can afford 4 quadros but not 2 rtx pro 6000s, for the price of a car. lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T03:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3dfib</id>
    <title>GPT-OSS from Scratch on AMD GPUs</title>
    <updated>2025-10-10T21:21:45+00:00</updated>
    <author>
      <name>/u/tuanlda78202</name>
      <uri>https://old.reddit.com/user/tuanlda78202</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt; &lt;img alt="GPT-OSS from Scratch on AMD GPUs" src="https://external-preview.redd.it/kr7-cYQLjVSYgCHTdYk0hOHo8LtDx1fruxInOK1rC5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df53654295de2b4add7e5b3992fa595b88f048f1" title="GPT-OSS from Scratch on AMD GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After six years-the first time since GPT-2, OpenAI has released new open-weight LLMs, gpt-oss-20b and gpt-oss-120b. From day one, many inference engines such as llama.cpp, vLLM, and sgl-project have supported these models; however, most focus on maximizing throughput using CUDA for NVIDIA GPUs, offering limited support for AMD* GPUs. Moreover, their library-oriented implementations are often complex to understand and difficult to adapt for personal or experimental use cases.&lt;/p&gt; &lt;p&gt;To address these limitations, my team introduce ‚Äúgpt-oss-amd‚Äù, a pure C++ implementation of OpenAI‚Äôs GPT-OSS models designed to maximize inference throughput on AMD GPUs without relying on external libraries. Our goal is to explore end-to-end LLM optimization, from kernel-level improvements to system-level design, providing insights for researchers and developers interested in high-performance computing and model-level optimization.&lt;/p&gt; &lt;p&gt;Inspired by llama2.c by Andrej Karpathy, our implementation uses HIP (an AMD programming model equivalent to CUDA) and avoids dependencies such as rocBLAS, hipBLAS, RCCL, and MPI. We utilize multiple optimization strategies for the 20B and 120B models, including efficient model loading, batching, multi-streaming, multi-GPU communication, optimized CPU‚ÄìGPU‚ÄìSRAM memory access, FlashAttention, matrix-core‚Äìbased GEMM, and load balancing for MoE routing.&lt;/p&gt; &lt;p&gt;Experiments on a single node with 8√ó AMD MI250 GPUs show that our implementation achieves over 30k TPS on the 20B model and nearly 10k TPS on the 120B model in custom benchmarks, demonstrating the effectiveness of our optimizations and the strong potential of AMD GPUs for large-scale LLM inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a"&gt;https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tuanlda78202/gpt-oss-amd"&gt;https://github.com/tuanlda78202/gpt-oss-amd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuanlda78202"&gt; /u/tuanlda78202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T21:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3atdu</id>
    <title>GLM 5 coming before the end of 2025</title>
    <updated>2025-10-10T19:41:19+00:00</updated>
    <author>
      <name>/u/Helpful_Jacket8953</name>
      <uri>https://old.reddit.com/user/Helpful_Jacket8953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt; &lt;img alt="GLM 5 coming before the end of 2025" src="https://a.thumbs.redditmedia.com/hVBgymEdEuqZZKnYwcpRm5UQBweQDbSo5chE1AGg9a8.jpg" title="GLM 5 coming before the end of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get ready. At this rate it seems like there's a real chance it'll start surpassing SOTA models on some benchmarks, not just DeepSeek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2"&gt;https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Helpful_Jacket8953"&gt; /u/Helpful_Jacket8953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T19:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o371t4</id>
    <title>bro disappeared like he never existed</title>
    <updated>2025-10-10T17:18:54+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt; &lt;img alt="bro disappeared like he never existed" src="https://preview.redd.it/2e01fz4pibuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2158310d30c308746ab8924442748cf6a37b692a" title="bro disappeared like he never existed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Knowing him is a sign you‚Äôve been in the AI game for a long time (iykyk)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e01fz4pibuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T17:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o394p3</id>
    <title>Here we go again</title>
    <updated>2025-10-10T18:36:34+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt; &lt;img alt="Here we go again" src="https://preview.redd.it/b2abfaikwbuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db3949cb0def07809e7a9ba9a730d1582083844" title="Here we go again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2abfaikwbuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T18:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3opq5</id>
    <title>What the sub feels like lately</title>
    <updated>2025-10-11T06:47:33+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt; &lt;img alt="What the sub feels like lately" src="https://preview.redd.it/92s8znbxifuf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4866bff0d572386ea47fc19d643a6b2261fbdb" title="What the sub feels like lately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92s8znbxifuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
