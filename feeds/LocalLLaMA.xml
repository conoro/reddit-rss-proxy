<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-01T15:24:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q0kndt</id>
    <title>made a simple CLI tool to pipe anything into an LLM. that follows unix philosophy.</title>
    <updated>2025-12-31T19:00:04+00:00</updated>
    <author>
      <name>/u/Famous-Koala-4352</name>
      <uri>https://old.reddit.com/user/Famous-Koala-4352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kndt/made_a_simple_cli_tool_to_pipe_anything_into_an/"&gt; &lt;img alt="made a simple CLI tool to pipe anything into an LLM. that follows unix philosophy." src="https://external-preview.redd.it/2mIni041WJ-kGj619KCJlOIIx8G0zfXcaEidwq1__hc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b62d97dea222252e53b2edb1e6837fc7dd52f067" title="made a simple CLI tool to pipe anything into an LLM. that follows unix philosophy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just finished building infer - it's inspired from grep but for asking an LLM questions about your command output. &lt;/p&gt; &lt;p&gt;the whole idea is you can do stuff like:&lt;br /&gt; ps aux | infer &amp;quot;what's eating my RAM&amp;quot; &lt;/p&gt; &lt;p&gt;dmesg | infer &amp;quot;any hardware errors?&amp;quot; &lt;/p&gt; &lt;p&gt;git log --oneline -20 | infer &amp;quot;what did I work on today&amp;quot; &lt;/p&gt; &lt;p&gt;infer &amp;quot;what's the tar command to extract .tar.gz?&amp;quot; &lt;/p&gt; &lt;p&gt;It's less than 200 lines of C, reads from stdin, spits out plain text. works with openai compatable api I got tired of copy-pasting logs into llms, so now I just pipe everything. been using it for a week and it's genuinely useful for debugging and remembering commands. so i tought of publishing it now.&lt;/p&gt; &lt;p&gt;feedbacks are welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Koala-4352"&gt; /u/Famous-Koala-4352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chethanreddy1/infer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kndt/made_a_simple_cli_tool_to_pipe_anything_into_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kndt/made_a_simple_cli_tool_to_pipe_anything_into_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T19:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0rrxr</id>
    <title>I built AIfred-Intelligence - a self-hosted AI assistant with automatic web research and multi-agent debates (AIfred with upper "i" instead of lower "L" :-)</title>
    <updated>2026-01-01T00:48:21+00:00</updated>
    <author>
      <name>/u/Peuqui</name>
      <uri>https://old.reddit.com/user/Peuqui</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0rrxr/i_built_aifredintelligence_a_selfhosted_ai/"&gt; &lt;img alt="I built AIfred-Intelligence - a self-hosted AI assistant with automatic web research and multi-agent debates (AIfred with upper &amp;quot;i&amp;quot; instead of lower &amp;quot;L&amp;quot; :-)" src="https://preview.redd.it/b8qjb0pzdmag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e64a13ef94483a1313f96b4738a7daf664a7ed2" title="I built AIfred-Intelligence - a self-hosted AI assistant with automatic web research and multi-agent debates (AIfred with upper &amp;quot;i&amp;quot; instead of lower &amp;quot;L&amp;quot; :-)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Been working just for fun and learning about LLM on this for a while:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AIfred Intelligence&lt;/strong&gt; is a self-hosted AI assistant that goes beyond simple chat.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Automatic Web Research&lt;/strong&gt; - AI autonomously decides when to search the web, scrapes sources in parallel, and cites them. No manual commands needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-Agent Debates&lt;/strong&gt; - Three AI personas with different roles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üé© AIfred (scholar) - answers your questions as an English butler&lt;/li&gt; &lt;li&gt;üèõÔ∏è Sokrates (critic) - as himself with ancient greek personality, challenges assumptions, finds weaknesses&lt;/li&gt; &lt;li&gt;üëë Salomo (judge) - as himself, synthesizes and delivers final verdict&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Editable system/personality prompts&lt;/p&gt; &lt;p&gt;As you can see in the screenshot, there's a &amp;quot;Discussion Mode&amp;quot; dropdown with options like Tribunal (agents debate X rounds ‚Üí judge decides) or Auto-Consensus (they discuss until 2/3 or 3/3 agree) and more modes.&lt;/p&gt; &lt;p&gt;History compression at 70% utilization. Conversations never hit the context wall (hopefully :-) ).&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Vision/OCR&lt;/strong&gt; - Crop tool, multiple vision models (Qwen3-VL, DeepSeek-OCR)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Voice Interface&lt;/strong&gt; - STT + TTS integration&lt;/p&gt; &lt;p&gt;UI internationalization in english / german per i18n&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Backends:&lt;/strong&gt; Ollama (best supported and most flexible), vLLM, KoboldCPP, (TabbyAPI coming (maybe) soon), - each remembers its own model preferences.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other stuff:&lt;/strong&gt; Thinking Mode (collapsible &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; blocks), LaTeX rendering, vector cache (ChromaDB), VRAM-aware context sizing, REST API for remote control to inject prompts and control the browser tab out of a script or per AI.&lt;/p&gt; &lt;p&gt;Built with Python/Reflex. Runs 100% local.&lt;/p&gt; &lt;p&gt;Extensive Debug Console output and debug.log file&lt;/p&gt; &lt;p&gt;Entire export of chat history&lt;/p&gt; &lt;p&gt;Tweaking of LLM parameters&lt;/p&gt; &lt;p&gt; &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Peuqui/AIfred-Intelligence"&gt;https://github.com/Peuqui/AIfred-Intelligence&lt;/a&gt;&lt;/p&gt; &lt;p&gt; Use larger models from 14B up, better 30B, for better context understanding and prompt following over large context windows&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;24/7 server:&lt;/strong&gt; AOOSTAR GEM 10 Mini-PC (32GB RAM) + 2x Tesla P40 on AG01/AG02 OCuLink adapters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Development:&lt;/strong&gt; AMD 9900X3D, 64GB RAM, RTX 3090 Ti&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions and like to read your opinions!&lt;/p&gt; &lt;p&gt;Happy new year and God bless you all,&lt;/p&gt; &lt;p&gt;Best wishes,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Peuqui&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peuqui"&gt; /u/Peuqui &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b8qjb0pzdmag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0rrxr/i_built_aifredintelligence_a_selfhosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0rrxr/i_built_aifredintelligence_a_selfhosted_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T00:48:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0i4g3</id>
    <title>Moonshot AI Completes $500 Million Series C Financing</title>
    <updated>2025-12-31T17:13:54+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI company Moonshot AI has completed a $500 million Series C financing. Founder Zhilin Yang revealed in an internal letter that the company‚Äôs global paid user base is growing at a monthly rate of 170%. Since November, driven by the K2 Thinking model, Moonshot AI‚Äôs overseas API revenue has increased fourfold. The company holds more than RMB 10 billion in cash reserves (approximately $1.4 billion). This scale is already on par with Zhipu AI and MiniMax after their IPOs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As of June 2025, Zhipu AI has RMB 2.55 billion in cash, with an IPO expected to raise about RMB 3.8 billion.&lt;/li&gt; &lt;li&gt;As of September 2025, MiniMax has RMB 7.35 billion in cash, with an IPO expected to raise RMB 3.4‚Äì3.8 billion.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the internal letter, Zhilin Yang stated that the funds from the Series C financing will be used to more aggressively expand GPU capacity, accelerate the training and R&amp;amp;D of the K3 model, and he also announced key priorities for 2026:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bring the K3 model‚Äôs pretraining performance up to par with the world‚Äôs leading models, leveraging technical improvements and further scaling to increase its equivalent FLOPs by at least an order of magnitude.&lt;/li&gt; &lt;li&gt;Make K3 a more &amp;quot;distinctive&amp;quot; model by vertically integrating training technologies and product taste, enabling users to experience entirely new capabilities that other models do not offer.&lt;/li&gt; &lt;li&gt;Achieve an order-of-magnitude increase in revenue scale, with products and commercialization focused on Agents, not targeting absolute user numbers, but pursuing the upper limits of intelligence to create greater productivity value.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0i4g3/moonshot_ai_completes_500_million_series_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0i4g3/moonshot_ai_completes_500_million_series_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0i4g3/moonshot_ai_completes_500_million_series_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T17:13:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q163um</id>
    <title>GLM 4.7 on 8x3090</title>
    <updated>2026-01-01T15:00:38+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone running GLM 4.7 (or 4.5-4.6) on eight 3090s? I was wondering what kind of speeds you were getting as I was considering this set up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T15:00:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ny4i</id>
    <title>Orange Pi Unveils AI Station with Ascend 310 and 176 TOPS Compute</title>
    <updated>2025-12-31T21:30:49+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Orange Pi closes the year by unveiling new details about the Orange Pi AI Station, a compact board-level edge computing platform built around the Ascend 310 series processor. The system targets high-density inference workloads with large memory options, NVMe storage support, and extensive I/O in a small footprint.&lt;/p&gt; &lt;p&gt;The AI Station is powered by an Ascend 310 series processor integrating 16 CPU cores clocked at up to 1.9 GHz, along with 10 AI cores running at up to 1.08 GHz and 8 vector cores operating at up to 1 GHz.&lt;/p&gt; &lt;p&gt;According to Orange Pi, the platform delivers up to 176 TOPS of AI compute performance, enabling large-scale inference and feature-extraction workloads.&lt;/p&gt; &lt;p&gt;Memory options include 48 GB or 96 GB of LPDDR4X operating at up to 4266 MHz. Storage support consists of a PCIe 4.0 √ó4 M.2 2280 slot for NVMe SSDs, onboard eMMC support up to 256 GB, a 16 MB SPI flash device, and a microSD card slot for removable storage.&lt;/p&gt; &lt;p&gt;The Orange Pi AI Station has an official product page already, though purchase links were unavailable at the time of publication.&lt;/p&gt; &lt;p&gt;&lt;a href="https://linuxgizmos.com/orange-pi-unveils-ai-station-with-ascend-310-and-176-tops-compute/"&gt;https://linuxgizmos.com/orange-pi-unveils-ai-station-with-ascend-310-and-176-tops-compute/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ny4i/orange_pi_unveils_ai_station_with_ascend_310_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ny4i/orange_pi_unveils_ai_station_with_ascend_310_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ny4i/orange_pi_unveils_ai_station_with_ascend_310_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T21:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0wenz</id>
    <title>2025: The year in LLMs</title>
    <updated>2026-01-01T05:12:43+00:00</updated>
    <author>
      <name>/u/ocirs</name>
      <uri>https://old.reddit.com/user/ocirs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wenz/2025_the_year_in_llms/"&gt; &lt;img alt="2025: The year in LLMs" src="https://external-preview.redd.it/Kl93K2bjo23tHSiycW5RpoDEJS3ZCajwxIKrIfs14L0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc88adfabadde09aed63b4ba2966a2e2cc0dd647" title="2025: The year in LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ocirs"&gt; /u/ocirs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://simonwillison.net/2025/Dec/31/the-year-in-llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wenz/2025_the_year_in_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wenz/2025_the_year_in_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T05:12:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0wkwc</id>
    <title>QWEN-Image-2512 Mflux Port available now</title>
    <updated>2026-01-01T05:23:22+00:00</updated>
    <author>
      <name>/u/Street-Buyer-2428</name>
      <uri>https://old.reddit.com/user/Street-Buyer-2428</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released the first MLX ports of Qwen-Image-2512 - Qwen's latest text-to-image model released TODAY.&lt;/p&gt; &lt;p&gt;5 quantizations for Apple Silicon:&lt;/p&gt; &lt;p&gt;- 8-bit (34GB)&lt;/p&gt; &lt;p&gt;- 6-bit (29GB)&lt;/p&gt; &lt;p&gt;- 5-bit (27GB)&lt;/p&gt; &lt;p&gt;- 4-bit (24GB)&lt;/p&gt; &lt;p&gt;- 3-bit (22GB)&lt;/p&gt; &lt;p&gt;Run locally on your Mac:&lt;/p&gt; &lt;p&gt; pip install mflux&lt;/p&gt; &lt;p&gt; mflux-generate-qwen --model machiabeli/Qwen-Image-2512-4bit-MLX --prompt &amp;quot;...&amp;quot; --steps 20&lt;/p&gt; &lt;p&gt; Links: &lt;a href="http://huggingface.co/machiabeli"&gt;huggingface.co/machiabeli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Buyer-2428"&gt; /u/Street-Buyer-2428 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wkwc/qwenimage2512_mflux_port_available_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wkwc/qwenimage2512_mflux_port_available_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wkwc/qwenimage2512_mflux_port_available_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T05:23:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q15ffw</id>
    <title>llama.cpp - Custom Optimized Builds?</title>
    <updated>2026-01-01T14:29:00+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm talking about &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md"&gt;cmake command to create builds&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'm trying to create optimized build for my Laptop config. Just trying to get additional t/s with my 8GB VRAM &amp;amp; 32GB RAM.&lt;/p&gt; &lt;p&gt;Do we have any &lt;strong&gt;page/repo/markdown on list of variables to use with cmake command&lt;/strong&gt;? Want to know which variables are better for each version(CUDA, CPU, Vulkan). That way I could pick suitable ones for my config.&lt;/p&gt; &lt;p&gt;At first, I was trying to create MKL build(Intel oneAPI &lt;strong&gt;M&lt;/strong&gt;ath &lt;strong&gt;K&lt;/strong&gt;ernel &lt;strong&gt;L&lt;/strong&gt;ibrary) for CPU-only. It didn't work. Totally Pain-in-@$$. Have to try again later. (Qwen suggested me MKL build for optimized performance .... for my CPU Intel(R) Core(TM) i7-14700HX)&lt;/p&gt; &lt;p&gt;After this MKL, I'm gonna try optimized CUDA build for my 4060 Laptop GPU. Heard that I have to add additional variable for architecture with some double digit number. Also my laptop supports AVX, AVX2(unfortunately no AVX512) which needs additional variable.&lt;/p&gt; &lt;p&gt;And please share your &lt;strong&gt;custom commands you're using for CUDA, CPU(also Vulkan, AMD)&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;In past, I saw some comments on random threads with very long build commands(&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ps4jho/comment/nvfhee2/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;here one example&lt;/a&gt;), unfortunately I forgot to save those at that time.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15ffw/llamacpp_custom_optimized_builds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15ffw/llamacpp_custom_optimized_builds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q15ffw/llamacpp_custom_optimized_builds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T14:29:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q094a3</id>
    <title>Qwen-Image-2512</title>
    <updated>2025-12-31T09:38:19+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt; &lt;img alt="Qwen-Image-2512" src="https://preview.redd.it/2vlr11yveiag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c1e7a0b7ea834c8babae002078a848096514e1b" title="Qwen-Image-2512" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth:&lt;br /&gt; Guide: &lt;a href="https://unsloth.ai/docs/models/qwen-image-2512"&gt;https://unsloth.ai/docs/models/qwen-image-2512&lt;/a&gt;&lt;br /&gt; GGUF: &lt;a href="https://huggingface.co/unsloth/Qwen-Image-2512-GGUF"&gt;https://huggingface.co/unsloth/Qwen-Image-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-----------------&lt;/p&gt; &lt;p&gt;üëâ Try it now in Qwen Chat: &lt;a href="https://chat.qwen.ai/?inputFeature=t2i"&gt;https://chat.qwen.ai/?inputFeature=t2i&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope: &lt;a href="https://modelscope.ai/models/Qwen/Qwen-Image-2512"&gt;https://modelscope.ai/models/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª GitHub: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìù Blog: &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;https://qwen.ai/blog?id=qwen-image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/spaces/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope Demo: &lt;a href="https://modelscope.cn/aigc/imageGeneration"&gt;https://modelscope.cn/aigc/imageGeneration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ú®API: &lt;a href="https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max"&gt;https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2vlr11yveiag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T09:38:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q12lzm</id>
    <title>I built a specific-domain Text-to-SQL Agent using Llama-3-70B (via Groq). It handles Railway IoT logs with 96% accuracy using strict schema binding and a custom 'Bouncer' guardrail</title>
    <updated>2026-01-01T11:56:48+00:00</updated>
    <author>
      <name>/u/BitFearless5307</name>
      <uri>https://old.reddit.com/user/BitFearless5307</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q12lzm/i_built_a_specificdomain_texttosql_agent_using/"&gt; &lt;img alt="I built a specific-domain Text-to-SQL Agent using Llama-3-70B (via Groq). It handles Railway IoT logs with 96% accuracy using strict schema binding and a custom 'Bouncer' guardrail" src="https://b.thumbs.redditmedia.com/U5PlF9kpraN7tXtJ7gcCVVHrnjQO6wK8vPXV7FJc2ls.jpg" title="I built a specific-domain Text-to-SQL Agent using Llama-3-70B (via Groq). It handles Railway IoT logs with 96% accuracy using strict schema binding and a custom 'Bouncer' guardrail" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I wanted to share a project I finished over the break. It‚Äôs an agent designed to help non-technical railway managers query fault detection logs without writing SQL.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack:&lt;/strong&gt; * &lt;strong&gt;Model:&lt;/strong&gt; Llama-3-70B (served via Groq for speed). * &lt;strong&gt;Orchestration:&lt;/strong&gt; LangChain. * &lt;strong&gt;Latency:&lt;/strong&gt; Sub-1.2s end-to-end.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Generic Text-to-SQL often hallucinates tables or allows dangerous queries.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Solution:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strict Schema Binding:&lt;/strong&gt; I inject the specific SQLite schema into the system prompt, restricting the LLM to only valid columns. 2. &lt;strong&gt;The 'Bouncer':&lt;/strong&gt; I wrote a pre-execution Python layer that sanitizes input and blocks 100% of destructive commands (DROP, DELETE, etc.) before they hit the DB.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; Tested on a golden set of 50 queries (aggregations, filters), it hit 96% accuracy.&lt;/p&gt; &lt;p&gt;Repo link is in the comments if anyone wants to roast my code. Feedback welcome!&lt;br /&gt; &lt;a href="https://github.com/hemanthmuralik/Rail-GPT-Text-to-SQL-Agent-for-Railway-Fault-Detection"&gt;Rail-GPT-Text-to-SQL-Agent-for-Railway-Fault-Detection&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6to4egeo8qag1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16e41b9faaf0f85aa160ab7ee65477b6efa6f07b"&gt;https://preview.redd.it/6to4egeo8qag1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16e41b9faaf0f85aa160ab7ee65477b6efa6f07b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitFearless5307"&gt; /u/BitFearless5307 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q12lzm/i_built_a_specificdomain_texttosql_agent_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q12lzm/i_built_a_specificdomain_texttosql_agent_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q12lzm/i_built_a_specificdomain_texttosql_agent_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T11:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1q110g5</id>
    <title>My third and final derivation post: Understanding GRPO step by step</title>
    <updated>2026-01-01T10:14:17+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q110g5/my_third_and_final_derivation_post_understanding/"&gt; &lt;img alt="My third and final derivation post: Understanding GRPO step by step" src="https://external-preview.redd.it/E--8HNmTu_NoPUyVcISVd4hoLpw2KZ2Gi_gnYk8f1WI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=455941393ba91ffd55472ff18fdbd18999911820" title="My third and final derivation post: Understanding GRPO step by step" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy New Year everyone!&lt;/p&gt; &lt;p&gt;I am starting my 2026 by finishing what I started a few days ago. This is the third and final post in my &lt;strong&gt;derive the RL loss(es) from first principles&lt;/strong&gt; series, following &lt;a href="https://huggingface.co/blog/garg-aayush/ppo-from-first-principle"&gt;PPO&lt;/a&gt; and &lt;a href="https://huggingface.co/blog/garg-aayush/derive-dpo-loss"&gt;DPO&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This time I focused on GRPO (Group Relative Policy Optimization), the algorithm introduced in the DeepSeekMath paper that has become one of the most widely used approaches for training reasoning models using RLVR throughout 2025.&lt;/p&gt; &lt;p&gt;In simple terms, GRPO tries to mitigate the memory and compute overhead associated with PPO due to training a critic (value function) model of similar size as the policy alongside the policy model.&lt;/p&gt; &lt;p&gt;The key insight is that the PPO value function is fundamentally just a baseline for variance reduction. Instead of training a separate critic model to estimate this baseline, we can sample multiple completions (&lt;strong&gt;group&lt;/strong&gt;) for each prompt and use their rewards to form a baseline for advantage computation.&lt;/p&gt; &lt;p&gt;This helps us eliminate the need to train a separate critic model and lowers training compute and memory footprint while still preserving PPO‚Äôs core stability mechanisms, including the clipped surrogate objective and KL regularization.&lt;/p&gt; &lt;p&gt;You can find the blog post here: &lt;a href="https://huggingface.co/blog/garg-aayush/derive-grpo-loss"&gt;https://huggingface.co/blog/garg-aayush/derive-grpo-loss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is probably my last mathematical derivation post for a while. Working through PPO, DPO, and GRPO derivations was both hectic and frustrating at times. However, it has been a great way to build intuition around the most popular RL algorithms. Moreover, it helped me understand the key differences and commonalities between all three and how they relate to each other.&lt;/p&gt; &lt;p&gt;As always, happy to discuss or get corrections if I have messed something up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/garg-aayush/derive-grpo-loss"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q110g5/my_third_and_final_derivation_post_understanding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q110g5/my_third_and_final_derivation_post_understanding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T10:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q13040</id>
    <title>Does anyone know good email clients with local LLM?</title>
    <updated>2026-01-01T12:20:29+00:00</updated>
    <author>
      <name>/u/TurthHurtsDoesntIt</name>
      <uri>https://old.reddit.com/user/TurthHurtsDoesntIt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to find some good email client for Linux/Windows/Android without success. I do not even have unreasonable requirements but not even one of currently accessible projects (for example: inbox-zero, eppie) that I found meet them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;finished application&lt;/li&gt; &lt;li&gt;imap login (no api key mumbo jumbos)&lt;/li&gt; &lt;li&gt;Local AI model usage only&lt;/li&gt; &lt;li&gt;Local AI needs to sort emails, automatically unsubscribe junk, remove spam, add events to calendar and set reminders.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Does anyone know anything that would fit above requirements?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TurthHurtsDoesntIt"&gt; /u/TurthHurtsDoesntIt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q13040/does_anyone_know_good_email_clients_with_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q13040/does_anyone_know_good_email_clients_with_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q13040/does_anyone_know_good_email_clients_with_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T12:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q15vr6</id>
    <title>Any clues as to what Gemma 3's training data consisted of?</title>
    <updated>2026-01-01T14:50:22+00:00</updated>
    <author>
      <name>/u/EducationalCicada</name>
      <uri>https://old.reddit.com/user/EducationalCicada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Google would never release this information, but has anyone been able to extract parts of the training data from Gemma 3? I'm really curious about what they used.&lt;/p&gt; &lt;p&gt;I'm guessing it was trained on public domain (and lower quality, compared to what they fed Gemini) data due to the existence of such attacks on open-weight models.&lt;/p&gt; &lt;p&gt;It's a bit frustrating because Google is sitting on some of the most valuable data on the planet , but Gemma will never see any of it in training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationalCicada"&gt; /u/EducationalCicada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T14:50:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0wemx</id>
    <title>Happy New Years everyone!</title>
    <updated>2026-01-01T05:12:40+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2026 will feel like a decade. Onward!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wemx/happy_new_years_everyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wemx/happy_new_years_everyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wemx/happy_new_years_everyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T05:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0vph6</id>
    <title>OpenForecaster Release</title>
    <updated>2026-01-01T04:30:45+00:00</updated>
    <author>
      <name>/u/logisbase2</name>
      <uri>https://old.reddit.com/user/logisbase2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"&gt; &lt;img alt="OpenForecaster Release" src="https://preview.redd.it/iuw1u1y61oag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39850abc1dea70f66b62d9a4cafb3424faf01f53" title="OpenForecaster Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/papers/2512.25070"&gt;https://huggingface.co/papers/2512.25070&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logisbase2"&gt; /u/logisbase2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iuw1u1y61oag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T04:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0x19t</id>
    <title>Anyone tried IQuest-Coder-V1 yet? The 40B numbers look wild</title>
    <updated>2026-01-01T05:51:19+00:00</updated>
    <author>
      <name>/u/Agile-Salamander1667</name>
      <uri>https://old.reddit.com/user/Agile-Salamander1667</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"&gt; &lt;img alt="Anyone tried IQuest-Coder-V1 yet? The 40B numbers look wild" src="https://a.thumbs.redditmedia.com/ORV5UJcDg_JMf-s_fhhD2rkOvb4H-H9k_D7F3CobGM8.jpg" title="Anyone tried IQuest-Coder-V1 yet? The 40B numbers look wild" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new IQuest-Coder-V1 family just dropped on GitHub and Hugging Face, and the benchmark numbers are honestly looking a bit wild for a 40B model. It‚Äôs claiming &lt;strong&gt;81.4% on SWE-Bench Verified&lt;/strong&gt; and over &lt;strong&gt;81% on LiveCodeBench v6&lt;/strong&gt;, which puts it right up there with (or ahead of) much larger proprietary models like GPT-5.1 and Claude 4.5 Sonnet. What's interesting is their &amp;quot;Code-Flow&amp;quot; training approach‚Äîinstead of just learning from static files, they trained it on repository evolution and commit transitions to better capture how logic actually changes over time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vek0sb18foag1.png?width=3022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=560bd32d14cdc982931196028beafea8dc97d3a1"&gt;https://preview.redd.it/vek0sb18foag1.png?width=3022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=560bd32d14cdc982931196028beafea8dc97d3a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They've released both &amp;quot;Instruct&amp;quot; and &amp;quot;Thinking&amp;quot; versions, with the latter using reasoning-driven RL to trigger better autonomous error recovery in long-horizon tasks. There's also a &amp;quot;Loop&amp;quot; variant that uses a recurrent transformer design to save on deployment footprint while keeping the capacity high. Since it supports a native &lt;strong&gt;128k context&lt;/strong&gt;, I‚Äôm curious if anyone has hooked this up to Aider or Cline yet.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1"&gt;https://github.com/IQuestLab/IQuest-Coder-V1&lt;/a&gt;&lt;br /&gt; &lt;a href="https://iquestlab.github.io/"&gt;https://iquestlab.github.io/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/IQuestLab"&gt;https://huggingface.co/IQuestLab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agile-Salamander1667"&gt; /u/Agile-Salamander1667 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T05:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0uoys</id>
    <title>Top 10 Open Models by Providers on LMArena</title>
    <updated>2026-01-01T03:32:13+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"&gt; &lt;img alt="Top 10 Open Models by Providers on LMArena" src="https://preview.redd.it/xo7h0asvqnag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f90fd4135eb4647968b39123e68d3e463e24269" title="Top 10 Open Models by Providers on LMArena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xo7h0asvqnag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T03:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q11bpg</id>
    <title>Upstage released an official response regarding the Solar 102B controversy</title>
    <updated>2026-01-01T10:34:37+00:00</updated>
    <author>
      <name>/u/Lucidstyle</name>
      <uri>https://old.reddit.com/user/Lucidstyle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"&gt; &lt;img alt="Upstage released an official response regarding the Solar 102B controversy" src="https://b.thumbs.redditmedia.com/UVpAFlyRpF-TXsFk90RKqLGFFfU7kttl0lfAk_EgShQ.jpg" title="Upstage released an official response regarding the Solar 102B controversy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;From Upstage CEO Sung Kim's Facebook:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;[Solar-Open-100B is not derived from GLM-4.5-Air]&lt;/p&gt; &lt;p&gt;Kevin Ko, who leads the open-source LLM development, has clearly addressed the issue.&lt;a href="https://github.com/hyunwoongko/solar-vs-glm-vs-phi"&gt;https://github.com/hyunwoongko/solar-vs-glm-vs-phi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's really great to see the ecosystem's self-correcting mechanism in action‚Äîwhere the community raises doubts and verifies them independently. Thank you.&lt;/p&gt; &lt;p&gt;Translated by Gemini&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ihjgtupfupag1.png?width=492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ac7f1a4b5675b48a4f6cc951ba09a29ac66b0cb"&gt;https://preview.redd.it/ihjgtupfupag1.png?width=492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ac7f1a4b5675b48a4f6cc951ba09a29ac66b0cb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lucidstyle"&gt; /u/Lucidstyle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T10:34:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0vom4</id>
    <title>IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)</title>
    <updated>2026-01-01T04:29:26+00:00</updated>
    <author>
      <name>/u/TellMeAboutGoodManga</name>
      <uri>https://old.reddit.com/user/TellMeAboutGoodManga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"&gt; &lt;img alt="IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)" src="https://external-preview.redd.it/BV6FpKtNQWUgU3wdfJKH5UR3dlogn4uR0Fs4eIn5vSk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3873330c45da75d454dd4483d37c39a58e5c6810" title="IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TellMeAboutGoodManga"&gt; /u/TellMeAboutGoodManga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T04:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0zk1u</id>
    <title>DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections</title>
    <updated>2026-01-01T08:35:29+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt; &lt;img alt="DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections" src="https://b.thumbs.redditmedia.com/lonIaVTlKZO_iYxmJPzoCCxeHd37wnAoXeHPfGhOUqA.jpg" title="DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.24880"&gt;https://arxiv.org/abs/2512.24880&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bovsed0x8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e292dc415f7fda8b1211ffe34864bb25ed4f32fe"&gt;https://preview.redd.it/bovsed0x8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e292dc415f7fda8b1211ffe34864bb25ed4f32fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g9986afz8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe031ea160ebff21a0dc46196d3dcf3b1b58548b"&gt;https://preview.redd.it/g9986afz8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe031ea160ebff21a0dc46196d3dcf3b1b58548b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T08:35:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0x8ci</id>
    <title>Software FP8 for GPUs without hardware support - 3x speedup on memory-bound operations</title>
    <updated>2026-01-01T06:03:27+00:00</updated>
    <author>
      <name>/u/Venom1806</name>
      <uri>https://old.reddit.com/user/Venom1806</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of my RTX 3050 not supporting FP8, so I built a workaround. Packs lower-precision values into FP32 using bitwise operations + Triton kernels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: 3x faster on memory-bound operations (GEMV, FlashAttention)&lt;/p&gt; &lt;p&gt;Works on any GPU - RTX 30/20 series, older cards without native FP8 support. Early stage but functional. Open to feedback.&lt;/p&gt; &lt;p&gt;&lt;a href="https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/"&gt;Article Link&lt;/a&gt; | &lt;a href="https://github.com/SuriyaaMM/feather"&gt;Github Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Venom1806"&gt; /u/Venom1806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T06:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0uuqt</id>
    <title>Happy New Year: Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning - Fine Tune. (based on recent find of L3.3 8b in the wild)</title>
    <updated>2026-01-01T03:41:30+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Special thanks to :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/user/jacek2023/"&gt;jacek2023&lt;/a&gt; [posting about this model]&lt;/p&gt; &lt;p&gt;and extra special thanks for &amp;quot;&lt;strong&gt;allura-forge&lt;/strong&gt; &amp;quot; for finding this model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;( For an incredible find of Llama 3.3 8B &amp;quot;in the wild&amp;quot; !!)&lt;/p&gt; &lt;p&gt;I fine tuned it using Unsloth and Claude 4.5 Opus High Reasoning Dataset:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning"&gt;https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This has created a reasoning/instruct hybrid.&lt;br /&gt; Details at the repo, along with credits and links.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ADDED:&lt;/strong&gt;&lt;br /&gt; - 1 example generation at repo&lt;br /&gt; - special instructions on how to control &amp;quot;instruct&amp;quot; or &amp;quot;thinking&amp;quot; modes.&lt;/p&gt; &lt;p&gt;GGUF quants are now available.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt;&lt;br /&gt; Working on a Heretic (&amp;quot;uncensored&amp;quot;) tune of this next.&lt;/p&gt; &lt;p&gt;DavidAU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T03:41:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0zst6</id>
    <title>Upstage Solar-Open-100B Public Validation</title>
    <updated>2026-01-01T08:52:25+00:00</updated>
    <author>
      <name>/u/PerPartes</name>
      <uri>https://old.reddit.com/user/PerPartes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"&gt; &lt;img alt="Upstage Solar-Open-100B Public Validation" src="https://preview.redd.it/w789uyo0cpag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b12cc1f1a7783b1d9a40f9851206fbcdbdbf782" title="Upstage Solar-Open-100B Public Validation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Official company counterstrike to the claim that Solar 100B Open is just finetuned GLM-Air-4.5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerPartes"&gt; /u/PerPartes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w789uyo0cpag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T08:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
