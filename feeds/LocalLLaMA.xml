<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-31T01:34:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ojvgsx</id>
    <title>manifestai releases Brumby-14B-Base weights, claims "attention free" and inference "hundreds of time faster" for long context</title>
    <updated>2025-10-30T09:55:08+00:00</updated>
    <author>
      <name>/u/ArcadesOfAntiquity</name>
      <uri>https://old.reddit.com/user/ArcadesOfAntiquity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojvgsx/manifestai_releases_brumby14bbase_weights_claims/"&gt; &lt;img alt="manifestai releases Brumby-14B-Base weights, claims &amp;quot;attention free&amp;quot; and inference &amp;quot;hundreds of time faster&amp;quot; for long context" src="https://external-preview.redd.it/u286g9i_4XNK4XbToyvDxLSuC8KvkBPXz6zTs2VwH_4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=501921f84aaec52c66e095e5820853a5d045617d" title="manifestai releases Brumby-14B-Base weights, claims &amp;quot;attention free&amp;quot; and inference &amp;quot;hundreds of time faster&amp;quot; for long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;also check out their blog page for the release:&lt;/p&gt; &lt;p&gt;&lt;a href="https://manifestai.com/articles/release-brumby-14b/"&gt;https://manifestai.com/articles/release-brumby-14b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I only skimmed the hf card and blog, and one thing that struck me is they seem to initizialize their weights for their so called &amp;quot;power retention&amp;quot; model architecture, using the weights of Qwen3-14B, and they call the technique &amp;quot;retraining&amp;quot;...&lt;/p&gt; &lt;p&gt;I guess this makes me a bit skeptical as we might just refer to it as &amp;quot;fine tuning&amp;quot;. And makes me worry this is just a way to publish something AI-related so they can get wrap their mouths around that VC money firehose.&lt;/p&gt; &lt;p&gt;But, they said they spent $4000 to &amp;quot;retrain&amp;quot; it, so maybe...?&lt;/p&gt; &lt;p&gt;Anyway, the real promising aspect here is the claim in the &amp;quot;Coming soon&amp;quot; section at the bottom of the hugging face page:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Fast long-context inference: Our fastest power retention inference kernels are hundreds of times faster than equivalent attention kernels on long contexts. We will update the architecture to incorporate these fast kernels.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;If this turns out to be even 50% true that would be amazing. Suddenly Mac would be totally legitimate for serious industrial scale inference. Which makes me think it's too good to be true...&lt;/p&gt; &lt;p&gt;Time will tell&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArcadesOfAntiquity"&gt; /u/ArcadesOfAntiquity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/manifestai/Brumby-14B-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojvgsx/manifestai_releases_brumby14bbase_weights_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojvgsx/manifestai_releases_brumby14bbase_weights_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T09:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok4d2n</id>
    <title>ü¶ôüí• Building llama.cpp with Vulkan backend on Android (Termux ARM64)</title>
    <updated>2025-10-30T16:27:22+00:00</updated>
    <author>
      <name>/u/Brahmadeo</name>
      <uri>https://old.reddit.com/user/Brahmadeo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Pre-script(PS)- I wrote/copied this using AI. I am not a writer, yet. Everything was done natively on Snapdragon 7 Plus Gen 3/12 GB RAM Phone using Termux.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;AI- Since there‚Äôs almost zero info out there on building both &lt;code&gt;glslc(Arm64)&lt;/code&gt; and &lt;code&gt;llama.cpp (Vulkan)&lt;/code&gt; natively on Android, here‚Äôs the working procedure.&lt;/p&gt; &lt;p&gt;üß© &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You‚Äôll need:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash pkg install git cmake ninja clang python vulkan-tools &lt;/code&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;üß† Tip: Ensure your Termux has Vulkan-capable drivers. You can verify with:&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;code&gt;bash vulkaninfo | head &lt;/code&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;If it prints valid info (not &lt;code&gt;segfault&lt;/code&gt;), you‚Äôre good. (H- Vulkan is pretty much on every phone made post 2016, I think.)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üì¶ &lt;strong&gt;Step 1 ‚Äî Clone and build Shaderc (for glslc)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash cd ~ git clone --recursive https://github.com/google/shaderc cd shaderc mkdir build &amp;amp;&amp;amp; cd build cmake .. -G Ninja \ -DCMAKE_BUILD_TYPE=Release \ -DSHADERC_SKIP_TESTS=ON ninja glslc_exe &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This builds the GLSL compiler (&lt;code&gt;glslc_exe&lt;/code&gt;), needed by Vulkan.&lt;/p&gt; &lt;p&gt;üëâ The working binary will be here:&lt;/p&gt; &lt;p&gt;&lt;code&gt;~/shaderc/build/glslc/glslc&lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Step 2 ‚Äî Clone and prepare llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;H- You already know how.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Now comes the critical step.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üöÄ &lt;strong&gt;Step 3 ‚Äî Build llama.cpp with Vulkan backend&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The key flag is &lt;code&gt;-DVulkan_GLSLC_EXECUTABLE&lt;/code&gt;, which must point to the actual binary (&lt;code&gt;glslc&lt;/code&gt;), not just the directory.&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash cmake .. -G Ninja \ -DGGML_VULKAN=ON \ -DVulkan_GLSLC_EXECUTABLE=/data/data/com.termux/files/home/shaderc/build/glslc/glslc \ -DCMAKE_BUILD_TYPE=Release ninja &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üß† &lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;glslc_exe&lt;/code&gt; builds fine on Termux without cross-compiling.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; detects Vulkan properly if vulkaninfo works.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You can confirm Vulkan backend built by checking:&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;bash ./bin/llama-cli --help | grep vulkan &lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expect a longer build due to shader compilation steps. (Human- It's quick, with &lt;code&gt;ninja -j$(nproc)&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;üß© &lt;strong&gt;Tested on&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Device:&lt;/strong&gt; Snapdragon 7+ Gen 3&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Termux:&lt;/strong&gt; 0.118 (Android 15)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Compiler:&lt;/strong&gt; Clang 17&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Vulkan:&lt;/strong&gt; Working via system drivers (H- kinda)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;H- After this, &lt;code&gt;llama.cpp&lt;/code&gt; executables i.e. &lt;code&gt;llama-cli/server&lt;/code&gt; etc were running but phone wouldn't expose GPU driver, and &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; did nothing (poor human logic). So a hacky workaround and possible rebuild below-&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;How I Ran &lt;code&gt;llama.cpp&lt;/code&gt; on Vulkan with Adreno GPU in Termux on Android (Snapdragon 7+ Gen 3)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/termux"&gt;r/termux&lt;/a&gt; / &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; / &lt;a href="/r/MachineLearning"&gt;r/MachineLearning&lt;/a&gt; ‚Äî after days (H- hours) of wrestling, I got llama.cpp running with Vulkan backend on my phone in Termux. It detects the Adreno 732 GPU and offloads layers, but beware: it's unstable (OOM, DeviceLostError, gibberish output). OpenCL works better for stable inference, but Vulkan is a fun hack.&lt;/p&gt; &lt;p&gt;This is a step-by-step guide for posterity. Tested on Android 14, Termux from F-Droid. Your mileage may vary on other devices ‚Äî Snapdragon with Adreno GPU required.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Termux installed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Storage access: &lt;code&gt;termux-setup-storage&lt;/code&gt; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Basic packages: &lt;code&gt;pkg install clang cmake ninja git vulkan-headers vulkan-tools vulkan-loader&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;~~ &lt;strong&gt;Step 1: Build shaderc and glslc (Vulkan Shader Compiler)&lt;/strong&gt; Vulkan needs glslc for shaders. Build from source:~~&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 2: Clone and Configure &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash cd ~ git clone https://github.com/ggerganov/llama.cpp cd llama.cpp mkdir build_vulkan &amp;amp;&amp;amp; cd build_vulkan cmake .. -G Ninja -DGGML_VULKAN=ON -DVulkan_GLSLC_EXECUTABLE=$HOME/shaderc/build/glslc/glslc &lt;/code&gt;&lt;/p&gt; &lt;p&gt;If &lt;code&gt;CMake&lt;/code&gt; complains about &lt;code&gt;libvulkan.so&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Remove broken symlink: &lt;code&gt;rm $PREFIX/lib/libvulkan.so&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Copy real loader: &lt;code&gt;cp /system/lib64/libvulkan.so $PREFIX/lib/libvulkan.so&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Clear cache: &lt;code&gt;rm -rf CMakeCache.txt CMakeFiles/&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Re-run &lt;code&gt;CMake&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 3: Build&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash ninja -j$(nproc) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Binary is at &lt;code&gt;bin/llama-cli&lt;/code&gt;&lt;/p&gt; &lt;p&gt;**Step 4: Create &lt;code&gt;ICD JSON&lt;/code&gt; for Adreno Vulkan loader needs this to find the driver.&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash cat &amp;gt; $HOME/adreno.json &amp;lt;&amp;lt; 'EOF' { &amp;quot;file_format_version&amp;quot;: &amp;quot;1.0.0&amp;quot;, &amp;quot;ICD&amp;quot;: { &amp;quot;library_path&amp;quot;: &amp;quot;/vendor/lib64/hw/vulkan.adreno.so&amp;quot;, &amp;quot;api_version&amp;quot;: &amp;quot;1.3.268&amp;quot; } } EOF &lt;/code&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Hint - find your own &lt;code&gt;api_version&lt;/code&gt; etc to put inside .json. It is somewhere in root and I also used vulkanCapsViewer app on Android.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Step 5: Set Environment Variables&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash export VK_ICD_FILENAMES=$HOME/adreno.json export LD_LIBRARY_PATH=/vendor/lib64/hw:$PREFIX/lib:$LD_LIBRARY_PATH &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Add to &lt;code&gt;~/.bashrc&lt;/code&gt; for persistence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 6: Test Detection&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash bin/llama-cli --version &lt;/code&gt;&lt;/p&gt; &lt;p&gt;You should see: &lt;code&gt; ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Adreno (TM) 732 (Qualcomm Technologies Inc. Adreno Vulkan Driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: none &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Download a small GGUF model (e.g., Phi-3 Mini Q4_K_M from HuggingFace). &lt;code&gt;bash bin/llama-cli \ -m phi-3-mini-4k-instruct-q4_K_M.gguf \ -p &amp;quot;Test prompt:&amp;quot; \ -n 128 \ --n-gpu-layers 20 \ --color &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Offloads layers to GPU. But often OOM (reduce --n-gpu-layers), DeviceLostError, or gibberish. Q4_0/Q4_K may fail shaders; Q8_0 is safer but larger.&lt;/p&gt; &lt;p&gt;&lt;em&gt;PS- I tested multiple models. &lt;code&gt;OpenCL&lt;/code&gt; crashes &lt;code&gt;Termux&lt;/code&gt; with exit code -9 on my phone if total GPU Load crosses ~3 GB. Something like that is happening with &lt;code&gt;Vulkan&lt;/code&gt; build as well. All models that run fine on CPU or CPU+OpenCL generate gibberish. I'll post samples below if I get the time, however those of you who want to experiment yourselves can do so, now the build instructions have been shared with you. If some of you are able to fix inference please post a comment with &lt;code&gt;llama-cli/server&lt;/code&gt; options.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brahmadeo"&gt; /u/Brahmadeo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok4d2n/building_llamacpp_with_vulkan_backend_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok4d2n/building_llamacpp_with_vulkan_backend_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok4d2n/building_llamacpp_with_vulkan_backend_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T16:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok1tkh</id>
    <title>Users of REAP Pruned models, So far how's your experience?</title>
    <updated>2025-10-30T14:51:45+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been 1-2 week(s), please share your experience on those. Speed-wise fine as I saw some stats from few threads. Quality wise? And Stuffs like Tool calling &amp;amp; etc.,??&lt;/p&gt; &lt;p&gt;So far I see Pruned models of Qwen3-Coder-480B, GLM-4.5-Air, GLM-4.6, Qwen3-Coder-30B, GPT-OSS-20B, GPT-OSS-120B, Qwen3-30B-A3B, Qwen3-30B-A3B-Instruct on &lt;a href="https://huggingface.co/models?library=safetensors&amp;amp;sort=created&amp;amp;search=REAP"&gt;HuggingFace&lt;/a&gt;(Filtered HF URL of REAP Pruned models).&lt;/p&gt; &lt;p&gt;Personally I would try (25% Pruned versions of) GPT-OSS-20B &amp;amp; Qwen3-30B models on my 8GB VRAM(and 32GB VRAM).&lt;/p&gt; &lt;p&gt;REAP Prune Experts, please consider these models if possible. Thanks&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI21-Jamba-Mini-1.7&lt;/li&gt; &lt;li&gt;GroveMoE-Inst&lt;/li&gt; &lt;li&gt;FlexOlmo-7x7B-1T&lt;/li&gt; &lt;li&gt;Phi-3.5-MoE-instruct&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For others, here some threads to start.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1octe2s/pruned_moe_reap_quants_for_testing/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1octe2s/pruned_moe_reap_quants_for_testing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ogz0b7/oh_my_reapness_qwen3coder30ba3binstruct_pruned/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ogz0b7/oh_my_reapness_qwen3coder30ba3binstruct_pruned/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok1tkh/users_of_reap_pruned_models_so_far_hows_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok1tkh/users_of_reap_pruned_models_so_far_hows_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok1tkh/users_of_reap_pruned_models_so_far_hows_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok59j7</id>
    <title>Chrono Edit Released</title>
    <updated>2025-10-30T17:00:43+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;&lt;strong&gt;ChronoEdit-14B&lt;/strong&gt; enables physics-aware image editing and action-conditioned world simulation through temporal reasoning. It distills priors from a 14B-parameter pretrained video generative model and separates inference into (i) a &lt;strong&gt;video reasoning stage&lt;/strong&gt; for latent trajectory denoising, and (ii) an &lt;strong&gt;in-context editing stage&lt;/strong&gt; for pruning trajectory tokens. ChronoEdit-14B was developed by NVIDIA as part of the &lt;strong&gt;ChronoEdit&lt;/strong&gt; family of multimodal foundation models. This model is ready for commercial use.&amp;quot;&lt;br /&gt; From There Repo&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers"&gt;https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok59j7/chrono_edit_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok59j7/chrono_edit_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok59j7/chrono_edit_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T17:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1okehd9</id>
    <title>What are the best Open Source OCR models currently?</title>
    <updated>2025-10-30T23:01:25+00:00</updated>
    <author>
      <name>/u/WittyWithoutWorry</name>
      <uri>https://old.reddit.com/user/WittyWithoutWorry</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(the title says it all)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WittyWithoutWorry"&gt; /u/WittyWithoutWorry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okehd9/what_are_the_best_open_source_ocr_models_currently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okehd9/what_are_the_best_open_source_ocr_models_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okehd9/what_are_the_best_open_source_ocr_models_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T23:01:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1okbxki</id>
    <title>Choose Your Own Adventure App (Ollama compatible &amp; Open Source)</title>
    <updated>2025-10-30T21:14:51+00:00</updated>
    <author>
      <name>/u/thedelusionist</name>
      <uri>https://old.reddit.com/user/thedelusionist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okbxki/choose_your_own_adventure_app_ollama_compatible/"&gt; &lt;img alt="Choose Your Own Adventure App (Ollama compatible &amp;amp; Open Source)" src="https://preview.redd.it/35xxswhlcbyf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=365a13b33df250d636388ff2339b1b3aeb30d79d" title="Choose Your Own Adventure App (Ollama compatible &amp;amp; Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to play DnD and love the choose you own adventure genre, so I made a mac app that lets you do it with custom local models through Ollama and if you don't have the compute, you can use a Groq API key.&lt;/p&gt; &lt;p&gt;Everything is local (except for Groq API calls), and free. Just fun little app I made for myself that I figured I would share. Enjoy!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gmfennema/CYOLLMA"&gt;Github Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thedelusionist"&gt; /u/thedelusionist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35xxswhlcbyf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okbxki/choose_your_own_adventure_app_ollama_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okbxki/choose_your_own_adventure_app_ollama_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T21:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1okg0gm</id>
    <title>While Qwen3-vl has very good OCR/image caption abilities, it still doesn't seem to generate accurate coordinates nor bounding boxes of objects in the screen. I just take a screenshot and send as-is and its accuracy is off. Tried resizing, no dice neither. Anyone else have this problem?</title>
    <updated>2025-10-31T00:07:41+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okg0gm/while_qwen3vl_has_very_good_ocrimage_caption/"&gt; &lt;img alt="While Qwen3-vl has very good OCR/image caption abilities, it still doesn't seem to generate accurate coordinates nor bounding boxes of objects in the screen. I just take a screenshot and send as-is and its accuracy is off. Tried resizing, no dice neither. Anyone else have this problem?" src="https://external-preview.redd.it/YmZrMzA3dmQ5Y3lmMX0gATsSAWoFF6TAKNuD6ORWkn_7ZV7_9K96KQVqnCU1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fa5cc99424ca27c3fdd6eb58fd0d650411325fd" title="While Qwen3-vl has very good OCR/image caption abilities, it still doesn't seem to generate accurate coordinates nor bounding boxes of objects in the screen. I just take a screenshot and send as-is and its accuracy is off. Tried resizing, no dice neither. Anyone else have this problem?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running this on Ollama, &lt;code&gt;qwen3-vl-30b-a3b-instruct-q8_0&lt;/code&gt; and the thinking variant as well. Neither seem to be working adequately in the coordinates scene, despite being able to accurately describe the region where the object in question is located. &lt;/p&gt; &lt;p&gt;I don't know if the problem was &lt;code&gt;pyautogui.screenshot()&lt;/code&gt; taking the image and sending it as a &lt;code&gt;.png&lt;/code&gt; image as-is or if I need to include an offset in the returned output or scale the image prior to sending it to the model. &lt;/p&gt; &lt;p&gt;I tried different sampling parameters, no luck there. Doesn't seem to make a difference. &lt;code&gt;chat()&lt;/code&gt; vs &lt;code&gt;generate&lt;/code&gt; are not working neither, it seems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wlb2h7vd9cyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okg0gm/while_qwen3vl_has_very_good_ocrimage_caption/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okg0gm/while_qwen3vl_has_very_good_ocrimage_caption/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T00:07:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1okejek</id>
    <title>How to Use Local Models as Security Monitors (using Change Detection)</title>
    <updated>2025-10-30T23:03:47+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okejek/how_to_use_local_models_as_security_monitors/"&gt; &lt;img alt="How to Use Local Models as Security Monitors (using Change Detection)" src="https://external-preview.redd.it/bXB0NG4yZXh0YnlmMT2J2kYzLJNIJRLgvAJtIGfYFD-rBBzPhvsMkGzxXpRE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e616fb9750999dc06a5dec6de8ff7d4c5ee27050" title="How to Use Local Models as Security Monitors (using Change Detection)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: The &lt;strong&gt;#1 feedback&lt;/strong&gt; I got from you guys was about the &lt;strong&gt;inefficiency&lt;/strong&gt; of leaving LLMs watching over and over, so now there's &lt;strong&gt;Change Detection!&lt;/strong&gt; üéâ It doesn't call a model &lt;strong&gt;unless something significant changes&lt;/strong&gt;, saving resources and powering up your small models! &lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !!&lt;/p&gt; &lt;p&gt;I added this to Observer because of all of the feedback about the &lt;strong&gt;inefficiency of using LLMs&lt;/strong&gt; to watch something, the cool part is that they are &lt;strong&gt;small and local&lt;/strong&gt;, so no API costs whatsoever! &lt;/p&gt; &lt;p&gt;So now you can have agent loops of &amp;lt;30s &lt;strong&gt;without spamming model calls&lt;/strong&gt; to your Ollama/vLLM/llama.cpp server, and just call them when it matters. &lt;/p&gt; &lt;p&gt;Here's the nerdy details for anyone that's interested, It has &lt;strong&gt;three modes&lt;/strong&gt; &amp;quot;Camera Feed&amp;quot;, &amp;quot;Screen UI&amp;quot; or &amp;quot;Hybrid&amp;quot;. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;For cameras (noisy inputs) it uses&lt;/strong&gt; &lt;a href="https://github.com/Tom64b/dHash"&gt;&lt;strong&gt;dhash&lt;/strong&gt;&lt;/a&gt;, which is a perceptual hashing algorithm. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;For UIs it uses Pixel Difference&lt;/strong&gt;, which is literally just how much percent the pixels are the same in greyscale. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid does both and then makes an &amp;quot;educated guess&amp;quot;&lt;/strong&gt;, if dhash~100% it assumes it's a UI and it uses pixel difference. (It's the default setting, but It's better to set manually)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you have any other suggestions for using lightweight Computer Vision as change detection please let me know!&lt;/p&gt; &lt;p&gt;This project is Open Source and can be self-hosted: &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;You can try it out without downloading anything, on: &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I'll hang out here in the comments if you have suggestions/questions c: &lt;/p&gt; &lt;p&gt;Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/of2xdsextbyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okejek/how_to_use_local_models_as_security_monitors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okejek/how_to_use_local_models_as_security_monitors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T23:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok6w8r</id>
    <title>I Bought the Intel ARC B50 to use with LM Studio</title>
    <updated>2025-10-30T18:01:21+00:00</updated>
    <author>
      <name>/u/tony10000</name>
      <uri>https://old.reddit.com/user/tony10000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I checked my email, and a message was waiting for me from B&amp;amp;H Photo: ‚ÄúIntel Arc Pro B50 Workstation SFF Graphics Card is now in stock!‚Äù&lt;/p&gt; &lt;p&gt;The moment of decision had arrived.&lt;/p&gt; &lt;p&gt;Since I got into running LLMs on my Ryzen 5700 several months ago, I had been exploring all sorts of options to improve my rig. The first step was to upgrade to 64GB of RAM (the two 32 GB RAM modules proved to be flaky, so I am in the process of returning them).&lt;/p&gt; &lt;p&gt;While 64GB allowed me to run larger models, the speeds were not that impressive.&lt;/p&gt; &lt;p&gt;For example, with DeepSeek R1/Qwen 8B and a 4K context window in LM Studio, I get 6‚Äì7 tokens per second (tps). Not painfully slow, but not very fast either.&lt;/p&gt; &lt;p&gt;After sitting and waiting for tokens to flow, at some point I said, ‚ÄúI feel the need for speed!‚Äù&lt;/p&gt; &lt;p&gt;Enter the Intel ARC B50. After looking at all of the available gaming graphics cards, I found them to be too power hungry, too expensive, too loud, and some of them generate enough heat to make a room comfy on a winter day.&lt;/p&gt; &lt;p&gt;When I finally got the alert that it was back in stock, it did not take me long to pull the trigger. It had been unavailable for weeks, was heavily allocated, and I knew it would sell out fast.&lt;/p&gt; &lt;p&gt;My needs were simple: better speed and enough VRAM to hold the models that I use daily without having to overhaul my system that lives in a mini tower case with a puny 400-watt power supply.&lt;/p&gt; &lt;p&gt;The B50 checked all the boxes. It has 16GB of GDDR6 memory, a 128-bit interface, and 224 GB/s of bandwidth.&lt;/p&gt; &lt;p&gt;Its Xe¬≤ architecture uses XMX (Intel Xe Matrix eXtensions) engines that accelerate AI inference far beyond what my CPU can deliver.&lt;/p&gt; &lt;p&gt;With a 70-watt thermal design power and no external power connectors, the card fits easily into compact systems like mine. That mix of performance and ease of installation made it completely irresistible.&lt;/p&gt; &lt;p&gt;And the price was only around $350, exceptional for a 16GB card.&lt;/p&gt; &lt;p&gt;During my first week of testing, the B50 outperformed my 5700G setup by 2 to 4 times in inference throughput. For example, DeepSeek R1/Qwen 8B in LM Studio using the Vulkan driver delivers 32‚Äì33 tps, over 4X the CPU-only speed.&lt;/p&gt; &lt;p&gt;Plus, most of the 64GB system memory is now freed for other tasks when LM Studio is generating text.&lt;/p&gt; &lt;p&gt;When I first considered the Intel B50, I was initially skeptical. Intel‚Äôs GPU division has only recently re-entered the workstation space, and driver support is a valid concern.&lt;/p&gt; &lt;p&gt;AMD and especially Nvidia have much more mature and well-supported drivers, and the latter company‚Äôs architecture is considered to be the industry standard.&lt;/p&gt; &lt;p&gt;But the Intel drivers have proven to be solid, and the company seems to be committed to improving performance with every revision. For someone like me who values efficiency and longevity over pure speed, that kind of stability and support are reassuring.&lt;/p&gt; &lt;p&gt;I think that my decision to buy the B50 was the right one for my workflow.&lt;/p&gt; &lt;p&gt;The Intel Arc Pro B50 doesn‚Äôt just power my machine. It accelerates the pace of my ideas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tony10000"&gt; /u/tony10000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok6w8r/i_bought_the_intel_arc_b50_to_use_with_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok6w8r/i_bought_the_intel_arc_b50_to_use_with_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok6w8r/i_bought_the_intel_arc_b50_to_use_with_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T18:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0voi</id>
    <title>Introducing Hephaestus: AI workflows that build themselves as agents discover what needs to be done</title>
    <updated>2025-10-30T14:14:41+00:00</updated>
    <author>
      <name>/u/Standard_Excuse7988</name>
      <uri>https://old.reddit.com/user/Standard_Excuse7988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0voi/introducing_hephaestus_ai_workflows_that_build/"&gt; &lt;img alt="Introducing Hephaestus: AI workflows that build themselves as agents discover what needs to be done" src="https://external-preview.redd.it/aHpncHF4ZXpiOXlmMZc87bODxInUab1QdzAVIKt_p_AEHL5YkJEhQTcdw4CD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67b99040969c4f9ad4800522558e3abdd06aa68b" title="Introducing Hephaestus: AI workflows that build themselves as agents discover what needs to be done" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I've been working on Hephaestus - an open-source framework that changes how we think about AI agent workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Most agentic frameworks make you define every step upfront. But complex tasks don't work like that - you discover what needs to be done as you go.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; Semi-structured workflows. You define &lt;em&gt;phases&lt;/em&gt; - the logical steps needed to solve a problem (like &amp;quot;Reconnaissance ‚Üí Investigation ‚Üí Validation&amp;quot; for pentesting). Then agents dynamically create tasks across these phases based on what they discover.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; During a pentest, a validation agent finds an IDOR vulnerability that exposes API keys. Instead of being stuck in validation, it spawns a new reconnaissance task: &amp;quot;Enumerate internal APIs using these keys.&amp;quot; Another agent picks it up, discovers admin endpoints, chains discoveries together, and the workflow branches naturally.&lt;/p&gt; &lt;p&gt;Agents share discoveries through RAG-powered memory and coordinate via a Kanban board. A Guardian agent continuously tracks each agent's behavior and trajectory, steering them in real-time to stay focused on their tasks and prevent drift.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ido-Levi/Hephaestus"&gt;https://github.com/Ido-Levi/Hephaestus&lt;/a&gt; üìö &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://ido-levi.github.io/Hephaestus/"&gt;https://ido-levi.github.io/Hephaestus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fair warning:&lt;/strong&gt; This is a brand new framework I built alone, so expect rough edges and issues. The repo is a bit of a mess right now. If you find any problems, please report them - feedback is very welcome! And if you want to contribute, I'll be more than happy to review it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Excuse7988"&gt; /u/Standard_Excuse7988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uwogrxezb9yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0voi/introducing_hephaestus_ai_workflows_that_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0voi/introducing_hephaestus_ai_workflows_that_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:14:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1okf629</id>
    <title>Built a fully offline voice assistant with Mistral + RAG - runs on consumer hardware (GTX 1650)</title>
    <updated>2025-10-30T23:30:39+00:00</updated>
    <author>
      <name>/u/curvebass</name>
      <uri>https://old.reddit.com/user/curvebass</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okf629/built_a_fully_offline_voice_assistant_with/"&gt; &lt;img alt="Built a fully offline voice assistant with Mistral + RAG - runs on consumer hardware (GTX 1650)" src="https://external-preview.redd.it/nJc0I_Gh8teCbZSiQQQqVTdlqzmo0QdtnrSSL2vtRho.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b2bdf1dfc71f142f18e494d0d26031092f44255" title="Built a fully offline voice assistant with Mistral + RAG - runs on consumer hardware (GTX 1650)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1okf629/video/wkf47jyg0cyf1/player"&gt;please suggest a better prompt to feed into the LLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, Been lurking here for a while and finally have something to share. &lt;/p&gt; &lt;p&gt;Built Solus - a completely offline voice assistant that runs locally with no cloud dependency. &lt;/p&gt; &lt;p&gt;**What it does:**&lt;br /&gt; - Real-time voice conversations using Mistral LLM via Ollama&lt;br /&gt; - Context-aware responses with RAG (text based)&lt;br /&gt; - Continuous conversation memory - Local STT (Whisper) and TTS (Piper)&lt;br /&gt; - Simple web UI with audio visualization&lt;/p&gt; &lt;p&gt;**Tech stack:**&lt;br /&gt; - Whisper (openai-whisper) for speech recognition&lt;br /&gt; - Mistral 7B via Ollama for LLM inference&lt;br /&gt; - Piper TTS for voice synthesis&lt;br /&gt; - Python + Node.js backend&lt;br /&gt; - Single HTML file frontend (no build process)&lt;/p&gt; &lt;p&gt;**Performance on GTX 1650 + Ryzen 5 5600H:**&lt;br /&gt; - Whisper STT: ~2s (up to 65% CPU&lt;br /&gt; - offloaded to CPU to preserve GPU)&lt;br /&gt; - Mistral inference: ~6-8s (100% GPU utilization, 4GB VRAM)&lt;br /&gt; - Piper TTS: ~1s (variable CPU) - Total latency: ~10s request-to-response cycle&lt;/p&gt; &lt;p&gt;With Mistral using all 4GB VRAM, keeping Whisper on CPU was necessary. Turns out this split actually optimizes overall latency anyway.&lt;/p&gt; &lt;p&gt;**GitHub:** &lt;a href="https://github.com/AadityaSharma01/solus.AI"&gt;https://github.com/AadityaSharma01/solus.AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Running on: Windows | GTX 1650 4GB | Ryzen 5 5600H | 16GB RAM&lt;/p&gt; &lt;p&gt;please help me improve the prompt for better replies from the LLM I'm experimenting with different prompts&lt;/p&gt; &lt;p&gt;Thanks you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curvebass"&gt; /u/curvebass &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okf629/built_a_fully_offline_voice_assistant_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okf629/built_a_fully_offline_voice_assistant_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okf629/built_a_fully_offline_voice_assistant_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T23:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1okguct</id>
    <title>Another dim of scaling? ByteDance drops ‚ÄúOuro‚Äù: 1.4B ‚âà 4B, 2.6B ‚âà/Ôºû 8B</title>
    <updated>2025-10-31T00:45:55+00:00</updated>
    <author>
      <name>/u/RunTop7329</name>
      <uri>https://old.reddit.com/user/RunTop7329</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"&gt; &lt;img alt="Another dim of scaling? ByteDance drops ‚ÄúOuro‚Äù: 1.4B ‚âà 4B, 2.6B ‚âà/Ôºû 8B" src="https://preview.redd.it/5nubsersacyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22934e9d6b42924c165caa81b6e7db406e1bd999" title="Another dim of scaling? ByteDance drops ‚ÄúOuro‚Äù: 1.4B ‚âà 4B, 2.6B ‚âà/Ôºû 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;recurrent depth with shared weights + early-exit gates; trained to 7.7T tokens.&lt;/li&gt; &lt;li&gt;2.6B model ‚â• 8B baselines on reasoning (e.g., MMLU-Pro 55.73, BBH 80.46, MATH500 90.85); 1.4B ‚âà 4B.&lt;/li&gt; &lt;li&gt;Gains credited to better reasoning/knowledge manipulation, not more memorized facts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I guess it is more friendly to individual home users. The logic goes the opposite of MoE. Basically, activated parameters &amp;gt; 100%. Correct me if wrong.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.25741"&gt;Scaling Latent Reasoning via Looped Language Models&lt;/a&gt;, &lt;a href="https://ouro-llm.github.io/"&gt;https://ouro-llm.github.io/&lt;/a&gt;, &lt;a href="https://x.com/tianyu_zh/status/1983784440829522364"&gt;https://x.com/tianyu_zh/status/1983784440829522364&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RunTop7329"&gt; /u/RunTop7329 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nubsersacyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T00:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojqvwe</id>
    <title>Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source</title>
    <updated>2025-10-30T04:54:59+00:00</updated>
    <author>
      <name>/u/Shockbum</name>
      <uri>https://old.reddit.com/user/Shockbum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"&gt; &lt;img alt="Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source" src="https://external-preview.redd.it/YW51aGVlN3ZpNnlmMbXr-Angp87qmunczlw3KeeJxKKBK56n_5S01mafzhX6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac34d32c78cef1f79f10ce908455ea3f27985eef" title="Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 12 hours working on a song, and without any prior notice, I can no longer download it as a .wav file. I‚Äôll have to find other ways to recover the song. I‚Äôve been a South American subscriber for months, and I trust North American companies less and less because of these anti-consumer practices. If I could give $10 a month to an open-source developer working on AI music generation, I‚Äôd gladly do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shockbum"&gt; /u/Shockbum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r40nze7vi6yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T04:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok80pp</id>
    <title>mradermacher published the entire qwen3-vl series and You can now run it in Jan; just download the latest version of llama.cpp and you're good to go.</title>
    <updated>2025-10-30T18:43:23+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok80pp/mradermacher_published_the_entire_qwen3vl_series/"&gt; &lt;img alt="mradermacher published the entire qwen3-vl series and You can now run it in Jan; just download the latest version of llama.cpp and you're good to go." src="https://b.thumbs.redditmedia.com/CoQXv-WWBe9urpTWr3EXJZWX8jRA57HfssDQO-d6Wnc.jpg" title="mradermacher published the entire qwen3-vl series and You can now run it in Jan; just download the latest version of llama.cpp and you're good to go." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Profile with all models qwen3-vl series : &lt;a href="https://huggingface.co/mradermacher"&gt;https://huggingface.co/mradermacher&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0hpbr15unayf1.png?width=968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7dfbd6deab21e5a1b70e94ec479fe4ac284e4beb"&gt;https://preview.redd.it/0hpbr15unayf1.png?width=968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7dfbd6deab21e5a1b70e94ec479fe4ac284e4beb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok80pp/mradermacher_published_the_entire_qwen3vl_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok80pp/mradermacher_published_the_entire_qwen3vl_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok80pp/mradermacher_published_the_entire_qwen3vl_series/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T18:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok5fqf</id>
    <title>Qwen3-VL-32B Q8 speeds in llama.cpp vs vLLM FP8 on a RTX PRO 6000</title>
    <updated>2025-10-30T17:07:05+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok5fqf/qwen3vl32b_q8_speeds_in_llamacpp_vs_vllm_fp8_on_a/"&gt; &lt;img alt="Qwen3-VL-32B Q8 speeds in llama.cpp vs vLLM FP8 on a RTX PRO 6000" src="https://b.thumbs.redditmedia.com/rbQvQCrnwolmYiViCuyGEd3DxU57TDojNyP4GBKuFTY.jpg" title="Qwen3-VL-32B Q8 speeds in llama.cpp vs vLLM FP8 on a RTX PRO 6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Support for Qwen3-VL has just been merged to llama.cpp, thanks to all the contributors and the qwen team!&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16780"&gt;https://github.com/ggml-org/llama.cpp/pull/16780&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The speed for the Q8 gguf's is actually faster* in llama.cpp vs the FP8 version in vLLM, and it works pretty well. In particular the 32B model seems to be an improvement over the old 32B even only for the text gen outputs. &lt;/p&gt; &lt;p&gt;Both tests done on a RTX PRO 6000.&lt;/p&gt; &lt;p&gt;Llama.cpp Q8:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zh4606w24ayf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=897326f3c93a8c903cf0b774b249137adba18bde"&gt;https://preview.redd.it/zh4606w24ayf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=897326f3c93a8c903cf0b774b249137adba18bde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM FP8:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ymls02k44ayf1.png?width=1568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d708a687593811d9c2828382a1ef13af259bb8b"&gt;https://preview.redd.it/ymls02k44ayf1.png?width=1568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d708a687593811d9c2828382a1ef13af259bb8b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, openwebui shows the average t/s for the response, so total pp+tg averaged (ignore the $ amount, that's just a function of owui). &lt;/p&gt; &lt;p&gt;*In a single request&lt;br /&gt; *With limited context&lt;br /&gt; *In a short query&lt;/p&gt; &lt;p&gt;I used my own quants for the Qwen3-VL-32B-instruct, that I uploaded here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bullerwins/Qwen3-VL-32B-Instruct-GGUF"&gt;https://huggingface.co/bullerwins/Qwen3-VL-32B-Instruct-GGUF&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Usage:&lt;br /&gt; &lt;code&gt;llama-server --model Qwen3-VL-32B-Instruct-Q8_0.gguf --ctx-size 32000 -ngl 99 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 5000 --mmproj Qwen3-VL-32B-Instruct.mmproj&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You need to download the .mmproj too which is found in the repo too.&lt;/p&gt; &lt;p&gt;I've never quantized a VL model in gguf, only with llm-compressor for awq and fp8 so your mileage may vary, wait for the pros (Thireus/Bart/Aes...) quants for imatrix versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok5fqf/qwen3vl32b_q8_speeds_in_llamacpp_vs_vllm_fp8_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok5fqf/qwen3vl32b_q8_speeds_in_llamacpp_vs_vllm_fp8_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok5fqf/qwen3vl32b_q8_speeds_in_llamacpp_vs_vllm_fp8_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T17:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok8oi0</id>
    <title>Qwen3-32B Nemotron GGUFs with extended context</title>
    <updated>2025-10-30T19:08:07+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok8oi0/qwen332b_nemotron_ggufs_with_extended_context/"&gt; &lt;img alt="Qwen3-32B Nemotron GGUFs with extended context" src="https://external-preview.redd.it/yBqusUNjfr-V7_y7nYhSlrXelFBeODD4ShwNRxhNyPs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f4514e47d6186caa6f939f91a47d0f1a5a78fda" title="Qwen3-32B Nemotron GGUFs with extended context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and get them while they're hot!&lt;/p&gt; &lt;p&gt;Fresh new GGUFs for the Nemotron Qwen3 32B version. Since nowadays 40k context is kind of meh, I uploaded all the GGUFs with Yarn RoPE extension factor 4 to extend the context to 160k. Have fun :&amp;gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ilintar/Qwen3-Nemotron-32B-160k-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok8oi0/qwen332b_nemotron_ggufs_with_extended_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok8oi0/qwen332b_nemotron_ggufs_with_extended_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T19:08:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok2kr3</id>
    <title>support for Qwen3 VL has been merged into llama.cpp</title>
    <updated>2025-10-30T15:20:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2kr3/support_for_qwen3_vl_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support for Qwen3 VL has been merged into llama.cpp" src="https://external-preview.redd.it/UOy5aO0wGYsAGLQPGdTQTLob2dx_kI7dhbOgTtyABGg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61edd5c6fafdde538522a801c54ffb4af2835bea" title="support for Qwen3 VL has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16780"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2kr3/support_for_qwen3_vl_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2kr3/support_for_qwen3_vl_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T15:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok5rn2</id>
    <title>Locally hosted Loveable with full stack support and llama.cpp, and more</title>
    <updated>2025-10-30T17:19:29+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok5rn2/locally_hosted_loveable_with_full_stack_support/"&gt; &lt;img alt="Locally hosted Loveable with full stack support and llama.cpp, and more" src="https://b.thumbs.redditmedia.com/Ns9mgOkUfJVshoTnHr9O_NNlscx2zBmgkayW05s7pWs.jpg" title="Locally hosted Loveable with full stack support and llama.cpp, and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I wanted to share my story. This year in February, I came up with some notion (mostly just pissed) that we couldn't use AI models as good as claude locally to design. The fact that they had all this training and design data held behind a wall (which you had to pay for) was super unnatural so I just started learning about AI and wanted to train my own model.&lt;/p&gt; &lt;p&gt;The very first model that I trained, I put it on huggingface and it went trending overnight. It was on the front page right next to DeepSeek etc and people kept asking me who did all that? Was I part of a research group or academic? And I was just like no... just 22 year old with a laptop lol. Ever since then, I used my off hours from my full time job to train models and code software, with the intention of keeping everything open source. (Just angry again that we don't have gpus haha).The future of AI is definitely open source.&lt;/p&gt; &lt;p&gt;Along the way I kept talking to people and realized that AI assisted coding is the future as well, freeing up mental capacity and space to do better things with your time like architecture and proper planning. Technology enabled a lot more people to become builders and I thought that was so cool, until I realized... Not open sourced again. Loveable, Cursor, etc.. Just a system prompt and tools. Why can I not change my own system prompts? Everythings closed source these days. So I built the opposite. My goal is to make coding models that look as good as Claude and a tool to use said coding models.&lt;/p&gt; &lt;p&gt;So I built Tesslate Studio. Its open sourced, Apache 2.0. Bring your own models (llama.cpp, ollama, openrouter, lm studio, Litellm or your own urls), Bring your own agents (you can define the system prompt or tools or add in a new agent with the factory), and bring your own github urls to start with. AI should be open sourced and accessible to everyone. I don't want people changing my system prompts again as well as I would like to choose on my own when I would want to change the prompt for the stuff I'm building.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/TesslateAI/Studio"&gt;https://github.com/TesslateAI/Studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Each project also gets a Kanban board, notes. You can switch the agent whenever you want and try other people's agents if you have it hosted in a multi user environment. Drop any model in. use any agents with whatever tools you define. I am actively developing this and will continue to improve it based on feedback. The open source project will always be 100% free and I'm definitely looking for contributions, suggestions, issues, etc. Would love to work with some talented engineers.&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://docs.tesslate.com"&gt;https://docs.tesslate.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Locally Hosting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can create multiple accounts and share it across your local net&lt;/li&gt; &lt;li&gt;Create agents that you can share across all the account&lt;/li&gt; &lt;li&gt;Users can fork their own agents and add in their own models&lt;/li&gt; &lt;li&gt;Collaboration coming soon!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have it hosted online for (free, Free GPT-5 and Qwen-coder) at &lt;a href="https://tesslate.com"&gt;https://tesslate.com&lt;/a&gt; using cloud credits until they run out on the 12th of November.&lt;/p&gt; &lt;p&gt;Thank You for taking the time to read this, I appreciate it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ok5rn2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok5rn2/locally_hosted_loveable_with_full_stack_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok5rn2/locally_hosted_loveable_with_full_stack_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T17:19:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojzekg</id>
    <title>moonshotai/Kimi-Linear-48B-A3B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-10-30T13:15:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/"&gt; &lt;img alt="moonshotai/Kimi-Linear-48B-A3B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/o39DhNeoqy1hllYVOdco9J5dVQYgSgBGi2OS_-lCbh8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b08e7d9c4f153bd13eb2212c501cdd1bc28bdfa2" title="moonshotai/Kimi-Linear-48B-A3B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including short, long, and reinforcement learning (RL) scaling regimes. At its core is Kimi Delta Attention (KDA)‚Äîa refined version of &lt;a href="https://arxiv.org/abs/2412.06464"&gt;Gated DeltaNet&lt;/a&gt; that introduces a more efficient gating mechanism to optimize the use of finite-state RNN memory.&lt;/p&gt; &lt;p&gt;Kimi Linear achieves superior performance and hardware efficiency, especially for long-context tasks. It reduces the need for large KV caches by up to 75% and boosts decoding throughput by up to $6\times$ for contexts as long as 1M tokens.&lt;/p&gt; &lt;p&gt;We open-source the KDA kernel in &lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/kda"&gt;FLA&lt;/a&gt;, and release two versions model checkpoints trained with 5.7T tokens.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;#Activated Params&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Download Link&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi-Linear-Base&lt;/td&gt; &lt;td align="left"&gt;48B&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;1M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Base"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi-Linear-Instruct&lt;/td&gt; &lt;td align="left"&gt;48B&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;1M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi Delta Attention (KDA):&lt;/strong&gt; A linear attention mechanism that refines the gated delta rule with finegrained gating.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Architecture:&lt;/strong&gt; A 3:1 KDA-to-global MLA ratio reduces memory usage while maintaining or surpassing the quality of full attention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Superior Performance:&lt;/strong&gt; Outperforms full attention in a variety of tasks, including long-context and RL-style benchmarks on 1.4T token training runs with fair comparisons.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Throughput:&lt;/strong&gt; Achieves up to $6\times$ faster decoding and significantly reduces time per output token (TPOT).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T13:15:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oka1pi</id>
    <title>Llama-cpp QWen3-VL + Flux Image-to-Image Locally on Dual GPUs (3090 + 3060Ti)</title>
    <updated>2025-10-30T20:00:54+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oka1pi/llamacpp_qwen3vl_flux_imagetoimage_locally_on/"&gt; &lt;img alt="Llama-cpp QWen3-VL + Flux Image-to-Image Locally on Dual GPUs (3090 + 3060Ti)" src="https://preview.redd.it/c1lxfyn21byf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d841491ac14c9cb1ed37dacfbc96291c6143561" title="Llama-cpp QWen3-VL + Flux Image-to-Image Locally on Dual GPUs (3090 + 3060Ti)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Just wanted to share my setup for a fully local multimodal AI stack ‚Äî combining LLaMA.cpp (Qwen3-VL 32B) for vision + text and Stable Diffusion WebUI Forge (Flux-dev model) for image generation.&lt;/p&gt; &lt;p&gt;This runs entirely offline on my 14900K, RTX 3090, and RTX 3060 Ti, with GPU separation for text vs image workloads. Works for chat, vision tasks, and full image-to-image transformations. There is enough free Vram on the 3090 to run GPT-OSS-120b with cpu-moe at the same time!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-VL-32B-Instruct (quantized Q4_K_M)&lt;/li&gt; &lt;li&gt;GPT-OSS-120b mxfp4&lt;/li&gt; &lt;li&gt;Flux1-dev-bnb-nf4-v2.safetensors (SD Forge)&lt;/li&gt; &lt;li&gt;OpenWebUI&lt;/li&gt; &lt;li&gt;llama.cpp (with CUDA + vision enabled)&lt;/li&gt; &lt;li&gt;Stable Diffusion WebUI Forge (API mode)&lt;/li&gt; &lt;li&gt;i9-14900K&lt;/li&gt; &lt;li&gt;RTX 3090 (for LLM)&lt;/li&gt; &lt;li&gt;RTX 3060 Ti (for Flux)&lt;/li&gt; &lt;li&gt;96GB DDR5 6800&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Workflow will be in a separate post below if enough interest&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1lxfyn21byf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oka1pi/llamacpp_qwen3vl_flux_imagetoimage_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oka1pi/llamacpp_qwen3vl_flux_imagetoimage_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T20:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojz8pz</id>
    <title>Kimi Linear released</title>
    <updated>2025-10-30T13:08:45+00:00</updated>
    <author>
      <name>/u/Badger-Purple</name>
      <uri>https://old.reddit.com/user/Badger-Purple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badger-Purple"&gt; /u/Badger-Purple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojz8pz/kimi_linear_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojz8pz/kimi_linear_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojz8pz/kimi_linear_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T13:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok7hd4</id>
    <title>Faster llama.cpp ROCm performance for AMD RDNA3 (tested on Strix Halo/Ryzen AI Max 395)</title>
    <updated>2025-10-30T18:23:16+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The other day I was doing some &lt;a href="https://github.com/lhl/strix-halo-testing/blob/main/llama-cpp-fix-wmma/llama-cpp-cuda-hip.md"&gt;exploring on how ggml-cuda works&lt;/a&gt; and I found that there were some easy fixes for llama.cpp's ROCm/HIP backend performance with rocWMMA (which sees bigger-than-expected drops with long context). These fixes I believe also solve most of the ROCm backend crashing problems (the default HIP path in llama.cpp's ROCm backend does not have a guard for fallback if there are missing tiles, I added a VEC fallback for those cases - without the guard, weird dimensions w/ missing tiles results in crashes).&lt;/p&gt; &lt;p&gt;With these fixes, I believe this is the overall fastest/best RDNA3 backend (caveat: only tested on Strix Halo gfx1151, a few models at long context). It has had some positive feedback from testing by a few community members so I figure I'd share it somewhere more publicly so that those that are interested can poke around (NOTE: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16827"&gt;this branch will not be merged upstream&lt;/a&gt;).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feature Branch: &lt;a href="https://github.com/lhl/llama.cpp/tree/rocm-wmma-tune"&gt;https://github.com/lhl/llama.cpp/tree/rocm-wmma-tune&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Actual changes: &lt;a href="https://github.com/ggml-org/llama.cpp/compare/master...lhl:llama.cpp:rocm-wmma-tune"&gt;https://github.com/ggml-org/llama.cpp/compare/master...lhl:llama.cpp:rocm-wmma-tune&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Testing and docs: &lt;a href="https://github.com/lhl/strix-halo-testing/tree/main/llama-cpp-fix-wmma"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llama-cpp-fix-wmma&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's an example of how significant the performance improvements are for me:&lt;/p&gt; &lt;h2&gt;Llama 3.2 1B Q4_K_M&lt;/h2&gt; &lt;h3&gt;My rocWMMA vs HIP&lt;/h3&gt; &lt;p&gt;Prefill (pp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;HIP&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;4703.28&lt;/td&gt; &lt;td align="right"&gt;4970.14&lt;/td&gt; &lt;td align="right"&gt;5.67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d1024&lt;/td&gt; &lt;td align="right"&gt;4076.03&lt;/td&gt; &lt;td align="right"&gt;4575.18&lt;/td&gt; &lt;td align="right"&gt;12.25%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;2936.89&lt;/td&gt; &lt;td align="right"&gt;3788.92&lt;/td&gt; &lt;td align="right"&gt;29.01%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;1350.48&lt;/td&gt; &lt;td align="right"&gt;2064.78&lt;/td&gt; &lt;td align="right"&gt;52.89%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d65536&lt;/td&gt; &lt;td align="right"&gt;424.76&lt;/td&gt; &lt;td align="right"&gt;706.46&lt;/td&gt; &lt;td align="right"&gt;66.32%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Decode (tg)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;HIP&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;195.65&lt;/td&gt; &lt;td align="right"&gt;195.59&lt;/td&gt; &lt;td align="right"&gt;-0.03%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d1024&lt;/td&gt; &lt;td align="right"&gt;188.79&lt;/td&gt; &lt;td align="right"&gt;188.84&lt;/td&gt; &lt;td align="right"&gt;0.03%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;173.36&lt;/td&gt; &lt;td align="right"&gt;173.28&lt;/td&gt; &lt;td align="right"&gt;-0.05%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;126.86&lt;/td&gt; &lt;td align="right"&gt;127.01&lt;/td&gt; &lt;td align="right"&gt;0.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d65536&lt;/td&gt; &lt;td align="right"&gt;64.62&lt;/td&gt; &lt;td align="right"&gt;64.55&lt;/td&gt; &lt;td align="right"&gt;-0.10%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;My rocWMMA vs Previous rocWMMA&lt;/h3&gt; &lt;p&gt;Prefill (pp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;default-rocwmma&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;4884.42&lt;/td&gt; &lt;td align="right"&gt;4970.14&lt;/td&gt; &lt;td align="right"&gt;1.75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d1024&lt;/td&gt; &lt;td align="right"&gt;4204.81&lt;/td&gt; &lt;td align="right"&gt;4575.18&lt;/td&gt; &lt;td align="right"&gt;8.81%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;2959.54&lt;/td&gt; &lt;td align="right"&gt;3788.92&lt;/td&gt; &lt;td align="right"&gt;28.02%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;1265.62&lt;/td&gt; &lt;td align="right"&gt;2064.78&lt;/td&gt; &lt;td align="right"&gt;63.14%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;pp512 @ d65536&lt;/td&gt; &lt;td align="right"&gt;360.24&lt;/td&gt; &lt;td align="right"&gt;706.46&lt;/td&gt; &lt;td align="right"&gt;96.11%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Decode (tg)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;default-rocwmma&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;193.01&lt;/td&gt; &lt;td align="right"&gt;195.59&lt;/td&gt; &lt;td align="right"&gt;1.34%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d1024&lt;/td&gt; &lt;td align="right"&gt;182.6&lt;/td&gt; &lt;td align="right"&gt;188.84&lt;/td&gt; &lt;td align="right"&gt;3.42%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;143.51&lt;/td&gt; &lt;td align="right"&gt;173.28&lt;/td&gt; &lt;td align="right"&gt;20.74%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;87.53&lt;/td&gt; &lt;td align="right"&gt;127.01&lt;/td&gt; &lt;td align="right"&gt;45.11%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 1B Q4_K - Medium&lt;/td&gt; &lt;td&gt;762.81 MiB&lt;/td&gt; &lt;td&gt;1.24 B&lt;/td&gt; &lt;td&gt;tg128 @ d65536&lt;/td&gt; &lt;td align="right"&gt;27.35&lt;/td&gt; &lt;td align="right"&gt;64.55&lt;/td&gt; &lt;td align="right"&gt;136.06%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;gpt-oss-20b F16/MXFP4&lt;/h2&gt; &lt;h3&gt;My rocWMMA vs HIP&lt;/h3&gt; &lt;p&gt;Prefill (pp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;HIP&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1472.01&lt;/td&gt; &lt;td align="right"&gt;1495.97&lt;/td&gt; &lt;td align="right"&gt;1.63%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d1024&lt;/td&gt; &lt;td align="right"&gt;1387.58&lt;/td&gt; &lt;td align="right"&gt;1456.15&lt;/td&gt; &lt;td align="right"&gt;4.94%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;1175.72&lt;/td&gt; &lt;td align="right"&gt;1347.75&lt;/td&gt; &lt;td align="right"&gt;14.63%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;713.9&lt;/td&gt; &lt;td align="right"&gt;962.98&lt;/td&gt; &lt;td align="right"&gt;34.89%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d65536&lt;/td&gt; &lt;td align="right"&gt;277.58&lt;/td&gt; &lt;td align="right"&gt;426.81&lt;/td&gt; &lt;td align="right"&gt;53.76%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Decode (tg)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;HIP&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;49.92&lt;/td&gt; &lt;td align="right"&gt;49.9&lt;/td&gt; &lt;td align="right"&gt;-0.04%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d1024&lt;/td&gt; &lt;td align="right"&gt;49.27&lt;/td&gt; &lt;td align="right"&gt;49.21&lt;/td&gt; &lt;td align="right"&gt;-0.11%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;48.15&lt;/td&gt; &lt;td align="right"&gt;48.05&lt;/td&gt; &lt;td align="right"&gt;-0.20%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;44.38&lt;/td&gt; &lt;td align="right"&gt;44.34&lt;/td&gt; &lt;td align="right"&gt;-0.11%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d65536&lt;/td&gt; &lt;td align="right"&gt;34.76&lt;/td&gt; &lt;td align="right"&gt;34.77&lt;/td&gt; &lt;td align="right"&gt;0.03%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;My rocWMMA vs Previous rocWMMA&lt;/h3&gt; &lt;p&gt;Prefill (pp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;default-rocwmma&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1513.79&lt;/td&gt; &lt;td align="right"&gt;1495.97&lt;/td&gt; &lt;td align="right"&gt;-1.18%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d1024&lt;/td&gt; &lt;td align="right"&gt;1417.45&lt;/td&gt; &lt;td align="right"&gt;1456.15&lt;/td&gt; &lt;td align="right"&gt;2.73%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;1205.37&lt;/td&gt; &lt;td align="right"&gt;1347.75&lt;/td&gt; &lt;td align="right"&gt;11.81%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;669.77&lt;/td&gt; &lt;td align="right"&gt;962.98&lt;/td&gt; &lt;td align="right"&gt;43.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;pp512 @ d65536&lt;/td&gt; &lt;td align="right"&gt;227.24&lt;/td&gt; &lt;td align="right"&gt;426.81&lt;/td&gt; &lt;td align="right"&gt;87.83%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Decode (tg)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th align="right"&gt;default-rocwmma&lt;/th&gt; &lt;th align="right"&gt;lhl-tune-tile&lt;/th&gt; &lt;th align="right"&gt;Œî%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;50.23&lt;/td&gt; &lt;td align="right"&gt;49.9&lt;/td&gt; &lt;td align="right"&gt;-0.64%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d1024&lt;/td&gt; &lt;td align="right"&gt;48.65&lt;/td&gt; &lt;td align="right"&gt;49.21&lt;/td&gt; &lt;td align="right"&gt;1.16%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;45.11&lt;/td&gt; &lt;td align="right"&gt;48.05&lt;/td&gt; &lt;td align="right"&gt;6.53%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;32.91&lt;/td&gt; &lt;td align="right"&gt;44.34&lt;/td&gt; &lt;td align="right"&gt;34.72%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td&gt;13141.28 MiB&lt;/td&gt; &lt;td&gt;20.91 B&lt;/td&gt; &lt;td&gt;tg128 @ d65536&lt;/td&gt; &lt;td align="right"&gt;14.63&lt;/td&gt; &lt;td align="right"&gt;34.77&lt;/td&gt; &lt;td align="right"&gt;137.71%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Strix Halo vs DGX Spark&lt;/h2&gt; &lt;p&gt;As another point of comparison, compared to ggeranov's recent &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;DGX Spark llama.cpp performance sweeps&lt;/a&gt;, both prefill and decode degradation are massively reduced, with decode (tg/token generation) now basically stably matching the DGX Spark (~-10%) from 0-32K context depth. (%'s here are how much faster the DGX Spark is vs the Strix Halo)&lt;/p&gt; &lt;h3&gt;Vulkan AMDVLK&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th align="right"&gt;DGX&lt;/th&gt; &lt;th align="right"&gt;STXH&lt;/th&gt; &lt;th align="right"&gt;%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;1689.47&lt;/td&gt; &lt;td align="right"&gt;729.10&lt;/td&gt; &lt;td align="right"&gt;+131.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d4096&lt;/td&gt; &lt;td align="right"&gt;1733.41&lt;/td&gt; &lt;td align="right"&gt;562.15&lt;/td&gt; &lt;td align="right"&gt;+208.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d8192&lt;/td&gt; &lt;td align="right"&gt;1705.93&lt;/td&gt; &lt;td align="right"&gt;424.50&lt;/td&gt; &lt;td align="right"&gt;+301.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d16384&lt;/td&gt; &lt;td align="right"&gt;1514.78&lt;/td&gt; &lt;td align="right"&gt;249.68&lt;/td&gt; &lt;td align="right"&gt;+506.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d32768&lt;/td&gt; &lt;td align="right"&gt;1221.23&lt;/td&gt; &lt;td align="right"&gt;137.08&lt;/td&gt; &lt;td align="right"&gt;+790.9%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th align="right"&gt;DGX&lt;/th&gt; &lt;th align="right"&gt;STXH&lt;/th&gt; &lt;th align="right"&gt;%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;tg32&lt;/td&gt; &lt;td align="right"&gt;52.87&lt;/td&gt; &lt;td align="right"&gt;50.05&lt;/td&gt; &lt;td align="right"&gt;+5.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d4096&lt;/td&gt; &lt;td align="right"&gt;51.02&lt;/td&gt; &lt;td align="right"&gt;46.11&lt;/td&gt; &lt;td align="right"&gt;+10.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d8192&lt;/td&gt; &lt;td align="right"&gt;48.46&lt;/td&gt; &lt;td align="right"&gt;43.15&lt;/td&gt; &lt;td align="right"&gt;+12.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d16384&lt;/td&gt; &lt;td align="right"&gt;44.78&lt;/td&gt; &lt;td align="right"&gt;38.46&lt;/td&gt; &lt;td align="right"&gt;+16.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d32768&lt;/td&gt; &lt;td align="right"&gt;38.76&lt;/td&gt; &lt;td align="right"&gt;31.54&lt;/td&gt; &lt;td align="right"&gt;+22.9%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;ROCm w/ rocWMMA&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th align="right"&gt;DGX&lt;/th&gt; &lt;th align="right"&gt;STXH&lt;/th&gt; &lt;th align="right"&gt;%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;1689.47&lt;/td&gt; &lt;td align="right"&gt;1006.65&lt;/td&gt; &lt;td align="right"&gt;+67.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d4096&lt;/td&gt; &lt;td align="right"&gt;1733.41&lt;/td&gt; &lt;td align="right"&gt;790.45&lt;/td&gt; &lt;td align="right"&gt;+119.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d8192&lt;/td&gt; &lt;td align="right"&gt;1705.93&lt;/td&gt; &lt;td align="right"&gt;603.83&lt;/td&gt; &lt;td align="right"&gt;+182.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d16384&lt;/td&gt; &lt;td align="right"&gt;1514.78&lt;/td&gt; &lt;td align="right"&gt;405.53&lt;/td&gt; &lt;td align="right"&gt;+273.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d32768&lt;/td&gt; &lt;td align="right"&gt;1221.23&lt;/td&gt; &lt;td align="right"&gt;223.82&lt;/td&gt; &lt;td align="right"&gt;+445.6%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th align="right"&gt;DGX&lt;/th&gt; &lt;th align="right"&gt;STXH&lt;/th&gt; &lt;th align="right"&gt;%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;tg32&lt;/td&gt; &lt;td align="right"&gt;52.87&lt;/td&gt; &lt;td align="right"&gt;46.56&lt;/td&gt; &lt;td align="right"&gt;+13.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d4096&lt;/td&gt; &lt;td align="right"&gt;51.02&lt;/td&gt; &lt;td align="right"&gt;38.25&lt;/td&gt; &lt;td align="right"&gt;+33.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d8192&lt;/td&gt; &lt;td align="right"&gt;48.46&lt;/td&gt; &lt;td align="right"&gt;32.65&lt;/td&gt; &lt;td align="right"&gt;+48.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d16384&lt;/td&gt; &lt;td align="right"&gt;44.78&lt;/td&gt; &lt;td align="right"&gt;25.50&lt;/td&gt; &lt;td align="right"&gt;+75.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d32768&lt;/td&gt; &lt;td align="right"&gt;38.76&lt;/td&gt; &lt;td align="right"&gt;17.82&lt;/td&gt; &lt;td align="right"&gt;+117.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;&lt;a href="https://github.com/lhl/llama.cpp/tree/rocm-wmma-tune"&gt;My Tuned rocWMMA&lt;/a&gt;&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th align="right"&gt;DGX&lt;/th&gt; &lt;th align="right"&gt;STXH&lt;/th&gt; &lt;th align="right"&gt;%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;1689.47&lt;/td&gt; &lt;td align="right"&gt;977.22&lt;/td&gt; &lt;td align="right"&gt;+72.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d4096&lt;/td&gt; &lt;td align="right"&gt;1733.41&lt;/td&gt; &lt;td align="right"&gt;878.54&lt;/td&gt; &lt;td align="right"&gt;+97.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d8192&lt;/td&gt; &lt;td align="right"&gt;1705.93&lt;/td&gt; &lt;td align="right"&gt;743.36&lt;/td&gt; &lt;td align="right"&gt;+129.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d16384&lt;/td&gt; &lt;td align="right"&gt;1514.78&lt;/td&gt; &lt;td align="right"&gt;587.25&lt;/td&gt; &lt;td align="right"&gt;+157.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;pp2048@d32768&lt;/td&gt; &lt;td align="right"&gt;1221.23&lt;/td&gt; &lt;td align="right"&gt;407.87&lt;/td&gt; &lt;td align="right"&gt;+199.4%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th align="right"&gt;DGX&lt;/th&gt; &lt;th align="right"&gt;STXH&lt;/th&gt; &lt;th align="right"&gt;%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;tg32&lt;/td&gt; &lt;td align="right"&gt;52.87&lt;/td&gt; &lt;td align="right"&gt;48.97&lt;/td&gt; &lt;td align="right"&gt;+8.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d4096&lt;/td&gt; &lt;td align="right"&gt;51.02&lt;/td&gt; &lt;td align="right"&gt;45.42&lt;/td&gt; &lt;td align="right"&gt;+12.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d8192&lt;/td&gt; &lt;td align="right"&gt;48.46&lt;/td&gt; &lt;td align="right"&gt;43.55&lt;/td&gt; &lt;td align="right"&gt;+11.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d16384&lt;/td&gt; &lt;td align="right"&gt;44.78&lt;/td&gt; &lt;td align="right"&gt;40.91&lt;/td&gt; &lt;td align="right"&gt;+9.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;tg32@d32768&lt;/td&gt; &lt;td align="right"&gt;38.76&lt;/td&gt; &lt;td align="right"&gt;36.43&lt;/td&gt; &lt;td align="right"&gt;+6.4%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Note on Vulkan drivers and batch sizes:&lt;/strong&gt; - AMDVLK (shown below) uses optimal &lt;code&gt;-ub 512&lt;/code&gt; and has better &lt;code&gt;pp&lt;/code&gt; performance - RADV uses optimal &lt;code&gt;-ub 1024&lt;/code&gt; with lower &lt;code&gt;pp&lt;/code&gt; but &lt;code&gt;tg&lt;/code&gt; decreases less at depth - ROCm tested with standard &lt;code&gt;-ub 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;NOTE: for those that aren't interested in compiling their own llama.cpp, the Vulkan (RADV) backend is probably still the best from a stability and long-context token generation perspective, but the prompt processing (pp) will be significantly slower.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok7hd4/faster_llamacpp_rocm_performance_for_amd_rdna3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok7hd4/faster_llamacpp_rocm_performance_for_amd_rdna3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok7hd4/faster_llamacpp_rocm_performance_for_amd_rdna3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T18:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1okbrz4</id>
    <title>IBM just released unsloth for finetinuing Granite4.0_350M</title>
    <updated>2025-10-30T21:08:34+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okbrz4/ibm_just_released_unsloth_for_finetinuing/"&gt; &lt;img alt="IBM just released unsloth for finetinuing Granite4.0_350M" src="https://preview.redd.it/vn84zysldbyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3e80873a766a970e98766336d69701437c1c9c8" title="IBM just released unsloth for finetinuing Granite4.0_350M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/unslothai/notebooks/blob/main/nb/Granite4.0_350M.ipynb"&gt;https://github.com/unslothai/notebooks/blob/main/nb/Granite4.0_350M.ipynb&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Big ups for the IBM folks for following up so quickly&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vn84zysldbyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okbrz4/ibm_just_released_unsloth_for_finetinuing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okbrz4/ibm_just_released_unsloth_for_finetinuing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T21:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok2lht</id>
    <title>Qwen 3 VL merged into llama.cpp!</title>
    <updated>2025-10-30T15:21:24+00:00</updated>
    <author>
      <name>/u/ervertes</name>
      <uri>https://old.reddit.com/user/ervertes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16780"&gt;https://github.com/ggml-org/llama.cpp/pull/16780&lt;/a&gt;&lt;/p&gt; &lt;p&gt;WE ARE SO BACK!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ervertes"&gt; /u/ervertes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2lht/qwen_3_vl_merged_into_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2lht/qwen_3_vl_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2lht/qwen_3_vl_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T15:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok3xie</id>
    <title>200+ pages of Hugging Face secrets on how to train an LLM</title>
    <updated>2025-10-30T16:11:22+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt; &lt;img alt="200+ pages of Hugging Face secrets on how to train an LLM" src="https://preview.redd.it/s12qz4k3w9yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c78fbb2faf8b6857633466eb7cf34609898a57" title="200+ pages of Hugging Face secrets on how to train an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's elie from the hugging face pre-training team! We're very excited to share our new blog (book?) that cover the full pipeline: pre-training, post-training and infra. 200+ pages of what worked, what didn‚Äôt, and how to make it run reliably :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook"&gt;https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope yall will enjoy it, don't hesitate to make feedback on the community tab :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s12qz4k3w9yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T16:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0i7q</id>
    <title>AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo</title>
    <updated>2025-10-30T14:00:16+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" src="https://b.thumbs.redditmedia.com/hb9XoRhxPRhv8ljYkjnVbJWgnClXeLGMxbG1TEDCwos.jpg" title="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We‚Äôre super excited to host this week‚Äôs AMA! &lt;/p&gt; &lt;p&gt;Join us and ask your questions directly to the human minds behind all things Liquid: Liquid Foundational Models, the Liquid Edge AI Platform (LEAP) for model customization and deployment, and Apollo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our participants:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks &lt;a href="https://www.reddit.com/user/jamarks13/"&gt;u/jamarks13&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith &lt;a href="https://www.reddit.com/user/jimmysmith1919/"&gt;u/jimmysmith1919&lt;/a&gt; (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne &lt;a href="https://www.reddit.com/user/mlabonne/"&gt;u/mlabonne&lt;/a&gt; (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes &lt;a href="https://www.reddit.com/user/Wide-Half-7982/"&gt;u/Wide-Half-7982&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak &lt;a href="https://www.reddit.com/user/ankebananke/"&gt;u/ankebananke&lt;/a&gt; (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k &lt;a href="https://www.reddit.com/user/ManWithARedFace/"&gt;u/ManWithARedFace&lt;/a&gt; (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev &lt;a href="https://www.reddit.com/user/ykhrustalev/"&gt;u/ykhrustalev&lt;/a&gt; (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena &lt;a href="https://www.reddit.com/user/humble_pi_314/"&gt;u/humble_pi_314&lt;/a&gt; (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca &lt;a href="https://www.reddit.com/user/Ok-Safe-5316/"&gt;u/Ok-Safe-5316&lt;/a&gt; (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale &lt;a href="https://www.reddit.com/user/anthony-liquidai/"&gt;u/anthony-liquidai&lt;/a&gt; (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo &lt;a href="https://www.reddit.com/user/PauLabartaBajo/"&gt;u/PauLabartaBajo&lt;/a&gt; (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from &lt;strong&gt;10 AM - 1 PM PST&lt;/strong&gt;. The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&amp;gt; &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198"&gt;https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks to everyone who participated in this AMA. It was a pleasure.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://discord.gg/DFU3WQeaYD"&gt;Join the Liquid AI Discord Community&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:00:16+00:00</published>
  </entry>
</feed>
