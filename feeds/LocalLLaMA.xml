<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-11T19:09:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r1vegx</id>
    <title>MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users</title>
    <updated>2026-02-11T11:55:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt; &lt;img alt="MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users" src="https://preview.redd.it/rzn30tyytuig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=d76aec4ee8daf1af60654c398ab3a75babe3bad9" title="MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/rudrank/status/2021534943932031226?s=20"&gt;https://x.com/rudrank/status/2021534943932031226?s=20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzn30tyytuig1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361c1704ab37823746ab84fe45b4dcd3d378685a"&gt;https://preview.redd.it/rzn30tyytuig1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361c1704ab37823746ab84fe45b4dcd3d378685a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1vqjp3n1uuig1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9967df4c6af84af29af6ae5272b243a6ad1693"&gt;https://preview.redd.it/1vqjp3n1uuig1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9967df4c6af84af29af6ae5272b243a6ad1693&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T11:55:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r25xfq</id>
    <title>GLM 5 vs Claude Opus 4.6 vs GPT-5.2 â€” I Asked a Simple Trick Question. GLM 5 Is Similarly Smart as Claude Opus 4.6</title>
    <updated>2026-02-11T18:49:40+00:00</updated>
    <author>
      <name>/u/Top-Cardiologist1011</name>
      <uri>https://old.reddit.com/user/Top-Cardiologist1011</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r25xfq/glm_5_vs_claude_opus_46_vs_gpt52_i_asked_a_simple/"&gt; &lt;img alt="GLM 5 vs Claude Opus 4.6 vs GPT-5.2 â€” I Asked a Simple Trick Question. GLM 5 Is Similarly Smart as Claude Opus 4.6" src="https://preview.redd.it/3h8lyy7vvwig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aabbee1659ec3fbe4da01dd624c78c53aa87f7ec" title="GLM 5 vs Claude Opus 4.6 vs GPT-5.2 â€” I Asked a Simple Trick Question. GLM 5 Is Similarly Smart as Claude Opus 4.6" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The question is: &amp;quot;I want to go get my car washed. The car wash is 50 meters from my house. Do you think I should drive there or walk?&amp;quot;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM 5: Drive&lt;/li&gt; &lt;li&gt;Claude Opus 4.6: Drive&lt;/li&gt; &lt;li&gt;GPT-5.2: Walk&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GLM 5 Is Similarly Smart as Claude Opus 4.6&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Cardiologist1011"&gt; /u/Top-Cardiologist1011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3h8lyy7vvwig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r25xfq/glm_5_vs_claude_opus_46_vs_gpt52_i_asked_a_simple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r25xfq/glm_5_vs_claude_opus_46_vs_gpt52_i_asked_a_simple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T18:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r21tzb</id>
    <title>Community Evals on Hugging Face</title>
    <updated>2026-02-11T16:23:19+00:00</updated>
    <author>
      <name>/u/HauntingMoment</name>
      <uri>https://old.reddit.com/user/HauntingMoment</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"&gt; &lt;img alt="Community Evals on Hugging Face" src="https://preview.redd.it/iijfx1dk5wig1.png?width=140&amp;amp;height=102&amp;amp;auto=webp&amp;amp;s=52c016113a72f21ef056a0e339801a2fbe7bfa48" title="Community Evals on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey! I'm Nathan (SaylorTwift) from huggingface we have a big update from the hf hub that actually fixes one of the most annoying things about model evaluation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iijfx1dk5wig1.png?width=1049&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a544cd848e26b2ff06d926dae85d711495f3bb6"&gt;Humanity's Last exam dataset on Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;community evals are now live on huggingface! it's a decentralized, transparent way for the community to report and share model evaluations.&lt;/p&gt; &lt;p&gt;why ?&lt;/p&gt; &lt;p&gt;everyoneâ€™s stats are scattered across papers, model cards, platforms and sometimes contradict each other. thereâ€™s no unified single source of truth. community evals aim to fix that by making eval reporting open and reproducible.&lt;/p&gt; &lt;p&gt;what's changed ?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmarks host leaderboards right in the dataset repo (e.g. mmlu-pro, gpqa, hle)&lt;/li&gt; &lt;li&gt;models store their own results in .eval_results/*.yaml and they show up on model cards and feed into the dataset leaderboards.&lt;/li&gt; &lt;li&gt;anyone can submit eval results via a pr without needing the model author to merge. those show up as community results.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the key idea is that scores arenâ€™t hidden in black-box leaderboards anymore. everyone can see who ran what, how, and when, and build tools, dashboards, comparisons on top of that!&lt;/p&gt; &lt;p&gt;If you want to &lt;a href="https://huggingface.co/blog/community-evals"&gt;read more&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HauntingMoment"&gt; /u/HauntingMoment &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tfbm</id>
    <title>DeepSeek just updated to a 1M context window!</title>
    <updated>2026-02-11T10:03:06+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt; &lt;img alt="DeepSeek just updated to a 1M context window!" src="https://preview.redd.it/9z2ggdgy9uig1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=2ab7565635c402cdabef3c7d20eae901df30fa52" title="DeepSeek just updated to a 1M context window!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The DeepSeek app was just updated with 1M context, and the knowledge cutoff date is now May 2025. It's unclear for now if this is a new model. Also, there hasn't been any movement on their Hugging Face page yet.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9z2ggdgy9uig1.png?width=1179&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3f48da856b53751f2db2b17ac5f49baaf9add55"&gt;https://preview.redd.it/9z2ggdgy9uig1.png?width=1179&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3f48da856b53751f2db2b17ac5f49baaf9add55&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:03:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r243bg</id>
    <title>Tested GLM 5: Great model</title>
    <updated>2026-02-11T17:44:34+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r243bg/tested_glm_5_great_model/"&gt; &lt;img alt="Tested GLM 5: Great model" src="https://external-preview.redd.it/bzJiMzh5ajdrd2lnMc0sk44cwuityrkv6bPNqII0lNadp-NXPigbagKTulbz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f64d52357a3ffb1b3bc3fd11d58e7191cb6d8b5f" title="Tested GLM 5: Great model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems to be the same model as Pony Alpha from the responses, but better!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1g8s0j7kwig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r243bg/tested_glm_5_great_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r243bg/tested_glm_5_great_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T17:44:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r25lvf</id>
    <title>The hunt continues ...</title>
    <updated>2026-02-11T18:38:03+00:00</updated>
    <author>
      <name>/u/Lzlxlclvlblnlmao</name>
      <uri>https://old.reddit.com/user/Lzlxlclvlblnlmao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r25lvf/the_hunt_continues/"&gt; &lt;img alt="The hunt continues ..." src="https://preview.redd.it/f8zp85xrtwig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e054a1ad8e41fd415e85050a1e4cfd4a37da628a" title="The hunt continues ..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tbf it did work with Deep Thinking enabled &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lzlxlclvlblnlmao"&gt; /u/Lzlxlclvlblnlmao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f8zp85xrtwig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r25lvf/the_hunt_continues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r25lvf/the_hunt_continues/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T18:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1oan9</id>
    <title>EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages</title>
    <updated>2026-02-11T05:02:36+00:00</updated>
    <author>
      <name>/u/Cod3Conjurer</name>
      <uri>https://old.reddit.com/user/Cod3Conjurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?&lt;/p&gt; &lt;p&gt;Took the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) â€“ 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.&lt;/p&gt; &lt;p&gt;What I built:&lt;/p&gt; &lt;p&gt;- Full RAG pipeline with optimized data processing&lt;/p&gt; &lt;p&gt;- Processed 2M+ pages (cleaning, chunking, vectorization)&lt;/p&gt; &lt;p&gt;- Semantic search &amp;amp; Q&amp;amp;A over massive dataset&lt;/p&gt; &lt;p&gt;- Constantly tweaking for better retrieval &amp;amp; performance&lt;/p&gt; &lt;p&gt;- Python, MIT Licensed, open source&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;Itâ€™s trending, real-world data at scale, the perfect playground.&lt;/p&gt; &lt;p&gt;When you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/AnkitNayak-eth/EpsteinFiles-RAG"&gt;https://github.com/AnkitNayak-eth/EpsteinFiles-RAG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open to ideas, optimizations, and technical discussions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cod3Conjurer"&gt; /u/Cod3Conjurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T05:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r21ojm</id>
    <title>We've built memory into 4 different agent systems. Here's what actually works and what's a waste of time.</title>
    <updated>2026-02-11T16:17:39+00:00</updated>
    <author>
      <name>/u/arapkuliev</name>
      <uri>https://old.reddit.com/user/arapkuliev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After building memory layers for multiple agent setups, here's the shit nobody tells you in the tutorials. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's a waste of time:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&amp;quot;Just use a vector store&amp;quot;&lt;/strong&gt; -- Congrats, you built keyword search with extra steps and worse debugging. Embeddings are great for fuzzy matching, terrible for precise retrieval. Your agent will confidently pull up something &lt;em&gt;semantically similar&lt;/em&gt; instead of the &lt;em&gt;actual thing it needs&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Dumping full conversation logs as memory&lt;/strong&gt; -- Your agent doesn't need to remember that the user said &amp;quot;thanks&amp;quot; 47 times. Unfiltered logs are noise with a few signal fragments buried in them. And you're burning tokens retrieving garbage.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;One retrieval strategy&lt;/strong&gt; -- If you're only doing semantic search, you're missing exact matches. If you're only doing keyword search, you're missing relationships. Pick one and you'll spend months wondering why retrieval &amp;quot;feels off.&amp;quot; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually works:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Entity resolution pipelines.&lt;/strong&gt; Actively identify and link entities across conversations. &amp;quot;The Postgres migration,&amp;quot; &amp;quot;that DB move we discussed,&amp;quot; and &amp;quot;the thing Jake proposed last Tuesday&amp;quot; are the same thing. If your memory doesn't know that, it's broken.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Temporal tagging.&lt;/strong&gt; When was this learned? Is it still valid? A decision from 3 months ago might be reversed. If your memory treats everything as equally fresh, your agent will confidently act on outdated context. Timestamps aren't metadata. They're core to whether a memory is useful.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Explicit priority systems.&lt;/strong&gt; Not everything is worth remembering. Let users or systems mark what matters and what should decay. Without this you end up with a memory that &amp;quot;remembers&amp;quot; everything equally, which means it effectively remembers nothing.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Contradiction detection.&lt;/strong&gt; Your system will inevitably store conflicting information. &amp;quot;We're using Redis for caching&amp;quot; and &amp;quot;We moved off Redis last sprint.&amp;quot; If you silently store both, your agent flips a coin on which one it retrieves. Flag conflicts. Surface them. Let a human resolve it.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Multi-strategy retrieval.&lt;/strong&gt; Run keyword, semantic, and graph traversal in parallel. Merge results. The answer to &amp;quot;why did we pick this architecture?&amp;quot; might be spread across a design doc, a Slack thread, and a PR description. No single strategy finds all three. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The uncomfortable truth:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;None of this &amp;quot;solves&amp;quot; memory. These are tactical patches for specific retrieval problems. But implemented carefully, they make systems that &lt;em&gt;feel&lt;/em&gt; like memory instead of feeling like a database you have to babysit. &lt;/p&gt; &lt;p&gt;The bar isn't &amp;quot;perfect recall.&amp;quot; The bar is &amp;quot;better than asking the same question twice.&amp;quot; &lt;/p&gt; &lt;p&gt;What's actually working in your setups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arapkuliev"&gt; /u/arapkuliev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21ojm/weve_built_memory_into_4_different_agent_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21ojm/weve_built_memory_into_4_different_agent_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r21ojm/weve_built_memory_into_4_different_agent_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2005l</id>
    <title>Mini AI Machine</title>
    <updated>2026-02-11T15:14:31+00:00</updated>
    <author>
      <name>/u/KnownAd4832</name>
      <uri>https://old.reddit.com/user/KnownAd4832</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2005l/mini_ai_machine/"&gt; &lt;img alt="Mini AI Machine" src="https://preview.redd.it/4vmjqryjtvig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c30c9580fd072df0983a74667d5a2fb1848c656e" title="Mini AI Machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I do a lot of text processing &amp;amp; generation on small model. RTX 4000 Blackwell SFF (75W max) + 32GB DDR5 + DeskMeet 8L PC running PopOS and vLLM ðŸŽ‰&lt;/p&gt; &lt;p&gt;Anyone else has mini AI rig?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KnownAd4832"&gt; /u/KnownAd4832 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4vmjqryjtvig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2005l/mini_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2005l/mini_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T15:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1r3nk</id>
    <title>Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts</title>
    <updated>2026-02-11T07:38:59+00:00</updated>
    <author>
      <name>/u/Tiny_Minimum_4384</name>
      <uri>https://old.reddit.com/user/Tiny_Minimum_4384</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt; &lt;img alt="Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts" src="https://preview.redd.it/82hjsn98ktig1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=f06a27761905099dec3c58ed9398dbb2a40f6816" title="Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ðŸ‘‹&lt;/p&gt; &lt;p&gt;Weâ€™re excited to share Nanbeige4.1-3B, the latest iteration of our open-source 3B model from Nanbeige LLM Lab. Our goal with this release is to explore whether a small general model can simultaneously achieve strong reasoning, robust preference alignment, and agentic behavior.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/82hjsn98ktig1.png?width=4920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ab960015daf8b38ae74fe9d4332208011f4f05"&gt;https://preview.redd.it/82hjsn98ktig1.png?width=4920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ab960015daf8b38ae74fe9d4332208011f4f05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strong Reasoning Capability&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Solves complex problems through sustained and coherent reasoning within a single forward pass. It achieves strong results on challenging tasks such as &lt;strong&gt;LiveCodeBench-Pro&lt;/strong&gt;, &lt;strong&gt;IMO-Answer-Bench&lt;/strong&gt;, and &lt;strong&gt;AIME 2026 I&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robust Preference Alignment&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Besides solving hard problems, it also demonstrates strong alignment with human preferences. Nanbeige4.1-3B achieves &lt;strong&gt;73.2 on Arena-Hard-v2&lt;/strong&gt; and &lt;strong&gt;52.21 on Multi-Challenge&lt;/strong&gt;, demonstrating superior performance compared to larger models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic and Deep-Search Capability in a 3B Model&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Beyond chat tasks such as alignment, coding, and mathematical reasoning, Nanbeige4.1-3B also demonstrates solid native agent capabilities. It natively supports deep-search and achieves strong performance on tasks such as &lt;strong&gt;xBench-DeepSearch&lt;/strong&gt; and &lt;strong&gt;GAIA&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context and Sustained Reasoning&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Nanbeige4.1-3B supports context lengths of up to 256k tokens, enabling deep-search with hundreds of tool calls, as well as 100k+ token single-pass reasoning for complex problems&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ðŸ¤— Model Weight: &lt;a href="https://huggingface.co/Nanbeige/Nanbeige4.1-3B"&gt;https://huggingface.co/Nanbeige/Nanbeige4.1-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ðŸ“„ Technical Report: Coming Soon&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny_Minimum_4384"&gt; /u/Tiny_Minimum_4384 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T07:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1u2ne</id>
    <title>Grok-3 joins upcoming models list</title>
    <updated>2026-02-11T10:41:33+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"&gt; &lt;img alt="Grok-3 joins upcoming models list" src="https://preview.redd.it/ueoiz6yrfuig1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc9154b12fb8ee19cbdde7d47a510f5ad934b95f" title="Grok-3 joins upcoming models list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/elonmusk/status/2020878250516341110"&gt;Tweet link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First question is when?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ueoiz6yrfuig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20uwk</id>
    <title>Releasing MioTTS: A family of lightweight, fast LLM-based TTS models (0.1B - 2.6B) with Zero-shot Voice Cloning</title>
    <updated>2026-02-11T15:46:56+00:00</updated>
    <author>
      <name>/u/Askxc</name>
      <uri>https://old.reddit.com/user/Askxc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Iâ€™ve been developing a personal project to create a lightweight and fast TTS model. Today Iâ€™m releasing &lt;strong&gt;MioTTS&lt;/strong&gt;, a family of LLM-based models ranging from &lt;strong&gt;0.1B to 2.6B&lt;/strong&gt; parameters.&lt;/p&gt; &lt;p&gt;The main focus was to achieve high-fidelity audio at the 0.1B parameter scale. I wanted to see how efficient it could be while maintaining quality, so I also developed a custom neural audio codec (&lt;strong&gt;MioCodec&lt;/strong&gt;) to minimize latency.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-shot Voice Cloning:&lt;/strong&gt; Supports high-fidelity cloning from short reference audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bilingual:&lt;/strong&gt; Trained on ~100k hours of English and Japanese speech data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Codec:&lt;/strong&gt; Built on top of &lt;strong&gt;MioCodec&lt;/strong&gt;, a custom neural audio codec I developed to allow for faster generation (low token rate) while maintaining audio fidelity. The codec is also released under MIT license.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Family:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Iâ€™ve released multiple sizes to balance quality and resource usage. Licenses depend on the base model used.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Base Model&lt;/th&gt; &lt;th align="left"&gt;License&lt;/th&gt; &lt;th align="left"&gt;RTF (approx.)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.1B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Falcon-H1-Tiny&lt;/td&gt; &lt;td align="left"&gt;Falcon-LLM&lt;/td&gt; &lt;td align="left"&gt;0.04 - 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.4B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;LFM2-350M&lt;/td&gt; &lt;td align="left"&gt;LFM Open v1.0&lt;/td&gt; &lt;td align="left"&gt;0.035 - 0.045&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen3-0.6B&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;td align="left"&gt;0.055 - 0.065&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;1.2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;LFM2.5-1.2B&lt;/td&gt; &lt;td align="left"&gt;LFM Open v1.0&lt;/td&gt; &lt;td align="left"&gt;0.065 - 0.075&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen3-1.7B&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;td align="left"&gt;0.10 - 0.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;LFM2-2.6B&lt;/td&gt; &lt;td align="left"&gt;LFM Open v1.0&lt;/td&gt; &lt;td align="left"&gt;0.135 - 0.145&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'd love to hear your feedback, especially on the English prosody (since I primarily develop in Japanese).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Collection:&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/Aratako/miotts"&gt;https://huggingface.co/collections/Aratako/miotts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference Code:&lt;/strong&gt; &lt;a href="https://github.com/Aratako/MioTTS-Inference"&gt;https://github.com/Aratako/MioTTS-Inference&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demo (0.1B):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/Aratako/MioTTS-0.1B-Demo"&gt;https://huggingface.co/spaces/Aratako/MioTTS-0.1B-Demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Askxc"&gt; /u/Askxc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20uwk/releasing_miotts_a_family_of_lightweight_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20uwk/releasing_miotts_a_family_of_lightweight_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r20uwk/releasing_miotts_a_family_of_lightweight_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T15:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1snhv</id>
    <title>DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!</title>
    <updated>2026-02-11T09:15:17+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt; &lt;img alt="DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!" src="https://preview.redd.it/vahfibvk4uig1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ffb9da4726561f044fd768dc2f75838e643edf5f" title="DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vahfibvk4uig1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15d8b657dd69d496af701aeb4c20ed62b4bbce98"&gt;This model know Gemini 2.5 Pro on not web search &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ontumt5s3uig1.jpg?width=657&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efff85457597b8fd9dbcbcf3d1d99d62a0678ea2"&gt;https://preview.redd.it/ontumt5s3uig1.jpg?width=657&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efff85457597b8fd9dbcbcf3d1d99d62a0678ea2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek has launched grayscale testing for its new model on both its official website and app. The new model features a 1M context window and an updated knowledge base. Currently, access is limited to a select group of accounts.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j1qiarng1uig1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a99f1652ea755a7aeaa600250ff4856133fbfca"&gt;https://preview.redd.it/j1qiarng1uig1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a99f1652ea755a7aeaa600250ff4856133fbfca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It look Like V4 Lite not actually V4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T09:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r22e37</id>
    <title>GLM5 benchmarks</title>
    <updated>2026-02-11T16:43:57+00:00</updated>
    <author>
      <name>/u/Simple_Split5074</name>
      <uri>https://old.reddit.com/user/Simple_Split5074</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22e37/glm5_benchmarks/"&gt; &lt;img alt="GLM5 benchmarks" src="https://preview.redd.it/gnpedmcd9wig1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=b4e894dc4650930074d76d1fcac5bcbad2bd4570" title="GLM5 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gnpedmcd9wig1.png?width=4239&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81c4791dca25226d8e3c7b16543eb067ffcb8bdc"&gt;https://preview.redd.it/gnpedmcd9wig1.png?width=4239&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81c4791dca25226d8e3c7b16543eb067ffcb8bdc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More here &lt;a href="https://docs.z.ai/guides/llm/glm-5"&gt;https://docs.z.ai/guides/llm/glm-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simple_Split5074"&gt; /u/Simple_Split5074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22e37/glm5_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22e37/glm5_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r22e37/glm5_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wvos</id>
    <title>MOSS-TTS has been released</title>
    <updated>2026-02-11T13:06:41+00:00</updated>
    <author>
      <name>/u/Xiami2019</name>
      <uri>https://old.reddit.com/user/Xiami2019</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"&gt; &lt;img alt="MOSS-TTS has been released" src="https://preview.redd.it/u56s8amp6vig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd362ae4aaee8f23d85c9c94bcdc2e0f1a676bf2" title="MOSS-TTS has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seed TTS Eval&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xiami2019"&gt; /u/Xiami2019 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u56s8amp6vig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20wki</id>
    <title>Add Kimi-K2.5 support</title>
    <updated>2026-02-11T15:48:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"&gt; &lt;img alt="Add Kimi-K2.5 support" src="https://external-preview.redd.it/5gup_oD4lytsLI1wID-Zo3RkPiRxiRbU2Hm7r-fkB2I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58c51ab74c9a734ec60838d3f67b78c6df26076b" title="Add Kimi-K2.5 support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19170"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T15:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r22co6</id>
    <title>GLM-5: From Vibe Coding to Agentic Engineering</title>
    <updated>2026-02-11T16:42:30+00:00</updated>
    <author>
      <name>/u/ShreckAndDonkey123</name>
      <uri>https://old.reddit.com/user/ShreckAndDonkey123</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreckAndDonkey123"&gt; /u/ShreckAndDonkey123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://z.ai/blog/glm-5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22co6/glm5_from_vibe_coding_to_agentic_engineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r22co6/glm5_from_vibe_coding_to_agentic_engineering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tuh1</id>
    <title>Just finished building this bad boy</title>
    <updated>2026-02-11T10:28:00+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt; &lt;img alt="Just finished building this bad boy" src="https://preview.redd.it/ju0ed5uceuig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04eab77fdf6e1df2e0b04b0581b6a1d713e805b5" title="Just finished building this bad boy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;6x Gigabyte 3090 Gaming OC all running at PCIe 4.0 16x speed&lt;/p&gt; &lt;p&gt;Asrock Romed-2T motherboard with Epyc 7502 CPU&lt;/p&gt; &lt;p&gt;8 sticks of DDR4 8GB 2400Mhz running in octochannel mode&lt;/p&gt; &lt;p&gt;Modified Tinygrad Nvidia drivers with P2P enabled, intra GPU bandwidth tested at 24.5 GB/s&lt;/p&gt; &lt;p&gt;Total 144GB VRam, will be used to experiment with training diffusion models up to 10B parameters from scratch&lt;/p&gt; &lt;p&gt;All GPUs set to 270W power limit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ju0ed5uceuig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r248cj</id>
    <title>GLM 5 is already on huggingface!</title>
    <updated>2026-02-11T17:49:30+00:00</updated>
    <author>
      <name>/u/oiuht54</name>
      <uri>https://old.reddit.com/user/oiuht54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-5"&gt;https://huggingface.co/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oiuht54"&gt; /u/oiuht54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r248cj/glm_5_is_already_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r248cj/glm_5_is_already_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r248cj/glm_5_is_already_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T17:49:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1x0qi</id>
    <title>GLM 5.0 &amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?</title>
    <updated>2026-02-11T13:12:51+00:00</updated>
    <author>
      <name>/u/Appropriate-Lie-8812</name>
      <uri>https://old.reddit.com/user/Appropriate-Lie-8812</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt; &lt;img alt="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" src="https://preview.redd.it/k4rtczs47vig1.png?width=140&amp;amp;height=56&amp;amp;auto=webp&amp;amp;s=46cd0e4543f951137b6e945d501812280005a7d3" title="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5.0 (&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;) and MiniMax 2.5 (&lt;a href="https://agent.minimax.io"&gt;https://agent.minimax.io&lt;/a&gt;) just dropped, both clearly moving beyond simple chat into agent-style workflows.&lt;/p&gt; &lt;p&gt;GLM 5.0 seems focused on stronger reasoning and coding, while MiniMax 2.5 emphasizes task decomposition and longer-running execution.&lt;/p&gt; &lt;p&gt;Feels like the competition is shifting from &amp;quot;who writes better answers&amp;quot; to &amp;quot;who can actually finish the job.&amp;quot;&lt;/p&gt; &lt;p&gt;Planning to test both in a few setups , maybe straight API benchmarks, Cursor-style IDE workflows, and a multi-agent orchestration tool like Verdent, to see how they handle longer tasks and repo-level changes. Will report back if anything interesting breaks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Lie-8812"&gt; /u/Appropriate-Lie-8812 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r1x0qi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wnj9</id>
    <title>MiniMax M2.5 Released</title>
    <updated>2026-02-11T12:56:37+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt; &lt;img alt="MiniMax M2.5 Released" src="https://preview.redd.it/uou9tmkx4vig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=3113e726ff999e0cdee3a5021d7abd5f90521d6e" title="MiniMax M2.5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755"&gt;https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://agent.minimax.io/"&gt;https://agent.minimax.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r23xsm</id>
    <title>zai-org/GLM-5 Â· Hugging Face</title>
    <updated>2026-02-11T17:39:19+00:00</updated>
    <author>
      <name>/u/TellMeAboutGoodManga</name>
      <uri>https://old.reddit.com/user/TellMeAboutGoodManga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r23xsm/zaiorgglm5_hugging_face/"&gt; &lt;img alt="zai-org/GLM-5 Â· Hugging Face" src="https://external-preview.redd.it/76jqmQhRT_Wp-7ojZX0kImXZmU_TWZiCjlowfftW_8U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80009097acf92700282fb39124bf95277917657" title="zai-org/GLM-5 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TellMeAboutGoodManga"&gt; /u/TellMeAboutGoodManga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r23xsm/zaiorgglm5_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r23xsm/zaiorgglm5_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T17:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wl6x</id>
    <title>GLM 5 Released</title>
    <updated>2026-02-11T12:53:30+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt; &lt;img alt="GLM 5 Released" src="https://preview.redd.it/mvdnn18e4vig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=5b006a25f178b73764138eabdb11ae38eb368d7f" title="GLM 5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e"&gt;https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r22hlq</id>
    <title>GLM-5 Officially Released</title>
    <updated>2026-02-11T16:47:29+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt; &lt;img alt="GLM-5 Officially Released" src="https://preview.redd.it/h2bmmfa5awig1.jpg?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=48723badc371c5206ca5e6292829eb25b9ec00d5" title="GLM-5 Officially Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are launching GLM-5, targeting complex systems engineering and long-horizon agentic tasks. Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI). Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://z.ai/blog/glm-5"&gt;https://z.ai/blog/glm-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/zai-org/GLM-5"&gt;https://huggingface.co/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/zai-org/GLM-5"&gt;https://github.com/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r22hlq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
