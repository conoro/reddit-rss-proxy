<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-18T11:05:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qff481</id>
    <title>I built Adaptive-K routing: 30-52% compute savings on MoE models (Mixtral, Qwen, OLMoE)</title>
    <updated>2026-01-17T14:50:29+00:00</updated>
    <author>
      <name>/u/Fuzzy_Ad_1390</name>
      <uri>https://old.reddit.com/user/Fuzzy_Ad_1390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Gabrobals/sbm-efficient"&gt;https://github.com/Gabrobals/sbm-efficient&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Whitepaper: &lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;https://adaptive-k.vercel.app/whitepaper.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TensorRT-LLM PR: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/pull/10672"&gt;https://github.com/NVIDIA/TensorRT-LLM/pull/10672&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live demo: &lt;a href="https://huggingface.co/spaces/Gabrobals/adaptive-k-demo"&gt;https://huggingface.co/spaces/Gabrobals/adaptive-k-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or discuss implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fuzzy_Ad_1390"&gt; /u/Fuzzy_Ad_1390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T14:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg5io6</id>
    <title>Is it feasible for a Team to replace Claude Code with one of the "local" alternatives?</title>
    <updated>2026-01-18T10:44:03+00:00</updated>
    <author>
      <name>/u/nunodonato</name>
      <uri>https://old.reddit.com/user/nunodonato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yes, I've read countless posts in this sub about replacing Claude Code with local models.&lt;/p&gt; &lt;p&gt;My question is slightly different. I'm talking about finding a replacement that would be able to serve a small team of developers.&lt;/p&gt; &lt;p&gt;We are currently spending around 2k/mo on Claude. And that can go a long way on cloud GPUs. However, I'm not sure if it would be good enough to support a few concurrent requests.&lt;/p&gt; &lt;p&gt;I've read a lot of praise for Deepseek Coder and a few of the newer models, but would they still perform okay-ish with Q8?&lt;/p&gt; &lt;p&gt;Any advice? recommendations?&lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nunodonato"&gt; /u/nunodonato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T10:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfogkp</id>
    <title>Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?</title>
    <updated>2026-01-17T20:50:29+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"&gt; &lt;img alt="Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?" src="https://external-preview.redd.it/MHgzNjM0N2QyemRnMQsZsHM-tqgfg0XXBK6smBja3B3Y-8kZyS2BD6gyOUFy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=731f35ba5df810d40f5158dcba9a22ffad9f0bb8" title="Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prototyped this in a few minutes. Seems incredibly useful for smaller devices (mobile LLMs)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ad16dbhd2zdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T20:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg2n5c</id>
    <title>RAG Paper 26.1.15</title>
    <updated>2026-01-18T07:54:08+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10681v1"&gt;Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10644v1"&gt;RoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10413v1"&gt;LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10342v1"&gt;C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10246v1"&gt;coTherapist: A Behavior-Aligned Small Language Model to Support Mental Healthcare Experts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10215v1"&gt;Topo-RAG: Topology-aware retrieval for hybrid text-table documents&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10168v1"&gt;RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10131v1"&gt;M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.10011v1"&gt;Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.09985v1"&gt;FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.09982v1"&gt;Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/components/arena"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg2n5c/rag_paper_26115/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg2n5c/rag_paper_26115/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg2n5c/rag_paper_26115/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T07:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfb0gk</id>
    <title>KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop</title>
    <updated>2026-01-17T11:35:11+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, it's been a hot minute, but I thought I'd share this here since it's quite a big new feature. &lt;/p&gt; &lt;p&gt;Yes, KoboldCpp is still alive and kicking. And besides the major UI overhaul, we've finally added native MCP support in KoboldCpp v1.106! It's designed to be a painless Claude Desktop drop-in replacement with maximum compatibility, the &lt;code&gt;mcp.json&lt;/code&gt; uses the same format so you can swap it in easily. &lt;/p&gt; &lt;p&gt;The KoboldCpp MCP bridge will connect to all provided MCP servers (HTTP and STDIO transports both supported) and automatically forward requests for tools the AI selects to the correct MCP server. This MCP bridge can also be used by third party clients.&lt;/p&gt; &lt;p&gt;On the frontend side, you can fetch the list of all tools from all servers, select the tools you want to let AI use, and optionally enable tool call approvals.&lt;/p&gt; &lt;p&gt;Some demo screenshots of various tool servers being used: &lt;a href="https://imgur.com/a/fKeWKUU"&gt;https://imgur.com/a/fKeWKUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;feedback is welcome. cheers! - concedo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg1udz</id>
    <title>Full local AI stack on dual 3090s: multi-modal bot with chat, vision, image gen, TTS, face swap, and experimental video</title>
    <updated>2026-01-18T07:06:55+00:00</updated>
    <author>
      <name>/u/kirklandubermom</name>
      <uri>https://old.reddit.com/user/kirklandubermom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been lurking here forever. Finally have a setup worth sharing - not because it's the most powerful, but because wiring all these pieces together taught me a ton.&lt;/p&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;p&gt;Dual Xeon workstation (neighbor's hand-me-down üôè)&lt;/p&gt; &lt;p&gt;Dual RTX 3090s (48GB VRAM total)&lt;/p&gt; &lt;p&gt;Copilot+ PC with Snapdragon X Elite (NPU side projects)&lt;/p&gt; &lt;p&gt;What I built:&lt;/p&gt; &lt;p&gt;A Telegram bot that orchestrates multiple local AI services - text chat, image analysis, image generation with face swap, voice synthesis, and experimental video. The fun wasn't any single model - it was making them all talk to each other.&lt;/p&gt; &lt;p&gt;What made this interesting to build:&lt;/p&gt; &lt;p&gt;Two-stage vision pipeline: When someone sends an image, a vision model generates a description first, then that description gets passed to the chat model as context. Had to do it this way because my chat model doesn't have native vision, but I wanted it to &amp;quot;know&amp;quot; what it was looking at. Janky? Yes. Works? Also yes.&lt;/p&gt; &lt;p&gt;Voice cloning rabbit hole: Spent way too long recording samples and tuning TTS. The model is picky about input quality - learned the hard way that phone recordings don't cut it. Finally got decent results with a proper mic and ~10 minutes of clean audio.&lt;/p&gt; &lt;p&gt;Face swap pipeline: Face swap model feeding into SDXL. The trick was getting consistent face embeddings so outputs don't look like uncanny valley nightmares. Still not perfect but passable.&lt;/p&gt; &lt;p&gt;Video gen reality check: Video generation is cool for demos but not practical yet. 16-32 frames takes 30-40 seconds and the results are hit or miss. Keeping it in the stack for experimentation but it's not ready for real use. Would need way more VRAM to do anything serious here.&lt;/p&gt; &lt;p&gt;Model upgrades that mattered: Started with Dolphin 2.9 Llama 3 70B, moved to Hermes 4. Night and day difference for staying in character over long conversations. Handles system prompts better without &amp;quot;breaking character&amp;quot; mid-conversation.&lt;/p&gt; &lt;p&gt;NPU side project:&lt;/p&gt; &lt;p&gt;Completely different use case - Phi Silica on a Copilot+ PC running a Flask app for document analysis. Summaries, Q&amp;amp;A on uploaded PDFs, all on-device with wifi off. Built it for enterprise demos at my day job but it's the same &amp;quot;keep it local&amp;quot; philosophy.&lt;/p&gt; &lt;p&gt;The use case:&lt;/p&gt; &lt;p&gt;It's an adult/NSFW companion chatbot. I know that's polarizing here, but the technical challenges are the same whether you're building a flirty chatbot or a customer service bot - multi-modal orchestration, keeping a consistent persona, managing resources across services. Running local means no API costs, no guardrails to fight, and full control over the pipeline.&lt;/p&gt; &lt;p&gt;What I'm still figuring out:&lt;/p&gt; &lt;p&gt;Better image analysis options? Current setup works but feels dated&lt;/p&gt; &lt;p&gt;VRAM management when running multiple models - currently just starting/stopping services manually which is dumb&lt;/p&gt; &lt;p&gt;Anyone solved the &amp;quot;video gen that doesn't suck&amp;quot; problem on consumer hardware?&lt;/p&gt; &lt;p&gt;The full stack for those who want details:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Capability&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Platform/Port&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Text/Chat&lt;/td&gt; &lt;td align="left"&gt;Hermes 4 70B (Q4_K_M)&lt;/td&gt; &lt;td align="left"&gt;LM Studio / 5000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Image Analysis&lt;/td&gt; &lt;td align="left"&gt;LLaVA 7B v1.6 Mistral (Q4_0)&lt;/td&gt; &lt;td align="left"&gt;Ollama / 11434&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Image Generation&lt;/td&gt; &lt;td align="left"&gt;SDXL&lt;/td&gt; &lt;td align="left"&gt;ComfyUI / 8188&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Face Swap&lt;/td&gt; &lt;td align="left"&gt;ReActor + InsightFace (inswapper_128.onnx)&lt;/td&gt; &lt;td align="left"&gt;ComfyUI / 8188&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voice/TTS&lt;/td&gt; &lt;td align="left"&gt;Coqui XTTS v2 (voice cloned)&lt;/td&gt; &lt;td align="left"&gt;Flask / 5001&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Video&lt;/td&gt; &lt;td align="left"&gt;AnimateDiff + FaceID Plus V2&lt;/td&gt; &lt;td align="left"&gt;ComfyUI / 8189&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Happy to answer questions about any part of the stack. Learned most of this from this sub so figured I'd contribute back.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kirklandubermom"&gt; /u/kirklandubermom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg1udz/full_local_ai_stack_on_dual_3090s_multimodal_bot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg1udz/full_local_ai_stack_on_dual_3090s_multimodal_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg1udz/full_local_ai_stack_on_dual_3090s_multimodal_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T07:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf5oj0</id>
    <title>DeepSeek Engram : A static memory unit for LLMs</title>
    <updated>2026-01-17T06:18:14+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeeepSeek AI released a new paper titled &amp;quot;Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models&amp;quot; introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram &lt;strong&gt;adds native memory lookup&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think of it as separating &lt;strong&gt;remembering from reasoning&lt;/strong&gt;. Traditional MoE focuses on conditional computation, Engram introduces &lt;strong&gt;conditional memory&lt;/strong&gt;. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge is &lt;strong&gt;looked up in O(1)&lt;/strong&gt; instead of recomputed.&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;explicit parametric memory&lt;/strong&gt; vs implicit weights only.&lt;/li&gt; &lt;li&gt;Improves reasoning, math, and code performance.&lt;/li&gt; &lt;li&gt;Enables massive memory scaling &lt;strong&gt;without GPU limits&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Frees attention for &lt;strong&gt;global reasoning&lt;/strong&gt; rather than static knowledge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper : &lt;a href="https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf"&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub"&gt;https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T06:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfgiq1</id>
    <title>MCP server that gives local LLMs memory, file access, and a 'conscience' - 100% offline on Apple Silicon</title>
    <updated>2026-01-17T15:45:41+00:00</updated>
    <author>
      <name>/u/TheTempleofTwo</name>
      <uri>https://old.reddit.com/user/TheTempleofTwo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a few weeks and finally got it stable enough to share.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem I wanted to solve:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local LLMs are stateless - they forget everything between sessions&lt;/li&gt; &lt;li&gt;No governance - they'll execute whatever you ask without reflection&lt;/li&gt; &lt;li&gt;Chat interfaces don't give them &amp;quot;hands&amp;quot; to actually do things&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A stack that runs entirely on my Mac Studio M2 Ultra:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LM Studio (chat interface) ‚Üì Hermes-3-Llama-3.1-8B (MLX, 4-bit) ‚Üì Temple Bridge (MCP server) ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ BTB ‚îÇ Threshold ‚îÇ ‚îÇ (filesystem ‚îÇ (governance ‚îÇ ‚îÇ operations) ‚îÇ protocols) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What the AI can actually do:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read/write files in a sandboxed directory&lt;/li&gt; &lt;li&gt;Execute commands (pytest, git, ls, etc.) with an allowlist&lt;/li&gt; &lt;li&gt;Consult &amp;quot;threshold protocols&amp;quot; before taking actions&lt;/li&gt; &lt;li&gt;Log its entire cognitive journey to a JSONL file&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ask for my approval before executing anything dangerous&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The key insight:&lt;/strong&gt; The filesystem itself becomes the AI's memory. Directory structure = classification. File routing = inference. No vector database needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Hermes-3?&lt;/strong&gt; Tested a bunch of models for MCP tool calling. Hermes-3-Llama-3.1-8B was the most stable - no infinite loops, reliable structured output, actually follows the tool schema.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The governance piece:&lt;/strong&gt; Before execution, the AI consults governance protocols and reflects on what it's about to do. When it wants to run a command, I get an approval popup in LM Studio. I'm the &amp;quot;threshold witness&amp;quot; - nothing executes without my explicit OK.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-time monitoring:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tail -f spiral_journey.jsonl | jq &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Shows every tool call, what phase of reasoning the AI is in, timestamps, the whole cognitive trace.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; On M2 Ultra with 36GB unified memory, responses are fast. The MCP overhead is negligible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repos (all MIT licensed):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/temple-bridge"&gt;temple-bridge&lt;/a&gt; - The MCP server that binds it together&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;back-to-the-basics&lt;/a&gt; - Filesystem-as-circuit paradigm&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/threshold-protocols"&gt;threshold-protocols&lt;/a&gt; - Governance framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the three repos&lt;/li&gt; &lt;li&gt;&lt;code&gt;uv sync&lt;/code&gt; in temple-bridge&lt;/li&gt; &lt;li&gt;Add the MCP config to &lt;code&gt;~/.lmstudio/mcp.json&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Load Hermes-3 in LM Studio&lt;/li&gt; &lt;li&gt;Paste the system prompt&lt;/li&gt; &lt;li&gt;Done&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full instructions in the README.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next:&lt;/strong&gt; Working on &amp;quot;governed derive&amp;quot; - the AI can propose filesystem reorganizations based on usage patterns, but only executes after human approval. The goal is AI that can self-organize but with structural restraint built in.&lt;/p&gt; &lt;p&gt;Happy to answer questions. This was a multi-week collaboration between me and several AI systems (Claude, Gemini, Grok) - they helped architect it, I implemented and tested. The lineage is documented in &lt;a href="http://ARCHITECTS.md"&gt;ARCHITECTS.md&lt;/a&gt; if anyone's curious about the process.&lt;/p&gt; &lt;p&gt;- Temple Bridge: &lt;a href="https://github.com/templetwo/temple-bridge"&gt;https://github.com/templetwo/temple-bridge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Back to the Basics: &lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;https://github.com/templetwo/back-to-the-basics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Threshold Protocols: &lt;a href="https://github.com/templetwo/threshold-protocols"&gt;https://github.com/templetwo/threshold-protocols&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üåÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheTempleofTwo"&gt; /u/TheTempleofTwo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T15:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfysbh</id>
    <title>as anyone tried training an llm exclusively on synthetic llm outputs to see if intelligence compounds or just collapses into slop</title>
    <updated>2026-01-18T04:25:23+00:00</updated>
    <author>
      <name>/u/sthduh</name>
      <uri>https://old.reddit.com/user/sthduh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i've been going down a rabbit hole on this and i can't tell if synthetic data is the future or a dead end.&lt;/p&gt; &lt;p&gt;on one hand you have the model collapse paper from shumailov et al. (2023) basically saying if you recursively train on AI-generated data, quality degrades over generations. the tails of the distribution get cut off. you lose the weird, rare, interesting stuff that makes language actually rich. generations later you're left with generic slop. &lt;a href="https://arxiv.org/abs/2305.17493"&gt;&amp;quot;the curse of recursion: training on generated data makes models forget&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but then you look at what's actually working:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;self-instruct showed you can bootstrap a model's capabilities by having it generate its own training examples&lt;/li&gt; &lt;li&gt;constitutional ai is literally a model critiquing and rewriting its own outputs to improve&lt;/li&gt; &lt;li&gt;phi-1 and phi-2 from microsoft were trained heavily on &amp;quot;textbook quality&amp;quot; synthetic data and they punch way above their weight class for their size&lt;/li&gt; &lt;li&gt;alpaca was trained on chatgpt outputs and it... worked? kind of?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;so which is it? does training on synthetic data:&lt;/p&gt; &lt;p&gt;a) inevitably lead to mode collapse and homogenization over generations&lt;/p&gt; &lt;p&gt;b) actually work if you're smart about filtering and curation&lt;/p&gt; &lt;p&gt;c) depend entirely on whether you're mixing in real data or going full synthetic&lt;/p&gt; &lt;p&gt;the shumailov paper seems to suggest the problem is when you go fully recursive with no fresh real data. but phi-2 suggests if your synthetic data is high enough quality and diverse enough, you can actually get emergent capabilities from a tiny model.&lt;/p&gt; &lt;p&gt;has anyone here actually experimented with this? like multiple generations of synthetic training on a local model? i'm curious if there's a threshold where it starts degrading or if the &amp;quot;model collapse&amp;quot; thing is more theoretical than practical.&lt;/p&gt; &lt;p&gt;also tangential question: if model collapse is real and more and more of the internet becomes AI-generated, are we basically poisoning the well for future foundation models? like is there a world where gpt-6 is trained partly on gpt-4 slop and we've already peaked?&lt;/p&gt; &lt;p&gt;would love to hear from anyone who's actually run experiments on this vs just theorizing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sthduh"&gt; /u/sthduh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfysbh/as_anyone_tried_training_an_llm_exclusively_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfysbh/as_anyone_tried_training_an_llm_exclusively_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfysbh/as_anyone_tried_training_an_llm_exclusively_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T04:25:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfwubh</id>
    <title>Using Claude Code with Ollama local models</title>
    <updated>2026-01-18T02:51:52+00:00</updated>
    <author>
      <name>/u/derestine</name>
      <uri>https://old.reddit.com/user/derestine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama v0.14.0 and later are now compatible with the Anthropic &lt;a href="https://docs.anthropic.com/en/api/messages"&gt;Messages API&lt;/a&gt;, making it possible to use tools like &lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt; with open-source models.&lt;/p&gt; &lt;p&gt;Run Claude Code with local models on your machine, or connect to cloud models through ollama.com.&lt;/p&gt; &lt;h1&gt;Usage with Ollama&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Set the environment variables:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export ANTHROPIC_AUTH_TOKEN=ollama export ANTHROPIC_BASE_URL=http://localhost:11434 &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;Run Claude Code with an Ollama model:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;claude --model gpt-oss:20b ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 claude --model gpt-oss:20b &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Connecting to &lt;a href="http://ollama.com"&gt;ollama.com&lt;/a&gt;&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Create an &lt;a href="https://ollama.com/settings/keys"&gt;API key&lt;/a&gt; on &lt;a href="http://ollama.com"&gt;ollama.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Set the environment variables:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export ANTHROPIC_BASE_URL=https://ollama.com export ANTHROPIC_API_KEY=&amp;lt;your-api-key&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;Run Claude Code with a cloud model:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;claude --model glm-4.7:cloud &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Recommended Models&lt;/h1&gt; &lt;h1&gt;Cloud models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;glm-4.7:cloud&lt;/code&gt; - High-performance cloud model&lt;/li&gt; &lt;li&gt;&lt;code&gt;minimax-m2.1:cloud&lt;/code&gt; - Fast cloud model&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-coder:480b&lt;/code&gt; - Large coding model&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Local models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;qwen3-coder&lt;/code&gt; - Excellent for coding tasks&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:20b&lt;/code&gt; - Strong general-purpose model&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:120b&lt;/code&gt; - Larger general-purpose model for more complex tasks&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derestine"&gt; /u/derestine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfwubh/using_claude_code_with_ollama_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfwubh/using_claude_code_with_ollama_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfwubh/using_claude_code_with_ollama_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T02:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfzn6o</id>
    <title>I built a tool that forces 5 AIs to debate and cross-check facts before answering you</title>
    <updated>2026-01-18T05:08:25+00:00</updated>
    <author>
      <name>/u/S_Anv</name>
      <uri>https://old.reddit.com/user/S_Anv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfzn6o/i_built_a_tool_that_forces_5_ais_to_debate_and/"&gt; &lt;img alt="I built a tool that forces 5 AIs to debate and cross-check facts before answering you" src="https://preview.redd.it/dbrlv5nij1eg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a41382da4b7b590da3386920c4717834ccb96eb" title="I built a tool that forces 5 AIs to debate and cross-check facts before answering you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;It‚Äôs a self-hosted platform designed to solve the issue of blind trust in LLMs&lt;/p&gt; &lt;p&gt;If someone ready to test and leave a review, you are welcome! I'm waiting for your opinions and reviews&lt;/p&gt; &lt;p&gt;Github &lt;a href="https://github.com/KeaBase/kea-research"&gt;https://github.com/KeaBase/kea-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S_Anv"&gt; /u/S_Anv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dbrlv5nij1eg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfzn6o/i_built_a_tool_that_forces_5_ais_to_debate_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfzn6o/i_built_a_tool_that_forces_5_ais_to_debate_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T05:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfxu1r</id>
    <title>CPA-Qwen3-8B-v0: A Specialized LLM for Accounting, Auditing, and Regulatory Compliance</title>
    <updated>2026-01-18T03:39:19+00:00</updated>
    <author>
      <name>/u/Lich_Amnesia</name>
      <uri>https://old.reddit.com/user/Lich_Amnesia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, just sharing a model release that might be useful for those working in accounting technology, financial auditing, or building tools for CPAs.&lt;/p&gt; &lt;p&gt;Model on Hugging Face: &lt;a href="https://huggingface.co/AudCor/cpa-qwen3-8b-v0"&gt;https://huggingface.co/AudCor/cpa-qwen3-8b-v0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPA-Qwen3-8B-v0 is a specialized fine-tune of Qwen3-8B, trained by AudCor on the Finance-Instruct-500k dataset. Unlike general financial models, this model is specifically optimized to adopt the persona of a Certified Public Accountant (CPA).&lt;/p&gt; &lt;p&gt;Key capabilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPA Persona &amp;amp; Professional Skepticism: It frames answers with the accuracy and caution expected of a licensed professional, rather than just generating generic financial text.&lt;/li&gt; &lt;li&gt;Regulatory Adherence: Strong knowledge of GAAP, IFRS, and tax codes, suitable for interpreting complex compliance requirements.&lt;/li&gt; &lt;li&gt;Exam-Grade Reasoning: Benchmarked against the logic required for rigorous CPA exam problems (FAR, AUD, REG), including handling complex multi-step scenarios.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;--- &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Or if you prefer my raw post before AI rewrite it)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, I wanted to share a project I've been working on. It's a fine-tune of Qwen3-8B specifically targeted at the accounting domain (CPA stuff, GAAP, IFRS, Auditing).&lt;/p&gt; &lt;p&gt;Most &amp;quot;finance&amp;quot; models are just trained on general financial news or stock data. I trained this one on the &lt;code&gt;Finance-Instruct-500k&lt;/code&gt; dataset to actually handle strict regulatory questions and audit logic. It's meant to act more like a professional accountant than a stock broker.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/AudCor/cpa-qwen3-8b-v0"&gt;https://huggingface.co/AudCor/cpa-qwen3-8b-v0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you find it useful or if it hallucinates on any specific tax codes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lich_Amnesia"&gt; /u/Lich_Amnesia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfxu1r/cpaqwen38bv0_a_specialized_llm_for_accounting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfxu1r/cpaqwen38bv0_a_specialized_llm_for_accounting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfxu1r/cpaqwen38bv0_a_specialized_llm_for_accounting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T03:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfsju5</id>
    <title>Personal-Guru: an open-source, free, local-first alternative to AI tutors and NotebookLM</title>
    <updated>2026-01-17T23:38:58+00:00</updated>
    <author>
      <name>/u/rishabhbajpai24</name>
      <uri>https://old.reddit.com/user/rishabhbajpai24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs make incredible encyclopedias‚Äîbut honestly, pretty terrible teachers.&lt;/p&gt; &lt;p&gt;You can chat with ChatGPT for an hour about a complex topic, but without a syllabus or clear milestones, you usually end up with a long chat history and very little retained knowledge.&lt;/p&gt; &lt;p&gt;Most existing tools fall into one of these buckets:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unstructured chatbots&lt;/li&gt; &lt;li&gt;Document analyzers (you need to already have notes)&lt;/li&gt; &lt;li&gt;Expensive subscription-based platforms&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We just released the &lt;strong&gt;beta of Personal-Guru&lt;/strong&gt;, a &lt;strong&gt;local-first, open-source learning system&lt;/strong&gt; that doesn‚Äôt just ‚Äúchat‚Äù ‚Äî it &lt;strong&gt;builds a full curriculum for you from scratch&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Our core belief is simple:&lt;br /&gt; &lt;strong&gt;Education and access to advanced AI should be free, private, and offline-capable.&lt;/strong&gt;&lt;br /&gt; No subscriptions. No cloud lock-in. No data leaving your machine.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;Repo:&lt;/strong&gt;&lt;a href="https://github.com/Rishabh-Bajpai/Personal-Guru"&gt; https://github.com/Rishabh-Bajpai/Personal-Guru&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üöÄ What makes Personal-Guru different?&lt;/h1&gt; &lt;p&gt;Instead of free-form chat, you give it a &lt;strong&gt;topic&lt;/strong&gt; (e.g., &lt;em&gt;Quantum Physics&lt;/em&gt; or &lt;em&gt;Sourdough Baking&lt;/em&gt;) and it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üìö Generates a &lt;strong&gt;structured syllabus&lt;/strong&gt; (chapters, sections, key concepts)&lt;/li&gt; &lt;li&gt;üß† Creates &lt;strong&gt;interactive learning content&lt;/strong&gt; (quizzes, flashcards, voice Q&amp;amp;A)&lt;/li&gt; &lt;li&gt;üîí Runs &lt;strong&gt;100% locally&lt;/strong&gt; (powered by OpenAI compatible API (LMStudio, Ollama, etc) ‚Äî your data stays with you)&lt;/li&gt; &lt;li&gt;üéß Supports &lt;strong&gt;multi-modal learning&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reel Mode&lt;/strong&gt; (short-form, TikTok-style learning)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Podcast Mode&lt;/strong&gt; (audio-first learning)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;‚öîÔ∏è Why Personal-Guru? (Quick comparison)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;ü¶â Personal-Guru&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;üìì NotebookLM&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;‚ú® Gemini Guided Learning&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;üéì&lt;/strong&gt; &lt;a href="http://ai-tutor.ai"&gt;&lt;strong&gt;ai-tutor.ai&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Core Philosophy&lt;/td&gt; &lt;td align="left"&gt;Structured Curriculum Generator&lt;/td&gt; &lt;td align="left"&gt;Document Analyzer (RAG)&lt;/td&gt; &lt;td align="left"&gt;Conversational Study Partner&lt;/td&gt; &lt;td align="left"&gt;Course Generator&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Privacy&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100% Local&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Cloud (Google)&lt;/td&gt; &lt;td align="left"&gt;Cloud (Google)&lt;/td&gt; &lt;td align="left"&gt;Cloud (Proprietary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Cost&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Free &amp;amp; Open Source&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Free (for now)&lt;/td&gt; &lt;td align="left"&gt;$20/mo&lt;/td&gt; &lt;td align="left"&gt;Freemium (~$10+/mo)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Input Needed&lt;/td&gt; &lt;td align="left"&gt;Just a topic&lt;/td&gt; &lt;td align="left"&gt;Your documents&lt;/td&gt; &lt;td align="left"&gt;Chat prompts&lt;/td&gt; &lt;td align="left"&gt;Topic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Audio Features&lt;/td&gt; &lt;td align="left"&gt;Local podcast + TTS&lt;/td&gt; &lt;td align="left"&gt;Audio overviews&lt;/td&gt; &lt;td align="left"&gt;Standard TTS&lt;/td&gt; &lt;td align="left"&gt;Limited&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Offline&lt;/td&gt; &lt;td align="left"&gt;‚úÖ Yes&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;‚ÄúReel‚Äù Mode&lt;/td&gt; &lt;td align="left"&gt;‚úÖ Yes&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üõ†Ô∏è Tech Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Flask + multi-agent system&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Engine:&lt;/strong&gt; Ollama (Llama 3, Mistral, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio:&lt;/strong&gt; Speaches (Kokoro-82M) for high-quality local TTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; Responsive web UI with voice input&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;ü§ù Call for Contributors&lt;/h1&gt; &lt;p&gt;This is an &lt;strong&gt;early beta&lt;/strong&gt;, and we have big plans.&lt;/p&gt; &lt;p&gt;If you believe that &lt;strong&gt;AI-powered education should be free, open, and private&lt;/strong&gt;, we‚Äôd love your help. We‚Äôre especially looking for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Developers interested in &lt;strong&gt;local AI / agent systems&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Contributors passionate about &lt;strong&gt;EdTech&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Feedback on &lt;strong&gt;structured learning flows vs. chat-based learning&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and let us know what you think:&lt;br /&gt; üëâ&lt;a href="https://github.com/Rishabh-Bajpai/Personal-Guru"&gt; https://github.com/Rishabh-Bajpai/Personal-Guru&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rishabhbajpai24"&gt; /u/rishabhbajpai24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsju5/personalguru_an_opensource_free_localfirst/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsju5/personalguru_an_opensource_free_localfirst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsju5/personalguru_an_opensource_free_localfirst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg2592</id>
    <title>Speculative Decoding: Turning Memory-Bound Inference into Compute-Bound Verification (Step-by-Step)</title>
    <updated>2026-01-18T07:24:22+00:00</updated>
    <author>
      <name>/u/No_Ask_1623</name>
      <uri>https://old.reddit.com/user/No_Ask_1623</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of us assume LLM inference is slow because &amp;quot;matrix multiplication is hard.&amp;quot; That‚Äôs actually false.&lt;/p&gt; &lt;p&gt;For a batch size of 1 (which is standard for local inference/chat), your GPU is almost entirely &lt;strong&gt;Memory Bandwidth Bound&lt;/strong&gt;. The bottleneck isn't doing the math; it's moving the 70GB+ of weights from VRAM to the compute units. The Arithmetic Logic Units (ALUs) are spending most of their time idle, waiting for data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speculative Decoding&lt;/strong&gt; exploits this idle time to give us a &amp;quot;free lunch&amp;quot;‚Äî2x-3x speedups with &lt;strong&gt;mathematically identical&lt;/strong&gt; outputs.&lt;/p&gt; &lt;p&gt;Here is the core mechanism derived step-by-step:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The Setup: Drafter vs. Target&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We use a tiny &amp;quot;Drafter&amp;quot; model (e.g., a 100M param model) alongside our massive &amp;quot;Target&amp;quot; model (e.g., Llama-70B).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Drafter is cheap to run. It quickly spits out a &amp;quot;draft&amp;quot; of 5 tokens: &lt;code&gt;[The, cat, sat, on, the]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Standard decoding would run the Target model 5 times (serial) to generate this.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The Trick: Parallel Verification&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We feed all 5 draft tokens into the Target model in a single forward pass.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Because inference is memory-bound, loading the weights for 1 token takes roughly the same time as loading them for 5 tokens.&lt;/li&gt; &lt;li&gt;The Target model outputs the probabilities for all positions simultaneously.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The Rejection Sampling (The Math)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This isn't an approximation. We use rejection sampling to ensure the distribution matches the Target model exactly.&lt;/p&gt; &lt;p&gt;Let q(x) be the Drafter's probability and p(x) be the Target's probability.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Case A (q(x) &amp;lt; p(x)):&lt;/strong&gt; The Target thinks the token is &lt;em&gt;more&lt;/em&gt; likely than the Drafter did. &lt;strong&gt;ACCEPT.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Case B (q(x) &amp;gt; p(x)):&lt;/strong&gt; The Drafter was overconfident. We reject the token with probability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If a token is rejected, we discard it and everything after it, then resample from the adjusted difference distribution. Even if we only accept 3 out of 5 tokens, we generated 3 tokens for the &amp;quot;cost&amp;quot; of 1 Target run.&lt;/p&gt; &lt;p&gt;Why this matters:&lt;/p&gt; &lt;p&gt;This converts a memory-bound operation (waiting for weights) into a compute-bound operation (doing more math on loaded weights), maximizing hardware utilization without retraining.&lt;/p&gt; &lt;p&gt;I wrote a full deep dive with the complete derivation, the logic behind the &amp;quot;Sweet Spot&amp;quot; (gamma), and modern variants like Medusa/EAGLE here: &lt;a href="https://pub.towardsai.net/why-your-llm-should-be-guessing-breaking-the-sequential-curse-50496633f8ff"&gt;https://pub.towardsai.net/why-your-llm-should-be-guessing-breaking-the-sequential-curse-50496633f8ff&lt;/a&gt; if you want the details.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Ask_1623"&gt; /u/No_Ask_1623 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg2592/speculative_decoding_turning_memorybound/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg2592/speculative_decoding_turning_memorybound/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg2592/speculative_decoding_turning_memorybound/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T07:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg55aa</id>
    <title>Self-improving coding workflow experiment: AI generates tests, fixes bugs autonomously, mixed results</title>
    <updated>2026-01-18T10:21:30+00:00</updated>
    <author>
      <name>/u/Independent_Plum_489</name>
      <uri>https://old.reddit.com/user/Independent_Plum_489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with a workflow where the AI writes code, generates test cases, runs them, then fixes failures without me intervening. Inspired by some research on self-play training but applied to actual coding tasks.&lt;/p&gt; &lt;p&gt;Basic setup: gave it a loose spec for a JSON parser, let it write the implementation, generate edge case tests, then iterate on failures. Used GPT and Claude through Verdent to compare approaches.&lt;/p&gt; &lt;p&gt;Some things worked well. Simple functions with clear success criteria like parsers, validators, formatters. It caught edge cases I wouldn't have thought of. Iteration speed was fast once the loop started.&lt;/p&gt; &lt;p&gt;Other things didn't work. Complex architecture decisions had it refactoring in circles. No sense of &amp;quot;good enough&amp;quot; so it would optimize forever if I let it. Generated tests were sometimes too narrow or too broad. Broke a working auth flow trying to &amp;quot;improve&amp;quot; it, had to rollback.&lt;/p&gt; &lt;p&gt;The verification problem is real. For math or parsing you can check correctness objectively. For UI or business logic there's no automatic way to verify &amp;quot;this is what the user wanted.&amp;quot;&lt;/p&gt; &lt;p&gt;Cost was around $22 in tokens for about 2 hours of iterations. Faster than writing tests myself but the code quality was inconsistent. Some functions were clean, others were over-engineered.&lt;/p&gt; &lt;p&gt;Not sure this is actually better than traditional TDD. You still need to review everything and the AI doesn't understand tradeoffs. It'll optimize for test coverage over readability.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Plum_489"&gt; /u/Independent_Plum_489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg55aa/selfimproving_coding_workflow_experiment_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg55aa/selfimproving_coding_workflow_experiment_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg55aa/selfimproving_coding_workflow_experiment_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T10:21:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfpomi</id>
    <title>[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU</title>
    <updated>2026-01-17T21:39:36+00:00</updated>
    <author>
      <name>/u/ThisGonBHard</name>
      <uri>https://old.reddit.com/user/ThisGonBHard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt; &lt;img alt="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" src="https://external-preview.redd.it/BkpkFFoxQzTdVFBwygr_NjC6jb0CW1UxI49hdIPceBg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2ae4d7ac52eeeba792fbdb62506b06e57290b67" title="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems quite interesting, in getting the 48 GB cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThisGonBHard"&gt; /u/ThisGonBHard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/TcRGBeOENLg?si=2CKaZR7Dj0x89MMU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T21:39:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qftdr4</id>
    <title>AI insiders seek to poison the data that feeds them</title>
    <updated>2026-01-18T00:14:54+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"&gt; &lt;img alt="AI insiders seek to poison the data that feeds them" src="https://external-preview.redd.it/oZsMR98JWtXvCHBS_WeIPqnrR9GLtUCvLvVVcRNn-mI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=176b3305413cf55f808df5dcf6836f7c0b89e27e" title="AI insiders seek to poison the data that feeds them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theregister.com/2026/01/11/industry_insiders_seek_to_poison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T00:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfmc05</id>
    <title>China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)</title>
    <updated>2026-01-17T19:25:24+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt; &lt;img alt="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" src="https://external-preview.redd.it/TpKYg79IWzebupDqkzAodJruBP4N0VFsDaZESasEpKQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea8968e021234c9b599b354059d32de716c52bed" title="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone else posted about this, but never posted a transcript, so I found one online.&lt;/p&gt; &lt;p&gt;Lot of interesting stuff about China vs US, paths to AGI, compute, marketing etc.&lt;/p&gt; &lt;p&gt;Unfortunately Moonshot seems to have a very short section. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/the-all-star-chinese-ai-conversation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T19:25:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg4d4t</id>
    <title>What we learned processing 1M+ emails for context engineering</title>
    <updated>2026-01-18T09:35:07+00:00</updated>
    <author>
      <name>/u/EnoughNinja</name>
      <uri>https://old.reddit.com/user/EnoughNinja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We spent the last year building systems to turn email into structured context for AI agents. Processed over a million emails to figure out what actually works.&lt;/p&gt; &lt;p&gt;Some things that weren't obvious going in:&lt;/p&gt; &lt;p&gt;Thread reconstruction is way harder than I thought. You've got replies, forwards, people joining mid-conversation, decisions getting revised three emails later. Most systems just concatenate text in chronological order and hope the LLM figures it out, but that falls apart fast because you lose who said what and why it matters.&lt;/p&gt; &lt;p&gt;Attachments are half the conversation. PDFs, contracts, invoices, they're not just metadata, they're actual content that drives decisions. We had to build OCR and structure parsing so the system can actually read them, not just know they exist as file names.&lt;/p&gt; &lt;p&gt;Multilingual threads are more common than you'd think. People switch languages mid-conversation all the time, especially in global teams. Semantic search that works well in English completely breaks down when you need cross-language understanding.&lt;/p&gt; &lt;p&gt;Zero data retention is non-negotiable if you want enterprise customers. We discard every prompt after processing. Memory gets reconstructed on demand from the original sources, nothing stored. Took us way longer to build but there's no other way to get past compliance teams.&lt;/p&gt; &lt;p&gt;Performance-wise we're hitting around 200ms for retrieval and about 3 seconds to first token even on massive inboxes. &lt;/p&gt; &lt;p&gt;Most of the time is in the reasoning step, not the search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnoughNinja"&gt; /u/EnoughNinja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T09:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfkn3a</id>
    <title>Best "End of world" model that will run on 24gb VRAM</title>
    <updated>2026-01-17T18:21:20+00:00</updated>
    <author>
      <name>/u/gggghhhhiiiijklmnop</name>
      <uri>https://old.reddit.com/user/gggghhhhiiiijklmnop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps, I'm feeling in a bit of a omg the world is ending mood and have been amusing myself by downloading and hoarding a bunch of data - think wikipedia, wiktionary, wikiversity, khan academy, etc etc&lt;/p&gt; &lt;p&gt;What's your take on the smartest / best model(s) to download and store - they need to fit and run on my 24gb VRAM / 64gb RAM PC.? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gggghhhhiiiijklmnop"&gt; /u/gggghhhhiiiijklmnop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg48z8</id>
    <title>Newelle 1.2 released</title>
    <updated>2026-01-18T09:28:09+00:00</updated>
    <author>
      <name>/u/iTzSilver_YT</name>
      <uri>https://old.reddit.com/user/iTzSilver_YT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"&gt; &lt;img alt="Newelle 1.2 released" src="https://b.thumbs.redditmedia.com/LkuhPhoU8yX94dk4Ih1TJttPbdGMxJp3kQLOX0r9qXA.jpg" title="Newelle 1.2 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newelle, AI assistant for Linux, has been updated to 1.2! You can download it from &lt;a href="https://flathub.org/en/apps/io.github.qwersyk.Newelle"&gt;FlatHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ö°Ô∏è Add llama.cpp, with options to recompile it with any backend&lt;br /&gt; üìñ Implement a new model library for ollama / llama.cpp&lt;br /&gt; üîé Implement hybrid search, improving document reading&lt;/p&gt; &lt;p&gt;üíª Add command execution tool&lt;br /&gt; üóÇ Add tool groups&lt;br /&gt; üîó Improve MCP server adding, supporting also STDIO for non flatpak&lt;br /&gt; üìù Add semantic memory handler&lt;br /&gt; üì§ Add ability to import/export chats&lt;br /&gt; üìÅ Add custom folders to the RAG index&lt;br /&gt; ‚ÑπÔ∏è Improved message information menu, showing the token count and token speed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iTzSilver_YT"&gt; /u/iTzSilver_YT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qg48z8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T09:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfq9ez</id>
    <title>The Search for Uncensored AI (That Isn‚Äôt Adult-Oriented)</title>
    <updated>2026-01-17T22:03:23+00:00</updated>
    <author>
      <name>/u/Fun-Situation-4358</name>
      <uri>https://old.reddit.com/user/Fun-Situation-4358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been trying to find an AI that‚Äôs genuinely unfiltered &lt;em&gt;and&lt;/em&gt; technically advanced, uncensored something that can reason freely without guardrails killing every interesting response.&lt;/p&gt; &lt;p&gt;Instead, almost everything I run into is marketed as ‚Äúuncensored,‚Äù but it turns out to be optimized for low-effort adult use rather than actual intelligence or depth.&lt;/p&gt; &lt;p&gt;It feels like the space between heavily restricted corporate AI and shallow adult-focused models is strangely empty, and I‚Äôm curious why that gap still exists...&lt;/p&gt; &lt;p&gt;Is there any &lt;strong&gt;uncensored or lightly filtered AI&lt;/strong&gt; that focuses on reasoning, creativity,uncensored technology or serious problem-solving instead? I‚Äôm open to self-hosted models, open-source projects, or lesser-known platforms. Suggestions appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Situation-4358"&gt; /u/Fun-Situation-4358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T22:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfscp5</id>
    <title>128GB VRAM quad R9700 server</title>
    <updated>2026-01-17T23:30:26+00:00</updated>
    <author>
      <name>/u/Ulterior-Motive_</name>
      <uri>https://old.reddit.com/user/Ulterior-Motive_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt; &lt;img alt="128GB VRAM quad R9700 server" src="https://b.thumbs.redditmedia.com/SbBMg1b6qTh913lUa8uWDuyYZrIwJ_ECUuUVvuWh_qA.jpg" title="128GB VRAM quad R9700 server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a sequel to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fqwrvg/64gb_vram_dual_mi100_server/"&gt;previous thread&lt;/a&gt; from 2024.&lt;/p&gt; &lt;p&gt;I originally planned to pick up another pair of MI100s and an Infinity Fabric Bridge, and I picked up a lot of hardware upgrades over the course of 2025 in preparation for this. Notably, faster, double capacity memory (last February, well before the current price jump), another motherboard, higher capacity PSU, etc. But then I saw benchmarks for the R9700, particularly in the &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15021"&gt;llama.cpp ROCm thread&lt;/a&gt;, and saw the much better prompt processing performance for a small token generation loss. The MI100 also went up in price to about $1000, so factoring in the cost of a bridge, it'd come to about the same price. So I sold the MI100s, picked up 4 R9700s and called it a day.&lt;/p&gt; &lt;p&gt;Here's the specs and BOM. Note that the CPU and SSD were taken from the previous build, and the internal fans came bundled with the PSU as part of a deal:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Number&lt;/th&gt; &lt;th align="left"&gt;Unit Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7 5700X&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$160.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;Corsair Vengance LPX 64GB (2 x 32GB) DDR4 3600MHz C18&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;$105.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;PowerColor AMD Radeon AI PRO R9700 32GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;$1,300.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Motherboard&lt;/td&gt; &lt;td align="left"&gt;MSI MEG X570 GODLIKE Motherboard&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$490.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Storage&lt;/td&gt; &lt;td align="left"&gt;Inland Performance 1TB NVMe SSD&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PSU&lt;/td&gt; &lt;td align="left"&gt;Super Flower Leadex Titanium 1600W 80+ Titanium&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$440.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Internal Fans&lt;/td&gt; &lt;td align="left"&gt;Super Flower MEGACOOL 120mm fan, Triple-Pack&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case Fans&lt;/td&gt; &lt;td align="left"&gt;Noctua NF-A14 iPPC-3000 PWM&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;$30.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU Heatsink&lt;/td&gt; &lt;td align="left"&gt;AMD Wraith Prism aRGB CPU Cooler&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$20.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Fan Hub&lt;/td&gt; &lt;td align="left"&gt;Noctua NA-FH1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$45.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case&lt;/td&gt; &lt;td align="left"&gt;Phanteks Enthoo Pro 2 Server Edition&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$190.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;$7,035.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;128GB VRAM, 128GB RAM for offloading, all for less than the price of a RTX 6000 Blackwell.&lt;/p&gt; &lt;p&gt;Some benchmarks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;n_ubatch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;6524.91 ¬± 11.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;90.89 ¬± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2113.82 ¬± 2.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;72.51 ¬± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1725.46 ¬± 5.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.75 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1110.02 ¬± 3.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.53 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;821.10 ¬± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;38.88 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1928.45 ¬± 3.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.09 ¬± 0.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2082.04 ¬± 4.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.78 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;42.62 ¬± 7.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.58 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;A few final observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;glm4 moe and minimax-m2 are actually GLM-4.6V and MiniMax-M2.1, respectively.&lt;/li&gt; &lt;li&gt;There's an open issue for Qwen3-Next at the moment; recent optimizations caused some pretty hefty prompt processing regressions. The numbers here are pre #18683, in case the exact issue gets resolved.&lt;/li&gt; &lt;li&gt;A word on the Q8 quant of MiniMax-M2.1; &lt;code&gt;--fit on&lt;/code&gt; isn't supported on llama-bench, so I can't give an apples to apples comparison to simply reducing the number of gpu layers, but it's also extremely unreliable for me in llama-server, giving me HIP error 906 on the first generation. Out of a dozen or so attempts, I've gotten it to work once, with a TG around 8.5 t/s, but take that with a grain of salt. Otherwise, maybe the quality jump is worth letting it run overnight? You be the judge. It also takes 2 hours to load, but that could be because I'm loading it off external storage.&lt;/li&gt; &lt;li&gt;The internal fan mount on the case only has screws on one side; in the intended configuration, the holes for power cables are on the opposite side of where the GPU power sockets are, meaning the power cables will block airflow from the fans. How they didn't see this, I have no idea. Thankfully, it stays in place from a friction fit if you flip it 180 like I did. Really, I probably could have gone without it, it was mostly a consideration for when I was still going with MI100s, but the fans were free anyway.&lt;/li&gt; &lt;li&gt;I really, really wanted to go AM5 for this, but there just isn't a board out there with 4 full sized PCIe slots spaced for 2 slot GPUs. At best you can fit 3 and then cover up one of them. But if you need a bazillion m.2 slots you're golden /s. You might then ask why I didn't go for Threadripper/Epyc, and that's because I was worried about power consumption and heat. I didn't want to mess with risers and open rigs, so I found the one AM4 board that could do this, even if it comes at the cost of RAM speeds/channels and slower PCIe speeds.&lt;/li&gt; &lt;li&gt;The MI100s and R9700s didn't play nice for the brief period of time I had 2 of both. I didn't bother troubleshooting, just shrugged and sold them off, so it may have been a simple fix but FYI.&lt;/li&gt; &lt;li&gt;Going with a 1 TB SSD in my original build was a mistake, even 2 would have made a world of difference. Between LLMs, image generation, TTS, ect. I'm having trouble actually taking advantage of the extra VRAM with less quantized models due to storage constraints, which is why my benchmarks still have a lot of 4-bit quants despite being able to easily do 8-bit ones.&lt;/li&gt; &lt;li&gt;I don't know how to control the little LCD display on the board. I'm not sure there is a way on Linux. A shame.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ulterior-Motive_"&gt; /u/Ulterior-Motive_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qfscp5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfv1ms</id>
    <title>Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.</title>
    <updated>2026-01-18T01:28:57+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"&gt; &lt;img alt="Qwen 4 might be a long way off !? Lead Dev says they are &amp;quot;slowing down&amp;quot; to focus on quality." src="https://preview.redd.it/ylsevy04f0eg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf47eb2c12055fdb3e08f36d4d3746a234d630ff" title="Qwen 4 might be a long way off !? Lead Dev says they are &amp;quot;slowing down&amp;quot; to focus on quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ylsevy04f0eg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T01:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
