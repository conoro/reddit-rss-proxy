<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-14T21:06:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o6c9vs</id>
    <title>I built a fully automated AI podcast generator that connects to ollama</title>
    <updated>2025-10-14T10:54:44+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a fun side project ‚Äî an &lt;strong&gt;AI-powered podcast generator&lt;/strong&gt; built entirely with &lt;strong&gt;Ollama (for the LLM)&lt;/strong&gt; and &lt;strong&gt;Piper (for TTS)&lt;/strong&gt;. üéôÔ∏è&lt;/p&gt; &lt;p&gt;The system takes any topic and automatically:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Write a complete script&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generates the audio&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôve &lt;strong&gt;open-sourced the full project&lt;/strong&gt; on GitHub so anyone can explore, use, or contribute to it. If you‚Äôre into AI, audio, or automation, I‚Äôd love your feedback and ideas!&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Laszlobeer/AI-podcast"&gt;https://github.com/Laszlobeer/AI-podcast&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6c9vs/i_built_a_fully_automated_ai_podcast_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6c9vs/i_built_a_fully_automated_ai_podcast_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6c9vs/i_built_a_fully_automated_ai_podcast_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T10:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o65di4</id>
    <title>Nvidia DGX Spark reviews started</title>
    <updated>2025-10-14T03:54:37+00:00</updated>
    <author>
      <name>/u/raphaelamorim</name>
      <uri>https://old.reddit.com/user/raphaelamorim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"&gt; &lt;img alt="Nvidia DGX Spark reviews started" src="https://external-preview.redd.it/uAv19XEYpnCDKkb7y0-bzGB8la4s2d7ck6vl-XJBRac.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=576e994d52db2badc52382731998578b9ff595e5" title="Nvidia DGX Spark reviews started" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably start selling on October 15th&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raphaelamorim"&gt; /u/raphaelamorim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/zs-J9sKxvoM?si=237f_mBVyLH7QBOE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T03:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6iyhu</id>
    <title>Mi50 replacement over P40</title>
    <updated>2025-10-14T15:40:37+00:00</updated>
    <author>
      <name>/u/gamma647</name>
      <uri>https://old.reddit.com/user/gamma647</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a P40 in my server. Would it be worth it to swap the p40 for a Mi50 or maybe 2 Mi50s?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamma647"&gt; /u/gamma647 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iyhu/mi50_replacement_over_p40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iyhu/mi50_replacement_over_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iyhu/mi50_replacement_over_p40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6163l</id>
    <title>DGX Spark review with benchmark</title>
    <updated>2025-10-14T00:33:01+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"&gt; &lt;img alt="DGX Spark review with benchmark" src="https://external-preview.redd.it/WNdw4kTz_uFbrszyWcTmBGBzFo8R71Bs5ZxJc5c0h-o.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6226efb1a534fbfbdcc59966a365bcdb316c259" title="DGX Spark review with benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As expected, not the best performer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/-3r2woTQjec?si=PruuNNLJVTwCYvC7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o69jfe</id>
    <title>Still no qwen3 next 80b gguf?</title>
    <updated>2025-10-14T08:05:39+00:00</updated>
    <author>
      <name>/u/LebiaseD</name>
      <uri>https://old.reddit.com/user/LebiaseD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it coming will it come?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LebiaseD"&gt; /u/LebiaseD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6jn0u</id>
    <title>Best uncensored Qwen 3 based LLM? 8B or less?</title>
    <updated>2025-10-14T16:05:30+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thx.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6jn0u/best_uncensored_qwen_3_based_llm_8b_or_less/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6jn0u/best_uncensored_qwen_3_based_llm_8b_or_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6jn0u/best_uncensored_qwen_3_based_llm_8b_or_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T16:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o623qi</id>
    <title>I tested if tiny LLMs can self-improve through memory: Qwen3-1.7B gained +8% accuracy on MATH problems</title>
    <updated>2025-10-14T01:16:29+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;TL;DR&lt;/h2&gt; &lt;p&gt;Implemented Google's ReasoningBank paper on small models (1.7B params). Built a memory system that extracts reasoning strategies from successful solutions and retrieves them for similar problems. &lt;strong&gt;Result: 1.7B model went from 40% ‚Üí 48% accuracy on MATH Level 3-4 problems (+20% relative improvement).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smaller models benefited MORE than larger ones.&lt;/strong&gt; Afer phase 1 is finished tuning phase 2 will attempt to answer, &amp;quot;can the model recursively improve by fine-tuning on its own successful traces?&amp;quot;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What I Built&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;reasoning-bank-slm&lt;/strong&gt; - Testing if small language models can bootstrap their reasoning ability through: 1. &lt;strong&gt;Memory extraction&lt;/strong&gt;: When the model solves a problem, extract generalizable strategies 2. &lt;strong&gt;Semantic retrieval&lt;/strong&gt;: For new problems, retrieve relevant strategies from memory 3. &lt;strong&gt;Guided solving&lt;/strong&gt;: Inject retrieved strategies as hints into the prompt 4. &lt;strong&gt;Recursive loop&lt;/strong&gt; (Phase 2): Fine-tune the model on successful reasoning traces, repeat&lt;/p&gt; &lt;p&gt;Full code on GitHub: &lt;a href="https://github.com/Lanerra/reasoning-bank-slm"&gt;https://github.com/Lanerra/reasoning-bank-slm&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Experimental Setup&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; - Ryzen 9 7950X, 128GB RAM - RTX 4090 + RTX 3090 - Running llama-server locally&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models tested:&lt;/strong&gt; - Qwen3-1.7B-Instruct (primary) - Qwen3-4B-Instruct (comparison) - Qwen3-Embedding-0.6B (retrieval)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; MATH Level 3-4 (harder than GSM8K) - 100 training problems ‚Üí build memory bank - 100 test problems ‚Üí baseline vs memory-augmented&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Design features:&lt;/strong&gt; - Answer leak prevention (filters memories containing expected answer) - Wilson confidence intervals for statistical rigor - Deterministic seeding for reproducibility&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Phase 1 Results (Qwen3-1.7B)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;With Memory&lt;/th&gt; &lt;th&gt;Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Accuracy&lt;/td&gt; &lt;td&gt;40.0%&lt;/td&gt; &lt;td&gt;48.0%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;+8.0%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Problems solved&lt;/td&gt; &lt;td&gt;40/100&lt;/td&gt; &lt;td&gt;48/100&lt;/td&gt; &lt;td&gt;+8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Improvements&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Regressions&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Net effect: +8 problems (2:1 improvement ratio)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Memory bank: 223 strategies extracted from training set&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What Actually Improved&lt;/h2&gt; &lt;p&gt;Sample problems where memory helped:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Complex plane geometry:&lt;/strong&gt; - Baseline: Failed (wrong format) - Retrieved: &amp;quot;Vector Magnitude Method&amp;quot; - Result: ‚úì Correct (25œÄ)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Polynomial analysis:&lt;/strong&gt; - Baseline: Failed (no answer) - Retrieved: &amp;quot;Equate Target Value to Function&amp;quot; - Result: ‚úì Correct (5)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Fibonacci series summation:&lt;/strong&gt; - Baseline: Failed - Retrieved: &amp;quot;Coefficient Multiplication and Summation&amp;quot; - Result: ‚úì Correct (1)&lt;/p&gt; &lt;p&gt;These aren't edge cases - the retrieved strategies were genuinely applicable.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Regressions (The Honest Part)&lt;/h2&gt; &lt;p&gt;8 problems got worse with memory. All showed the same pattern: model failed to produce an answer (not wrong answer, but no answer at all).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; 223 memories is too many. Retrieval pulls less-relevant strategies ‚Üí context bloat ‚Üí model confusion.&lt;/p&gt; &lt;p&gt;Supporting evidence: Runs with fewer memories (10, 40) had zero regressions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix for Phase 2:&lt;/strong&gt; Better retrieval filtering, quality thresholds, or reduce k.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Comparison: Model Size Matters&lt;/h2&gt; &lt;p&gt;Tested both 1.7B and 4B on same problems:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;With Memory&lt;/th&gt; &lt;th&gt;Improvement&lt;/th&gt; &lt;th&gt;Regressions&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4B&lt;/td&gt; &lt;td&gt;76%&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;+4%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.7B&lt;/td&gt; &lt;td&gt;40%&lt;/td&gt; &lt;td&gt;48%&lt;/td&gt; &lt;td&gt;+8%&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Key insight:&lt;/strong&gt; Smaller models benefit more from memory but are more fragile. The 4B already knows most strategies; the 1.7B needs the hints.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Why This Might Matter&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Small models can punch above their weight&lt;/strong&gt; with the right scaffolding&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory &amp;gt; parameters&lt;/strong&gt; for certain reasoning tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opens path to recursive self-improvement&lt;/strong&gt;: If Phase 2 works (fine-tuning on successful traces), models could bootstrap capability without human supervision&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;Phase 2 Preview&lt;/h2&gt; &lt;p&gt;Next up: Can the model improve by learning from its own successes?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Loop:&lt;/strong&gt; 1. Harvest successful reasoning traces from memory bank 2. Fine-tune via LoRA on these traces 3. Test on problems the original model failed 4. Measure differential improvement 5. Hot-swap improved model, repeat&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; The 16 improvements from Phase 1 suggest the model can apply better strategies. If we fine-tune on those successful traces, can we bake the improvements in?&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Reproducibility&lt;/h2&gt; &lt;p&gt;Everything is open source. The repo includes: - Full code with fixes and improvements - Dataset preparation scripts (GSM8K and MATH) - Statistical analysis tools - Diagnostic scripts for debugging - Instructions for running locally&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware requirements (All models used for testing are quantized to Q8):&lt;/strong&gt; - 4.3GB+ VRAM for 4B model - 1.7GB+ VRAM for 1.7B model&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Limitations &amp;amp; Honesty&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Not statistically significant&lt;/strong&gt; (95% CI overlap) - need larger n&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Regressions exist&lt;/strong&gt; - memory can confuse small models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extraction variance&lt;/strong&gt; - same training set produces 29-223 memories depending on run&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset ceiling&lt;/strong&gt; - 4B at 76% baseline doesn't have much room to improve&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phase 2 unproven&lt;/strong&gt; - recursive loop might amplify errors instead of improvements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is early research. I'm sharing to get feedback and replication attempts.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Why I'm Posting&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Validation&lt;/strong&gt;: Want others to check my work&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;: Ideas for improving retrieval/extraction?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Curiosity&lt;/strong&gt;: Has anyone else tried this with small models?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt;: This could fail spectacularly in Phase 2 - documenting either way&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you replicate this and get different results, please let me know. Science requires replication.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Lanerra/reasoning-bank-slm"&gt;https://github.com/Lanerra/reasoning-bank-slm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback, criticisms, and replication attempts welcome. Especially interested if anyone has ideas for: - Better memory extraction methods - Smarter retrieval filtering - Handling the regression problem - Phase 2 design approaches&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T01:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6omjf</id>
    <title>Is anyone considering the DGX Spark</title>
    <updated>2025-10-14T19:08:17+00:00</updated>
    <author>
      <name>/u/Commercial-West3390</name>
      <uri>https://old.reddit.com/user/Commercial-West3390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got in line to reserve one a few months back, and as of this morning they can be ordered. Should I make the jump? Haven't been keeping up with developments over the last few months so I'm not sure how it stacks up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-West3390"&gt; /u/Commercial-West3390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6omjf/is_anyone_considering_the_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6omjf/is_anyone_considering_the_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6omjf/is_anyone_considering_the_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:08:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6d6xh</id>
    <title>SHAI ‚Äì (yet another) open-source Terminal AI coding assistant</title>
    <updated>2025-10-14T11:42:29+00:00</updated>
    <author>
      <name>/u/Fit_Temperature7246</name>
      <uri>https://old.reddit.com/user/Fit_Temperature7246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;At OVHcloud, we built SHAI for our internal needs as a coding assistant that wouldn‚Äôt rely on proprietary models or closed services. We‚Äôve now open-sourced it (Apache 2.0) so the community can use and improve it too, including for local use.&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;What is SHAI? üîé&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A terminal-based AI assistant to help you:&lt;br /&gt; ‚Ä¢ Build &amp;amp; edit code&lt;br /&gt; ‚Ä¢ Run shell commands&lt;br /&gt; ‚Ä¢ Automate workflows&lt;br /&gt; ‚Ä¢ Or even run headless as part of your stack&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it‚Äôs cool ?&lt;/strong&gt; üòé&lt;/p&gt; &lt;p&gt;‚Ä¢ Fully Open Source + developer-first design&lt;br /&gt; ‚Ä¢ No vendor lock-in (configure any LLM endpoint)&lt;br /&gt; ‚Ä¢ Works out of the box with pre-configured OVHCloud AI Endpoints (free tier with low rate limiting - you can add your API key later)&lt;br /&gt; ‚Ä¢ Supports Function Calling + MCP&lt;br /&gt; Also ‚Üí SHAI is part of &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hacktoberfest&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This year! If you want to contribute &amp;amp; grab some swag, it‚Äôs a great time: &lt;a href="https://github.com/ovh/shai"&gt;https://github.com/ovh/shai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Temperature7246"&gt; /u/Fit_Temperature7246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d6xh/shai_yet_another_opensource_terminal_ai_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d6xh/shai_yet_another_opensource_terminal_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d6xh/shai_yet_another_opensource_terminal_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T11:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6d4a6</id>
    <title>CPU Only OSS 120</title>
    <updated>2025-10-14T11:38:44+00:00</updated>
    <author>
      <name>/u/Wisepunter</name>
      <uri>https://old.reddit.com/user/Wisepunter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive sold my 3090 and im selling my 4090 as we speak, mostly because the stuff I really need LLMs for I need huge models and the other stuff I only need really small models 4B or less. Also I tend to game on my PS5 as work at my PC all day.&lt;/p&gt; &lt;p&gt;So I used to run OSS120 partially in GPU with the rest offloaded to CPU and it used to fly. Also it was a pretty good model IMO for logic etc for its speed.&lt;/p&gt; &lt;p&gt;So decided to just try it on CPU only (gulp) on my home lab server and actually it's more than usable at a fraction of the power cost too. This is also running in a VM with only half cores given.&lt;/p&gt; &lt;p&gt;prompt eval time = 260.39 ms / 13 tokens ( 20.03 ms per token, 49.92 tokens per second)eval time = 51470.09 ms / 911 tokens ( 56.50 ms per token, 17.70 tokens per second)total time = 51730.48 ms / 924 tokens&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wisepunter"&gt; /u/Wisepunter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d4a6/cpu_only_oss_120/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d4a6/cpu_only_oss_120/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6d4a6/cpu_only_oss_120/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T11:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ghgh</id>
    <title>MIT SEAL (Self-Adapting LLMs)</title>
    <updated>2025-10-14T14:07:08+00:00</updated>
    <author>
      <name>/u/ravage382</name>
      <uri>https://old.reddit.com/user/ravage382</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had MIT SEAL come up in my news feed and it seems interested. Here's the &lt;a href="https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal"&gt;Venture Beat story&lt;/a&gt; on it and the &lt;a href="https://github.com/Continual-Intelligence/SEAL/tree/main"&gt;SEAL Github page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&amp;quot;SEAL (&lt;strong&gt;Se&lt;/strong&gt;lf-&lt;strong&gt;A&lt;/strong&gt;dapting &lt;strong&gt;L&lt;/strong&gt;LMs) is a framework for training language models via RL to generate self-edits (finetuning data and other update directives for themselves) in response to new inputs.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;All experiments can be run with 2 A100/H100 GPUs&amp;quot;&lt;/p&gt; &lt;p&gt;Anyone happen to have tried this out?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ravage382"&gt; /u/ravage382 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ghgh/mit_seal_selfadapting_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ghgh/mit_seal_selfadapting_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ghgh/mit_seal_selfadapting_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6kuso</id>
    <title>KAT-Dev-72B-Exp I tried from the community a couple of days ago: high scores don‚Äôt mean it wins everywhere</title>
    <updated>2025-10-14T16:50:17+00:00</updated>
    <author>
      <name>/u/Hairy-Librarian3796</name>
      <uri>https://old.reddit.com/user/Hairy-Librarian3796</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Credit where it‚Äôs due: what first caught my eye was its 74.6% on SWE-Bench Verified among open-source models (evaluated with the SWE-agent scaffold) , pretty encouraging. But in the engineering world, ‚Äúbenchmarks = reality‚Äù rarely holds. Cross-repo coupling, legacy landmines, and CI magic can all throw a model off rhythm. I care more about ‚Äústeady-state performance‚Äù in real repos: first-pass success rate, average time-to-fix, rollback rate, these numbers guide team decisions better than a single score.&lt;/p&gt; &lt;p&gt;The official messaging is candid too: KAT-Dev-72B-Exp is an experimental RL line of KAT-Coder to showcase RL innovations; the stronger KAT-Coder has a free trial on StreamLake, which basically gives everyone ready-made conditions for A/B testing. I recommend benchmarking on your own repo and workflow, not just staring at promo charts. RL can easily pick up ‚Äúbenchmark-friendly habits,‚Äù but in real repos with crusty scripts, cross-service changes, and quirky pipelines, my hands-on experience wasn‚Äôt as stellar as the benchmark results suggest.&lt;/p&gt; &lt;p&gt;Weights and docs: &lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairy-Librarian3796"&gt; /u/Hairy-Librarian3796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kuso/katdev72bexp_i_tried_from_the_community_a_couple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kuso/katdev72bexp_i_tried_from_the_community_a_couple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kuso/katdev72bexp_i_tried_from_the_community_a_couple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T16:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o69vm5</id>
    <title>What‚Äôs the point of a DGX Spark for inference if a Mac Studio M1 Ultra beats it at TG and equals it at PP at half the price?</title>
    <updated>2025-10-14T08:28:09+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I might be missing something here, but with the results I‚Äôve seen, the DGX does what Apple did 3 years ago (actually worse token generation). &lt;/p&gt; &lt;p&gt;Is the DGX as bad as it seems for inference? We all knew that TG would have been shit with that bandwidth, but even prompt processing doesn‚Äôt seem great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ik01</id>
    <title>GLM-4.6 | Gut feel after sparring with Sonnet for half a day: more of a ‚Äústeady player‚Äù</title>
    <updated>2025-10-14T15:25:37+00:00</updated>
    <author>
      <name>/u/xieyutong</name>
      <uri>https://old.reddit.com/user/xieyutong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cutting to the chase: it feels steadier, especially for small code-review fixes, short-chain reasoning, and toning down overhyped copy. Officially, they say across eight public benchmarks (like AIME25, LCB v6, HLE, SWE-Bench Verified, BrowseComp, Terminal-Bench, œÑ¬≤-Bench, GPQA) it‚Äôs overall aligned with Sonnet 4, parts of its coding performance approach Sonnet 4.5, and there‚Äôs a ‚Äú48.6% ties‚Äù line. I don‚Äôt obsess over perfect number matching; what matters is that I can reproduce results and it saves me hassle.&lt;/p&gt; &lt;p&gt;I used it for three things. First, code review. I told it ‚Äúonly fix unsafe code and keep function signatures,‚Äù and it gave a diff-like display, then pasted the full function; very low reading overhead. Second, terminal task planning. I didn‚Äôt let it actually run commands; I just wanted a small blueprint of ‚Äúplan ‚Üí expected output ‚Üí fallback path.‚Äù It gave a clean structure that I could execute manually. Third, neutralizing overly promotional copy its touch is just right, and it keeps the numbers and sources.&lt;/p&gt; &lt;p&gt;I put GLM-4.6 into four everyday buckets: small code fixes, short-chain reasoning, tool awareness (planning only, no network), and rewriting. Settings per the official guidance: temperature = 1.0; for code, top_p = 0.95 and top_k = 40; 200K context makes reproducibility easier. For routine code/writing/short-chain reasoning, you can use it as-is; for heavy retrieval and strong evidence chains, plug in your own tools first and swap it in afterward.&lt;/p&gt; &lt;p&gt;Reference: &lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;https://huggingface.co/zai-org/GLM-4.6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xieyutong"&gt; /u/xieyutong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ik01/glm46_gut_feel_after_sparring_with_sonnet_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ik01/glm46_gut_feel_after_sparring_with_sonnet_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ik01/glm46_gut_feel_after_sparring_with_sonnet_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:25:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6oaa4</id>
    <title>Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails</title>
    <updated>2025-10-14T18:55:40+00:00</updated>
    <author>
      <name>/u/sketharapu</name>
      <uri>https://old.reddit.com/user/sketharapu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"&gt; &lt;img alt="Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails" src="https://preview.redd.it/7w1yhhrhj4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31ba63d90457b18277246650e0e8756589cac761" title="Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just received this email&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sketharapu"&gt; /u/sketharapu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7w1yhhrhj4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6izz2</id>
    <title>DGX Spark vs AI Max 395+</title>
    <updated>2025-10-14T15:42:08+00:00</updated>
    <author>
      <name>/u/Responsible-Let9423</name>
      <uri>https://old.reddit.com/user/Responsible-Let9423</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone has fair comparison between two tiny AI PCs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Let9423"&gt; /u/Responsible-Let9423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6iwrd</id>
    <title>Performance of llama.cpp on NVIDIA DGX Spark ¬∑ ggml-org/llama.cpp ¬∑ Discussion #16578</title>
    <updated>2025-10-14T15:38:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"&gt; &lt;img alt="Performance of llama.cpp on NVIDIA DGX Spark ¬∑ ggml-org/llama.cpp ¬∑ Discussion #16578" src="https://external-preview.redd.it/jHQdSuZPiOdCRrmqghCS01mFwWiPh61nOi8HvEEUkiw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd9276c6ee8e677f95f5cd9bcd71ed7dde0ad2a" title="Performance of llama.cpp on NVIDIA DGX Spark ¬∑ ggml-org/llama.cpp ¬∑ Discussion #16578" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5v78n</id>
    <title>The top open models on are now all by Chinese companies</title>
    <updated>2025-10-13T20:27:10+00:00</updated>
    <author>
      <name>/u/k_schaul</name>
      <uri>https://old.reddit.com/user/k_schaul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt; &lt;img alt="The top open models on are now all by Chinese companies" src="https://preview.redd.it/xhsv9ilkuxuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17f3ce5e0a0548bdb8546f46e0f43b1b008af719" title="The top open models on are now all by Chinese companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full analysis here (üéÅ gift link): &lt;a href="https://wapo.st/4nPUBud"&gt;wapo.st/4nPUBud&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_schaul"&gt; /u/k_schaul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhsv9ilkuxuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T20:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6pmxt</id>
    <title>Real-time study buddy that sees your screen and talks back</title>
    <updated>2025-10-14T19:46:04+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt; &lt;img alt="Real-time study buddy that sees your screen and talks back" src="https://external-preview.redd.it/ZXlwZW5pYTNuNHZmMUsYDHUptOq0sYO1cNNkCl_tbC9KzkSKWyT6VTZxxWFL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff99a7daf7c346698fa23df7509fc456a4b3edc3" title="Real-time study buddy that sees your screen and talks back" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a real-time learning assistant that sees your screen, talks, and learns alongside you. All open models (Qwen3-VL, Parakeet, Orpheus) wired together. &lt;/p&gt; &lt;p&gt;I shared a biology site on cell structure to see if it could describe the page, identify the diagram, and answer targeted questions about the mitochondria. &lt;/p&gt; &lt;p&gt;These text and vision models are getting so good. Wiring them together levels them all up. Next step: going to try running it across multiple sites and have it auto-summarize my learnings into a study guide or PDF after.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ctp0k9a3n4vf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o61gzs</id>
    <title>Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8</title>
    <updated>2025-10-14T00:47:06+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt; &lt;img alt="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" src="https://preview.redd.it/fjr53w0m4zuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1986eb8662405e67e0522e5d8d37f03ea577ffc" title="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;-NVFP4 is a way to store numbers for training large models using just 4 bits instead of 8 or 16. This makes training faster and use less memory&lt;/p&gt; &lt;p&gt;-NVFP4 shows 4-bit pretraining of a 12B Mamba Transformer on 10T tokens can match FP8 accuracy while cutting compute and memory.&lt;/p&gt; &lt;p&gt;-The validation loss stays within 1% of FP8 for most of training and grows to about 1.5% late during learning rate decay. &lt;/p&gt; &lt;p&gt;-Task scores stay close, for example MMLU Pro 62.58% vs 62.62%, while coding dips a bit like MBPP+ 55.91% vs 59.11%.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/godofprompt/status/1977678347879714912"&gt;X thread&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2509.25149"&gt;Arxiv paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjr53w0m4zuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ofr9</id>
    <title>Intel Crescent Island GPU: 160GB of LPDDR5X memory</title>
    <updated>2025-10-14T19:01:14+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;About the GPU:&lt;/strong&gt; The new data center GPU code-named Crescent Island is being designed to be power and cost-optimized for air-cooled enterprise servers and to incorporate large amounts of memory capacity and bandwidth, optimized for inference workflows. &lt;/p&gt; &lt;p&gt;Key features include: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Xe3P microarchitecture with optimized performance-per-watt &lt;/li&gt; &lt;li&gt;160GB of LPDDR5X memory &lt;/li&gt; &lt;li&gt;Support for a broad range of data types, ideal for ‚Äútokens-as-a-service‚Äù providers and inference use cases &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory"&gt;https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu"&gt;https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6hjgw</id>
    <title>[Open Source] We built a production-ready GenAI framework after deploying 50+ agents. Here's what we learned üçï</title>
    <updated>2025-10-14T14:48:01+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! üëã&lt;/p&gt; &lt;p&gt;After building and deploying 50+ GenAI solutions in production, we got tired of fighting with bloated frameworks, debugging black boxes, and dealing with vendor lock-in. So we built Datapizza AI - a Python framework that actually respects your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem We Solved&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most LLM frameworks give you two bad options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Too much magic ‚Üí You have no idea why your agent did what it did&lt;/li&gt; &lt;li&gt;Too little structure ‚Üí You're rebuilding the same patterns over and over&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We wanted something that's predictable, debuggable, and production-ready from day one.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes It Different&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üîç Built-in Observability: OpenTelemetry tracing out of the box. See exactly what your agents are doing, track token usage, and debug performance issues without adding extra libraries.&lt;/p&gt; &lt;p&gt;ü§ù Multi-Agent Collaboration: Agents can call other specialized agents. Build a trip planner that coordinates weather experts and web researchers - it just works.&lt;/p&gt; &lt;p&gt;üìö Production-Grade RAG: From document ingestion to reranking, we handle the entire pipeline. No more duct-taping 5 different libraries together.&lt;/p&gt; &lt;p&gt;üîå Vendor Agnostic: Start with OpenAI, switch to Claude, add Gemini - same code. We support OpenAI, Anthropic, Google, Mistral, and Azure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why We're Sharing This&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe in less abstraction, more control. If you've ever been frustrated by frameworks that hide too much or provide too little, this might be for you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üêô GitHub: &lt;a href="https://github.com/datapizza-labs/datapizza-ai"&gt;https://github.com/datapizza-labs/datapizza-ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìñ Docs: &lt;a href="https://docs.datapizza.ai"&gt;https://docs.datapizza.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üè† Website: &lt;a href="https://datapizza.tech/en/ai-framework/"&gt;https://datapizza.tech/en/ai-framework/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;We Need Your Help! üôè&lt;/h1&gt; &lt;p&gt;We're actively developing this and would love to hear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What features would make this useful for YOUR use case?&lt;/li&gt; &lt;li&gt;What problems are you facing with current LLM frameworks?&lt;/li&gt; &lt;li&gt;Any bugs or issues you encounter (we respond fast!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Star us on GitHub if you find this interesting,&lt;/strong&gt; it genuinely helps us understand if we're solving real problems.&lt;/p&gt; &lt;p&gt;Happy to answer any questions in the comments! üçï&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6h8jn</id>
    <title>We tested Claude Sonnet 4.5, GPT-5-codex, Qwen3-Coder, GLM and other 25+ models on fresh SWE-Bench like tasks from September 2025</title>
    <updated>2025-10-14T14:36:10+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with September runs on &lt;strong&gt;49 fresh GitHub PR bug-fix tasks&lt;/strong&gt; (last-month PR issues only). It‚Äôs a SWE-bench‚Äìstyle setup: models read real PR issues, run tests, edit code, and must make the suite pass.&lt;/p&gt; &lt;p&gt;Models: &lt;strong&gt;Sonnet-4.5, GPT-5-Codex, Grok Code Fast 1, GLM, Qwen, Kimi&lt;/strong&gt; and others&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4.5 achieved the highest &lt;em&gt;pass@5&lt;/em&gt; (&lt;strong&gt;55.1%&lt;/strong&gt;) and uniquely solving several instances that &lt;strong&gt;no other model&lt;/strong&gt; on the leaderboard managed to resolve: &lt;a href="https://github.com/python-trio/trio/pull/3334"&gt;&lt;strong&gt;python-trio/trio-3334&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/cubed-dev/cubed/pull/799"&gt;&lt;strong&gt;cubed-dev/cubed-799&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/canopen-python/canopen/pull/613"&gt;&lt;strong&gt;canopen-python/canopen-613&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the &lt;strong&gt;best open-source performer&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All models on the leaderboard were evaluated using the ChatCompletions API, except for &lt;a href="https://platform.openai.com/docs/models/gpt-5-codex"&gt;&lt;strong&gt;gpt-5-codex&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://platform.openai.com/docs/models/gpt-oss-120b"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;&lt;/a&gt;, which are only accessible via the Responses API.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please check the leaderboard, the insights, and write if you want to request some models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6n0tm</id>
    <title>Qwen3-VL-4B and 8B Instruct &amp; Thinking are here</title>
    <updated>2025-10-14T18:08:00+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF, MLX, and NexaML collection on HuggingFace: &lt;a href="https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a"&gt;https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6n0tm/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6n0tm/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6n0tm/qwen3vl4b_and_8b_instruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:08:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ocfs</id>
    <title>If it's not local, it's not yours.</title>
    <updated>2025-10-14T18:57:54+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt; &lt;img alt="If it's not local, it's not yours." src="https://preview.redd.it/zzv4ey22j4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebc1f207746b0fa04e90a129bafad3aef0ca9971" title="If it's not local, it's not yours." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzv4ey22j4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
