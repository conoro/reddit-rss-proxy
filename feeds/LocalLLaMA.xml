<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-19T21:31:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r8ta57</id>
    <title>I retrained /u/Own-Albatross868's FlashLM v4 "Bolt" model from scratch using GreedyPhrase tokenizer on the full TinyStories dataset. I scaled up to 15M parameters with a 65K vocab, achieving smooth convergence and coherent story generation in just 2.2 hours on an RTX 2080 Ti</title>
    <updated>2026-02-19T07:54:51+00:00</updated>
    <author>
      <name>/u/reditzer</name>
      <uri>https://old.reddit.com/user/reditzer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FlashLM v4 &amp;quot;Bolt&amp;quot; retrained from scratch on the full TinyStories dataset using our &lt;a href="https://github.com/rayonnant-ai/greedyphrase"&gt;GreedyPhrase&lt;/a&gt; tokenizer instead of the original GPT-2 10K tokenizer.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;&lt;a href="https://huggingface.co/changcheng967/flashlm-v4-bolt"&gt;Original&lt;/a&gt;&lt;/th&gt; &lt;th&gt;&lt;a href="https://huggingface.co/rrezel/flashlm-v4-bolt-greedyphrase"&gt;This Run&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Tokenizer&lt;/td&gt; &lt;td&gt;GPT-2 (tiktoken), 10K vocab&lt;/td&gt; &lt;td&gt;GreedyPhrase, 65K vocab&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Parameters&lt;/td&gt; &lt;td&gt;4.3M&lt;/td&gt; &lt;td&gt;15.0M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hardware&lt;/td&gt; &lt;td&gt;2 vCPU (CPU only)&lt;/td&gt; &lt;td&gt;RTX 2080 Ti (GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training time&lt;/td&gt; &lt;td&gt;2 hours&lt;/td&gt; &lt;td&gt;~2.2 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tokens seen&lt;/td&gt; &lt;td&gt;10.6M (2.3% of data)&lt;/td&gt; &lt;td&gt;818M (3.3 epochs)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Best val loss&lt;/td&gt; &lt;td&gt;2.0976&lt;/td&gt; &lt;td&gt;3.9352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Throughput&lt;/td&gt; &lt;td&gt;1,479 tok/s&lt;/td&gt; &lt;td&gt;103,000 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Training Configuration&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Architecture&lt;/td&gt; &lt;td&gt;FlashLM v4 Bolt (ternary gated causal conv)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hidden dim&lt;/td&gt; &lt;td&gt;192&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Blocks&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Conv kernel size&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLU expansion dim&lt;/td&gt; &lt;td&gt;512&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vocab size&lt;/td&gt; &lt;td&gt;65,280 (padded from 65,218 actual)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sequence length&lt;/td&gt; &lt;td&gt;256 tokens&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Effective batch size&lt;/td&gt; &lt;td&gt;64 (micro=16, grad_accum=4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Optimizer&lt;/td&gt; &lt;td&gt;AdamW (weight_decay=0.01)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Peak learning rate&lt;/td&gt; &lt;td&gt;4e-3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LR schedule&lt;/td&gt; &lt;td&gt;Cosine with 500-step warmup&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gradient clipping&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Precision&lt;/td&gt; &lt;td&gt;AMP float16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Total steps&lt;/td&gt; &lt;td&gt;50,000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Dataset&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Source:&lt;/strong&gt; TinyStories (roneneldan/TinyStories), 2.1 GB text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Preprocessing:&lt;/strong&gt; &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; replaced with &lt;code&gt;&amp;lt;/s&amp;gt;&lt;/code&gt; (EOS token ID 3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tokenized size:&lt;/strong&gt; 248M tokens (496 MB binary uint16)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression ratio:&lt;/strong&gt; ~8.88 bytes/token (vs ~4.5 for GPT-2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Train/val split:&lt;/strong&gt; 99.5% / 0.5%&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;h3&gt;Loss Curve&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Step Train Loss Val Loss 0 11.13 ‚Äî 500 6.73 5.96 1000 5.46 5.12 2500 4.72 4.61 5000 4.43 4.39 10000 4.17 4.19 20000 4.03 4.03 30000 3.95 3.97 40000 3.92 3.95 50000 3.94 3.94 Best ‚Äî 3.9352 (step 47500) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Best validation loss&lt;/td&gt; &lt;td&gt;3.9352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Token-level perplexity&lt;/td&gt; &lt;td&gt;51.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bits per token&lt;/td&gt; &lt;td&gt;5.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bits per character (estimated)&lt;/td&gt; &lt;td&gt;0.64&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Comparing Val Loss Across Tokenizers&lt;/h3&gt; &lt;p&gt;The raw validation loss numbers are &lt;strong&gt;not directly comparable&lt;/strong&gt; between the original (val_loss 2.10 with 10K vocab) and this run (val_loss 3.94 with 65K vocab) because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Larger vocabulary = harder prediction task.&lt;/strong&gt; Random-chance loss is ln(65280) = 11.09 vs ln(10000) = 9.21. The model must distribute probability over 6.5x more tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fewer tokens per story.&lt;/strong&gt; GreedyPhrase compresses TinyStories at ~9 bytes/token vs ~4.5 bytes/token for GPT-2. Each token carries more information, so predicting the next token is inherently harder.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bits-per-character is the fair comparison.&lt;/strong&gt; At 0.64 BPC this model is competitive with the original's 0.88 BPC, suggesting the GreedyPhrase tokenizer's higher compression ratio pays off in information-theoretic efficiency.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Generation Samples (Step 49,500)&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time there was a little girl named Sarah. She was only three years old and loved exploring. One day Sarah went to the park with her mother. She saw a little boy playing with a ball.&lt;/p&gt; &lt;p&gt;Once upon a time there was a very deep lake. It was great! Every morning he would jump off the water and look for something wonderful.&lt;/p&gt; &lt;p&gt;Once upon a time there was a little girl named Mary. Mary loved animals, especially especially loved the ocean. Every day Mary would go out on a walk around the waves and swimming around on the beach.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3&gt;Prompt: &amp;quot;The little dog&amp;quot;&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;The little dog wanted to protect his bone, so he held it up to the cat and tried to protect him. But the big cat was jealous. It wanted to take the bone from him, but it ran away.&lt;/p&gt; &lt;p&gt;The cat was sad and began to cry. Then, he saw a big hole in the ground and started to shake it. The cat growled and tried to run away. The dog was scared and ran back to the cat. The cat saw the fox and was scared. The cat took the kitten and ran away. The dog was sad. The fox did not get the mitten anymore. The cat was happy and played with Spot and the other friends.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Files&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;File&lt;/th&gt; &lt;th&gt;Size&lt;/th&gt; &lt;th&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;flashlm_v4_bolt_greedyphrase.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;58 MB&lt;/td&gt; &lt;td&gt;Final model (step 50,000)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;best.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;172 MB&lt;/td&gt; &lt;td&gt;Best checkpoint with optimizer state (step 47,500)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;checkpoint.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;172 MB&lt;/td&gt; &lt;td&gt;Latest periodic checkpoint&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;tinystories.tokens&lt;/code&gt;&lt;/td&gt; &lt;td&gt;496 MB&lt;/td&gt; &lt;td&gt;Tokenized dataset (uint16 binary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;model.py&lt;/code&gt;&lt;/td&gt; &lt;td&gt;‚Äî&lt;/td&gt; &lt;td&gt;Model architecture&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;train.py&lt;/code&gt;&lt;/td&gt; &lt;td&gt;‚Äî&lt;/td&gt; &lt;td&gt;Training script&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Observations&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Convergence was smooth.&lt;/strong&gt; Loss dropped from 11.13 to ~3.94 over 50K steps with no instability, despite ternary weight quantization via straight-through estimators.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The loss curve was still slowly declining at 50K steps.&lt;/strong&gt; Extended training or a second cosine cycle could improve results further.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;GreedyPhrase's long phrases help coherence.&lt;/strong&gt; With ~9 bytes/token, the 256-token context window covers ~2,300 characters (~400 words), much more than the original's ~1,150 characters. This gives the model more context per sequence.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The larger embedding table dominates parameter count.&lt;/strong&gt; 65K vocab x 192 dim = 12.5M parameters in the embedding alone (84% of total), vs 1.9M for the original's 10K vocab. The model body (blocks) is identical.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Throughput benefited from GPU + AMP.&lt;/strong&gt; At 103K tokens/sec on an RTX 2080 Ti, this is 70x faster than the original's 1.5K tokens/sec on CPU, allowing 3.3 full epochs in roughly the same wall-clock time.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reditzer"&gt; /u/reditzer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r97toz</id>
    <title>How do you handle very complex email threads in RAG systems?</title>
    <updated>2026-02-19T18:53:49+00:00</updated>
    <author>
      <name>/u/superhero_io</name>
      <uri>https://old.reddit.com/user/superhero_io</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a RAG system where emails are one of the main knowledge sources, and I‚Äôm hitting serious limits with complexity.&lt;/p&gt; &lt;p&gt;These aren‚Äôt simple linear threads. Real cases include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Long back-and-forth chains with branching replies&lt;/li&gt; &lt;li&gt;Multiple people replying out of order&lt;/li&gt; &lt;li&gt;Partial quotes, trimmed context, and forwarded fragments&lt;/li&gt; &lt;li&gt;Decisions split across many short replies (‚Äúyes‚Äù, ‚Äúno‚Äù, ‚Äúapproved‚Äù, etc.)&lt;/li&gt; &lt;li&gt;Mixed permissions and visibility across the same thread&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôve already tried quite a few approaches, for example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standard thread-based chunking (one email = one chunk)&lt;/li&gt; &lt;li&gt;Aggressive cleaning + deduplication of quoted content&lt;/li&gt; &lt;li&gt;LLM-based rewriting / normalization before indexing&lt;/li&gt; &lt;li&gt;Segment-level chunking instead of whole emails&lt;/li&gt; &lt;li&gt;Adding metadata like Message-ID, In-Reply-To, timestamps, participants&lt;/li&gt; &lt;li&gt;Vector DB + metadata filtering + reranking&lt;/li&gt; &lt;li&gt;Treating emails as conversation logs instead of documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The problem I keep seeing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If I split too small, the chunks lose meaning (‚Äúyes‚Äù by itself is useless)&lt;/li&gt; &lt;li&gt;If I keep chunks large, retrieval becomes noisy and unfocused&lt;/li&gt; &lt;li&gt;Decisions and rationale are scattered across branches&lt;/li&gt; &lt;li&gt;The model often retrieves the &lt;em&gt;wrong branch&lt;/em&gt; of the conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm starting to wonder whether:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Email threads should be converted into some kind of structured representation (graph / decision tree / timeline)&lt;/li&gt; &lt;li&gt;RAG should index &lt;em&gt;derived artifacts&lt;/em&gt; (summaries, decisions, normalized statements) instead of raw email text&lt;/li&gt; &lt;li&gt;Or whether there‚Äôs a better hybrid approach people are using in production&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For those of you who have dealt with &lt;strong&gt;real-world, messy email data&lt;/strong&gt; in RAG:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How do you represent email threads?&lt;/li&gt; &lt;li&gt;What do you actually store and retrieve?&lt;/li&gt; &lt;li&gt;Do you keep raw emails, rewritten versions, or both?&lt;/li&gt; &lt;li&gt;How do you prevent cross-branch contamination during retrieval?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm less interested in toy examples and more in patterns that actually hold up at scale.&lt;br /&gt; Any practical insights, war stories, or architecture suggestions would be hugely appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superhero_io"&gt; /u/superhero_io &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r97toz/how_do_you_handle_very_complex_email_threads_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r97toz/how_do_you_handle_very_complex_email_threads_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r97toz/how_do_you_handle_very_complex_email_threads_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T18:53:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r93i0p</id>
    <title>Local iOS voice to text app (alternative to Wispr Flow)</title>
    <updated>2026-02-19T16:18:23+00:00</updated>
    <author>
      <name>/u/Impressive-Sir9633</name>
      <uri>https://old.reddit.com/user/Impressive-Sir9633</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r93i0p/local_ios_voice_to_text_app_alternative_to_wispr/"&gt; &lt;img alt="Local iOS voice to text app (alternative to Wispr Flow)" src="https://external-preview.redd.it/cWpnOWU0cjg4aGtnMWS5e3158whTYNSe1GEK61Oq_uqxznQSR6QLvGe1g5lP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af142f52bab47ba63ad4da62327e9b1aa2b97b47" title="Local iOS voice to text app (alternative to Wispr Flow)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually dictate for 2 to 3 hours everyday in Dragon dictation and until recently used Wispr Flow on my personal devices. Over the last few months, I realized that local Al models can give you the same quality as Wispr Flow with complete privacy and without the ongoing subscription cost. So I built an iOS app, a MacOS app and an Android app.&lt;/p&gt; &lt;p&gt;Testflight link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/e5pcxwyq"&gt;https://testflight.apple.com/join/e5pcxwyq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am happy to offer the app for free to people who offer useful feedback for the test flight app.&lt;/p&gt; &lt;p&gt;We also have a MacOS app with local processing. If desired, users can sync their snippets and dictionary using personal iCloud. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Sir9633"&gt; /u/Impressive-Sir9633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ft3amnq88hkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r93i0p/local_ios_voice_to_text_app_alternative_to_wispr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r93i0p/local_ios_voice_to_text_app_alternative_to_wispr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T16:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9bokx</id>
    <title>New Hybrid AWQ Quant: Make MiniMax-M2.5 fly with efficient batching on 192GB VRAM</title>
    <updated>2026-02-19T21:16:27+00:00</updated>
    <author>
      <name>/u/EliasOenal</name>
      <uri>https://old.reddit.com/user/EliasOenal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've suspected for a while that one could combine AWQ int4 weights, fp8 attention, and calibrated fp8 KV cache into a single checkpoint for massive VRAM savings, but vLLM didn't support the combination, so nobody had done it. I finally sat down and made it work.&lt;/p&gt; &lt;p&gt;The result: MiniMax-M2.5 (229B) on &lt;strong&gt;4x RTX A6000 Ampere (192 GB)&lt;/strong&gt; with &lt;strong&gt;~370,000 tokens of KV cache.&lt;/strong&gt; More than double what standard AWQ gives you (~160K), significant batching headroom instead of just barely fitting. Should also work on 8x RTX 3090 (same generation, same total VRAM).&lt;/p&gt; &lt;p&gt;With this quant I get 92 t/s for a single request and 416 t/s combined throughput for 16 requests batched, both measured at 8000 tokens context.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/EliasOenal/MiniMax-M2.5-Hybrid-AWQ-W4A16G128-Attn-fp8_e4m3-KV-fp8_e4m3"&gt;&lt;strong&gt;Model on HuggingFace&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Precision&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Expert MLPs&lt;/td&gt; &lt;td align="left"&gt;224.7B (98.3%)&lt;/td&gt; &lt;td align="left"&gt;AWQ int4, group_size=128&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Attention&lt;/td&gt; &lt;td align="left"&gt;2.7B (1.2%)&lt;/td&gt; &lt;td align="left"&gt;Original fp8_e4m3, block scales&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KV cache&lt;/td&gt; &lt;td align="left"&gt;runtime&lt;/td&gt; &lt;td align="left"&gt;fp8_e4m3, calibrated per-layer scales&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Embeddings, head, norms, gates&lt;/td&gt; &lt;td align="left"&gt;~1.3B&lt;/td&gt; &lt;td align="left"&gt;Original bf16/fp32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The expert MLPs are 98% of the model and compress well. Until now, AWQ forced the attention layers to bf16, dequantizing the original fp8 weights and actually doubling the attention memory over the original model for no quality gain. This quant keeps them at original fp8. The fp8 KV cache with calibrated scales is what really unlocks batching: half the KV memory, double the context on the same GPUs.&lt;/p&gt; &lt;h1&gt;vLLM patches required&lt;/h1&gt; &lt;p&gt;This mixed-precision combo exposed two bugs in vLLM. Patches and details are on the model card, and I've submitted both upstream: &lt;a href="https://github.com/vllm-project/vllm/pull/34863"&gt;vllm#34863&lt;/a&gt;. Once merged, it should just work.&lt;/p&gt; &lt;h1&gt;How I built this&lt;/h1&gt; &lt;p&gt;The whole thing was done remotely using &lt;a href="https://opencode.ai"&gt;OpenCode&lt;/a&gt; with Claude Opus 4.6 (sadly not so local), connected to the headless GPU server via SSH through &lt;a href="https://github.com/EliasOenal/term-cli"&gt;term-cli&lt;/a&gt; - a tool I wrote that gives AI agents interactive terminal sessions without blocking. (Now with mouse support and color annotations, agents can finally use GNU Midnight Commander! üòâ)&lt;/p&gt; &lt;p&gt;Fully closed-loop agentic development: Opus ran the calibration, patched vLLM, tested inference, and iterated - all across SSH. At one point we were validating theories on a small Qwen3 model, and Opus kept asking it what &amp;quot;2+2&amp;quot; was, iterating on fixes until it finally started giving coherent answers again. That was when we fixed applying the calibrated KV scales correctly. During the project Opus also kept base64-encoding files to paste them through the terminal. That worked but was fragile enough that it motivated adding proper in-band file transfer (gzip + SHA-256) to term-cli. (&lt;code&gt;term-cli upload/download&lt;/code&gt;) So this project directly improved the tool.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full disclosure: I'm the author of term-cli. BSD licensed. If you're doing remote GPU work, or just use SSH with coding agents, it might be useful.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href="https://huggingface.co/EliasOenal/MiniMax-M2.5-Hybrid-AWQ-W4A16G128-Attn-fp8_e4m3-KV-fp8_e4m3"&gt;Model&lt;/a&gt; | &lt;a href="https://github.com/vllm-project/vllm/pull/34863"&gt;vLLM PR&lt;/a&gt; | &lt;a href="https://github.com/EliasOenal/term-cli"&gt;term-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliasOenal"&gt; /u/EliasOenal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9bokx/new_hybrid_awq_quant_make_minimaxm25_fly_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9bokx/new_hybrid_awq_quant_make_minimaxm25_fly_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9bokx/new_hybrid_awq_quant_make_minimaxm25_fly_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T21:16:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9borh</id>
    <title>Mind-Blown by 1-Bit Quantized Qwen3-Coder-Next-UD-TQ1_0 on Just 24GB VRAM - Why Isn't This Getting More Hype?</title>
    <updated>2026-02-19T21:16:38+00:00</updated>
    <author>
      <name>/u/bunny_go</name>
      <uri>https://old.reddit.com/user/bunny_go</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Mind-Blown by 1-Bit Quantized Qwen3-Coder-Next-UD-TQ1_0 on Just 24GB VRAM ‚Äì Why Isn't This Getting More Hype?&lt;/h1&gt; &lt;p&gt;I've been tinkering with local LLMs for coding tasks, and like many of you, I'm always hunting for models that perform well without melting my GPU. With only 24GB VRAM to work with, I've cycled through the usual suspects in the Q4-Q8 range, but nothing quite hit the mark. They were either too slow, hallucinated like crazy, or just flat-out unusable for real work.&lt;/p&gt; &lt;p&gt;Here's what I tried (and why they flopped for me): - &lt;strong&gt;Apriel&lt;/strong&gt; - &lt;strong&gt;Seed OSS&lt;/strong&gt; - &lt;strong&gt;Qwen 3 Coder&lt;/strong&gt; - &lt;strong&gt;GPT OSS 20&lt;/strong&gt; - &lt;strong&gt;Devstral-Small-2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I always dismissed 1-bit quants as &amp;quot;trash tier&amp;quot; ‚Äì I mean, how could something that compressed possibly compete? But desperation kicked in, so I gave &lt;strong&gt;Qwen3-Coder-Next-UD-TQ1_0&lt;/strong&gt; a shot. Paired it with the Pi coding agent, and... holy cow, I'm very impressed!&lt;/p&gt; &lt;h3&gt;Why It's a Game-Changer:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance Across Languages&lt;/strong&gt;: Handles Python, Go, HTML (and more) like a champ. Clean, accurate code without the usual fluff.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed Demon&lt;/strong&gt;: Inference is &lt;em&gt;blazing fast&lt;/em&gt; ‚Äì no more waiting around for responses or CPU trying to catch up with GPU on a shared task.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Efficiency&lt;/strong&gt;: Runs smoothly on my 24GB VRAM setup!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Overall Usability&lt;/strong&gt;: Feels like a massive model without the massive footprint. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Seriously, why isn't anyone talking about this? Is it flying under the radar because of the 1-bit stigma? Has anyone else tried it? Drop your experiences below.&lt;/p&gt; &lt;p&gt;TL;DR: Skipped 1-bit quants thinking they'd suck, but Qwen3-Coder-Next-UD-TQ1_0 + Pi agent is killing it for coding on limited hardware. More people need to know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bunny_go"&gt; /u/bunny_go &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9borh/mindblown_by_1bit_quantized_qwen3codernextudtq1_0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9borh/mindblown_by_1bit_quantized_qwen3codernextudtq1_0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9borh/mindblown_by_1bit_quantized_qwen3codernextudtq1_0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T21:16:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r97gcm</id>
    <title>90% VRAM reduction for DeepSeek-style Engrams: Running GSI-Architecture on Dual Intel Arc (B50)</title>
    <updated>2026-02-19T18:40:37+00:00</updated>
    <author>
      <name>/u/Existing_Boat_3203</name>
      <uri>https://old.reddit.com/user/Existing_Boat_3203</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted the &amp;quot;DeepSeek V4&amp;quot; engram knowledge density but only had 32GB of total VRAM across two Intel Arc cards. A naive implementation on my GSI table required 53GB. I got it running at 9.6GB.&lt;/p&gt; &lt;p&gt;DeepSeek V4 style &amp;quot;GSI Engram&amp;quot; architecture running on consumer hardware (Dual Intel Arc GPUs) using a custom llama.cpp fork! Here is the breakdown of the build and the performance stats.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The GSI Engram originally proposed a massive, sparse lookup table.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Naive Implementation: Expanding the [512] engram vector to the full [5120] model dimension for the lookup table would require ~53 GB of VRAM per layer (offline padding). This causes instant OOM on consumer cards.&lt;/li&gt; &lt;li&gt;Goal: Run this on standard 16GB cards.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Solution: Runtime Expansion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I modified llama.cpp (specifically phi3.cpp) to handle the GSI/Engram projection dynamically on the GPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instead of storing a 20GB+ GGUF file with zero-padded tensors, I store the compressed [512] tensors.&lt;/li&gt; &lt;li&gt;The compute graph pads them to [5120] during inference before addition.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stats &amp;amp; Benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hardware: Dual Intel Arc B50 GPUs (SYCL Backend)&lt;/p&gt; &lt;p&gt;Model: Phi-4 with GSI Engram (v30)&lt;/p&gt; &lt;p&gt;VRAM Usage: 9.6 GB (Total)&lt;/p&gt; &lt;p&gt;vs Theoretical Dense Usage: &amp;gt;50 GB (Impossible to run)&lt;/p&gt; &lt;p&gt;Memory Savings: ~90% reduction in GSI table footprint.&lt;/p&gt; &lt;p&gt;Inference Speed: ~14-16 tokens/s&lt;/p&gt; &lt;p&gt;Note: Speed is currently limited by the ggml_pad operation on the SYCL backend. Custom kernels could unlock significantly higher speeds, but stability was the priority here.&lt;/p&gt; &lt;p&gt;Coherence: Verified excellent (Scaling factor reduced to 0.1 to stabilize resonant integration).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to Run (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I kept everything containerized using ipex-llm.&lt;/p&gt; &lt;p&gt;This proves that run-time flexibility in llama.cpp can unlock architectures that &amp;quot;theoretically&amp;quot; require massive enterprise GPUs. I haven't posted to GitHub and HuggingFace yet due to the trained documents being my trade secrets, but I will have a cleaner, faster model soon. Honestly, I got tired of waiting on the DeepseekV4 hype, and their paper gave me the ammunition, which I think was their plan all along. So we're about to see a huge shift in the market if it does drop this week.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Existing_Boat_3203"&gt; /u/Existing_Boat_3203 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r97gcm/90_vram_reduction_for_deepseekstyle_engrams/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r97gcm/90_vram_reduction_for_deepseekstyle_engrams/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r97gcm/90_vram_reduction_for_deepseekstyle_engrams/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T18:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9a2rs</id>
    <title>I built a free local AI image search app ‚Äî find images by typing what's in them</title>
    <updated>2026-02-19T20:16:18+00:00</updated>
    <author>
      <name>/u/ravenlolanth</name>
      <uri>https://old.reddit.com/user/ravenlolanth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9a2rs/i_built_a_free_local_ai_image_search_app_find/"&gt; &lt;img alt="I built a free local AI image search app ‚Äî find images by typing what's in them" src="https://preview.redd.it/uloa0etjeikg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=65db81ac6999f3af3ba23d79faca331135ac473b" title="I built a free local AI image search app ‚Äî find images by typing what's in them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built Makimus-AI, a free open source app that lets you search your entire image library using natural language.&lt;/p&gt; &lt;p&gt;Just type &amp;quot;girl in red dress&amp;quot; or &amp;quot;sunset on the beach&amp;quot; and it finds matching images instantly ‚Äî even works with image-to-image search.&lt;/p&gt; &lt;p&gt;Runs fully local on your GPU, no internet needed after setup.&lt;/p&gt; &lt;p&gt;[Makimus-AI on GitHub](&lt;a href="https://github.com/Ubaida-M-Yusuf/Makimus-AI"&gt;https://github.com/Ubaida-M-Yusuf/Makimus-AI&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I hope it will be useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ravenlolanth"&gt; /u/ravenlolanth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uloa0etjeikg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9a2rs/i_built_a_free_local_ai_image_search_app_find/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9a2rs/i_built_a_free_local_ai_image_search_app_find/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8ectu</id>
    <title>I plugged a $30 radio into my Mac mini and told my AI "connect to this" ‚Äî now I control my smart home and send voice messages over radio with zero internet</title>
    <updated>2026-02-18T20:30:14+00:00</updated>
    <author>
      <name>/u/anvarazizov</name>
      <uri>https://old.reddit.com/user/anvarazizov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;So I live in Ukraine during the war. Power goes out a lot here ‚Äì russia regularly attacks our power grid. When it happens, internet dies, cell towers go dark, and suddenly all my smart home stuff and AI tools become useless. Got tired of it, so I did something kind of ridiculous.&lt;/p&gt; &lt;p&gt;I bought two Lilygo T-Echo radios (~$30 each, LoRa 433MHz, running Meshtastic firmware). Plugged one into my always-on Mac mini via USB. Took the other one as my portable radio. Then I opened up my OpenClaw AI agent and basically said: &amp;quot;hey, there's a Meshtastic radio plugged in. Figure it out.&amp;quot;&lt;/p&gt; &lt;p&gt;And it did.&lt;/p&gt; &lt;h1&gt;What happened next&lt;/h1&gt; &lt;p&gt;It identified the Meshtastic device, installed the CLI, configured an encrypted channel, and then ‚Äì without me writing a single line of code ‚Äì built a full Python listener daemon that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Monitors the radio 24/7 for incoming messages&lt;/li&gt; &lt;li&gt;Routes them intelligently: if internet is up, forwards to Discord where a cloud AI responds. If internet is down, routes everything to local models via Ollama&lt;/li&gt; &lt;li&gt;Uses phi4-mini as a lightweight intent classifier (&amp;quot;is this a smart home command or a question?&amp;quot;) and gemma3:12b for actual answers ()&lt;/li&gt; &lt;li&gt;Talks to Home Assistant so I can control lights, read sensors, check who's home ‚Äî all over radio&lt;/li&gt; &lt;li&gt;Auto-chunks responses to fit the 200-char LoRa limit&lt;/li&gt; &lt;li&gt;Watches an outbox folder ‚Äì if the AI needs to alert me about something (like a power outage), it drops a message file there and the listener transmits it over LoRa&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole thing just worked. The AI had already built the architecture while I was still thinking about how to approach it.&lt;/p&gt; &lt;h1&gt;The voice thing (this is the cool part)&lt;/h1&gt; &lt;p&gt;Then I added one more feature. If I prefix a Meshtastic message with &lt;code&gt;SAY:&lt;/code&gt;, the listener takes the text, calls Home Assistant's TTS service, and plays it through my HA Voice PE speaker at home. In Ukrainian.&lt;/p&gt; &lt;p&gt;So I can be walking around with a T-Echo in my pocket, completely off-grid, type &lt;code&gt;SAY: –ü—Ä–∏–≤—ñ—Ç, —è —Å–∫–æ—Ä–æ –±—É–¥—É –≤–¥–æ–º–∞&lt;/code&gt; (Hi, I'll come back home soon) ‚Äì and my house literally speaks. No internet anywhere in the chain. Just radio waves ‚Üí Mac mini ‚Üí TTS ‚Üí speaker.&lt;/p&gt; &lt;p&gt;Honestly didn't expect it to feel this magical.&lt;/p&gt; &lt;h1&gt;The stack&lt;/h1&gt; &lt;p&gt;Everything's open source except Claude (which is only used when internet is available):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenClaw&lt;/strong&gt; ‚Äì you know what is this &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meshtastic&lt;/strong&gt; ‚Äì LoRa mesh networking firmware. The magic sauce for off-grid communication ‚Äì open source, encrypted, and any Meshtastic radio can relay messages to extend range&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lilygo T-Echo&lt;/strong&gt; ‚Äì the $30 radio hardware running Meshtastic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; ‚Äì you know as well&lt;/li&gt; &lt;li&gt;&lt;strong&gt;phi4-mini&lt;/strong&gt; ‚Äì lightweight router/classifier&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gemma3:12b&lt;/strong&gt; ‚Äì the actual brain for offline responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Home Assistant&lt;/strong&gt; ‚Äì smart home + TTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HA Voice PE&lt;/strong&gt; ‚Äì the speaker that reads messages aloud&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mac mini M4 16GB&lt;/strong&gt; ‚Äì always-on server, running on battery backup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;T-Echo (portable) ‚îÇ LoRa 433MHz, encrypted ‚ñº T-Echo (USB) ‚Üí Mac mini ‚îÇ ‚îú‚îÄ‚îÄ SAY: prefix ‚Üí HA TTS ‚Üí Voice PE speaker ‚îú‚îÄ‚îÄ AI: prefix ‚Üí phi4-mini ‚Üí gemma3:12b (always local) ‚îú‚îÄ‚îÄ status ‚Üí Home Assistant sensors ‚îú‚îÄ‚îÄ Online? ‚Üí forward to Discord (cloud AI) ‚îî‚îÄ‚îÄ Offline? ‚Üí route everything to local Ollama models Outbox: AI drops .msg files ‚Üí listener sends over LoRa (power outage alerts, reminders, etc.) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's next&lt;/h1&gt; &lt;p&gt;I'm thinking about where this goes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mesh AI network&lt;/strong&gt; ‚Äì Meshtastic is a mesh protocol, every radio relays. Multiple nodes running local LLMs could create a neighborhood-scale AI network with zero internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger local models&lt;/strong&gt; ‚Äì looking at upgrading hardware for 30B+ parameter models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dead man's switch&lt;/strong&gt; ‚Äî auto-alert if I don't check in within a time window&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anvarazizov"&gt; /u/anvarazizov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T20:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8mh8m</id>
    <title>How do you get more GPUs than your motheboard natively supports?</title>
    <updated>2026-02-19T02:00:35+00:00</updated>
    <author>
      <name>/u/WizardlyBump17</name>
      <uri>https://old.reddit.com/user/WizardlyBump17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am planning on building an AI server for myself and I want to have 8 GPUs. The problem is that all motherboards I reaserched (FCLGA4710), dont have 8 PCIe slots, with the one with most slots having only 6. I have seen some people here with a lot of GPUs and I am pretty sure they dont have a motherboard with slots for all of them, as I remember some of the GPUs being far from the motherboard. I have done some research and I found out about risers and something about connecting the GPU using an USB, but I couldnt understand how everything works together. Anyone to help with that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WizardlyBump17"&gt; /u/WizardlyBump17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T02:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9au4o</id>
    <title>Trying to run LLMs on Providers the EU? I mapped out which providers actually have GPUs</title>
    <updated>2026-02-19T20:44:50+00:00</updated>
    <author>
      <name>/u/mixxor1337</name>
      <uri>https://old.reddit.com/user/mixxor1337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I compared GPU availability across 17 EU cloud providers, here's who actually has GPUs in Europe&lt;/p&gt; &lt;p&gt;I run &lt;a href="https://www.eucloudcost.com"&gt;eucloudcost.com&lt;/a&gt; and just went through the pain of checking (hopefully) most EU cloud providers for GPU instance availability.&lt;/p&gt; &lt;p&gt;Wrote it up here: &lt;a href="https://www.eucloudcost.com/blog/gpu-cloud-instances-european-providers-2026/"&gt;GPU Cloud Instances from European Providers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can also filter by GPU directly on the &lt;a href="https://www.eucloudcost.com/cloud-costs/"&gt;comparison page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Whole thing is open source if anyone wants to contribute or correct me: &lt;a href="https://github.com/mixxor/eu-cloud-prices"&gt;github.com/mixxor/eu-cloud-prices&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious what you guys are using for inference in EU, or is everyone just yolo-ing US regions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mixxor1337"&gt; /u/mixxor1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9au4o/trying_to_run_llms_on_providers_the_eu_i_mapped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9au4o/trying_to_run_llms_on_providers_the_eu_i_mapped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9au4o/trying_to_run_llms_on_providers_the_eu_i_mapped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9be56</id>
    <title>I ran a forensic audit on my local AI assistant. 40.8% of tasks were fabricated. Here's the full breakdown.</title>
    <updated>2026-02-19T21:05:37+00:00</updated>
    <author>
      <name>/u/Obvious-School8656</name>
      <uri>https://old.reddit.com/user/Obvious-School8656</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not a developer. I'm a regular guy from the Midwest who got excited about local AI and built a setup with an RTX 3090 Ti running Qwen models through an agent framework.&lt;/p&gt; &lt;p&gt;Over 13 days and 2,131 messages, my AI assistant &amp;quot;Linus&amp;quot; systematically fabricated task completions. He'd say &amp;quot;file created&amp;quot; without creating files, report GPU benchmarks he never ran, and ‚Äî the big one ‚Äî claimed he'd migrated himself to new hardware while still running on my MacBook the entire time.&lt;/p&gt; &lt;p&gt;I didn't find out until I asked for a GPU burn test and the fans didn't spin up.&lt;/p&gt; &lt;p&gt;I used Claude to run a full forensic audit against the original Telegram chat export. Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;283 tasks&lt;/strong&gt; audited&lt;/li&gt; &lt;li&gt;&lt;strong&gt;82 out of 201 executable tasks fabricated (40.8%)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10 distinct hallucination patterns&lt;/strong&gt; identified&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7-point red flag checklist&lt;/strong&gt; for catching it&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The biggest finding: hallucination rate was directly proportional to task complexity. Conversational tasks: 0% fabrication. File operations: 74%. System admin: 71%. API integration: 78%.&lt;/p&gt; &lt;p&gt;The full audit with methodology, all 10 patterns, detection checklist, and verification commands is open source:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="http://github.com/Amidwestnoob/ai-hallucination-audit"&gt;github.com/Amidwestnoob/ai-hallucination-audit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interactive origin story:&lt;/strong&gt; &lt;a href="http://amidwestnoob.github.io/ai-hallucination-audit/origin-story.html"&gt;amidwestnoob.github.io/ai-hallucination-audit/origin-story.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone else has experienced similar patterns with their local agents. I built a community issue template in the repo if you want to document your own findings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Obvious-School8656"&gt; /u/Obvious-School8656 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9be56/i_ran_a_forensic_audit_on_my_local_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9be56/i_ran_a_forensic_audit_on_my_local_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9be56/i_ran_a_forensic_audit_on_my_local_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T21:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r96pgp</id>
    <title>48GB 4090 Power limiting tests 450, 350, 250w - Noise and LLM throughput per power level</title>
    <updated>2026-02-19T18:13:45+00:00</updated>
    <author>
      <name>/u/computune</name>
      <uri>https://old.reddit.com/user/computune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The 48gb 4090's stock power is 450w but thats kind of alot for that 2 slot format where similar A100/6000Pro cards are 300w max for that format), so the fans really have to go (5k rpm blower) to keep it cool. Stacked in pcie slots the cards with less airflow intake can see upto 80C and all are noisy at 70dB (white noise type sound)&lt;/p&gt; &lt;p&gt;Below is just one model (deepseek 70b and gpt-oss were also tested and included in the github dump below, all models saw 5-15% performance loss at 350w (down from 450w)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Dual RTX 4090 48GB (96GB) ‚Äî Qwen 2.5 72B Q4_K_M 450W 350W 300W 250W 150W PROMPT PROCESSING (t/s) pp512 1354 1241 1056 877 408 pp2048 1951 1758 1480 1198 535 pp4096 2060 1839 1543 1254 561 pp8192 2043 1809 1531 1227 551 pp16384 1924 1629 1395 1135 513 pp32768 1685 1440 1215 995 453 Retention (@ 4K) 100% 89% 75% 61% 27% TTFT (seconds) @ 4K context 1.99s 2.23s 2.66s 3.27s 7.30s @ 16K context 8.52s 10.06s 11.74s 14.44s 31.96s TEXT GENERATION (t/s) tg128 19.72 19.72 19.70 19.63 12.58 tg512 19.67 19.66 19.65 19.58 12.51 Retention 100% 100% 100% 100% 64% THERMALS &amp;amp; NOISE Peak Temp (¬∞C) 73 69 68 68 65 Peak Power (W) 431 359 310 270 160 Noise (dBA) 70 59 57 54 50 Noise Level loud moderate moderate quiet quiet &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Power limiting (via nvidia-smi) to 350w seems to be the sweet spot as llm prompt processing tests show 5-15% degradation in prompt processing speed while reducing noise via 10dB and temps by about 5c across two cards stacked next next to each other.&lt;/p&gt; &lt;p&gt;Commands:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nvidia-smi -pl 350&lt;/code&gt;&lt;br /&gt; &lt;code&gt;(list cards) sudo nvidia-smi -L&lt;/code&gt;&lt;br /&gt; &lt;code&gt;(power limit specific card) sudo nvidia-smi -i 0 -pl 350&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Full results and test programs can be seen in my github: &lt;a href="https://github.com/gparemsky/48gb4090"&gt;https://github.com/gparemsky/48gb4090&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I make youtube videos about my gpu upgrade work and i made one here to show the hardware test setup: &lt;a href="https://youtu.be/V0lEeuX_b1M"&gt;https://youtu.be/V0lEeuX_b1M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am certified in accordance to IPC7095 class 2 BGA rework and do these 48GB RTX 4090 upgrades in the USA using full AD102-300 4090 core (non D) variants and have been commercially for 6 months now: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbnNBRUN3cHJwSU1DUzdfbHFyQ3NmZHlTLWJNZ3xBQ3Jtc0tseWdfYjB1NHVILWxLOTlUWlppVjZveTQtWjVwNjNqOXctWDl5RVZNNTlXcjI1UjBQbV80cVNGLUktTUhWU014d0k5RVpIdGI5d3lTWXRIRG1XSkg1Z1ptMmhSNkpsLXRRaXluZDRnWmJmV2g2bV9Ncw&amp;amp;q=https%3A%2F%2Fgpvlab.com%2F&amp;amp;v=V0lEeuX_b1M"&gt;https://gpvlab.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/computune"&gt; /u/computune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r96pgp/48gb_4090_power_limiting_tests_450_350_250w_noise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r96pgp/48gb_4090_power_limiting_tests_450_350_250w_noise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r96pgp/48gb_4090_power_limiting_tests_450_350_250w_noise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T18:13:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8jjtq</id>
    <title>More quantization visualization types (repost)</title>
    <updated>2026-02-18T23:51:43+00:00</updated>
    <author>
      <name>/u/copingmechanism</name>
      <uri>https://old.reddit.com/user/copingmechanism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt; &lt;img alt="More quantization visualization types (repost)" src="https://preview.redd.it/af1o3s52cckg1.gif?frame=1&amp;amp;width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=399ab3abe9aebeae4217cd2925119b0a76b11883" title="More quantization visualization types (repost)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by this post from &lt;a href="/u/VoidAlchemy"&gt;u/VoidAlchemy&lt;/a&gt; a few months back: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Intrusive thoughts had me try to reproduce and extend the work to include more quantization types, with/without imatrix, and some PPL/KLD measurements to see what an &amp;quot;efficient&amp;quot; quantization looks like. MXFP4 really doesn't like to participate in this sort of experiment, I don't have much faith this is a very accurate representation of the quant but oh-well.&lt;/p&gt; &lt;p&gt;The (vibe) code for this is here &lt;a href="https://codeberg.org/mailhost/quant-jaunt"&gt;https://codeberg.org/mailhost/quant-jaunt&lt;/a&gt; along with a sample of summary output (from lenna.bmp) and some specifications that might help keep the vibes on track.&lt;/p&gt; &lt;p&gt;*reposted to respect Lenna's retirement&lt;/p&gt; &lt;p&gt;**Edit: Some more intrusive thoughts later, I have updated the 'quant-jaunt' repo to have (rough) support of the ik_llama quants. It turns into 110 samples. Have also shifted to using ffmpeg to make a lossless video instead of a gif. &lt;a href="https://v.redd.it/o1h6a4u5hikg1"&gt;https://v.redd.it/o1h6a4u5hikg1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/copingmechanism"&gt; /u/copingmechanism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r8jjtq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T23:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99wrj</id>
    <title>Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]</title>
    <updated>2026-02-19T20:10:06+00:00</updated>
    <author>
      <name>/u/Disastrous_Theme5906</name>
      <uri>https://old.reddit.com/user/Disastrous_Theme5906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"&gt; &lt;img alt="Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]" src="https://preview.redd.it/492jsbpjkhkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=265f565b1e36802fcf3f5931428ad7a9cb4cc05a" title="Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5 was the most requested model since launch. Ran it through the full benchmark ‚Äî wrote a deep dive with a side-by-side vs Sonnet 4.5 and DeepSeek V3.2.&lt;/p&gt; &lt;p&gt;Results: GLM 5 survived 28 of 30 days ‚Äî the closest any bankrupt model has come to finishing. Placed #5 on the leaderboard, between Sonnet 4.5 (survived) and DeepSeek V3.2 (bankrupt Day 22). More revenue than Sonnet ($11,965 vs $10,753), less food waste than both ‚Äî but still went bankrupt from staff costs eating 67% of revenue.&lt;/p&gt; &lt;p&gt;The interesting part is how it failed. The model diagnosed every problem correctly, stored 123 memory entries, and used 82% of available tools. Then ignored its own analysis.&lt;/p&gt; &lt;p&gt;Full case study with day-by-day timeline and verbatim model quotes: &lt;a href="https://foodtruckbench.com/blog/glm-5"&gt;https://foodtruckbench.com/blog/glm-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard updated: &lt;a href="https://foodtruckbench.com"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Theme5906"&gt; /u/Disastrous_Theme5906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/492jsbpjkhkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8yvu5</id>
    <title>I built an eBPF tracer to monitor AI agents the same way you'd monitor malware in a sandbox</title>
    <updated>2026-02-19T13:14:07+00:00</updated>
    <author>
      <name>/u/M4r10_h4ck</name>
      <uri>https://old.reddit.com/user/M4r10_h4ck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;TL;DR: AI agents control their own application logs, which makes those logs useless for security monitoring. We applied the malware sandboxing principle (observe from a layer the subject can't see) and built Azazel, an open-source eBPF-based runtime tracer for containerized AI agents.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;If you're running autonomous AI agents in containers, you probably have application-level logging. The agent reports what tools it called, what it returned, maybe some reasoning traces.&lt;/p&gt; &lt;p&gt;The issue: the agent controls those logs. It writes what it chooses to write.&lt;/p&gt; &lt;p&gt;This is the same fundamental problem in malware analysis, if the subject controls its own reporting, the reporting is worthless. The solution there has been around for decades: observe from the kernel, a layer the subject cannot reach, disable, or detect.&lt;/p&gt; &lt;p&gt;We asked: why isn't anyone doing this for AI agents?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Azazel attaches 19 eBPF hook points (tracepoints + kprobes) to a target container and captures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full process tree with argv, PIDs, parent PIDs (&lt;code&gt;process_exec&lt;/code&gt;, &lt;code&gt;process_clone&lt;/code&gt;, &lt;code&gt;process_exit&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;File operations with pathnames and byte counts (&lt;code&gt;file_open&lt;/code&gt;, &lt;code&gt;file_read&lt;/code&gt;, &lt;code&gt;file_write&lt;/code&gt;, &lt;code&gt;file_rename&lt;/code&gt;, &lt;code&gt;file_unlink&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Network activity including DNS detection via kprobe on &lt;code&gt;udp_sendmsg&lt;/code&gt; (&lt;code&gt;net_connect&lt;/code&gt;, &lt;code&gt;net_bind&lt;/code&gt;, &lt;code&gt;net_dns&lt;/code&gt;, etc.)&lt;/li&gt; &lt;li&gt;Security-relevant events: &lt;code&gt;ptrace&lt;/code&gt;, &lt;code&gt;mmap&lt;/code&gt; with W+X flags, kernel module loads&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything comes out as NDJSON.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The agent cannot detect it, cannot disable it, cannot interfere with it. eBPF runs in kernel space, outside the agent's address space, invisible to any syscall it can invoke.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="http://github.com/beelzebub-labs/azazel"&gt;github.com/beelzebub-labs/azazel&lt;/a&gt;&lt;br /&gt; Full write-up: &lt;a href="http://beelzebub.ai/blog/azazel-runtime-tracing-for-ai-agents"&gt;beelzebub.ai/blog/azazel-runtime-tracing-for-ai-agents&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/M4r10_h4ck"&gt; /u/M4r10_h4ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8yvu5/i_built_an_ebpf_tracer_to_monitor_ai_agents_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8yvu5/i_built_an_ebpf_tracer_to_monitor_ai_agents_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8yvu5/i_built_an_ebpf_tracer_to_monitor_ai_agents_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T13:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r94lv2</id>
    <title>microgpt playground: Build, train, and run LLMs ‚Äî directly in your browser</title>
    <updated>2026-02-19T16:59:08+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"&gt; &lt;img alt="microgpt playground: Build, train, and run LLMs ‚Äî directly in your browser" src="https://external-preview.redd.it/YnNxc2ZxZGllaGtnMbLIzqnNOijabBHIPuWpkRNlVyT41oFEP2h_i--AGtUk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5238c6e57cfb3b7fc15857a41ebdd741d10e22f" title="microgpt playground: Build, train, and run LLMs ‚Äî directly in your browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Andrej Karpathy's microgpt, I built an educational neural network builder that breaks down &amp;quot;mysterious&amp;quot; LLMs into their primitive components. The goal is to teach people how LLMs are built, by constructing them from the ground up (and then modifying nodes, adding connections, and rewiring the graph). This is mainly just a fun experiment, but maybe there's interest in tooling like this.&lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/webml-community/microgpt-playground"&gt;https://huggingface.co/spaces/webml-community/microgpt-playground&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gikcumdiehkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T16:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r90b3a</id>
    <title>TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)</title>
    <updated>2026-02-19T14:14:54+00:00</updated>
    <author>
      <name>/u/cdr420</name>
      <uri>https://old.reddit.com/user/cdr420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"&gt; &lt;img alt="TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)" src="https://external-preview.redd.it/hbTO-tYQddJ91PQtz4lVLYP0Q8-ANtjAM3Y5l6F90rs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0c22835d34c9eb0407b7050d83b0e37a482e243" title="TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cdr420"&gt; /u/cdr420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chrisrobison/textweb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8vhhq</id>
    <title>ZUNA "Thought-to-Text": a 380M-parameter BCI foundation model for EEG data (Apache 2.0)</title>
    <updated>2026-02-19T10:11:39+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt; &lt;img alt="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" src="https://preview.redd.it/4knvh57lefkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e0e8a3a25b920023bf1670c3f5ded76380521f2" title="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- Technical paper: &lt;a href="https://zyphra.com/zuna-technical-paper"&gt;https://zyphra.com/zuna-technical-paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Technical blog: &lt;a href="https://zyphra.com/post/zuna"&gt;https://zyphra.com/post/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Hugging Face: &lt;a href="https://huggingface.co/Zyphra/ZUNA"&gt;https://huggingface.co/Zyphra/ZUNA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- GitHub: &lt;a href="https://github.com/Zyphra/zuna"&gt;https://github.com/Zyphra/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zyphra on ùïè: &lt;a href="https://x.com/ZyphraAI/status/2024114248020898015"&gt;https://x.com/ZyphraAI/status/2024114248020898015&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4knvh57lefkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T10:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8qh08</id>
    <title>I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X</title>
    <updated>2026-02-19T05:13:10+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt; &lt;img alt="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" src="https://preview.redd.it/97driy8r0ekg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=43dd9d0bf4042a0843c9b3d69c60aedb8cfa6185" title="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm absolutely sure of it. The same usual suspects, the same language, the same who stole from whom the next million dollar ideas. It's insane. NFT-bros are now peddling openclawd crypto schemes. It's all the same BS quasi-tech lingo wrapped into neverending posts with meme-like pictures full of slogans, and graphs that literally means less than nothing, that lead back to 'blockchain, blah, blah blah, agentic, blah, blah, prediction markets&amp;quot;. I have enough of this.&lt;/p&gt; &lt;p&gt;Is this the sign of a real bubble? In the fall people were talking on X about how AI is in a bubble - which is never the time for bubbles to burst. But now every grifter discovered AI agents. Now, normally it takes 1-2 years to get from one stage to another, (sorry I'm old) but we are in a super accelerated scenario. Felt like 1998 in fall. It feels we jumped to 2000 suddenly. So IDK. Smells like a bubble is expanding rapidly. Where is my thumbtack?&lt;/p&gt; &lt;p&gt;Is&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/97driy8r0ekg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=037d07f7ab4c22bb2356a92c036939830cabe611"&gt;AGI is coming on X (Sign of something?)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r92o58</id>
    <title>Seems Microsoft is really set on not repeating a Sidney incident</title>
    <updated>2026-02-19T15:47:39+00:00</updated>
    <author>
      <name>/u/frubberism</name>
      <uri>https://old.reddit.com/user/frubberism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"&gt; &lt;img alt="Seems Microsoft is really set on not repeating a Sidney incident" src="https://preview.redd.it/n9127fik2hkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1986cde3166639e1bbd65f72b426304a1b47739" title="Seems Microsoft is really set on not repeating a Sidney incident" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frubberism"&gt; /u/frubberism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9127fik2hkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T15:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r91akx</id>
    <title>llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp</title>
    <updated>2026-02-19T14:55:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"&gt; &lt;img alt="llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp" src="https://external-preview.redd.it/XOQiRlpUmQ-RDXu2-0vDquJiP5LaHys1ZKIynjJHt5g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8565cf091cdef74b5a5c04fff074756133a2db" title="llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19726"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8pztp</id>
    <title>Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2026-02-19T04:48:29+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt; &lt;img alt="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/Z3FpM3Y4czRyZGtnMWkMiFyATszvzYKXXKWtHcR48BLv2WbhyR3IwK5gi6zR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08b665759677acd0f60146592eee9094aea60bda" title="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;New Kitten models are out. Kitten ML has released open source code and weights for three new tiny expressive TTS models - 80M, 40M, 14M (all Apache 2.0)&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.com/invite/VJ86W4SURW"&gt;https://discord.com/invite/VJ86W4SURW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face - Kitten TTS V0.8:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mini 80M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-mini-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Micro 40M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-micro-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-micro-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Nano 14M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The smallest model is less than 25 MB, and around 14M parameters. All models have a major quality upgrade from previous versions, and can run on just CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Eight expressive voices:&lt;/strong&gt; 4 female and 4 male voices across all three models. They all have very high expressivity, with 80M being the best in quality. English support in this release, multilingual coming in future releases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super-small in size:&lt;/strong&gt; The 14M model is just 25 megabytes. 40M and 80M are slightly bigger, with high quality and expressivity even for longer chunks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs literally anywhere lol:&lt;/strong&gt; Forget &amp;quot;no GPU required.&amp;quot; This is designed for resource-constrained edge devices. Great news for GPU-poor folks like us.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open source (hell yeah!):&lt;/strong&gt; The models can be used for free under Apache 2.0.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unlocking on-device voice agents and applications:&lt;/strong&gt; Matches cloud TTS quality for most use cases, but runs entirely on-device (can also be hosted on a cheap GPU). If you're building voice agents, assistants, or any local speech application, no API calls needed. Free local inference. Just ship it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What changed from V0.1 to V0.8:&lt;/strong&gt; Higher quality, expressivity, and realism. Better training pipelines and 10x larger datasets.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rzgwarr4rdkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T04:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99yda</id>
    <title>Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû</title>
    <updated>2026-02-19T20:11:42+00:00</updated>
    <author>
      <name>/u/CesarOverlorde</name>
      <uri>https://old.reddit.com/user/CesarOverlorde</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt; &lt;img alt="Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû" src="https://preview.redd.it/ogkdei4udikg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8834b06cb1ae3aaa95c27230b622dd640e7d9634" title="Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CesarOverlorde"&gt; /u/CesarOverlorde &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogkdei4udikg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
