<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-05T18:25:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lrqoul</id>
    <title>i made a script to train your own transformer model on a custom dataset on your machine</title>
    <updated>2025-07-04T19:04:17+00:00</updated>
    <author>
      <name>/u/samas69420</name>
      <uri>https://old.reddit.com/user/samas69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;over the last couple of years we have seen LLMs become super duper popular and some of them are small enough to run on consumer level hardware, but in most cases we are talking about pre-trained models that can be used only in inference mode without considering the full training phase. Something that i was cuorious about tho is what kind of performance i could get if i did everything, including the full training without using other tools like lora or quantization, on my own everyday machine so i made a script that does exactly that, the script contains also a file (config.py) that can be used to tune the hyperparameters of the architecture so that anyone running it can easily set them to have the largest model as possible with their hardware (in my case with the model in the script and with a 12gb 3060 i can train about 50M params, 300M with smaller batch and mixed precision) here is the repo &lt;a href="https://github.com/samas69420/transformino"&gt;https://github.com/samas69420/transformino&lt;/a&gt; , to run the code the only thing you'll need is a dataset in the form of a csv file with a column containing the text that will be used for training (tweets, sentences from a book etc), the project also have a very low number of dependencies to make it more easy to run (you'll need only pytorch, pandas and tokenizers), every kind of feedback would be appreciated &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samas69420"&gt; /u/samas69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T19:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrjy15</id>
    <title>Anyone else feel like working with LLM libs is like navigating a minefield ?</title>
    <updated>2025-07-04T14:22:03+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've worked about 7 years in software development companies, and it's &amp;quot;easy&amp;quot; to be a software/backend/web developer because we use tools/frameworks/libs that are mature and battle-tested.&lt;/p&gt; &lt;p&gt;Problem with Django? Update it, the bug was probably fixed ages ago.&lt;/p&gt; &lt;p&gt;With LLMs it's an absolute clusterfuck. You just bought an RTX 5090? Boom, you have to recompile everything to make it work with SM_120. And I'm skipping the hellish Ubuntu installation part with cursed headers just to get it running in degraded mode.&lt;/p&gt; &lt;p&gt;Example from last week: vLLM implemented Dual Chunked Attention for Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unmerged bugfix that makes it &lt;strong&gt;UNUSABLE&lt;/strong&gt; &lt;a href="https://github.com/vllm-project/vllm/pull/19084"&gt;https://github.com/vllm-project/vllm/pull/19084&lt;/a&gt;&lt;/li&gt; &lt;li&gt;FP8 wasn't working, I had to make the PR myself &lt;a href="https://github.com/vllm-project/vllm/pull/19420"&gt;https://github.com/vllm-project/vllm/pull/19420&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Some guy broke Dual Chunk attention because of CUDA kernel and division by zero, had to write another PR &lt;a href="https://github.com/vllm-project/vllm/pull/20488"&gt;https://github.com/vllm-project/vllm/pull/20488&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.&lt;/p&gt; &lt;p&gt;Am I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????&lt;/p&gt; &lt;p&gt;And I'm not even talking about the nightmare of having to use virtualized GPUs with NVIDIA GRID drivers that you can't download yourself and that EXPLODE at the slightest conflict: &lt;/p&gt; &lt;p&gt;&lt;code&gt;driver versions &amp;lt;----&amp;gt; torch version &amp;lt;-----&amp;gt; vLLM version&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It's driving me insane.&lt;/p&gt; &lt;p&gt;I don't understand how Ggerganov can keep working on llama.cpp every single day with no break and not turn INSANE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T14:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsenvs</id>
    <title>Open Source AI Finder &amp; Newsletter</title>
    <updated>2025-07-05T17:04:22+00:00</updated>
    <author>
      <name>/u/psd-dude</name>
      <uri>https://old.reddit.com/user/psd-dude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsenvs/open_source_ai_finder_newsletter/"&gt; &lt;img alt="Open Source AI Finder &amp;amp; Newsletter" src="https://external-preview.redd.it/NrT_SNL_wa86IMbwnW4MPtd3d204mm27LdiEg89l49Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3810646d4fd053d439277b79f91d505fedc8bd5" title="Open Source AI Finder &amp;amp; Newsletter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psd-dude"&gt; /u/psd-dude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.coding-dude.com/wp/open-source-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsenvs/open_source_ai_finder_newsletter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsenvs/open_source_ai_finder_newsletter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsfj67</id>
    <title>What motherboard for 4xK80s?</title>
    <updated>2025-07-05T17:42:38+00:00</updated>
    <author>
      <name>/u/itsacommon</name>
      <uri>https://old.reddit.com/user/itsacommon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking to build a budget experimentation machine for inference and perhaps training some multimodal models and such. I saw that there are lots of refurbished K80s available on eBay for quite cheap that appear to be in ok condition. I‚Äôm wondering what kind of backbone I would need to support say 4 or even 8x of them. Has anyone heard of similar builds?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsacommon"&gt; /u/itsacommon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfj67/what_motherboard_for_4xk80s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfj67/what_motherboard_for_4xk80s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfj67/what_motherboard_for_4xk80s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:42:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsfv8c</id>
    <title>SoTA Audio native models?</title>
    <updated>2025-07-05T17:57:28+00:00</updated>
    <author>
      <name>/u/Theboyscampus</name>
      <uri>https://old.reddit.com/user/Theboyscampus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is locallama but what is the SoTA speech to speech model right now? We've been testing with gemini 2.5 audio native preview at work and while it still has some issues, it's looking real good. Ive been limited to Gemini cause we got free GCP credits to play with at work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Theboyscampus"&gt; /u/Theboyscampus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfv8c/sota_audio_native_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfv8c/sota_audio_native_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfv8c/sota_audio_native_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsaczg</id>
    <title>Utilize iGPU (AMD Radeon 780m) even if the dGPU is running via MUX switch</title>
    <updated>2025-07-05T13:51:24+00:00</updated>
    <author>
      <name>/u/panther_ra</name>
      <uri>https://old.reddit.com/user/panther_ra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;br /&gt; I'm wandering if it possible to use iGPU for inference in Windows despite the dGPU is online and connected to the Display.&lt;br /&gt; The whole idea that I can use idling iGPU for the AI tasks (small 7b models).&lt;br /&gt; The MUX switch itself is not limiting the iGPU for the general tasks (not related to the video rendering, right?).&lt;br /&gt; I've a modern laptop with a ryzen 7840hs and MUX switch for the dGPU - RTX4060.&lt;br /&gt; I know, that I can do opposite - run a display on the iGPU and use dGPU for the AI inference. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panther_ra"&gt; /u/panther_ra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T13:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrzrmd</id>
    <title>Best model at the moment for 128GB M4 Max</title>
    <updated>2025-07-05T02:44:52+00:00</updated>
    <author>
      <name>/u/Xx_DarDoAzuL_xX</name>
      <uri>https://old.reddit.com/user/Xx_DarDoAzuL_xX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;Recently got myself a brand new M4 Max 128Gb ram Mac Studio. &lt;/p&gt; &lt;p&gt;I saw some old posts about the best models to use with this computer, but I am wondering if that has changed throughout the months/years. &lt;/p&gt; &lt;p&gt;Currently, what is the best model and settings to use with this machine? &lt;/p&gt; &lt;p&gt;Cheers! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xx_DarDoAzuL_xX"&gt; /u/Xx_DarDoAzuL_xX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T02:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsevb1</id>
    <title>Finetuning a youtuber persona without expensive hardware or buying expensive cloud computing</title>
    <updated>2025-07-05T17:13:12+00:00</updated>
    <author>
      <name>/u/Khushalgogia</name>
      <uri>https://old.reddit.com/user/Khushalgogia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I want to finetune any model good or bad, into a youtuber persona My idea is i will download youtube videos of that youtuber and generate transcript and POFF! I have the youtuber data, now i just need train the model on that data&lt;/p&gt; &lt;p&gt;My idea is Gemini have gems, can that be useful? If not, can i achieve my goal for free? Btw, i have gemini advanced subscription &lt;/p&gt; &lt;p&gt;P.S, I am not a technical person, i can write python code, but thats it, so think of me as dumb, and then read the question again&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Khushalgogia"&gt; /u/Khushalgogia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsevb1/finetuning_a_youtuber_persona_without_expensive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsevb1/finetuning_a_youtuber_persona_without_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsevb1/finetuning_a_youtuber_persona_without_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrss4u</id>
    <title>THUDM/GLM-4.1V-9B-Thinking looks impressive</title>
    <updated>2025-07-04T20:37:49+00:00</updated>
    <author>
      <name>/u/ConfidentTrifle7247</name>
      <uri>https://old.reddit.com/user/ConfidentTrifle7247</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"&gt; &lt;img alt="THUDM/GLM-4.1V-9B-Thinking looks impressive" src="https://preview.redd.it/62vkwepq4xaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ded30c7e3562f37aa41502883d7aa8b656c68551" title="THUDM/GLM-4.1V-9B-Thinking looks impressive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking forward to the GGUF quants to give it a shot. Would love if the awesome Unsloth team did their magic here, too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking"&gt;https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConfidentTrifle7247"&gt; /u/ConfidentTrifle7247 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/62vkwepq4xaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrsf6x</id>
    <title>OCRFlux-3B</title>
    <updated>2025-07-04T20:21:20+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"&gt; &lt;img alt="OCRFlux-3B" src="https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dcd5f194d87db7612ad57b0c05d46dc6f0cfae3" title="OCRFlux-3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the HF repo:&lt;/p&gt; &lt;p&gt;&amp;quot;OCRFlux is a multimodal large language model based toolkit for converting PDFs and images into clean, readable, plain Markdown text. It aims to push the current state-of-the-art to a significantly higher level.&amp;quot;&lt;/p&gt; &lt;p&gt;Claims to beat other models like olmOCR and Nanonets-OCR-s by a substantial margin. Read online that it can also merge content spanning multiple pages such as long tables. There's also a docker container with the full toolkit and a github repo. What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ChatDOC/OCRFlux-3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls8c2s</id>
    <title>Aveni Labs releases FinLLM technical report: a 7B domain-specific model for financial services outperforming some frontier LLMs</title>
    <updated>2025-07-05T12:04:41+00:00</updated>
    <author>
      <name>/u/Ok-Cryptographer9361</name>
      <uri>https://old.reddit.com/user/Ok-Cryptographer9361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read the &lt;a href="https://aveni.ai/wp-content/uploads/2025/05/Aveni-Detect-Combined-Case-Study.pdf"&gt;FinLLM technical report&lt;/a&gt; from Aveni Labs. It‚Äôs a 7B parameter language model built specifically for UK financial services, trained with regulatory alignment and fine-tuned for tasks like compliance monitoring, adviser QA, and KYC review.&lt;/p&gt; &lt;p&gt;Key points that stood out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms GPT-4o mini, Gemini 1.5 Flash, and LLaMA-based models on financial domain tasks like tabular data analysis, multi-turn customer dialogue, long-context reasoning, and document QA&lt;/li&gt; &lt;li&gt;Built using a filtering pipeline called Finance Classifier 2.0 that selects high-quality, in-domain training data (regulatory guidance, advice transcripts, etc.)&lt;/li&gt; &lt;li&gt;Open 1B and 7B variants designed for fine-tuning and secure deployment in VPC or on-prem environments&lt;/li&gt; &lt;li&gt;Optimized for agentic RAG setups where traceability and source-grounding are required&lt;/li&gt; &lt;li&gt;Benchmarked using their own dataset, AveniBench, which focuses on real FS tasks like consumer vulnerability detection and conduct risk spotting&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They are also working on a 30B version, but the current 7B model is already matching or beating much larger models in this domain.&lt;/p&gt; &lt;p&gt;Anyone else here working on small or mid-scale domain-specific models in regulated industries? Curious how others are handling fine-tuning and evaluation for high-risk applications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Cryptographer9361"&gt; /u/Ok-Cryptographer9361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T12:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lri12r</id>
    <title>Great price on a 5090</title>
    <updated>2025-07-04T12:53:29+00:00</updated>
    <author>
      <name>/u/psdwizzard</name>
      <uri>https://old.reddit.com/user/psdwizzard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt; &lt;img alt="Great price on a 5090" src="https://preview.redd.it/1en1lic1uuaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7df49758108c1feb283e4286654e01dbd232a219" title="Great price on a 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About to pull the trigger on this one I can't believe how cheap it is. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psdwizzard"&gt; /u/psdwizzard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1en1lic1uuaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T12:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsflii</id>
    <title>Anyone built a home 2√ó A100 SXM4 node?</title>
    <updated>2025-07-05T17:45:34+00:00</updated>
    <author>
      <name>/u/Fun_Nefariousness228</name>
      <uri>https://old.reddit.com/user/Fun_Nefariousness228</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm doing self-funded AI research and recently got access to 2√ó NVIDIA A100 SXM4 GPUs. I want to build a quiet, stable node at home to run local models and training workloads ‚Äî no cloud.&lt;/p&gt; &lt;p&gt;Has anyone here actually built a DIY system with A100 SXM4s (not PCIe)? If so: What HGX carrier board or server chassis did you use? How did you handle power + cooling safely at home? Any tips on finding used baseboards or reference systems?&lt;/p&gt; &lt;p&gt;I‚Äôm not working for any company ‚Äî just serious about doing advanced AI work locally and learning by building. Happy to share progress once it‚Äôs working.&lt;/p&gt; &lt;p&gt;Thanks in advance ‚Äî would love any help or photos from others doing the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Nefariousness228"&gt; /u/Fun_Nefariousness228 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:45:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsfpi0</id>
    <title>Help setting up an uncensored local LLM for a text-based RPG adventure / DMing</title>
    <updated>2025-07-05T17:50:29+00:00</updated>
    <author>
      <name>/u/tac7878</name>
      <uri>https://old.reddit.com/user/tac7878</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I apologize if this is the Nth time something like this was posted, but I am just at my wit's end. As the title says, I need help setting up an uncensored local LLM for the purpose of running / DMing a single player text-based RPG adventure. I have tried online services like Kobold AI Lite, etc. but I always encounter issues with them (AI deciding my actions on my behalf even after numerous corrections, AI forgetting important details just after they occurred, etc.), perhaps due to my lack of knowledge and experience in this field.&lt;/p&gt; &lt;p&gt;To preface, I'm basically a boomer when it comes to AI related things. This all started when I tried a mobile app called Everweave and I was hooked immediately. Unfortunately, the monthly limit and monetization scheme is not something I am inclined to participate in. After trying online services and finding them unsatisfactory (see reasons above), I instead decided to try hosting an LLM that does the same, locally. I tried to search online and watch videos, but there is only so much I can &amp;quot;learn&amp;quot; if I couldn't even understand the terminologies being used. I really did try to take this on by myself and be independent but my brain just could not absorb this new paradigm.&lt;/p&gt; &lt;p&gt;So far what I had done is download LM Studio and search for LLMs that would fit my intended purpose and that works with the limitations of my machine (R7 4700G 3.6 GHz, 24 GB RAM, RX 6600 8 GB VRAM). Chat GPT suggested I use Mythomist 7b and Mythomax L2 13b, so I tried both. I also wrote a long, detailed system prompt to tell it exactly what I want it to do, but the issues tend to persist.&lt;/p&gt; &lt;p&gt;So my question is, can anyone who has done the same and found it without any issues, tell me exactly what I should do? Explain it to me like I'm 5, because with all these new emerging fields I'm pretty much a child.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tac7878"&gt; /u/tac7878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls95oj</id>
    <title>Apple MLX Quantizations Royal Rumble üî•</title>
    <updated>2025-07-05T12:50:36+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt; &lt;img alt="Apple MLX Quantizations Royal Rumble üî•" src="https://a.thumbs.redditmedia.com/-1Fa5pMoUdufyX7EbInbdiYRv8uh6lx6DYYAentaN_0.jpg" title="Apple MLX Quantizations Royal Rumble üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-8B model using Winogrande as benchmark.&lt;br /&gt; DWQ and 5bit rule! &lt;/p&gt; &lt;p&gt;ü•á dwq ‚Äì 68.82%&lt;br /&gt; ü•à 5bit ‚Äì 68.51%&lt;br /&gt; ü•â 6bit ‚Äì 68.35%&lt;br /&gt; bf16 ‚Äì 67.64%&lt;br /&gt; dynamic ‚Äì 67.56%&lt;br /&gt; 8bit ‚Äì 67.56%&lt;br /&gt; 4bit ‚Äì 66.30%&lt;br /&gt; 3bit ‚Äì 63.85%&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3"&gt;https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T12:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrz5uy</id>
    <title>Got some real numbers how llama.cpp got FASTER over last 3-months</title>
    <updated>2025-07-05T02:08:31+00:00</updated>
    <author>
      <name>/u/AggressiveHunt2300</name>
      <uri>https://old.reddit.com/user/AggressiveHunt2300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I am author of Hyprnote(&lt;a href="https://github.com/fastrepl/hyprnote"&gt;https://github.com/fastrepl/hyprnote&lt;/a&gt;) - privacy-first notepad for meetings. We regularly test out the AI models we use in various devices to make sure it runs well.&lt;/p&gt; &lt;p&gt;When testing MacBook, Qwen3 1.7B is used, and for Windows, Qwen3 0.6B is used. (All Q4 KM)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/tree/b5828"&gt;b5828&lt;/a&gt;(newer) .. &lt;a href="https://github.com/ggml-org/llama.cpp/tree/b5162"&gt;b5162&lt;/a&gt;(older)&lt;/p&gt; &lt;p&gt;Thinking of writing lot longer blog post with lots of numbers &amp;amp; what I learned during the experiment. Please let me know if that is something you guys are interested in.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;OS&lt;/th&gt; &lt;th align="left"&gt;SoC&lt;/th&gt; &lt;th align="left"&gt;RAM&lt;/th&gt; &lt;th align="left"&gt;Compute&lt;/th&gt; &lt;th align="left"&gt;Prefill Tok/s&lt;/th&gt; &lt;th align="left"&gt;Gen Tok/s&lt;/th&gt; &lt;th align="left"&gt;Median Load (ms)&lt;/th&gt; &lt;th align="left"&gt;Prefill RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;Gen RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;Load RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;SHA&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook Pro 14-inch&lt;/td&gt; &lt;td align="left"&gt;macOS 15.3.2&lt;/td&gt; &lt;td align="left"&gt;Apple M2 Pro&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;Metal&lt;/td&gt; &lt;td align="left"&gt;615.20&lt;/td&gt; &lt;td align="left"&gt;21.69&lt;/td&gt; &lt;td align="left"&gt;362.52&lt;/td&gt; &lt;td align="left"&gt;2332.28&lt;/td&gt; &lt;td align="left"&gt;2337.67&lt;/td&gt; &lt;td align="left"&gt;2089.56&lt;/td&gt; &lt;td align="left"&gt;b5828&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;571.85&lt;/td&gt; &lt;td align="left"&gt;21.43&lt;/td&gt; &lt;td align="left"&gt;372.32&lt;/td&gt; &lt;td align="left"&gt;2341.77&lt;/td&gt; &lt;td align="left"&gt;2347.05&lt;/td&gt; &lt;td align="left"&gt;2102.27&lt;/td&gt; &lt;td align="left"&gt;b5162&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HP EliteBook 660 16-inch G11&lt;/td&gt; &lt;td align="left"&gt;Windows 11.24H2&lt;/td&gt; &lt;td align="left"&gt;Intel Core Ultra 7 155U&lt;/td&gt; &lt;td align="left"&gt;32GB&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;162.52&lt;/td&gt; &lt;td align="left"&gt;14.05&lt;/td&gt; &lt;td align="left"&gt;1533.99&lt;/td&gt; &lt;td align="left"&gt;3719.23&lt;/td&gt; &lt;td align="left"&gt;3641.65&lt;/td&gt; &lt;td align="left"&gt;3535.43&lt;/td&gt; &lt;td align="left"&gt;b5828&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;148.52&lt;/td&gt; &lt;td align="left"&gt;12.89&lt;/td&gt; &lt;td align="left"&gt;2487.26&lt;/td&gt; &lt;td align="left"&gt;3719.96&lt;/td&gt; &lt;td align="left"&gt;3642.34&lt;/td&gt; &lt;td align="left"&gt;3535.24&lt;/td&gt; &lt;td align="left"&gt;b5162&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveHunt2300"&gt; /u/AggressiveHunt2300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T02:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls3gho</id>
    <title>Open source tool for generating training datasets from text files and pdf for fine-tuning language models.</title>
    <updated>2025-07-05T06:36:35+00:00</updated>
    <author>
      <name>/u/Idonotknow101</name>
      <uri>https://old.reddit.com/user/Idonotknow101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall I made a new open-source tool.&lt;/p&gt; &lt;p&gt;It's an app that &lt;strong&gt;creates training data for AI models from your text and PDFs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to make your own AI smarter. The data comes out ready for different models.&lt;/p&gt; &lt;p&gt;Super simple, super useful, and it's all open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Idonotknow101"&gt; /u/Idonotknow101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MonkWarrior08/Dataset_Generator_for_Fine-tuning?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T06:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsdxc2</id>
    <title>New app for locally running AI models on Android your smartphone</title>
    <updated>2025-07-05T16:32:35+00:00</updated>
    <author>
      <name>/u/RomanKryvolapov</name>
      <uri>https://old.reddit.com/user/RomanKryvolapov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt; &lt;img alt="New app for locally running AI models on Android your smartphone" src="https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dba60f6210ef2ca39ebd034795d1371f991f9f7d" title="New app for locally running AI models on Android your smartphone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;I create Android application for locally running AI models on smartphone&lt;/p&gt; &lt;p&gt;I am interested in your opinion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher"&gt;https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5"&gt;https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RomanKryvolapov"&gt; /u/RomanKryvolapov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T16:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls9jvu</id>
    <title>Which open source LLM has the most genuine sense of humor?</title>
    <updated>2025-07-05T13:10:47+00:00</updated>
    <author>
      <name>/u/UltrMgns</name>
      <uri>https://old.reddit.com/user/UltrMgns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm genuinely struggling with everything out there in terms of making me smile and general joke quality. If there is such a model, at what settings should it run? (temp/top_k etc). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UltrMgns"&gt; /u/UltrMgns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T13:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsfmcj</id>
    <title>I created this tool I named ReddSummary.com ‚Äì just paste a link and boom you got the summary</title>
    <updated>2025-07-05T17:46:40+00:00</updated>
    <author>
      <name>/u/Himanshu507</name>
      <uri>https://old.reddit.com/user/Himanshu507</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"&gt; &lt;img alt="I created this tool I named ReddSummary.com ‚Äì just paste a link and boom you got the summary" src="https://preview.redd.it/2exxosoue3bf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c1d1e13fd17ff4381b38e36072d089c11c07e48" title="I created this tool I named ReddSummary.com ‚Äì just paste a link and boom you got the summary" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have developed the web app and chrome extension to summarize the long reddit threads discussion using chatgpt, it helps user to analyize thread discussions and sentiments of the discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Himanshu507"&gt; /u/Himanshu507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2exxosoue3bf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls70r2</id>
    <title>Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance</title>
    <updated>2025-07-05T10:43:05+00:00</updated>
    <author>
      <name>/u/d5dq</name>
      <uri>https://old.reddit.com/user/d5dq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt; &lt;img alt="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" src="https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86ebf97301bc00e90b9b236ebf2a2bb13dae2a1a" title="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d5dq"&gt; /u/d5dq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T10:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrsx20</id>
    <title>How RAG actually works ‚Äî a toy example with real math</title>
    <updated>2025-07-04T20:44:15+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most RAG explainers jump into theories and scary infra diagrams. Here‚Äôs the tiny end-to-end demo that can easy to understand for me:&lt;/p&gt; &lt;p&gt;Suppose we have a documentation like this: &amp;quot;Boil an egg. Poach an egg. How to change a tire&amp;quot;&lt;/p&gt; &lt;h1&gt;Step 1: Chunk&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;S0: &amp;quot;Boil an egg&amp;quot; S1: &amp;quot;Poach an egg&amp;quot; S2: &amp;quot;How to change a tire&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 2: Embed&lt;/h1&gt; &lt;p&gt;After the words ‚ÄúBoil an egg‚Äù pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned ‚Äúmeaning point‚Äù in vector space.&lt;/p&gt; &lt;p&gt;Toy demo values:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;V0 = [ 0.90, 0.10, 0.00, 0.10] # ‚ÄúBoil an egg‚Äù V1 = [ 0.88, 0.12, 0.00, 0.09] # ‚ÄúPoach an egg‚Äù V2 = [-0.20, 0.40, 0.80, 0.10] # ‚ÄúHow to change a tire‚Äù &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)&lt;/p&gt; &lt;h1&gt;Step 3: Normalize&lt;/h1&gt; &lt;p&gt;Put every vector on the unit sphere:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Normalised (unit-length) vectors V0ÃÇ = [ 0.988, 0.110, 0.000, 0.110] # 0.988¬≤ + 0.110¬≤ + 0.000¬≤ + 0.110¬≤ ‚âà 1.000 ‚Üí 1 V1ÃÇ = [ 0.986, 0.134, 0.000, 0.101] # 0.986¬≤ + 0.134¬≤ + 0.000¬≤ + 0.101¬≤ ‚âà 1.000 ‚Üí 1 V2ÃÇ = [-0.217, 0.434, 0.868, 0.108] # (-0.217)¬≤ + 0.434¬≤ + 0.868¬≤ + 0.108¬≤ ‚âà 1.001 ‚Üí 1 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 4: Index&lt;/h1&gt; &lt;p&gt;Drop V0^,V1^,V2^ into a similarity index (FAISS, Qdrant, etc.).&lt;br /&gt; Keep a side map &lt;code&gt;{0:S0, 1:S1, 2:S2}&lt;/code&gt; so IDs can turn back into text later.&lt;/p&gt; &lt;h1&gt;Step 5: Similarity Search&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;User asks&lt;/strong&gt;&lt;br /&gt; ‚ÄúBest way to cook an egg?‚Äù&lt;/p&gt; &lt;p&gt;We embed this sentence and normalize it as well, which gives us something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ = [0.989, 0.086, 0.000, 0.118] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then we need to find the vector that‚Äôs &lt;em&gt;closest&lt;/em&gt; to this one.&lt;br /&gt; The most common way is cosine similarity ‚Äî often written as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(Œ∏) = (A ‚ãÖ B) / (‚ÄñA‚Äñ √ó ‚ÄñB‚Äñ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But since we already normalized all vectors,&lt;br /&gt; ‚ÄñA‚Äñ = ‚ÄñB‚Äñ = 1 ‚Üí so the formula becomes just:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(Œ∏) = A ‚ãÖ B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means we just need to calculate the &lt;strong&gt;dot product&lt;/strong&gt; between the user input vector and each stored vector.&lt;br /&gt; If two vectors are exactly the same, dot product = 1.&lt;br /&gt; So we sort by which ones have values closest to 1 - higher = more similar.&lt;/p&gt; &lt;p&gt;Let‚Äôs calculate the scores (example, not real)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ ‚ãÖ V0ÃÇ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110) ‚âà 0.977 + 0.009 + 0 + 0.013 = 0.999 Vi^ ‚ãÖ V1ÃÇ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101) ‚âà 0.975 + 0.012 + 0 + 0.012 = 0.999 Vi^ ‚ãÖ V2ÃÇ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108) ‚âà -0.214 + 0.037 + 0 + 0.013 = -0.164 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So we find that sentence 0 (‚ÄúBoil an egg‚Äù) and sentence 1 (‚ÄúPoach an egg‚Äù)&lt;br /&gt; are both very close to the user input.&lt;/p&gt; &lt;p&gt;We &lt;strong&gt;retrieve those two as context&lt;/strong&gt;, and pass them to the LLM.&lt;br /&gt; Now the LLM has relevant info to answer accurately, instead of guessing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsdjnb</id>
    <title>Llama-4-Maverick 402B on a oneplus 13</title>
    <updated>2025-07-05T16:15:49+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt; &lt;img alt="Llama-4-Maverick 402B on a oneplus 13" src="https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcc6e776cfcd25daabf167946f5f41fbb1c23e70" title="Llama-4-Maverick 402B on a oneplus 13" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)&lt;/p&gt; &lt;p&gt;Here's the command used: &lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p &amp;quot;hi&amp;quot; -c 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/QwkaFHf"&gt;https://imgur.com/a/QwkaFHf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;picture shows the model layers as seen on huggingface tensor viewer: &lt;/p&gt; &lt;p&gt;- Green: in RAM&lt;/p&gt; &lt;p&gt;- Red: read from DISC&lt;/p&gt; &lt;p&gt;Other MOEs will have less impressive results due to a difference in architecture.&lt;/p&gt; &lt;p&gt;Greater results can be obtained by increasing the quantity of Q4_0 tensors for repeating layers in place of other types IQ4_XS, Q6_K, Q4_K, Q3_K, Q2_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tletuj5ov2bf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T16:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls5b89</id>
    <title>Powerful 4B Nemotron based finetune</title>
    <updated>2025-07-05T08:43:38+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt; &lt;img alt="Powerful 4B Nemotron based finetune" src="https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d882c129728e2bb772cd8f145ea68d43d0c6637" title="Powerful 4B Nemotron based finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I present to you &lt;strong&gt;Impish_LLAMA_4B&lt;/strong&gt;, one of the most powerful roleplay \ adventure finetunes at its size category.&lt;/p&gt; &lt;p&gt;TL;DR:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;incredibly powerful&lt;/strong&gt; roleplay model for the size. It has &lt;strong&gt;sovl !&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;Adventure&lt;/strong&gt; very well for such size!&lt;/li&gt; &lt;li&gt;Characters have &lt;strong&gt;agency&lt;/strong&gt;, and might surprise you! &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here"&gt;See the examples in the logs&lt;/a&gt; üôÇ&lt;/li&gt; &lt;li&gt;Roleplay &amp;amp; Assistant data used plenty of &lt;strong&gt;16K&lt;/strong&gt; examples.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very responsive&lt;/strong&gt;, feels 'in the moment', kicks &lt;strong&gt;far above&lt;/strong&gt; its weight. You might forget it's a &lt;strong&gt;4B&lt;/strong&gt; if you squint.&lt;/li&gt; &lt;li&gt;Based on a lot of the data in &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;Impish_Magic_24B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super long context&lt;/strong&gt; as well as context attention for &lt;strong&gt;4B&lt;/strong&gt;, personally tested for up to &lt;strong&gt;16K&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Can run on &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; with ease.&lt;/li&gt; &lt;li&gt;Trained on over &lt;strong&gt;400m tokens&lt;/strong&gt; with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.&lt;/li&gt; &lt;li&gt;Very decent assistant.&lt;/li&gt; &lt;li&gt;Mostly &lt;strong&gt;uncensored&lt;/strong&gt; while retaining plenty of intelligence.&lt;/li&gt; &lt;li&gt;Less &lt;strong&gt;positivity&lt;/strong&gt; &amp;amp; &lt;strong&gt;uncensored&lt;/strong&gt;, &lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data, adjusted for &lt;strong&gt;4B&lt;/strong&gt;, with serious upgrades. Training data contains combat scenarios. And it &lt;strong&gt;shows&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt;Trained on &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity, quirkiness, and naturally‚Äî less positivity, and the inclination to... argue üôÉ&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Short length&lt;/strong&gt; response (1-3 paragraphs, usually 1-2). CAI Style.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the model card for more details &amp;amp; character cards for Roleplay \ Adventure:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (~&lt;strong&gt;3600&lt;/strong&gt; tokens per second, &lt;strong&gt;96 threads&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd"&gt;Horde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;~3600 tokens per second, 96 threads)Would love some feedback! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T08:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsbhzs</id>
    <title>When Should We Expect Affordable Hardware That Will Run Large LLMs With Usable Speed?</title>
    <updated>2025-07-05T14:44:47+00:00</updated>
    <author>
      <name>/u/spiritxfly</name>
      <uri>https://old.reddit.com/user/spiritxfly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.&lt;/p&gt; &lt;p&gt;When can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spiritxfly"&gt; /u/spiritxfly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T14:44:47+00:00</published>
  </entry>
</feed>
