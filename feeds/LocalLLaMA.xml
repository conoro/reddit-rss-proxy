<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-26T13:07:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qmsk9w</id>
    <title>LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens</title>
    <updated>2026-01-25T19:22:14+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt; &lt;img alt="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" src="https://preview.redd.it/gai51kz2pjfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f44d218e6b2a5dc8982ffb434c4c01e0cf195277" title="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Generated from lineage-128 and lineage-192 &lt;a href="http://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; &lt;a href="https://github.com/fairydreaming/lineage-bench-results/tree/main/lineage-8_64_128_192"&gt;benchmark results&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Sorry for overlapping labels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gai51kz2pjfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T19:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn68ih</id>
    <title>Suggestion Needed: Large Context Model For Summarizing Text</title>
    <updated>2026-01-26T04:37:02+00:00</updated>
    <author>
      <name>/u/Professional-Yak4359</name>
      <uri>https://old.reddit.com/user/Professional-Yak4359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to summarize very long, somewhat technical papers, and I am wondering if anyone has any good suggestions? I do not need the model to be super smart; I just want it to be able to chew through 200 pages or so at a time, in context, so I can ask questions. &lt;/p&gt; &lt;p&gt;In terms of hardware, I am rocking 8 x 5070 Ti under Ubuntu in a headless box where I serve VLLM to myself on another desktop. Ideally, I would love to have something 256k or even 512k context that fits fully in VRAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Yak4359"&gt; /u/Professional-Yak4359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn68ih/suggestion_needed_large_context_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn68ih/suggestion_needed_large_context_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn68ih/suggestion_needed_large_context_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T04:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn7mmh</id>
    <title>I built a local "Cognitive IDE" to manage multi-agent workflows</title>
    <updated>2026-01-26T05:46:55+00:00</updated>
    <author>
      <name>/u/Healthy-Basil-7521</name>
      <uri>https://old.reddit.com/user/Healthy-Basil-7521</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"&gt; &lt;img alt="I built a local &amp;quot;Cognitive IDE&amp;quot; to manage multi-agent workflows" src="https://b.thumbs.redditmedia.com/kN_DAjYgrrfSfACl7J_j_VELjorkGTe4hpFMMfGDYzc.jpg" title="I built a local &amp;quot;Cognitive IDE&amp;quot; to manage multi-agent workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After months of using llms for a research project and personal use , I hit a wall. I needed to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maintain separate &amp;quot;expert&amp;quot; agents that remember their domain&lt;/li&gt; &lt;li&gt;See how ideas flowed between conversations&lt;/li&gt; &lt;li&gt;Pull context from multiple chats into a single synthesis&lt;/li&gt; &lt;li&gt;A quick way to build detailed system personas&lt;/li&gt; &lt;li&gt;Search by concept not by chat name&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Cognitive OS&lt;/strong&gt; - a local-first desktop environment for managing AI workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Persistent State:&lt;/strong&gt; Agents are treated as files, not temporary sessions. They remember everything across reloads. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge Graph:&lt;/strong&gt; Visualizes the &amp;quot;lineage of thought.&amp;quot; You can see exactly how an insight flowed from Agent A to Agent B.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Context Forwarding (MCF):&lt;/strong&gt; Select specific messages from multiple different agents and bundle them into a payload to pipe into a &amp;quot;Synthesis Bot.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JIT (Just-In-Time) Injection:&lt;/strong&gt; Instead of dumping a whole chat history, you can query an agent to generate a specific summary of its knowledge on the fly, and inject that summary into another agent's context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrated Prompter Bot:&lt;/strong&gt; A built-in meta-agent dedicated to interviewing you and crafting high-fidelity system prompts to spin up new experts quickly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Search:&lt;/strong&gt; A global memory search that finds insights by concept, not just keyword.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Librarian Bot:&lt;/strong&gt; I have initial deterministic labels based on how the chat was created, and also overtime a dynamic labeling that uses the JIT to give more nuanced labels for chats.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python Backend (Logic &amp;amp; State Management)&lt;/li&gt; &lt;li&gt;Frontend (The UI in the screenshot is hosted on ViteJs, but I will add it to the source code)&lt;/li&gt; &lt;li&gt;Model Agnostic (Currently running on Gemini Flash, but architected to swap easily)&lt;/li&gt; &lt;li&gt;100% Local Storage (JSON filesystem + Vector DB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking for feedback from other users hitting the same walls. What workflows would you want supported?&lt;/p&gt; &lt;p&gt;&lt;a href="https://stackblitz.com/edit/vitejs-vite-ske72rwt?file=src%2FApp.tsx"&gt;Link for demo seen in image&lt;/a&gt; (Not every tab mentioned is in the demo, I just wanted to see if a larger audience than me is interested in the idea)&lt;br /&gt; &lt;a href="https://github.com/8lak/Cognitive_OS"&gt;Repo &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nx0ko55jtmfg1.png?width=1917&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfbce46e34e8bef9d49b34c3be126f41816b35f9"&gt;https://preview.redd.it/nx0ko55jtmfg1.png?width=1917&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfbce46e34e8bef9d49b34c3be126f41816b35f9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Basil-7521"&gt; /u/Healthy-Basil-7521 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T05:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnespt</id>
    <title>Is it possible to connect local LLM with cloud GPU?</title>
    <updated>2026-01-26T12:32:34+00:00</updated>
    <author>
      <name>/u/Significant_Loss_541</name>
      <uri>https://old.reddit.com/user/Significant_Loss_541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been running GLM 4.6 via API for agentic coding tasks like tool calling and multi-step reasoning on my python repos.... and it's solid on benchmarks but the privacy leaks from sending data to providers are really hitting me. Want to shift to fully local inference for sensitive workflows without constant API calls.&lt;/p&gt; &lt;p&gt;Sadly, the issue is that my laptop (windows core i5 9th Gen) lacks thunderbolt 3/eGPU support and cannot handle external NVIDIA cards natively... so integrated graphics and RAM top out at 16GB which is barely enough for q4 quants on 30B models without offloading hacks that kill speed&lt;/p&gt; &lt;p&gt;Currently thinking of bridging to cloud GPUs for the heavy lifting while keeping the LLM local ish like using hosted instances from deepinfra, together or maybe vast as a remote backend for Vllm inference servers. Technically, could i tunnel the API endpoint over SSH or VPN to my local setup like open webUI proxying the cloud GPU? or would latency spikes (100-200ms roundtrip) make token gen inconsistent for interactive stuff?? Worried about context drift or dropout on local chains too..... anyone got a seamless hybrid config like this without running major perf hits? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Loss_541"&gt; /u/Significant_Loss_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnespt/is_it_possible_to_connect_local_llm_with_cloud_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnespt/is_it_possible_to_connect_local_llm_with_cloud_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnespt/is_it_possible_to_connect_local_llm_with_cloud_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T12:32:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnfb9t</id>
    <title>Local LLMs CPU usage</title>
    <updated>2026-01-26T12:57:22+00:00</updated>
    <author>
      <name>/u/FixGood6833</name>
      <uri>https://old.reddit.com/user/FixGood6833</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Should localllms utilize CPU by default? I see VRAM usage but GPU usage itself is very low while CPU is 100%. &lt;/p&gt; &lt;p&gt;I am running few local LLM 7b, 8b and sometimes 20b.&lt;/p&gt; &lt;p&gt;My specs:&lt;/p&gt; &lt;p&gt;CPU: 9800X3D&lt;/p&gt; &lt;p&gt;GPU: RX 6900XT 16GB&lt;/p&gt; &lt;p&gt;RAM: 48GB&lt;/p&gt; &lt;p&gt;OS: Bazzite&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FixGood6833"&gt; /u/FixGood6833 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnfb9t/local_llms_cpu_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnfb9t/local_llms_cpu_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnfb9t/local_llms_cpu_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T12:57:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn1uux</id>
    <title>On-device tool calling with Llama 3.2 3B on iPhone - made it suggest sushi restaurants [Open Source, React Native]</title>
    <updated>2026-01-26T01:20:05+00:00</updated>
    <author>
      <name>/u/New_Inflation_6927</name>
      <uri>https://old.reddit.com/user/New_Inflation_6927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just built a tool calling POC - Llama 3.2 3B doing tool calls entirely on-device (iPhone 16 Pro Max).&lt;/p&gt; &lt;p&gt;Demo: DoorDash-style food ordering app where you chat with a local LLM that searches restaurants and helps you order.&lt;/p&gt; &lt;p&gt;On-device: LLM inference + Tool call decisions + Response parsing&lt;br /&gt; API: Foursquare for restaurant places info&lt;/p&gt; &lt;p&gt;No cloud AI. The brain is local, it just reaches out for data when needed.&lt;/p&gt; &lt;p&gt;Stack: React Native, RunAnywhere SDK (open source), Llama 3.2 3B&lt;/p&gt; &lt;p&gt;Source code in comments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qn1uux/video/sugg6e6ehlfg1/player"&gt;https://reddit.com/link/1qn1uux/video/sugg6e6ehlfg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Inflation_6927"&gt; /u/New_Inflation_6927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn1uux/ondevice_tool_calling_with_llama_32_3b_on_iphone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn1uux/ondevice_tool_calling_with_llama_32_3b_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn1uux/ondevice_tool_calling_with_llama_32_3b_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T01:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn7ew9</id>
    <title>RAG Paper 26.1.22</title>
    <updated>2026-01-26T05:35:56+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.16027v1"&gt;Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15849v1"&gt;CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15820v1"&gt;ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15816v1"&gt;Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15678v1"&gt;Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15645v1"&gt;Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/components/arena"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7ew9/rag_paper_26122/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7ew9/rag_paper_26122/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7ew9/rag_paper_26122/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T05:35:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmu1a1</id>
    <title>GLM-4.7-Flash context slowdown</title>
    <updated>2026-01-25T20:15:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt; &lt;img alt="GLM-4.7-Flash context slowdown" src="https://b.thumbs.redditmedia.com/1XaF-XYj4di3wcMyTbjLQ43nM77xN3jdiZdZndwBcDM.jpg" title="GLM-4.7-Flash context slowdown" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;UPDATE &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to check on your setup, run:&lt;br /&gt; (you can use higher -p and -n and modify -d to your needs)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;jacek@AI-SuperComputer:~$ CUDA_VISIBLE_DEVICES=0,1,2 llama-bench -m /mnt/models1/GLM/GLM-4.7-Flash-Q8_0.gguf -d 0,5000,10000,15000,20000,25000,30000,35000,40000,45000,50000 -p 200 -n 200 -fa 1 ggml_cuda_init: found 3 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 | 1985.41 ¬± 11.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 | 95.65 ¬± 0.44 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d5000 | 1392.15 ¬± 12.63 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d5000 | 81.83 ¬± 0.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d10000 | 1027.56 ¬± 13.50 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d10000 | 72.60 ¬± 0.07 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d15000 | 824.05 ¬± 8.08 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d15000 | 64.24 ¬± 0.46 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d20000 | 637.06 ¬± 79.79 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d20000 | 58.46 ¬± 0.14 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d25000 | 596.69 ¬± 11.13 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d25000 | 53.31 ¬± 0.18 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d30000 | 518.71 ¬± 5.25 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d30000 | 49.41 ¬± 0.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d35000 | 465.65 ¬± 2.69 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d35000 | 45.80 ¬± 0.04 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d40000 | 417.97 ¬± 1.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d40000 | 42.65 ¬± 0.05 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d45000 | 385.33 ¬± 1.80 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d45000 | 40.01 ¬± 0.03 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d50000 | 350.91 ¬± 2.17 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d50000 | 37.63 ¬± 0.02 | build: 8f91ca54e (7822) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;real usage of opencode (with 200000 context):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 0 | task 2495 | processing task, is_child = 0 slot update_slots: id 0 | task 2495 | new prompt, n_ctx_slot = 200192, n_keep = 0, task.n_tokens = 66276 slot update_slots: id 0 | task 2495 | n_tokens = 63140, memory_seq_rm [63140, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 65188, batch.n_tokens = 2048, progress = 0.983584 slot update_slots: id 0 | task 2495 | n_tokens = 65188, memory_seq_rm [65188, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 66276, batch.n_tokens = 1088, progress = 1.000000 slot update_slots: id 0 | task 2495 | prompt done, n_tokens = 66276, batch.n_tokens = 1088 slot init_sampler: id 0 | task 2495 | init sampler, took 8.09 ms, tokens: text = 66276, total = 66276 slot print_timing: id 0 | task 2495 | prompt eval time = 10238.44 ms / 3136 tokens ( 3.26 ms per token, 306.30 tokens per second) eval time = 11570.90 ms / 355 tokens ( 32.59 ms per token, 30.68 tokens per second) total time = 21809.34 ms / 3491 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;n_tokens = 66276, 306.30t/s, 30.68t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmu1a1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnbfdb</id>
    <title>Best model for 6 GB Vram 16 GM Ram?</title>
    <updated>2026-01-26T09:25:02+00:00</updated>
    <author>
      <name>/u/JS1DH</name>
      <uri>https://old.reddit.com/user/JS1DH</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Which would be the best model for research and coding. My specs are are follows&lt;/p&gt; &lt;p&gt;Nvidia 3060 6 GB&lt;/p&gt; &lt;p&gt;16 GB DDR5 Ram&lt;/p&gt; &lt;p&gt;Nvme SSD 1 TB&lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JS1DH"&gt; /u/JS1DH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnbfdb/best_model_for_6_gb_vram_16_gm_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnbfdb/best_model_for_6_gb_vram_16_gm_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnbfdb/best_model_for_6_gb_vram_16_gm_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T09:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmir5d</id>
    <title>What do you actually want from a private AI chat on your phone?</title>
    <updated>2026-01-25T13:12:00+00:00</updated>
    <author>
      <name>/u/AppDeveloperAsdf</name>
      <uri>https://old.reddit.com/user/AppDeveloperAsdf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt; &lt;img alt="What do you actually want from a private AI chat on your phone?" src="https://external-preview.redd.it/b2k1d3JkaHV0aGZnMbzKSbNJeiRdJL3Vv6uz8BgUY-ES1g_l6yTqUuzYy_d7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f45f222f3ee31c2716f286b9cf0998d79f80e6f" title="What do you actually want from a private AI chat on your phone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends. We are building zerotap - an Android app where AI can control your phone like a human (taps, scrolls, reads screen). It supports Ollama, proxies like OpenRouter and Straico and models directly such as OpenAI, Claude, Gemini and DeepSeek.&lt;/p&gt; &lt;p&gt;Recently we added a chat interface, so now it works like a regular AI chat that can take over your device when needed.&lt;/p&gt; &lt;p&gt;Now we are planning what to focus on next and we'd love your input. Some options we're considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; - connect your chat to external tools and services&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep research&lt;/strong&gt; - letting the AI browse and gather information for you&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modality&lt;/strong&gt; ‚Äî image read &amp;amp; write (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-device models&lt;/strong&gt; ‚Äî we are working on Gemma 3n and Qwen support, but small context windows are hurting performance so much&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speaking of which - for those of you running Ollama: do you expose your instance to the internet or keep it local network only?&lt;/p&gt; &lt;p&gt;Honest question: what would make an AI chat on your phone actually useful for you on a daily basis? Not as a toy, but as something you would rely on - what's missing from current mobile AI apps (that supports ollama) that annoys you the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppDeveloperAsdf"&gt; /u/AppDeveloperAsdf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/em3174huthfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn7zhi</id>
    <title>GLM 4.7: Why does explicit "--threads -1" ruin my t/s in llama-server?</title>
    <updated>2026-01-26T06:05:39+00:00</updated>
    <author>
      <name>/u/phwlarxoc</name>
      <uri>https://old.reddit.com/user/phwlarxoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using unsloth GLM-4.7 UD-Q8_K_XL quants on a dual RTX 5090 machine with 512 GB of system RAM and a 32 core Zen5 Threadripper Pro 9975WX. I run llama-server like so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1 llama.cpp/build/bin/llama-server --model ./GLM-4.7-UD-Q8_K_XL-00001-of-00009.gguf \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --jinja \ --ctx-size 40000 \ --temp 1.0 \ --top-p 0.95 \ --top-k 40 \ --min-p 0.0 \ --fit on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This yields about 9 t/s, wonderful! CPU load is constantly 51% and GPU load varies between 6 and 20%. However, if I add &amp;quot;--threads -1&amp;quot; with the idea to increase idling CPU core usage, the CPU is indeed used at nearly 100%, but t/s plummets to about 2.5 t/s. Why is that? There is definitely no thermal throttling involved, as on different setups all 64 threads run 100% indefinitely without any issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phwlarxoc"&gt; /u/phwlarxoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7zhi/glm_47_why_does_explicit_threads_1_ruin_my_ts_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7zhi/glm_47_why_does_explicit_threads_1_ruin_my_ts_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7zhi/glm_47_why_does_explicit_threads_1_ruin_my_ts_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T06:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn217z</id>
    <title>Practical use of local AI: Get a daily postcard with an anime girl inviting you to a local event based on your interests</title>
    <updated>2026-01-26T01:27:42+00:00</updated>
    <author>
      <name>/u/catplusplusok</name>
      <uri>https://old.reddit.com/user/catplusplusok</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/catplusplus/vibecheck/"&gt;https://github.com/catplusplus/vibecheck/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unique use case should run well on a good desktop or Apple laptop, cloud APIs would have real costs or at least discourage me from burning tokens with abandon for cosmetic improvements. Feel free to laugh at the anime girls, I am sure nobody else on this forum has similar AI use cases! The bottom line is that the app is for self improvement, encouraging me to get out of the house, go to events, learn new things and meet new people. &lt;/p&gt; &lt;p&gt;I have another even more compute intensive projects that involves mass describing my entire photo library, so local is not always just for the sake of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catplusplusok"&gt; /u/catplusplusok &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn217z/practical_use_of_local_ai_get_a_daily_postcard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn217z/practical_use_of_local_ai_get_a_daily_postcard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn217z/practical_use_of_local_ai_get_a_daily_postcard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T01:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnau4g</id>
    <title>A look inside the latest build - Nvidia GH200 desktop 144GB HBM3e, 624GB RAM, RTX Pro 6000, liquid-cooled.</title>
    <updated>2026-01-26T08:49:07+00:00</updated>
    <author>
      <name>/u/GPTshop---ai</name>
      <uri>https://old.reddit.com/user/GPTshop---ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnau4g/a_look_inside_the_latest_build_nvidia_gh200/"&gt; &lt;img alt="A look inside the latest build - Nvidia GH200 desktop 144GB HBM3e, 624GB RAM, RTX Pro 6000, liquid-cooled." src="https://external-preview.redd.it/ejltNzAzNXdwbmZnMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c76286512b166b9ef7c53a331c8fc9a21bf73e0" title="A look inside the latest build - Nvidia GH200 desktop 144GB HBM3e, 624GB RAM, RTX Pro 6000, liquid-cooled." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop---ai"&gt; /u/GPTshop---ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ocgeub4wpnfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnau4g/a_look_inside_the_latest_build_nvidia_gh200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnau4g/a_look_inside_the_latest_build_nvidia_gh200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T08:49:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjzx1</id>
    <title>KV cache fix for GLM 4.7 Flash</title>
    <updated>2026-01-25T14:06:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt; &lt;img alt="KV cache fix for GLM 4.7 Flash" src="https://external-preview.redd.it/Yd6yP0tYXhTq7c3g8_wDa0Z1Zijr0IAXDTPXGjQc7ts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=697845700fcf489c797d62fb0a23359703d41821" title="KV cache fix for GLM 4.7 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: remove Air from GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;KV cache uses a lot of VRAM. GLM 4.7 Flash doesn‚Äôt even use V in the KV cache. With long contexts, this means gigabytes of VRAM saved, so you can run much longer context on the same setup.&lt;/p&gt; &lt;p&gt;UPDATE &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnblrd</id>
    <title>Minimax-m2.1 looping and heavily hallucinating (only change was updating llama.cpp)</title>
    <updated>2026-01-26T09:36:02+00:00</updated>
    <author>
      <name>/u/relmny</name>
      <uri>https://old.reddit.com/user/relmny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using minimax-m2.1 now and then with good results but today, after updating llama.cpp, ud-q4-kxl started to loop heavily (never saw that before) and ud-q5-kxl answered a completely different question (not even &amp;quot;hallucinating&amp;quot;, as from start it gave an answer to an entire different question/prompt).&lt;/p&gt; &lt;p&gt;As the only thing I changed was updating llama.cpp (which I previously updated a week ago), I wonder if this happens to anyone else?&lt;/p&gt; &lt;p&gt;I've never, ever, seen that kind of &amp;quot;hallucination&amp;quot; before, in any model...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/relmny"&gt; /u/relmny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnblrd/minimaxm21_looping_and_heavily_hallucinating_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnblrd/minimaxm21_looping_and_heavily_hallucinating_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnblrd/minimaxm21_looping_and_heavily_hallucinating_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T09:36:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlpjp</id>
    <title>Internet blackout and Local LLMs</title>
    <updated>2026-01-25T15:15:05+00:00</updated>
    <author>
      <name>/u/DunderSunder</name>
      <uri>https://old.reddit.com/user/DunderSunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).&lt;/p&gt; &lt;p&gt;Meanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunderSunder"&gt; /u/DunderSunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn0dl8</id>
    <title>Backporting FP8 to the RTX 3090 (No H100 Required)</title>
    <updated>2026-01-26T00:16:50+00:00</updated>
    <author>
      <name>/u/one_does_not_just</name>
      <uri>https://old.reddit.com/user/one_does_not_just</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Worked on this project over the weekend, was curious if I can get fp8 compute going without decoding to fp16 in global memory or storing fp16 intermediates. Sacrificed some compute perf, but did achieve the intended VRAM savings. I did add a torch extension, if you wanna try it in your workflow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_does_not_just"&gt; /u/one_does_not_just &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://amohan.dev/blog/2026/fp8-as-storage-imma-ampere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dl8/backporting_fp8_to_the_rtx_3090_no_h100_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dl8/backporting_fp8_to_the_rtx_3090_no_h100_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T00:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn3evg</id>
    <title>~60GB models on coding: GLM 4.7 Flash vs. GPT OSS 120B vs. Qwen3 Coder 30B -- your comparisons?</title>
    <updated>2026-01-26T02:28:31+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All three of the models seem really strong. Qwen is the oldest, being from 2025 July, while we have about a week of experience with the GLM model now. They're all on the same class, taking ~60GB storage.&lt;/p&gt; &lt;p&gt;So just out of curiosity, what have your experiences been between the three models? What do you think the pros/cons are for each of the models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3evg/60gb_models_on_coding_glm_47_flash_vs_gpt_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3evg/60gb_models_on_coding_glm_47_flash_vs_gpt_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3evg/60gb_models_on_coding_glm_47_flash_vs_gpt_oss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T02:28:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn9jg3</id>
    <title>Reflow Studio v0.5: A fully local, portable Neural Dubbing Workstation (RVC + Wav2Lip + GFPGAN). No Python install required.</title>
    <updated>2026-01-26T07:32:05+00:00</updated>
    <author>
      <name>/u/MeanManagement834</name>
      <uri>https://old.reddit.com/user/MeanManagement834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jg3/reflow_studio_v05_a_fully_local_portable_neural/"&gt; &lt;img alt="Reflow Studio v0.5: A fully local, portable Neural Dubbing Workstation (RVC + Wav2Lip + GFPGAN). No Python install required." src="https://external-preview.redd.it/ajZpMWRzZGRjbmZnMd4x9B9VcVNKweplxa8BtHpj-my1OgtVfslok1CAkfL6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=918eeb6d7bdf677a3dd2d221ac8a00a14d7aa3b3" title="Reflow Studio v0.5: A fully local, portable Neural Dubbing Workstation (RVC + Wav2Lip + GFPGAN). No Python install required." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;I got tired of relying on cloud services or setting up complex Python environments just to run basic AI dubbing workflows. I wanted something that felt like a proper &amp;quot;app&amp;quot;‚Äîoffline, private, and cool to look at.&lt;/p&gt; &lt;h1&gt;The Solution: Reflow Studio v0.5&lt;/h1&gt; &lt;p&gt;I built a fully portable, local workstation that combines &lt;strong&gt;RVC&lt;/strong&gt; (Voice Cloning) and &lt;strong&gt;Wav2Lip&lt;/strong&gt; (Lip Sync) into a single Cyberpunk-themed interface.&lt;/p&gt; &lt;h2&gt;Features in v0.5:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ü§ñ Neural Voice Cloning:&lt;/strong&gt; Integrated RVC for instant, high-quality voice cloning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üëÑ Wav2Lip Sync:&lt;/strong&gt; Automatically matches the video mouth movements to the dubbed audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üëÅÔ∏è Face Enhancement:&lt;/strong&gt; Built-in GFPGAN to fix the blurry mouth issues common with Wav2Lip.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Vision Meter:&lt;/strong&gt; Real-time content filtering.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üöÄ Portable:&lt;/strong&gt; No Python/CUDA installation needed. Download the zip, extract, and run the &lt;code&gt;.bat&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Tech Stack&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; Gradio (Heavily customized CSS)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; PyTorch, FFmpeg&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; RVC v2, Wav2Lip-GAN, GFPGAN&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Try it out&lt;/h2&gt; &lt;p&gt;It's open source and available now. I'd love feedback on the UI and performance on different GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub &amp;amp; Download:&lt;/strong&gt; &lt;a href="https://github.com/ananta-sj/ReFlow-Studio"&gt;https://github.com/ananta-sj/ReFlow-Studio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MeanManagement834"&gt; /u/MeanManagement834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sesj5xfdcnfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jg3/reflow_studio_v05_a_fully_local_portable_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jg3/reflow_studio_v05_a_fully_local_portable_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T07:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnan7u</id>
    <title>How Did We Get Here? The largest companies are replacing their already cheap outsourced support staff with AI chatbots,</title>
    <updated>2026-01-26T08:37:40+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and they hallucinate back completely irrelevant responses. I had to choose the flair but this is not funny, especially given that a magic phrase &amp;quot;chat with human&amp;quot; does not work anymore.&lt;/p&gt; &lt;p&gt;Personal experience with Ebay: &amp;quot;I completely understand your frustration with $something&amp;quot; (the question was about a very different thing), &amp;quot;After &lt;strong&gt;thoroughly reviewing&lt;/strong&gt; the details of your transaction, &lt;strong&gt;I can confirm&lt;/strong&gt; that it occurred on Mar 2025&amp;quot; (the transaction was just 2 weeks ago in Jan 2026), and so on.&lt;/p&gt; &lt;p&gt;Personal experience with Payoneer: &amp;quot;Please reply with the reason why you want to block your card.&amp;quot; (the support request was about Payoneer website returning an error when withdrawing funds to a bank account), &amp;quot;Please provide A video or A screenshot of the page that leads to the error and a screenshot of the error itself&amp;quot; (detailed screenshots were already provided in the previous message), and so on.&lt;/p&gt; &lt;p&gt;which other companies have also fired their live human support staff? Share your horror stories.&lt;/p&gt; &lt;p&gt;Update: I forgot to mention that my quoted stories happened not in the live chats but over email communication which should have been answered by the live humans not chatbots.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnan7u/how_did_we_get_here_the_largest_companies_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnan7u/how_did_we_get_here_the_largest_companies_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnan7u/how_did_we_get_here_the_largest_companies_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T08:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qneoxf</id>
    <title>Running KimiK2 locally</title>
    <updated>2026-01-26T12:27:25+00:00</updated>
    <author>
      <name>/u/Temporary-Sector-947</name>
      <uri>https://old.reddit.com/user/Temporary-Sector-947</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qneoxf/running_kimik2_locally/"&gt; &lt;img alt="Running KimiK2 locally" src="https://b.thumbs.redditmedia.com/q8jqVYfm6HQkkhUQTOg4ktSbn6zS9DbrlDP6D-pIrrI.jpg" title="Running KimiK2 locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/c5o6r624sofg1.png?width=2293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15717e01766e67ace0a412bc6039fd731ce06929"&gt;https://preview.redd.it/c5o6r624sofg1.png?width=2293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15717e01766e67ace0a412bc6039fd731ce06929&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just build a local rig which could fit to Lancool 216&lt;br /&gt; - Epyc 9455p&lt;br /&gt; - Supermicro H13SSL-NT&lt;br /&gt; - 12 x 6400 DDR5 RDIMM 16 Gb&lt;br /&gt; - 6000 rtx pro maxq 96 Gb&lt;br /&gt; - 2x 4000 rtx pro 24 Gb&lt;br /&gt; - 2x4090 48Gb watercoolled (China mod)&lt;br /&gt; - 2x5090 32Gb watercooled&lt;br /&gt; - custom loop &lt;/p&gt; &lt;p&gt;VRAM - 305 Gb&lt;br /&gt; RAM - 188 Gb&lt;/p&gt; &lt;p&gt;Just testing and benching it now, for example, can run a Kimi K2 Q3 455Gb locally with 256k context.&lt;br /&gt; Will share some benches later today/&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Sector-947"&gt; /u/Temporary-Sector-947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qneoxf/running_kimik2_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qneoxf/running_kimik2_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qneoxf/running_kimik2_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T12:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn2n4p</id>
    <title>I reverse-engineered Microsoft AutoGen‚Äôs reasoning loop and cut agent latency by 85% (13.4s ‚Üí 1.6s). Here is the architecture.</title>
    <updated>2026-01-26T01:54:35+00:00</updated>
    <author>
      <name>/u/New_Care3681</name>
      <uri>https://old.reddit.com/user/New_Care3681</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been building voice agents using AutoGen, and the &amp;quot;awkward silence&amp;quot; during the Chain-of-Thought (CoT) phase was killing the UX. The standard sequential loop (Think ‚Üí Wait ‚Üí Execute Tool ‚Üí Wait ‚Üí Speak) just doesn't work for real-time interaction.&lt;/p&gt; &lt;p&gt;Instead of waiting for a v2 update, I dug into the ConversableAgent class and implemented a module for Speculative Reasoning Execution (SRE).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Idea:&lt;/strong&gt;&lt;br /&gt; Standard Speculative Decoding predicts tokens. I adapted this to predict Tool Calls.&lt;br /&gt; While the LLM is still generating its &amp;quot;Reasoning&amp;quot; text (e.g., &amp;quot;I need to search for weather...&amp;quot;), my module regex-sniffs the stream for intent. If it detects a high-confidence tool pattern, it executes the tool asynchronously in a background thread before the LLM finishes the sentence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Benchmarks (NVIDIA A100):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Baseline: 13.4s Time-to-Action (Sequential)&lt;/li&gt; &lt;li&gt;With SRE: 1.6s Time-to-Action (Parallel)&lt;/li&gt; &lt;li&gt;Reduction: ~85%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The PR is currently approved by the AutoGen core team:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fautogen%2Fpull%2F7179"&gt;https://github.com/microsoft/autogen/pull/7179&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I also built a distributed training rig for Whisper on Ray (SpeechLab):&lt;/strong&gt;&lt;br /&gt; To verify if my infra skills scaled, I built a fault-tolerant training engine for Whisper using Ray Train + PyTorch DDP. It handles streaming audio ingestion (so no OOM on Terabyte datasets) and hit 94% scaling efficiency on 4x A100s.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Demo (Vimeo): &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvimeo.com%2F1156797116"&gt;https://vimeo.com/1156797116&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repo: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FYash3561%2Fspeechlab"&gt;https://github.com/Yash3561/speechlab&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Looking for Feedback:&lt;/strong&gt;&lt;br /&gt; I built this to solve the &amp;quot;awkward silence&amp;quot; bottleneck in my own voice agents, but I'm curious how others are handling CoT latency in production.&lt;/p&gt; &lt;p&gt;If you are running agentic runtimes or distributed training platforms, I‚Äôd love to roast your architecture (or have you roast mine). Happy to answer questions about the regex sniffing logic or Ray actor pool management in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Care3681"&gt; /u/New_Care3681 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn2n4p/i_reverseengineered_microsoft_autogens_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn2n4p/i_reverseengineered_microsoft_autogens_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn2n4p/i_reverseengineered_microsoft_autogens_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T01:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmvny5</id>
    <title>GLM-4.7-Flash is even faster now</title>
    <updated>2026-01-25T21:14:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt; &lt;img alt="GLM-4.7-Flash is even faster now" src="https://external-preview.redd.it/y3hK5MFwhoK-QUOGog7BpAan8RKjGCnfL7Xowe9Lb4E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=880502683b1e21f9efe3ec41ebe19f6a59040622" title="GLM-4.7-Flash is even faster now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T21:14:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn3xig</id>
    <title>I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?</title>
    <updated>2026-01-26T02:51:42+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/"&gt; &lt;img alt="I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?" src="https://preview.redd.it/wky8vuufylfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce8114a0cbc29adcc5dff5d6dd9ef4259bf40636" title="I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;Noob here. I just won an Nvidia Hackathon and the prize was a Dell DGX Spark GB10.&lt;/p&gt; &lt;p&gt;I‚Äôve never fine tuned a model before and I was just using it for inferencing a nemotron 30B with vLLM that took 100+ GB of memory.&lt;/p&gt; &lt;p&gt;Anything you all would recommend me doing with it first?&lt;/p&gt; &lt;p&gt;NextJS was using around 60GB+ at one point so maybe I can run 2 nextJS apps at the same time potentially.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wky8vuufylfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T02:51:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
