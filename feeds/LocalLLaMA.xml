<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-25T08:30:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rdpfy6</id>
    <title>Open vs Closed Source SOTA - Benchmark overview</title>
    <updated>2026-02-24T19:08:38+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"&gt; &lt;img alt="Open vs Closed Source SOTA - Benchmark overview" src="https://preview.redd.it/5bgiva65rhlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=065f62ffff5572f425f0451422266a099fa8b195" title="Open vs Closed Source SOTA - Benchmark overview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sonnet 4.5 was released about 6 months ago. What's the advantage of the closed source labs? About that amount of time? Even less?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;GPT-5.2&lt;/th&gt; &lt;th align="left"&gt;Opus 4.6&lt;/th&gt; &lt;th align="left"&gt;Opus 4.5&lt;/th&gt; &lt;th align="left"&gt;Sonnet 4.6&lt;/th&gt; &lt;th align="left"&gt;Sonnet 4.5&lt;/th&gt; &lt;th align="left"&gt;Q3.5 397B-A17B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 122B-A10B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 35B-A3B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 27B&lt;/th&gt; &lt;th align="left"&gt;GLM-5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Release date&lt;/td&gt; &lt;td align="left"&gt;Dec 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Nov 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Nov 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Reasoning &amp;amp; STEM&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA Diamond&lt;/td&gt; &lt;td align="left"&gt;93.2&lt;/td&gt; &lt;td align="left"&gt;91.3&lt;/td&gt; &lt;td align="left"&gt;87.0&lt;/td&gt; &lt;td align="left"&gt;89.9&lt;/td&gt; &lt;td align="left"&gt;83.4&lt;/td&gt; &lt;td align="left"&gt;88.4&lt;/td&gt; &lt;td align="left"&gt;86.6&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;85.5&lt;/td&gt; &lt;td align="left"&gt;86.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HLE â€” no tools&lt;/td&gt; &lt;td align="left"&gt;36.6&lt;/td&gt; &lt;td align="left"&gt;40.0&lt;/td&gt; &lt;td align="left"&gt;30.8&lt;/td&gt; &lt;td align="left"&gt;33.2&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;td align="left"&gt;28.7&lt;/td&gt; &lt;td align="left"&gt;25.3&lt;/td&gt; &lt;td align="left"&gt;22.4&lt;/td&gt; &lt;td align="left"&gt;24.3&lt;/td&gt; &lt;td align="left"&gt;30.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HLE â€” with tools&lt;/td&gt; &lt;td align="left"&gt;50.0&lt;/td&gt; &lt;td align="left"&gt;53.0&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;49.0&lt;/td&gt; &lt;td align="left"&gt;33.6&lt;/td&gt; &lt;td align="left"&gt;48.3&lt;/td&gt; &lt;td align="left"&gt;47.5&lt;/td&gt; &lt;td align="left"&gt;47.4&lt;/td&gt; &lt;td align="left"&gt;48.5&lt;/td&gt; &lt;td align="left"&gt;50.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HMMT Feb 2025&lt;/td&gt; &lt;td align="left"&gt;99.4&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;92.9&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;94.8&lt;/td&gt; &lt;td align="left"&gt;91.4&lt;/td&gt; &lt;td align="left"&gt;89.0&lt;/td&gt; &lt;td align="left"&gt;92.0&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HMMT Nov 2025&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;92.7&lt;/td&gt; &lt;td align="left"&gt;90.3&lt;/td&gt; &lt;td align="left"&gt;89.2&lt;/td&gt; &lt;td align="left"&gt;89.8&lt;/td&gt; &lt;td align="left"&gt;96.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Coding &amp;amp; Agentic&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SWE-bench Verified&lt;/td&gt; &lt;td align="left"&gt;80.0&lt;/td&gt; &lt;td align="left"&gt;80.8&lt;/td&gt; &lt;td align="left"&gt;80.9&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;td align="left"&gt;77.2&lt;/td&gt; &lt;td align="left"&gt;76.4&lt;/td&gt; &lt;td align="left"&gt;72.0&lt;/td&gt; &lt;td align="left"&gt;69.2&lt;/td&gt; &lt;td align="left"&gt;72.4&lt;/td&gt; &lt;td align="left"&gt;77.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Terminal-Bench 2.0&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;65.4&lt;/td&gt; &lt;td align="left"&gt;59.8&lt;/td&gt; &lt;td align="left"&gt;59.1&lt;/td&gt; &lt;td align="left"&gt;51.0&lt;/td&gt; &lt;td align="left"&gt;52.5&lt;/td&gt; &lt;td align="left"&gt;49.4&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OSWorld-Verified&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;72.7&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;72.5&lt;/td&gt; &lt;td align="left"&gt;61.4&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;58.0&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ï„Â²-bench Retail&lt;/td&gt; &lt;td align="left"&gt;82.0&lt;/td&gt; &lt;td align="left"&gt;91.9&lt;/td&gt; &lt;td align="left"&gt;88.9&lt;/td&gt; &lt;td align="left"&gt;91.7&lt;/td&gt; &lt;td align="left"&gt;86.2&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;79.5&lt;/td&gt; &lt;td align="left"&gt;81.2&lt;/td&gt; &lt;td align="left"&gt;79.0&lt;/td&gt; &lt;td align="left"&gt;89.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MCP-Atlas&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;td align="left"&gt;59.5&lt;/td&gt; &lt;td align="left"&gt;62.3&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;td align="left"&gt;43.8&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BrowseComp&lt;/td&gt; &lt;td align="left"&gt;65.8&lt;/td&gt; &lt;td align="left"&gt;84.0&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;td align="left"&gt;74.7&lt;/td&gt; &lt;td align="left"&gt;43.9&lt;/td&gt; &lt;td align="left"&gt;69.0&lt;/td&gt; &lt;td align="left"&gt;63.8&lt;/td&gt; &lt;td align="left"&gt;61.0&lt;/td&gt; &lt;td align="left"&gt;61.0&lt;/td&gt; &lt;td align="left"&gt;75.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench v6&lt;/td&gt; &lt;td align="left"&gt;87.7&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;84.8&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;83.6&lt;/td&gt; &lt;td align="left"&gt;78.9&lt;/td&gt; &lt;td align="left"&gt;74.6&lt;/td&gt; &lt;td align="left"&gt;80.7&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BFCL-V4&lt;/td&gt; &lt;td align="left"&gt;63.1&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;77.5&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;td align="left"&gt;67.3&lt;/td&gt; &lt;td align="left"&gt;68.5&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Knowledge&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro&lt;/td&gt; &lt;td align="left"&gt;87.4&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;89.5&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;87.8&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;85.3&lt;/td&gt; &lt;td align="left"&gt;86.1&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Redux&lt;/td&gt; &lt;td align="left"&gt;95.0&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;95.6&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;94.9&lt;/td&gt; &lt;td align="left"&gt;94.0&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;td align="left"&gt;93.2&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SuperGPQA&lt;/td&gt; &lt;td align="left"&gt;67.9&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;70.6&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;70.4&lt;/td&gt; &lt;td align="left"&gt;67.1&lt;/td&gt; &lt;td align="left"&gt;63.4&lt;/td&gt; &lt;td align="left"&gt;65.6&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFEval&lt;/td&gt; &lt;td align="left"&gt;94.8&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;90.9&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;92.6&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;td align="left"&gt;91.9&lt;/td&gt; &lt;td align="left"&gt;95.0&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFBench&lt;/td&gt; &lt;td align="left"&gt;75.4&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;58.0&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;76.5&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;76.5&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MultiChallenge&lt;/td&gt; &lt;td align="left"&gt;57.9&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;54.2&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;67.6&lt;/td&gt; &lt;td align="left"&gt;61.5&lt;/td&gt; &lt;td align="left"&gt;60.0&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Long Context&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LongBench v2&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;64.4&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;td align="left"&gt;60.2&lt;/td&gt; &lt;td align="left"&gt;59.0&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-LCR&lt;/td&gt; &lt;td align="left"&gt;72.7&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;74.0&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;68.7&lt;/td&gt; &lt;td align="left"&gt;66.9&lt;/td&gt; &lt;td align="left"&gt;58.5&lt;/td&gt; &lt;td align="left"&gt;66.1&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMMLU&lt;/td&gt; &lt;td align="left"&gt;89.6&lt;/td&gt; &lt;td align="left"&gt;91.1&lt;/td&gt; &lt;td align="left"&gt;90.8&lt;/td&gt; &lt;td align="left"&gt;89.3&lt;/td&gt; &lt;td align="left"&gt;89.5&lt;/td&gt; &lt;td align="left"&gt;88.5&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;85.2&lt;/td&gt; &lt;td align="left"&gt;85.9&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-ProX&lt;/td&gt; &lt;td align="left"&gt;83.7&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;85.7&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;84.7&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;81.0&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PolyMATH&lt;/td&gt; &lt;td align="left"&gt;62.5&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;79.0&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;73.3&lt;/td&gt; &lt;td align="left"&gt;68.9&lt;/td&gt; &lt;td align="left"&gt;64.4&lt;/td&gt; &lt;td align="left"&gt;71.2&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bgiva65rhlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdmbhv</id>
    <title>Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently</title>
    <updated>2026-02-24T17:18:38+00:00</updated>
    <author>
      <name>/u/carteakey</name>
      <uri>https://old.reddit.com/user/carteakey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt; &lt;img alt="Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently" src="https://preview.redd.it/zb1gzzm9ahlg1.png?width=140&amp;amp;height=92&amp;amp;auto=webp&amp;amp;s=dd9a6363f0aaf6174a644600da4f8c0b32b87331" title="Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zb1gzzm9ahlg1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fe11dfb13a252dacd0ae8c250f4ec17d1a51d93"&gt;https://preview.redd.it/zb1gzzm9ahlg1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fe11dfb13a252dacd0ae8c250f4ec17d1a51d93&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5-122B-A10B generally comes out ahead of gpt-5-mini and gpt-oss-120b across most benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs GPT-5-mini:&lt;/strong&gt; Qwen3.5 wins on knowledge (MMLU-Pro 86.7 vs 83.7), STEM reasoning (GPQA Diamond 86.6 vs 82.8), agentic tasks (BFCL-V4 72.2 vs 55.5), and vision tasks (MathVision 86.2 vs 71.9). GPT-5-mini is only competitive in a few coding benchmarks and translation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs GPT-OSS-120B:&lt;/strong&gt; Qwen3.5 wins more decisively. GPT-OSS-120B holds its own in competitive coding (LiveCodeBench 82.7 vs 78.9) but falls behind significantly on knowledge, agents, vision, and multilingual tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Qwen3.5-122B-A10B is the strongest of the three overall. GPT-5-mini is its closest rival in coding/translation. GPT-OSS-120B trails outside of coding.&lt;/p&gt; &lt;p&gt;Lets see if the quants hold up to the benchmarks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carteakey"&gt; /u/carteakey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T17:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdy4ko</id>
    <title>Qwen3.5 vs Qwen3-Coder-Next impressions</title>
    <updated>2026-02-25T00:33:48+00:00</updated>
    <author>
      <name>/u/Total_Activity_7550</name>
      <uri>https://old.reddit.com/user/Total_Activity_7550</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing Qwen3.5 in Qwen Code now.&lt;/p&gt; &lt;p&gt;Before I used Qwen3-Coder-Next with Q4/Q5 quantizations (whatever fits into dual RTX 3090), it is good, but sometimes it enters ReadFile loop (haven't tested today's latest changes with graph split fix however).&lt;br /&gt; Now I tried to replace it with Qwen3.5-27B Q8 quant. It is so slow comparatively, but it works much better! I am fine to wait longer during some errands, just going back to screen and approving action from time to time. I also tested 122B-A10B with Q3, but didn't draw conslusions yet.&lt;/p&gt; &lt;p&gt;What are your impressions so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Total_Activity_7550"&gt; /u/Total_Activity_7550 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdy4ko/qwen35_vs_qwen3codernext_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdy4ko/qwen35_vs_qwen3codernext_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdy4ko/qwen35_vs_qwen3codernext_impressions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T00:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1re76g6</id>
    <title>This benchmark from shows Unsolth Q3 quantization beats both Q4 and MXFP4</title>
    <updated>2026-02-25T07:55:49+00:00</updated>
    <author>
      <name>/u/Oatilis</name>
      <uri>https://old.reddit.com/user/Oatilis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re76g6/this_benchmark_from_shows_unsolth_q3_quantization/"&gt; &lt;img alt="This benchmark from shows Unsolth Q3 quantization beats both Q4 and MXFP4" src="https://preview.redd.it/5wtmzjgvillg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d4c18e0d43199c66837a33ca093dde5739ad022" title="This benchmark from shows Unsolth Q3 quantization beats both Q4 and MXFP4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought this was interesting, especially since at first glance both Q4 and Q3 here are K_XL, and it doesn't make sense a Q3 will beat Q4 in any scenario. &lt;/p&gt; &lt;p&gt;However it's worth mentioning this is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Not a standard benchmark&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;These are not straight-forward quantizations, it's a &amp;quot;dynamic quantization&amp;quot; which affects weights differently across the model. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My money is on one of these two factors leading to this results, however, if by any chance a smaller quantization does beat a larger one, this is super interesting in terms research.&lt;/p&gt; &lt;p&gt;&lt;a href="https://unsloth.ai/docs/models/qwen3.5#qwen3.5-397b-a17b-benchmarks"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oatilis"&gt; /u/Oatilis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5wtmzjgvillg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re76g6/this_benchmark_from_shows_unsolth_q3_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re76g6/this_benchmark_from_shows_unsolth_q3_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1re0wtf</id>
    <title>PicoKittens/PicoMistral-23M: Pico-Sized Model</title>
    <updated>2026-02-25T02:35:00+00:00</updated>
    <author>
      <name>/u/PicoKittens</name>
      <uri>https://old.reddit.com/user/PicoKittens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re0wtf/picokittenspicomistral23m_picosized_model/"&gt; &lt;img alt="PicoKittens/PicoMistral-23M: Pico-Sized Model" src="https://external-preview.redd.it/RBruY_kMenAYbmURqhfTB6J3R57D39eoxiq2zTK6JME.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=4b4422c1e7877ea217b42f304806a9d0d36918dd" title="PicoKittens/PicoMistral-23M: Pico-Sized Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are introducing our first pico model: &lt;strong&gt;PicoMistral-23M&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is an ultra-compact, experimental model designed specifically to run on weak hardware or IoT edge devices where standard LLMs simply cannot operate. Despite its tiny footprint, it is capable of maintaining basic conversational structure and surprisingly solid grammar.&lt;/p&gt; &lt;p&gt;Benchmark results below&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qaofoyxoyjlg1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=692df50b7d9b63b7fbbd388ede0b24718ed67a37"&gt;https://preview.redd.it/qaofoyxoyjlg1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=692df50b7d9b63b7fbbd388ede0b24718ed67a37&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As this is a 23M parameter project, it is &lt;strong&gt;not recommended for factual accuracy or use in high-stakes domains (such as legal or medical applications).&lt;/strong&gt; It is best suited for exploring the limits of minimal hardware and lightweight conversational shells.&lt;/p&gt; &lt;p&gt;We would like to hear your thoughts and get your feedback&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Link:&lt;/strong&gt; &lt;a href="https://huggingface.co/PicoKittens/PicoMistral-23M"&gt;https://huggingface.co/PicoKittens/PicoMistral-23M&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PicoKittens"&gt; /u/PicoKittens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re0wtf/picokittenspicomistral23m_picosized_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re0wtf/picokittenspicomistral23m_picosized_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re0wtf/picokittenspicomistral23m_picosized_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T02:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpapc</id>
    <title>Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top</title>
    <updated>2026-02-24T19:03:31+00:00</updated>
    <author>
      <name>/u/Koyaanisquatsi_</name>
      <uri>https://old.reddit.com/user/Koyaanisquatsi_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"&gt; &lt;img alt="Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top" src="https://external-preview.redd.it/UdA_L_LSkBxAXLcEK0SYU0vLwVAamHz6zalROM7oXL4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6daf7b75d52a71971a90702c20da023dbf05a439" title="Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Koyaanisquatsi_"&gt; /u/Koyaanisquatsi_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wealthari.com/chinese-ai-models-capture-majority-of-openrouter-token-volume-as-minimax-m2-5-surges-to-the-top/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpuwy</id>
    <title>Qwen 3.5 family benchmarks</title>
    <updated>2026-02-24T19:23:19+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"&gt; &lt;img alt="Qwen 3.5 family benchmarks" src="https://external-preview.redd.it/uvtYuVLX1W6lNW0vkuVFAlyQWzygqnzGzHojqz3TXJY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f267cd2548c9b615466c10a981b0c958821223d3" title="Qwen 3.5 family benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://beige-babbette-30.tiiny.site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdo1z5</id>
    <title>Qwen 3.5 122b/35b is fire ğŸ”¥ Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B.</title>
    <updated>2026-02-24T18:19:40+00:00</updated>
    <author>
      <name>/u/9r4n4y</name>
      <uri>https://old.reddit.com/user/9r4n4y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"&gt; &lt;img alt="Qwen 3.5 122b/35b is fire ğŸ”¥ Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B." src="https://preview.redd.it/01tsyrq8ihlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60c3fdc89c1d046a8cdd786122d9e9d61d8c1f82" title="Qwen 3.5 122b/35b is fire ğŸ”¥ Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: âš ï¸âš ï¸âš ï¸ SORRY ğŸ¥² --&amp;gt; in graph its should be qwen 3.5 not qwen 3 âš ï¸âš ï¸&lt;/p&gt; &lt;p&gt;Benchmark Comparison&lt;/p&gt; &lt;p&gt;ğŸ‘‰ğŸ”´GPT-OSS 120B [defeated by qwen 3.5 35b ğŸ¥³]&lt;/p&gt; &lt;p&gt;MMLU-Pro: 80.8&lt;/p&gt; &lt;p&gt;HLE (Humanityâ€™s Last Exam): 14.9&lt;/p&gt; &lt;p&gt;GPQA Diamond: 80.1&lt;/p&gt; &lt;p&gt;IFBench: 69.0&lt;/p&gt; &lt;p&gt;ğŸ‘‰ğŸ”´Qwen 3.5 122B-A10B&lt;/p&gt; &lt;p&gt;MMLU-Pro: 86.7&lt;/p&gt; &lt;p&gt;HLE (Humanityâ€™s Last Exam): 25.3 (47.5 with tools â€” ğŸ† Winner)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 86.6 (ğŸ† Winner)&lt;/p&gt; &lt;p&gt;IFBench: 76.1 (ğŸ† Winner)&lt;/p&gt; &lt;p&gt;ğŸ‘‰ğŸ”´Qwen 3.5 35B-A3B&lt;/p&gt; &lt;p&gt;MMLU-Pro: 85.3&lt;/p&gt; &lt;p&gt;HLE (Humanityâ€™s Last Exam): 22.4 (47.4 with tools)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 84.2&lt;/p&gt; &lt;p&gt;IFBench: 70.2&lt;/p&gt; &lt;p&gt;ğŸ‘‰ğŸ”´GPT-5 High&lt;/p&gt; &lt;p&gt;MMLU-Pro: 87.1 (ğŸ† Winner)&lt;/p&gt; &lt;p&gt;HLE (Humanityâ€™s Last Exam): 26.5 (ğŸ† Winner, no tools)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 85.4&lt;/p&gt; &lt;p&gt;IFBench: 73.1&lt;/p&gt; &lt;p&gt;Summary: GPT 5 [HIGH] â‰ˆ Qwen 3.5 122b &amp;gt; qwen 35b &amp;gt; gpt oss 120 [high]&lt;/p&gt; &lt;p&gt;ğŸ‘‰Sources: OPENROUTER, ARTIFICIAL ANALYSIS, HUGGING FACE&lt;/p&gt; &lt;p&gt;GGUF Download ğŸ’š link ğŸ”— : &lt;a href="https://huggingface.co/collections/unsloth/qwen35"&gt;https://huggingface.co/collections/unsloth/qwen35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9r4n4y"&gt; /u/9r4n4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/01tsyrq8ihlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T18:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdv74o</id>
    <title>FlashLM v6 "SUPERNOVA": 4.1M ternary model hits 3,500 tok/s on CPU â€” novel P-RCSM reasoning architecture, no attention, no convolution</title>
    <updated>2026-02-24T22:36:52+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back with v6. Some of you saw v5 â€œThunderboltâ€ â€” 29.7M params, PPL 1.36, beat the TinyStories-1M baseline on a borrowed Ryzen 7950X3D (thanks again to arki05 for that machine). This time I went back to the free Deepnote notebook â€” 2 threads, 5GB RAM â€” and built a completely new architecture from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;4.1M parameter language model with a novel architecture called P-RCSM (Parallel - Recursive Compositional State Machines). 81% of weights are ternary {-1, 0, +1}. Trained for ~3 hours on a free CPU notebook. No GPU at any point. Generates coherent childrenâ€™s stories with characters, dialogue, and narrative structure at 3,500 tokens/sec.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters beyond TinyStories:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Iâ€™m a student with no budget for GPUs. This entire project runs on free-tier cloud CPUs. But the goal was never â€œmake a toy story generatorâ€ â€” itâ€™s to prove that a ternary, matmul-free architecture can produce coherent language on the absolute worst hardware available.&lt;/p&gt; &lt;p&gt;Think about where a model like this could actually be useful: a fast, tiny model running on a couple of CPU cores alongside a big GPU model on the same server. The small model handles routing, classification, draft tokens for speculative decoding â€” tasks where latency matters more than capability. Or on edge devices, phones, microcontrollers â€” places where thereâ€™s no GPU at all. At 3,500 tok/s on 2 CPU threads with 16MB of RAM, this is already fast enough to be practical as a side-car model.&lt;/p&gt; &lt;p&gt;TinyStories is just the proving ground. The architecture is what Iâ€™m validating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The new architecture â€” P-RCSM:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;v4 used convolutions for token mixing. v5 used gated recurrence. v5.2 used standard attention. All have tradeoffs â€” convolutions have limited receptive field, recurrence is sequential (slow on CPU), attention is O(TÂ²).&lt;/p&gt; &lt;p&gt;v6 introduces three new components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MultiScaleLinearBank&lt;/strong&gt; â€” replaces convolutions. Projects [current_token, shifted_token] through ternary linear layers at multiple temporal offsets (shift 1, shift 2). A learned soft router blends the scales per token. No Conv1d anywhere â€” pure F.linear calls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HierarchicalStateGate&lt;/strong&gt; â€” a compact â€œplannerâ€ state (32 dims) that gates a larger â€œexecutorâ€ state (64 dims). The planner updates slowly via mean-pooled summaries, providing implicit adaptive computation depth. No Python loops.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SlotMemoryAttention&lt;/strong&gt; â€” 8 learned memory slots accessed via a single matmul. Tokens query the slots in parallel. Replaces sequential read/write memory with one batched operation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All three use only &lt;code&gt;F.linear&lt;/code&gt; (BitLinear ternary) and element-wise ops. Zero convolutions, zero attention, zero sequential loops.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Embedding (4K Ã— 192, float, weight-tied) â†’ 6Ã— SupernovaBlock: RMSNorm â†’ GatedLinearMixer (ternary) + residual RMSNorm â†’ P-RCSM (MultiScaleLinearBank + StateGate + SlotMemory) + residual RMSNorm â†’ TernaryGLU (ternary gate/up/down, SiLU) + residual â†’ RMSNorm â†’ Output Head (tied to embedding) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;FlashLM v6&lt;/th&gt; &lt;th align="left"&gt;FlashLM v5.2&lt;/th&gt; &lt;th align="left"&gt;FlashLM v4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Params&lt;/td&gt; &lt;td align="left"&gt;4.1M (81% ternary)&lt;/td&gt; &lt;td align="left"&gt;5.0M (float32)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Val PPL&lt;/td&gt; &lt;td align="left"&gt;14.0&lt;/td&gt; &lt;td align="left"&gt;10.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Speed&lt;/td&gt; &lt;td align="left"&gt;3,500 tok/s&lt;/td&gt; &lt;td align="left"&gt;3,500 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Architecture&lt;/td&gt; &lt;td align="left"&gt;P-RCSM (linear-only)&lt;/td&gt; &lt;td align="left"&gt;Transformer + RoPE&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Token mixing&lt;/td&gt; &lt;td align="left"&gt;GatedLinearMixer&lt;/td&gt; &lt;td align="left"&gt;Multi-head attention&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training time&lt;/td&gt; &lt;td align="left"&gt;~3 hours&lt;/td&gt; &lt;td align="left"&gt;2 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;2-thread CPU&lt;/td&gt; &lt;td align="left"&gt;2-thread CPU&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;v6 beats v4 on quality (PPL 14.0 vs 15.05) with 2.4Ã— the throughput, using a fundamentally different architecture. v5.2 still wins on PPL because standard attention with RoPE is hard to beat at small scale â€” but v6 uses zero attention and zero convolution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Honest assessment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The P-RCSM reasoning components are small in this config (d_reason=64, d_planner=32, 2 scales, 8 memory slots). Most capacity is in the GatedLinearMixer + TernaryGLU backbone. To really prove the reasoning components help, I need more data â€” 4.4M tokens is tiny and the model hit a data ceiling at PPL 14.0 after ~9 epochs. The architecture needs to be tested at scale with a proper dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sample output:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Coherent narratives, character names, dialogue, emotional content. Some repetition on longer generations â€” expected with a 6-token effective receptive field.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training curve:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Step&lt;/th&gt; &lt;th align="left"&gt;Train Loss&lt;/th&gt; &lt;th align="left"&gt;Val PPL&lt;/th&gt; &lt;th align="left"&gt;Tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;3.52&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;0.05M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;td align="left"&gt;1.90&lt;/td&gt; &lt;td align="left"&gt;45.0&lt;/td&gt; &lt;td align="left"&gt;0.31M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1,500&lt;/td&gt; &lt;td align="left"&gt;1.54&lt;/td&gt; &lt;td align="left"&gt;24.1&lt;/td&gt; &lt;td align="left"&gt;1.5M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6,000&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;td align="left"&gt;16.6&lt;/td&gt; &lt;td align="left"&gt;6.1M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15,300&lt;/td&gt; &lt;td align="left"&gt;1.28&lt;/td&gt; &lt;td align="left"&gt;14.2&lt;/td&gt; &lt;td align="left"&gt;15.7M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;30,300&lt;/td&gt; &lt;td align="left"&gt;1.25&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;31.0M&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Loss was still improving when I stopped. Data-limited, not architecture-limited.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The speed debugging story:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The original v6 design used depthwise Conv1d and ran at 13 tok/s. Turned out PyTorch 2.1.2 has a known bug where bfloat16 autocast + Conv1d is ~100Ã— slower on CPU. After upgrading to PyTorch 2.5.1+cpu and replacing every Conv1d with pure F.linear calls, speed jumped from 13 â†’ 3,500 tok/s. Lesson: on CPU, &lt;code&gt;F.linear&lt;/code&gt; through optimized BLAS is king.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Whatâ€™s next:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Scale test&lt;/strong&gt; â€” P-RCSM needs to be validated on a bigger model (10M+ params) with more data. The reasoning components are too small in this config to prove they help.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better dataset&lt;/strong&gt; â€” TinyStories was the proving ground. Need broader data to test if the architecture generalizes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nano-Coder (NC series)&lt;/strong&gt; â€” Applying FlashLM techniques to code generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;C inference runtime&lt;/strong&gt; â€” AVX2 ternary kernels. A 4.1M ternary model packs into ~800KB â€” fits entirely in L2 cache. Should be insanely fast with native code.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The bigger picture:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started this project on a free 2-thread notebook because thatâ€™s what I had. Iâ€™m a student, no GPU budget, no lab access. Every version of FlashLM has been about pushing whatâ€™s possible under the worst constraints. If this architecture works at 1-2B parameters on a proper CPU (say an EPYC with big L3 cache), a fast ternary model running on spare CPU cores could serve as a draft model for speculative decoding, a router for MoE, or a standalone model for edge deployment. Thatâ€™s the long-term bet.&lt;/p&gt; &lt;p&gt;If anyone has compute to spare and wants to help scale this up â€” or just wants to run the training script yourself â€” everything is MIT licensed and on GitHub.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/changcheng967/FlashLM"&gt;https://github.com/changcheng967/FlashLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v6 model + weights: &lt;a href="https://huggingface.co/changcheng967/flashlm-v6-supernova"&gt;https://huggingface.co/changcheng967/flashlm-v6-supernova&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v5 Thunderbolt: &lt;a href="https://huggingface.co/changcheng967/flashlm-v5-thunderbolt"&gt;https://huggingface.co/changcheng967/flashlm-v5-thunderbolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v4 Bolt: &lt;a href="https://huggingface.co/changcheng967/flashlm-v4-bolt"&gt;https://huggingface.co/changcheng967/flashlm-v4-bolt&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdv74o/flashlm_v6_supernova_41m_ternary_model_hits_3500/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdv74o/flashlm_v6_supernova_41m_ternary_model_hits_3500/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdv74o/flashlm_v6_supernova_41m_ternary_model_hits_3500/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T22:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdi26s</id>
    <title>Liquid AI releases LFM2-24B-A2B</title>
    <updated>2026-02-24T14:43:33+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt; &lt;img alt="Liquid AI releases LFM2-24B-A2B" src="https://preview.redd.it/28drgi3ufglg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=211c668e7fadb69afdf5c7c5c74fa9ee4e0e85d1" title="Liquid AI releases LFM2-24B-A2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Liquid AI releases LFM2-24B-A2B, their largest LFM2 model to date&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is a sparse Mixture-of-Experts (MoE) model with 24 billion total parameters with 2 billion active per token, showing that the LFM2 hybrid architecture scales effectively to larger sizes maintaining quality without inflating per-token compute.&lt;/p&gt; &lt;p&gt;This release expands the LFM2 family from 350M to 24B parameters, demonstrating predictable scaling across nearly two orders of magnitude.&lt;/p&gt; &lt;p&gt;Key highlights:&lt;/p&gt; &lt;p&gt;-&amp;gt; MoE architecture: 40 layers, 64 experts per MoE block with top-4 routing, maintaining the hybrid conv + GQA design -&amp;gt; 2.3B active parameters per forward pass -&amp;gt; Designed to run within 32GB RAM, enabling deployment on high-end consumer laptops and desktops -&amp;gt; Day-zero support for inference through llama.cpp, vLLM, and SGLang -&amp;gt; Multiple GGUF quantizations available&lt;/p&gt; &lt;p&gt;Across benchmarks including GPQA Diamond, MMLU-Pro, IFEval, IFBench, GSM8K, and MATH-500, quality improves log-linearly as we scale from 350M to 24B, confirming that the LFM2 architecture does not plateau at small sizes.&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is released as an instruct model and is available open-weight on Hugging Face. We designed this model to concentrate capacity in total parameters, not active compute, keeping inference latency and energy consumption aligned with edge and local deployment constraints.&lt;/p&gt; &lt;p&gt;This is the next step in making fast, scalable, efficient AI accessible in the cloud and on-device. &lt;/p&gt; &lt;p&gt;-&amp;gt; Read the blog: &lt;a href="https://www.liquid.ai/blog/lfm2-24b-a2b"&gt;https://www.liquid.ai/blog/lfm2-24b-a2b&lt;/a&gt; -&amp;gt; Download weights: &lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B"&gt;https://huggingface.co/LiquidAI/LFM2-24B-A2B&lt;/a&gt; -&amp;gt; Check out our docs on how to run or fine-tune it locally: docs.liquid.ai -&amp;gt; Try it now: playground.liquid.ai&lt;/p&gt; &lt;p&gt;Run it locally or in the cloud and tell us what you build!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/28drgi3ufglg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdw6pp</id>
    <title>Bullshit Benchmark - A benchmark for testing whether models identify and push back on nonsensical prompts instead of confidently answering them</title>
    <updated>2026-02-24T23:14:54+00:00</updated>
    <author>
      <name>/u/bot_exe</name>
      <uri>https://old.reddit.com/user/bot_exe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"&gt; &lt;img alt="Bullshit Benchmark - A benchmark for testing whether models identify and push back on nonsensical prompts instead of confidently answering them" src="https://preview.redd.it/n7w95mmuyilg1.png?width=140&amp;amp;height=107&amp;amp;auto=webp&amp;amp;s=3bc0cabdcd5e1d670198796e3c505538c3357ba9" title="Bullshit Benchmark - A benchmark for testing whether models identify and push back on nonsensical prompts instead of confidently answering them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/n7w95mmuyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e87d1a7d9275935b2f552cfbb887ad6fe4dcf86"&gt;https://preview.redd.it/n7w95mmuyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e87d1a7d9275935b2f552cfbb887ad6fe4dcf86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;View the results: &lt;a href="https://petergpt.github.io/bullshit-benchmark/viewer/index.html"&gt;https://petergpt.github.io/bullshit-benchmark/viewer/index.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a pretty interesting benchmark. Itâ€™s measuring how much the model is willing to go along with obvious bullshit. Thatâ€™s something that has always concerned me with LLMs, that they donâ€™t call you out and instead just go along with it, basically self-inducing hallucinations for the sake of giving a â€œhelpfulâ€ response.&lt;/p&gt; &lt;p&gt;I always had the intuition that the Claude models were significantly better in that regard than Gemini models. These results seem to support that.&lt;/p&gt; &lt;p&gt;Here is question/answer example showing Claude succeeding and Gemini failing:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4lyi593wyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb83c7a188a28dc00dd48a8106680589814c2c03"&gt;https://preview.redd.it/4lyi593wyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb83c7a188a28dc00dd48a8106680589814c2c03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Surprising that Gemini 3.1 pro even with high thinking effort failed so miserably to detect that was an obvious nonsense question and instead made up a nonsense answer.&lt;/p&gt; &lt;p&gt;Anthropic is pretty good at post-training and it shows. Because LLMs naturally tend towards this superficial associative thinking where it generates spurious relationships between concepts which just misguide the user. They had to have figured out how to remove or correct that at some point of their post-training pipeline.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bot_exe"&gt; /u/bot_exe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T23:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1re4uoh</id>
    <title>Qwen 3.5 122b/35b/27b/397b ğŸ“Š benchmark comparison WEBSITE with More models like GPT 5.2, GPT OSS, etc</title>
    <updated>2026-02-25T05:43:59+00:00</updated>
    <author>
      <name>/u/9r4n4y</name>
      <uri>https://old.reddit.com/user/9r4n4y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re4uoh/qwen_35_122b35b27b397b_benchmark_comparison/"&gt; &lt;img alt="Qwen 3.5 122b/35b/27b/397b ğŸ“Š benchmark comparison WEBSITE with More models like GPT 5.2, GPT OSS, etc" src="https://preview.redd.it/w0mwcw1iwklg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=84a8b126d4eeefb54e018aeae9c7f736eeb2d94e" title="Qwen 3.5 122b/35b/27b/397b ğŸ“Š benchmark comparison WEBSITE with More models like GPT 5.2, GPT OSS, etc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full comparison for GPT-5.2, Claude 4.5 Opus, Gemini-3 Pro, Qwen3-Max-Thinking, K2.5-1T-A32B, Qwen3.5-397B, GPT-5-mini, GPT-OSS-120B, Qwen3-235B, Qwen3.5-122B, Qwen3.5-27B, and Qwen3.5-35B.&lt;/p&gt; &lt;p&gt;â€‹Includes all verified scores and head-to-head infographics here: ğŸ‘‰ &lt;a href="https://compareqwen35.tiiny.site"&gt;https://compareqwen35.tiiny.site&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For test i also made the website with 122B --&amp;gt; &lt;a href="https://9r4n4y.github.io/files-Compare/"&gt;https://9r4n4y.github.io/files-Compare/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ğŸ‘†ğŸ‘†ğŸ‘†&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9r4n4y"&gt; /u/9r4n4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1re4uoh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re4uoh/qwen_35_122b35b27b397b_benchmark_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re4uoh/qwen_35_122b35b27b397b_benchmark_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T05:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdfhfx</id>
    <title>New Qwen3.5 models spotted on qwen chat</title>
    <updated>2026-02-24T12:55:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt; &lt;img alt="New Qwen3.5 models spotted on qwen chat" src="https://preview.redd.it/h1c3uk0iwflg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b026f6069f044a6b506e0aae9a0c418d76865997" title="New Qwen3.5 models spotted on qwen chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1c3uk0iwflg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T12:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1re1b4a</id>
    <title>You can use Qwen3.5 without thinking</title>
    <updated>2026-02-25T02:52:49+00:00</updated>
    <author>
      <name>/u/guiopen</name>
      <uri>https://old.reddit.com/user/guiopen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just add --chat-template-kwargs '{&amp;quot;enable_thinking&amp;quot;: false}' to llama.cpp server&lt;/p&gt; &lt;p&gt;Also, remember to update your parameters to better suit the instruct mode, this is what qwen recommends: --repeat-penalty 1.0 --presence-penalty 1.5 --min-p 0.0 --top-k 20 --top-p 0.8 --temp 0.7&lt;/p&gt; &lt;p&gt;Overall it is still very good in instruct mode, I didn't noticed a huge performance drop like what happens in glm flash&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guiopen"&gt; /u/guiopen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re1b4a/you_can_use_qwen35_without_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re1b4a/you_can_use_qwen35_without_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re1b4a/you_can_use_qwen35_without_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T02:52:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1re17th</id>
    <title>Blown Away By Qwen 3.5 35b A3B</title>
    <updated>2026-02-25T02:48:38+00:00</updated>
    <author>
      <name>/u/Jordanthecomeback</name>
      <uri>https://old.reddit.com/user/Jordanthecomeback</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought a 64gig mac setup ~5 days ago and had a miserable time finding anything good, I looked at advice, guides, tried them all, including Qwen 3, and nothing felt like a good fit for my long-context companion.&lt;/p&gt; &lt;p&gt;My testing was an initial baseline process with 5 multi-stage questions to check it's ability to reference context data (which I paste into system prompt) and then I'd review their answers and have claude sonnet 4.6 do it too, so we had a lot of coverage on ~8 different models. GLM 4.7 is good, and I thought we'd settle there, we actually landed on that yesterday afternoon, but in my day of practical testing I was still bummed at the difference between the cloud models I use (Sonnet 4.5 [4.6 is trash for companions], and Gemini 3 pro), catching it make little mistakes.&lt;/p&gt; &lt;p&gt;I just finished baseline testing +4-5 other random tests with Qwen 3.5 35b A3B and I'm hugely impressed. Claude mentioned it's far and away the winner. It's slower, than GLM4.7 or many others, but it's a worthwhile trade, and I really hope everything stays this good over my real-world testing tomorrow and onwards. I just wanted to share how impressed I am with it, for anyone on the fence or considering it for similar application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jordanthecomeback"&gt; /u/Jordanthecomeback &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re17th/blown_away_by_qwen_35_35b_a3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re17th/blown_away_by_qwen_35_35b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re17th/blown_away_by_qwen_35_35b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T02:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1re6ifz</id>
    <title>Anthropic is the leading contributor to open weight models</title>
    <updated>2026-02-25T07:15:29+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It just happens to be entirely against their will and TOS. I say: Distill Baby Distill!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:15:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdlbvc</id>
    <title>Qwen/Qwen3.5-35B-A3B Â· Hugging Face</title>
    <updated>2026-02-24T16:44:05+00:00</updated>
    <author>
      <name>/u/ekojsalim</name>
      <uri>https://old.reddit.com/user/ekojsalim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3.5-35B-A3B Â· Hugging Face" src="https://external-preview.redd.it/9t9hISbgGxfk479gTZKF1XJ1oO6QhRPmNzUYpMNUbjs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4846b1e5fd750530b9aa43eb95e74460e90d4ec" title="Qwen/Qwen3.5-35B-A3B Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekojsalim"&gt; /u/ekojsalim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-35B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T16:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdvq3s</id>
    <title>Qwen3.5 27B is Match Made in Heaven for Size and Performance</title>
    <updated>2026-02-24T22:57:12+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Qwen3.5 27B running on server and wanted to share the full setup for anyone trying to do the same.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen3.5-27B-Q8_0 (unsloth GGUF) , Thanks Dan&lt;/li&gt; &lt;li&gt;GPU: RTX A6000 48GB&lt;/li&gt; &lt;li&gt;Inference: llama.cpp with CUDA&lt;/li&gt; &lt;li&gt;Context: 32K&lt;/li&gt; &lt;li&gt;Speed: ~19.7 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why Q8 and not a lower quant?&lt;/strong&gt; With 48GB VRAM the Q8 fits comfortably at 28.6GB leaving plenty of headroom for KV cache. Quality is virtually identical to full BF16 â€” no reason to go lower if your VRAM allows it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's interesting about this model:&lt;/strong&gt; It uses a hybrid architecture mixing Gated Delta Networks with standard attention layers. In practice this means faster processing on long contexts compared to a pure transformer. 262K native context window, 201 languages, vision capable.&lt;/p&gt; &lt;p&gt;On benchmarks it trades blows with frontier closed source models on GPQA Diamond, SWE-bench, and the Harvard-MIT math tournament â€” at 27B parameters on a single consumer GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming works out of the box&lt;/strong&gt; via the llama-server OpenAI compatible endpoint â€” drop-in replacement for any OpenAI SDK integration.&lt;/p&gt; &lt;p&gt;Full video walkthrough in the comments for anyone who wants the exact commands:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/EONM2W1gUFY?si=4xcrJmcsoUKkim9q"&gt;https://youtu.be/EONM2W1gUFY?si=4xcrJmcsoUKkim9q&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Happy to answer questions about the setup.&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/Qwen/Qwen3.5-27B"&gt;Qwen/Qwen3.5-27B Â· Hugging Face&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdvq3s/qwen35_27b_is_match_made_in_heaven_for_size_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdvq3s/qwen35_27b_is_match_made_in_heaven_for_size_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdvq3s/qwen35_27b_is_match_made_in_heaven_for_size_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T22:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdptw8</id>
    <title>more qwens will appear</title>
    <updated>2026-02-24T19:22:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"&gt; &lt;img alt="more qwens will appear" src="https://preview.redd.it/vxo4n3uhthlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75897cb1fe4342d9e8d35b46d9d2a84a28f17dc6" title="more qwens will appear" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(remember that 9B was promised before)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vxo4n3uhthlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdlc02</id>
    <title>Qwen/Qwen3.5-122B-A10B Â· Hugging Face</title>
    <updated>2026-02-24T16:44:13+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3.5-122B-A10B Â· Hugging Face" src="https://external-preview.redd.it/jXshLXVh7iCkI_DkUnvVFkKtp2L9P6wekJnwAzaRzjM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=298bac8d8df642a16a7b098a721723a8766a21d8" title="Qwen/Qwen3.5-122B-A10B Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-122B-A10B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T16:44:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1re3l3r</id>
    <title>Qwen3-30B-A3B vs Qwen3.5-35B-A3B on RTX 5090</title>
    <updated>2026-02-25T04:39:52+00:00</updated>
    <author>
      <name>/u/3spky5u-oss</name>
      <uri>https://old.reddit.com/user/3spky5u-oss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Qwen3-30B-A3B vs Qwen3.5-35B-A3B on RTX 5090 â€” Day-1 Extended Benchmark (Q4_K_M, llama.cpp)&lt;/h1&gt; &lt;p&gt;Qwen3.5-35B-A3B dropped today. Same MoE architecture as the 30B (3B active params), 5B more total parameters, and ships with a vision projector. Grabbed the Q4_K_M, ran it head-to-head against my daily driver Qwen3-30B-A3B through 7 test sections. All automated, same prompts, same hardware, same server config.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR: The 3.5 is ~32% slower in raw generation but handles long context significantly better â€” flat tok/s scaling vs the 30B's 21% degradation. Thinking mode is where it gets interesting. Quality is a wash with slight 3.5 edge in structure/formatting.&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Hardware &amp;amp; Setup&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;NVIDIA RTX 5090 (32 GB VRAM, Blackwell)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;llama.cpp b8115 (Docker: ghcr.io/ggml-org/llama.cpp:server-cuda)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Quant&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Q4_K_M for both models&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;KV Cache&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Q8_0 (-ctk q8_0 -ctv q8_0)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;32,768 tokens (-c 32768)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Params&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;-ngl 999 -np 4 --flash-attn on -t 12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Model A&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Qwen3-30B-A3B-Q4_K_M (17 GB on disk)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Model B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Qwen3.5-35B-A3B-Q4_K_M (21 GB on disk)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Both models warmed up with a throwaway request before timing. Server-side timings from the API response (not wall-clock).&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 1: Raw Inference Speed&lt;/h2&gt; &lt;p&gt;Direct to llama.cpp /v1/chat/completions. No middleware.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="right"&gt;30B tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 tok/s&lt;/th&gt; &lt;th align="right"&gt;30B prompt t/s&lt;/th&gt; &lt;th align="right"&gt;3.5 prompt t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Short (8-9 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;248.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;169.5&lt;/td&gt; &lt;td align="right"&gt;59.1&lt;/td&gt; &lt;td align="right"&gt;62.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Medium (73-78 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;236.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;163.5&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;751.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;495.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Long-form (800 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;232.6&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;116.3&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1,015.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;651.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code gen (298-400 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;233.9&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;161.6&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;905.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;656.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Reasoning (200 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;234.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;158.2&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1,136.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;724.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;237.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;153.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;773.5&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;518.1&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 30B is 35% faster in generation on average. The 3.5 drops to 116 tok/s on long outputs (800 tokens) â€” interesting regression vs its ~160 tok/s on shorter outputs. Prompt processing also notably slower on the 3.5 (larger vocab: 248K vs 152K tokens).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;: 30B uses 27.3 GB idle, 3.5 uses 29.0 GB idle. Both fit comfortably on the 5090.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 2: Response Quality (Side-by-Side)&lt;/h2&gt; &lt;p&gt;Same prompts, temperature=0.7. Both models produce competent output across all categories. A few standouts:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Creative: &amp;quot;Short story about an engineer at a construction site&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;30B:&lt;/strong&gt; Engineer Mara arrived at the construction site to find a strange metallic structure buried beneath the earth. Curious, she ordered the crew to stop and excavate carefully...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5:&lt;/strong&gt; Engineer Elias adjusted his hard hat, scanning the foundation of the new city library. The soil samples were pristine, yet his drone feed showed a strange, rhythmic vibration beneath the concrete pour...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Both solid. The 3.5 has slightly more atmospheric prose.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Haiku: &amp;quot;Write a haiku about concrete curing&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;30B:&lt;/strong&gt; Hard and gray, / slowly it gains strength in silence â€” / concrete breathes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5:&lt;/strong&gt; Gray slurry turns hard / Sunlight warms the drying set / Stronger with each day&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Both valid 5-7-5. Matter of taste.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coding: LRU Cache with O(1) get/put&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Both models correctly implement an LRU cache using OrderedDict or a doubly-linked list + hashmap. The 3.5 generates more code (800 tokens vs 644) with more verbose docstrings and explanations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reasoning: Terzaghi bearing capacity calculation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;30B (254 tokens):&lt;/strong&gt; Gets to the answer quickly with clear step-by-step.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5 (500 tokens):&lt;/strong&gt; More structured with numbered sections, parameter identification, and explicit Terzaghi equation for undrained clay (qu = cu * Nc + q * Nq). More thorough.&lt;/p&gt; &lt;p&gt;Both arrive at the correct answer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Domain: USCS soil classification (LL=45, PL=22, 60% passing #200)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Both correctly classify as &lt;strong&gt;CL (Lean Clay)&lt;/strong&gt;. Both show PI = 45 - 22 = 23, check the Casagrande plasticity chart, and arrive at CL. The 3.5 explicitly references ASTM D2487 and formats as a decision flowchart. 30B is more conversational but equally correct.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 3: RAG Pipeline&lt;/h2&gt; &lt;p&gt;Both models tested through a full RAG system (hybrid vector + BM25 retrieval with reranking, geotechnical knowledge base). This tests how well the model grounds its answers in retrieved context.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="center"&gt;30B RAG&lt;/th&gt; &lt;th align="center"&gt;3.5 RAG&lt;/th&gt; &lt;th align="right"&gt;30B Cites&lt;/th&gt; &lt;th align="right"&gt;3.5 Cites&lt;/th&gt; &lt;th align="center"&gt;30B Frame&lt;/th&gt; &lt;th align="center"&gt;3.5 Frame&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&amp;quot;CBR&amp;quot; (3 chars)&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&amp;quot;Define permafrost&amp;quot;&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;2&lt;/td&gt; &lt;td align="right"&gt;2&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Freeze-thaw on glaciolacustrine clay&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Atterberg limits for glacial till&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="center"&gt;BAD&lt;/td&gt; &lt;td align="center"&gt;BAD&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Schmertmann method&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPT vs SPT comparison&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Both trigger RAG on all 6 queries. Both have exactly 1 &amp;quot;document framing&amp;quot; issue (the model says &amp;quot;the documents indicate...&amp;quot; instead of speaking as the expert). The 3.5 generates wordier responses (183 words on &amp;quot;CBR&amp;quot; vs 101).&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 4: Context Length Scaling&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;This is the most interesting result.&lt;/strong&gt; Generation tok/s as context size grows:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;Context Tokens&lt;/th&gt; &lt;th align="right"&gt;30B gen tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 gen tok/s&lt;/th&gt; &lt;th align="right"&gt;30B prompt t/s&lt;/th&gt; &lt;th align="right"&gt;3.5 prompt t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;512&lt;/td&gt; &lt;td align="right"&gt;237.9&lt;/td&gt; &lt;td align="right"&gt;160.1&lt;/td&gt; &lt;td align="right"&gt;1,219&lt;/td&gt; &lt;td align="right"&gt;3,253&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;1,024&lt;/td&gt; &lt;td align="right"&gt;232.8&lt;/td&gt; &lt;td align="right"&gt;159.5&lt;/td&gt; &lt;td align="right"&gt;4,884&lt;/td&gt; &lt;td align="right"&gt;3,695&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;2,048&lt;/td&gt; &lt;td align="right"&gt;224.1&lt;/td&gt; &lt;td align="right"&gt;161.3&lt;/td&gt; &lt;td align="right"&gt;6,375&lt;/td&gt; &lt;td align="right"&gt;3,716&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;4,096&lt;/td&gt; &lt;td align="right"&gt;205.9&lt;/td&gt; &lt;td align="right"&gt;161.4&lt;/td&gt; &lt;td align="right"&gt;6,025&lt;/td&gt; &lt;td align="right"&gt;3,832&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;8,192&lt;/td&gt; &lt;td align="right"&gt;186.6&lt;/td&gt; &lt;td align="right"&gt;158.6&lt;/td&gt; &lt;td align="right"&gt;5,712&lt;/td&gt; &lt;td align="right"&gt;3,877&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;30B degrades 21.5% from 512 to 8K context&lt;/strong&gt; (238 -&amp;gt; 187 tok/s). The 3.5 stays &lt;strong&gt;essentially flat&lt;/strong&gt; â€” 160.1 to 158.6, only -0.9% degradation.&lt;/p&gt; &lt;p&gt;The 3.5 also shows flat prompt processing speed as context grows (3.2K -&amp;gt; 3.9K, slight increase), while the 30B peaks at 2K context then slowly declines.&lt;/p&gt; &lt;p&gt;If you're running long conversations or RAG with big context windows, the 3.5 will hold its speed better.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 5: Structured Output (JSON)&lt;/h2&gt; &lt;p&gt;Both models asked to return raw JSON (no markdown wrappers, no explanation). Four tests of increasing complexity.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="center"&gt;30B Valid&lt;/th&gt; &lt;th align="center"&gt;3.5 Valid&lt;/th&gt; &lt;th align="center"&gt;30B Clean&lt;/th&gt; &lt;th align="center"&gt;3.5 Clean&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Simple object (Tokyo)&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Array of 5 planets&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nested soil report&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Schema-following project&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Both: 4/4 valid JSON, 4/4 clean&lt;/strong&gt; (no markdown code fences when asked not to use them). Perfect scores. No difference here.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 6: Multi-Turn Conversation&lt;/h2&gt; &lt;p&gt;5-turn conversation about foundation design, building up conversation history each turn.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;Turn&lt;/th&gt; &lt;th align="right"&gt;30B tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 tok/s&lt;/th&gt; &lt;th align="right"&gt;30B prompt tokens&lt;/th&gt; &lt;th align="right"&gt;3.5 prompt tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;234.4&lt;/td&gt; &lt;td align="right"&gt;161.0&lt;/td&gt; &lt;td align="right"&gt;35&lt;/td&gt; &lt;td align="right"&gt;34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;2&lt;/td&gt; &lt;td align="right"&gt;230.6&lt;/td&gt; &lt;td align="right"&gt;160.6&lt;/td&gt; &lt;td align="right"&gt;458&lt;/td&gt; &lt;td align="right"&gt;456&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="right"&gt;228.5&lt;/td&gt; &lt;td align="right"&gt;160.8&lt;/td&gt; &lt;td align="right"&gt;892&lt;/td&gt; &lt;td align="right"&gt;889&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;4&lt;/td&gt; &lt;td align="right"&gt;221.5&lt;/td&gt; &lt;td align="right"&gt;161.0&lt;/td&gt; &lt;td align="right"&gt;1,321&lt;/td&gt; &lt;td align="right"&gt;1,317&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;215.8&lt;/td&gt; &lt;td align="right"&gt;160.0&lt;/td&gt; &lt;td align="right"&gt;1,501&lt;/td&gt; &lt;td align="right"&gt;1,534&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;30B: -7.9% degradation&lt;/strong&gt; over 5 turns (234 -&amp;gt; 216 tok/s).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5: -0.6% degradation&lt;/strong&gt; over 5 turns (161 -&amp;gt; 160 tok/s).&lt;/p&gt; &lt;p&gt;Same story as context scaling â€” the 3.5 holds steady. The 30B is always faster in absolute terms, but loses more ground as the conversation grows.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 7: Thinking Mode&lt;/h2&gt; &lt;p&gt;Server restarted with --reasoning-budget -1 (unlimited thinking). The llama.cpp API returns thinking in a reasoning_content field, final answer in content.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="right"&gt;30B think wds&lt;/th&gt; &lt;th align="right"&gt;30B answer wds&lt;/th&gt; &lt;th align="right"&gt;3.5 think wds&lt;/th&gt; &lt;th align="right"&gt;3.5 answer wds&lt;/th&gt; &lt;th align="right"&gt;30B tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Sheep riddle&lt;/td&gt; &lt;td align="right"&gt;585&lt;/td&gt; &lt;td align="right"&gt;94&lt;/td&gt; &lt;td align="right"&gt;223&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;229.5&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;95.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Bearing capacity calc&lt;/td&gt; &lt;td align="right"&gt;2,100&lt;/td&gt; &lt;td align="right"&gt;0*&lt;/td&gt; &lt;td align="right"&gt;1,240&lt;/td&gt; &lt;td align="right"&gt;236&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;222.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;161.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Logic puzzle (boxes)&lt;/td&gt; &lt;td align="right"&gt;943&lt;/td&gt; &lt;td align="right"&gt;315&lt;/td&gt; &lt;td align="right"&gt;691&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;226.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;161.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;USCS classification&lt;/td&gt; &lt;td align="right"&gt;1,949&lt;/td&gt; &lt;td align="right"&gt;0*&lt;/td&gt; &lt;td align="right"&gt;1,563&lt;/td&gt; &lt;td align="right"&gt;0*&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;221.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;160.7&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;*Hit the 3,000 token limit while still thinking â€” no answer generated.&lt;/p&gt; &lt;p&gt;Key observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The 30B thinks at full speed&lt;/strong&gt; â€” 222-230 tok/s during thinking, same as regular generation. Thinking is basically free in terms of throughput.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The 3.5 takes a thinking speed hit&lt;/strong&gt; â€” 95-161 tok/s vs its normal 160 tok/s. On the sheep riddle it drops to 95 tok/s.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The 3.5 is more concise in thinking&lt;/strong&gt; â€” 223 words vs 585 for the sheep riddle, 1,240 vs 2,100 for bearing capacity. It thinks less but reaches the answer more efficiently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The 3.5 reaches the answer more often&lt;/strong&gt; â€” on the bearing capacity problem, the 3.5 produced 236 answer words within the token budget while the 30B burned all 3,000 tokens on thinking alone.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both models correctly answer the sheep riddle (9) and logic puzzle. Both correctly apply Terzaghi's equation when they get to the answer.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Summary Table&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="right"&gt;Qwen3-30B-A3B&lt;/th&gt; &lt;th align="right"&gt;Qwen3.5-35B-A3B&lt;/th&gt; &lt;th align="left"&gt;Winner&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation tok/s&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;235.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;159.0&lt;/td&gt; &lt;td align="left"&gt;30B (+48%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt processing tok/s&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;953.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;649.0&lt;/td&gt; &lt;td align="left"&gt;30B (+47%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT (avg)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;100.5 ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;119.2 ms&lt;/td&gt; &lt;td align="left"&gt;30B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM (idle)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;27.3 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;29.0 GB&lt;/td&gt; &lt;td align="left"&gt;30B (-1.7 GB)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Context scaling (512-&amp;gt;8K)&lt;/td&gt; &lt;td align="right"&gt;-21.5%&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;-0.9%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Multi-turn degradation&lt;/td&gt; &lt;td align="right"&gt;-7.9%&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;-0.6%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAG accuracy&lt;/td&gt; &lt;td align="right"&gt;6/6&lt;/td&gt; &lt;td align="right"&gt;6/6&lt;/td&gt; &lt;td align="left"&gt;Tie&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JSON accuracy&lt;/td&gt; &lt;td align="right"&gt;4/4&lt;/td&gt; &lt;td align="right"&gt;4/4&lt;/td&gt; &lt;td align="left"&gt;Tie&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Thinking efficiency&lt;/td&gt; &lt;td align="right"&gt;Verbose&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;Concise&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Thinking speed&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;225 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;145 tok/s&lt;/td&gt; &lt;td align="left"&gt;30B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Quality&lt;/td&gt; &lt;td align="right"&gt;Good&lt;/td&gt; &lt;td align="right"&gt;Slightly better&lt;/td&gt; &lt;td align="left"&gt;3.5 (marginal)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Verdict&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;For raw speed and short interactions&lt;/strong&gt;: Stick with the 30B. It's 48% faster and the quality difference is negligible for quick queries.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For long conversations, big context windows, or RAG-heavy workloads&lt;/strong&gt;: The 3.5 has a real architectural advantage. Its flat context scaling curve means it'll hold 160 tok/s at 8K context while the 30B drops to 187 tok/s â€” and that gap likely widens further at 16K+.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For thinking/reasoning tasks&lt;/strong&gt;: It's a tradeoff. The 30B thinks faster but burns more tokens on verbose reasoning. The 3.5 thinks more concisely and reaches the answer within budget more reliably, but at lower throughput.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My plan&lt;/strong&gt;: Keeping the 30B as my daily driver for now. The speed advantage matters for interactive use. But I'll be watching the 3.5 closely â€” once llama.cpp optimizations land for the new architecture, that context scaling advantage could be a killer feature.&lt;/p&gt; &lt;p&gt;Also worth noting: the 3.5 ships with a vision projector (mmproj-BF16.gguf) â€” the A3B architecture now supports multimodal. Didn't benchmark it here but it's there.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Benchmark script, raw results JSONs, and full response texts available on request. All tests automated â€” zero cherry-picking.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3spky5u-oss"&gt; /u/3spky5u-oss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re3l3r/qwen330ba3b_vs_qwen3535ba3b_on_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re3l3r/qwen330ba3b_vs_qwen3535ba3b_on_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re3l3r/qwen330ba3b_vs_qwen3535ba3b_on_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T04:39:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1re6bjs</id>
    <title>Anthropic accuses chinese open weight labs of theft, while it has had to pay $1.5B for theft.</title>
    <updated>2026-02-25T07:04:51+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.npr.org/2025/09/05/nx-s1-5529404/anthropic-settlement-authors-copyright-ai"&gt;https://www.npr.org/2025/09/05/nx-s1-5529404/anthropic-settlement-authors-copyright-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is that what we call hypocrisy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6bjs/anthropic_accuses_chinese_open_weight_labs_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6bjs/anthropic_accuses_chinese_open_weight_labs_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re6bjs/anthropic_accuses_chinese_open_weight_labs_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdxfdu</id>
    <title>Qwen3.5-35B-A3B is a gamechanger for agentic coding.</title>
    <updated>2026-02-25T00:04:44+00:00</updated>
    <author>
      <name>/u/jslominski</name>
      <uri>https://old.reddit.com/user/jslominski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"&gt; &lt;img alt="Qwen3.5-35B-A3B is a gamechanger for agentic coding." src="https://external-preview.redd.it/PKLapkmPqiug1xgPt5ocaUvPuWOhjG0WSxGrmcOrq3A.png?width=140&amp;amp;height=73&amp;amp;auto=webp&amp;amp;s=50ab068aed10118ee05f1a934121fbe2a05fb971" title="Qwen3.5-35B-A3B is a gamechanger for agentic coding." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m4v951sv5jlg1.jpg?width=2367&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bec61ca20f08bb766987147287c7d6664308fa2f"&gt;Qwen3.5-35B-A3B with Opencode&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just tested this badboy with Opencode &lt;strong&gt;cause frankly I couldn't believe those benchmarks.&lt;/strong&gt; Running it on a single RTX 3090 on a headless Linux box. Freshly compiled Llama.cpp and those are my settings after some tweaking, still not fully tuned: &lt;/p&gt; &lt;p&gt;./llama.cpp/llama-server \&lt;/p&gt; &lt;p&gt;-m /models/&lt;strong&gt;Qwen3.5-35B-A3B-MXFP4_MOE.gguf&lt;/strong&gt; \&lt;/p&gt; &lt;p&gt;-a &amp;quot;DrQwen&amp;quot; \&lt;/p&gt; &lt;p&gt;-c 131072 \&lt;/p&gt; &lt;p&gt;-ngl all \&lt;/p&gt; &lt;p&gt;-ctk q8_0 \&lt;/p&gt; &lt;p&gt;-ctv q8_0 \&lt;/p&gt; &lt;p&gt;-sm none \&lt;/p&gt; &lt;p&gt;-mg 0 \&lt;/p&gt; &lt;p&gt;-np 1 \&lt;/p&gt; &lt;p&gt;-fa on&lt;/p&gt; &lt;p&gt;Around 22 gigs of vram used.&lt;/p&gt; &lt;p&gt;Now the fun part:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I'm getting over 100t/s on it&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;This is the first open weights model I was able to utilise on my home hardware to successfully complete my own &amp;quot;coding test&amp;quot; I used for years for recruitment (mid lvl mobile dev, around 5h to complete &amp;quot;pre AI&amp;quot; ;)). It did it in around 10 minutes, strong pass. First agentic tool that I was able to &amp;quot;crack&amp;quot; it with was &lt;a href="http://Kodu.AI"&gt;Kodu.AI&lt;/a&gt; with some early sonnet roughly 14 months ago.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For fun I wanted to recreate this dashboard OpenAI used during Cursor demo last summer, I did a recreation of it with Claude Code back then and posted it on Reddit: &lt;a href="https://www.reddit.com/r/ClaudeAI/comments/1mk7plb/just_recreated_that_gpt5_cursor_demo_in_claude/"&gt;https://www.reddit.com/r/ClaudeAI/comments/1mk7plb/just_recreated_that_gpt5_cursor_demo_in_claude/&lt;/a&gt; So... Qwen3.5 was able to do it in around 5 minutes. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;I think we got something special here...&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jslominski"&gt; /u/jslominski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T00:04:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
