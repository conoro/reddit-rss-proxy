<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-30T09:27:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pyntxz</id>
    <title>BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language</title>
    <updated>2025-12-29T14:36:30+00:00</updated>
    <author>
      <name>/u/AgencyInside407</name>
      <uri>https://old.reddit.com/user/AgencyInside407</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"&gt; &lt;img alt="BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language" src="https://external-preview.redd.it/dWN4dmJ1ZmttNWFnMY4GET1d7wPCVUTfL2kwUQvVU9zlAAxjJ2PRs52epB4h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=604938b875bebc02ca972dc626bb9367cd1bc819" title="BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everybody! I hope all is well. I just wanted to share a project that I have been working on for the last several months called BULaMU-Dream. It is the first text to image model in the world that has been trained from scratch to respond to prompts in an African Language (Luganda). The details of how I trained it are &lt;a href="https://zenodo.org/records/18086776"&gt;here&lt;/a&gt; and a demo can be found &lt;a href="https://x.com/mwebazarick/status/2005643851655168146?s=12"&gt;here&lt;/a&gt;. I am open to any feedback that you are willing to share because I am going to continue working on improving BULaMU-Dream. I really believe that tiny conditional diffusion models like this can broaden access to multimodal AI tools by allowing people train and use these models on relatively inexpensive setups, like the M4 Mac Mini. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencyInside407"&gt; /u/AgencyInside407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ty3cvnfkm5ag1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T14:36:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzb6x7</id>
    <title>A zero-setup agent that benchmarks multiple open / closed source LLMs on your specific problem / data</title>
    <updated>2025-12-30T06:52:25+00:00</updated>
    <author>
      <name>/u/Ok-Introduction354</name>
      <uri>https://old.reddit.com/user/Ok-Introduction354</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzb6x7/a_zerosetup_agent_that_benchmarks_multiple_open/"&gt; &lt;img alt="A zero-setup agent that benchmarks multiple open / closed source LLMs on your specific problem / data" src="https://b.thumbs.redditmedia.com/-d529AFaDwUkF2gVCjyZeusNJobqnxjv-hwejr7X9lE.jpg" title="A zero-setup agent that benchmarks multiple open / closed source LLMs on your specific problem / data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Comparing different open and closed source LLMs, and analyzing their pros and cons on your own specific problem or dataset is a common task while building agents or LLM workflows.&lt;/p&gt; &lt;p&gt;We built an agent that makes it simple to do this. Just load or connect your dataset, explain the problem and ask our agent to prompt different LLMs.&lt;/p&gt; &lt;p&gt;Here's an example of doing this on the TweetEval tweet emoji prediction task (predict the right emoji given a tweet):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ask the agent to curate an eval set from your data, and write a script to run inference on a model of your choice.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/47okjm08gaag1.png?width=3430&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=777da114202ea0c534226cc2a9088c824723fb1e"&gt;Dataset curation and model inference script (the agent calls OpenRouter in this example)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a-zero-setup-agent-that-benchmarks-multiple-llms-on-your-v0-ww5t5mnd0aag1.png?width=3430&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1970020ca8b34e7dd993e486c5a06fcbd57a911"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The agent kicks off a background job and reports key metrics.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ljnlsk8cgaag1.png?width=3428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c360c783c8d3601eeec7c0235262d3e3d85dbd0"&gt;Background job execution of the inference script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a-zero-setup-agent-that-benchmarks-multiple-llms-on-your-v0-l8oa3eqk0aag1.png?width=3428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=340e4e18123265b0ff0eee3ba6eb8222dddc55e9"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You can ask the agent to analyze the predictions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dg3wuftdgaag1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bad6f03fc89299fa28f2ebc6f82e2c80ec8bedd0"&gt;Agent puts the true and predicted emojis in a table&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a-zero-setup-agent-that-benchmarks-multiple-llms-on-your-v0-vx2jcawh1aag1.png?width=3428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf7325a564b392006c1def5fe2cedd06ba18ccb1"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Next, ask the agent to benchmark 5 additional open + closed source models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z2haiccfgaag1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7f1759b925350df2626d50e84dd1c9b41ad0f12"&gt;Agent uses Search to compute the cost of benchmarking additional models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a-zero-setup-agent-that-benchmarks-multiple-llms-on-your-v0-j0hef1x61aag1.png?width=3430&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77cf286cf91217607e7576c4ac036e1fc35d3965"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;After the new inference background job finishes, you can ask the agent to plot the metrics for all the benchmarked agents.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zmn0j5phgaag1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bc370b84f6dae33a677fa733ad62da116690ee4"&gt;Relative performance of different models on this task&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a-zero-setup-agent-that-benchmarks-multiple-llms-on-your-v0-oxo8l7no1aag1.png?width=3424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df3bccaba81548e2bf97c23b4375c55c707b4643"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this particular task, surprisingly, Llama-3-70b performs the best, even better than closed source models like GPT-4o and Claude-3.5!&lt;/p&gt; &lt;p&gt;You can check out this workflow at &lt;a href="https://nexttoken.co/app/share/9c8ad40c-0a35-4c45-95c3-31eb73cf7879"&gt;https://nexttoken.co/app/share/9c8ad40c-0a35-4c45-95c3-31eb73cf7879&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Introduction354"&gt; /u/Ok-Introduction354 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzb6x7/a_zerosetup_agent_that_benchmarks_multiple_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzb6x7/a_zerosetup_agent_that_benchmarks_multiple_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzb6x7/a_zerosetup_agent_that_benchmarks_multiple_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T06:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz6vju</id>
    <title>An open source implementation of that refusal steering paper</title>
    <updated>2025-12-30T03:12:53+00:00</updated>
    <author>
      <name>/u/Remarkable_Threes</name>
      <uri>https://old.reddit.com/user/Remarkable_Threes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone - I just released the code for the refusal steering paper that uses LLM-Refusal-Evaluation. TLDR: Surgical refusal removal with statistical validation instead of vibes-based steering. Main features:&lt;/p&gt; &lt;p&gt;Judge scores validate your training data&lt;/p&gt; &lt;p&gt;Correlation analysis picks best layers automatically&lt;/p&gt; &lt;p&gt;Confidence-weighted steering vectors (WRMD from the paper)&lt;/p&gt; &lt;p&gt;Auto alpha optimization with early stopping&lt;/p&gt; &lt;p&gt;Can merge permanently into weights&lt;/p&gt; &lt;p&gt;It's more setup than simpler steering repos (multi-stage pipeline, needs the eval framework), but you get actual statistical validation at each step instead of guessing.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ElSnacko/llm-steering"&gt;https://github.com/ElSnacko/llm-steering&lt;/a&gt; Paper: &lt;a href="https://arxiv.org/abs/2512.16602"&gt;https://arxiv.org/abs/2512.16602&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from anyone who tries it! Especially curious how it stacks up against abliteration in practice.I will be testing and benchmarking this implementation and so likely more posts to come. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Threes"&gt; /u/Remarkable_Threes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6vju/an_open_source_implementation_of_that_refusal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6vju/an_open_source_implementation_of_that_refusal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6vju/an_open_source_implementation_of_that_refusal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz9x3u</id>
    <title>One answer to "what do you use local LLMs for?": a hyper-personalized multimodal event crawler</title>
    <updated>2025-12-30T05:42:32+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see the &amp;quot;what do you use local LLMs for?&amp;quot; question come up every month, so here's one example: a multimodal agent that crawls local websites to find events happening around me.&lt;/p&gt; &lt;h1&gt;Why local instead of API?&lt;/h1&gt; &lt;p&gt;People ask me this a lot. Cloud providers are cheap, until you're generating millions of tokens. I'm crawling dozens of event sources, processing images, deduplicating across sites. That adds up fast.&lt;/p&gt; &lt;p&gt;Local is also faster for my use case. Claude and GPT grind to a halt during peak loads. &lt;a href="https://www.ovidiudan.com/2025/12/25/dual-rtx-pro-6000-llm-guide.html"&gt;My home server&lt;/a&gt; gives me consistent throughput whenever I need it.&lt;/p&gt; &lt;h1&gt;The setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Dual RTX Pro 6000 (96GB VRAM each)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V"&gt;GLM-4.6V&lt;/a&gt; (106B parameter multimodal model) running on vLLM&lt;/li&gt; &lt;li&gt;The crawler, backend, and mobile app were all vibe coded with Claude Opus&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What GLM-4.6V actually does&lt;/h1&gt; &lt;p&gt;The crawler uses the model for five tasks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Extracting info from event flyers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where multimodal models shine. &lt;a href="https://whidbeycamanoislands.com/event/the-dead-guise-new-years-eve/"&gt;Here's an event&lt;/a&gt; where the text description doesn't mention the price, but the flyer image does. The LLM reads the flyer and extracts &amp;quot;$25&amp;quot; into a structured field.&lt;/p&gt; &lt;p&gt;OCR can read text from an image, but it can't understand that &amp;quot;$25&amp;quot; on a psychedelic Grateful Dead flyer is the ticket price and not a date or an address. That requires a model that actually understands what it's looking at.&lt;/p&gt; &lt;p&gt;The model also extracts venue names, performer lineups, age restrictions, and registration requirements from a combination of the raw HTML and the accompanying image.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Rewriting messy descriptions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Scraped event descriptions are a mess: HTML artifacts, escaped characters, inconsistent formatting. The LLM rewrites these into clean paragraphs while preserving the essential info.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Link classification&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rather than fragile regex to find ticket links, the LLM analyzes all links on a page and identifies the primary registration URL (not the &amp;quot;Buy Tickets&amp;quot; link for a different event in the sidebar).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Cross-source deduplication&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The same event appears on multiple websites. The LLM compares new events against existing ones and determines if it's a duplicate. It understands that &amp;quot;NYE Party at The Clyde&amp;quot; and &amp;quot;New Year's Eve Celebration - Clyde Theatre&amp;quot; are the same event.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Multi-event extraction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some sources publish newsletter images containing multiple events. The LLM extracts each event separately from a single composite image.&lt;/p&gt; &lt;h1&gt;The point&lt;/h1&gt; &lt;p&gt;A few years ago, some of this would have been practically impossible. Not just expensive or slow, but actually impossible. Multimodal understanding of unstructured visual data wasn't something you could just spin up.&lt;/p&gt; &lt;p&gt;Now I can throw together a custom tool over a weekend that does exactly what I need. Tools built for an audience of one, running on hardware I control.&lt;/p&gt; &lt;p&gt;Full writeup with more details on the Firebase backend and Flutter app: &lt;a href="https://www.ovidiudan.com/2025/12/30/age-customized-software.html"&gt;The age of hyper-personalized software&lt;/a&gt; (I am not selling or promoting anything, I do this for fun.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T05:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz3643</id>
    <title>So any rumours about llama?</title>
    <updated>2025-12-30T00:28:24+00:00</updated>
    <author>
      <name>/u/AdventurousFly4909</name>
      <uri>https://old.reddit.com/user/AdventurousFly4909</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While others have been cooking, the llama team had been radio silent. Has any interesting news about llama surfaced?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousFly4909"&gt; /u/AdventurousFly4909 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz3643/so_any_rumours_about_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz3643/so_any_rumours_about_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz3643/so_any_rumours_about_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T00:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzbw4k</id>
    <title>I built a fully offline text to speech Mac app because cloud TTS annoyed me</title>
    <updated>2025-12-30T07:33:06+00:00</updated>
    <author>
      <name>/u/tarunyadav9761</name>
      <uri>https://old.reddit.com/user/tarunyadav9761</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzbw4k/i_built_a_fully_offline_text_to_speech_mac_app/"&gt; &lt;img alt="I built a fully offline text to speech Mac app because cloud TTS annoyed me" src="https://external-preview.redd.it/ODd5Z2hya3duYWFnMT5ICs7kDawfo1Fgfnw0GJpF94UbYpqrosjPAEkkD-iX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe6d2d2d85378994415d31998572ce471e9c6a93" title="I built a fully offline text to speech Mac app because cloud TTS annoyed me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm an indie maker, and I shipped a native macOS app to solve a problem I personally had.&lt;/p&gt; &lt;p&gt;I deal with a lot of long text (articles, drafts, AI outputs), and I wanted to listen to it while working without sending my content to the cloud or paying subscriptions.&lt;/p&gt; &lt;p&gt;So I built Murmur:&lt;/p&gt; &lt;p&gt;‚Ä¢ Runs entirely offline on Apple Silicon&lt;/p&gt; &lt;p&gt;‚Ä¢ No accounts, no quotas, no subscriptions&lt;/p&gt; &lt;p&gt;‚Ä¢ High-quality AI voices&lt;/p&gt; &lt;p&gt;‚Ä¢ One-time purchase&lt;/p&gt; &lt;p&gt;I mainly use it to:&lt;/p&gt; &lt;p&gt;‚Ä¢ Listen to long articles while doing admin work&lt;/p&gt; &lt;p&gt;‚Ä¢ Review AI outputs hands-free&lt;/p&gt; &lt;p&gt;‚Ä¢ Catch mistakes in drafts by listening instead of rereading&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Le Giveaway&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;To get early feedback on UX and improve the product, I‚Äôm giving away 100% off codes to 10 people!&lt;/p&gt; &lt;p&gt;Just drop a comment on what they would actually use it for and I‚Äôll generate and publish a randomized list of winners by the &lt;strong&gt;end of this week.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'll DM the codes to the most interesting use cases in 48 hours&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1pzbkqk"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarunyadav9761"&gt; /u/tarunyadav9761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fgdnzrkwnaag1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzbw4k/i_built_a_fully_offline_text_to_speech_mac_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzbw4k/i_built_a_fully_offline_text_to_speech_mac_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T07:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjjbw</id>
    <title>Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together</title>
    <updated>2025-12-29T11:02:29+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt; &lt;img alt="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" src="https://a.thumbs.redditmedia.com/bB4zUj7vleOqJdnXmwlZ9s4tewjkzGrf2kPk8Dbebv4.jpg" title="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HyperCLOVA X SEED 32B Think: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HyperCLOVA X SEED 8B Omni: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed"&gt;https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Artificial Analysis on ùïè: &lt;a href="https://x.com/ArtificialAnlys/status/2005429176615174207"&gt;https://x.com/ArtificialAnlys/status/2005429176615174207&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyjjbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzd0j7</id>
    <title>Has anyone built a RAG on WikiLeaks?</title>
    <updated>2025-12-30T08:41:14+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because that would be a useful application. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrjke</id>
    <title>Benchmarks for Quantized Models? (for users locally running Q8/Q6/Q2 precision)</title>
    <updated>2025-12-29T17:00:09+00:00</updated>
    <author>
      <name>/u/No-Grapefruit-1358</name>
      <uri>https://old.reddit.com/user/No-Grapefruit-1358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;Many of us use the quantized Q8/Q6/Q2 model instead of fp16 for obvious reasons. Is there a collection of benchmarks which show SWE, HLE etc on Q8/Q6/Q2 quantized models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Grapefruit-1358"&gt; /u/No-Grapefruit-1358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pywgsb</id>
    <title>Best LLM Related Open Source Tools - 2025?</title>
    <updated>2025-12-29T20:00:49+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think 2025 is good year &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;LLM wise&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now please share the tools you're using with LLMs. I know that half of us here involves with coding by using tools such as Cline, RooCode, KiloCode, QwenCode, MistralVibe, etc.,&lt;/p&gt; &lt;p&gt;Similarly some of us here involves with writing by using Finetuned Writing models. Of course we need tools for writing too. I came across Mikupad, Writingway2, Arrows(p-e-w), WritingTools(theJayTea)&lt;/p&gt; &lt;p&gt;Coding &amp;amp; Writing are just 2 categories I mentioned. Also I mentioned only few tools here(from my bookmarks) &amp;amp; Of course there are so many more other tools exist online which everyone yet to catch. I'm sure around 50 tools available for each category, lets bring those here.&lt;/p&gt; &lt;p&gt;So what other tools are you using? (Please mention category or concise use case)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Just mentioning some categories below to get quick &amp;amp; more replies&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt&lt;/li&gt; &lt;li&gt;RAG, &lt;/li&gt; &lt;li&gt;Brainstorm&lt;/li&gt; &lt;li&gt;AudioBook Maker&lt;/li&gt; &lt;li&gt;Ebook Maker&lt;/li&gt; &lt;li&gt;Second brain&lt;/li&gt; &lt;li&gt;Benchmarks&lt;/li&gt; &lt;li&gt;AI Assistant&lt;/li&gt; &lt;li&gt;Agents&lt;/li&gt; &lt;li&gt;Notebook&lt;/li&gt; &lt;li&gt;NoCode&lt;/li&gt; &lt;li&gt;Wiki&lt;/li&gt; &lt;li&gt;Storytelling/Worldbuilding&lt;/li&gt; &lt;li&gt;Image processing&lt;/li&gt; &lt;li&gt;Game creation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mentioned tools are from github only, I can share link if you need. The reason I didn't include links in this thread because sometime reddit filters remove threads automatically if multiple links present.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So far got mostly coding related tools. Though good to have more tools on coding, lets have more tools on all other categories. I'm thinking of sharing my bookmarks(List of LLM related tools' github repos) later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T20:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyg4yt</id>
    <title>Tencent just released WeDLM 8B Instruct on Hugging Face</title>
    <updated>2025-12-29T07:38:43+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt; &lt;img alt="Tencent just released WeDLM 8B Instruct on Hugging Face" src="https://b.thumbs.redditmedia.com/C56gntSOSvM_cfj95m0peqGLfh8p1Tnt02oONhKPwFM.jpg" title="Tencent just released WeDLM 8B Instruct on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/tencent/WeDLM-8B-Instruct"&gt;https://huggingface.co/tencent/WeDLM-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A diffusion language model that runs 3-6√ó faster than vLLM-optimized Qwen3-8B on math reasoning tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyg4yt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T07:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrn9v</id>
    <title>I Finished a Fully Local Agentic RAG Tutorial</title>
    <updated>2025-12-29T17:03:44+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôve just finished a &lt;strong&gt;complete Agentic RAG tutorial + repository&lt;/strong&gt; that shows how to build a fully local, end-to-end system.&lt;/p&gt; &lt;p&gt;No APIs, no cloud, no hidden costs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üí° What‚Äôs inside&lt;/h3&gt; &lt;p&gt;The tutorial covers the full pipeline, including the parts most examples skip:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF ‚Üí Markdown ingestion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hierarchical chunking (parent / child)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hybrid retrieval (dense + sparse)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Vector store with &lt;strong&gt;Qdrant&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Query rewriting + &lt;strong&gt;human-in-the-loop&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Context summarization&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-agent map-reduce&lt;/strong&gt; with &lt;strong&gt;LangGraph&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Local inference with &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Simple &lt;strong&gt;Gradio&lt;/strong&gt; UI&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üéØ Who it‚Äôs for&lt;/h3&gt; &lt;p&gt;If you want to &lt;strong&gt;understand Agentic RAG by building it&lt;/strong&gt;, not just reading theory, this might help.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üîó Repo&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz4x0v</id>
    <title>RAG Paper 25.12.24</title>
    <updated>2025-12-30T01:45:44+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.21280v1"&gt;SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20916v1"&gt;MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20884v1"&gt;The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T01:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyyp59</id>
    <title>What is the best way to allocated $15k right now for local LLMs?</title>
    <updated>2025-12-29T21:26:38+00:00</updated>
    <author>
      <name>/u/LargelyInnocuous</name>
      <uri>https://old.reddit.com/user/LargelyInnocuous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best bang for $15k right now? Would like to be able to run DeepSeek, Kimi K2 and GLM 4.5+. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LargelyInnocuous"&gt; /u/LargelyInnocuous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T21:26:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz6hol</id>
    <title>Meta acquired Manus !!</title>
    <updated>2025-12-30T02:55:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Manus is a general-purpose autonomous AI agent developed by Butterfly Effect Technology, a Singapore-based startup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://manus.im/blog/manus-joins-meta-for-next-era-of-innovation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6hol/meta_acquired_manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz6hol/meta_acquired_manus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz67hm</id>
    <title>5 new korean models will be released in 2 hours</title>
    <updated>2025-12-30T02:42:37+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura"&gt;https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Naver, LG, SK, NC, Upstage&lt;/p&gt; &lt;p&gt;All 5 models will be released in 2 to 3 hours. Follow with the YouTube link&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcrtb</id>
    <title>Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model</title>
    <updated>2025-12-30T08:26:06+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt; &lt;img alt="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" src="https://preview.redd.it/yq8uriwhxaag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d990cf5383783b3e2aa22351ddeb29ebac5eb2b2" title="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to open-source Tencent HY-Motion 1.0, a billion-parameter text-to-motion model built on the Diffusion Transformer (DiT) architecture and flow matching. Tencent HY-Motion 1.0 empowers developers and individual creators alike by transforming natural language into high-fidelity, fluid, and diverse 3D character animations, delivering exceptional instruction-following capabilities across a broad range of categories. The generated 3D animation assets can be seamlessly integrated into typical 3D animation pipelines.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;üîπBillion-Scale DiT: Successfully scaled flow-matching DiT to 1B+ parameters, setting a new ceiling for instruction-following capability and generated motion quality.&lt;/p&gt; &lt;p&gt;üîπFull-Stage Training Strategy: The industry‚Äôs first motion generation model featuring a complete Pre-training ‚Üí SFT ‚Üí RL loop to optimize physical plausibility and semantic accuracy.&lt;/p&gt; &lt;p&gt;üîπComprehensive Category Coverage: Features 200+ motion categories across 6 major classes‚Äîthe most comprehensive in the industry, curated via a meticulous data pipeline.&lt;/p&gt; &lt;p&gt;üåêProject Page: &lt;a href="https://hunyuan.tencent.com/motion"&gt;https://hunyuan.tencent.com/motion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóGithub: &lt;a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0"&gt;https://github.com/Tencent-Hunyuan/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§óHugging Face: &lt;a href="https://huggingface.co/tencent/HY-Motion-1.0"&gt;https://huggingface.co/tencent/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑTechnical report: &lt;a href="https://arxiv.org/pdf/2512.23464"&gt;https://arxiv.org/pdf/2512.23464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yq8uriwhxaag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzaz73</id>
    <title>[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the "Thinking Process" (Hidden States).</title>
    <updated>2025-12-30T06:40:32+00:00</updated>
    <author>
      <name>/u/JB_King1919</name>
      <uri>https://old.reddit.com/user/JB_King1919</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt; &lt;img alt="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." src="https://b.thumbs.redditmedia.com/Bpxf3FjdwheVRpbXrcXJxisJg0Hd6sGLSEAe6P2x7Fg.jpg" title="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm a PhD student in &lt;strong&gt;Electromagnetics&lt;/strong&gt;. In my daily work, I deal with fields, waves, and trajectories. When I started playing with Local LLMs, I felt something was missing: we usually look at the &lt;em&gt;output&lt;/em&gt; text or the &lt;em&gt;loss curves&lt;/em&gt;, but we rarely see &lt;strong&gt;how&lt;/strong&gt; the model gets from A to B.&lt;/p&gt; &lt;p&gt;To an RF engineer, reasoning isn't just a probability distribution‚Äîit's a &lt;strong&gt;dynamic flow&lt;/strong&gt; through a high-dimensional space.&lt;/p&gt; &lt;p&gt;So, I built a lightweight Python toolkit to extract hidden states layer-by-layer and visualize them as continuous &lt;strong&gt;2D/3D trajectories&lt;/strong&gt;. I wanted to see if &amp;quot;thoughts&amp;quot; have a geometric shape.&lt;/p&gt; &lt;p&gt;The results were surprisingly consistent. I‚Äôm sharing the tool so you can run it on your own models (Llama, Qwen, Mistral, etc.).&lt;/p&gt; &lt;h1&gt;1. The &amp;quot;Confidence Funnel&amp;quot; (Convergence)&lt;/h1&gt; &lt;p&gt;I found that if you feed the model slightly different prompts about the same concept (e.g., &amp;quot;Define Justice&amp;quot;, &amp;quot;What is Fairness&amp;quot;), the internal states start far apart but &lt;strong&gt;physically collapse&lt;/strong&gt; into a single &amp;quot;attractor basin&amp;quot; as the layers get deeper.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c"&gt;https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; You can use this to test &lt;strong&gt;Prompt Stability&lt;/strong&gt;. If the funnel is tight, the model is sure. If it sprays out at the end, the model is confused or hallucinating.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Llama-3 vs. Qwen-2.5: Different &amp;quot;Thinking Styles&amp;quot;&lt;/h1&gt; &lt;p&gt;This was the coolest find. When I ran the same prompts through different architectures, the &amp;quot;shape&amp;quot; of their thinking was totally different.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8"&gt;https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama-3 (Left):&lt;/strong&gt; Seems to &amp;quot;decide&amp;quot; on the semantics very early (Layers 5-10). The trajectory is direct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen-2.5 (Right):&lt;/strong&gt; Keeps the trajectory expanded (in superposition?) until the very last layers (Layer 20+). It seems to &amp;quot;hold&amp;quot; the ambiguity much longer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; This might give us a geometric way to profile model behaviors beyond just benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Visualizing &amp;quot;Refusal&amp;quot; (The Safety Spike)&lt;/h1&gt; &lt;p&gt;I was curious what RLHF looks like geometrically. I visualized the trajectory when the model refuses a jailbreak versus when it follows a safe instruction.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca"&gt;https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hard Refusal(Red):&lt;/strong&gt; Looks like a particle hitting a brick wall‚Äîa sharp, high-curvature spike.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Soft Steering(Green):&lt;/strong&gt; Looks like a smooth turn. And an obvious &amp;quot;U-turn&amp;quot; at the end of its trajectory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; A visual &amp;quot;Geiger Counter&amp;quot; for safety tuning. You can see if your system prompt is creating a hard wall or a soft guide.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì• The Toolkit&lt;/h1&gt; &lt;p&gt;I packaged this into a Python library with example scripts. It works with local HuggingFace weights (no API needed).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/JBKing514/map_llm_toolkit"&gt;LLM Toolkit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß† The Theory (Optional)&lt;/h1&gt; &lt;p&gt;I‚Äôm not an AI researcher, but I wrote up some notes on the &lt;strong&gt;manifold dynamics&lt;/strong&gt; perspective behind this tool (treating inference as a Langevin flow). If you are interested in the math/physics intuition behind these visualizations or need more info about my experiment setup, I put up a page and my notes here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Project Page &amp;amp; Math:&lt;/strong&gt; &lt;a href="https://jbking514.github.io/map_blog/"&gt;Project GitHub Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Foundational Notes:&lt;/strong&gt; &lt;a href="https://zenodo.org/records/17900444"&gt;Manifold Alignment Protocol (MAP)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what &lt;strong&gt;Mistral&lt;/strong&gt; or &lt;strong&gt;Gemma&lt;/strong&gt; trajectories look like if anyone runs this. Let me know what you find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JB_King1919"&gt; /u/JB_King1919 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T06:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcj1q</id>
    <title>Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy</title>
    <updated>2025-12-30T08:11:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt; &lt;img alt="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" src="https://b.thumbs.redditmedia.com/d_jApTNkEXlNvJcoJA6qryuDnUo0ni-DFWBY6RTdAfg.jpg" title="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/collections/tencent/hy-mt15"&gt;https://huggingface.co/collections/tencent/hy-mt15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights: üîπ 1.8B On-Device Power: Optimized for consumer hardware with a 1GB memory footprint. Using on-policy distillation to align with larger models, it delivers 0.18s latency (50 tokens), outperforming mainstream commercial APIs. üîπ 7B SOTA Performance: An upgraded version of our WMT25 champion, surpassing mid-sized open-source models and rivaling the 90th percentile of closed-source giants like Gemini-3.0-Pro. üîπ 33+ Languages &amp;amp; Dialects: High-fidelity translation across 33 languages and 5 Chinese dialects. üîπ Production-Ready: Native support for custom terminology, long-dialogue context, and maintaining document formatting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzcj1q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcdu1</id>
    <title>LG K EXAONE 236b</title>
    <updated>2025-12-30T08:02:08+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt; &lt;img alt="LG K EXAONE 236b" src="https://preview.redd.it/1wirc918taag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fcab270f79f71dba1d330db0ee8e85422de763b" title="LG K EXAONE 236b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will be released in few days &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wirc918taag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7mxr</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:49:11+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if this is real, but the author provides a fascinating story behind its acquisition. I would like for it to be real!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bartowski GGUFs: &lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz68fz</id>
    <title>Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.</title>
    <updated>2025-12-30T02:43:48+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt; &lt;img alt="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." src="https://preview.redd.it/ocq43c2a79ag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e9f7477bee69f806ab9bab82c73557ea1345393" title="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ocq43c2a79ag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7bmv</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:34:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt; &lt;img alt="Llama-3.3-8B-Instruct" src="https://external-preview.redd.it/F-RvVhAB2x8ac9OzOxDw905YUEWDIOQBeDMa2ZyMwo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a109a6917bc66847cb36c61990f58523049b666" title="Llama-3.3-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;allura-forge&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct#llama-33-8b-instruct"&gt;&lt;/a&gt;&lt;strong&gt;Llama 3.3 8B Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yes, this is official, and yes, this is, to my knowledge, a real version of Llama 3.3 8B. (I think, anyways)&lt;/p&gt; &lt;p&gt;Facebook has a &lt;a href="https://llama.developer.meta.com"&gt;Llama API&lt;/a&gt; available that allows for inference of the other Llama models (L3.3 70B, L4 Scout and Maverick), but &lt;em&gt;also&lt;/em&gt; includes a special, new (according to the original press release) &amp;quot;Llama 3.3 8B&amp;quot; that didn't exist anywhere else and was stuck behind the Facebook API!&lt;/p&gt; &lt;p&gt;However. The Llama API supports finetuning L3.3... &lt;em&gt;and downloading the final model in HF format.&lt;/em&gt; Problem solved, right?&lt;/p&gt; &lt;p&gt;Wellllllllllllllll. Not really. The finetuning API was hidden behind layers of support tickets. I tried when the original API dropped in April, and was just told &amp;quot;We'll think about it and send you any updates&amp;quot; (there never were any updates).&lt;/p&gt; &lt;p&gt;Flash forward to December, on a whim I decide to look at the API again. And... by god... the finetuning tab was there. I could click on it and start a job (please ignore that I have no idea how it works, and in fact the finetuning tab actually disappeared after the first time I clicked on it, though I could still manually go to the page).&lt;/p&gt; &lt;p&gt;Apparently, this was not very well tested, as there were a good few bugs, the UI was janky, and the download model function did not actually work due to CORS (I had to manually curl things to get the CDN link).&lt;/p&gt; &lt;p&gt;But... by god... the zip file downloaded, and I had my slightly finetuned model.&lt;/p&gt; &lt;p&gt;To my shock and delight, however, they also provide the adapter that they merged into the model. That means I can &lt;em&gt;subtract&lt;/em&gt; that adapter and get the original model. And... here we are!&lt;/p&gt; &lt;p&gt;(actually, it should be ‚Äúnew model,‚Äù but I used ‚Äúother‚Äù to avoid triggering people)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
