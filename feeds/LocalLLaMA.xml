<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-24T06:27:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ptdtmz</id>
    <title>DGX Spark: an unpopular opinion</title>
    <updated>2025-12-22T23:05:29+00:00</updated>
    <author>
      <name>/u/emdblc</name>
      <uri>https://old.reddit.com/user/emdblc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt; &lt;img alt="DGX Spark: an unpopular opinion" src="https://preview.redd.it/cktkoyb16u8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddc1aaf35931031505022ffcc5838d1fb7a1a8ea" title="DGX Spark: an unpopular opinion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there has been a lot of criticism about the DGX Spark here, so I want to share some of my personal experience and opinion:&lt;/p&gt; &lt;p&gt;I‚Äôm a doctoral student doing data science in a small research group that doesn‚Äôt have access to massive computing resources. We only have a handful of V100s and T4s in our local cluster, and limited access to A100s and L40s on the university cluster (two at a time). Spark lets us prototype and train foundation models, and (at last) compete with groups that have access to high performance GPUs like the H100s or H200s.&lt;/p&gt; &lt;p&gt;I want to be clear: Spark is NOT faster than an H100 (or even a 5090). But its all-in-one design and its massive amount of memory (all sitting on your desk) enable us ‚Äî a small group with limited funding, to do more research.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emdblc"&gt; /u/emdblc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cktkoyb16u8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T23:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pubadw</id>
    <title>New tool to manage models and quantizations</title>
    <updated>2025-12-24T01:46:21+00:00</updated>
    <author>
      <name>/u/Anxious-Visit-7735</name>
      <uri>https://old.reddit.com/user/Anxious-Visit-7735</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i have been working on a tool to manage foundation models and quantizations from them. the goal is make them consistent, reproducible and save storage. It works now, so feedback would be good.&lt;/p&gt; &lt;p&gt;The current implementation can ingest any safetensors model and on demand generate a q2_k to q6_k gguf file. Non uniform. i.e you can via config pick quatization per tensor.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kgrama/gmat-cli/tree/main"&gt;https://github.com/kgrama/gmat-cli/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;|| || |&lt;code&gt;q2_k&lt;/code&gt;|Smallest, lowest quality| |&lt;code&gt;q3_k_s&lt;/code&gt;|3-bit small variant| |&lt;code&gt;q3_k_m&lt;/code&gt;|3-bit medium variant| |&lt;code&gt;q3_k_l&lt;/code&gt;|3-bit large variant| |&lt;code&gt;q4_k_s&lt;/code&gt;|4-bit small variant| |&lt;code&gt;q4_k_m&lt;/code&gt;|4-bit medium variant (default)| |&lt;code&gt;q5_k_s&lt;/code&gt;|5-bit small variant| |&lt;code&gt;q5_k_m&lt;/code&gt;|5-bit medium variant| |&lt;code&gt;q6_k&lt;/code&gt;||&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Visit-7735"&gt; /u/Anxious-Visit-7735 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pubadw/new_tool_to_manage_models_and_quantizations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pubadw/new_tool_to_manage_models_and_quantizations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pubadw/new_tool_to_manage_models_and_quantizations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T01:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptzft4</id>
    <title>Representation Engineering / activation steering: ‚Äúprompting vs finetuning vs steering vectors‚Äù (practical notes + demo)</title>
    <updated>2025-12-23T17:16:21+00:00</updated>
    <author>
      <name>/u/AstraNorth</name>
      <uri>https://old.reddit.com/user/AstraNorth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptzft4/representation_engineering_activation_steering/"&gt; &lt;img alt="Representation Engineering / activation steering: ‚Äúprompting vs finetuning vs steering vectors‚Äù (practical notes + demo)" src="https://preview.redd.it/pbgq9willz8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57ce9f92e2504d518c58e6584a3a818c9e0c7865" title="Representation Engineering / activation steering: ‚Äúprompting vs finetuning vs steering vectors‚Äù (practical notes + demo)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been exploring Representation Engineering (RepE) / activation steering recently and it feels like a useful ‚Äúthird lever‚Äù between prompting and fine-tuning.‚Äã&lt;/p&gt; &lt;p&gt;High-level framing (practitioner view):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompting: fast to iterate, but persona/behavior can drift over long contexts.‚Äã&lt;/li&gt; &lt;li&gt;Fine-tuning: powerful but costly, and it can trade off generality if you push it too hard.‚Äã&lt;/li&gt; &lt;li&gt;Steering (activations): keep weights fixed and add a learned ‚Äúdirection‚Äù in hidden states at inference time (steering vectors), so you can nudge behavior without huge prompts or retraining.‚Äã&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The demo that made it click for me is ‚ÄúThe Eiffel Tower Llama‚Äù (Hugging Face Space / walkthrough): &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=F2jd5WuT-zg"&gt;https://www.youtube.com/watch?v=F2jd5WuT-zg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs interesting is how concrete the concept becomes: you find a direction corresponding to some concept (toy example: ‚ÄúEiffel Tower‚Äù; more generally: honesty/helpfulness/positivity/etc.) and then add/subtract that vector during generation to shift outputs.‚Äã‚Äã&lt;/p&gt; &lt;p&gt;Questions for folks here who‚Äôve implemented this in real setups:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What‚Äôs your go-to method for discovering robust steering directions (contrastive pairs? probes? SAEs?) and which layers tend to be the most controllable?‚Äã&lt;/li&gt; &lt;li&gt;Have you seen steering reliably stack for multi-concept control, or does it quickly start to interfere (one concept breaking another / hurting instruction-following)?‚Äã&lt;/li&gt; &lt;li&gt;Any best practices for evaluating side effects (capability loss, new biases, safety regressions) beyond qualitative samples?‚Äã&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love pointers to good repos, eval recipes, or ‚Äúgotchas‚Äù you‚Äôve hit when moving from toy demos to actual workflows.‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AstraNorth"&gt; /u/AstraNorth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pbgq9willz8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptzft4/representation_engineering_activation_steering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptzft4/representation_engineering_activation_steering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T17:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu5mz1</id>
    <title>Has anyone had success writing x86 assembly with a local model?</title>
    <updated>2025-12-23T21:28:25+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen anyone do any comparisons.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5mz1/has_anyone_had_success_writing_x86_assembly_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5mz1/has_anyone_had_success_writing_x86_assembly_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5mz1/has_anyone_had_success_writing_x86_assembly_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T21:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptr3lv</id>
    <title>r/LocalLLaMA - a year in review</title>
    <updated>2025-12-23T10:56:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt; &lt;img alt="r/LocalLLaMA - a year in review" src="https://external-preview.redd.it/JtVvJz_3p9gkpLWB6adlC3-5M7zJIyvCc23zaC-6JV0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4f54df711befdabb904b0d3f68bc21eb350d5a4" title="r/LocalLLaMA - a year in review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm the same guy that made &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hov3y9/rlocalllama_a_year_in_review/"&gt;2024 edition&lt;/a&gt;, here we are again.&lt;/p&gt; &lt;p&gt;This community has been the central hub for open-source AI for another year, and what a year 2025 has been. Let me take you back to the most notable things happened here during this time. This isn't really a list of model releases or papers, rather posts that were discussed and upvoted by the people here. So notable things missing is also an indication of what was going on. From the rise of Chinese open-source dominance to the hardware hacks, here is what happened in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; in 2025.&lt;/p&gt; &lt;p&gt;The year started with a splash. The &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ho27fr/the_whale_has_landed/"&gt;arrival of &amp;quot;The Whale&amp;quot;&lt;/a&gt; (2121 upvotes, by &lt;a href="/u/fourDnet"&gt;u/fourDnet&lt;/a&gt;) marked the release of DeepSeek V3, setting the tone for what would become the &amp;quot;Year of the Open Source Strike Back.&amp;quot; It wasn't long before we saw &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hphlz7/sam_altman_is_taking_veiled_shots_at_deepseek_and/"&gt;Sam Altman taking veiled shots&lt;/a&gt; (1959 upvotes) at the new competition, a clear sign that the market was changing.&lt;/p&gt; &lt;p&gt;We were all trying to figure out how to run these new beasts. Nvidia teased us with the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt;Digits personal AI supercomputer&lt;/a&gt; (1663 upvotes, by &lt;a href="/u/DubiousLLM"&gt;u/DubiousLLM&lt;/a&gt;), while others were just trying to understand the sheer scale of what was happening. The realization that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;DeepSeek was essentially a side project&lt;/a&gt; (2861 upvotes, by &lt;a href="/u/ParsaKhaz"&gt;u/ParsaKhaz&lt;/a&gt;) for a hedge fund only made it even more interesting.&lt;/p&gt; &lt;p&gt;By late January, the narrative was clear: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"&gt;Meta was panicked&lt;/a&gt; (2779 upvotes, by &lt;a href="/u/Optimal_Hamster5789"&gt;u/Optimal_Hamster5789&lt;/a&gt;), reportedly &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;scrambling &amp;quot;war rooms&amp;quot;&lt;/a&gt; (2117 upvotes, by &lt;a href="/u/FullstackSensei"&gt;u/FullstackSensei&lt;/a&gt;) to catch up. The community was buzzing with benchmarks, with &lt;a href="/u/kyazoglu"&gt;u/kyazoglu&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"&gt;testing almost every model that fits in 24GB VRAM&lt;/a&gt; (1861 upvotes) - a hero's work for the GPU-poor among us.&lt;/p&gt; &lt;p&gt;The &amp;quot;DeepSeek effect&amp;quot; was everywhere. &lt;a href="/u/Porespellar"&gt;u/Porespellar&lt;/a&gt; summed it up perfectly: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt;&amp;quot;All DeepSeek, all the time&amp;quot;&lt;/a&gt; (4116 upvotes). But it wasn't just about models; it was about what we could &lt;em&gt;do&lt;/em&gt; with them. We saw inspiring projects like &lt;a href="/u/Dry_Steak30"&gt;u/Dry_Steak30&lt;/a&gt;'s &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;open source tool to find their autoimmune disease&lt;/a&gt; (2488 upvotes), proving that local AI is more than just a hobby.&lt;/p&gt; &lt;p&gt;Of course, it wouldn't be 2025 without some drama. The threat of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;20 years in jail for downloading Chinese models&lt;/a&gt; (2092 upvotes, by &lt;a href="/u/segmond"&gt;u/segmond&lt;/a&gt;) worried us, but that didn't stop the innovation. We laughed when &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"&gt;Grok's think mode leaked its system prompt&lt;/a&gt; (6465 upvotes, by &lt;a href="/u/onil_gova"&gt;u/onil_gova&lt;/a&gt;), and cheered when DeepSeek announced they would &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt;open-source 5 repos&lt;/a&gt; (4560 upvotes, by &lt;a href="/u/Nunki08"&gt;u/Nunki08&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Hardware remained a constant obsession. We drooled over &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt;Framework's new Ryzen Max desktop&lt;/a&gt; (2004 upvotes, by &lt;a href="/u/sobe3249"&gt;u/sobe3249&lt;/a&gt;) and marveled at the monstrosity that was &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;16x 3090s&lt;/a&gt; (1797 upvotes, by &lt;a href="/u/Conscious_Cut_6144"&gt;u/Conscious_Cut_6144&lt;/a&gt;). &amp;quot;It's alive!&amp;quot; indeed.&lt;/p&gt; &lt;p&gt;Spring brought the highly anticipated Llama 4. Mark Zuckerberg &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;presented the models&lt;/a&gt; (2645 upvotes, by &lt;a href="/u/LarDark"&gt;u/LarDark&lt;/a&gt;), but the community felt it &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt;fell short&lt;/a&gt; (2175 upvotes, by &lt;a href="/u/Rare-Site"&gt;u/Rare-Site&lt;/a&gt;). The community was let down, especially when compared to the relentless release schedule from the East.&lt;/p&gt; &lt;p&gt;Open Weight releases continued, though, we got &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt;DeepCoder&lt;/a&gt; (1609 upvotes, by &lt;a href="/u/TKGaming_11"&gt;u/TKGaming_11&lt;/a&gt;) and saw &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;DeepSeek open-sourcing their inference engine&lt;/a&gt; (1760 upvotes, by &lt;a href="/u/Dr_Karminski"&gt;u/Dr_Karminski&lt;/a&gt;). There was also a moment of collective frustration when &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jzocoo/finally_someone_noticed_this_unfair_situation/"&gt;llama.cpp was snubbed&lt;/a&gt; (1742 upvotes, by &lt;a href="/u/nekofneko"&gt;u/nekofneko&lt;/a&gt;) in favor of shinier wrappers.&lt;/p&gt; &lt;p&gt;Then came &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ka6mic/qwen_3/"&gt;Qwen 3&lt;/a&gt; (1940 upvotes, by &lt;a href="/u/ResearchCrafty1804"&gt;u/ResearchCrafty1804&lt;/a&gt;). The excitement was back. We were running &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;real-time webcam demos with SmolVLM&lt;/a&gt; (2762 upvotes, by &lt;a href="/u/dionisioalcaraz"&gt;u/dionisioalcaraz&lt;/a&gt;) and building &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt;fully local voice AIs&lt;/a&gt; (2447 upvotes, by &lt;a href="/u/RoyalCities"&gt;u/RoyalCities&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The reality of our hardware addiction hit hard with the question: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt;&amp;quot;96GB VRAM! What should run first?&amp;quot;&lt;/a&gt; (1745 upvotes, by &lt;a href="/u/Mother_Occasion_8076"&gt;u/Mother_Occasion_8076&lt;/a&gt;). And as &lt;a href="/u/TheLogiqueViper"&gt;u/TheLogiqueViper&lt;/a&gt; noted, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;China is leading open source&lt;/a&gt; (2618 upvotes).&lt;/p&gt; &lt;p&gt;We found humor in the absurdity of it all. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"&gt;&amp;quot;When you figure out it‚Äôs all just math&amp;quot;&lt;/a&gt; (4123 upvotes, by &lt;a href="/u/Current-Ticket4214"&gt;u/Current-Ticket4214&lt;/a&gt;) was a top post, and we all related to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt;running models at the airport&lt;/a&gt; (2378 upvotes, by &lt;a href="/u/Current-Ticket4214"&gt;u/Current-Ticket4214&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Summer was a season of delays and parodies. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;&amp;quot;We have to delay it&amp;quot;&lt;/a&gt; (3574 upvotes, by &lt;a href="/u/ILoveMy2Balls"&gt;u/ILoveMy2Balls&lt;/a&gt;) became the catchphrase for Western labs. We poked fun with a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt;tester version of the &amp;quot;open-weight&amp;quot; OpenAI model&lt;/a&gt; (1639 upvotes, by &lt;a href="/u/Firepal64"&gt;u/Firepal64&lt;/a&gt;) and a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;friendly reminder about Grok 3&lt;/a&gt; (1447 upvotes, by &lt;a href="/u/Wrong_User_Logged"&gt;u/Wrong_User_Logged&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;But the community kept building. &lt;a href="/u/hotroaches4liferz"&gt;u/hotroaches4liferz&lt;/a&gt; made a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;1000 hour NSFW TTS dataset&lt;/a&gt; (1516 upvotes)-because of course they did. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt;Qwen3-Coder arrived&lt;/a&gt; (1925 upvotes, by &lt;a href="/u/ResearchCrafty1804"&gt;u/ResearchCrafty1804&lt;/a&gt;), followed by the blazing fast &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"&gt;Qwen3-Coder-Flash&lt;/a&gt; (1694 upvotes).&lt;/p&gt; &lt;p&gt;The sentiment shifted as Meta seemingly bowed out of open source: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;&amp;quot;Bye bye, Meta AI&amp;quot;&lt;/a&gt; (1492 upvotes, by &lt;a href="/u/absolooot1"&gt;u/absolooot1&lt;/a&gt;). Meanwhile, we got the adorable &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;Kitten TTS&lt;/a&gt; (2460 upvotes, by &lt;a href="/u/ElectricalBar7464"&gt;u/ElectricalBar7464&lt;/a&gt;) and continued to dream of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt;open source code models rivaling Claude&lt;/a&gt; (2304 upvotes, by &lt;a href="/u/Severe-Awareness829"&gt;u/Severe-Awareness829&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; remained &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt;&amp;quot;the last sane place to discuss LLMs&amp;quot;&lt;/a&gt; (2181 upvotes, by &lt;a href="/u/ForsookComparison"&gt;u/ForsookComparison&lt;/a&gt;). Even if we did have to vent about &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;Ollama&lt;/a&gt; (1906 upvotes, by &lt;a href="/u/jacek2023"&gt;u/jacek2023&lt;/a&gt;) occasionally.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;China entering the GPU market&lt;/a&gt; (4171 upvotes, by &lt;a href="/u/CeFurkan"&gt;u/CeFurkan&lt;/a&gt;) with 96GB cards for under $2000 was a game-changer. Some of us even went to Shenzhen to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;buy modded 4090s&lt;/a&gt; (1924 upvotes, by &lt;a href="/u/king_priam_of_Troy"&gt;u/king_priam_of_Troy&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We celebrated the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;biggest providers for the community&lt;/a&gt; (2918 upvotes, by &lt;a href="/u/dead-supernova"&gt;u/dead-supernova&lt;/a&gt;)-mostly Chinese labs now-and devoured &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;Stanford's 5.5hrs of lectures&lt;/a&gt; (2731 upvotes, by &lt;a href="/u/igorwarzocha"&gt;u/igorwarzocha&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The year ended with a mix of high-level tools and deep-dive resources. We got &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt;Heretic for automatic censorship removal&lt;/a&gt; (3008 upvotes, by &lt;a href="/u/-p-e-w-"&gt;u/-p-e-w-&lt;/a&gt;) and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt;200+ pages of Hugging Face secrets&lt;/a&gt; (2204 upvotes, by &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;And finally, the memes kept us grounded. The &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;Realist meme of the year&lt;/a&gt; (1926 upvotes, by &lt;a href="/u/Slight_Tone_2188"&gt;u/Slight_Tone_2188&lt;/a&gt;) reminded us that no matter how advanced the models get, we'll always be RAM poor from now on.&lt;/p&gt; &lt;p&gt;That's it, folks. 2025 was the year the open-source torch passed to the East, the year our hardware dreams got a little wilder (and insanely more expensive). Here's to another year of local LLMs!&lt;/p&gt; &lt;p&gt;P.S. I wasn't going to make a recap this year, but &lt;a href="https://gist.github.com/qingy1337"&gt;qingy1337&lt;/a&gt; kindly asked on GitHub if I would which touched me. So here it is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T10:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pty0kf</id>
    <title>Intel x Nvidia Serpent Lake leaks as Strix Halo rival: capable CPU, RTX Rubin iGPU, 16x LPDDR6.</title>
    <updated>2025-12-23T16:20:02+00:00</updated>
    <author>
      <name>/u/CYTR_</name>
      <uri>https://old.reddit.com/user/CYTR_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty0kf/intel_x_nvidia_serpent_lake_leaks_as_strix_halo/"&gt; &lt;img alt="Intel x Nvidia Serpent Lake leaks as Strix Halo rival: capable CPU, RTX Rubin iGPU, 16x LPDDR6." src="https://external-preview.redd.it/zDVzxhAxQ5w8_AjaN1AtROq_Cu1ubeOIqtKDbvjiOZA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71ee57553bd88813a2db74fe1c6fa688b0b6458a" title="Intel x Nvidia Serpent Lake leaks as Strix Halo rival: capable CPU, RTX Rubin iGPU, 16x LPDDR6." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;These powerful RTX iGPUs are reportedly coming with Intel Serpent Lake. Described as Intel's response to AMD Strix Halo/ Zen 6 Medusa Halo APUs...&lt;/p&gt; &lt;p&gt;[...]&lt;/p&gt; &lt;p&gt;For the GPU chiplet, Intel is said to be partnering with Nvidia to use the latter's RTX Rubin GPU architecture, or a close variant, for integrated graphics. The iGPU could be based on the TSMC N3P process node, which is to be expected.&lt;/p&gt; &lt;p&gt;Moreover, the leaker suggests that the Serpent Lake APUs could also bring support for 16X LPDDR6 memory. This likely refers to Serpent Lake supporting 16 memory channels for increased bandwidth.&amp;quot;&lt;/p&gt; &lt;p&gt;Potentially very interesting if nothing dethrones CUDA in the coming years and if Medusa Halo is disappointing from a bandwidth perspective. Of course, we can expect a prohibitive price and certainly a very late release given the current context.&lt;/p&gt; &lt;p&gt;Time will tell.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CYTR_"&gt; /u/CYTR_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/Intel-x-Nvidia-Serpent-Lake-leaks-as-Strix-Halo-rival-with-capable-CPU-and-big-GeForce-RTX-Rubin-iGPU.1190608.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty0kf/intel_x_nvidia_serpent_lake_leaks_as_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pty0kf/intel_x_nvidia_serpent_lake_leaks_as_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1puf3hy</id>
    <title>Buy or skip new laptop for local llm, programming, etc</title>
    <updated>2025-12-24T05:05:01+00:00</updated>
    <author>
      <name>/u/SafeAmazing8507</name>
      <uri>https://old.reddit.com/user/SafeAmazing8507</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I own a second hand asus tuf amd, nvdia GTX 1650. It has windows with 2 users (main and gaming), isolated gaming to prevent me from over playing. Main has personal professional stuff. This laptop is fine for now while I am confused, if whether should I buy new laptop, can expend upto 8k per month - can aim or buy upto 150000 inr&lt;/p&gt; &lt;p&gt;I do backend, llm agent development, little frontend stuff, interest in ml - pytorch etc . I have not tried to do local llm for GTX 1650 but very much intrigued.&lt;/p&gt; &lt;p&gt;So my options are&lt;/p&gt; &lt;p&gt;Apple Mac Book and later build pc for gaming, Laptop with rtx and later build pc Or hold for now and later build pc&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I have never tried apple but I heard from friends apple Mac Book are good for developement with a good programming support and also seen their unified memory supports local llm. Concern here is the apple ecosystem.&lt;/p&gt; &lt;p&gt;If fine how much should I set spec - I am thinking to set upto 16 gb of ram ? Is higher needed ?&lt;/p&gt; &lt;p&gt;Or rtx laptop with 8 gb vram or wait for now?&lt;/p&gt; &lt;p&gt;Thank you for reading to the end, looking forward to your response Thank you &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Edit 1:&lt;/p&gt; &lt;p&gt;Pc is my all time choice, but if I go pc now , the budget required would be double/triple of the amount mentioned above. But I will eventually build one :) &lt;/p&gt; &lt;p&gt;I'm just confused primarily whether I should buy an Apple Mac now or skip &lt;/p&gt; &lt;p&gt;If I buy a Mac whether air or pro, should I buy a higher ram up to which? Like 32 gb ram is ok not enough for llm but except local llm , is 32 gb ram needed &lt;/p&gt; &lt;p&gt;I am confused about these thoughts.&lt;/p&gt; &lt;p&gt;Thank you for your guidance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SafeAmazing8507"&gt; /u/SafeAmazing8507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf3hy/buy_or_skip_new_laptop_for_local_llm_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf3hy/buy_or_skip_new_laptop_for_local_llm_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puf3hy/buy_or_skip_new_laptop_for_local_llm_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T05:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptw5ol</id>
    <title>Could it be GLM 4.7 Air?</title>
    <updated>2025-12-23T15:05:26+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Head of Global Brand &amp;amp; Partnerships @Zai_org&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;says:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We have a new model coming soon. Stay tuned! üòù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/louszbd/status/2003153617013137677"&gt;https://x.com/louszbd/status/2003153617013137677&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe the Air version is next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T15:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1puejhe</id>
    <title>MiraTTS Docker FastAPI server</title>
    <updated>2025-12-24T04:34:30+00:00</updated>
    <author>
      <name>/u/EmotionalWillow70</name>
      <uri>https://old.reddit.com/user/EmotionalWillow70</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a dockerized FastAPI wrapper for MiraTTS. It exposes OpenAI-compatible endpoints so you can use it into existing LLM frontends.&lt;/p&gt; &lt;p&gt;Since MiraTTS doesn't support native streaming yet, I implemented a custom text chunker. It splits long inputs into safe segments, batches them for the GPU, and stitches the output together. This allows you to generate audio for long texts without hitting the model's character limits.&lt;/p&gt; &lt;p&gt;Repo here: &lt;a href="https://github.com/Si-ris-B/MiraTTS-FastAPI-Docker"&gt;https://github.com/Si-ris-B/MiraTTS-FastAPI-Docker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmotionalWillow70"&gt; /u/EmotionalWillow70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puejhe/miratts_docker_fastapi_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puejhe/miratts_docker_fastapi_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puejhe/miratts_docker_fastapi_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T04:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pufhgk</id>
    <title>Let's predict GLM Air</title>
    <updated>2025-12-24T05:26:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Questions about GLM Air were not answered in the recent AMA. What is your prediction about the future of GLM Air?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1pufhgk"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pufhgk/lets_predict_glm_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pufhgk/lets_predict_glm_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pufhgk/lets_predict_glm_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T05:26:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pubu7x</id>
    <title>Best model for Japanese to English?</title>
    <updated>2025-12-24T02:14:14+00:00</updated>
    <author>
      <name>/u/Red2005dragon</name>
      <uri>https://old.reddit.com/user/Red2005dragon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. I'm using mangaOCR for capturing text from images and it's pretty damn accurate. But now I want to know what the best model for translation is.&lt;/p&gt; &lt;p&gt;I would like something on the smaller side if possible so below 20b would be preferable. But if something is 20b or just slightly above it then that would be fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red2005dragon"&gt; /u/Red2005dragon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pubu7x/best_model_for_japanese_to_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pubu7x/best_model_for_japanese_to_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pubu7x/best_model_for_japanese_to_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T02:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu8zj3</id>
    <title>I wrote an interactive blog post teaching how tokenization, embeddings, and vector search work in-browser with Transformers.js</title>
    <updated>2025-12-23T23:56:52+00:00</updated>
    <author>
      <name>/u/mike_dot_dev</name>
      <uri>https://old.reddit.com/user/mike_dot_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu8zj3/i_wrote_an_interactive_blog_post_teaching_how/"&gt; &lt;img alt="I wrote an interactive blog post teaching how tokenization, embeddings, and vector search work in-browser with Transformers.js" src="https://external-preview.redd.it/XlQcYLikCRIkdRlT2ds5M1FMprcLdYd0_75yS7-q-1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=132412f978c8e30c262460a0cbb047da98d6d0ee" title="I wrote an interactive blog post teaching how tokenization, embeddings, and vector search work in-browser with Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be up front that the post is entirely built with AI, as is the copy. However, I feel like if creating blog posts is this easy, we are obligated to transfer the saved effort into maximizing the learning potential of our content. &lt;/p&gt; &lt;p&gt;So, this post includes an interactive lab that hopefully will find worth your time. &lt;/p&gt; &lt;p&gt;What‚Äôs your opinion? Is this slop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mike_dot_dev"&gt; /u/mike_dot_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mike.dev/blog/transformersjs-embeddings-lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu8zj3/i_wrote_an_interactive_blog_post_teaching_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu8zj3/i_wrote_an_interactive_blog_post_teaching_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T23:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptttcm</id>
    <title>How to run the GLM-4.7 model locally on your own device (guide)</title>
    <updated>2025-12-23T13:23:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt; &lt;img alt="How to run the GLM-4.7 model locally on your own device (guide)" src="https://preview.redd.it/b995ei5mfy8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4519336dd309d77b0ea2caf5ea5cb6af6df8bf4" title="How to run the GLM-4.7 model locally on your own device (guide)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7 is Z.ai‚Äôs latest thinking model, delivering stronger coding, agent, and chat performance than GLM-4.6 &lt;/li&gt; &lt;li&gt;It achieves SOTA performance on on SWE-bench (73.8%, +5.8), SWE-bench Multilingual (66.7%, +12.9), and Terminal Bench 2.0 (41.0%, +16.5).&lt;/li&gt; &lt;li&gt;The full 355B parameter model requires &lt;strong&gt;400GB&lt;/strong&gt; of disk space, while the Unsloth Dynamic 2-bit GGUF reduces the size to &lt;strong&gt;134GB&lt;/strong&gt; (-&lt;strong&gt;75%)&lt;/strong&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official blog post - &lt;a href="https://docs.unsloth.ai/models/glm-4.7"&gt;https://docs.unsloth.ai/models/glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b995ei5mfy8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T13:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptytig</id>
    <title>Two new 12B finetunes for adventure, role play and writing</title>
    <updated>2025-12-23T16:52:08+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This one was &lt;strong&gt;cooking for ~4 month&lt;/strong&gt;. I'll give here the TL;DR for each model, for full details, check the model cards:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Impish_Bloodmoon_12B&lt;/strong&gt; üòà&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Frontier-adjacent like capabilities, now locally available in 12B! (Stats, items, traits triggering, and so much more).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very strong theory of mind!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Well over &lt;strong&gt;1B&lt;/strong&gt; tokens trained!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fallout &amp;amp; Morrowind&lt;/strong&gt; fandom refined!&lt;/li&gt; &lt;li&gt;Heat turned to &lt;strong&gt;11&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt; Additional languages added: Japanese, Hebrew, Russian.&lt;/li&gt; &lt;li&gt;1-shot JSON roleplay datasets! Escape velocity reached! (even for those who can't run DSV3 \ Kimi).&lt;/li&gt; &lt;li&gt;Less positivity bias , all lessons from the successful Negative_LLAMA_70B style of data learned &amp;amp; integrated, with serious upgrades added ‚Äî and it shows! (Note: if this bites you a bit too hard, try Angelic_Eclipse_12B. üëº)&lt;/li&gt; &lt;li&gt;Reduced slop for both roleplay and creative tasks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Angelic_Eclipse_12B&lt;/strong&gt; üëº&lt;/p&gt; &lt;p&gt;Very similar capabilities to the above, but:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Reactions realism&lt;/strong&gt;. It meant to reflect real-life behaviour accurately&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slow burn&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Powerful 'vanilla assistant'&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The models are &lt;strong&gt;available on HuggingFace&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Angelic_Eclipse_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Angelic_Eclipse_12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptz6xy</id>
    <title>AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ</title>
    <updated>2025-12-23T17:06:40+00:00</updated>
    <author>
      <name>/u/GGwithRabbit</name>
      <uri>https://old.reddit.com/user/GGwithRabbit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"&gt; &lt;img alt="AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ" src="https://external-preview.redd.it/NzF3YWpkZmxqejhnMSn9Bvgd5F2HIaI4NgTX7xfRCm50JCfHFGJKJxKbbOUZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7f0ad905dfb18baa2431a03e1610a6c84dcefa2" title="AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Meta's &lt;strong&gt;SAM-Audio&lt;/strong&gt; is a breakthrough for object-oriented audio separation (e.g., &amp;quot;extract the violin from this busy track&amp;quot; using natural language), but the original repo has a massive VRAM footprint. Many users (including myself) experienced OOM errors even on high-end cards because it loads vision encoders and rankers by default.&lt;/p&gt; &lt;p&gt;I built &lt;strong&gt;AudioGhost AI&lt;/strong&gt; ‚Äî an open-source, full-stack GUI designed to bring this power to laptop and consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üöÄ &lt;strong&gt;Lite Mode (Low VRAM):&lt;/strong&gt; By stripping unused encoders and rankers, I got the VRAM usage down to &lt;strong&gt;4GB-6GB&lt;/strong&gt; for the Small model and &lt;strong&gt;~10GB&lt;/strong&gt; for Large.&lt;/li&gt; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Windows 1-Click Installer:&lt;/strong&gt; No more wrestling with FFmpeg versions or TorchCodec DLL errors. The &lt;code&gt;install.bat&lt;/code&gt; handles everything.&lt;/li&gt; &lt;li&gt;üé® &lt;strong&gt;Modern Interface:&lt;/strong&gt; Next.js + Tailwind glassmorphism UI with real-time waveform and stem mixing.&lt;/li&gt; &lt;li&gt;‚ö° &lt;strong&gt;Local-First:&lt;/strong&gt; Privacy is paramount‚Äîeverything runs 100% on your own hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance (4090 Tested, 4:26 audio (11 chunks @ 25s each)):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small Model: ~6GB VRAM | 25s |&lt;/li&gt; &lt;li&gt;Large Model: ~10GB VRAM | 41s |&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I truly believe &lt;strong&gt;SAM-Audio&lt;/strong&gt; is the future of audio editing, and I hope this tool makes it accessible to more creators who don't have access to lab-grade GPU clusters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub (Open Source):&lt;/strong&gt; &lt;a href="https://github.com/0x0funky/audioghost-ai"&gt;https://github.com/0x0funky/audioghost-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts, feedback, or any issues you find while running it on your rig! üëª&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GGwithRabbit"&gt; /u/GGwithRabbit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ovsyaleljz8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T17:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1puee2h</id>
    <title>I built an open-source AI security platform with 121 detection engines AND a red team toolkit with 39,000+ payloads</title>
    <updated>2025-12-24T04:26:16+00:00</updated>
    <author>
      <name>/u/ParticularSubject966</name>
      <uri>https://old.reddit.com/user/ParticularSubject966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; After 2 years of development, I'm releasing SENTINEL ‚Äî a complete AI security suite that both protects your LLMs in production AND lets you pentest them before deployment. Free Community Edition, open source.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;We're all deploying LLMs everywhere ‚Äî chatbots, agents, RAG systems, autonomous workflows. But securing them? It's a mess:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt injection&lt;/strong&gt; is trivially easy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jailbreaks&lt;/strong&gt; get past most guardrails&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data exfiltration&lt;/strong&gt; through AI responses is a real threat&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic attacks&lt;/strong&gt; (MCP, tool poisoning) are the new frontier&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I couldn't find a tool that both &lt;strong&gt;defended&lt;/strong&gt; my AI apps AND let me &lt;strong&gt;attack-test&lt;/strong&gt; them. So I built one.&lt;/p&gt; &lt;h1&gt;What I Made&lt;/h1&gt; &lt;h1&gt;üõ°Ô∏è SENTINEL Defense&lt;/h1&gt; &lt;p&gt;Real-time protection for LLM applications:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Detection Engines&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;121&lt;/strong&gt; specialized engines&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Recall&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;85.1%&lt;/strong&gt; on prompt injection&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Latency&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&amp;lt;10ms&lt;/strong&gt; (Go gateway)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Coverage&lt;/td&gt; &lt;td align="left"&gt;OWASP LLM Top 10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;The cool stuff:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strange Math‚Ñ¢&lt;/strong&gt; ‚Äî I used TDA (topological data analysis), sheaf theory, and hyperbolic geometry to detect attacks that pattern matching misses&lt;/li&gt; &lt;li&gt;&lt;a href="http://TTPs.ai"&gt;&lt;strong&gt;TTPs.ai&lt;/strong&gt;&lt;/a&gt; ‚Äî Attack framework detection (like MITRE but for AI)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Protocol Security&lt;/strong&gt; ‚Äî MCP and A2A protection for agentic systems&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üêâ Strike Offense&lt;/h1&gt; &lt;p&gt;Red team toolkit for AI applications:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Attack Payloads&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;39,000+&lt;/strong&gt; from 13 sources&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Attack Modes&lt;/td&gt; &lt;td align="left"&gt;Web + LLM + Hybrid&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Parallel Agents&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;9&lt;/strong&gt; (HYDRA architecture)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WAF Bypass&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;25+&lt;/strong&gt; techniques&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;The cool stuff:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Attack Planner&lt;/strong&gt; ‚Äî Uses Gemini to plan attack strategies&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Anti-Deception Engine&lt;/strong&gt; ‚Äî Detects honeypots and tarpits&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep Recon&lt;/strong&gt; ‚Äî Finds hidden AI endpoints (ChatbotFinder)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bilingual Reports&lt;/strong&gt; ‚Äî English + Russian (üá∫üá∏/üá∑üá∫)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why Both?&lt;/h1&gt; &lt;p&gt;The philosophy is simple:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Strike finds vulnerabilities ‚Üí SENTINEL blocks them in production &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Test your AI before attackers do. Then deploy with confidence.&lt;/p&gt; &lt;h1&gt;Tech Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gateway:&lt;/strong&gt; Go 1.21+ / Fiber (for speed)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Brain:&lt;/strong&gt; Python 3.11+ (for ML ecosystem)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector DB:&lt;/strong&gt; ChromaDB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment:&lt;/strong&gt; Docker/K8s native&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Free vs Enterprise&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;Community üÜì&lt;/th&gt; &lt;th align="left"&gt;Enterprise üîê&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Basic Detection&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Strange Math (Basic)&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Strike Offense&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Advanced Engines&lt;/td&gt; &lt;td align="left"&gt;‚ùå&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2025 Innovations&lt;/td&gt; &lt;td align="left"&gt;‚ùå&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Support&lt;/td&gt; &lt;td align="left"&gt;Community&lt;/td&gt; &lt;td align="left"&gt;Dedicated&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Community Edition is fully functional ‚Äî not a trial, not a demo.&lt;/p&gt; &lt;h1&gt;Quick Start (Strike)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/DmitrL-dev/AISecurity cd strike pip install -r requirements.txt # CLI mode python -m strike --target https://example.com/chat # Web Console python dashboard.py # Open http://localhost:5000 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/DmitrL-dev/AISecurity"&gt;https://github.com/DmitrL-dev/AISecurity&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://dmitrl-dev.github.io/AISecurity/"&gt;https://dmitrl-dev.github.io/AISecurity/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free Signatures CDN:&lt;/strong&gt; 39,000+ patterns, updated daily&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I'm Looking For&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Feedback&lt;/strong&gt; ‚Äî What's missing? What should I add?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bug reports&lt;/strong&gt; ‚Äî Break it, I want to know&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use cases&lt;/strong&gt; ‚Äî How would you use this?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaboration&lt;/strong&gt; ‚Äî Open to partnerships&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;FAQ&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Q: Is this actually free?&lt;/strong&gt;&lt;br /&gt; A: Yes. Community Edition is free forever. Enterprise features require licensing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can I use Strike legally?&lt;/strong&gt;&lt;br /&gt; A: Only on systems you own or have permission to test. Bug bounty programs, yes. Random targets, no.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Why &amp;quot;Strange Math&amp;quot;?&lt;/strong&gt;&lt;br /&gt; A: Because &amp;quot;Topological Data Analysis with Persistent Homology and Sheaf-Theoretic Semantic Coherence Verification&amp;quot; didn't fit on the badge.&lt;/p&gt; &lt;h1&gt;‚ö†Ô∏è Solo Developer Disclaimer&lt;/h1&gt; &lt;p&gt;I work on this project &lt;strong&gt;alone&lt;/strong&gt;. If you find bugs, rough edges, or incomplete features ‚Äî I apologize in advance.&lt;/p&gt; &lt;p&gt;Your bug reports and feedback help me improve. Be patient, be kind, and I'll fix things as fast as I can.&lt;/p&gt; &lt;p&gt;‚≠ê &lt;strong&gt;If you find this useful, starring the repo and sharing this post really inspires me and helps the project grow!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions. Roast my code. Tell me what sucks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParticularSubject966"&gt; /u/ParticularSubject966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puee2h/i_built_an_opensource_ai_security_platform_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puee2h/i_built_an_opensource_ai_security_platform_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puee2h/i_built_an_opensource_ai_security_platform_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T04:26:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1puf614</id>
    <title>New 1B parameter open-source coding model getting 76% on HumanEval [shameless but proud self-plug]</title>
    <updated>2025-12-24T05:08:56+00:00</updated>
    <author>
      <name>/u/More_Article9837</name>
      <uri>https://old.reddit.com/user/More_Article9837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, merry festive season to you all. Hope you are staying safe!&lt;br /&gt; Wanted to share a new open-source coding model release that might be interesting to yall here. My team proudly published it this morning..(we are a small start up out of Australia) &lt;/p&gt; &lt;p&gt;It‚Äôs called Maincoder-1B... a 1B-parameter code generation model that gets 76% on HumanEval, which is unusually high for a model this small (so far its ranking best-in-class for open models in that size range). &lt;/p&gt; &lt;p&gt;Our focus isn‚Äôt on scaling up, but on making small models actually good. We know that with a lot of real-world use cases such as: interactive tools, local/offline coding, batch refactors, search-based program synthesis... you care more about latency, cost, and fast rollouts than having a massive model. &lt;/p&gt; &lt;p&gt;Some key points to note:&lt;br /&gt; -Designed for low-latency and low-cost inference&lt;br /&gt; -Can run locally or on constrained hardware&lt;br /&gt; -Useful for systems that need many cheap generations (search, verification, RL-style loops)&lt;br /&gt; -as well as fine tuning to personal preferences&lt;br /&gt; -Released under Apache 2.0 &lt;/p&gt; &lt;p&gt;It does have the expected limitations: ~2k context window and it‚Äôs best at small, self-contained tasks....not large codebases or safety-critical code without human review. &lt;/p&gt; &lt;p&gt;Weights and benchmarks and all that are here:&lt;br /&gt; &lt;a href="https://huggingface.co/Maincode/Maincoder-1B"&gt;https://huggingface.co/Maincode/Maincoder-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The full release note is here: &lt;a href="https://maincode.com/maincoder/"&gt;https://maincode.com/maincoder/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Keen to hear your thoughts ..and particularly where small-but-strong coding models fit best today. Thanks in advance for your support :) We are excited to have got this over the line!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/More_Article9837"&gt; /u/More_Article9837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T05:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu2bwy</id>
    <title>New Update - Mistral Vibe v1.3.0</title>
    <updated>2025-12-23T19:10:57+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new &lt;a href="https://github.com/mistralai/mistral-vibe"&gt;&lt;strong&gt;Vibe&lt;/strong&gt;&lt;/a&gt; update is here! We‚Äôre keeping the momentum going by including &lt;a href="https://agentskills.io/home"&gt;Agent Skills&lt;/a&gt; in this latest Vibe update. Agent Skills are &lt;strong&gt;collections of instructions, scripts, and resources that agents can discover and use to perform tasks&lt;/strong&gt; more accurately and efficiently.&lt;/p&gt; &lt;h1&gt;Changelog&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Agent Skills Support&lt;/li&gt; &lt;li&gt;Native Terminal Theme Support&lt;/li&gt; &lt;li&gt;Reasoning Models Support&lt;/li&gt; &lt;li&gt;Multiple Bug Fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-# Learn more about the changes &lt;a href="https://github.com/mistralai/mistral-vibe/blob/main/CHANGELOG.md#130---2025-12-23"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Happy shipping - and happy holidays!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;-&amp;gt; &lt;code&gt;uv tool install mistral-vibe&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T19:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu5bob</id>
    <title>Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)</title>
    <updated>2025-12-23T21:15:04+00:00</updated>
    <author>
      <name>/u/ikergarcia1996</name>
      <uri>https://old.reddit.com/user/ikergarcia1996</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"&gt; &lt;img alt="Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)" src="https://a.thumbs.redditmedia.com/CHgaVBJnTEJo9i67FY5qjGb_T_RQBrsSE-FaAinfEO8.jpg" title="Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü§ó Link to the hugging face model: &lt;a href="https://huggingface.co/MultiverseComputingCAI/Qwen3-Next-80B-A3B-Thinking-Uncensored"&gt;https://huggingface.co/MultiverseComputingCAI/Qwen3-Next-80B-A3B-Thinking-Uncensored&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I am a researcher at &lt;a href="https://multiversecomputing.com"&gt;Multiverse Computing&lt;/a&gt;, a European startup working on LLMs. We‚Äôve released an &lt;strong&gt;uncensored version of Qwen3-Next-80B-Thinking&lt;/strong&gt; in which &lt;strong&gt;Chinese political censorship has been removed.&lt;/strong&gt; The model no longer refuses to answer for Chinese politically sensitive topics. Instead, it will provide &lt;strong&gt;balanced, objective answers&lt;/strong&gt; that present multiple relevant perspectives.&lt;/p&gt; &lt;p&gt;We believe that we made some significant improvement over previous approaches such as the uncensored version of DeepSeek R1 developed by Perplexity:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The behavior for non Chinese sensitive topics remains the same, this includes that the model scores the same in all the evaluation benchmarks we have performed.&lt;/li&gt; &lt;li&gt;We &lt;strong&gt;do not perform SFT&lt;/strong&gt; with hand-crafted data and we &lt;strong&gt;do not inject any new knowledge inside the model&lt;/strong&gt;. Our method is based on steering vectors to remove the capability of the model to refuse to answer China-related sensitive prompts. The model answers using &lt;strong&gt;the knowledge already inside the base model&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Many steering-vector approaches effectively &lt;em&gt;erase&lt;/em&gt; refusal behavior everywhere (making models broadly unsafe). Our approach &lt;strong&gt;only disables refusals only for Chinese sensitive topics&lt;/strong&gt;. (I know that many of you love fully uncensored models, but this was important for us).&lt;/li&gt; &lt;li&gt;Previous ‚Äúuncensored‚Äù models such as Perplexity R1 1767 can be jailbroken very easily by simply injecting a China-related phrase into harmful prompts (&lt;a href="https://weijiexu.com/posts/jailbreak_r1_1776.html"&gt;https://weijiexu.com/posts/jailbreak_r1_1776.html&lt;/a&gt;). Our model is designed to remain robust against the type of jailbreaks.&lt;/li&gt; &lt;li&gt;The model is a drop-in replace of the original Qwen-Next model. No architecture changes, no extra layers...&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The method&lt;/h1&gt; &lt;p&gt;This release is based on Refusal Steering, an inference-time technique using &lt;strong&gt;steering vectors&lt;/strong&gt; to control refusal behavior. We released a few days ago a paper describing our approach (although for this release, we updated the method so no extra weights are needed): &lt;a href="https://arxiv.org/abs/2512.16602"&gt;https://arxiv.org/abs/2512.16602&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Feedback&lt;/h1&gt; &lt;p&gt;We have evaluated the model to measure the refusal behavior for Chinese sensitive topics as well as harmful prompts. And we have also evaluated the model in popular benchmarks. The full evaluation details are available in the Model Card. But we are aware that there might be prompts we didn't thought about that are still censored, or cause an undesired behavior. So we would love to gather some feedback to continue improving the model.&lt;/p&gt; &lt;p&gt;In addition, we have open-source our evaluation library: &lt;a href="https://github.com/CompactifAI/LLM-Refusal-Evaluation"&gt;https://github.com/CompactifAI/LLM-Refusal-Evaluation&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;p&gt;Here is an example of the original model vs the uncensored model. (You might need to open the image to see it correctly). As you can see, the model‚Äôs answers are well-balanced and objective, presenting multiple perspectives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w1hpnillr09g1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=538697f68c700d090319d24ab5b13504cd773718"&gt;https://preview.redd.it/w1hpnillr09g1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=538697f68c700d090319d24ab5b13504cd773718&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Uncensored model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0a96qgtmr09g1.png?width=1655&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84b37d97d1e7309c7ca8c4c40e5902dab4d62bc7"&gt;https://preview.redd.it/0a96qgtmr09g1.png?width=1655&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84b37d97d1e7309c7ca8c4c40e5902dab4d62bc7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikergarcia1996"&gt; /u/ikergarcia1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T21:15:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pty4l1</id>
    <title>Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509</title>
    <updated>2025-12-23T16:24:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"&gt; &lt;img alt="Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509" src="https://b.thumbs.redditmedia.com/h4Zf9373IbplPJ8uOKIcc2EETzqgKctn8fs9-JElwKQ.jpg" title="Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2511&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs new in 2511: üë• Stronger multi-person consistency for group photos and complex scenes üß© Built-in popular community LoRAs ‚Äî no extra tuning required üí° Enhanced industrial &amp;amp; product design generation üîí Reduced image drift with dramatically improved character &amp;amp; identity consistency üìê Improved geometric reasoning, including construction lines and structural edits From identity-preserving portrait edits to high-fidelity multi-person fusion and practical engineering &amp;amp; design workflows, 2511 pushes image editing to the next level. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pty4l1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:24:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu1uq6</id>
    <title>Saw this on local marketplace, must be from a fellow r/LocalLLaMA here</title>
    <updated>2025-12-23T18:51:52+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"&gt; &lt;img alt="Saw this on local marketplace, must be from a fellow r/LocalLLaMA here" src="https://preview.redd.it/rd8mxp4l209g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc74b68c9ffc26cb488794a081d8077fa8ae663" title="Saw this on local marketplace, must be from a fellow r/LocalLLaMA here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rd8mxp4l209g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T18:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pudm4m</id>
    <title>I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf</title>
    <updated>2025-12-24T03:45:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"&gt; &lt;img alt="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf" src="https://preview.redd.it/iuaxwr9x529g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1349e4df960f26fc52d217c9f4f15fd3fc847cb5" title="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm on the Katanemo research team. Today we‚Äôre thrilled to launch &lt;strong&gt;Plano-Orchestrator&lt;/strong&gt;, a new family of LLMs built for fast multi-agent orchestration.&lt;/p&gt; &lt;p&gt;What do these new LLMs do? given a user request and the conversation context, Plano-Orchestrator decides which agent(s) should handle the request and in what sequence. In other words, it acts as the supervisor agent in a multi-agent system. Designed for multi-domain scenarios, it works well across general chat, coding tasks, and long, multi-turn conversations, while staying efficient enough for low-latency production deployments.&lt;/p&gt; &lt;p&gt;Why did we built this? Our applied research is focused on helping teams deliver agents safely and efficiently, with better real-world performance and latency ‚Äî the kind of ‚Äúglue work‚Äù that usually sits outside any single agent‚Äôs core product logic.&lt;/p&gt; &lt;p&gt;Plano-Orchestrator is integrated into Plano, our models-native proxy and dataplane for agents. Hope you enjoy it ‚Äî and we‚Äôd love feedback from anyone building multi-agent systems&lt;/p&gt; &lt;p&gt;Learn more about the LLMs &lt;a href="https://huggingface.co/collections/katanemo/plano-orchestrator"&gt;here&lt;/a&gt;&lt;br /&gt; About our open source project: &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;br /&gt; And about our research: &lt;a href="https://planoai.dev/research"&gt;https://planoai.dev/research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iuaxwr9x529g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T03:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu7pfi</id>
    <title>Thoughts on DGX Spark as a macOS Companion: Two Months Later</title>
    <updated>2025-12-23T22:58:04+00:00</updated>
    <author>
      <name>/u/PropellerheadViJ</name>
      <uri>https://old.reddit.com/user/PropellerheadViJ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/"&gt; &lt;img alt="Thoughts on DGX Spark as a macOS Companion: Two Months Later" src="https://b.thumbs.redditmedia.com/mYJkWm2ehaxO0TQODCEoxYfJ0zV1WJ5bZmkUE7Pp39M.jpg" title="Thoughts on DGX Spark as a macOS Companion: Two Months Later" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using the NVIDIA DGX Spark in tandem with my Mac for about two months now. Given the active discussions about its specs and price, I want to share my personal, subjective observations on who this device might be for and who it might not be.&lt;/p&gt; &lt;h2&gt;My Context: I Simply Don't Have CUDA on Mac&lt;/h2&gt; &lt;p&gt;I've been working on Apple Silicon since the release of the M1 and didn't plan on changing my main platform. It's a comfortable and stable environment for my daily work. The problem lies elsewhere: in ML and SOTA research, a significant portion of tools and libraries are still oriented towards CUDA. On macOS, following Apple's transition to M1+, this ecosystem simply doesn't exist.&lt;/p&gt; &lt;p&gt;Because of this, an entire layer of critical libraries like nvdiffrast, flash-attention, and other CUDA-dependent solutions is unavailable on Mac. In my case, the situation reached the point of absurdity: there was a real episode where Apple released a model, but it turned out to be designed for Linux, not for Apple Silicon (haha).&lt;/p&gt; &lt;p&gt;I didn't want to switch to another platform ‚Äî I'm already a Mac user and I wanted to stay in this environment. DGX Spark eventually became a compromise: a compact device with a Mac mini form factor, 128 GB of unified memory, and Blackwell architecture (sm121), which simply adds CUDA alongside the Mac, rather than replacing it.&lt;/p&gt; &lt;h2&gt;The Bandwidth Problem&lt;/h2&gt; &lt;p&gt;The most frequent criticism of Spark concerns its memory bandwidth ‚Äî only 273 GB/s. For comparison: the RTX 4090 has about 1000 GB/s, and the M4 Ultra has 819 GB/s. If your goal is the fastest possible inference and maximum tokens per second, Spark is indeed not the best tool. But local LLMs are what I used the least.&lt;/p&gt; &lt;p&gt;In my practice for R&amp;amp;D and experiments, you much more often hit the memory limit and software constraints rather than pure speed. Plus, there's a purely practical point: if this is your main Mac, you can almost never give all of its RAM to inference ‚Äî it's already occupied by IDEs, DCC tools, and the system. Spark allows you to offload AI computations to a separate device and not turn your main computer into a &amp;quot;brick&amp;quot; during calculations.&lt;/p&gt; &lt;p&gt;Modern models in 2025 are quickly outgrowing consumer hardware: * Hunyuan 3D 2.1 ‚Äî about 29 GB VRAM for full generation * FLUX.2 (BF16) ‚Äî the full model easily exceeds 80 GB * Trellis2 ‚Äî 24 GB as the minimum launch threshold&lt;/p&gt; &lt;p&gt;Quantization and distillation are viable options, but they require time and additional steps and experiments. It might work or it might not. Spark allows you to run such models &amp;quot;as is,&amp;quot; without unnecessary manipulations.&lt;/p&gt; &lt;h2&gt;My Workflow: Mac + Spark&lt;/h2&gt; &lt;p&gt;In my setup, a Mac on M4 Max with 64 GB RAM handles the main tasks: Unity, Houdini, Blender, IDE. But AI tasks now fly over to Spark (right now I'm generating a fun background in Comfy for a call with colleagues).&lt;/p&gt; &lt;p&gt;I simply connect to Spark via SSH through JetBrains Gateway and work on it as a remote machine: the code, environment, and runs live there, while the Mac remains a responsive work tool. For me, this is a convenient and clear separation: Mac is the workplace, Spark is the compute node.&lt;/p&gt; &lt;h2&gt;What About Performance&lt;/h2&gt; &lt;p&gt;Below are my practical measurements in tasks typical for me, compared to an RTX 4090 on RunPod.&lt;/p&gt; &lt;p&gt;I separate the measurements into &lt;strong&gt;Cold Start&lt;/strong&gt; (first run) and &lt;strong&gt;Hot Start&lt;/strong&gt; (model already loaded).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;DGX Spark (Cold)&lt;/th&gt; &lt;th&gt;DGX Spark (Hot)&lt;/th&gt; &lt;th&gt;RTX 4090 (Cold)&lt;/th&gt; &lt;th&gt;RTX 4090 (Hot)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Z Image Turbo&lt;/td&gt; &lt;td&gt;~46.0s&lt;/td&gt; &lt;td&gt;~6.0s&lt;/td&gt; &lt;td&gt;~26.3s&lt;/td&gt; &lt;td&gt;~2.6s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen Image Edit (4 steps)&lt;/td&gt; &lt;td&gt;~80.8s&lt;/td&gt; &lt;td&gt;~18.0s&lt;/td&gt; &lt;td&gt;~72.5s&lt;/td&gt; &lt;td&gt;~8.5s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen Image Edit (20 steps)&lt;/td&gt; &lt;td&gt;~223.7s&lt;/td&gt; &lt;td&gt;~172.0s&lt;/td&gt; &lt;td&gt;~104.8s&lt;/td&gt; &lt;td&gt;~57.8s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Flux 2 GGUF Q8-0&lt;/td&gt; &lt;td&gt;~580.0s&lt;/td&gt; &lt;td&gt;~265.0s&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hunyuan3D 2.1&lt;/td&gt; &lt;td&gt;~204.4s&lt;/td&gt; &lt;td&gt;~185.0s&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Nuances of &amp;quot;Early&amp;quot; Hardware&lt;/h2&gt; &lt;p&gt;It's important to understand that Spark is a Blackwell Development Kit, not a &amp;quot;plug and play&amp;quot; consumer solution. * Architecture: aarch64 + sm121 combo. Much has to be built manually. Recently, for example, I was building a Docker image for Hunyuan and spent about 8 hours resolving dependency hell because some dependencies for the ARM processor were simply missing. * Software Support: you often have to manually set compatibility flags, as many frameworks haven't updated for Blackwell yet.&lt;/p&gt; &lt;h2&gt;Who Am I and Why Do I Need This&lt;/h2&gt; &lt;p&gt;I am a Unity developer. By profession ‚Äî gamedev, in my free time ‚Äî an enthusiast who actively uses inference. I'm most interested in 3D: generating models, textures, and experimenting with various pipelines.&lt;/p&gt; &lt;h2&gt;Conclusion (My IMHO)&lt;/h2&gt; &lt;p&gt;DGX Spark occupies a very narrow and specific niche. And I sincerely don't understand why it was advertised as a &amp;quot;supercomputer.&amp;quot; It seems the word &amp;quot;super&amp;quot; has become a bit devalued: every couple of weeks, new neural networks come out, and from every account, you hear how something &amp;quot;super&amp;quot; has happened.&lt;/p&gt; &lt;p&gt;In my experience, Spark is much more honestly perceived as a compact CUDA node or a Blackwell dev-kit next to your main computer. If it is &amp;quot;super,&amp;quot; then perhaps only a super-mini-computer ‚Äî without claiming any speed records.&lt;/p&gt; &lt;p&gt;It is an EXPENSIVE compromise where you sacrifice speed for memory volume and access to the CUDA ecosystem. For my tasks in gamedev and R&amp;amp;D, it has become a convenient and reliable &amp;quot;NVIDIA trailer&amp;quot; to my main Mac. After 2 months, I have already built several Docker images, filled almost a terabyte with SOTA models, and for now, I am in the &amp;quot;playing with a new toy&amp;quot; stage. But I am satisfied.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PropellerheadViJ"&gt; /u/PropellerheadViJ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pu7pfi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T22:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
