<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-18T00:35:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qeuh0z</id>
    <title>Prompt Repetition Improves Non-Reasoning LLMs - a paper</title>
    <updated>2026-01-16T22:35:01+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2512.14982"&gt;https://arxiv.org/pdf/2512.14982&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I love these little tiny prompt techniques that can potentially lead to greater model accuracy and performance. Simply repeating the prompt twice lead to notable performance gains.&lt;/p&gt; &lt;p&gt;From the paper:&lt;/p&gt; &lt;p&gt;&amp;quot;We show that repeating the prompts consistently improves model performance for a range of models and benchmarks, when not using reasoning. In addition, latency is not impacted, as only the parallelizable pre-fill stage is affected. Prompt repetition does not change the lengths or formats of the generated outputs, and it might be a good default for many models and tasks, when reasoning is not used.&lt;/p&gt; &lt;p&gt;So simple but they demonstrate impressive gains on several benchmark scores. Looks like Deepseek is the only open weights model put through the wringer.&lt;/p&gt; &lt;p&gt;Best of wishes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qftdr4</id>
    <title>AI insiders seek to poison the data that feeds them</title>
    <updated>2026-01-18T00:14:54+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"&gt; &lt;img alt="AI insiders seek to poison the data that feeds them" src="https://external-preview.redd.it/oZsMR98JWtXvCHBS_WeIPqnrR9GLtUCvLvVVcRNn-mI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=176b3305413cf55f808df5dcf6836f7c0b89e27e" title="AI insiders seek to poison the data that feeds them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theregister.com/2026/01/11/industry_insiders_seek_to_poison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T00:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfcg4h</id>
    <title>Need to know more about less known engines (ik_llama.cpp, exllamav3..)</title>
    <updated>2026-01-17T12:51:26+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually stick to llama.cpp and vllm but llama.cpp speed may not be the best and vllm/sglang can be really annoying if you have several gpus without respecting the power of 2 for tp.&lt;/p&gt; &lt;p&gt;So, for people who really know others projects (I mainly know ik_llama and exl3) could you please provide some feedback on where they really shine and what are their main constraints and limits (model/hardware support, tool calling, stability‚Ä¶).&lt;/p&gt; &lt;p&gt;Testing / understanding stuff may take some time so any usefull info is good to have, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfquj8</id>
    <title>Linux distros (strix halo, llama.cpp, media server)</title>
    <updated>2026-01-17T22:27:31+00:00</updated>
    <author>
      <name>/u/a-wiseman-speaketh</name>
      <uri>https://old.reddit.com/user/a-wiseman-speaketh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to test out my strix halo as an LLM/SLM server + mini media server. I don't have a ton of media, so I'm hoping it will work well for us, but we'll see. I'd also like to run it headless, so RDP support or similar would be nice.&lt;/p&gt; &lt;p&gt;Right now I have Fedora 43 installed but I was considering workstation for the RDP support. Or maybe I'm running down the wrong path and another distro would work better. LLM support is top priority really, I'd rather work around everything else that I'm more familiar with and isn't in constant flux.&lt;/p&gt; &lt;p&gt;Anything anyone's really happy with? Fedora 43 worked out of box for stuff that used to be a real pain (it's been 20+ years since I built a Linux box) but I haven't tried setting up everything yet &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a-wiseman-speaketh"&gt; /u/a-wiseman-speaketh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfquj8/linux_distros_strix_halo_llamacpp_media_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfquj8/linux_distros_strix_halo_llamacpp_media_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfquj8/linux_distros_strix_halo_llamacpp_media_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T22:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfbt9f</id>
    <title>Local Replacement for Phind.com</title>
    <updated>2026-01-17T12:19:01+00:00</updated>
    <author>
      <name>/u/Past-Economist7732</name>
      <uri>https://old.reddit.com/user/Past-Economist7732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As many are aware, &lt;a href="https://www.phind.com/"&gt;https://www.phind.com/&lt;/a&gt; has shut down. I don‚Äôt know how many people on here used it, but I used to love the service back when it was an ai search engine, you could prompt the ai and it would search the internet for relevant info, and ONLY THEN respond. (Don‚Äôt get me started on the final iteration of phind, the atrocious ‚ÄúI‚Äôm going to build you a website to answer your question‚Äù, that was not useful to me). &lt;/p&gt; &lt;p&gt;Is there any way to recreate ai search behavior with local models? Maybe with openwebui somehow? There are some agentic workflows that can kick out to do a web search but sometimes I want to begin with the search and see the results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Past-Economist7732"&gt; /u/Past-Economist7732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfsxbd</id>
    <title>syntux - the generative UI library for the web!</title>
    <updated>2026-01-17T23:55:17+00:00</updated>
    <author>
      <name>/u/Possible-Session9849</name>
      <uri>https://old.reddit.com/user/Possible-Session9849</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsxbd/syntux_the_generative_ui_library_for_the_web/"&gt; &lt;img alt="syntux - the generative UI library for the web!" src="https://external-preview.redd.it/4qVo0EVwZYEJGQDfBCp-Hw-UL5s4yTmCT_y49L4o6UE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6add6e212210c38aa118098c32afa7b4b3a5e585" title="syntux - the generative UI library for the web!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible-Session9849"&gt; /u/Possible-Session9849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/puffinsoft/syntux"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsxbd/syntux_the_generative_ui_library_for_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsxbd/syntux_the_generative_ui_library_for_the_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfrvxw</id>
    <title>DetLLM ‚Äì Deterministic Inference Checks</title>
    <updated>2026-01-17T23:10:50+00:00</updated>
    <author>
      <name>/u/Cerru905</name>
      <uri>https://old.reddit.com/user/Cerru905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept getting annoyed by LLM inference non-reproducibility, and one thing that really surprised me is that changing batch size can change outputs even under ‚Äúdeterministic‚Äù settings.&lt;/p&gt; &lt;p&gt;So I built DetLLM: it measures and proves repeatability using token-level traces + a first-divergence diff, and writes a minimal repro pack for every run (env snapshot, run config, applied controls, traces, report).&lt;/p&gt; &lt;p&gt;I prototyped this version today in a few hours with Codex. The hardest part was the HLD I did a few days ago, but I was honestly surprised by how well Codex handled the implementation. I didn‚Äôt expect it to come together in under a day.&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/tommasocerruti/detllm"&gt;https://github.com/tommasocerruti/detllm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, and if you find any prompts/models/setups that still make it diverge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cerru905"&gt; /u/Cerru905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfrvxw/detllm_deterministic_inference_checks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfrvxw/detllm_deterministic_inference_checks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfrvxw/detllm_deterministic_inference_checks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qft49b</id>
    <title>Benchmarks measuring time to resolve? SWE like benchmark with headers like | TIME to Resolve | Resolve Rate % | Cost $ ?</title>
    <updated>2026-01-18T00:03:35+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;do you know any benchmarks that not only measure %, $ but also time?&lt;br /&gt; I have a feeling that we will soon approach quality so high that only time and $ will be worth measuring. Curious if there is any team that actually checks that currently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qft49b/benchmarks_measuring_time_to_resolve_swe_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qft49b/benchmarks_measuring_time_to_resolve_swe_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qft49b/benchmarks_measuring_time_to_resolve_swe_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T00:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf514i</id>
    <title>"Welcome to the Local Llama. How janky's your rig?</title>
    <updated>2026-01-17T05:44:01+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt; &lt;img alt="&amp;quot;Welcome to the Local Llama. How janky's your rig?" src="https://preview.redd.it/rzbni3vvkudg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d48b6efbc81ebde1857313c676df8a19d5193dbc" title="&amp;quot;Welcome to the Local Llama. How janky's your rig?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzbni3vvkudg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T05:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfm8vn</id>
    <title>Why are all quants almost the same size?</title>
    <updated>2026-01-17T19:22:02+00:00</updated>
    <author>
      <name>/u/tecneeq</name>
      <uri>https://old.reddit.com/user/tecneeq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are all quants almost the same size?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tecneeq"&gt; /u/tecneeq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfm8vn/why_are_all_quants_almost_the_same_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfm8vn/why_are_all_quants_almost_the_same_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfm8vn/why_are_all_quants_almost_the_same_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T19:22:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfsju5</id>
    <title>Personal-Guru: an open-source, free, local-first alternative to AI tutors and NotebookLM</title>
    <updated>2026-01-17T23:38:58+00:00</updated>
    <author>
      <name>/u/rishabhbajpai24</name>
      <uri>https://old.reddit.com/user/rishabhbajpai24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs make incredible encyclopedias‚Äîbut honestly, pretty terrible teachers.&lt;/p&gt; &lt;p&gt;You can chat with ChatGPT for an hour about a complex topic, but without a syllabus or clear milestones, you usually end up with a long chat history and very little retained knowledge.&lt;/p&gt; &lt;p&gt;Most existing tools fall into one of these buckets:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unstructured chatbots&lt;/li&gt; &lt;li&gt;Document analyzers (you need to already have notes)&lt;/li&gt; &lt;li&gt;Expensive subscription-based platforms&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We just released the &lt;strong&gt;beta of Personal-Guru&lt;/strong&gt;, a &lt;strong&gt;local-first, open-source learning system&lt;/strong&gt; that doesn‚Äôt just ‚Äúchat‚Äù ‚Äî it &lt;strong&gt;builds a full curriculum for you from scratch&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Our core belief is simple:&lt;br /&gt; &lt;strong&gt;Education and access to advanced AI should be free, private, and offline-capable.&lt;/strong&gt;&lt;br /&gt; No subscriptions. No cloud lock-in. No data leaving your machine.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;Repo:&lt;/strong&gt;&lt;a href="https://github.com/Rishabh-Bajpai/Personal-Guru"&gt; https://github.com/Rishabh-Bajpai/Personal-Guru&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üöÄ What makes Personal-Guru different?&lt;/h1&gt; &lt;p&gt;Instead of free-form chat, you give it a &lt;strong&gt;topic&lt;/strong&gt; (e.g., &lt;em&gt;Quantum Physics&lt;/em&gt; or &lt;em&gt;Sourdough Baking&lt;/em&gt;) and it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üìö Generates a &lt;strong&gt;structured syllabus&lt;/strong&gt; (chapters, sections, key concepts)&lt;/li&gt; &lt;li&gt;üß† Creates &lt;strong&gt;interactive learning content&lt;/strong&gt; (quizzes, flashcards, voice Q&amp;amp;A)&lt;/li&gt; &lt;li&gt;üîí Runs &lt;strong&gt;100% locally&lt;/strong&gt; (powered by Ollama ‚Äî your data stays with you)&lt;/li&gt; &lt;li&gt;üéß Supports &lt;strong&gt;multi-modal learning&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reel Mode&lt;/strong&gt; (short-form, TikTok-style learning)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Podcast Mode&lt;/strong&gt; (audio-first learning)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;‚öîÔ∏è Why Personal-Guru? (Quick comparison)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;ü¶â Personal-Guru&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;üìì NotebookLM&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;‚ú® Gemini Guided Learning&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;üéì&lt;/strong&gt; &lt;a href="http://ai-tutor.ai"&gt;&lt;strong&gt;ai-tutor.ai&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Core Philosophy&lt;/td&gt; &lt;td align="left"&gt;Structured Curriculum Generator&lt;/td&gt; &lt;td align="left"&gt;Document Analyzer (RAG)&lt;/td&gt; &lt;td align="left"&gt;Conversational Study Partner&lt;/td&gt; &lt;td align="left"&gt;Course Generator&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Privacy&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100% Local&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Cloud (Google)&lt;/td&gt; &lt;td align="left"&gt;Cloud (Google)&lt;/td&gt; &lt;td align="left"&gt;Cloud (Proprietary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Cost&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Free &amp;amp; Open Source&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Free (for now)&lt;/td&gt; &lt;td align="left"&gt;$20/mo&lt;/td&gt; &lt;td align="left"&gt;Freemium (~$10+/mo)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Input Needed&lt;/td&gt; &lt;td align="left"&gt;Just a topic&lt;/td&gt; &lt;td align="left"&gt;Your documents&lt;/td&gt; &lt;td align="left"&gt;Chat prompts&lt;/td&gt; &lt;td align="left"&gt;Topic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Audio Features&lt;/td&gt; &lt;td align="left"&gt;Local podcast + TTS&lt;/td&gt; &lt;td align="left"&gt;Audio overviews&lt;/td&gt; &lt;td align="left"&gt;Standard TTS&lt;/td&gt; &lt;td align="left"&gt;Limited&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Offline&lt;/td&gt; &lt;td align="left"&gt;‚úÖ Yes&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;‚ÄúReel‚Äù Mode&lt;/td&gt; &lt;td align="left"&gt;‚úÖ Yes&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;td align="left"&gt;‚ùå No&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üõ†Ô∏è Tech Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Flask + multi-agent system&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Engine:&lt;/strong&gt; Ollama (Llama 3, Mistral, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio:&lt;/strong&gt; Speaches (Kokoro-82M) for high-quality local TTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; Responsive web UI with voice input&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;ü§ù Call for Contributors&lt;/h1&gt; &lt;p&gt;This is an &lt;strong&gt;early beta&lt;/strong&gt;, and we have big plans.&lt;/p&gt; &lt;p&gt;If you believe that &lt;strong&gt;AI-powered education should be free, open, and private&lt;/strong&gt;, we‚Äôd love your help. We‚Äôre especially looking for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Developers interested in &lt;strong&gt;local AI / agent systems&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Contributors passionate about &lt;strong&gt;EdTech&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Feedback on &lt;strong&gt;structured learning flows vs. chat-based learning&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and let us know what you think:&lt;br /&gt; üëâ&lt;a href="https://github.com/Rishabh-Bajpai/Personal-Guru"&gt; https://github.com/Rishabh-Bajpai/Personal-Guru&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rishabhbajpai24"&gt; /u/rishabhbajpai24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsju5/personalguru_an_opensource_free_localfirst/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsju5/personalguru_an_opensource_free_localfirst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfsju5/personalguru_an_opensource_free_localfirst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfaxpx</id>
    <title>Analysis of running local LLMs on Blackwell GPUs. TLDR: cheaper to run than cloud api services</title>
    <updated>2026-01-17T11:30:51+00:00</updated>
    <author>
      <name>/u/cchung261</name>
      <uri>https://old.reddit.com/user/cchung261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;May provide support to management for the cost savings of running local LLMs. The paper also includes amortization costs for the GPUs. I was surprised by the findings and the short break even time with cloud api costs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2601.09527"&gt;https://arxiv.org/abs/2601.09527&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cchung261"&gt; /u/cchung261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfojft</id>
    <title>Are any small or medium-sized businesses here actually using AI in a meaningful way?</title>
    <updated>2026-01-17T20:53:45+00:00</updated>
    <author>
      <name>/u/brentmeistergeneral_</name>
      <uri>https://old.reddit.com/user/brentmeistergeneral_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to figure out how to apply AI at work beyond the obvious stuff. Looking for real examples where it‚Äôs improved efficiency, reduced workload, or added value.&lt;/p&gt; &lt;p&gt;I work at a design and production house and I am seeing AI starting to get used for example client design renders to staff generally using co pilot chatgpt and Gemini etc.&lt;/p&gt; &lt;p&gt;Just wondering if you guys can tell me other ways I can use AI that could help small companies that aren't really mainstream yet? Whether it's for day to day admin, improving operational efficiencies etc. &lt;/p&gt; &lt;p&gt;Thanks guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brentmeistergeneral_"&gt; /u/brentmeistergeneral_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfojft/are_any_small_or_mediumsized_businesses_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfojft/are_any_small_or_mediumsized_businesses_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfojft/are_any_small_or_mediumsized_businesses_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T20:53:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfk2ky</id>
    <title>Optimizing GPT-OSS 120B on Strix Halo 128GB?</title>
    <updated>2026-01-17T18:00:26+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per the title, I want to optimize running GPT-OSS 120B on a strix halo box with 128GB RAM. I've seen plenty of posts over time about optimizations and tweaks people have used (eg. particular drivers, particular memory mappings, etc). I'm searching around &lt;a href="/r/localllama"&gt;/r/localllama&lt;/a&gt;, but figured I would also post and ask directly for your tips and tricks. Planning on running Ubuntu 24.04 LTS. &lt;/p&gt; &lt;p&gt;Very much appreciate any of your hard-earned tips and tricks!&lt;/p&gt; &lt;p&gt;Edit: some more info: &lt;/p&gt; &lt;p&gt;Planning on running Ubuntu 24.04 LTS and llama.cpp + vulkan (or rocm if it is faster for inference, but that has not been my experience previously). I currently run the UD 2.0 FP16 quant (unsloth/gpt-oss-120b-GGUF/gpt-oss-120b-F16.gguf) on an AMD 7040U series apu with 128GB DDR5 RAM, with 96GB dedicated GTT, and get ~13tps with that setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; Much gold advice, many thanks! I'm reminded by a few responses: I'm also interested in serving llama.cpp server on my local LAN to other machines on the network, what do I need to keep in mind to not foot-gun here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qff481</id>
    <title>I built Adaptive-K routing: 30-52% compute savings on MoE models (Mixtral, Qwen, OLMoE)</title>
    <updated>2026-01-17T14:50:29+00:00</updated>
    <author>
      <name>/u/Fuzzy_Ad_1390</name>
      <uri>https://old.reddit.com/user/Fuzzy_Ad_1390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Gabrobals/sbm-efficient"&gt;https://github.com/Gabrobals/sbm-efficient&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Whitepaper: &lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;https://adaptive-k.vercel.app/whitepaper.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TensorRT-LLM PR: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/pull/10672"&gt;https://github.com/NVIDIA/TensorRT-LLM/pull/10672&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live demo: &lt;a href="https://huggingface.co/spaces/Gabrobals/adaptive-k-demo"&gt;https://huggingface.co/spaces/Gabrobals/adaptive-k-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or discuss implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fuzzy_Ad_1390"&gt; /u/Fuzzy_Ad_1390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T14:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfogkp</id>
    <title>Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?</title>
    <updated>2026-01-17T20:50:29+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"&gt; &lt;img alt="Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?" src="https://external-preview.redd.it/MHgzNjM0N2QyemRnMQsZsHM-tqgfg0XXBK6smBja3B3Y-8kZyS2BD6gyOUFy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=731f35ba5df810d40f5158dcba9a22ffad9f0bb8" title="Prototype: What if local LLMs used Speed Reading Logic to avoid ‚Äúwall of text‚Äù overload?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prototyped this in a few minutes. Seems incredibly useful for smaller devices (mobile LLMs)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ad16dbhd2zdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfogkp/prototype_what_if_local_llms_used_speed_reading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T20:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfgiq1</id>
    <title>MCP server that gives local LLMs memory, file access, and a 'conscience' - 100% offline on Apple Silicon</title>
    <updated>2026-01-17T15:45:41+00:00</updated>
    <author>
      <name>/u/TheTempleofTwo</name>
      <uri>https://old.reddit.com/user/TheTempleofTwo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a few weeks and finally got it stable enough to share.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem I wanted to solve:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local LLMs are stateless - they forget everything between sessions&lt;/li&gt; &lt;li&gt;No governance - they'll execute whatever you ask without reflection&lt;/li&gt; &lt;li&gt;Chat interfaces don't give them &amp;quot;hands&amp;quot; to actually do things&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A stack that runs entirely on my Mac Studio M2 Ultra:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LM Studio (chat interface) ‚Üì Hermes-3-Llama-3.1-8B (MLX, 4-bit) ‚Üì Temple Bridge (MCP server) ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ BTB ‚îÇ Threshold ‚îÇ ‚îÇ (filesystem ‚îÇ (governance ‚îÇ ‚îÇ operations) ‚îÇ protocols) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What the AI can actually do:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read/write files in a sandboxed directory&lt;/li&gt; &lt;li&gt;Execute commands (pytest, git, ls, etc.) with an allowlist&lt;/li&gt; &lt;li&gt;Consult &amp;quot;threshold protocols&amp;quot; before taking actions&lt;/li&gt; &lt;li&gt;Log its entire cognitive journey to a JSONL file&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ask for my approval before executing anything dangerous&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The key insight:&lt;/strong&gt; The filesystem itself becomes the AI's memory. Directory structure = classification. File routing = inference. No vector database needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Hermes-3?&lt;/strong&gt; Tested a bunch of models for MCP tool calling. Hermes-3-Llama-3.1-8B was the most stable - no infinite loops, reliable structured output, actually follows the tool schema.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The governance piece:&lt;/strong&gt; Before execution, the AI consults governance protocols and reflects on what it's about to do. When it wants to run a command, I get an approval popup in LM Studio. I'm the &amp;quot;threshold witness&amp;quot; - nothing executes without my explicit OK.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-time monitoring:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tail -f spiral_journey.jsonl | jq &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Shows every tool call, what phase of reasoning the AI is in, timestamps, the whole cognitive trace.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; On M2 Ultra with 36GB unified memory, responses are fast. The MCP overhead is negligible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repos (all MIT licensed):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/temple-bridge"&gt;temple-bridge&lt;/a&gt; - The MCP server that binds it together&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;back-to-the-basics&lt;/a&gt; - Filesystem-as-circuit paradigm&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/threshold-protocols"&gt;threshold-protocols&lt;/a&gt; - Governance framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the three repos&lt;/li&gt; &lt;li&gt;&lt;code&gt;uv sync&lt;/code&gt; in temple-bridge&lt;/li&gt; &lt;li&gt;Add the MCP config to &lt;code&gt;~/.lmstudio/mcp.json&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Load Hermes-3 in LM Studio&lt;/li&gt; &lt;li&gt;Paste the system prompt&lt;/li&gt; &lt;li&gt;Done&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full instructions in the README.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next:&lt;/strong&gt; Working on &amp;quot;governed derive&amp;quot; - the AI can propose filesystem reorganizations based on usage patterns, but only executes after human approval. The goal is AI that can self-organize but with structural restraint built in.&lt;/p&gt; &lt;p&gt;Happy to answer questions. This was a multi-week collaboration between me and several AI systems (Claude, Gemini, Grok) - they helped architect it, I implemented and tested. The lineage is documented in &lt;a href="http://ARCHITECTS.md"&gt;ARCHITECTS.md&lt;/a&gt; if anyone's curious about the process.&lt;/p&gt; &lt;p&gt;- Temple Bridge: &lt;a href="https://github.com/templetwo/temple-bridge"&gt;https://github.com/templetwo/temple-bridge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Back to the Basics: &lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;https://github.com/templetwo/back-to-the-basics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Threshold Protocols: &lt;a href="https://github.com/templetwo/threshold-protocols"&gt;https://github.com/templetwo/threshold-protocols&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üåÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheTempleofTwo"&gt; /u/TheTempleofTwo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T15:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfb0gk</id>
    <title>KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop</title>
    <updated>2026-01-17T11:35:11+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, it's been a hot minute, but I thought I'd share this here since it's quite a big new feature. &lt;/p&gt; &lt;p&gt;Yes, KoboldCpp is still alive and kicking. And besides the major UI overhaul, we've finally added native MCP support in KoboldCpp v1.106! It's designed to be a painless Claude Desktop drop-in replacement with maximum compatibility, the &lt;code&gt;mcp.json&lt;/code&gt; uses the same format so you can swap it in easily. &lt;/p&gt; &lt;p&gt;The KoboldCpp MCP bridge will connect to all provided MCP servers (HTTP and STDIO transports both supported) and automatically forward requests for tools the AI selects to the correct MCP server. This MCP bridge can also be used by third party clients.&lt;/p&gt; &lt;p&gt;On the frontend side, you can fetch the list of all tools from all servers, select the tools you want to let AI use, and optionally enable tool call approvals.&lt;/p&gt; &lt;p&gt;Some demo screenshots of various tool servers being used: &lt;a href="https://imgur.com/a/fKeWKUU"&gt;https://imgur.com/a/fKeWKUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;feedback is welcome. cheers! - concedo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf5oj0</id>
    <title>DeepSeek Engram : A static memory unit for LLMs</title>
    <updated>2026-01-17T06:18:14+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeeepSeek AI released a new paper titled &amp;quot;Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models&amp;quot; introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram &lt;strong&gt;adds native memory lookup&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think of it as separating &lt;strong&gt;remembering from reasoning&lt;/strong&gt;. Traditional MoE focuses on conditional computation, Engram introduces &lt;strong&gt;conditional memory&lt;/strong&gt;. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge is &lt;strong&gt;looked up in O(1)&lt;/strong&gt; instead of recomputed.&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;explicit parametric memory&lt;/strong&gt; vs implicit weights only.&lt;/li&gt; &lt;li&gt;Improves reasoning, math, and code performance.&lt;/li&gt; &lt;li&gt;Enables massive memory scaling &lt;strong&gt;without GPU limits&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Frees attention for &lt;strong&gt;global reasoning&lt;/strong&gt; rather than static knowledge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper : &lt;a href="https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf"&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub"&gt;https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T06:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfpomi</id>
    <title>[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU</title>
    <updated>2026-01-17T21:39:36+00:00</updated>
    <author>
      <name>/u/ThisGonBHard</name>
      <uri>https://old.reddit.com/user/ThisGonBHard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt; &lt;img alt="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" src="https://external-preview.redd.it/BkpkFFoxQzTdVFBwygr_NjC6jb0CW1UxI49hdIPceBg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2ae4d7ac52eeeba792fbdb62506b06e57290b67" title="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems quite interesting, in getting the 48 GB cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThisGonBHard"&gt; /u/ThisGonBHard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/TcRGBeOENLg?si=2CKaZR7Dj0x89MMU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T21:39:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfmc05</id>
    <title>China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)</title>
    <updated>2026-01-17T19:25:24+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt; &lt;img alt="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" src="https://external-preview.redd.it/TpKYg79IWzebupDqkzAodJruBP4N0VFsDaZESasEpKQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea8968e021234c9b599b354059d32de716c52bed" title="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone else posted about this, but never posted a transcript, so I found one online.&lt;/p&gt; &lt;p&gt;Lot of interesting stuff about China vs US, paths to AGI, compute, marketing etc.&lt;/p&gt; &lt;p&gt;Unfortunately Moonshot seems to have a very short section. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/the-all-star-chinese-ai-conversation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T19:25:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfq9ez</id>
    <title>The Search for Uncensored AI (That Isn‚Äôt Adult-Oriented)</title>
    <updated>2026-01-17T22:03:23+00:00</updated>
    <author>
      <name>/u/Fun-Situation-4358</name>
      <uri>https://old.reddit.com/user/Fun-Situation-4358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been trying to find an AI that‚Äôs genuinely unfiltered &lt;em&gt;and&lt;/em&gt; technically advanced, uncensored something that can reason freely without guardrails killing every interesting response.&lt;/p&gt; &lt;p&gt;Instead, almost everything I run into is marketed as ‚Äúuncensored,‚Äù but it turns out to be optimized for low-effort adult use rather than actual intelligence or depth.&lt;/p&gt; &lt;p&gt;It feels like the space between heavily restricted corporate AI and shallow adult-focused models is strangely empty, and I‚Äôm curious why that gap still exists...&lt;/p&gt; &lt;p&gt;Is there any &lt;strong&gt;uncensored or lightly filtered AI&lt;/strong&gt; that focuses on reasoning, creativity,uncensored technology or serious problem-solving instead? I‚Äôm open to self-hosted models, open-source projects, or lesser-known platforms. Suggestions appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Situation-4358"&gt; /u/Fun-Situation-4358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T22:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfkn3a</id>
    <title>Best "End of world" model that will run on 24gb VRAM</title>
    <updated>2026-01-17T18:21:20+00:00</updated>
    <author>
      <name>/u/gggghhhhiiiijklmnop</name>
      <uri>https://old.reddit.com/user/gggghhhhiiiijklmnop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps, I'm feeling in a bit of a omg the world is ending mood and have been amusing myself by downloading and hoarding a bunch of data - think wikipedia, wiktionary, wikiversity, khan academy, etc etc&lt;/p&gt; &lt;p&gt;What's your take on the smartest / best model(s) to download and store - they need to fit and run on my 24gb VRAM / 64gb RAM PC.? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gggghhhhiiiijklmnop"&gt; /u/gggghhhhiiiijklmnop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfscp5</id>
    <title>128GB VRAM quad R9700 server</title>
    <updated>2026-01-17T23:30:26+00:00</updated>
    <author>
      <name>/u/Ulterior-Motive_</name>
      <uri>https://old.reddit.com/user/Ulterior-Motive_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt; &lt;img alt="128GB VRAM quad R9700 server" src="https://b.thumbs.redditmedia.com/SbBMg1b6qTh913lUa8uWDuyYZrIwJ_ECUuUVvuWh_qA.jpg" title="128GB VRAM quad R9700 server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a sequel to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fqwrvg/64gb_vram_dual_mi100_server/"&gt;previous thread&lt;/a&gt; from 2024.&lt;/p&gt; &lt;p&gt;I originally planned to pick up another pair of MI100s and an Infinity Fabric Bridge, and I picked up a lot of hardware upgrades over the course of 2025 in preparation for this. Notably, faster, double capacity memory (last February, well before the current price jump), another motherboard, higher capacity PSU, etc. But then I saw benchmarks for the R9700, particularly in the &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15021"&gt;llama.cpp ROCm thread&lt;/a&gt;, and saw the much better prompt processing performance for a small token generation loss. The MI100 also went up in price to about $1000, so factoring in the cost of a bridge, it'd come to about the same price. So I sold the MI100s, picked up 4 R9700s and called it a day.&lt;/p&gt; &lt;p&gt;Here's the specs and BOM. Note that the CPU and SSD were taken from the previous build, and the internal fans came bundled with the PSU as part of a deal:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Number&lt;/th&gt; &lt;th align="left"&gt;Unit Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7 5700X&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$160.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;Corsair Vengance LPX 64GB (2 x 32GB) DDR4 3600MHz C18&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;$105.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;PowerColor AMD Radeon AI PRO R9700 32GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;$1,300.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Motherboard&lt;/td&gt; &lt;td align="left"&gt;MSI MEG X570 GODLIKE Motherboard&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$490.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Storage&lt;/td&gt; &lt;td align="left"&gt;Inland Performance 1TB NVMe SSD&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PSU&lt;/td&gt; &lt;td align="left"&gt;Super Flower Leadex Titanium 1600W 80+ Titanium&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$440.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Internal Fans&lt;/td&gt; &lt;td align="left"&gt;Super Flower MEGACOOL 120mm fan, Triple-Pack&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case Fans&lt;/td&gt; &lt;td align="left"&gt;Noctua NF-A14 iPPC-3000 PWM&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;$30.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU Heatsink&lt;/td&gt; &lt;td align="left"&gt;AMD Wraith Prism aRGB CPU Cooler&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$20.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Fan Hub&lt;/td&gt; &lt;td align="left"&gt;Noctua NA-FH1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$45.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case&lt;/td&gt; &lt;td align="left"&gt;Phanteks Enthoo Pro 2 Server Edition&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$190.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;$7,035.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;128GB VRAM, 128GB RAM for offloading, all for less than the price of a RTX 6000 Blackwell.&lt;/p&gt; &lt;p&gt;Some benchmarks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;n_ubatch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;6524.91 ¬± 11.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;90.89 ¬± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2113.82 ¬± 2.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;72.51 ¬± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1725.46 ¬± 5.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.75 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1110.02 ¬± 3.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.53 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;821.10 ¬± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;38.88 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1928.45 ¬± 3.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.09 ¬± 0.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2082.04 ¬± 4.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.78 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;42.62 ¬± 7.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.58 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;A few final observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;glm4 moe and minimax-m2 are actually GLM-4.6V and MiniMax-M2.1, respectively.&lt;/li&gt; &lt;li&gt;There's an open issue for Qwen3-Next at the moment; recent optimizations caused some pretty hefty prompt processing regressions. The numbers here are pre #18683, in case the exact issue gets resolved.&lt;/li&gt; &lt;li&gt;A word on the Q8 quant of MiniMax-M2.1; &lt;code&gt;--fit on&lt;/code&gt; isn't supported on llama-bench, so I can't give an apples to apples comparison to simply reducing the number of gpu layers, but it's also extremely unreliable for me in llama-server, giving me HIP error 906 on the first generation. Out of a dozen or so attempts, I've gotten it to work once, with a TG around 8.5 t/s, but take that with a grain of salt. Otherwise, maybe the quality jump is worth letting it run overnight? You be the judge. It also takes 2 hours to load, but that could be because I'm loading it off external storage.&lt;/li&gt; &lt;li&gt;The internal fan mount on the case only has screws on one side; in the intended configuration, the holes for power cables are on the opposite side of where the GPU power sockets are, meaning the power cables will block airflow from the fans. How they didn't see this, I have no idea. Thankfully, it stays in place from a friction fit if you flip it 180 like I did. Really, I probably could have gone without it, it was mostly a consideration for when I was still going with MI100s, but the fans were free anyway.&lt;/li&gt; &lt;li&gt;I really, really wanted to go AM5 for this, but there just isn't a board out there with 4 full sized PCIe slots spaced for 2 slot GPUs. At best you can fit 3 and then cover up one of them. But if you need a bazillion m.2 slots you're golden /s. You might then ask why I didn't go for Threadripper/Epyc, and that's because I was worried about power consumption and heat. I didn't want to mess with risers and open rigs, so I found the one AM4 board that could do this, even if it comes at the cost of RAM speeds/channels and slower PCIe speeds.&lt;/li&gt; &lt;li&gt;The MI100s and R9700s didn't play nice for the brief period of time I had 2 of both. I didn't bother troubleshooting, just shrugged and sold them off, so it may have been a simple fix but FYI.&lt;/li&gt; &lt;li&gt;Going with a 1 TB SSD in my original build was a mistake, even 2 would have made a world of difference. Between LLMs, image generation, TTS, ect. I'm having trouble actually taking advantage of the extra VRAM with less quantized models due to storage constraints, which is why my benchmarks still have a lot of 4-bit quants despite being able to easily do 8-bit ones.&lt;/li&gt; &lt;li&gt;I don't know how to control the little LCD display on the board. I'm not sure there is a way on Linux. A shame.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ulterior-Motive_"&gt; /u/Ulterior-Motive_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qfscp5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
