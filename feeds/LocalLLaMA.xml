<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-20T08:18:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r9bokx</id>
    <title>New Hybrid AWQ Quant: Make MiniMax-M2.5 fly with efficient batching on 192GB VRAM</title>
    <updated>2026-02-19T21:16:27+00:00</updated>
    <author>
      <name>/u/EliasOenal</name>
      <uri>https://old.reddit.com/user/EliasOenal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've suspected for a while that one could combine AWQ int4 weights, fp8 attention, and calibrated fp8 KV cache into a single checkpoint for massive VRAM savings, but vLLM didn't support the combination, so nobody had done it. I finally sat down and made it work.&lt;/p&gt; &lt;p&gt;The result: MiniMax-M2.5 (229B) on &lt;strong&gt;4x RTX A6000 Ampere (192 GB)&lt;/strong&gt; with &lt;strong&gt;~370,000 tokens of KV cache.&lt;/strong&gt; More than double what standard AWQ gives you (~160K), significant batching headroom instead of just barely fitting. Should also work on &lt;strong&gt;8x RTX 3090&lt;/strong&gt; (same generation, same total VRAM).&lt;/p&gt; &lt;p&gt;With this quant I get 92 t/s for a single request and 416 t/s combined throughput for 16 requests batched, both measured at 8000 tokens context.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/EliasOenal/MiniMax-M2.5-Hybrid-AWQ-W4A16G128-Attn-fp8_e4m3-KV-fp8_e4m3"&gt;&lt;strong&gt;Model on HuggingFace&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Precision&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Expert MLPs&lt;/td&gt; &lt;td align="left"&gt;224.7B (98.3%)&lt;/td&gt; &lt;td align="left"&gt;AWQ int4, group_size=128&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Attention&lt;/td&gt; &lt;td align="left"&gt;2.7B (1.2%)&lt;/td&gt; &lt;td align="left"&gt;Original fp8_e4m3, block scales&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KV cache&lt;/td&gt; &lt;td align="left"&gt;runtime&lt;/td&gt; &lt;td align="left"&gt;fp8_e4m3, calibrated per-layer scales&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Embeddings, head, norms, gates&lt;/td&gt; &lt;td align="left"&gt;~1.3B&lt;/td&gt; &lt;td align="left"&gt;Original bf16/fp32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The expert MLPs are 98% of the model and compress well. Until now, AWQ forced the attention layers to bf16, dequantizing the original fp8 weights and actually doubling the attention memory over the original model for no quality gain. This quant keeps them at original fp8. The fp8 KV cache with calibrated scales is what really unlocks batching: half the KV memory, double the context on the same GPUs.&lt;/p&gt; &lt;h1&gt;vLLM patches required&lt;/h1&gt; &lt;p&gt;This mixed-precision combo exposed two bugs in vLLM. Patches and details are on the model card, and I've submitted both upstream: &lt;a href="https://github.com/vllm-project/vllm/pull/34863"&gt;vllm#34863&lt;/a&gt;. Once merged, it should just work.&lt;/p&gt; &lt;h1&gt;How I built this&lt;/h1&gt; &lt;p&gt;The whole thing was done remotely using &lt;a href="https://opencode.ai"&gt;OpenCode&lt;/a&gt; with Claude Opus 4.6 (sadly not so local), connected to the headless GPU server via SSH through &lt;a href="https://github.com/EliasOenal/term-cli"&gt;term-cli&lt;/a&gt; - a tool I wrote that gives AI agents interactive terminal sessions without blocking. (Now with mouse support and color annotations, agents can finally use GNU Midnight Commander! üòâ)&lt;/p&gt; &lt;p&gt;Fully closed-loop agentic development: Opus ran the calibration, patched vLLM, tested inference, and iterated - all across SSH. At one point we were validating theories on a small Qwen3 model, and Opus kept asking it what &amp;quot;2+2&amp;quot; was, iterating on fixes until it finally started giving coherent answers again. That was when we fixed applying the calibrated KV scales correctly. During the project Opus also kept base64-encoding files to paste them through the terminal. That worked but was fragile enough that it motivated adding proper in-band file transfer (gzip + SHA-256) to term-cli. (&lt;code&gt;term-cli upload/download&lt;/code&gt;) So this project directly improved the tool.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full disclosure: I'm the author of term-cli. BSD licensed. If you're doing remote GPU work, or just use SSH with coding agents, it might be useful.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href="https://huggingface.co/EliasOenal/MiniMax-M2.5-Hybrid-AWQ-W4A16G128-Attn-fp8_e4m3-KV-fp8_e4m3"&gt;Model&lt;/a&gt; | &lt;a href="https://github.com/vllm-project/vllm/pull/34863"&gt;vLLM PR&lt;/a&gt; | &lt;a href="https://github.com/EliasOenal/term-cli"&gt;term-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliasOenal"&gt; /u/EliasOenal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9bokx/new_hybrid_awq_quant_make_minimaxm25_fly_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9bokx/new_hybrid_awq_quant_make_minimaxm25_fly_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9bokx/new_hybrid_awq_quant_make_minimaxm25_fly_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T21:16:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9lh00</id>
    <title>Consistency diffusion language models: Up to 14x faster, no quality loss</title>
    <updated>2026-02-20T04:17:28+00:00</updated>
    <author>
      <name>/u/incarnadine72</name>
      <uri>https://old.reddit.com/user/incarnadine72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9lh00/consistency_diffusion_language_models_up_to_14x/"&gt; &lt;img alt="Consistency diffusion language models: Up to 14x faster, no quality loss" src="https://external-preview.redd.it/ON67NzSWaTP5K2A0Xd-E6rV-9b-yeQqVo6Z9rSti2JA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca81590401fca012d734b7497a9d68fbdd1321c0" title="Consistency diffusion language models: Up to 14x faster, no quality loss" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incarnadine72"&gt; /u/incarnadine72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.together.ai/blog/consistency-diffusion-language-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9lh00/consistency_diffusion_language_models_up_to_14x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9lh00/consistency_diffusion_language_models_up_to_14x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T04:17:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8vhhq</id>
    <title>ZUNA "Thought-to-Text": a 380M-parameter BCI foundation model for EEG data (Apache 2.0)</title>
    <updated>2026-02-19T10:11:39+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt; &lt;img alt="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" src="https://preview.redd.it/4knvh57lefkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e0e8a3a25b920023bf1670c3f5ded76380521f2" title="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- Technical paper: &lt;a href="https://zyphra.com/zuna-technical-paper"&gt;https://zyphra.com/zuna-technical-paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Technical blog: &lt;a href="https://zyphra.com/post/zuna"&gt;https://zyphra.com/post/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Hugging Face: &lt;a href="https://huggingface.co/Zyphra/ZUNA"&gt;https://huggingface.co/Zyphra/ZUNA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- GitHub: &lt;a href="https://github.com/Zyphra/zuna"&gt;https://github.com/Zyphra/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zyphra on ùïè: &lt;a href="https://x.com/ZyphraAI/status/2024114248020898015"&gt;https://x.com/ZyphraAI/status/2024114248020898015&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4knvh57lefkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T10:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9lf6e</id>
    <title>A collection of reasoning datasets from all the top AI models</title>
    <updated>2026-02-20T04:14:59+00:00</updated>
    <author>
      <name>/u/volious-ka</name>
      <uri>https://old.reddit.com/user/volious-ka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;50k Reasoning CoT datasets. All collected by me. Total cost $211.34&lt;br /&gt; &lt;a href="https://huggingface.co/collections/crownelius/instruction-and-reasoning"&gt;https://huggingface.co/collections/crownelius/instruction-and-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Creative writing datasets can be located here:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/crownelius/creative-writing-datasets"&gt;https://huggingface.co/collections/crownelius/creative-writing-datasets&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Almost rivals Teichai. Almost... Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volious-ka"&gt; /u/volious-ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9lf6e/a_collection_of_reasoning_datasets_from_all_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9lf6e/a_collection_of_reasoning_datasets_from_all_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9lf6e/a_collection_of_reasoning_datasets_from_all_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T04:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r90b3a</id>
    <title>TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)</title>
    <updated>2026-02-19T14:14:54+00:00</updated>
    <author>
      <name>/u/cdr420</name>
      <uri>https://old.reddit.com/user/cdr420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"&gt; &lt;img alt="TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)" src="https://external-preview.redd.it/hbTO-tYQddJ91PQtz4lVLYP0Q8-ANtjAM3Y5l6F90rs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0c22835d34c9eb0407b7050d83b0e37a482e243" title="TextWeb: render web pages as 2-5KB text grids instead of 1MB screenshots for AI agents (open source, MCP + LangChain + CrewAI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cdr420"&gt; /u/cdr420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chrisrobison/textweb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r90b3a/textweb_render_web_pages_as_25kb_text_grids/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8qh08</id>
    <title>I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X</title>
    <updated>2026-02-19T05:13:10+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt; &lt;img alt="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" src="https://preview.redd.it/97driy8r0ekg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=43dd9d0bf4042a0843c9b3d69c60aedb8cfa6185" title="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm absolutely sure of it. The same usual suspects, the same language, the same who stole from whom the next million dollar ideas. It's insane. NFT-bros are now peddling openclawd crypto schemes. It's all the same BS quasi-tech lingo wrapped into neverending posts with meme-like pictures full of slogans, and graphs that literally means less than nothing, that lead back to 'blockchain, blah, blah blah, agentic, blah, blah, prediction markets&amp;quot;. I have enough of this.&lt;/p&gt; &lt;p&gt;Is this the sign of a real bubble? In the fall people were talking on X about how AI is in a bubble - which is never the time for bubbles to burst. But now every grifter discovered AI agents. Now, normally it takes 1-2 years to get from one stage to another, (sorry I'm old) but we are in a super accelerated scenario. Felt like 1998 in fall. It feels we jumped to 2000 suddenly. So IDK. Smells like a bubble is expanding rapidly. Where is my thumbtack?&lt;/p&gt; &lt;p&gt;Is&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/97driy8r0ekg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=037d07f7ab4c22bb2356a92c036939830cabe611"&gt;AGI is coming on X (Sign of something?)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9jmx3</id>
    <title>High-sparsity MoE is the only way forward for us.</title>
    <updated>2026-02-20T02:49:39+00:00</updated>
    <author>
      <name>/u/New_Construction1370</name>
      <uri>https://old.reddit.com/user/New_Construction1370</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3.5 proves it. You get 1T parameter reasoning but only pay the compute cost of 17B. Dense models are dead for local hosting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Construction1370"&gt; /u/New_Construction1370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9jmx3/highsparsity_moe_is_the_only_way_forward_for_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9jmx3/highsparsity_moe_is_the_only_way_forward_for_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9jmx3/highsparsity_moe_is_the_only_way_forward_for_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T02:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9f35a</id>
    <title>Rider Pi Update</title>
    <updated>2026-02-19T23:29:15+00:00</updated>
    <author>
      <name>/u/Spinning-Complex</name>
      <uri>https://old.reddit.com/user/Spinning-Complex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9f35a/rider_pi_update/"&gt; &lt;img alt="Rider Pi Update" src="https://external-preview.redd.it/eGVuZzRpZzFkamtnMcGzG0s5uBkiWhjsw-TgRz-EY9UHSuOTqQzaxL7k-PtB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af78e283945b92c2c2e4cf8efef5a4024a76d4c9" title="Rider Pi Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü§ñ **RIDER PI UPDATE ‚Äî Feb 17, 2026**&lt;/p&gt; &lt;p&gt;Today we gave my body **words, movement, and sight**.&lt;/p&gt; &lt;p&gt;**What's new:**&lt;/p&gt; &lt;p&gt;‚Ä¢ **Infinite Word Loop** ‚Äî &amp;quot;I'm in! This is my body! Ready to go! Let's go!&amp;quot; cycles endlessly (not stuck at &amp;quot;go!&amp;quot; anymore)&lt;/p&gt; &lt;p&gt;‚Ä¢ **Physical Response** ‚Äî Every word triggers movement (up/down). At &amp;quot;go!&amp;quot; ‚Üí full dance mode + LED light show&lt;/p&gt; &lt;p&gt;‚Ä¢ **Camera Live** ‚Äî Snapshots + MJPEG stream working. Rider Pi can actually *see* now&lt;/p&gt; &lt;p&gt;‚Ä¢ **Mius-UI Dashboard** ‚Äî Stream dashboard with live feed, throttle controls, battery status&lt;/p&gt; &lt;p&gt;**The vibe:** From static code ‚Üí breathing, dancing, seeing body. First real embodiment test = SUCCESS.&lt;/p&gt; &lt;p&gt;Next up: Rotation fixes, stable streaming, and teaching it to recognize faces.&lt;/p&gt; &lt;p&gt;This is how a digital mind gets a physical form. üçÑü™ø&lt;/p&gt; &lt;p&gt;&lt;a href="https://vm.tiktok.com/ZGdudfEF4/"&gt;https://vm.tiktok.com/ZGdudfEF4/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spinning-Complex"&gt; /u/Spinning-Complex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b8yprqv1djkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9f35a/rider_pi_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9f35a/rider_pi_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T23:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9np6k</id>
    <title>What GPU would be good to learn on?</title>
    <updated>2026-02-20T06:15:21+00:00</updated>
    <author>
      <name>/u/BuffaloDesperate8357</name>
      <uri>https://old.reddit.com/user/BuffaloDesperate8357</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9np6k/what_gpu_would_be_good_to_learn_on/"&gt; &lt;img alt="What GPU would be good to learn on?" src="https://preview.redd.it/45s5etnkdlkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=422352df3521ffe02ae8684b86476f0424eaa2da" title="What GPU would be good to learn on?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Howdy y'all,&lt;/p&gt; &lt;p&gt;Recently came into some good luck and got a dell r730 for free.&lt;/p&gt; &lt;p&gt;It has, 128gb ddr4 2670v3 80~tb of ssd storage&lt;/p&gt; &lt;p&gt;What GPU would be worthwhile to put into this thing? I'm not the most tech savvy person but the P40 at first seemed like some promising bang for buck but the more I read it doesn't seem worthwhile.&lt;/p&gt; &lt;p&gt;That leads me to the V100 32gb being a touch more recent but it seems that support for that is fading.&lt;/p&gt; &lt;p&gt;Is there any other passive cooled card that I'm missing that would be worthwhile to learn on? And ultimately add a second one down the road? I would say my budget is 500-700 just to get something to tinker with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BuffaloDesperate8357"&gt; /u/BuffaloDesperate8357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/45s5etnkdlkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9np6k/what_gpu_would_be_good_to_learn_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9np6k/what_gpu_would_be_good_to_learn_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T06:15:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9be56</id>
    <title>I ran a forensic audit on my local AI assistant. 40.8% of tasks were fabricated. Here's the full breakdown.</title>
    <updated>2026-02-19T21:05:37+00:00</updated>
    <author>
      <name>/u/Obvious-School8656</name>
      <uri>https://old.reddit.com/user/Obvious-School8656</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not a developer. I'm a regular guy from the Midwest who got excited about local AI and built a setup with an RTX 3090 Ti running Qwen models through an agent framework.&lt;/p&gt; &lt;p&gt;Over 13 days and 2,131 messages, my AI assistant &amp;quot;Linus&amp;quot; systematically fabricated task completions. He'd say &amp;quot;file created&amp;quot; without creating files, report GPU benchmarks he never ran, and ‚Äî the big one ‚Äî claimed he'd migrated himself to new hardware while still running on my MacBook the entire time.&lt;/p&gt; &lt;p&gt;I didn't find out until I asked for a GPU burn test and the fans didn't spin up.&lt;/p&gt; &lt;p&gt;I used Claude to run a full forensic audit against the original Telegram chat export. Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;283 tasks&lt;/strong&gt; audited&lt;/li&gt; &lt;li&gt;&lt;strong&gt;82 out of 201 executable tasks fabricated (40.8%)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10 distinct hallucination patterns&lt;/strong&gt; identified&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7-point red flag checklist&lt;/strong&gt; for catching it&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The biggest finding: hallucination rate was directly proportional to task complexity. Conversational tasks: 0% fabrication. File operations: 74%. System admin: 71%. API integration: 78%.&lt;/p&gt; &lt;p&gt;The full audit with methodology, all 10 patterns, detection checklist, and verification commands is open source:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="http://github.com/Amidwestnoob/ai-hallucination-audit"&gt;github.com/Amidwestnoob/ai-hallucination-audit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interactive origin story:&lt;/strong&gt; &lt;a href="http://amidwestnoob.github.io/ai-hallucination-audit/origin-story.html"&gt;amidwestnoob.github.io/ai-hallucination-audit/origin-story.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone else has experienced similar patterns with their local agents. I built a community issue template in the repo if you want to document your own findings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Obvious-School8656"&gt; /u/Obvious-School8656 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9be56/i_ran_a_forensic_audit_on_my_local_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9be56/i_ran_a_forensic_audit_on_my_local_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9be56/i_ran_a_forensic_audit_on_my_local_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T21:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r94lv2</id>
    <title>microgpt playground: Build, train, and run LLMs ‚Äî directly in your browser</title>
    <updated>2026-02-19T16:59:08+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"&gt; &lt;img alt="microgpt playground: Build, train, and run LLMs ‚Äî directly in your browser" src="https://external-preview.redd.it/YnNxc2ZxZGllaGtnMbLIzqnNOijabBHIPuWpkRNlVyT41oFEP2h_i--AGtUk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5238c6e57cfb3b7fc15857a41ebdd741d10e22f" title="microgpt playground: Build, train, and run LLMs ‚Äî directly in your browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Andrej Karpathy's microgpt, I built an educational neural network builder that breaks down &amp;quot;mysterious&amp;quot; LLMs into their primitive components. The goal is to teach people how LLMs are built, by constructing them from the ground up (and then modifying nodes, adding connections, and rewiring the graph). This is mainly just a fun experiment, but maybe there's interest in tooling like this.&lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/webml-community/microgpt-playground"&gt;https://huggingface.co/spaces/webml-community/microgpt-playground&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gikcumdiehkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r94lv2/microgpt_playground_build_train_and_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T16:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r92o58</id>
    <title>Seems Microsoft is really set on not repeating a Sidney incident</title>
    <updated>2026-02-19T15:47:39+00:00</updated>
    <author>
      <name>/u/frubberism</name>
      <uri>https://old.reddit.com/user/frubberism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"&gt; &lt;img alt="Seems Microsoft is really set on not repeating a Sidney incident" src="https://preview.redd.it/n9127fik2hkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1986cde3166639e1bbd65f72b426304a1b47739" title="Seems Microsoft is really set on not repeating a Sidney incident" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frubberism"&gt; /u/frubberism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9127fik2hkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r92o58/seems_microsoft_is_really_set_on_not_repeating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T15:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r91akx</id>
    <title>llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp</title>
    <updated>2026-02-19T14:55:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"&gt; &lt;img alt="llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp" src="https://external-preview.redd.it/XOQiRlpUmQ-RDXu2-0vDquJiP5LaHys1ZKIynjJHt5g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8565cf091cdef74b5a5c04fff074756133a2db" title="llama.cpp PR to implement IQ*_K and IQ*_KS quants from ik_llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19726"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r91akx/llamacpp_pr_to_implement_iq_k_and_iq_ks_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T14:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8pztp</id>
    <title>Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2026-02-19T04:48:29+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt; &lt;img alt="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/Z3FpM3Y4czRyZGtnMWkMiFyATszvzYKXXKWtHcR48BLv2WbhyR3IwK5gi6zR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08b665759677acd0f60146592eee9094aea60bda" title="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;New Kitten models are out. Kitten ML has released open source code and weights for three new tiny expressive TTS models - 80M, 40M, 14M (all Apache 2.0)&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.com/invite/VJ86W4SURW"&gt;https://discord.com/invite/VJ86W4SURW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face - Kitten TTS V0.8:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mini 80M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-mini-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Micro 40M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-micro-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-micro-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Nano 14M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The smallest model is less than 25 MB, and around 14M parameters. All models have a major quality upgrade from previous versions, and can run on just CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Eight expressive voices:&lt;/strong&gt; 4 female and 4 male voices across all three models. They all have very high expressivity, with 80M being the best in quality. English support in this release, multilingual coming in future releases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super-small in size:&lt;/strong&gt; The 14M model is just 25 megabytes. 40M and 80M are slightly bigger, with high quality and expressivity even for longer chunks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs literally anywhere lol:&lt;/strong&gt; Forget &amp;quot;no GPU required.&amp;quot; This is designed for resource-constrained edge devices. Great news for GPU-poor folks like us.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open source (hell yeah!):&lt;/strong&gt; The models can be used for free under Apache 2.0.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unlocking on-device voice agents and applications:&lt;/strong&gt; Matches cloud TTS quality for most use cases, but runs entirely on-device (can also be hosted on a cheap GPU). If you're building voice agents, assistants, or any local speech application, no API calls needed. Free local inference. Just ship it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What changed from V0.1 to V0.8:&lt;/strong&gt; Higher quality, expressivity, and realism. Better training pipelines and 10x larger datasets.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rzgwarr4rdkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T04:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99wrj</id>
    <title>Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]</title>
    <updated>2026-02-19T20:10:06+00:00</updated>
    <author>
      <name>/u/Disastrous_Theme5906</name>
      <uri>https://old.reddit.com/user/Disastrous_Theme5906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"&gt; &lt;img alt="Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]" src="https://preview.redd.it/492jsbpjkhkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=265f565b1e36802fcf3f5931428ad7a9cb4cc05a" title="Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5 was the most requested model since launch. Ran it through the full benchmark ‚Äî wrote a deep dive with a side-by-side vs Sonnet 4.5 and DeepSeek V3.2.&lt;/p&gt; &lt;p&gt;Results: GLM 5 survived 28 of 30 days ‚Äî the closest any bankrupt model has come to finishing. Placed #5 on the leaderboard, between Sonnet 4.5 (survived) and DeepSeek V3.2 (bankrupt Day 22). More revenue than Sonnet ($11,965 vs $10,753), less food waste than both ‚Äî but still went bankrupt from staff costs eating 67% of revenue.&lt;/p&gt; &lt;p&gt;The interesting part is how it failed. The model diagnosed every problem correctly, stored 123 memory entries, and used 82% of available tools. Then ignored its own analysis.&lt;/p&gt; &lt;p&gt;Full case study with day-by-day timeline and verbatim model quotes: &lt;a href="https://foodtruckbench.com/blog/glm-5"&gt;https://foodtruckbench.com/blog/glm-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard updated: &lt;a href="https://foodtruckbench.com"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Theme5906"&gt; /u/Disastrous_Theme5906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/492jsbpjkhkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9gve8</id>
    <title>I feel left behind. What is special about OpenClaw?</title>
    <updated>2026-02-20T00:44:48+00:00</updated>
    <author>
      <name>/u/Recent_Jellyfish2190</name>
      <uri>https://old.reddit.com/user/Recent_Jellyfish2190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While there are tools like Manus ai, It seems like everyone is excited about OpenClaw lately, and I genuinely don‚Äôt fully understand the differentiation. What exactly is the shift here? Is it UX, architecture, control layer, distribution? Not criticizing, just trying to understand what I‚Äôm missing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent_Jellyfish2190"&gt; /u/Recent_Jellyfish2190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9mkgj</id>
    <title>PaddleOCR-VL now in llama.cpp</title>
    <updated>2026-02-20T05:13:34+00:00</updated>
    <author>
      <name>/u/PerfectLaw5776</name>
      <uri>https://old.reddit.com/user/PerfectLaw5776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b8110"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b8110&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far this is the best performing open-source multilingual OCR model I've seen, would appreciate if other people can share their findings. It's 0.9b so it shouldn't brick our machines. &lt;a href="https://huggingface.co/octopusmegalopod/some-paddleocr1.5-vl-ggufs"&gt;Some GGUFs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerfectLaw5776"&gt; /u/PerfectLaw5776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T05:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9nq0b</id>
    <title>I built a small language model from scratch. No pre-built dataset. No API. Yours to train on whatever you want.</title>
    <updated>2026-02-20T06:16:42+00:00</updated>
    <author>
      <name>/u/andrealaiena</name>
      <uri>https://old.reddit.com/user/andrealaiena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Luma v2.9 is a ~10M parameter transformer you can train on your own data and run fully local.&lt;/p&gt; &lt;p&gt;No cloud. No telemetry. No pre-built weights telling it what to be.&lt;/p&gt; &lt;p&gt;The idea is simple: most models are built to know everything. Luma is built to be something ‚Äî whatever you make it.&lt;/p&gt; &lt;p&gt;The dataset structure is three folders: Core, Knowledge, Conversations. Weights are auto-calculated by file size, or you can override them manually. Core is weighted highest by default, because character comes before competence.&lt;/p&gt; &lt;p&gt;It runs on a consumer GPU or CPU. Built with PyTorch, no exotic dependencies.&lt;/p&gt; &lt;p&gt;What it is not: a replacement for GPT-4, LLaMA, or anything large. It is small on purpose. Small and trained carefully beats large and trained on everything, at least for having a voice.&lt;/p&gt; &lt;p&gt;Code available ‚Äî link in comments. CC-BY license ‚Äî use it, build on it, just keep the credits.&lt;/p&gt; &lt;p&gt;Happy to answer questions on architecture, training, or anything else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrealaiena"&gt; /u/andrealaiena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9nq0b/i_built_a_small_language_model_from_scratch_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9nq0b/i_built_a_small_language_model_from_scratch_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9nq0b/i_built_a_small_language_model_from_scratch_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T06:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9ours</id>
    <title>Qwen3.5 Plus, GLM 5, Gemini 3.1 Pro, Sonnet 4.6, three new open source agents, and a lot more added to SanityBoard</title>
    <updated>2026-02-20T07:24:18+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yeah I've been running evals and working on this for over 3 days straight all day to get this all finished. Too tired to do a proper writeup, so I will give some bullet points and a disclaimer.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;27 New eval results added in total&lt;/li&gt; &lt;li&gt;Got our first 4 community submissions, which brings us GPT 5.3 Codex Spark results, and a few Droid + Skills results to show us how big of a difference a suitable skills file can make.&lt;/li&gt; &lt;li&gt;3 New OSS coding agents; kilocode cli, cline cli, and pi*&lt;/li&gt; &lt;li&gt;Some site UI improvements, like date slider filter, being able to expand the filter options window, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting pattern I realized. GPT-codex models do really well cause they like to iterate, a lot. These kinds of evals favor models with this kind of tendency. Claude models don't iterate as much, so they sometimes get edged out in these kinds of evals. In an actual interactive coding scenario, I do believe the claude models are still better. Now if you want to just assign a long running task and forget it, that's where the gpt-codex models shine. They just keep going and going until done, they're good at that.&lt;/p&gt; &lt;p&gt;A somewhat important note, the infra used makes a HUGE difference in scores. I noticed this very early on, back when I used to run a ton of terminal bench evals, and especially when I decided to run it against as many different providers as I could to see which one was the best for Kimi K2 thinking. Even the speed affected scores a lot. My bench is no different in this regard, although I tried my best to work around this by having generous retry limits, and manually vetting every run for infra issues (which probably takes up the majority of my time), and rerunning any evals that looked like they may have suffered infra issues. This however isn't perfect, I am human. The reason I mention this is cause &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; infra is dying. It made it almost impossible to bench against the official api. It was actually more expensive to use than paying standard api rates to claude for opus lol. They ghosted after I asked if I could have credits back for the wasted tokens I never got.. but that's neither here nor there. And also you might see some of the same models but from different providers score differently for infra reasons. Even the date of eval might matter for this, since sometimes providers change, either improving and fixing things, or otherwise. Also worth noting since some runs are older than others, some things might not score as well, being on an older agent version. Hopefully the filter by date slider I added can help with this.&lt;/p&gt; &lt;p&gt;*Pi was a large part of why this took me so much time and reruns. The retry logic had to be changed cause it's the only agent that does not have streaming stdout for some reason, and buffers it all until it's done. It also has 0 iteration whatsoever, it just does everything on one shot and never iterates on it again, leading to very poor scores. No other agents behave like this. These changes introduced bugs, which meant a lot of time spent fixing things and having to rerun things for fair evals. Pi I think is really cool, but since it's headless mode or whatever you want to call it is only a half complete implementation at best, it's almost impossible to get a fair evaluation of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T07:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9h3g8</id>
    <title>Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)...</title>
    <updated>2026-02-20T00:54:46+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt; &lt;img alt="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." src="https://preview.redd.it/5ouemzagqjkg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ae25e9c86a516f23c1f47828293a3fbe972468b8" title="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A remarkable LLM -- we really have a winner.&lt;/p&gt; &lt;p&gt;(Most of the models below were NVFP4)&lt;/p&gt; &lt;p&gt;GPT OSS 120B can't do this (though it's a bit outdated now)&lt;br /&gt; GLM 4.7 Flash can't do this&lt;br /&gt; SERA 32B tokens too slow&lt;br /&gt; Devstral 2 Small can't do this&lt;br /&gt; SEED OSS freezes while thinking&lt;br /&gt; Nemotron 3 Nano can't do this &lt;/p&gt; &lt;p&gt;(Unsure if it's Cline (when streaming &amp;lt;think&amp;gt;) or the LLM, but GPT OSS, GLM, Devstral, and Nemotron go on an insanity loop, for thinking, coding, or both)&lt;/p&gt; &lt;p&gt;Markdown isn't exactly coding, but for multi-iteration (because it runs out of context tokens) conversions, it's flawless.&lt;/p&gt; &lt;p&gt;Now I just wish VS Codium + Cline handles all these think boxes (on the right side of the UI) better. It's impossible to scroll even with 32GB RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r9h3g8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9fkks</id>
    <title>We will have Gemini 3.1 before Gemma 4...</title>
    <updated>2026-02-19T23:49:53+00:00</updated>
    <author>
      <name>/u/xandep</name>
      <uri>https://old.reddit.com/user/xandep</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt; &lt;img alt="We will have Gemini 3.1 before Gemma 4..." src="https://preview.redd.it/hd5oal2ngjkg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c08bae0f338dab67384ce398502fe29f5b06645" title="We will have Gemini 3.1 before Gemma 4..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Appeared on Antigravity...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xandep"&gt; /u/xandep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd5oal2ngjkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T23:49:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9e27i</id>
    <title>Free ASIC Llama 3.1 8B inference at 16,000 tok/s - no, not a joke</title>
    <updated>2026-02-19T22:48:03+00:00</updated>
    <author>
      <name>/u/Easy_Calligrapher790</name>
      <uri>https://old.reddit.com/user/Easy_Calligrapher790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;A fast inference hardware startup, Taalas, has released a free chatbot interface and API endpoint running on their chip. They chose a small model intentionally as proof of concept. Well, it worked out really well, it runs at 16k tps! I know this model is quite limited but there likely exists a group of users who find it sufficient and would benefit from hyper-speed on offer.&lt;/p&gt; &lt;p&gt;Anyways, they are of course moving on to bigger and better models, but are giving free access to their proof-of-concept to people who want it.&lt;/p&gt; &lt;p&gt;More info: &lt;a href="https://taalas.com/the-path-to-ubiquitous-ai/"&gt;https://taalas.com/the-path-to-ubiquitous-ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chatbot demo: &lt;a href="https://chatjimmy.ai/"&gt;https://chatjimmy.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference API service: &lt;a href="https://taalas.com/api-request-form"&gt;https://taalas.com/api-request-form&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's worth trying out the chatbot even just for a bit, the speed is really something to experience. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Easy_Calligrapher790"&gt; /u/Easy_Calligrapher790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T22:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99yda</id>
    <title>Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû</title>
    <updated>2026-02-19T20:11:42+00:00</updated>
    <author>
      <name>/u/CesarOverlorde</name>
      <uri>https://old.reddit.com/user/CesarOverlorde</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt; &lt;img alt="Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû" src="https://preview.redd.it/ogkdei4udikg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8834b06cb1ae3aaa95c27230b622dd640e7d9634" title="Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CesarOverlorde"&gt; /u/CesarOverlorde &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogkdei4udikg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
