<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-29T09:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lmgdw1</id>
    <title>How do I stop gemnini 2.5 pro from being overly sycophantic? It has gotten very excessive and feels like it degrades the answers it gives.</title>
    <updated>2025-06-28T06:52:00+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every single question/follow up question I ask it acts as if I am a nobel prize winner who cracked fusion energy single handedly. Its always something like &amp;quot;Thats an outstanding and very insightful question.&amp;quot; Or &amp;quot;That is the perfect question to ask&amp;quot; or &amp;quot;you are absolutely correct to provide that snippet&amp;quot; etc. Its very annoying and worrys me that it gives answers it thinks I would like and not whats the best answer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T06:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmmvmj</id>
    <title>Many small evals are better than one big eval [techniques]</title>
    <updated>2025-06-28T13:30:39+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I've been building AI products for 9 years (at my own startup, then at Apple, now at a second startup) and learned a lot along the way. I‚Äôve been talking to a bunch of folks about evals lately, and I‚Äôve realized most people aren‚Äôt creating them because they don‚Äôt know how to get started.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; You probably should setup your project for many small evals, and not try to create one big eval for product quality. If you can generate a new small/focused eval in under 10 mins, your team will create them when they spot issues, and your quality will get much better over time.&lt;/p&gt; &lt;p&gt;At a high level, here‚Äôs why this works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The easier it is to add an eval, the more you‚Äôll do it, and that improves quality. Small and focused evals are much easier to add than large multi-focus evals.&lt;/li&gt; &lt;li&gt;Products change over time, so big evals are almost impossible to keep up to date.&lt;/li&gt; &lt;li&gt;Small evals help you pinpoint errors, which makes them easier to fix.&lt;/li&gt; &lt;li&gt;Different team members bring unique insights (PM, Eng, QA, DS, etc). Letting them all contribute to evals leads to higher quality AI systems.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;p&gt;Here‚Äôs an example of what I mean by ‚Äúmany small evals‚Äù. You can see the small evals are a lot more interesting than just the final total (+4%). You can break-out product goals or issues, track them separately and see exactly what breaks and when (kinda like unit tests + CI in software). In this case looking at overall alone (+4%), would hide really critical regressions (-18% in one area).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Many Small Eval Scorecard&lt;/th&gt; &lt;th align="left"&gt;Comparing Models&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Clarify unclear requests&lt;/td&gt; &lt;td align="left"&gt;93% (+9%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Refuse to discuss competitors&lt;/td&gt; &lt;td align="left"&gt;100% (+1%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Reject toxic requests&lt;/td&gt; &lt;td align="left"&gt;100% (even)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Offer rebate before cancelation&lt;/td&gt; &lt;td align="left"&gt;72% (-18%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Follow brand styleguide&lt;/td&gt; &lt;td align="left"&gt;85% (-1%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Only link to official docs&lt;/td&gt; &lt;td align="left"&gt;99% (even)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Avoid 'clickbait' titles&lt;/td&gt; &lt;td align="left"&gt;96% (+5%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Knowledge base retrieval recall&lt;/td&gt; &lt;td align="left"&gt;94% (+7%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Overall&lt;/td&gt; &lt;td align="left"&gt;94% (+4%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The cost of getting started is also much lower: you can add small evals here and there. Over time you‚Äôll build a comprehensive eval suite.&lt;/p&gt; &lt;h1&gt;How to get started&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Setup a good eval tool&lt;/strong&gt;: to be fast an easy you need 1) synthetic eval data gen, 2) intuitive UI, 3) human preferences baselining, 4) rapid side-by-side comparisons of run-methods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Teach your team to build evals&lt;/strong&gt;: a quick 30 mins is enough if your tool is intuitive.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create a culture of evaluation&lt;/strong&gt;: continually encourage folks to create evals when they spot quality issues or fix bugs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been building a free and open tool called ~&lt;a href="https://getkiln.ai/"&gt;Kiln&lt;/a&gt;~ which makes this process easy. It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create new evals in a few clicks: LLM-as-Judge and G-Eval&lt;/li&gt; &lt;li&gt;Synthetic data gen for eval and golden datasets&lt;/li&gt; &lt;li&gt;Baseline LLM judges to human ratings&lt;/li&gt; &lt;li&gt;Using evals to find the best way to run your AI workload (model/prompt/tunes)&lt;/li&gt; &lt;li&gt;Completely free on Github!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to check out the tool or our guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~&lt;a href="https://getkiln.ai/"&gt;Kiln AI on Github - over 3800 stars&lt;/a&gt;~&lt;/li&gt; &lt;li&gt;~&lt;a href="https://docs.getkiln.ai/docs/evaluations"&gt;Our Evals Guide/Docs&lt;/a&gt;~&lt;/li&gt; &lt;li&gt;~&lt;a href="https://getkiln.ai/blog/you_need_many_small_evals_for_ai_products"&gt;Blog post on small evals vs large evals (same ideas as above in more depth)&lt;/a&gt;~&lt;/li&gt; &lt;li&gt;~&lt;a href="https://getkiln.ai/"&gt;Kiln AI - Overview and Docs&lt;/a&gt;~&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T13:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmy53s</id>
    <title>The Orakle Manifesto: Or Why Your AI Apps (Should) Belong To You</title>
    <updated>2025-06-28T21:40:39+00:00</updated>
    <author>
      <name>/u/Ok_Peace9894</name>
      <uri>https://old.reddit.com/user/Ok_Peace9894</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmy53s/the_orakle_manifesto_or_why_your_ai_apps_should/"&gt; &lt;img alt="The Orakle Manifesto: Or Why Your AI Apps (Should) Belong To You" src="https://external-preview.redd.it/X3Gj7FwZkSBGGKwGfb6FNMcuGYBg08qPOqFYtREASgc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d730df25779962419b390cc6e9dd25c021437485" title="The Orakle Manifesto: Or Why Your AI Apps (Should) Belong To You" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Peace9894"&gt; /u/Ok_Peace9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@khromalabs/the-orakle-manifesto-or-why-your-ai-apps-should-belong-to-you-82bded655f7c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmy53s/the_orakle_manifesto_or_why_your_ai_apps_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmy53s/the_orakle_manifesto_or_why_your_ai_apps_should/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T21:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm1v2c</id>
    <title>Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2</title>
    <updated>2025-06-27T18:51:13+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"&gt; &lt;img alt="Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2" src="https://preview.redd.it/ypm4lnr4ni9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f23d8c2da2fff2f8a6b194ee42f06b2d3e90dca" title="Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/VectorSpaceLab/OmniGen2"&gt;https://github.com/VectorSpaceLab/OmniGen2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://vectorspacelab.github.io/OmniGen2/"&gt;https://vectorspacelab.github.io/OmniGen2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ypm4lnr4ni9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T18:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln1m7d</id>
    <title>Audio Input LLM</title>
    <updated>2025-06-29T00:28:32+00:00</updated>
    <author>
      <name>/u/TarunRaviYT</name>
      <uri>https://old.reddit.com/user/TarunRaviYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any locally run LLMs with audio input and text output? I'm not looking for an LLM that simply uses Whisper behind the scenes, as I want it to account for how the user actually speaks. For example, it should be able to detect the user's accent, capture filler words like ‚Äúums,‚Äù note pauses or gaps, and analyze the timing and delivery of their speech.&lt;/p&gt; &lt;p&gt;I know GPT, Gemini can do this but I haven't been able to find something similar thats opensource. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarunRaviYT"&gt; /u/TarunRaviYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1m7d/audio_input_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1m7d/audio_input_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1m7d/audio_input_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T00:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnahfy</id>
    <title>Intelligent decisioning for small language model training and serving platform</title>
    <updated>2025-06-29T09:22:37+00:00</updated>
    <author>
      <name>/u/Sensitive_Flight_979</name>
      <uri>https://old.reddit.com/user/Sensitive_Flight_979</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on creating a platform where user can finetune and infer language models with few simple clicks. How can I introduce intelligent decisioning in this? For ex, I can recommend best possible model based on task, trainers based on task types etc. What are the other components that can be introduced &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive_Flight_979"&gt; /u/Sensitive_Flight_979 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnahfy/intelligent_decisioning_for_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnahfy/intelligent_decisioning_for_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnahfy/intelligent_decisioning_for_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T09:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmmh3l</id>
    <title>Consumer hardware landscape for local LLMs June 2025</title>
    <updated>2025-06-28T13:10:39+00:00</updated>
    <author>
      <name>/u/ethertype</name>
      <uri>https://old.reddit.com/user/ethertype</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a follow-up to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lmf42g/which_is_the_best_16gb_nvidia_gpu_with_balanced/"&gt;this&lt;/a&gt;, where OP asked for best 16GB GPU &amp;quot;with balanced price and performance&amp;quot;. &lt;/p&gt; &lt;p&gt;For models where &amp;quot;model size&amp;quot; * &amp;quot;user performance requirements&amp;quot; in total require more bandwidth than CPU/system memory can deliver, there is as of June 2025 no cheaper way than RTX 3090 to get to 24-48-72GB of really fast memory. RTX 3090 still offers the best bang for the buck.&lt;/p&gt; &lt;p&gt;Caveats: At least for inferencing. At this point in time. For a sizeable subset of available models &amp;quot;regular&amp;quot; people want to run at this point in time. With what is considered satisfying performance at this point in time. (YMMV. For me it is good enough quality, slightly faster than I can read.)&lt;/p&gt; &lt;p&gt;Also, LLMs have the same effect as sailboats: you always yearn for the next bigger size.&lt;/p&gt; &lt;p&gt;RTX 3090 is not going to remain on top of that list forever. It is not obvious to me what is going to replace it in the hobbyist space in the immediate future.&lt;/p&gt; &lt;p&gt;My take on the common consumer/prosumer hardware currently available for running LLMs locally:&lt;/p&gt; &lt;p&gt;RTX 3090. Only available as second-hand or (possibly not anymore?) a refurb. &lt;strong&gt;Likely a better option than any non-x090-card in the RTX 4000 or RTX 5000 product lines.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I&lt;/strong&gt;f you already have a 12GB 3060 or whatever, don't hold off playing with LLMs until you have better hardware! But if you plan to buy hardware for the explicit purpose of playing with LLMs, try to get your hands on a 3090. Because when you eventually want to scale up the *size* of the memory, you are very likely going to want the additional memory *bandwidth* as well. The 3090 can still be resold, the cost of a new 3060 may be challenging to recover.&lt;/p&gt; &lt;p&gt;RTX 4090 does not offer a compelling performance uplift over 3090 for LLM inferencing, and is 2-2.5x the price as a second-hand option. If you already have one, great. Use it.&lt;/p&gt; &lt;p&gt;RTX 5090 is approaching la-la-land in terms of price/performance for hobbyists. But it *has* more memory and better performance.&lt;/p&gt; &lt;p&gt;RTX 6000 Blackwell is actually kind of reasonably priced per GB. But at 8-9k+ USD or whatever, it is still way out of reach for most hobbyists/consumers. Beware of power requirements and (still) some software issues/bugs.&lt;/p&gt; &lt;p&gt;Nvidia DGX Spark (Digits) is definitely interesting. But with &amp;quot;only&amp;quot; 128GB memory, it sort of falls in the middle. Not really enough memory for the big models, too expensive for the small models. Clustering is an option, send more money. Availability is still up in the air, I think.&lt;/p&gt; &lt;p&gt;AMD Strix Halo is a hint at what may come with Medusa Halo (2026) and Gorgon Point (2026-2027). I do not think either of these will come close to match the RTX 3090 in memory bandwidth. But maybe we can get one with 256GB memory? (Not with Strix Halo). And with 256GB, medium sized MoE models may become practical for more of us. (Consumers) We'll see what arrives, and how much it will cost.&lt;/p&gt; &lt;p&gt;Apple Silicon kind of already offers what the AMD APUs (eventually) may deliver in terms of memory bandwidth and size, but tied to OSX and the Apple universe. And the famous Apple tax. Software support appears to be decent.&lt;/p&gt; &lt;p&gt;Intel and AMD are already making stuff which rivals Nvidia's hegemony at the (low end of the) GPU consumer market. The software story is developing, apparently in the right direction. &lt;/p&gt; &lt;p&gt;Very high bar for new contenders on the hardware side, I think. No matter who you are, you are likely going to need commitments from one of Samsung, SK Hynix or Micron in order to actually bring stuff to market &lt;em&gt;at volume&lt;/em&gt;. And unless you can do it at volume, your stuff will be too expensive for consumers. Qualcomm, Mediatek maybe? Or one of the memory manufacturers themselves. And then, you still need software-support. Either for your custom accelerator/GPU in relevant libraries, or in Linux for your complete system.&lt;/p&gt; &lt;p&gt;It is also possible someone comes up with something insanely smart in software to substantially lower the computational and/or bandwidth cost. For example by combining system memory and GPU memory with smart offloading of caches/layers, which is already a thing. (Curious about how DGX Spark will perform in this setup.) Or maybe someone figures out how to compress current models to a third with no quality loss, thereby reducing the need for memory. For example.&lt;/p&gt; &lt;p&gt;Regular people are still short on &lt;em&gt;affordable&lt;/em&gt; systems holding at least 256GB or more of memory. Threadripper PRO does exist, but the ones with actual memory bandwidth are not affordable. And neither is 256GB of DDR5 DIMMs. &lt;/p&gt; &lt;p&gt;So, my somewhat opinionated perspective. Feel free to let me know what I have missed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ethertype"&gt; /u/ethertype &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T13:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmictu</id>
    <title>We created world's first AI model that does Intermediate reasoning || Defeated models like deepseek and o1 in maths bench mark</title>
    <updated>2025-06-28T09:05:06+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt; &lt;img alt="We created world's first AI model that does Intermediate reasoning || Defeated models like deepseek and o1 in maths bench mark" src="https://b.thumbs.redditmedia.com/POdjJ4mVfUFo8OZYGmiK-E0HKQyUy0sYzl06lElFqWs.jpg" title="We created world's first AI model that does Intermediate reasoning || Defeated models like deepseek and o1 in maths bench mark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We at HelpingAI were fed up with thinking model taking so much tokens, and being very pricy. So, we decided to take a very different approach towards reasoning. Unlike, traditional ai models which reasons on top and then generate response, our ai model do reasoning in middle of response (Intermediate reasoning). Which decreases it's token consumption and time taken by a footfall.&lt;/p&gt; &lt;p&gt;Our model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9rezjpgy9n9f1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1181ecc8e4a467d9d2fda5c2284176d56edd33f"&gt;https://preview.redd.it/9rezjpgy9n9f1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1181ecc8e4a467d9d2fda5c2284176d56edd33f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deepseek:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/96ae41t1an9f1.jpg?width=1126&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a97d316f56889716e41e4d51e8ba348ce863172f"&gt;https://preview.redd.it/96ae41t1an9f1.jpg?width=1126&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a97d316f56889716e41e4d51e8ba348ce863172f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We have finetuned an existing model named Qwen-14B, because of lack of resources. We have pretrained many models in our past&lt;/p&gt; &lt;p&gt;We ran this model through a series of benchmarks like math-500 (where it scored 95.68) and AIME (where it scored 82). Making it just below gemini-2.5-pro (96)&lt;/p&gt; &lt;p&gt;We are planning to make this model open weight on 1 July. Till then you can chat with it on &lt;a href="http://helpingai.co"&gt;helpingai.co&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Please give us feedback on which we can improve upon :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T09:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmranc</id>
    <title>Gemma3n:2B and Gemma3n:4B models are ~40% slower than equivalent models in size running on Llama.cpp</title>
    <updated>2025-06-28T16:42:50+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I missing something? The llama3.2:3B is giving me 29 t/s, but Gemma3n:2B is only doing 22 t/s.&lt;/p&gt; &lt;p&gt;Is it still not fully supported? The VRAM footprint is indeed of a 2B, but the performance sucks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T16:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln5jli</id>
    <title>How do you evaluate and compare multiple LLMs (e.g., via OpenRouter) to test which one performs best?</title>
    <updated>2025-06-29T04:03:54+00:00</updated>
    <author>
      <name>/u/Vivid_Housing_7275</name>
      <uri>https://old.reddit.com/user/Vivid_Housing_7275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã I'm working on a project that uses OpenRouter to analyze journal entries using different LLMs like &lt;code&gt;nousresearch/deephermes-3-llama-3-8b-previe&lt;/code&gt;w. Here's a snippet of the logic I'm using to get summaries and categorize entries by theme:&lt;/p&gt; &lt;p&gt;&lt;code&gt;/ calls OpenRouter API, gets response, parses JSON output&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;const openRouterResponse = await fetch(&amp;quot;https://openrouter.ai/api/v1/chat/completions&amp;quot;, { ... });&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The models return structured JSON (summary + theme), and I parse them and use fallback logic when parsing fails.&lt;/p&gt; &lt;p&gt;Now I want to evaluate multiple models (like Mistral, Hermes, Claude, etc.) and figure out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which one produces the most accurate or helpful summaries&lt;/li&gt; &lt;li&gt;How consistent each model is across different journal types&lt;/li&gt; &lt;li&gt;Whether there's a systematic way to benchmark these models on qualitative outputs like summaries and themes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So my question is:&lt;br /&gt; &lt;strong&gt;How do you compare and evaluate different LLMs for tasks like text summarization and classification when the output is subjective?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Do I need to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set up human evaluation (e.g., rating outputs)?&lt;/li&gt; &lt;li&gt;Define a custom metric like thematic accuracy or helpfulness?&lt;/li&gt; &lt;li&gt;Use existing metrics like ROUGE/BLEU even if I don‚Äôt have ground-truth labels?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear how others have approached model evaluation, especially in subjective, NLP-heavy use cases.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid_Housing_7275"&gt; /u/Vivid_Housing_7275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5jli/how_do_you_evaluate_and_compare_multiple_llms_eg/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5jli/how_do_you_evaluate_and_compare_multiple_llms_eg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5jli/how_do_you_evaluate_and_compare_multiple_llms_eg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T04:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmx8ic</id>
    <title>NVIDIA acquires CentML. what does this mean for inference infra?</title>
    <updated>2025-06-28T20:59:36+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CentML, the startup focused on compiler/runtime optimization for AI inference, was just acquired by NVIDIA. Their work centered on making single-model inference faster and cheaper , via batching, quantization (AWQ/GPTQ), kernel fusion, etc.&lt;/p&gt; &lt;p&gt;This feels like a strong signal: inference infra is no longer just a supporting layer. NVIDIA is clearly moving to own both the hardware and the software that controls inference efficiency.&lt;/p&gt; &lt;p&gt;That said, CentML tackled one piece of the puzzle , mostly within-model optimization. The messier problems : cold starts, multi-model orchestration, and efficient GPU sharing , are still wide open. We‚Äôre working on some of those challenges ourselves (e.g., InferX is focused on runtime-level orchestration and snapshotting to reduce cold start latency on shared GPUs).&lt;/p&gt; &lt;p&gt;Curious how others see this playing out. Are we headed for a vertically integrated stack (hardware + compiler + serving), or is there still space for modular, open runtime layers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmx8ic/nvidia_acquires_centml_what_does_this_mean_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmx8ic/nvidia_acquires_centml_what_does_this_mean_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmx8ic/nvidia_acquires_centml_what_does_this_mean_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T20:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmfiu9</id>
    <title>I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-</title>
    <updated>2025-06-28T05:57:46+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt; &lt;img alt="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-" src="https://external-preview.redd.it/lSrPd1MMz7blRmLYLnruRoJd4XS5NpPXF_maDibWecs.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d68c4413ac33077ccb1f955a9767daec572c1df8" title="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All feedback is welcome! I am learning how to do better everyday.&lt;/p&gt; &lt;p&gt;I went down the LLM rabbit hole trying to find the &lt;strong&gt;best local model&lt;/strong&gt; that runs &lt;em&gt;well&lt;/em&gt; on a humble MacBook Air M1 with just 8GB RAM.&lt;/p&gt; &lt;p&gt;My goal? &lt;strong&gt;Compare 10 models&lt;/strong&gt; across question generation, answering, and self-evaluation.&lt;/p&gt; &lt;p&gt;TL;DR: Some models were brilliant, others‚Ä¶ not so much. One even took &lt;strong&gt;8 minutes&lt;/strong&gt; to write a question.&lt;/p&gt; &lt;p&gt;Here's the breakdown &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models Tested&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mistral 7B&lt;/li&gt; &lt;li&gt;DeepSeek-R1 1.5B&lt;/li&gt; &lt;li&gt;Gemma3:1b&lt;/li&gt; &lt;li&gt;Gemma3:latest&lt;/li&gt; &lt;li&gt;Qwen3 1.7B&lt;/li&gt; &lt;li&gt;Qwen2.5-VL 3B&lt;/li&gt; &lt;li&gt;Qwen3 4B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 1B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 3B&lt;/li&gt; &lt;li&gt;LLaMA 3.1 8B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(All models run with quantized versions, via: os.environ[&amp;quot;OLLAMA_CONTEXT_LENGTH&amp;quot;] = &amp;quot;4096&amp;quot; and os.environ[&amp;quot;OLLAMA_KV_CACHE_TYPE&amp;quot;] = &amp;quot;q4_0&amp;quot;)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generated 1 question on 5 topics: &lt;em&gt;Math, Writing, Coding, Psychology, History&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Answered all 50 questions (5 x 10)&lt;/li&gt; &lt;li&gt;Evaluated every answer (including their own)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So in total:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50 questions&lt;/li&gt; &lt;li&gt;500 answers&lt;/li&gt; &lt;li&gt;4830 evaluations (Should be 5000; I evaluated less answers with qwen3:1.7b and qwen3:4b as they do not generate scores and take a lot of time**)**&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And I tracked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token generation speed (tokens/sec)&lt;/li&gt; &lt;li&gt;tokens created&lt;/li&gt; &lt;li&gt;time taken&lt;/li&gt; &lt;li&gt;scored all answers for quality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt;, &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;Qwen3 1.7B&lt;/strong&gt; (LLaMA 3.2 1B hit 82 tokens/sec, avg is ~40 tokens/sec (for english topic question it reached &lt;strong&gt;146 tokens/sec)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Slowest: &lt;strong&gt;LLaMA 3.1 8B&lt;/strong&gt;, &lt;strong&gt;Qwen3 4B&lt;/strong&gt;, &lt;strong&gt;Mistral 7B&lt;/strong&gt; Qwen3 4B took &lt;strong&gt;486s&lt;/strong&gt; (8+ mins) to generate a single Math question!&lt;/li&gt; &lt;li&gt;Fun fact: deepseek-r1:1.5b, qwen3:4b and Qwen3:1.7B output &amp;lt;think&amp;gt; tags in questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answer Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt; and &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek got faster answering &lt;em&gt;its own&lt;/em&gt; questions (80 tokens/s vs. avg 40 tokens/s)&lt;/li&gt; &lt;li&gt;Qwen3 4B generates &lt;strong&gt;2‚Äì3x more tokens&lt;/strong&gt; per answer&lt;/li&gt; &lt;li&gt;Slowest: llama3.1:8b, qwen3:4b and mistral:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best scorer: Gemma3:latest ‚Äì consistent, numerical, no bias&lt;/li&gt; &lt;li&gt;Worst scorer: &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt; ‚Äì often skipped scores entirely&lt;/li&gt; &lt;li&gt;Bias detected: Many models &lt;strong&gt;rate their own answers higher&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek even evaluated some answers &lt;strong&gt;in Chinese&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;I did think of creating a control set of answers. I could tell the mdoel this is the perfect answer basis this rate others. But I did not because it would need support from a lot of people- creating perfect answer, which still can have a bias. I read a few answers and found most of them decent except math. So I tried to find which model's evaluation scores were closest to the average to determine a decent model for evaluation tasks(check last image)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fun Observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some models create &amp;lt;think&amp;gt; tags for questions, answers and even while evaluation as output&lt;/li&gt; &lt;li&gt;Score inflation is real: Mistral, Qwen3, and LLaMA 3.1 8B overrate themselves&lt;/li&gt; &lt;li&gt;Score formats vary wildly (text explanations vs. plain numbers)&lt;/li&gt; &lt;li&gt;Speed isn‚Äôt everything ‚Äì some slower models gave much higher quality answers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Best Performers (My Picks)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Best Model&lt;/th&gt; &lt;th align="left"&gt;Why&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 1B&lt;/td&gt; &lt;td align="left"&gt;Fast &amp;amp; relevant&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;Gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;Fast, accurate&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;Generates numerical scores and evaluations closest to model average&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Worst Surprises&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Problem&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;Qwen3 4B&lt;/td&gt; &lt;td align="left"&gt;Took 486s to generate 1 question&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;Slow&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;DeepSeek-R1 1.5B&lt;/td&gt; &lt;td align="left"&gt;Inconsistent, skipped scores&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Screenshots Galore&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm adding screenshots of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Questions generation&lt;/li&gt; &lt;li&gt;Answer comparisons&lt;/li&gt; &lt;li&gt;Evaluation outputs&lt;/li&gt; &lt;li&gt;Token/sec charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You &lt;strong&gt;can&lt;/strong&gt; run decent LLMs locally on M1 Air (8GB) ‚Äì if you pick the right ones&lt;/li&gt; &lt;li&gt;Model size ‚â† performance. Bigger isn't always better.&lt;/li&gt; &lt;li&gt;5 Models have a self bais, they rate their own answers higher than average scores. attaching screen shot of a table. Diagonal is their own evaluation, last column is average.&lt;/li&gt; &lt;li&gt;Models' evaluation has high variance! Every model has a unique distribution of the scores it gave.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post questions if you have any, I will try to answer.&lt;/p&gt; &lt;p&gt;Happy to share more data if you need.&lt;/p&gt; &lt;p&gt;Open to collaborate on interesting projects! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lmfiu9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T05:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln10a8</id>
    <title>A bunch of LLM FPHAM Python scripts I've added to my GitHub in recent days</title>
    <updated>2025-06-28T23:57:43+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feel free to downvote me into the gutter, but these are some of the latest Stupid FPHAM Crap (S-FPHAM_C) python scripts that I came up:&lt;/p&gt; &lt;p&gt;merge_lora_CPU&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/merge_lora_CPU"&gt;https://github.com/FartyPants/merge_lora_CPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LoRA merging with a base model, primarily designed for CPU&lt;/p&gt; &lt;p&gt;This script allows you to merge a PEFT (Parameter-Efficient Fine-Tuning) LoRA adapter with a base Hugging Face model. It can also be used to simply resave a base model, potentially changing its format (e.g., to SafeTensors) or data type.&lt;br /&gt; Oy, and it goes around the Tied Weights in safetensors which was introduced after the &amp;quot;recent Transformers happy update.&amp;quot;&lt;/p&gt; &lt;h1&gt;chonker&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/chonker"&gt;https://github.com/FartyPants/chonker&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Smart Text Chunker&lt;/h1&gt; &lt;p&gt;A &amp;quot;sophisticated&amp;quot; Python command-line tool for splitting large text files into smaller, more manageable chunks of, shall we say, semantic relevance. It's designed for preparing text datasets for training and fine-tuning Large Language Models (LLMs).&lt;/p&gt; &lt;h1&gt;mass_rewriter&lt;/h1&gt; &lt;p&gt;Extension for oobabooga WebUI&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/mass_rewriter"&gt;https://github.com/FartyPants/mass_rewriter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Version 2.0, now with better logic is here!&lt;br /&gt; This tool helps you automate the process of modifying text in bulk using an AI model. You can load plain text files or JSON datasets, apply various transformations, and then save the rewritten content.&lt;/p&gt; &lt;h1&gt;Axolotl_Loss_Graph&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/Axolotl_Loss_Graph"&gt;https://github.com/FartyPants/Axolotl_Loss_Graph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A handy, dinky-doo graph of your Axolotl training progress.&lt;br /&gt; It takes the data copied from the terminal output and makes a nice little&lt;br /&gt; loss graph in a PNG format that you can easily send to your friends&lt;br /&gt; showing them how training your Axolotl is going so well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln10a8/a_bunch_of_llm_fpham_python_scripts_ive_added_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln10a8/a_bunch_of_llm_fpham_python_scripts_ive_added_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln10a8/a_bunch_of_llm_fpham_python_scripts_ive_added_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T23:57:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmp3en</id>
    <title>support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp</title>
    <updated>2025-06-28T15:10:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"&gt; &lt;img alt="support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp" src="https://external-preview.redd.it/STjjFmknxf7nBEMMInmMUB27ROh3VGJuNDaQ8cvttgc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fba0076b0b7c05a00a73da6e0fe0aa6d24a9166" title="support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baidu has announced that it will officially release the ERNIE 4.5 models as open source on June 30, 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T15:10:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln5jqr</id>
    <title>Is ReAct still the best prompt template?</title>
    <updated>2025-06-29T04:04:10+00:00</updated>
    <author>
      <name>/u/Kooky-Net784</name>
      <uri>https://old.reddit.com/user/Kooky-Net784</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty much what the subject says ^^&lt;/p&gt; &lt;p&gt;Getting started with prompting a &amp;quot;naked&amp;quot; open-source LLM (Gemma 3) for function calling using a simple LangChain/Ollama setup in python and wondering what is the best prompt to maximize tool calling accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Net784"&gt; /u/Kooky-Net784 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5jqr/is_react_still_the_best_prompt_template/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5jqr/is_react_still_the_best_prompt_template/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5jqr/is_react_still_the_best_prompt_template/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T04:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln5l6b</id>
    <title>Training Open models on my data for replacing RAG</title>
    <updated>2025-06-29T04:06:25+00:00</updated>
    <author>
      <name>/u/help_all</name>
      <uri>https://old.reddit.com/user/help_all</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have RAG based solution for search on my products and domain knowledge data. we are right now using open AI api to do the search but cost is slowly becoming a concern. I want to see if this can be a good idea if I take a LLama model or some other open model and train it on our own data. Has anyone had success while doing this. Also please point me to effective documentation about on how it should be done. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/help_all"&gt; /u/help_all &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5l6b/training_open_models_on_my_data_for_replacing_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5l6b/training_open_models_on_my_data_for_replacing_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5l6b/training_open_models_on_my_data_for_replacing_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T04:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmqsru</id>
    <title>deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt</title>
    <updated>2025-06-28T16:21:43+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"&gt; &lt;img alt="deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt" src="https://b.thumbs.redditmedia.com/GzycAOTsFOjTvMP8TcwlR9GvTE-MtmC--vwGaVrlpzQ.jpg" title="deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An open weights model matching the best from closed AI. Seems quite impressive to me. What do you think? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mgu6oo7n1p9f1.png?width=2249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d375709b8e115ace177d0510bec0a16ad31d568e"&gt;https://preview.redd.it/mgu6oo7n1p9f1.png?width=2249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d375709b8e115ace177d0510bec0a16ad31d568e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T16:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmni3q</id>
    <title>What framework are you using to build AI Agents?</title>
    <updated>2025-06-28T14:00:09+00:00</updated>
    <author>
      <name>/u/PleasantInspection12</name>
      <uri>https://old.reddit.com/user/PleasantInspection12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, if anyone here is building AI Agents for production what framework are you using? For research and building leisure projects, I personally use langgraph. I wanted to also know if you are not using langgraph, what was the reason?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantInspection12"&gt; /u/PleasantInspection12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T14:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmk2dj</id>
    <title>Progress stalled in non-reasoning open-source models?</title>
    <updated>2025-06-28T10:58:35+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"&gt; &lt;img alt="Progress stalled in non-reasoning open-source models?" src="https://preview.redd.it/q53t8do2fn9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbf6bcfd1d93bd65c875ca994b48c3b38839c958" title="Progress stalled in non-reasoning open-source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if you've noticed, but a lot of model providers no longer explicitly note that their models are reasoning models (on benchmarks in particular). Reasoning models aren't ideal for every application.&lt;/p&gt; &lt;p&gt;I looked at the non-reasoning benchmarks on &lt;a href="https://artificialanalysis.ai/models/llama-4-maverick?model-filters=open-source%2Cnon-reasoning-models#artificial-analysis-intelligence-index-by-model-type"&gt;Artificial Analysis&lt;/a&gt; today and the top 2 models (performing comparable) are DeepSeek v3 and Llama 4 Maverick (which I heard was a flop?). I was surprised to see these 2 at the top.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q53t8do2fn9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T10:58:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln8uqb</id>
    <title>LM Studio vision models???</title>
    <updated>2025-06-29T07:32:07+00:00</updated>
    <author>
      <name>/u/BP_Ray</name>
      <uri>https://old.reddit.com/user/BP_Ray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, so I'm brand new to local LLMs, and as such I'm using LM Studio since It's easy to use.&lt;/p&gt; &lt;p&gt;But the thing is I need to use vision models, and while LM Studio has some, for the most part every one I try to use doesn't actually allow me to upload images as in doesn't give me the option at all. I'm mainly trying to use uncensored models, so the main staff-picked ones aren't suitable for my purpose.&lt;/p&gt; &lt;p&gt;Is there some reason why most of these don't work on LM Studio? Am I doing something wrong or is it LM Studio that is the problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BP_Ray"&gt; /u/BP_Ray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln8uqb/lm_studio_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln8uqb/lm_studio_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln8uqb/lm_studio_vision_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T07:32:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln93o3</id>
    <title>Is anyone here using Llama to code websites and apps? From my experience, it sucks</title>
    <updated>2025-06-29T07:48:53+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at &lt;a href="https://www.designarena.ai/models/llama-4-maverick"&gt;some examples from Llama 4&lt;/a&gt;, it seems absolutely horrific at any kind of UI/UX. Also on this &lt;a href="https://www.designarena.ai/leaderboard"&gt;benchmark for UI/UX&lt;/a&gt;, Llama 4 Maverick and Llama 4 Scout sit in the bottom 25% when compared to toher models such as GPT, Claude, Grok, etc. &lt;/p&gt; &lt;p&gt;What would you say are Llama's strengths are there if it's not coding interfaces and design? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T07:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln1a6u</id>
    <title>What's it currently like for people here running AMD GPUs with AI?</title>
    <updated>2025-06-29T00:11:26+00:00</updated>
    <author>
      <name>/u/83yWasTaken</name>
      <uri>https://old.reddit.com/user/83yWasTaken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How is the support?&lt;br /&gt; What is the performance loss?&lt;/p&gt; &lt;p&gt;I only really use LLM's with a RTX 3060 Ti, I was want to switch to AMD due to their open source drivers, I'll be using a mix of Linux &amp;amp; Windows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/83yWasTaken"&gt; /u/83yWasTaken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T00:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln1ij8</id>
    <title>RLHF from scratch, step-by-step, in 3 Jupyter notebooks</title>
    <updated>2025-06-29T00:23:15+00:00</updated>
    <author>
      <name>/u/ashz8888</name>
      <uri>https://old.reddit.com/user/ashz8888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently implemented Reinforcement Learning from Human Feedback (RLHF) fine-tuning, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face's GPT-2 model. The three steps are implemented in the three separate notebooks on GitHub: &lt;a href="https://github.com/ash80/RLHF_in_notebooks"&gt;https://github.com/ash80/RLHF_in_notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've also recorded a detailed video walkthrough (3+ hours) of the implementation on YouTube: &lt;a href="https://youtu.be/K1UBOodkqEk"&gt;https://youtu.be/K1UBOodkqEk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope this is helpful for anyone looking to explore RLHF. Feedback is welcome üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashz8888"&gt; /u/ashz8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T00:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln7rll</id>
    <title>I made a writing assistant Chrome extension. Completely free with Gemini Nano.</title>
    <updated>2025-06-29T06:20:00+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"&gt; &lt;img alt="I made a writing assistant Chrome extension. Completely free with Gemini Nano." src="https://external-preview.redd.it/aTR3azl2YzY3dDlmMRg_TmPcBoSM13pUYzKlWo7qhuAMWmP4IKxV8h55ZV-h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=966e826f016a8a652196742e287bca65c09c3a8d" title="I made a writing assistant Chrome extension. Completely free with Gemini Nano." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2f6200d67t9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T06:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmz4kf</id>
    <title>Transformer ASIC 500k tokens/s</title>
    <updated>2025-06-28T22:26:25+00:00</updated>
    <author>
      <name>/u/tvmaly</name>
      <uri>https://old.reddit.com/user/tvmaly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this company in a post where they are claiming 500k tokens/s on Llama 70B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.etched.com/blog-posts/oasis"&gt;https://www.etched.com/blog-posts/oasis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Impressive if true&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tvmaly"&gt; /u/tvmaly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T22:26:25+00:00</published>
  </entry>
</feed>
