<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-05T17:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1op55nk</id>
    <title>Are there still good models that aren‚Äôt chat finetuned?</title>
    <updated>2025-11-05T15:00:49+00:00</updated>
    <author>
      <name>/u/No-Yak4416</name>
      <uri>https://old.reddit.com/user/No-Yak4416</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking for 2 models that I can feed context and predict next few words, one should be 1-2b and the other should be 24-30b. I‚Äôm not an expert and it‚Äôs possible in my searches I‚Äôm just using the wrong terms&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Yak4416"&gt; /u/No-Yak4416 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op55nk/are_there_still_good_models_that_arent_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op55nk/are_there_still_good_models_that_arent_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op55nk/are_there_still_good_models_that_arent_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T15:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooyg33</id>
    <title>LMArena.ai Paradox: Votes Flow 24/7, But the Leaderboard is Frozen for Weeks. What's the Point?</title>
    <updated>2025-11-05T09:31:42+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I have a REALLY HUGE question for you guys. It's about &lt;a href="http://LMArena.ai"&gt;LMArena.ai&lt;/a&gt; and their absolutely weird ranking updates. I'm a regular there, and this whole setup just keeps breaking my brain, to be honest.&lt;/p&gt; &lt;p&gt;We keep voting in these &amp;quot;Battles&amp;quot; every single day, bringing them tons of super-fresh data on which LLMs people are into. But the leaderboard? BUT WHAT THE HELL!? It can just be frozen for weeks. That seriously pisses me off, and makes you wonder: can we even trust this site at all?&lt;br /&gt; -----------&lt;br /&gt; The Main Question: Why are We Wasting Time?&lt;/p&gt; &lt;p&gt;If my votes today aren't going to budge the rating for like, two weeks, what's the point of even showing up?! It honestly feels like the site is turning into some kind of shady data vacuum with zero real payback.&lt;/p&gt; &lt;p&gt;And seriously: if the admins are filtering those votes anyway, why not just put out an official statement about a schedule? Like, &amp;quot;updates strictly every Monday&amp;quot; or something? The lack of transparency is the biggest killer here.&lt;br /&gt; ----------&lt;br /&gt; The Elo Paradox&lt;/p&gt; &lt;p&gt;Logically, shouldn't those Elo scores be changing incrementally, little by little, as votes come in? But NO! They just dump a giant load of data at once, and BOOM! -ratings jump all over the place for absolutely no reason. This totally disconnects the rank from how the models are actually performing day-to-day. So we're just stuck staring at &amp;quot;yesterday's news&amp;quot; and we have no clue which model is actually crushing it right now.&lt;br /&gt; ----------&lt;br /&gt; The &amp;quot;Hype&amp;quot; Favoritism &lt;/p&gt; &lt;p&gt;This is the most annoying part.&lt;/p&gt; &lt;p&gt;When some super-hyped, new model drops (looking at you, Google or Anthropic), they throw it onto the board instantly. But what about smaller, Open-Source models????????? They can be left off for weeks, sometimes even longer. Seriously, it looks like they're just chasing commercial hype, instead of running a fair and consistent benchmark for everyone.&lt;br /&gt; ----------&lt;br /&gt; So, what do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooyg33/lmarenaai_paradox_votes_flow_247_but_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooyg33/lmarenaai_paradox_votes_flow_247_but_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooyg33/lmarenaai_paradox_votes_flow_247_but_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T09:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1onzrg9</id>
    <title>Qwen is roughly matching the entire American open model ecosystem today</title>
    <updated>2025-11-04T05:57:18+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt; &lt;img alt="Qwen is roughly matching the entire American open model ecosystem today" src="https://preview.redd.it/zvugibssj6zf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1b76885ebcc9a9fe34b1f3215330df073cc1f12" title="Qwen is roughly matching the entire American open model ecosystem today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvugibssj6zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomy4t</id>
    <title>NanoAgent ‚Äî A 135M Agentic LLM with Tool Calling That Runs on CPU</title>
    <updated>2025-11-04T23:27:16+00:00</updated>
    <author>
      <name>/u/TerribleDisaster0</name>
      <uri>https://old.reddit.com/user/TerribleDisaster0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I‚Äôm excited to share &lt;strong&gt;NanoAgent&lt;/strong&gt;, a &lt;strong&gt;135M parameter&lt;/strong&gt;, &lt;strong&gt;8k context&lt;/strong&gt; open-source model fine-tuned for &lt;strong&gt;agentic tasks&lt;/strong&gt; ‚Äî tool calling, instruction following, and lightweight reasoning ‚Äî all while being tiny enough (~135 MB in 8-bit) to run on a &lt;strong&gt;CPU or laptop&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs locally on CPU (tested on Mac M1, MLX framework)&lt;/li&gt; &lt;li&gt;Supports structured &lt;strong&gt;tool calling&lt;/strong&gt; (single &amp;amp; multi-tool)&lt;/li&gt; &lt;li&gt;Can parse &amp;amp; answer from web results via tools&lt;/li&gt; &lt;li&gt;Handles &lt;strong&gt;question decomposition&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ideal for &lt;strong&gt;edge AI agents&lt;/strong&gt;, &lt;strong&gt;copilots&lt;/strong&gt;, or &lt;strong&gt;IoT assistants&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/QuwsarOhi/NanoAgent"&gt;github.com/QuwsarOhi/NanoAgent&lt;/a&gt;&lt;br /&gt; Huggingface: &lt;a href="https://huggingface.co/quwsarohi/NanoAgent-135M"&gt;https://huggingface.co/quwsarohi/NanoAgent-135M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is still experimental and it is trained on limited resources. Will be very happy to have comments and feedbacks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerribleDisaster0"&gt; /u/TerribleDisaster0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1op7kmw</id>
    <title>What are some approaches taken for the problem of memory in LLMs?</title>
    <updated>2025-11-05T16:30:44+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long-term memory is currently one of the most important problems in LLMs.&lt;/p&gt; &lt;p&gt;What are some approaches taken by you or researchers to solve this problem?&lt;/p&gt; &lt;p&gt;For eg, using RAG, using summaries of context, making changes to the model architecture itself to store the memory in form of weights or cache. I very curious.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op7kmw/what_are_some_approaches_taken_for_the_problem_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op7kmw/what_are_some_approaches_taken_for_the_problem_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op7kmw/what_are_some_approaches_taken_for_the_problem_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T16:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oowt3g</id>
    <title>Testing local speech-to-speech on 8 GB Vram( RTX 4060).</title>
    <updated>2025-11-05T07:43:56+00:00</updated>
    <author>
      <name>/u/Icy_Gas8807</name>
      <uri>https://old.reddit.com/user/Icy_Gas8807</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowt3g/testing_local_speechtospeech_on_8_gb_vram_rtx_4060/"&gt; &lt;img alt="Testing local speech-to-speech on 8 GB Vram( RTX 4060)." src="https://external-preview.redd.it/aGdvNGU1Z3o0ZXpmMQtV4bEmE7OV1G8smVZqriF4a_nIyLOfzpAjqsBGXsQI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8a03dd69f43190f508d817b13a683765a3f3979" title="Testing local speech-to-speech on 8 GB Vram( RTX 4060)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the post last week regarding best TTS and STT models, forked the official hugging face repo on s2s -&amp;gt; &lt;a href="https://github.com/reenigne314/speech-to-speech.git"&gt;https://github.com/reenigne314/speech-to-speech.git&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;VAD -&amp;gt; mostly untouched except modified some deprecated package issues.&lt;/p&gt; &lt;p&gt;STT -&amp;gt; Still using whishper, most people preferred parakeet, but I faced some package dependency issues( I'll give it a shot again.) &lt;/p&gt; &lt;p&gt;LLM -&amp;gt; LM Studio(llamacpp) &amp;gt;&amp;gt;&amp;gt;&amp;gt; transformers, &lt;/p&gt; &lt;p&gt;TTS -&amp;gt; modified to Kokoro.&lt;/p&gt; &lt;p&gt;I even tried pushing it to use Granite 4H tiny(felt too professional), Gemma 3n E4B(not very satisfied). I stuck with Qwen3 4B despite it's urge to use emojis in every sentence( instructed not to use emojis twice in system prompt).&lt;/p&gt; &lt;p&gt;PS: I will try to run bigger models in my beelink strix halo and update you guys.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Gas8807"&gt; /u/Icy_Gas8807 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hu9907gz4ezf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowt3g/testing_local_speechtospeech_on_8_gb_vram_rtx_4060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oowt3g/testing_local_speechtospeech_on_8_gb_vram_rtx_4060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T07:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oowuqd</id>
    <title>Un-LOCC Wrapper: I built a Python library that compresses your OpenAI chats into images, saving up to 3√ó on tokens! (or even more :D)</title>
    <updated>2025-11-05T07:46:54+00:00</updated>
    <author>
      <name>/u/MaxDev0</name>
      <uri>https://old.reddit.com/user/MaxDev0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I turned my optical compression research into an &lt;strong&gt;actual Python library&lt;/strong&gt; that wraps the OpenAI SDK. Now you can compress large text contexts into images with a simple &lt;code&gt;compressed: True&lt;/code&gt; flag, achieving &lt;strong&gt;up to 2.8:1 token compression&lt;/strong&gt; while maintaining over &lt;strong&gt;93% accuracy&lt;/strong&gt;. Drop-in replacement for OpenAI client - sync/async support included.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC-Wrapper"&gt;https://github.com/MaxDevv/Un-LOCC-Wrapper&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What this is:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Un-LOCC Wrapper&lt;/strong&gt; - A Python library that takes my optical compression research and makes it &lt;strong&gt;actually usable&lt;/strong&gt; in your projects today. It's a simple wrapper around the OpenAI SDK that automatically converts text to compressed images when you add a &lt;code&gt;compressed: True&lt;/code&gt; flag.&lt;/p&gt; &lt;h1&gt;How it works:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Render text into optimized images (using research-tested fonts/sizes)&lt;/li&gt; &lt;li&gt;Pass images to Vision-Language Models instead of text tokens&lt;/li&gt; &lt;li&gt;Get the same responses while using WAY fewer tokens&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Code Example - It's this simple:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from un_locc import UnLOCC client = UnLOCC(api_key=&amp;quot;your-api-key&amp;quot;) # Compress large context with one flag messages = [ {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Summarize this document:&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: large_text, &amp;quot;compressed&amp;quot;: True} # ‚Üê That's it! ] response = client.chat.completions.create( model=&amp;quot;gpt-4o&amp;quot;, messages=messages ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Async version too:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from un_locc import AsyncUnLOCC client = AsyncUnLOCC(api_key=&amp;quot;your-api-key&amp;quot;) response = await client.chat.completions.create(...) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üöÄ &lt;strong&gt;Drop-in replacement&lt;/strong&gt; for OpenAI client&lt;/li&gt; &lt;li&gt;‚ö° &lt;strong&gt;Sync &amp;amp; async&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt;üéØ &lt;strong&gt;Research-backed defaults&lt;/strong&gt; (Atkinson Hyperlegible font, 864√ó864px, etc.)&lt;/li&gt; &lt;li&gt;üîß &lt;strong&gt;Customizable&lt;/strong&gt; - override any compression parameter&lt;/li&gt; &lt;li&gt;üìö &lt;strong&gt;Works with&lt;/strong&gt; chat completions &amp;amp; responses API&lt;/li&gt; &lt;li&gt;üèéÔ∏è &lt;strong&gt;Fast rendering&lt;/strong&gt; - ReportLab + pypdfium2 when available&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this matters:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pay ~3√ó less&lt;/strong&gt; for context tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extend context windows&lt;/strong&gt; without expensive upgrades&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Perfect for&lt;/strong&gt;: chat history compression, document analysis, large-context workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero model changes&lt;/strong&gt; - works with existing VLMs like GPT-4o&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Research Behind It:&lt;/h1&gt; &lt;p&gt;Based on my &lt;a href="https://github.com/MaxDevv/UN-LOCC"&gt;UN-LOCC research&lt;/a&gt; testing 90+ experiments across 6+ VLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.0 Flash Lite&lt;/strong&gt;: 93.65% accuracy @ 2.8:1 compression&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen2.5-VL-72B&lt;/strong&gt;: 99.26% accuracy @ 1.7:1 compression&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-VL-235B&lt;/strong&gt;: 95.24% accuracy @ 2.2:1 compression&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Install &amp;amp; Try:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install un-locc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The library handles all the complexity - fonts, rendering optimization, content type detection. You just add &lt;code&gt;compressed: True&lt;/code&gt; and watch your token usage plummet.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub repo (stars help a ton!):&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC-Wrapper"&gt;https://github.com/MaxDevv/Un-LOCC-Wrapper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Note&lt;/strong&gt;: While testing the library beyond my original research, I discovered that the compression limits are actually MUCH higher than the conservative 3x I reported. Gemini was consistently understanding text and accurately reading back sentences at &lt;strong&gt;6x compression&lt;/strong&gt; without issues. The 3x figure was just my research cutoff for quantifiable accuracy metrics, but for real-world use cases where perfect character-level retrieval isn't critical, we're looking at, maybe something like... &lt;strong&gt;6-7x compression&lt;/strong&gt; lol :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaxDev0"&gt; /u/MaxDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowuqd/unlocc_wrapper_i_built_a_python_library_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowuqd/unlocc_wrapper_i_built_a_python_library_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oowuqd/unlocc_wrapper_i_built_a_python_library_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T07:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oonomc</id>
    <title>Why the Strix Halo is a poor purchase for most people</title>
    <updated>2025-11-04T23:59:12+00:00</updated>
    <author>
      <name>/u/NeverEnPassant</name>
      <uri>https://old.reddit.com/user/NeverEnPassant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of posts that promote the Strix Halo as a good purchase, and I've often wondered if I should have purchased that myself. I've since learned a lot about how these models are executed. In this post I would like share empircal measurements, where I think those numbers come from, and make the case that few people should be purchasing this system. I hope you find it helpful!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Model under test&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama.cpp&lt;/li&gt; &lt;li&gt;Gpt-oss-120b&lt;/li&gt; &lt;li&gt;One the highest quality models that can run on mid range hardware. &lt;/li&gt; &lt;li&gt;Total size for this model is ~59GB and ~57GB of that are expert layers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Systems under test&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;First system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;128GB Strix Halo&lt;/li&gt; &lt;li&gt;Quad channel LDDR5-8000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Second System (my system):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dual channel DDR5-6000 + pcie5 x16 + an rtx 5090&lt;/li&gt; &lt;li&gt;An rtx 5090 with the largest context size requires about 2/3 of the experts (38GB of data) to live in system RAM.&lt;/li&gt; &lt;li&gt;cuda backed&lt;/li&gt; &lt;li&gt;mmap off&lt;/li&gt; &lt;li&gt;batch 4096&lt;/li&gt; &lt;li&gt;ubatch 4096&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Here are user submitted numbers for the Strix Halo:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;997.70 ¬± 0.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;46.18 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d20000&lt;/td&gt; &lt;td align="right"&gt;364.25 ¬± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d20000&lt;/td&gt; &lt;td align="right"&gt;18.16 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d48000&lt;/td&gt; &lt;td align="right"&gt;183.86 ¬± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="right"&gt;10.80 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;What can we learn from this?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Performance is acceptable only at context 0. As context grows performance drops off a cliff for both prefill and decode.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;And here are numbers from my system:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;4065.77 ¬± 25.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.35 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d20000&lt;/td&gt; &lt;td align="right"&gt;3267.95 ¬± 27.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d20000&lt;/td&gt; &lt;td align="right"&gt;36.96 ¬± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d48000&lt;/td&gt; &lt;td align="right"&gt;2497.25 ¬± 66.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="right"&gt;35.18 ¬± 0.62&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Wait a second, how are the decode numbers so close at context 0? The strix Halo has memory that is 2.5x faster than my system.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let's look closer at gpt-oss-120b. This model is 59 GB in size. There is roughly 0.76GB of layer data that is read for every single token. Since &lt;em&gt;every&lt;/em&gt; token needs this data, it is kept in VRAM. Each token also needs to read 4 arbitrary experts which is an additional 1.78 GB. Considering we can fit 1/3 of the experts in VRAM, this brings the total split to 1.35GB in VRAM and 1.18GB in system RAM at context 0.&lt;/p&gt; &lt;p&gt;Now VRAM on a 5090 is &lt;em&gt;much&lt;/em&gt; faster than both the Strix Halo unified memory and also dual channel DDR5-6000. When all is said and done, doing ~53% of your reads in ultra fast VRAM and 47% of your reads in somewhat slow system RAM, the decode time is roughly equal (a touch slower) than doing all your reads in Strix Halo's moderately fast memory.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Why does the Strix Halo have such a large slowdown in decode with large context?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's because when your context size grows, decode must also read the KV Cache once per layer. At 20k context, that is an extra ~4GB per token that needs to be read! Simple math (2.54 / 6.54) shows it should be run 0.38x as fast as context 0, and is almost exactly what we see in the chart above.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;And why does my system have a large lead in decode at larger context sizes?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's because all the KV Cache is stored in VRAM, which has ultra fast memory read. The decode time is dominated by the slow memory read in system RAM, so this barely moves the needle.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Why do prefill times degrade so quickly on the Strix Halo?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Good question! I would love to know!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Can I just add a GPU to the Strix Halo machine to improve my prefill?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unfortunately not. The ability to leverage a GPU to improve prefill times depends heavily on the pcie bandwidth and the Strix Halo only offers pcie x4.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Real world measurements of the effect of pcie bandwidth on prefill&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;These tests were performed by changing BIOS settings on my machine.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;config&lt;/th&gt; &lt;th align="right"&gt;prefill tps&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;pcie5 x16&lt;/td&gt; &lt;td align="right"&gt;~4100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pcie4 x16&lt;/td&gt; &lt;td align="right"&gt;~2700&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pcie4 x4&lt;/td&gt; &lt;td align="right"&gt;~1000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Why is pci bandwidth so important?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is my best high level understanding of what llama.cpp does with a gpu + cpu moe:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First it runs the router on all 4096 tokens to determine what experts it needs for each token.&lt;/li&gt; &lt;li&gt;Each token will use 4 of 128 experts, so on average each expert will map to 128 tokens (4096 * 4 / 128).&lt;/li&gt; &lt;li&gt;Then for each expert, upload the weights to the GPU and run on all tokens that need that expert.&lt;/li&gt; &lt;li&gt;This is well worth it because prefill is compute intensive and just running it on the CPU is much slower.&lt;/li&gt; &lt;li&gt;This process is pipelined: you upload the weights for the next token, when running compute for the current.&lt;/li&gt; &lt;li&gt;Now all experts for gpt-oss-120b is ~57GB. That will take ~0.9s to upload using pcie5 x16 at its maximum 64GB/s. That places a ceiling in pp of ~4600tps.&lt;/li&gt; &lt;li&gt;For pcie4 x16 you will only get 32GB/s, so your maximum is ~2300tps. For pcie4 x4 like the Strix Halo via occulink, its 1/4 of this number.&lt;/li&gt; &lt;li&gt;In practice neither will get their full bandwidth, but the absolute ratios hold.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Other benefits of a normal computer with a rtx 5090&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better cooling&lt;/li&gt; &lt;li&gt;Higher quality case&lt;/li&gt; &lt;li&gt;A 5090 will almost certainly have higher resale value than a Strix Halo machine&lt;/li&gt; &lt;li&gt;More extensible&lt;/li&gt; &lt;li&gt;More powerful CPU&lt;/li&gt; &lt;li&gt;Top tier gaming&lt;/li&gt; &lt;li&gt;Models that fit entirely in VRAM will absolutely &lt;em&gt;fly&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Image generation will be much much faster.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;What is Strix Halo good for&lt;/em&gt;&lt;/strong&gt;*&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extremely low idle power usage&lt;/li&gt; &lt;li&gt;It's small&lt;/li&gt; &lt;li&gt;Maybe all you care about is chat bots with close to 0 context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you can afford an extra $1000-1500, you are much better off just building a normal computer with an rtx 5090. The value per dollar is just so much stronger. Even if you don't want to spend that kind of money, you should ask yourself if your use case is actually covered by the Strix Halo. Maybe buy nothing instead.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Corrections&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Please correct me on anything I got wrong! I am just a novice!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I received a message that maybe llama.cpp + Strix Halo is not (fully?) leveraging it's NPU now, which should improve prefill numbers (but not decode). If anyone knows more about this or has preliminary benchmarks, please share them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Updated numbers from the latest llama someone commented here:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_batch&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;1012.63 ¬± 0.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;52.31 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp4096 @ d20000&lt;/td&gt; &lt;td align="right"&gt;357.27 ¬± 0.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d20000&lt;/td&gt; &lt;td align="right"&gt;32.46 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp4096 @ d48000&lt;/td&gt; &lt;td align="right"&gt;230.60 ¬± 0.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="right"&gt;32.76 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;WOW! The ddr5 kit I purchased in June has &lt;a href="https://camelcamelcamel.com/product/B0DFMFBVYP"&gt;doubled&lt;/a&gt; in price since I bought it. Maybe 50% more is now an underestimate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeverEnPassant"&gt; /u/NeverEnPassant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oonomc/why_the_strix_halo_is_a_poor_purchase_for_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oonomc/why_the_strix_halo_is_a_poor_purchase_for_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oonomc/why_the_strix_halo_is_a_poor_purchase_for_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:59:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1op2i14</id>
    <title>Best model to run on dual 3090 (48GB vram)</title>
    <updated>2025-11-05T13:13:42+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would be your model of choice if you had a 48GB VRAM setup on your desk? In my case it's dual 3090. &lt;/p&gt; &lt;p&gt;For coding I'm leaning towards qwen3-coder:30b-a3b-q8_0 after using qwen2.5-coder:32b-instruct-q8_0&lt;/p&gt; &lt;p&gt;For general chat mostly about work/software/cloud related topics can't decicde between qwq:32b-q8_0 and qwen2.5:72b-instruct-q4_0, i guess more parameters are better but output from qwq is often quite good &lt;/p&gt; &lt;p&gt;Any opinions? Are there other models that can outperform qwen locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2i14/best_model_to_run_on_dual_3090_48gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2i14/best_model_to_run_on_dual_3090_48gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op2i14/best_model_to_run_on_dual_3090_48gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T13:13:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooi8lk</id>
    <title>I built a leaderboard for Rerankers</title>
    <updated>2025-11-04T20:24:00+00:00</updated>
    <author>
      <name>/u/tifa2up</name>
      <uri>https://old.reddit.com/user/tifa2up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooi8lk/i_built_a_leaderboard_for_rerankers/"&gt; &lt;img alt="I built a leaderboard for Rerankers" src="https://preview.redd.it/lrdfuzpduazf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43e22daf4653431072cda6072129aec0e4f3f7e9" title="I built a leaderboard for Rerankers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is something that I wish I had when starting out.&lt;/p&gt; &lt;p&gt;When I built my first RAG project, I didn‚Äôt know what a reranker was. When I added one, I was blown away by how much of a quality improvement it added. Just 5 lines of code.&lt;/p&gt; &lt;p&gt;Like most people here, I defaulted to Cohere as it was the most popular.&lt;/p&gt; &lt;p&gt;Turns out there are better rerankers out there (and cheaper).&lt;/p&gt; &lt;p&gt;I built a leaderboard with the top reranking models: elo, accuracy, and latency compared.&lt;/p&gt; &lt;p&gt;I‚Äôll be keeping the leaderboard updated as new rerankers enter the arena. Let me kow if I should add any other ones.&lt;/p&gt; &lt;p&gt;&lt;a href="https://agentset.ai/leaderboard/rerankers"&gt;https://agentset.ai/leaderboard/rerankers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tifa2up"&gt; /u/tifa2up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lrdfuzpduazf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooi8lk/i_built_a_leaderboard_for_rerankers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooi8lk/i_built_a_leaderboard_for_rerankers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T20:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1op3g6z</id>
    <title>Kimi Thinking When?</title>
    <updated>2025-11-05T13:52:55+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"&gt; &lt;img alt="Kimi Thinking When?" src="https://b.thumbs.redditmedia.com/MndVbBgYMkDWeRdjxPeCx0iqNEf3LEN1ZZjUFdJ-Z2k.jpg" title="Kimi Thinking When?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vooceqxl1gzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c26cd866dae399e81ce33c794e17c2f9f1c5df04"&gt;https://preview.redd.it/vooceqxl1gzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c26cd866dae399e81ce33c794e17c2f9f1c5df04&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I really like Kimi K2. It‚Äôs way more emotionally intelligent than any other AI I‚Äôve tried. like, it never flatters me or sugarcoats things. If I mess up, it‚Äôll directly tell me that actually helps me improve. That kind of trust is rare.&lt;/p&gt; &lt;p&gt;I‚Äôm just sitting here wondering‚Ä¶ Kimi thinking when?&lt;/p&gt; &lt;p&gt;btw, if fix the hallucination issues, I swear this thing will be unstoppable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T13:52:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo6226</id>
    <title>Disappointed by dgx spark</title>
    <updated>2025-11-04T12:43:28+00:00</updated>
    <author>
      <name>/u/RockstarVP</name>
      <uri>https://old.reddit.com/user/RockstarVP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"&gt; &lt;img alt="Disappointed by dgx spark" src="https://preview.redd.it/a1tbzs1dk8zf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=090e0bdb3a3f9757ae6bdbff3964dc951a1361ed" title="Disappointed by dgx spark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just tried Nvidia dgx spark irl&lt;/p&gt; &lt;p&gt;gorgeous golden glow, feels like gpu royalty&lt;/p&gt; &lt;p&gt;‚Ä¶but 128gb shared ram still underperform whenrunning qwen 30b with context on vllm&lt;/p&gt; &lt;p&gt;for 5k usd, 3090 still king if you value raw speed over design&lt;/p&gt; &lt;p&gt;anyway, wont replce my mac anytime soon&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RockstarVP"&gt; /u/RockstarVP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1tbzs1dk8zf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T12:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooymcm</id>
    <title>Hephaestus: AI workflows that discover and create their own tasks as they work</title>
    <updated>2025-11-05T09:43:16+00:00</updated>
    <author>
      <name>/u/Standard_Excuse7988</name>
      <uri>https://old.reddit.com/user/Standard_Excuse7988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooymcm/hephaestus_ai_workflows_that_discover_and_create/"&gt; &lt;img alt="Hephaestus: AI workflows that discover and create their own tasks as they work" src="https://external-preview.redd.it/Y3kwMnpjZGtyZXpmMdAkQHndWWJHnvJDGx1uxfxLVUHIG9vtIHStiltBck3-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5ca7c1dc4a5849323fd49664a100572b45c26c5" title="Hephaestus: AI workflows that discover and create their own tasks as they work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;A week ago I shared Hephaestus - an open-source framework where AI agents dynamically build workflows based on what they discover. The response has been incredible (500+ stars already!)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Idea:&lt;/strong&gt; Instead of predefining every task upfront, you define &lt;em&gt;phase types&lt;/em&gt; (like &amp;quot;Analyze ‚Üí Implement ‚Üí Test&amp;quot;), and agents create specific tasks across these phases based on what they discover as they work.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real Example:&lt;/strong&gt; Give it a PRD for &amp;quot;Build a REST API with authentication.&amp;quot; A Phase 1 agent analyzes it and spawns 5 implementation tasks (auth system, database, API layer, tests, deployment). A Phase 3 validation agent testing the auth system discovers an elegant caching pattern that could speed up all API routes by 40%. Instead of being stuck or following rigid branching logic, it spawns a Phase 1 investigation task. Another agent picks it up, confirms it's viable, spawns a Phase 2 implementation task. The workflow just branched itself based on discovery.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different:&lt;/strong&gt; - üîÑ &lt;strong&gt;Self-building workflows&lt;/strong&gt; - Agents spawn tasks dynamically, not predefined branches - üß† &lt;strong&gt;RAG-powered coordination&lt;/strong&gt; - Agents share discoveries through semantic memory - üéØ &lt;strong&gt;Guardian monitoring&lt;/strong&gt; - Continuously tracks agent trajectories to prevent drift - üìä &lt;strong&gt;Kanban coordination&lt;/strong&gt; - Real-time task management with blocking relationships - And so much more...&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ido-Levi/Hephaestus"&gt;https://github.com/Ido-Levi/Hephaestus&lt;/a&gt; üìö &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://ido-levi.github.io/Hephaestus/"&gt;https://ido-levi.github.io/Hephaestus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fair warning: This is still new and rough around the edges. Issues and feedback are very welcome, and I'm happy to review contributions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Excuse7988"&gt; /u/Standard_Excuse7988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fdwlhddkrezf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooymcm/hephaestus_ai_workflows_that_discover_and_create/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooymcm/hephaestus_ai_workflows_that_discover_and_create/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T09:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomxt6</id>
    <title>Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)</title>
    <updated>2025-11-04T23:26:53+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"&gt; &lt;img alt="Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)" src="https://preview.redd.it/tu7jitwzqbzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=952d82b0d88e6a4bfa60cec0ff55072522800b4c" title="Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;STAY CALM! &lt;a href="https://arxiv.org/abs/2510.27688"&gt;https://arxiv.org/abs/2510.27688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tu7jitwzqbzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oogvcw</id>
    <title>I implemented GPT-OSS from scratch in pure Python, without PyTorch or a GPU</title>
    <updated>2025-11-04T19:33:07+00:00</updated>
    <author>
      <name>/u/ultimate_code</name>
      <uri>https://old.reddit.com/user/ultimate_code</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have also written a detailed and beginner friendly blog that explains every single concept, from simple modules such as Softmax and RMSNorm, to more advanced ones like Grouped Query Attention. I tried to justify the architectural decision behind every layer as well. &lt;/p&gt; &lt;p&gt;Key concepts: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Grouped Query Attention: with attention sinks and sliding window.&lt;/li&gt; &lt;li&gt;Mixture of Experts (MoE).&lt;/li&gt; &lt;li&gt;Rotary Position Embeddings (RoPE): with NTK-aware scaling.&lt;/li&gt; &lt;li&gt;Functional Modules: SwiGLU, RMSNorm, Softmax, Linear Layer.&lt;/li&gt; &lt;li&gt;Custom BFloat16 implementation in C++ for numerical precision.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you‚Äôve ever wanted to understand how modern LLMs really work, this repo + blog walk you through everything. I have also made sure that the implementation matches the official one in terms of numerical precision (check the &lt;a href="http://test.py"&gt;test.py&lt;/a&gt; file)&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://projektjoe.com/blog/gptoss"&gt;https://projektjoe.com/blog/gptoss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/projektjoe/gpt-oss"&gt;https://github.com/projektjoe/gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love any feedback, ideas for extensions, or just thoughts from others exploring transformers from first principles!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ultimate_code"&gt; /u/ultimate_code &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T19:33:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1op2d1a</id>
    <title>I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!</title>
    <updated>2025-11-05T13:07:46+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"&gt; &lt;img alt="I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!" src="https://preview.redd.it/7xx856mftfzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7a93e1d29328d3af61a2f1b635a73c9abf0f570" title="I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on: a full, beginner-friendly tutorial for fine-tuning the &lt;strong&gt;Qwen2.5-Coder-1.5B&lt;/strong&gt; model for a real-world task (Chinese sentiment analysis).&lt;/p&gt; &lt;p&gt;The best part? &lt;strong&gt;You can run the entire thing on a free Google Colab T4 GPU in about 20-30 minutes.&lt;/strong&gt; No local setup needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FIIIIQIIII%2FMSJ-Factory"&gt;https://github.com/IIIIQIIII/MSJ-Factory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚ñ∂Ô∏è Try it now on Google Colab:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2FIIIIQIIII%2FMSJ-Factory%2Fblob%2Fmain%2FQwen2_5_Sentiment_Fine_tuning_Tutorial.ipynb"&gt;https://colab.research.google.com/github/IIIIQIIII/MSJ-Factory/blob/main/Qwen2_5_Sentiment_Fine_tuning_Tutorial.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's inside:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;One-Click Colab Notebook:&lt;/strong&gt; The link above takes you straight there. Just open and run.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Freeze Training Method:&lt;/strong&gt; I only train the last 6 layers. It's super fast, uses ~9GB VRAM, and still gives amazing results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clear Results:&lt;/strong&gt; I was able to boost accuracy on the test set from &lt;strong&gt;91.6% to 97.8%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Walkthrough:&lt;/strong&gt; From cloning the repo, to training, evaluating, and even uploading your final model to Hugging Face, all within the notebook.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried to make this as easy as possible for anyone who wants to get their hands dirty with fine-tuning but might not have a beefy GPU at home. This method is great for my own quick experiments and for adapting models to new domains without needing an A100.&lt;/p&gt; &lt;p&gt;Hope you find it useful! Let me know if you have any feedback or questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7xx856mftfzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T13:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomyby</id>
    <title>Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment</title>
    <updated>2025-11-04T23:27:29+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"&gt; &lt;img alt="Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment" src="https://external-preview.redd.it/ZoS_79jqnoHBV3LsnI-z672RSVTI_TXKjzXarTZWduA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c34b5423e4b02583d66bdf088e63a71b2cb2167" title="Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/storage/server-dram-prices-surge-50-percent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1op0yep</id>
    <title>Build a DeepSeek Model from Scratch: A Book</title>
    <updated>2025-11-05T12:01:28+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"&gt; &lt;img alt="Build a DeepSeek Model from Scratch: A Book" src="https://b.thumbs.redditmedia.com/QDV-FHGI3pJaqfrG5j7eQL0VW9Xx6TSUjjj1pRQi0Ac.jpg" title="Build a DeepSeek Model from Scratch: A Book" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/13tb69tzhfzf1.jpg?width=2213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8efaf667406076744b34c26b91e755e81e2d8683"&gt;https://preview.redd.it/13tb69tzhfzf1.jpg?width=2213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8efaf667406076744b34c26b91e755e81e2d8683&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the first book which teaches everyone how to build your own DeepSeek model completely from scratch, on your local computer!&lt;/p&gt; &lt;p&gt;The idea for this book grew out of our YouTube series ‚ÄúVizuara‚Äôs Build DeepSeek from Scratch‚Äù which launched in February 2025. The series showed a clear demand for hands-on, first-principles material, encouraging us to create this more structured and detailed written guide.&lt;/p&gt; &lt;p&gt;We have worked super hard for 8 months on this project. &lt;/p&gt; &lt;p&gt;The book is structured around a four-stage roadmap, covering the innovations in a logical order:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The foundational Key-Value (KV) Cache for efficient inference.&lt;/li&gt; &lt;li&gt;The core architectural components: Multi-Head Latent Attention (MLA) and Deepseek&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Mixture-of-Experts (MoE).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Advanced training techniques, including Multi-Token Prediction (MTP) and FP8 quantization.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Post-training methods like Reinforcement Learning (RL) and Knowledge Distillation.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T12:01:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooa342</id>
    <title>llama.cpp releases new official WebUI</title>
    <updated>2025-11-04T15:26:30+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"&gt; &lt;img alt="llama.cpp releases new official WebUI" src="https://external-preview.redd.it/3mPqb7hXnKE3QMeOYvmnNH3HEJEfsY-FkGb0pZ8tDhU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23cd274a23b9ffd23182c3f9522388baf8354b97" title="llama.cpp releases new official WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16938"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T15:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1op73qb</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-11-05T16:13:48+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/YzA4MDdtb3NxZ3pmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6be507f0bbf2370a00306c48b3deca6f3dbc2931" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p5a328wsqgzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T16:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oojwpj</id>
    <title>The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency</title>
    <updated>2025-11-04T21:30:05+00:00</updated>
    <author>
      <name>/u/Imakerocketengine</name>
      <uri>https://old.reddit.com/user/Imakerocketengine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"&gt; &lt;img alt="The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency" src="https://b.thumbs.redditmedia.com/z3kkwZtyFH5pRyg7rq5O0EZiOQNj_2JQdPBRk1aeUeQ.jpg" title="The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://comparia.beta.gouv.fr/"&gt;https://comparia.beta.gouv.fr/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imakerocketengine"&gt; /u/Imakerocketengine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oojwpj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T21:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oozb8v</id>
    <title>aquif-3.5-Max-42B-A3B</title>
    <updated>2025-11-05T10:27:09+00:00</updated>
    <author>
      <name>/u/CoruNethronX</name>
      <uri>https://old.reddit.com/user/CoruNethronX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"&gt; &lt;img alt="aquif-3.5-Max-42B-A3B" src="https://external-preview.redd.it/iQbBlhyprDj6yO7lC6_VBVFWSoqEqFZE8HYk4JH2vHI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf206a8e60f2f9eb24b0998337979fd6ea8d653c" title="aquif-3.5-Max-42B-A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beats GLM 4.6 according to provided benchmarks Million context Apache 2.0 Works both with GGUF/llama.cpp and MLX/lmstudio out-of-box, as it's qwen3_moe architecture&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoruNethronX"&gt; /u/CoruNethronX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/aquif-ai/aquif-3.5-Max-42B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T10:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1op0j6j</id>
    <title>Recent VRAM Poll results</title>
    <updated>2025-11-05T11:38:38+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"&gt; &lt;img alt="Recent VRAM Poll results" src="https://preview.redd.it/i3y27zfpbfzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0823c607489b2850e90a3ca955b51e1f4428ecdf" title="Recent VRAM Poll results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1olildc/comment/nmi8ftm/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;As mentioned in that post&lt;/a&gt;, That poll missed below ranges.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;9-11GB&lt;/li&gt; &lt;li&gt;25-31GB&lt;/li&gt; &lt;li&gt;97-127GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Poll Results below:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0-8GB - &lt;strong&gt;718&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;12-24GB - &lt;strong&gt;1.1K&lt;/strong&gt; - I think some 10GB folks might have picked this option so this range came with big number.&lt;/li&gt; &lt;li&gt;32-48GB - &lt;strong&gt;348&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;48-96GB - &lt;strong&gt;284&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;128-256GB - &lt;strong&gt;138&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;256+ - &lt;strong&gt;93&lt;/strong&gt; - &lt;sup&gt;Last month someone asked me &amp;quot;Why are you calling yourself GPU Poor when you have 8GB VRAM&amp;quot;&lt;/sup&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next time onwards below ranges would be better to get better results as it covers all ranges. And this would be more useful for Model creators &amp;amp; Finetuners to pick better model sizes/types(MOE or Dense).&lt;/p&gt; &lt;p&gt;&lt;sup&gt;FYI Poll has only 6 options, otherwise I would add more ranges.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~12GB&lt;/li&gt; &lt;li&gt;13-32GB&lt;/li&gt; &lt;li&gt;33-64GB&lt;/li&gt; &lt;li&gt;65-96GB&lt;/li&gt; &lt;li&gt;97-128GB&lt;/li&gt; &lt;li&gt;128GB+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;RAM&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~32GB&lt;/li&gt; &lt;li&gt;33-64GB&lt;/li&gt; &lt;li&gt;65-128GB&lt;/li&gt; &lt;li&gt;129-256GB&lt;/li&gt; &lt;li&gt;257-512GB&lt;/li&gt; &lt;li&gt;513-1TB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Somebody please post above poll threads coming week. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i3y27zfpbfzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T11:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oosnaq</id>
    <title>New Qwen models are unbearable</title>
    <updated>2025-11-05T03:45:53+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using GPT-OSS-120B for the last couple months and recently thought I'd try Qwen3 32b VL and Qwen3 Next 80B. &lt;/p&gt; &lt;p&gt;They honestly might be worse than peak ChatGPT 4o. &lt;/p&gt; &lt;p&gt;Calling me a genius, telling me every idea of mine is brilliant, &amp;quot;this isnt just a great idea‚Äîyou're redefining what it means to be a software developer&amp;quot; type shit&lt;/p&gt; &lt;p&gt;I cant use these models because I cant trust them at all. They just agree with literally everything I say. &lt;/p&gt; &lt;p&gt;Has anyone found a way to make these models more usable? They have good benchmark scores so perhaps im not using them correctly&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T03:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooxple</id>
    <title>GLM 4.6 AIR is coming....?</title>
    <updated>2025-11-05T08:43:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt; &lt;img alt="GLM 4.6 AIR is coming....?" src="https://preview.redd.it/56uu0u1fiezf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4748db1df48b7ce94e935ab40a291000e001166f" title="GLM 4.6 AIR is coming....?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;or not yet? What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56uu0u1fiezf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T08:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
