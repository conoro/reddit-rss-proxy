<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-18T19:07:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mt6l87</id>
    <title>NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text</title>
    <updated>2025-08-17T23:53:47+00:00</updated>
    <author>
      <name>/u/RYSKZ</name>
      <uri>https://old.reddit.com/user/RYSKZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"&gt; &lt;img alt="NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text" src="https://external-preview.redd.it/A88FneA2E8vj8FZkQqex3y_zTUkEEvLZOR75ND29msM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faf8982f5868ceffad20906f1a118bd04e77ded1" title="NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA has launched &lt;strong&gt;Granary&lt;/strong&gt;, a massive open-source multilingual speech dataset with 1M hours of audio, supporting 25 European languages, including low-resource ones like Croatian, Estonian, and Maltese.&lt;/p&gt; &lt;p&gt;Alongside it, NVIDIA released &lt;strong&gt;two high-performance STT models&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Canary-1b-v2&lt;/strong&gt;: 1B parameters, top accuracy on Hugging Face for multilingual speech recognition, translating between English and 24 languages, 10√ó faster inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parakeet-tdt-0.6b-v3&lt;/strong&gt;: 600M parameters, designed for real-time and large-scale transcription with highest throughput in its class.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hugging Face links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Granary: &lt;a href="https://huggingface.co/datasets/nvidia/Granary"&gt;https://huggingface.co/datasets/nvidia/Granary&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Canary-1b-v2: &lt;a href="https://huggingface.co/nvidia/canary-1b-v2"&gt;https://huggingface.co/nvidia/canary-1b-v2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Parakeet-tdt-0.6b-v3: &lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RYSKZ"&gt; /u/RYSKZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/speech-ai-dataset-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T23:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt2iev</id>
    <title>GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure.</title>
    <updated>2025-08-17T21:01:10+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt; &lt;img alt="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." src="https://external-preview.redd.it/LOoaiYGJSklUhS_jyXNZtXqZW5tq1NuC9Dm2RNcy-8Q.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fb11e05f1a215e4e59de75726903bb0ecb7f1d6" title="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was doing some tool calling tests while figuring out how to work with the Harmony GPT-OSS prompt format. I made a little helpful tool here if you're trying to understand how harmony works (there's a whole repo there too with a bit deeper exploration if you're curious):&lt;br /&gt; &lt;a href="https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html"&gt;https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I wanted to benchmark the system so I asked it to make a fun benchmark, and this is what it came up with. In this video, missiles are falling from the sky and the agent has to see their trajectory and speed, run a tool call with python to anticipate where the missile will be in the future, and fire an explosive anti-missile at it so that it can hit the spot it'll be when the missile arrives. To do this, it needs to have low latency, understand its own latency, and be able to RAPIDLY fire off tool calls. This is firing with 100% accuracy (it technically missed 10 tool calls along the way but was able to recover and fire them before the missiles hit the ground).&lt;/p&gt; &lt;p&gt;So... here's GPT-OSS-20b running 100 agents simultaneously at 131,076 token context, each agent with its own 131k context window, each hitting sub-100ms ttft, blowing everything out of the sky at 10k tokens/second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=8T8drT0rwCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt3epi</id>
    <title>M4 Max generation speed vs context size</title>
    <updated>2025-08-17T21:37:31+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"&gt; &lt;img alt="M4 Max generation speed vs context size" src="https://b.thumbs.redditmedia.com/0KFbAMMgVPnCWsrraGEmcRNBchKGkvvSro4EHgyP7yQ.jpg" title="M4 Max generation speed vs context size" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a custom benchmark program to map out generation speed vs context size. The program will build up a prompt 10k tokens at a time and log the reported stats from LM Studio. The intention is to simulate agentic coding. Cline/Roo/Kilo use about 20k tokens for the system prompt.&lt;/p&gt; &lt;p&gt;Better images here: &lt;a href="https://oz9h.dk/benchmark/"&gt;https://oz9h.dk/benchmark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My computer is the M4 Max Macbook Pro 128 GB. All models at 4 bit quantization. KV-Cache at 8 bit.&lt;/p&gt; &lt;p&gt;I am quite sad that GLM 4.5 Air degrades so quickly. And impressed that GPT-OSS 120b manages to stay fast even with 100k context. I don't use Qwen3-Coder 30b-a3b much but I am still surprised at how quickly it crashes and it even gets slower than GPT-OSS - a model 4 times larger. And my old workhorse Devstral somehow manages to be the most consistent model regarding speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt3epi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtktjm</id>
    <title>my 2.4b llm in korean</title>
    <updated>2025-08-18T12:36:57+00:00</updated>
    <author>
      <name>/u/Patience2277</name>
      <uri>https://old.reddit.com/user/Patience2277</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtktjm/my_24b_llm_in_korean/"&gt; &lt;img alt="my 2.4b llm in korean" src="https://b.thumbs.redditmedia.com/rVRLNVMNNoUtXtB_eodydwBSHbpOQFXtur78DWudPtA.jpg" title="my 2.4b llm in korean" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Î¨∏Îß• ÌååÏïÖÏùÄ ÏÑ±Í≥µÏ†Å!&lt;br /&gt; ÏàòÎä•Î¨∏Ï†ú ÎπÑÏä∑ÌïúÍ±∞Îùº ÏòÅÏñ¥Î°ú Î≤àÏó≠Ïãú Ïù¥ÏÉÅÌï†ÏàòÏûàÏùå&lt;br /&gt; &amp;quot;As it's similar to a Suneung problem, it might sound awkward when translated into English.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6dta3nxwvrjf1.png?width=3366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2f630da02a8b53073a9a74a81e94284626f7efc"&gt;https://preview.redd.it/6dta3nxwvrjf1.png?width=3366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2f630da02a8b53073a9a74a81e94284626f7efc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2zaew5mxvrjf1.png?width=3350&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc09a925abb2aa97fadbf2b0e04210462ff2aa88"&gt;https://preview.redd.it/2zaew5mxvrjf1.png?width=3350&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc09a925abb2aa97fadbf2b0e04210462ff2aa88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ÏùºÎã® Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï∂îÎ°†Ìï¥! ÌååÏù∏ÌäúÎãù X&lt;/p&gt; &lt;p&gt;My model successfully! performed inference! No fine-tuning required.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patience2277"&gt; /u/Patience2277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtktjm/my_24b_llm_in_korean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtktjm/my_24b_llm_in_korean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtktjm/my_24b_llm_in_korean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T12:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtqdy8</id>
    <title>guide : running gpt-oss with llama.cpp</title>
    <updated>2025-08-18T16:09:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqdy8/guide_running_gptoss_with_llamacpp/"&gt; &lt;img alt="guide : running gpt-oss with llama.cpp" src="https://external-preview.redd.it/b2JdP9WbhqiSxugjTjM2AEgw1c-M7Kpl8_FDa-Zbacc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6b47fe63bd4a1548986fe336f3aed8b1bfaf5dd" title="guide : running gpt-oss with llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15396"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqdy8/guide_running_gptoss_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqdy8/guide_running_gptoss_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mti2eo</id>
    <title>Do SLMs make more sense than LLMs for agents?</title>
    <updated>2025-08-18T10:17:43+00:00</updated>
    <author>
      <name>/u/_mrsdangerous_</name>
      <uri>https://old.reddit.com/user/_mrsdangerous_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just read NVIDIA‚Äôs paper arguing that small language models (SLMs) might actually be a better fit for agentic AI than LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the idea makes sense:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;‚Äì SLMs are cheaper to run and easier to deploy locally.&lt;br /&gt; ‚Äì Most agentic tasks are repetitive and specialized, not broad open-ended chat.&lt;br /&gt; ‚Äì You can scale by running many small models instead of relying on one giant LLM&lt;/p&gt; &lt;p&gt;LLMs may still play a role in orchestration, but SLMs seem far more practical for the heavy lifting.&lt;/p&gt; &lt;p&gt;for those experimenting already: what useful things are you running locally with SLMs right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_mrsdangerous_"&gt; /u/_mrsdangerous_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mti2eo/do_slms_make_more_sense_than_llms_for_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mti2eo/do_slms_make_more_sense_than_llms_for_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mti2eo/do_slms_make_more_sense_than_llms_for_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T10:17:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mthaox</id>
    <title>üêß llama.cpp on Steam Deck (Ubuntu 25.04) with GPU (Vulkan) ‚Äî step-by-step that actually works</title>
    <updated>2025-08-18T09:32:57+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got &lt;strong&gt;llama.cpp&lt;/strong&gt; running on the Steam Deck APU (Van Gogh, &lt;code&gt;gfx1033&lt;/code&gt;) with &lt;strong&gt;GPU acceleration via Vulkan&lt;/strong&gt; on Ubuntu 25.04 (clean install on SteamDeck 256GB). Below are only the steps and commands that worked end-to-end, plus practical ways to verify the GPU is doing the work.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Build llama.cpp with &lt;code&gt;-DGGML_VULKAN=ON&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Use smaller GGUF models (1‚Äì3B, quantized) and push as many layers to GPU as VRAM allows via &lt;code&gt;--gpu-layers&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Verify with &lt;code&gt;radeontop&lt;/code&gt;, &lt;code&gt;vulkaninfo&lt;/code&gt;, and (optionally) &lt;code&gt;rocm-smi&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;0) Confirm the GPU is visible (optional sanity)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;rocminfo # should show Agent &amp;quot;gfx1033&amp;quot; (AMD Custom GPU 0405) rocm-smi --json # reports temp/power/VRAM (APUs show limited SCLK data; JSON is stable) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you‚Äôll run GPU tasks as a non-root user:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo usermod -aG render,video $USER # log out/in (or reboot) so group changes take effect &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;1) Install the required packages&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;sudo apt update sudo apt install -y \ build-essential cmake git \ mesa-vulkan-drivers libvulkan-dev vulkan-tools \ glslang-tools glslc libshaderc-dev spirv-tools \ libcurl4-openssl-dev ca-certificates &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Quick checks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vulkaninfo | head -n 20 # should print &amp;quot;Vulkan Instance Version: 1.4.x&amp;quot; glslc --version # shaderc + glslang versions print &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(Optional but nice)&lt;/em&gt; speed up rebuilds:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install -y ccache &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;2) Clone and build llama.cpp with Vulkan&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ggml-org/llama.cpp cd llama.cpp rm -rf build cmake -B build -DGGML_VULKAN=ON \ -DGGML_CCACHE=ON # optional, speeds up subsequent builds cmake --build build --config Release -j &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;3) Run a model on the GPU&lt;/h1&gt; &lt;h1&gt;a) Pull a model from Hugging Face (requires CURL enabled)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./build/bin/llama-cli \ -hf ggml-org/gemma-3-1b-it-GGUF \ --gpu-layers 32 \ -p &amp;quot;Say hello from Steam Deck GPU.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;b) Use a local model file&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./build/bin/llama-cli \ -m /path/to/model.gguf \ --gpu-layers 32 \ -p &amp;quot;Say hello from Steam Deck GPU.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start with quantized models (e.g., &lt;code&gt;*q4_0.gguf&lt;/code&gt;, &lt;code&gt;*q5_k.gguf&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Increase &lt;code&gt;--gpu-layers&lt;/code&gt; until you hit VRAM limits (Deck iGPU usually has ~1 GiB reserved VRAM + shared RAM; if it OOMs or slows down, lower it).&lt;/li&gt; &lt;li&gt;&lt;code&gt;--ctx-size&lt;/code&gt; / &lt;code&gt;-c&lt;/code&gt; increases memory use; keep moderate contexts on an APU.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4) Verify the GPU is actually working&lt;/h1&gt; &lt;h1&gt;Option A: radeontop (simple and effective)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;sudo apt install -y radeontop radeontop &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Watch the &lt;strong&gt;‚Äúgpu‚Äù&lt;/strong&gt; bar and rings (gfx/compute) jump when you run llama.cpp.&lt;/li&gt; &lt;li&gt;Run &lt;code&gt;radeontop&lt;/code&gt; in one terminal, start llama.cpp in another, and you should see load spike above idle.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Option B: Vulkan headless check&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;vulkaninfo | head -n 20 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;If you‚Äôre headless you‚Äôll see ‚ÄúDISPLAY not set ‚Ä¶ skipping surface info‚Äù, which is fine; compute still works.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Option C: ROCm SMI (APU metrics are limited but still useful)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;watch -n 1 rocm-smi --showtemp --showpower --showmeminfo vram --json &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Look for temperature/power bumps and VRAM use increasing under load.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Option D: DPM states (clock levels changing)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;watch -n 0.5 &amp;quot;cat /sys/class/drm/card*/device/pp_dpm_sclk; echo; cat /sys/class/drm/card*/device/pp_dpm_mclk&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;You should see the active &lt;code&gt;*&lt;/code&gt; move to higher SCLK/MCLK levels during inference.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5) What worked well on the Steam Deck APU (Van Gogh / gfx1033)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vulkan backend&lt;/strong&gt; is the most reliable path for AMD iGPUs/APUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small models (1‚Äì12B)&lt;/strong&gt; with &lt;strong&gt;q4/q5&lt;/strong&gt; quantization run smoothly enough for testing around 1b about 25 t/s and 12b (!) gemma3 at 10 t/s.&lt;/li&gt; &lt;li&gt;Pushing as many &lt;code&gt;--gpu-layers&lt;/code&gt; as memory allows gives the best speedup; if you see instability, dial it back.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rocm-smi&lt;/code&gt; on APUs may not show SCLK, but temp/power/VRAM are still indicative; &lt;code&gt;radeontop&lt;/code&gt; is the most convenient ‚Äúis it doing something?‚Äù view.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6) Troubleshooting quick hits&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CMake can‚Äôt find Vulkan/glslc&lt;/strong&gt; ‚Üí make sure &lt;code&gt;libvulkan-dev&lt;/code&gt;, &lt;code&gt;glslc&lt;/code&gt;, &lt;code&gt;glslang-tools&lt;/code&gt;, &lt;code&gt;libshaderc-dev&lt;/code&gt;, &lt;code&gt;spirv-tools&lt;/code&gt; are installed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CMake can‚Äôt find CURL&lt;/strong&gt; ‚Üí &lt;code&gt;sudo apt install -y libcurl4-openssl-dev&lt;/code&gt; or add &lt;code&gt;-DLLAMA_CURL=OFF&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low performance / stutter&lt;/strong&gt; ‚Üí reduce context size and/or &lt;code&gt;--gpu-layers&lt;/code&gt;, try a smaller quant, ensure no other heavy GPU tasks are running.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Permissions&lt;/strong&gt; ‚Üí ensure your user is in &lt;code&gt;render&lt;/code&gt; and &lt;code&gt;video&lt;/code&gt; groups and re-log.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That‚Äôs the whole path I used to get &lt;strong&gt;llama.cpp&lt;/strong&gt; running with GPU acceleration on the Steam Deck via Vulkan, including how to prove the GPU is active.&lt;/p&gt; &lt;h1&gt;Reflection&lt;/h1&gt; &lt;p&gt;The &lt;strong&gt;Steam Deck&lt;/strong&gt; offers a compelling alternative to the &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; as a low-power, compact home server, especially if you're interested in &lt;strong&gt;local LLM inference with GPU acceleration&lt;/strong&gt;. Unlike the Pi, the Deck includes a capable &lt;strong&gt;AMD RDNA2 iGPU&lt;/strong&gt;, substantial memory (16 GB LPDDR5), and &lt;strong&gt;NVMe SSD&lt;/strong&gt; support‚Äîmaking it great for &lt;strong&gt;virtualization&lt;/strong&gt; and &lt;strong&gt;LLM workloads&lt;/strong&gt; directly on the embedded SSD, all within a &lt;strong&gt;mobile, power-efficient&lt;/strong&gt; form factor.&lt;/p&gt; &lt;p&gt;Despite being designed for handheld gaming, the Steam Deck‚Äôs &lt;strong&gt;idle power draw&lt;/strong&gt; is surprisingly modest (around &lt;strong&gt;7 W&lt;/strong&gt;), yet it packs far more compute and GPU versatility than a Pi. In contrast, the Raspberry Pi 5 consumes only around &lt;strong&gt;2.5‚Äì2.75 W at idle&lt;/strong&gt;, but lacks any integrated GPU suitable for serious acceleration tasks. For tasks like running llama.cpp with a quantized model on &lt;strong&gt;GPU layers&lt;/strong&gt;, the Deck's iGPU opens performance doors the Pi simply can't match. Plus, with low TDP and idle power, the Deck consumes just a bit more energy but delivers &lt;strong&gt;far greater throughput&lt;/strong&gt; and flexibility.&lt;/p&gt; &lt;p&gt;All things considered, the Steam Deck presents a highly efficient and portable alternative for embedded LLM serving‚Äîor even broader home server applications‚Äîdelivering &lt;strong&gt;hardware acceleration, storage, memory, and low power&lt;/strong&gt; in one neat package.&lt;/p&gt; &lt;h1&gt;Power Consumption Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Idle Power (Typical)&lt;/th&gt; &lt;th align="left"&gt;Peak Power (Load)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Raspberry Pi 5 (idle)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~2.5 W ‚Äì 2.75 W&lt;/td&gt; &lt;td align="left"&gt;~5‚Äì6 W (CPU load; no GPU)&lt;a href="https://www.jeffgeerling.com/blog/2024/new-2gb-pi-5-has-33-smaller-die-30-idle-power-savings?utm_source=chatgpt.com"&gt;Pimoroni Buccaneers+6jeffgeerling.com+6jeffgeerling.com+6&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Steam Deck (idle)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~7 W&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://steamcommunity.com/app/1675200/discussions/0/3466100515576632731/?utm_source=chatgpt.com"&gt;steamcommunity.com&lt;/a&gt;up to ~25 W (max APU TDP)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Raspberry Pi 5&lt;/strong&gt;: Multiple sources confirm idle power around &lt;strong&gt;2.5 W&lt;/strong&gt;, nearly identical to Pi 4, with CPU-intensive tasks raising it modestly into the 5‚Äì6 W range &lt;a href="https://www.jeffgeerling.com/blog/2024/new-2gb-pi-5-has-33-smaller-die-30-idle-power-savings?utm_source=chatgpt.com"&gt;forums.raspberrypi.com+8jeffgeerling.com+8Home Assistant Community+8&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Steam Deck&lt;/strong&gt;: Users observe idle consumption at about &lt;strong&gt;7 W&lt;/strong&gt; when not charging &lt;a href="https://steamcommunity.com/app/1675200/discussions/0/3466100515576632731/?utm_source=chatgpt.com"&gt;steamcommunity.com+2WIRED+2&lt;/a&gt;. Official spec lists &lt;strong&gt;max APU draw 4‚Äì15 W&lt;/strong&gt;, with system‚Äëwide peaks reaching &lt;strong&gt;~25 W&lt;/strong&gt; under heavy load &lt;a href="https://www.reddit.com/r/SteamDeck/comments/xuy7o4/deck_using_too_many_watts_idle/?utm_source=chatgpt.com"&gt;linustechtips.com+9Reddit+9Reddit+9&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why the Deck still wins as a home server&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU Acceleration&lt;/strong&gt;: Built-in RDNA2 GPU enables Vulkan compute, perfect for llama.cpp or similar.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory &amp;amp; Storage&lt;/strong&gt;: 16 GB RAM + NVMe SSD vastly outclass the typical Pi setup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low Idle Draw with High Capability&lt;/strong&gt;: While idle wattage is higher than the Pi, it's still minimal for what the system can do.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Versatility&lt;/strong&gt;: Runs full Linux desktop environments, supports virtualization, containerization, and more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;IMHO why do I choose Steamdeck as home server instead of Rpi 5 16GB + accessories... &lt;/p&gt; &lt;p&gt;Steam Deck 256 GB LCD: 250 ‚Ç¨&lt;br /&gt; All‚Äëin: Zen 2 (4 core/8 thread) CPU, RDNA 2 iGPU, 16 GB RAM, 256 GB NVMe, built‚Äëin battery, LCD, Wi‚ÄëFi/Bluetooth, cooling, case, controls‚Äînothing else to buy.&lt;/p&gt; &lt;h1&gt;Raspberry Pi 5 (16 GB) Portable Build (microSD storage)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Raspberry Pi 5 (16 GB model): $120 (~110 ‚Ç¨) &lt;/li&gt; &lt;li&gt;PSU (5 V/5 A USB‚ÄëC PD): 15‚Äì20 ‚Ç¨&lt;/li&gt; &lt;li&gt;Active cooling (fan/heatsink): 10‚Äì15 ‚Ç¨&lt;/li&gt; &lt;li&gt;256 GB microSD (SDR104): 25‚Äì30 ‚Ç¨&lt;/li&gt; &lt;li&gt;Battery UPS HAT + 18650 cells: 40‚Äì60 ‚Ç¨&lt;/li&gt; &lt;li&gt;7‚Ä≥ LCD touchscreen: 75‚Äì90 ‚Ç¨&lt;/li&gt; &lt;li&gt;Cables/mounting/misc: 10‚Äì15 ‚Ç¨ Total: ‚âà 305‚Äì350 ‚Ç¨&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Raspberry Pi 5 (16 GB) Portable Build (SSD storage)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Raspberry Pi 5 (16 GB): ~110 ‚Ç¨&lt;/li&gt; &lt;li&gt;Case: 20‚Äì30 ‚Ç¨&lt;/li&gt; &lt;li&gt;PSU: 15‚Äì20 ‚Ç¨&lt;/li&gt; &lt;li&gt;Cooling: 10‚Äì15 ‚Ç¨&lt;/li&gt; &lt;li&gt;NVMe HAT (e.g. M.2 adapter): 60‚Äì80 ‚Ç¨&lt;/li&gt; &lt;li&gt;256 GB NVMe SSD: 25‚Äì35 ‚Ç¨&lt;/li&gt; &lt;li&gt;Battery UPS HAT + cells: 40‚Äì60 ‚Ç¨&lt;/li&gt; &lt;li&gt;7‚Ä≥ LCD touchscreen: 75‚Äì90 ‚Ç¨&lt;/li&gt; &lt;li&gt;Cables/mounting/misc: 10‚Äì15 ‚Ç¨ Total: ‚âà 355‚Äì405 ‚Ç¨&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why the Pi Isn‚Äôt Actually Cheaper Once Portable&lt;/h1&gt; &lt;p&gt;Sure, the bare Pi 5 16 GB costs around 110 ‚Ç¨, but once you add battery power, display, case, cooling, and storage, you're looking at ~305‚Äì405 ‚Ç¨ depending on microSD or SSD. It quickly becomes comparable‚Äîor even more expensive‚Äîthan the Deck.&lt;/p&gt; &lt;h1&gt;Capabilities: Steam Deck vs. Raspberry Pi 5 Portable&lt;/h1&gt; &lt;p&gt;Steam Deck (250 ‚Ç¨) capabilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local LLMs / Chatbots with Vulkan/HIP GPU acceleration&lt;/li&gt; &lt;li&gt;Plex / Jellyfin with smooth 1080p and even 4K transcoding&lt;/li&gt; &lt;li&gt;Containers &amp;amp; Virtualization via Docker, Podman, KVM&lt;/li&gt; &lt;li&gt;Game Streaming as a Sunshine/Moonlight box&lt;/li&gt; &lt;li&gt;Dev/Test Lab with fast NVMe and powerful CPU&lt;/li&gt; &lt;li&gt;Retro Emulation Server&lt;/li&gt; &lt;li&gt;Home Automation: Home Assistant, MQTT, Node‚ÄëRED&lt;/li&gt; &lt;li&gt;Edge AI: image/speech inference at the edge&lt;/li&gt; &lt;li&gt;Personal Cloud / NAS: Nextcloud, Syncthing, Samba&lt;/li&gt; &lt;li&gt;VPN / Firewall Gateway: WireGuard/OpenVPN with hardware crypto&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Raspberry Pi 5 (16 GB)‚Äîyes, it can do many of these‚Äîbut:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You'll need to assemble and configure everything manually&lt;/li&gt; &lt;li&gt;Limited GPU performance compared to RDNA2 and 16 GB RAM in a mobile form factor&lt;/li&gt; &lt;li&gt;It's more of a project, not a polished user-ready device&lt;/li&gt; &lt;li&gt;Users on forums note that by the time you add parts, the cost edges toward mini-x86 PCs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In summary: &lt;em&gt;Yes, the Steam Deck outshines the Raspberry Pi 5 as a compact, low-power, GPU-accelerated home server for LLMs and general compute.&lt;/em&gt; If you can tolerate the slightly higher idle draw (3‚Äì5 W more), you gain significant performance and flexibility for AI workloads at home.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthaox/llamacpp_on_steam_deck_ubuntu_2504_with_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthaox/llamacpp_on_steam_deck_ubuntu_2504_with_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mthaox/llamacpp_on_steam_deck_ubuntu_2504_with_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T09:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtqnwe</id>
    <title>kimi-vl-a3b-thinking-2506 launched today on LM Studio</title>
    <updated>2025-08-18T16:19:30+00:00</updated>
    <author>
      <name>/u/PhotographerUSA</name>
      <uri>https://old.reddit.com/user/PhotographerUSA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the VL version of Kimi . It works on my 8GB video card. It also has CPU offloading and it's real quick!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhotographerUSA"&gt; /u/PhotographerUSA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqnwe/kimivla3bthinking2506_launched_today_on_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqnwe/kimivla3bthinking2506_launched_today_on_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqnwe/kimivla3bthinking2506_launched_today_on_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:19:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1msr7j8</id>
    <title>To all vibe coders I present</title>
    <updated>2025-08-17T13:40:07+00:00</updated>
    <author>
      <name>/u/theundertakeer</name>
      <uri>https://old.reddit.com/user/theundertakeer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt; &lt;img alt="To all vibe coders I present" src="https://external-preview.redd.it/dXZiNzRocGcybGpmMeA17HlDZqcxGH0WPMXNGATdmxTbHU45E1nSLLgU5DlN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5981b886ff07914ad22d7db97d58fa9b60c3a9" title="To all vibe coders I present" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theundertakeer"&gt; /u/theundertakeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eckuwlog2ljf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9htu</id>
    <title>FlashAttention 4 Leak</title>
    <updated>2025-08-18T02:10:02+00:00</updated>
    <author>
      <name>/u/InevitableExtreme396</name>
      <uri>https://old.reddit.com/user/InevitableExtreme396</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt; &lt;img alt="FlashAttention 4 Leak" src="https://a.thumbs.redditmedia.com/5f6bt97vhO4cGlHOqVfCoRYr_m9nC10Eu8zHnCuyjR0.jpg" title="FlashAttention 4 Leak" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like the FA4 source code just got leaked here on a branch:&lt;/p&gt; &lt;p&gt;TLDR; As expected, it's mostly Blackwell (SM100+) and Tensor Core Generation 5, and uses the CuTe DSL (CUTLASS). There is also some handwritten PTX.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/46yfc8z3sojf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b1b33b3f27dfe41ec550142abaa8e0e97bc2449"&gt;https://preview.redd.it/46yfc8z3sojf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b1b33b3f27dfe41ec550142abaa8e0e97bc2449&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's from the SGlang codebase, one of the popular LLM inference engines (like Llama.cpp for distributed.)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang/compare/main...hieu/fa4"&gt;https://github.com/sgl-project/sglang/compare/main...hieu/fa4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableExtreme396"&gt; /u/InevitableExtreme396 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T02:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtn16b</id>
    <title>Presenton now supports presentation generation via MCP</title>
    <updated>2025-08-18T14:06:28+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtn16b/presenton_now_supports_presentation_generation/"&gt; &lt;img alt="Presenton now supports presentation generation via MCP" src="https://external-preview.redd.it/Ymhra2NpbTBjc2pmMROeZeQh15N-GStyHCbN5mnRNX7F3fY722SokL4VMsiB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df49af1b082df6243aee050951a1a16e3dfb5a8c" title="Presenton now supports presentation generation via MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Presenton, an open source AI presentation tool now supports presentation generation via MCP.&lt;/p&gt; &lt;p&gt;Simply connect to MCP and let you model or agent make calls for you to generate presentation.&lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://docs.presenton.ai/generate-presentation-over-mcp"&gt;https://docs.presenton.ai/generate-presentation-over-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4lu97hm0csjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtn16b/presenton_now_supports_presentation_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtn16b/presenton_now_supports_presentation_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T14:06:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtfw5j</id>
    <title>My open-source agent Maestro is now faster and lets you configure context limits for better local model support</title>
    <updated>2025-08-18T08:04:36+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfw5j/my_opensource_agent_maestro_is_now_faster_and/"&gt; &lt;img alt="My open-source agent Maestro is now faster and lets you configure context limits for better local model support" src="https://a.thumbs.redditmedia.com/2Jm7LDGCGGJuzu7lAAQM2I8TKlWM1S6jW2ieXBd-_l0.jpg" title="My open-source agent Maestro is now faster and lets you configure context limits for better local model support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I just pushed a big update for Maestro, my open-source AI research agent. I've focused on making it work better with local models.&lt;/p&gt; &lt;p&gt;The biggest change is that you can now fully configure research parameters like planning context limits directly in the UI. This should finally fix the context overflow issues some of you were seeing.&lt;/p&gt; &lt;p&gt;A few other key updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's much faster now due to a full database migration to PostgreSQL. Document processing and lookups are noticeably quicker.&lt;/li&gt; &lt;li&gt;There's an improved CPU-only mode with its own &lt;code&gt;docker-compose.cpu.yml&lt;/code&gt; for easier setup on machines that don't need/have a GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Heads up:&lt;/strong&gt; This is a breaking change because of the new database. You'll need to do a fresh install.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/maestro"&gt;The project is on GitHub if you'd like to check it out&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Happy to hear any feedback you have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mtfw5j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfw5j/my_opensource_agent_maestro_is_now_faster_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfw5j/my_opensource_agent_maestro_is_now_faster_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T08:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mthwu7</id>
    <title>LFM2-VL family support is now available in llama.cpp</title>
    <updated>2025-08-18T10:08:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthwu7/lfm2vl_family_support_is_now_available_in_llamacpp/"&gt; &lt;img alt="LFM2-VL family support is now available in llama.cpp" src="https://preview.redd.it/46xeh4zo5rjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64baef342c8b74950f383a955d91f66f7ede7ff6" title="LFM2-VL family support is now available in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models: &lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/46xeh4zo5rjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthwu7/lfm2vl_family_support_is_now_available_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mthwu7/lfm2vl_family_support_is_now_available_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T10:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttchz</id>
    <title>Deepseek R2 coming out ... when it gets more cowbell</title>
    <updated>2025-08-18T17:56:08+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what‚Äôs floating around it seems like we'll have to keep waiting a bit longer for Deepseek R2 to be released.&lt;/p&gt; &lt;p&gt;Apparently&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Liang Wenfeng has been sitting on R2's release because it still needs more cowbell&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Training DeepSeek R2 on Huawei Ascend chips ran into persistent stability and software problems and no full training run ever succeeded. So Deepseek went back to Nvidia GPUs for training and is using Ascend chips for inference only&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://youtu.be/PzlqRsuIo1w"&gt;https://youtu.be/PzlqRsuIo1w&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtsbsk</id>
    <title>I wish we could actually use Gemma 3n</title>
    <updated>2025-08-18T17:19:27+00:00</updated>
    <author>
      <name>/u/ai_fonsi</name>
      <uri>https://old.reddit.com/user/ai_fonsi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It would be the perfect model to run on my Nvidia Jetson Orin Nano, with native audio and video support. It's a great model that punches way above its weight and I like the general vibe a lot. And most importantly, it has some really cool optimizations for low-memory edge devices such as per-layer embeddings that can be loaded from flash memory (in theory).&lt;/p&gt; &lt;p&gt;However, all inference engines I tried either don't support the multimodal features (llama.cpp) or use much more memory than they're supposed to (HF transformers, mistral.rs). ONNX loads the weights in memory twice on the GPU executor for some reason and even Google's own &lt;a href="https://github.com/google-ai-edge/LiteRT-LM"&gt;LiteRT-LM&lt;/a&gt; doesn't support CUDA or &lt;a href="https://github.com/google-ai-edge/LiteRT-LM/issues/351"&gt;multimodal features in any environment&lt;/a&gt; yet (???). The engines' maintainers mostly don't seem to be actively working on improving the situation, at least as far as I can tell.&lt;/p&gt; &lt;p&gt;Don't get me wrong, I'm not trying to blame the OSS devs trying their best to support the current onslaught of models in their engines. However, I wish Google wouldn't &lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/#get-started-with-gemma-3n-today"&gt;boast about how their model is supported by so many tools&lt;/a&gt; when they clearly didn't provide enough help to support half the features or optimizations necessary to unlock the full potential. Contrast this with GPT-OSS, which mostly worked fine out of the box despite architectural differences. It's just a shame given how many cool things this model would have going for it (audio, PLE, MatFormer)...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_fonsi"&gt; /u/ai_fonsi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtsbsk/i_wish_we_could_actually_use_gemma_3n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtsbsk/i_wish_we_could_actually_use_gemma_3n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtsbsk/i_wish_we_could_actually_use_gemma_3n/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttad3</id>
    <title>Qwen-Image-Edit</title>
    <updated>2025-08-18T17:54:01+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"&gt; &lt;img alt="Qwen-Image-Edit" src="https://external-preview.redd.it/_phhztHOXP1EaSigwjpmdclnYlgiIY_nMfXGtHApDV0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9e4ac0aed522efe5c46173ca8aa2cff20f9af4" title="Qwen-Image-Edit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mthavv</id>
    <title>Best NSFW/uncensored LLM to generate prompts for image generation?</title>
    <updated>2025-08-18T09:33:17+00:00</updated>
    <author>
      <name>/u/irmesutb6</name>
      <uri>https://old.reddit.com/user/irmesutb6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm new to this, i basically use SD/PONY/Illustrious to generate images and they‚Äôre at a pretty good stage.&lt;/p&gt; &lt;p&gt;I‚Äôm trying to find a LLM that can generate NSFW prompts for image generation, i have 8GB VRAM on RTX 4060 locally could please anyone help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irmesutb6"&gt; /u/irmesutb6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthavv/best_nsfwuncensored_llm_to_generate_prompts_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthavv/best_nsfwuncensored_llm_to_generate_prompts_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mthavv/best_nsfwuncensored_llm_to_generate_prompts_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T09:33:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mto88l</id>
    <title>We open-sourced Memori: A memory engine for AI agents</title>
    <updated>2025-08-18T14:51:39+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I'm a part the team behind &lt;a href="https://memori.gibsonai.com/"&gt;Memori&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Memori adds a stateful memory engine to AI agents, enabling them to stay consistent, recall past work, and improve over time. With Memori, agents don‚Äôt lose track of multi-step workflows, repeat tool calls, or forget user preferences. Instead, they build up human-like memory that makes them more reliable and efficient across sessions.&lt;/p&gt; &lt;p&gt;We‚Äôve also put together demo apps (a personal diary assistant, a research agent, and a travel planner) so you can see memory in action.&lt;/p&gt; &lt;p&gt;Current LLMs are stateless, they forget everything between sessions. This leads to repetitive interactions, wasted tokens, and inconsistent results. When building AI agents, this problem gets even worse: without memory, they can‚Äôt recover from failures, coordinate across steps, or apply simple rules like ‚Äúalways write tests.‚Äù&lt;/p&gt; &lt;p&gt;We realized that for AI agents to work in production, they need memory. That‚Äôs why we built Memori.&lt;/p&gt; &lt;h1&gt;How Memori Works&lt;/h1&gt; &lt;p&gt;Memori uses a multi-agent architecture to capture conversations, analyze them, and decide which memories to keep active. It supports three modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conscious Mode:&lt;/strong&gt; short-term memory for recent, essential context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto Mode:&lt;/strong&gt; dynamic search across long-term memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Combined Mode:&lt;/strong&gt; blends both for fast recall and deep retrieval.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Under the hood, Memori is &lt;strong&gt;SQL-first&lt;/strong&gt;. You can use SQLite, PostgreSQL, or MySQL to store memory with built-in full-text search, versioning, and optimization. This makes it simple to deploy, production-ready, and extensible.&lt;/p&gt; &lt;h1&gt;Database-Backed for Reliability&lt;/h1&gt; &lt;p&gt;Memori is backed by GibsonAI‚Äôs database infrastructure, which supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instant provisioning&lt;/li&gt; &lt;li&gt;Autoscaling on demand&lt;/li&gt; &lt;li&gt;Database branching &amp;amp; versioning&lt;/li&gt; &lt;li&gt;Query optimization&lt;/li&gt; &lt;li&gt;Point of recovery&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This means memory isn‚Äôt just stored, it‚Äôs reliable, efficient, and scales with real-world workloads.&lt;/p&gt; &lt;h1&gt;Getting Started&lt;/h1&gt; &lt;p&gt;Install the SDK( `pip install memorisdk` ) and enable memory in one line:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from memori import Memori memori = Memori(conscious_ingest=True) memori.enable() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;From then on, every conversation is remembered and intelligently recalled when needed.&lt;/p&gt; &lt;p&gt;We‚Äôve open-sourced Memori under the Apache 2.0 license so anyone can build with it. You can check out the GitHub repo here: &lt;a href="https://github.com/GibsonAI/memori"&gt;https://github.com/GibsonAI/memori&lt;/a&gt;, and explore the docs.&lt;/p&gt; &lt;p&gt;We‚Äôd love to hear your thoughts. Please dive into the code, try out the demos, and share feedback, your input will help shape where we take Memori from here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto88l/we_opensourced_memori_a_memory_engine_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto88l/we_opensourced_memori_a_memory_engine_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mto88l/we_opensourced_memori_a_memory_engine_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T14:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtqz3u</id>
    <title>Test: can Qwen 2.5 Omni actually hear guitar chords?</title>
    <updated>2025-08-18T16:30:44+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"&gt; &lt;img alt="Test: can Qwen 2.5 Omni actually hear guitar chords?" src="https://external-preview.redd.it/ZHl5dWUwbXl6c2pmMYMQwk04cVObnLQ8K0JceAGBOHlOc1BqIEw787XqJHGO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f00d3d8d78e6d2d5c31925df7d0e8a596f74c0a" title="Test: can Qwen 2.5 Omni actually hear guitar chords?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried Qwen 2.5 Omni locally with vision + speech to see if it could hear music instead of just speech.&lt;/p&gt; &lt;p&gt;I played guitar, and it did a surprisingly solid job telling me which chords I was playing in real-time. At the end, I debugged what the LLM was ‚Äúhearing,‚Äù and input quality likely explained some of the misses it did have.&lt;/p&gt; &lt;p&gt;Next test: run it with the guitar out of sight, so we can confirm it‚Äôs not cheating by ‚Äúseeing‚Äù the guitar instead of listening. Will share results when I try that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/848676nyzsjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtrn1y</id>
    <title>Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!</title>
    <updated>2025-08-18T16:55:07+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"&gt; &lt;img alt="Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!" src="https://external-preview.redd.it/S-o7drNLzmVPRPUM6Ap2mbj65SEf6wzf8867vZcT5JE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9303bece3c938ba362682f24d93acffc4066003c" title="Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtct4y</id>
    <title>Elon didn't deliver on this announcement. It's already Monday.</title>
    <updated>2025-08-18T04:59:00+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt; &lt;img alt="Elon didn't deliver on this announcement. It's already Monday." src="https://preview.redd.it/rt8xgjaampjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2b6da79d84d1e52439441cafb251d7e1dc508f7" title="Elon didn't deliver on this announcement. It's already Monday." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rt8xgjaampjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtk03a</id>
    <title>Kimi K2 is really, really good.</title>
    <updated>2025-08-18T12:00:19+00:00</updated>
    <author>
      <name>/u/ThomasAger</name>
      <uri>https://old.reddit.com/user/ThomasAger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve spent a long time waiting for an open source model I can use in production for both multi-agent multi-turn workflows, as well as a capable instruction following chat model. &lt;/p&gt; &lt;p&gt;This was the first model that has ever delivered. &lt;/p&gt; &lt;p&gt;For a long time I was stuck using foundation models, writing prompts that did the job I knew fine-tuning an open source model could do so much more effectively.&lt;/p&gt; &lt;p&gt;This isn‚Äôt paid or sponsored. It‚Äôs available to talk to for free and on the LM arena leaderboard (a month or so ago it was #8 there). I know many of ya‚Äôll are already aware of this but I strongly recommend looking into integrating them into your pipeline.&lt;/p&gt; &lt;p&gt;They are already effective at long term agent workflows like building research reports with citations or websites. You can even try it for free. Has anyone else tried Kimi out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasAger"&gt; /u/ThomasAger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T12:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttgrf</id>
    <title>Qwen-Image-Edit Released!</title>
    <updated>2025-08-18T18:00:24+00:00</updated>
    <author>
      <name>/u/MohamedTrfhgx</name>
      <uri>https://old.reddit.com/user/MohamedTrfhgx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba‚Äôs Qwen team just released &lt;strong&gt;Qwen-Image-Edit&lt;/strong&gt;, an image editing model built on the &lt;strong&gt;20B Qwen-Image&lt;/strong&gt; backbone. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports &lt;strong&gt;precise bilingual (Chinese &amp;amp; English) text editing&lt;/strong&gt; while preserving style, plus both &lt;strong&gt;semantic&lt;/strong&gt; and &lt;strong&gt;appearance-level&lt;/strong&gt; edits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text editing with bilingual support&lt;/li&gt; &lt;li&gt;High-level semantic editing (object rotation, IP creation, concept edits)&lt;/li&gt; &lt;li&gt; Low-level appearance editing (add / delete / insert objects)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1957500569029079083"&gt;https://x.com/Alibaba_Qwen/status/1957500569029079083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen has been really prolific lately what do you think of the new model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MohamedTrfhgx"&gt; /u/MohamedTrfhgx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T18:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mto8fa</id>
    <title>New code benchmark puts Qwen 3 Coder at the top of the open models</title>
    <updated>2025-08-18T14:51:49+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt; &lt;img alt="New code benchmark puts Qwen 3 Coder at the top of the open models" src="https://external-preview.redd.it/-FhGljRcqsXlvJ4R58hFA0RMnpSa9fBguxJ8Dc9Mg-4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15659a7787e6ab90c61f6ea82b0300809513a758" title="New code benchmark puts Qwen 3 Coder at the top of the open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR of the open models results:&lt;/p&gt; &lt;p&gt;Q3C fp16 &amp;gt; Q3C fp8 &amp;gt; GPT-OSS-120b &amp;gt; V3 &amp;gt; K2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brokk.ai/power-ranking?round=open&amp;amp;models=flash-2.5%2Cgpt-oss-120b%2Cgpt5-mini%2Ck2%2Cq3c%2Cq3c-fp8%2Cv3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T14:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttcr9</id>
    <title>üöÄ Qwen released Qwen-Image-Edit!</title>
    <updated>2025-08-18T17:56:23+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt; &lt;img alt="üöÄ Qwen released Qwen-Image-Edit!" src="https://b.thumbs.redditmedia.com/oRveemue3RG8vuBdHGpOCwiYY2B-M7S5WjEjTkW73hM.jpg" title="üöÄ Qwen released Qwen-Image-Edit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Excited to introduce Qwen-Image-Edit! Built on 20B Qwen-Image, it brings precise bilingual text editing (Chinese &amp;amp; English) while preserving style, and supports both semantic and appearance-level editing.&lt;/p&gt; &lt;p&gt;‚ú® Key Features&lt;/p&gt; &lt;p&gt;‚úÖ Accurate text editing with bilingual support&lt;/p&gt; &lt;p&gt;‚úÖ High-level semantic editing (e.g. object rotation, IP creation)&lt;/p&gt; &lt;p&gt;‚úÖ Low-level appearance editing (e.g. addition/delete/insert)&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit/"&gt;https://qwenlm.github.io/blog/qwen-image-edit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mttcr9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
