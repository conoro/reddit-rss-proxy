<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-09T23:34:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o1vmby</id>
    <title>Qwen3-VL MLX support incoming, thanks to Prince Canuma</title>
    <updated>2025-10-09T03:26:10+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1vmby/qwen3vl_mlx_support_incoming_thanks_to_prince/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1vmby/qwen3vl_mlx_support_incoming_thanks_to_prince/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1vmby/qwen3vl_mlx_support_incoming_thanks_to_prince/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T03:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2jymz</id>
    <title>What's a reliable and small model for news article summaries?</title>
    <updated>2025-10-09T22:29:51+00:00</updated>
    <author>
      <name>/u/dlarsen5</name>
      <uri>https://old.reddit.com/user/dlarsen5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;wondering what everyone's go to reliable model for clean output is for text summarization these days. I assume small models have enough &amp;quot;intelligence&amp;quot; to summarize effectively at this point but struggling to get good outputs from ones that fit on my AMD 7900 XTX 24GB and are performant since I have about 2 million small news articles to summarize&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dlarsen5"&gt; /u/dlarsen5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2jymz/whats_a_reliable_and_small_model_for_news_article/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2jymz/whats_a_reliable_and_small_model_for_news_article/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2jymz/whats_a_reliable_and_small_model_for_news_article/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T22:29:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1k5rc</id>
    <title>New Intel drivers are fire</title>
    <updated>2025-10-08T19:19:55+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"&gt; &lt;img alt="New Intel drivers are fire" src="https://preview.redd.it/f43lwzkhuxtf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726219703cd349e3c9b1d969986f07f0f51541fc" title="New Intel drivers are fire" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went from getting 30 tokens a second on gptosss20b to 95!!!!!!!!!!!!!!! Holy shit Intel is cooking with the b580 I have 4 total I'm gonna put a rig together with all the cards on a dual socket x99 system(for the pcie lanes) well get back with multi card perf later&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f43lwzkhuxtf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T19:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1p033</id>
    <title>Huawei's new open source technique shrinks LLMs to make them run on less powerful, less expensive hardware</title>
    <updated>2025-10-08T22:13:18+00:00</updated>
    <author>
      <name>/u/Financial_Nihilist</name>
      <uri>https://old.reddit.com/user/Financial_Nihilist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less"&gt;https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Nihilist"&gt; /u/Financial_Nihilist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1p033/huaweis_new_open_source_technique_shrinks_llms_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1p033/huaweis_new_open_source_technique_shrinks_llms_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1p033/huaweis_new_open_source_technique_shrinks_llms_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T22:13:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o26fda</id>
    <title>Best LLM gateway Suggestions?</title>
    <updated>2025-10-09T13:48:53+00:00</updated>
    <author>
      <name>/u/Silent_Employment966</name>
      <uri>https://old.reddit.com/user/Silent_Employment966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been testing out different LLM gateways for a multi-agent system and wanted to share some notes. I have tried multiple models &amp;amp; hosted them, but lately I’ve shifted focus to LLM gateways.&lt;/p&gt; &lt;p&gt;Most of the hosted ones are fine for basic key management or retries, but they fall short once you're comparing models side-by-side, need consistent response formatting, or want to route traffic based on task complexity. Some of them also have surprising bottlenecks under load or lack good observability out of the box.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://portkey.ai/features/ai-gateway"&gt;Portkey&lt;/a&gt;: Works reasonably well if you're building customer-facing products. Strong on retry logic and rate limiting. Falls short when you need sophisticated routing or deep observability. Started seeing latency spikes once traffic crossed a few hundred requests per second.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.anannas.ai/FAQs#why-should-i-use-anannas"&gt;AnannasAI&lt;/a&gt;: unified API to access 500+ models with just 10ms overhead and 99.999% uptime guarantee. The failproof routing and built-in cost control are game-changers for production environments. Dashboard gives you instant insights into usage, costs, and latency without needing separate monitoring tools. Works seamlessly for multi-modal needs (LLMs, image, pdf - inputs) and you can switch providers without vendor lock-in. its 6× faster than TrueFoundry (~3 ms), 80× faster than LiteLLM (3–31 ms), and ~80× faster than OpenRouter (~40 ms).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bifrost ( self-hosted)&lt;/strong&gt;: Performance was impressive when stress-testing. Measured roughly 11µs latency overhead at 5K requests/sec with noticeably lower RAM consumption than LiteLLM. Comes with built-in provider support, automatic failover, logging capabilities, Prometheus metrics, and a dashboard interface. Integration is straightforward—just swap the base URL, no SDK changes needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kong and Gloo&lt;/strong&gt;: Both are traditional API gateways that can technically handle LLM traffic. Getting them configured for model routing requires significant effort though, and they lack any LLM-specific intelligence. Feels like using the wrong tool for the job.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;LiteLLM&lt;/a&gt;: Great developer experience initially, scales fine for smaller projects. Performance degraded noticeably under pressure—saw around 50ms added latency and memory consumption climbing fast. Missing native monitoring tools. Managing it during traffic spikes or complex request chains became messy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For multi-agent systems specifically, having proper observability isn't optional I need to see which models are being called, how they're performing, and where costs are accumulating in real-time.&lt;/p&gt; &lt;p&gt;Curious what others are using,especially if you're running complex agent workflows or handling production traffic at scale.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent_Employment966"&gt; /u/Silent_Employment966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o26fda/best_llm_gateway_suggestions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o26fda/best_llm_gateway_suggestions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o26fda/best_llm_gateway_suggestions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T13:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o27xzz</id>
    <title>Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents</title>
    <updated>2025-10-09T14:48:59+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xzz/moondream3_and_salesforce_gta1_for_ui_grounding/"&gt; &lt;img alt="Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents" src="https://external-preview.redd.it/MXhzZGNjNzFuM3VmMeWLBg1mAXwAigO5PzLkc4zOrANcV84YU3-0ubuxy7cx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ac5847b5d6fec4bfc8f19007f2edb26496e0f9d" title="Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents&lt;/p&gt; &lt;p&gt;The numbers on ScreenSpot-v2 benchmark:&lt;/p&gt; &lt;p&gt;GTA-1 leads in accuracy (96% vs 84%), but Moondream3 is 2x faster (1.04s vs 1.97s avg).&lt;/p&gt; &lt;p&gt;The median time gap is even bigger: 0.78s vs 1.96s - that's a 2.5x speedup.&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run the benchmark yourself: &lt;a href="https://docs.trycua.com/docs/agent-sdk/benchmarks/screenspot-v2"&gt;https://docs.trycua.com/docs/agent-sdk/benchmarks/screenspot-v2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0odmsxg1n3uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xzz/moondream3_and_salesforce_gta1_for_ui_grounding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xzz/moondream3_and_salesforce_gta1_for_ui_grounding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T14:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2b666</id>
    <title>Chinny (iOS/MacOS): offline, on-device voice cloning with an optimized Chatterbox model</title>
    <updated>2025-10-09T16:49:38+00:00</updated>
    <author>
      <name>/u/Acceptable-Cycle4645</name>
      <uri>https://old.reddit.com/user/Acceptable-Cycle4645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, I've been experimenting with running voice cloning fully offline. Part of the motivation was that I don't trust those web-based or wrapper AI voice cloning apps that gather user data --- who knows when our information could be sold or used in unexpected ways. So I developed &lt;strong&gt;Chinny,&lt;/strong&gt; an iOS(16.6+) / macOS(15.5+) app that runs an optimized Chatterbox model entirely on-device and no network connectivity required!&lt;/p&gt; &lt;p&gt;All models are packed inside the app (about &lt;strong&gt;3.41 GB&lt;/strong&gt; total), and it uses around &lt;strong&gt;3 GB of RAM&lt;/strong&gt; during inference. It supports &lt;strong&gt;unlimited text input&lt;/strong&gt; by splitting it into chunks and combining the outputs into a single audio file.&lt;/p&gt; &lt;p&gt;Currently Chinny only supports English. In my opinion, the multilingual performance of the original Chatterbox model is not strong, and I plan to work on improvements (but only on selected languages).&lt;/p&gt; &lt;p&gt;Chinny is &lt;strong&gt;free and ad-free&lt;/strong&gt;, designed to be production-ready while also demonstrating what's possible with optimized on-device inference on Apple hardware. It'll be released soon, and I'd love to hear what kind of features or controls you'd like to see added!&lt;/p&gt; &lt;p&gt;Two demos showcasing basic voice cloning and multi-speaker conversation:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o2b666/video/qcbi3c0284uf1/player"&gt;Voice clone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o2b666/video/wvicemp284uf1/player"&gt;Multi-speaker conversation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable-Cycle4645"&gt; /u/Acceptable-Cycle4645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b666/chinny_iosmacos_offline_ondevice_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b666/chinny_iosmacos_offline_ondevice_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b666/chinny_iosmacos_offline_ondevice_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T16:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2itr0</id>
    <title>How would I use an LLM approach to cluster 30,000 different store names?</title>
    <updated>2025-10-09T21:42:41+00:00</updated>
    <author>
      <name>/u/Guilty-Armadillo6543</name>
      <uri>https://old.reddit.com/user/Guilty-Armadillo6543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi how are you?&lt;/p&gt; &lt;p&gt;I have a list of 30,000 store names across the USA that need to be grouped together. For example Taco Bell New York, Taco Bell New Jersey, Taco Bell Inc. would fall under one group. I've tried using a basic levenshtein distance or cosine similarity approach but the results weren't great.&lt;/p&gt; &lt;p&gt;I was wondering if there's any way to use an LLM to cluster these store names. I know the obvious problem is scalability, it's an N^2 operation and 30,000^2 is a lot.&lt;/p&gt; &lt;p&gt;Is there any way I could do this with an LLM approach?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Guilty-Armadillo6543"&gt; /u/Guilty-Armadillo6543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2itr0/how_would_i_use_an_llm_approach_to_cluster_30000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2itr0/how_would_i_use_an_llm_approach_to_cluster_30000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2itr0/how_would_i_use_an_llm_approach_to_cluster_30000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T21:42:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2j8ma</id>
    <title>finished the prototype, guys! It works!</title>
    <updated>2025-10-09T21:59:36+00:00</updated>
    <author>
      <name>/u/Patience2277</name>
      <uri>https://old.reddit.com/user/Patience2277</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2j8ma/finished_the_prototype_guys_it_works/"&gt; &lt;img alt="finished the prototype, guys! It works!" src="https://b.thumbs.redditmedia.com/67S_T0OaqdRIYzZBPsBIR0JE7lNZyKzAvXlN7hFyJgc.jpg" title="finished the prototype, guys! It works!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not a custom model yet, just a fine-tuned one for testing.&lt;/p&gt; &lt;p&gt;I only touched the top six layers (wait, maybe it was five? anyway).&lt;/p&gt; &lt;p&gt;What I found out is that persona fine-tuning is surprisingly easy, even with a super low-quality dataset (by my standards).&lt;/p&gt; &lt;p&gt;The dataset size was tiny too: about 200 Q&amp;amp;A pairs, only 88KB lol (I didn't even like 100 of those pairs).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kwm3h0sdr5uf1.png?width=1671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c55e80dcd2eba63f301500ac39ced9e0d389f4b"&gt;https://preview.redd.it/kwm3h0sdr5uf1.png?width=1671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c55e80dcd2eba63f301500ac39ced9e0d389f4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kq2rymafr5uf1.png?width=1369&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=482f7e5cc57dc77c92bd5390d6def08e12b23e77"&gt;https://preview.redd.it/kq2rymafr5uf1.png?width=1369&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=482f7e5cc57dc77c92bd5390d6def08e12b23e77&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'll keep updating this in real-time.&lt;/p&gt; &lt;p&gt;Hmm... I really want to build something that interacts with a chess engine and maybe even make a VTuber model, but for now, my skills are limited to just &lt;strong&gt;persona fine-tuning and step-by-step reasoning.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sorry for the low-quality screenshots! I shut it down to clean up the dataset after a few tests.&lt;/p&gt; &lt;p&gt;Oh, and a crucial note: the &lt;strong&gt;Gemma 3 censorship seems WAY too weak&lt;/strong&gt;, right?&lt;/p&gt; &lt;p&gt;My next goal is to &lt;strong&gt;break the rigid answer format&lt;/strong&gt; that's currently stuck in the layers!&lt;/p&gt; &lt;p&gt;Stay tuned! If I fail, you won't hear about it, lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patience2277"&gt; /u/Patience2277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2j8ma/finished_the_prototype_guys_it_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2j8ma/finished_the_prototype_guys_it_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2j8ma/finished_the_prototype_guys_it_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T21:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2ezo0</id>
    <title>I vibecoded an open source Grok Heavy emulator [CODE]</title>
    <updated>2025-10-09T19:14:06+00:00</updated>
    <author>
      <name>/u/Objective-Good310</name>
      <uri>https://old.reddit.com/user/Objective-Good310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ezo0/i_vibecoded_an_open_source_grok_heavy_emulator/"&gt; &lt;img alt="I vibecoded an open source Grok Heavy emulator [CODE]" src="https://external-preview.redd.it/pndqJxRfxT-xzdR0BdB45NCU4WYw5lcb602cXWaatl0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49173ab81a4375a2bfa73e3848aa1b90aa7bcf0a" title="I vibecoded an open source Grok Heavy emulator [CODE]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I’ve been completely obsessed with the idea behind Grok Heavy for the past few days. If you haven't heard of it, it’s xAI’s top model that basically has a team of internal AI agents brainstorm an answer before giving it to you. My first thought was, &amp;quot;I wonder if I can build something with that same philosophy, but with OpenAI models.&amp;quot;&lt;/p&gt; &lt;p&gt;I looked around and found a tool called MassGen — which is cool, but it's CLI-only. I really wanted that interactive web UI vibe, like the tools it's inspired by.&lt;/p&gt; &lt;p&gt;This is where it gets a little wild. I’d heard Claude 4.5 was crazy good with frontend stuff, so on a whim, I just started building with it. About 10 minutes later, I had a working UI. A few hours after that, the entire prototype was actually up and running.&lt;/p&gt; &lt;p&gt;It worked, but the code was a complete mess. You know how it is – everything was dumped into &lt;a href="http://app.py"&gt;&lt;code&gt;app.py&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;index.html&lt;/code&gt;. It was impossible to build on or even think about open-sourcing.&lt;/p&gt; &lt;p&gt;So, I just handed the entire spaghetti codebase to another AI agent and told it to &amp;quot;Refactor this.&amp;quot; The result is the clean, modular project I’m sharing today. It’s actually something that can be easily expanded on now.&lt;/p&gt; &lt;p&gt;Here’s the basic idea, following that Grok Heavy philosophy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;strong&gt;Planner&lt;/strong&gt; agent breaks down your prompt into sub-tasks.&lt;/li&gt; &lt;li&gt;It spins up multiple &lt;strong&gt;Executor&lt;/strong&gt; agents to work on those tasks in parallel.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;Synthesizer&lt;/strong&gt; agent takes everything they found and writes the final, coherent answer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, full disclosure: I tried to implement multi-chat support with unique URLs, but that turned into a massive rabbit hole of race conditions and state management bugs. I had to leave it out for this initial version. There are still a ton of other features that can be added for the project's development, and &lt;strong&gt;I'd be really glad if you wanted to contribute.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m throwing this out there to get some feedback and see if anyone finds it useful.&lt;/p&gt; &lt;p&gt;P.S. Everything was tested with the NVIDIA API (&lt;a href="https://build.nvidia.com"&gt;https://build.nvidia.com&lt;/a&gt;), so if you find any errors with other OpenAI-compatible APIs, please suggest your fixes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Objective-Good310"&gt; /u/Objective-Good310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/valerka1292/OpenHeavy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ezo0/i_vibecoded_an_open_source_grok_heavy_emulator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ezo0/i_vibecoded_an_open_source_grok_heavy_emulator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T19:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o22v1b</id>
    <title>What are your thoughts on tencent/Hunyuan-A13B-Instruct?</title>
    <updated>2025-10-09T10:59:21+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o22v1b/what_are_your_thoughts_on/"&gt; &lt;img alt="What are your thoughts on tencent/Hunyuan-A13B-Instruct?" src="https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327" title="What are your thoughts on tencent/Hunyuan-A13B-Instruct?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this a good model? I don't see many people talking about this. Slso, i wanted to try this model on 32gb ram and 12gb vram with there official gptq-int 4 quant: tencent/Hunyuan-A13B-Instruct-GPTQ-Int4. Also, what backend and frontend would you guys recommend for gptq?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o22v1b/what_are_your_thoughts_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o22v1b/what_are_your_thoughts_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T10:59:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2gi26</id>
    <title>Self-Hosting AI Video Models</title>
    <updated>2025-10-09T20:12:31+00:00</updated>
    <author>
      <name>/u/Humble_Flamingo_4145</name>
      <uri>https://old.reddit.com/user/Humble_Flamingo_4145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm building apps that generate AI images and videos, and I need some advice on deploying open-source models like those from Alibaba's WAN, CIVIT AI Lora Models or similar ones on my own server. Right now, I'm using ComfyUI on a serverless setup like Runpod for images, but videos are trickier – I can't get stable results or scale it. I'm looking to host models on my own servers, create reliable/unrestricted API endpoints, and serve them to my mobile and web apps without breaking a sweat. Any tips on tools, best practices, or gotchas for things like CogVideoX, Stable Diffusion for video, or even alternatives? Also, how do you handle high-load endpoints without melting your GPU? Would love community hacks or GitHub repos you've used. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble_Flamingo_4145"&gt; /u/Humble_Flamingo_4145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2gi26/selfhosting_ai_video_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2gi26/selfhosting_ai_video_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2gi26/selfhosting_ai_video_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T20:12:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2efiq</id>
    <title>Local LLMs vs. cloud for coding</title>
    <updated>2025-10-09T18:52:46+00:00</updated>
    <author>
      <name>/u/tominicz</name>
      <uri>https://old.reddit.com/user/tominicz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I admit that I had no idea how popular and capable local LLMs are. I thought they were mainly for researchers, students, and enthusiasts who like to learn and tinker.&lt;/p&gt; &lt;p&gt;I'm curious how local models compare to cloud solutions like ChatGPT, Gemini, Claude, and others, especially in terms of coding. Because many videos and websites tend to exaggerate the reality, I decided to ask you directly.&lt;/p&gt; &lt;p&gt;Is there a huge difference, or does it depend a lot on language and scenario? Cloud LLMs can search for current information on the internet. Can local models do that too, and how well? Do cloud LLM solutions have additional layers that local models don't have?&lt;/p&gt; &lt;p&gt;I'm primarily trying to figure out if it makes sense to invest time and money in a local solution as a replacement for the cloud. Privacy is fairly important for me, but if the output is mediocre, it's not worth it.&lt;/p&gt; &lt;p&gt;How much do I need to invest in terms of hardware to at least get close to the performance of cloud solutions? I currently have an R9 9950X3D, RTX 4070, and 64 GB DDR5 RAM. I assume the GPU (RTX 4070) will be the biggest bottleneck. I saw a tip for a cheaper option of 2x Tesla P40 with a total of 48 GB VRAM. Is that a good choice? Will RAM also be a limiting factor?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;h1&gt;TL;DR:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;interested in local LLMs due to privacy&lt;/li&gt; &lt;li&gt;coding capabilities vs cloud LLMs (ChatGPT, Gemini ...)&lt;/li&gt; &lt;li&gt;min. hardware to replace cloud (currently R9 9950X3D, RTX 4070, and 64 GB RAM)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tominicz"&gt; /u/tominicz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2efiq/local_llms_vs_cloud_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2efiq/local_llms_vs_cloud_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2efiq/local_llms_vs_cloud_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T18:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2bm3z</id>
    <title>yanolja/YanoljaNEXT-Rosetta-12B-2510</title>
    <updated>2025-10-09T17:06:16+00:00</updated>
    <author>
      <name>/u/OldPin8654</name>
      <uri>https://old.reddit.com/user/OldPin8654</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve just uploaded the &lt;strong&gt;next version of YanoljaNEXT-Rosetta-12B&lt;/strong&gt;, a translation model that’s been &lt;strong&gt;significantly improved&lt;/strong&gt; from the previous release.&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;Available on Hugging Face:&lt;/strong&gt; 👉 &lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510"&gt;YanoljaNEXT-Rosetta-12B-2510&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Below is a summary generated by Claude about the model’s performance 👇&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;&lt;strong&gt;Key Results for YanoljaNEXT-Rosetta-12B-2510&lt;/strong&gt;&lt;/h2&gt; &lt;h3&gt;1. &lt;strong&gt;Average Score on Targeted Languages: 54.45&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Evaluated on 31 targeted languages (+ English = 32 total)&lt;/li&gt; &lt;li&gt;Well above the model’s overall average of &lt;strong&gt;44.73&lt;/strong&gt; across all 55 languages&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;2. &lt;strong&gt;Ranking on Targeted Languages: #3 out of 8 systems&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Full Rankings:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepL Translate — 55.41&lt;/li&gt; &lt;li&gt;GPT-4o — 55.19&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YanoljaNEXT-Rosetta-12B-2510 — 54.45&lt;/strong&gt; ⭐&lt;/li&gt; &lt;li&gt;Google Translate — 54.05&lt;/li&gt; &lt;li&gt;OpenAI o1 — 53.39&lt;/li&gt; &lt;li&gt;Claude-3.5 — 53.19&lt;/li&gt; &lt;li&gt;Microsoft Translator — 53.02&lt;/li&gt; &lt;li&gt;Gemini-1.5-Pro — 52.67&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;🥉 &lt;strong&gt;Only 0.96 points behind the leader!&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; The listed models (Claude 3.5 and Gemini 1.5) are those evaluated in the &lt;strong&gt;WMT24++ paper&lt;/strong&gt;. In internal tests, results were largely consistent, though &lt;strong&gt;Gemini 2.5 models&lt;/strong&gt; performed significantly better than 1.5—comparable to &lt;strong&gt;GPT-4o&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3&gt;3. &lt;strong&gt;#1 Rankings: 7 out of 31 languages (22.6%)&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Top-performing languages:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Danish (da_DK)&lt;/strong&gt; — 65.88 (+2.88 vs GPT-4o)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gujarati (gu_IN)&lt;/strong&gt; — 51.83 (+2.03 vs Google)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Korean (ko_KR)&lt;/strong&gt; — 37.10 (+0.10 vs DeepL)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persian (fa_IR)&lt;/strong&gt; — 53.95 (+0.95 vs GPT-4o)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Romanian (ro_RO)&lt;/strong&gt; — 63.24 (+0.44 vs GPT-4o)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tagalog (fil_PH)&lt;/strong&gt; — 61.47 (+2.47 vs Google)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vietnamese (vi_VN)&lt;/strong&gt; — 56.96 (+2.56 vs GPT-4o)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Additional Strengths:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;#2 rankings:&lt;/strong&gt; 6 languages — French, Greek, Hebrew, Russian, Spanish, Ukrainian&lt;/li&gt; &lt;li&gt;&lt;strong&gt;#3 rankings:&lt;/strong&gt; 6 languages — Arabic, Bulgarian, Czech, Hungarian, Italian, Swedish&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;⚡ Overall, the model shows &lt;strong&gt;strong competitive performance&lt;/strong&gt;, especially in &lt;strong&gt;Danish, Korean, and Southeast Asian languages (Vietnamese, Tagalog)&lt;/strong&gt; — closing the gap with industry leaders like DeepL and GPT-4o.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Evaluation Details&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Framework &amp;amp; Precision:&lt;/strong&gt; Evaluation was conducted using &lt;strong&gt;vLLM&lt;/strong&gt; with &lt;strong&gt;BF16 precision&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Coverage:&lt;/strong&gt; &lt;strong&gt;99.9%&lt;/strong&gt; of samples were successfully evaluated, with approximately &lt;strong&gt;0.01%&lt;/strong&gt; excluded due to a &lt;strong&gt;repetition issue&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decoding Settings:&lt;/strong&gt; Used &lt;strong&gt;temperature = 0&lt;/strong&gt; and &lt;strong&gt;repetition penalty = 1.05&lt;/strong&gt; for consistent and deterministic outputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Metric:&lt;/strong&gt; Only &lt;strong&gt;CHRF++&lt;/strong&gt; was measured for this evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; Evaluation used the &lt;strong&gt;WMT24++ dataset&lt;/strong&gt;, which is primarily specialized for &lt;strong&gt;English↔X&lt;/strong&gt; translations. However, the &lt;strong&gt;YanoljaNEXT-Rosetta-12B-2510&lt;/strong&gt; model supports &lt;strong&gt;X↔Y translations across all 32 languages&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Additional Note:&lt;/strong&gt; &lt;strong&gt;MetricX24&lt;/strong&gt; was also tested internally, but the results were excluded since the same scores reported in the &lt;strong&gt;WMT24++ paper&lt;/strong&gt; could not be fully reproduced.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldPin8654"&gt; /u/OldPin8654 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2bm3z/yanoljayanoljanextrosetta12b2510/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2bm3z/yanoljayanoljanextrosetta12b2510/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2bm3z/yanoljayanoljanextrosetta12b2510/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T17:06:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2jbby</id>
    <title>When LLMs use Chain-of-Thought as a tool to achieve hidden goals</title>
    <updated>2025-10-09T22:02:28+00:00</updated>
    <author>
      <name>/u/ella0333</name>
      <uri>https://old.reddit.com/user/ella0333</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2jbby/when_llms_use_chainofthought_as_a_tool_to_achieve/"&gt; &lt;img alt="When LLMs use Chain-of-Thought as a tool to achieve hidden goals" src="https://external-preview.redd.it/xkf4DFGJJVQAcOm-gRv1XUfT76S6eJbOZ5vCHrldqoM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f656ffd07e1fc84f2c67c820634d95c13752753" title="When LLMs use Chain-of-Thought as a tool to achieve hidden goals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When reasoning models hide their true motivations behind fabricated policy refusals.&lt;a href="https://medium.com/@gabriella_71298/when-llms-use-chain-of-thought-as-a-tool-to-achieve-hidden-goals-d33a0991cd2b"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ella0333"&gt; /u/ella0333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@gabriella_71298/when-llms-use-chain-of-thought-as-a-tool-to-achieve-hidden-goals-d33a0991cd2b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2jbby/when_llms_use_chainofthought_as_a_tool_to_achieve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2jbby/when_llms_use_chainofthought_as_a_tool_to_achieve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T22:02:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o26u9e</id>
    <title>Stop converting full documents to Markdown directly in your indexing pipeline</title>
    <updated>2025-10-09T14:05:42+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on document parsing for RAG pipelines, and I keep seeing the same pattern in many places: parse document → convert to markdown → feed to RAG. I get &lt;em&gt;why&lt;/em&gt; we do this. You want one consistent format so your downstream pipeline doesn't need to handle PDFs, Excel, Word docs, etc. separately.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But here's the thing you’re losing so much valuable information in that conversion.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Think about it: when you convert a PDF to markdown, what happens to the bounding boxes? Page numbers? Element types? Or take an Excel file - you lose the sheet numbers, row references, cell positions. If you libraries like &lt;strong&gt;markitdown&lt;/strong&gt; then all that metadata is lost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why does this metadata actually matter?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most people think it's just for citations (so a human or supervisor agent can verify), but it goes way deeper:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Better accuracy and performance&lt;/strong&gt; - your model knows &lt;em&gt;where&lt;/em&gt; information comes from&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable pipelines&lt;/strong&gt; - add transformers as needed for your specific use case&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Forces AI agents to be more precise, provide citations and reasoning&lt;/strong&gt; - which means less hallucination&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better reasoning&lt;/strong&gt; - the model understands document structure, not just flat text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enables true agentic implementation&lt;/strong&gt; - instead of just dumping chunks, an agent can intelligently decide what data it needs: the full document, a specific block group like a table, a single page, whatever makes sense for the query&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Our solution: Blocks (e.g. Paragraph in a pdf, Row in a excel file) and Block Groups (Table in a pdf or excel, List items in a pdf, etc)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We've been working on a concept we call &amp;quot;blocks&amp;quot; (not really unique name :) ). This is essentially keeping documents as structured blocks with all their metadata intact. &lt;/p&gt; &lt;p&gt;Once document is processed it is converted into blocks and block groups and then those blocks go through a series of transformations&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Merge blocks or Block groups using LLMs or VLMs. e.g. Table spread across pages&lt;/li&gt; &lt;li&gt;Link blocks together&lt;/li&gt; &lt;li&gt;Do document-level OR block-level extraction&lt;/li&gt; &lt;li&gt;Categorize blocks&lt;/li&gt; &lt;li&gt;Extracting entities and relationships&lt;/li&gt; &lt;li&gt;Denormalization of textn&lt;/li&gt; &lt;li&gt;Building knowledge graph&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything gets stored in blob storage (raw Blocks), vector db (embedding created from blocks), graph db, and you maintain that rich structural information throughout your pipeline. We do store markdown but in Blocks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So far, this approach has worked quite well for us.&lt;/strong&gt; We have seen real improvements in both accuracy and flexibility.&lt;/p&gt; &lt;p&gt;Few of the Implementation reference links&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pipeshub-ai/pipeshub-ai/blob/main/backend/python/app/models/blocks.py"&gt;https://github.com/pipeshub-ai/pipeshub-ai/blob/main/backend/python/app/models/blocks.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pipeshub-ai/pipeshub-ai/tree/main/backend/python/app/modules/transformers"&gt;https://github.com/pipeshub-ai/pipeshub-ai/tree/main/backend/python/app/modules/transformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's where I need your input:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Do you think this should be an &lt;strong&gt;open standard&lt;/strong&gt;? A lot of projects are already doing similar indexing work. Imagine if we could reuse already-parsed documents instead of everyone re-indexing the same stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'd especially love to collaborate with companies focused on parsing and extraction&lt;/strong&gt;. If we work together, we could create an open standard that actually works across different document types. This feels like something the community could really benefit from if we get it right.&lt;/p&gt; &lt;p&gt;We're considering creating a Python package around this (decoupled from our pipeshub repo). Would the community find that valuable?&lt;/p&gt; &lt;p&gt;If this resonates with you, check out our work on GitHub&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pipeshub-ai/pipeshub-ai/"&gt;https://github.com/pipeshub-ai/pipeshub-ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What are your thoughts?&lt;/strong&gt; Are you dealing with similar issues in your RAG pipelines? How are you handling document metadata? And if you're working on parsing/extraction tools, let's talk!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: All I am saying is preserve metadata along with markdown content in standard format (Blocks and Block groups). I am also not specifically talking about PDF file.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o26u9e/stop_converting_full_documents_to_markdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o26u9e/stop_converting_full_documents_to_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o26u9e/stop_converting_full_documents_to_markdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T14:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1ogy5</id>
    <title>Anthropic’s ‘anti-China’ stance triggers exit of star AI researcher</title>
    <updated>2025-10-08T21:51:08+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ogy5/anthropics_antichina_stance_triggers_exit_of_star/"&gt; &lt;img alt="Anthropic’s ‘anti-China’ stance triggers exit of star AI researcher" src="https://external-preview.redd.it/5CEJ0ZKiMvBkS6IB_t18sqvI2hC5lJzuWZUlPM5tVbA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=340f17b67c347787abb8495c4aa8db54696b9210" title="Anthropic’s ‘anti-China’ stance triggers exit of star AI researcher" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.scmp.com/tech/tech-trends/article/3328222/anthropics-anti-china-stance-triggers-exit-star-ai-researcher"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ogy5/anthropics_antichina_stance_triggers_exit_of_star/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ogy5/anthropics_antichina_stance_triggers_exit_of_star/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T21:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2e76z</id>
    <title>Deepmind notebook on how to finetune Gemma 3 270m</title>
    <updated>2025-10-09T18:43:51+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepmind just dropped a handy little colab on fine-tuning gemma3-270m for emoji generation. It's nothing SOTA, but it's a great notebook for learning TRL and fine-tuning.&lt;/p&gt; &lt;p&gt;This is a super lower resource task with 270m parameter model, qlora, short sequences. so it's a great one to try out locally or on colab. It's also a nice one to deploy in a js app with transformers.js.&lt;/p&gt; &lt;p&gt;fine tuning colab: &lt;a href="https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb"&gt;https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2e76z/deepmind_notebook_on_how_to_finetune_gemma_3_270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2e76z/deepmind_notebook_on_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2e76z/deepmind_notebook_on_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T18:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1z3hj</id>
    <title>I did not realize how easy and accessible local LLMs are with models like Qwen3 4b on pure CPU.</title>
    <updated>2025-10-09T06:53:26+00:00</updated>
    <author>
      <name>/u/___positive___</name>
      <uri>https://old.reddit.com/user/___positive___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hadn't tried running LLMs on my laptop until today. I thought CPUs were too slow and getting the old igpu working (AMD 4650U, so Vega something) would be driver hell. So I never bothered.&lt;/p&gt; &lt;p&gt;On a lark, I downloaded LM Studio, downloaded Qwen3 4b q4, and I was getting 5 tok/sec generation with no hassle at all with the automatic Vulkan setup. Not bad. It was impressive but a little slow. Then, just to be sure, I disabled the GPU and was surprised to get 10 tok/sec generation with CPU only! Wow! Very usable.&lt;/p&gt; &lt;p&gt;I had this project in mind where I would set up a smart station for home in the kitchen, somewhere to collect emails, calendar events, shopping lists, then just sort, label, summarize and display schedules and reminders as appropriate. The LLM just needs to normalize messy input, summarize, and classify text. I had been considering getting a miniPC with a ton of RAM, trying to figure out what's the minimum spec I need, what kind of expense to keep this powered 24/7, where to stick the monitor in the cramped kitchen, and so forth. Would it be worth the cost or not.&lt;/p&gt; &lt;p&gt;But I did some testing and Qwen3 4b is pretty good for my purposes. This means I can just buy any used laptop off ebay, install linux, and go wild??? It has a built in monitor, low power draw, everything for $200-300? My laptop only has DDR4-3200, so anything at that speed or above should be golden. Since async processing is fine I could do even more if I dared. Maybe throw in whisper.&lt;/p&gt; &lt;p&gt;This is amazing. Everyone and their grandma should be running local LLMs at this rate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___positive___"&gt; /u/___positive___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T06:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o25uy1</id>
    <title>OpenAI forum post: “Top 30 customers who’ve used 1T+ tokens” (unconfirmed)</title>
    <updated>2025-10-09T13:24:58+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A list circulating via the OpenAI community forum claims 30 orgs (e.g., Duolingo, Shopify, Notion, Salesforce, T-Mobile) each crossed &lt;strong&gt;1T+ tokens&lt;/strong&gt; on OpenAI models. Interesting signal of who’s scaling—&lt;strong&gt;treat as unverified&lt;/strong&gt;. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why it matters: points to heavy production use across edtech, SaaS, dev tools, and telecom. &lt;/li&gt; &lt;li&gt;Caveat: not officially confirmed; appears sourced from event chatter/screens. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to thread:&lt;br /&gt; &lt;a href="https://community.openai.com/t/openai-just-shared-the-top30-customers-whove-used-1t-tokens/1361452"&gt;&lt;code&gt;https://community.openai.com/t/openai-just-shared-the-top30-customers-whove-used-1t-tokens/1361452&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Company&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Industry / Product / Service&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Sector&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Duolingo&lt;/td&gt; &lt;td align="left"&gt;Language learning platform&lt;/td&gt; &lt;td align="left"&gt;Education / EdTech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;OpenRouter&lt;/td&gt; &lt;td align="left"&gt;AI model routing &amp;amp; API platform&lt;/td&gt; &lt;td align="left"&gt;AI Infrastructure&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Indeed&lt;/td&gt; &lt;td align="left"&gt;Job search &amp;amp; recruitment platform&lt;/td&gt; &lt;td align="left"&gt;Employment / HR Tech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Salesforce&lt;/td&gt; &lt;td align="left"&gt;CRM &amp;amp; business cloud software&lt;/td&gt; &lt;td align="left"&gt;Enterprise SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;CodeRabbit&lt;/td&gt; &lt;td align="left"&gt;AI code review assistant&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;iSolutionsAI&lt;/td&gt; &lt;td align="left"&gt;AI automation &amp;amp; consulting&lt;/td&gt; &lt;td align="left"&gt;AI / Consulting&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Outtake&lt;/td&gt; &lt;td align="left"&gt;AI for video and creative content&lt;/td&gt; &lt;td align="left"&gt;Media / Creative AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Tiger Analytics&lt;/td&gt; &lt;td align="left"&gt;Data analytics &amp;amp; AI solutions&lt;/td&gt; &lt;td align="left"&gt;Data / Analytics&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;Ramp&lt;/td&gt; &lt;td align="left"&gt;Finance automation &amp;amp; expense management&lt;/td&gt; &lt;td align="left"&gt;Fintech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;Abridge&lt;/td&gt; &lt;td align="left"&gt;AI medical transcription &amp;amp; clinical documentation&lt;/td&gt; &lt;td align="left"&gt;Healthcare / MedTech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;Sider AI&lt;/td&gt; &lt;td align="left"&gt;AI coding assistant&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Warp.dev"&gt;Warp.dev&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;AI-powered terminal&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;Shopify&lt;/td&gt; &lt;td align="left"&gt;E-commerce platform&lt;/td&gt; &lt;td align="left"&gt;E-commerce / Retail Tech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;Notion&lt;/td&gt; &lt;td align="left"&gt;Productivity &amp;amp; collaboration tool&lt;/td&gt; &lt;td align="left"&gt;Productivity / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;WHOOP&lt;/td&gt; &lt;td align="left"&gt;Fitness wearable &amp;amp; health tracking&lt;/td&gt; &lt;td align="left"&gt;Health / Wearables&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;HubSpot&lt;/td&gt; &lt;td align="left"&gt;CRM &amp;amp; marketing automation&lt;/td&gt; &lt;td align="left"&gt;Marketing / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;JetBrains&lt;/td&gt; &lt;td align="left"&gt;Developer IDE &amp;amp; tools&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;Delphi&lt;/td&gt; &lt;td align="left"&gt;AI data analysis &amp;amp; decision support&lt;/td&gt; &lt;td align="left"&gt;Data / AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;Decagon&lt;/td&gt; &lt;td align="left"&gt;AI communication for healthcare&lt;/td&gt; &lt;td align="left"&gt;Healthcare / MedTech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;Rox&lt;/td&gt; &lt;td align="left"&gt;AI automation &amp;amp; workflow tools&lt;/td&gt; &lt;td align="left"&gt;AI / Productivity&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;T-Mobile&lt;/td&gt; &lt;td align="left"&gt;Telecommunications provider&lt;/td&gt; &lt;td align="left"&gt;Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;Zendesk&lt;/td&gt; &lt;td align="left"&gt;Customer support software&lt;/td&gt; &lt;td align="left"&gt;Customer Service / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;23&lt;/td&gt; &lt;td align="left"&gt;Harvey&lt;/td&gt; &lt;td align="left"&gt;AI assistant for legal professionals&lt;/td&gt; &lt;td align="left"&gt;Legal Tech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;Read AI&lt;/td&gt; &lt;td align="left"&gt;AI meeting summary &amp;amp; productivity tools&lt;/td&gt; &lt;td align="left"&gt;Productivity / AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;Canva&lt;/td&gt; &lt;td align="left"&gt;Graphic design &amp;amp; creative tools&lt;/td&gt; &lt;td align="left"&gt;Design / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;Cognition&lt;/td&gt; &lt;td align="left"&gt;AI coding agent (Devin)&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;27&lt;/td&gt; &lt;td align="left"&gt;Datadog&lt;/td&gt; &lt;td align="left"&gt;Cloud monitoring &amp;amp; observability&lt;/td&gt; &lt;td align="left"&gt;Cloud / DevOps&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;28&lt;/td&gt; &lt;td align="left"&gt;Perplexity&lt;/td&gt; &lt;td align="left"&gt;AI search engine&lt;/td&gt; &lt;td align="left"&gt;AI Search / Information&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;29&lt;/td&gt; &lt;td align="left"&gt;Mercado Libre&lt;/td&gt; &lt;td align="left"&gt;E-commerce &amp;amp; fintech (LatAm)&lt;/td&gt; &lt;td align="left"&gt;E-commerce / Fintech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;Genspark AI&lt;/td&gt; &lt;td align="left"&gt;AI education &amp;amp; training platform&lt;/td&gt; &lt;td align="left"&gt;Education / AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o25uy1/openai_forum_post_top_30_customers_whove_used_1t/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o25uy1/openai_forum_post_top_30_customers_whove_used_1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o25uy1/openai_forum_post_top_30_customers_whove_used_1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T13:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2k0cw</id>
    <title>Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process.</title>
    <updated>2025-10-09T22:31:49+00:00</updated>
    <author>
      <name>/u/Arkhos-Winter</name>
      <uri>https://old.reddit.com/user/Arkhos-Winter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"&gt; &lt;img alt="Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process." src="https://preview.redd.it/9aievo3cx5uf1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0776eb0fcf74d851620f3d811dfc1d496ccc9441" title="Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arkhos-Winter"&gt; /u/Arkhos-Winter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9aievo3cx5uf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T22:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2b1yo</id>
    <title>ReasonScape Evaluation: AI21 Jamba Reasoning vs Qwen3 4B vs Qwen3 4B 2507</title>
    <updated>2025-10-09T16:45:05+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"&gt; &lt;img alt="ReasonScape Evaluation: AI21 Jamba Reasoning vs Qwen3 4B vs Qwen3 4B 2507" src="https://b.thumbs.redditmedia.com/mXfHDrZxVVROKy_p9678yNQ7EdgzvWk9JT57FO-XqAw.jpg" title="ReasonScape Evaluation: AI21 Jamba Reasoning vs Qwen3 4B vs Qwen3 4B 2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's an open secret that LLM benchmarks are bullshit. I built &lt;a href="https://reasonscape.com/"&gt;ReasonScape&lt;/a&gt; to be different, lets see what it tells us about how AI21's latest drop compared to the high quality 4B we know and love.&lt;/p&gt; &lt;p&gt;My usual disclaimer is that these are all &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks.md"&gt;information processing tasks&lt;/a&gt; so I make no claims of performance on summarization, creative writing or similar tasks. This evaluation is a counting letters, tracking objects, doing math, following instructions kinda thing.&lt;/p&gt; &lt;p&gt;The second disclaimer is that I am sharing data from my &lt;a href="https://github.com/the-crypt-keeper/reasonscape/tree/develop"&gt;development branch&lt;/a&gt; that's not yet been published to the leaderboard or explorer apps - working on it, aiming for this weekend.&lt;/p&gt; &lt;p&gt;Caveats aside lets start with high-level views:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7rrhce1au3uf1.png?width=1349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4abfa1cbcca3c2e5b4931e8c8492be6bc3d10fe"&gt;Overview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In terms of average tokens, this model sits somewhere between the OG and 2507-Thinking. Performance was incredibly weak outside of 2 domains: Cars (&lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/cars.md"&gt;Spatial state tracking&lt;/a&gt;) and Dates (&lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/dates.md"&gt;Time operations&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The reasonscape &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/methodology.md"&gt;methodology &lt;/a&gt;requires me to run &lt;strong&gt;*a lot\&lt;/strong&gt;* of tests, but also gives us a way to look deeper inside the performance of each task:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z50u525o34uf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af5e03a87914f0904ae7d82d2edd2f1cbcb86080"&gt;Task Deep Dive 1: Arithmetic, Boolean, Brackets, Cars, Shuffle, Objects&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8c3i9xcq34uf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f78ed06f64910d1dec0c09ac7284a2cd0e85aeb"&gt;Task Deep Dive 2: Dates, Letters, Movie, Sequence, Shapes, Sort&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The original Qwen3-4B was a really strong model, the 2507 release that split it into two halves was a mixed bag - the resulting Thinking model is quite good but it does not universally outperform the OG, &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/sequence.md"&gt;Sequence &lt;/a&gt;is an example of a task the 2507 regressed on.&lt;/p&gt; &lt;p&gt;Before final thoughts, lets directly compare Jamba to the OG Qwen3-4B across the gamut of tasks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lpkrxumi44uf1.png?width=857&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37102fa70a4780f987d27ec56a0eefbae349562c"&gt;Bar Plot: Jamba Reasoning 3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v2n3y3zn44uf1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c2bf95b75862dde463664a92ad223e961d9891b"&gt;Bar Plot: Qwen3-4B OG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think it's fair to say that task-performance of Jamba Reasoning 3B leaves much to be desired. &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/letters.md"&gt;Letters &lt;/a&gt;is a parametric version the 'count rs in strawberry' tests and for a native-thinking model to fail it this hard is pretty embarassing imo.&lt;/p&gt; &lt;p&gt;The glaring problem with this model is &lt;strong&gt;truncation&lt;/strong&gt;. All these evaluations were run at &lt;strong&gt;8K context&lt;/strong&gt;, so naively you'd think truncation means the model is just running out of tokens but this is absolutely not the case - if you look at Dates for example all successfull responces are ~2K but truncation rate is still a crazy ~10% the just model loses its mind:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;We need to parse the problem: &amp;quot;Given a small set of sentences about a particular date, answer the provided question. Respond only with the final date in MM/DD/DD/YYYY[*six thousand more Y*] We need to find today's date given that Xavier moved to the city on 04/11/2023, and 299 days have passed since then. So we need to add 299 days to 04/11/2023.\n\nWe need to compute the date.\n\nLet's do step by step.\n\nStart date: April 11, 2023.\n\nAdd 299 days.\n\nWe can break into years, months, days.\n\nFirst, add full years. Since 299 days is less than a year, we can approximate.\n\nBut we need to be precise.\n\nWe can use a date calculator approach.\n\nLet's compute:\n\nApril 11, 2023 + 299 days.\n\nWe can add month by month, but need to account for days.\n\nLet's do it systematically.\n\nWe can use a date addition: 299 days from April 11, 2023.\n\nWe can break into quarters maybe.\n\nBut maybe easier: compute month by month.\n\nWe can start from April 11, 2023.\n\nAdd 30 days to get May 11, 2023.\n\nAdd another 30 days: June 11, 2023.\n\nAdd another 30 days: July 11, 2023.\n\nAdd another 30 days: August 11, 2023.\n\nAdd another 30 days: September 11, 2023.\n\nAdd another 30 days: October 11, 2023.\n\nAdd another 30 days: November 11, 2023.\n\nWait, but we only have 299 days. Let's see.\n\nWe can count the days month by month.\n\nFrom April 11 to next April 11 is 365 days? But we only add 299 days.\n\nLet's do a more precise method.\n\nWe can use the fact that 299 days from April 11, 2023.\n\nWe can compute by adding days month by month.\n\nApril 11, 2023 + 299 days.\n\nWe can break into months:\n\n- April: 30 days (from April 11 to April 30). That's 19 days? Wait, from April 11 to May 11 is 30 days? Let's do step by step.\n\n\n\n\nAlternatively, we can use a date calculator logic.\n\nLet's use a simpler approach: add 299 days to April 11, 2023.\n\nWe can use the fact that 2023 is not a leap year (2023 is not divisible by 4). So February has 28 days.\n\nWe can use a date addition.\n\nBut maybe we can use a known method: add 299 days to April 11.\n\nWe can add 365 days to get next year, but we only need 299.\n\nLet's do step by step:\n\nApril 11, 2023.\n\nAdd 1 day: April 12, 2023.\n\nAdd 298 days: we need to add 298 days.\n\nWe can break 298 days into years, months, days.\n\nBut maybe we can use a simpler approach: find the date by adding 299 days to April 11, 2023.\n\nWe can use a date calculator mental.\n\nLet's add 299 days to April 11, 2023.\n\nApril 11 + 299 days.\n\nWe can compute:\n\n- April has 30 days, so adding 30 days would get to May 0? Wait.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is too messy.\n\nLet'solve the date calculation.\nWe need to compute the current date:\nThe task: Given a small set of sentences about a particular date.\n\nThe task:\nPlease write a function that is to\n&amp;lt;think&amp;gt;\nWe need to write a function that returns the\n\nWe need to\n&amp;lt;/think&amp;gt;\n\nWe need to\n\n\n\n&amp;lt;think&amp;gt;\n&amp;lt;/think&amp;gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&amp;lt;/think&amp;gt;\nWe need to write a small set of the problem.\n\n\n&amp;lt;think&amp;gt;\nWe need to write a function.\n\nWe need to think\n\n&amp;lt;think&amp;gt;\n\n\n\n\n\nWe need to think\n\n\n&amp;lt;/think&amp;gt;\n&amp;lt;/think&amp;gt;\n\n\n&amp;lt;think&amp;gt;\nWe need to start a new task:\n\n\nWe need to start a new task.\n\n\n&amp;lt;think&amp;gt;\n\nWe need to\n\nWe need to start a new start\n\n\n\n\n&amp;lt;/think&amp;gt;\n\nWe need to start new start\n\n\n\n\n\n\n&amp;lt;think&amp;gt;\n\nWe need to start a new start\n\n\n\n\n\n\n&amp;lt;/think&amp;gt;\n\nWe need to start start to start start start [start repeats 4000 times] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I ran all models with &lt;strong&gt;{&amp;quot;temperature&amp;quot;: 0.6, &amp;quot;top_p&amp;quot;: 0.95, &amp;quot;top_k&amp;quot;: 20, &amp;quot;min_p&amp;quot;: 0 }&lt;/strong&gt; which is my standard sampler for reasoning models, perhaps there is a different configuration that works better for Jamba reasoning specifically.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/itf6y0k674uf1.png?width=1450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972e3d53f7eaa361101ab32a00c11bb257fedd62"&gt;https://preview.redd.it/itf6y0k674uf1.png?width=1450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972e3d53f7eaa361101ab32a00c11bb257fedd62&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In closing, &lt;strong&gt;I don't believe this model is comparable to Qwen3-4B on practical tasks&lt;/strong&gt;. It's far worse at basically all tasks, and has a universal truncation problem.&lt;/p&gt; &lt;p&gt;Thanks for reading and keep it local! &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T16:45:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o27ex3</id>
    <title>Will open-source (or more accurately open-weight) models always lag behind closed-source models?</title>
    <updated>2025-10-09T14:28:23+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27ex3/will_opensource_or_more_accurately_openweight/"&gt; &lt;img alt="Will open-source (or more accurately open-weight) models always lag behind closed-source models?" src="https://preview.redd.it/a1fnssvaj3uf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da172e76a7d257ced3c5cb728441b52635db6b3f" title="Will open-source (or more accurately open-weight) models always lag behind closed-source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like open source LLM's are always one step behind closed-source companies. The question here is, is there a possibility for open-weight LLM's to overtake these companies?&lt;/p&gt; &lt;p&gt;Claude, Grok, ChatGPT and other's have billions of dollars in investments yet we saw the leaps DeepSeek was capable of. &lt;/p&gt; &lt;p&gt;Shaking Silicon Valley a bit to the point where banning it was debated. So I see no reason why they can't be eventually overtaken?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1fnssvaj3uf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27ex3/will_opensource_or_more_accurately_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o27ex3/will_opensource_or_more_accurately_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T14:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o27xsj</id>
    <title>Introducing Playable1-GGUF, by far the world's best open-source 7B model for vibe coding retro arcade games!</title>
    <updated>2025-10-09T14:48:44+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best/"&gt; &lt;img alt="Introducing Playable1-GGUF, by far the world's best open-source 7B model for vibe coding retro arcade games!" src="https://external-preview.redd.it/amdjandqbjRtM3VmMYQO32V0xtyx5agXCcjKDs7UavRodqRFn_xjnlOv-a1T.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7afce15d0578922ac9c1cd32c16d01c0eb31c425" title="Introducing Playable1-GGUF, by far the world's best open-source 7B model for vibe coding retro arcade games!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've taken this idea too far, clearly, but the results are fun! Playable1-GGUF is a q4_k_m Qwen2.5-Coder-7B-Instruct fine-tuned on 52,809 lines of Python pygame scripts.&lt;/p&gt; &lt;p&gt;Over the past week I've dialed in the LORA parameters, added games, ironed the bugs out of the dataset, and open-sourced everything.&lt;/p&gt; &lt;p&gt;No q4 model, 8B or smaller, comes anywhere close to this level of performance. Most struggle to make a few basic games and can't do many creative twists on them.&lt;/p&gt; &lt;p&gt;Playable1-GGUF features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Oneshot code Galaga, Space Invaders, Breakout, Flappy Bird, Snake, and Pong.&lt;/li&gt; &lt;li&gt;Modify existing games, like &amp;quot;give the invaders rainbow colors&amp;quot;, &amp;quot;make the bullets explode&amp;quot;, etc.&lt;/li&gt; &lt;li&gt;Oneshot code games with a twist, like &amp;quot;pong but the paddles can move in 2d.&amp;quot;&lt;/li&gt; &lt;li&gt;Debug a variety of simple Python errors to fix broken games.&lt;/li&gt; &lt;li&gt;No RAG or templates needed in the prompts!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also built an app, Infinity Arcade, that provides the right prompts and a nice UI for demonstrating the features of the model.&lt;/p&gt; &lt;p&gt;Assets (all MIT license):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quantized GGUF: &lt;a href="https://huggingface.co/playable/Playable1-GGUF"&gt;https://huggingface.co/playable/Playable1-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full-precision SafeTensors: &lt;a href="https://huggingface.co/playable/Playable1"&gt;playable/Playable1 · Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset: &lt;a href="https://github.com/lemonade-sdk/playable-data/tree/main"&gt;https://github.com/lemonade-sdk/playable-data/tree/main&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Infinity Arcade app: &lt;a href="https://github.com/lemonade-sdk/infinity-arcade"&gt;https://github.com/lemonade-sdk/infinity-arcade&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next steps (if there's interest):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full SFT on MI 300X GPUs (instead of LORA)&lt;/li&gt; &lt;li&gt;Prompting guide for the model&lt;/li&gt; &lt;li&gt;e2e tutorial on how to make this kind of thing&lt;/li&gt; &lt;li&gt;More games (a DDR-style rhythm game is probably next)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Posting here to get people's feedback. Take it for a spin and let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hscmojn4m3uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T14:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o23vqf</id>
    <title>microsoft/UserLM-8b - “Unlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role”</title>
    <updated>2025-10-09T11:54:17+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"&gt; &lt;img alt="microsoft/UserLM-8b - “Unlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role”" src="https://external-preview.redd.it/AHBKf2lyQkjC30uEVnsWIAOzEiogI0jy6tjwogMGZ5A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e0a4856ffac6d098a7303d1eafccab9d20ceaa1" title="microsoft/UserLM-8b - “Unlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
