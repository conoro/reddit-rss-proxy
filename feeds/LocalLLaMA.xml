<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-08T07:38:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ph219c</id>
    <title>Structuring context files to guide LLM code generation?</title>
    <updated>2025-12-08T03:29:34+00:00</updated>
    <author>
      <name>/u/gosh</name>
      <uri>https://old.reddit.com/user/gosh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph219c/structuring_context_files_to_guide_llm_code/"&gt; &lt;img alt="Structuring context files to guide LLM code generation?" src="https://preview.redd.it/q2zgnupnfw5g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=84ed3916fbc5e469cae68c8ac5084be2d526f49d" title="Structuring context files to guide LLM code generation?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a way to make an LLM write better code. I use a search tool called &lt;a href="https://github.com/perghosh/Data-oriented-design/releases/tag/cleaner.1.1.0"&gt;cleaner&lt;/a&gt; to gather info and put it in a file. I then give this file to the LLM as background context. This tells the model how to generate, and make it more accurate. &lt;/p&gt; &lt;p&gt;Have started to implement this using json, but are there better formats? Also are there some strange things when sending files to LLM, like that it is important to place important information at start of the file or does that matter?&lt;/p&gt; &lt;p&gt;What are the best practices for describing/structuring this kind of background file so the LLM uses it effectively?&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: cleaner are able to clean code but to clean it has to find code, so finding things is the main logic, just to explain the name.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gosh"&gt; /u/gosh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2zgnupnfw5g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph219c/structuring_context_files_to_guide_llm_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph219c/structuring_context_files_to_guide_llm_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T03:29:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgrevr</id>
    <title>Pro tip for Local LLM usage on the phone</title>
    <updated>2025-12-07T19:37:46+00:00</updated>
    <author>
      <name>/u/Seglem</name>
      <uri>https://old.reddit.com/user/Seglem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgrevr/pro_tip_for_local_llm_usage_on_the_phone/"&gt; &lt;img alt="Pro tip for Local LLM usage on the phone" src="https://a.thumbs.redditmedia.com/AJCKYWs4WfxH5ecM1yeAO1JYZeQEd6VYjDEAyblFIB0.jpg" title="Pro tip for Local LLM usage on the phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have it plugged in a charger and chat/work away. By classifying your LLM app of choice as a game, you can access the pause charging when &amp;quot;playing&amp;quot; in order to not heat up and throttle performance. But they use the power from the charger directly, instead of going through the battery, saving heat, battery cycles/wear and keeping the performance fast and the phone cooler. &lt;/p&gt; &lt;p&gt;I've also got a BodyGuardz Paradigm Pro case for my s25ultra, with better cooling than 99% of cases while protecting. And I sometimes use Baseus MagPro II. It has a fan so the charging and phone is cool &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Seglem"&gt; /u/Seglem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pgrevr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgrevr/pro_tip_for_local_llm_usage_on_the_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgrevr/pro_tip_for_local_llm_usage_on_the_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T19:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph4x4e</id>
    <title>Is colab pro or colab enterprise would be enough for finetuning LLMs?</title>
    <updated>2025-12-08T06:01:22+00:00</updated>
    <author>
      <name>/u/Motor_Ad6405</name>
      <uri>https://old.reddit.com/user/Motor_Ad6405</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, i was wondering if I can finetune models like 3B, 8B, 14B with 256k context window in google colab pro or enterprise without issues? I plan to finetune it using unsloth and Qlora for peft. I am still a beginner in finetuning and was wondering if anyone can provide me with some suggestions and ideas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor_Ad6405"&gt; /u/Motor_Ad6405 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph4x4e/is_colab_pro_or_colab_enterprise_would_be_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph4x4e/is_colab_pro_or_colab_enterprise_would_be_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph4x4e/is_colab_pro_or_colab_enterprise_would_be_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T06:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pggss8</id>
    <title>I'm tired of claude limits, what's the best alternative? (cloud based or local llm)</title>
    <updated>2025-12-07T12:04:59+00:00</updated>
    <author>
      <name>/u/Dry_Explanation_7774</name>
      <uri>https://old.reddit.com/user/Dry_Explanation_7774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone I hope y'all having a great day.&lt;/p&gt; &lt;p&gt;I've been using Claude Code since they released but I'm tired of the usage limits they have even when paying subscription.&lt;/p&gt; &lt;p&gt;I'm asking here since most of you have a great knowledge on what's the best and efficient way to run AI be it online with API or running a local LLM.&lt;/p&gt; &lt;p&gt;I'm asking, what's the best way to actually run Claude at cheap rates and at the same time getting the best of it without that ridiculous usage limits? &lt;/p&gt; &lt;p&gt;Or is there any other model that gives super similar or higher results for &amp;quot;coding&amp;quot; related activities but at the same time super cheap?&lt;/p&gt; &lt;p&gt;Or any of you recommend running my own local llm? which are your recommendations about this?&lt;/p&gt; &lt;p&gt;I currently have a GTX 1650 SUPER and 16GB RAM, i know it's super funny lol, but just lyk my current specs, so u can recommend me to buy something local or just deploy a local ai into a &amp;quot;custom ai hosting&amp;quot; and use the API?&lt;/p&gt; &lt;p&gt;I know there are a lot of questions, but I think you get my idea. I wanna get started to use the &amp;quot;&amp;quot;&amp;quot;tricks&amp;quot;&amp;quot;&amp;quot; that some of you use in order to use AI with the highest performace and at lowest rate.&lt;/p&gt; &lt;p&gt;Looking forward to hear ideas, recommendations or guidance!&lt;/p&gt; &lt;p&gt;Thanks a lot in advance, and I wish y'all a wonderful day :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Explanation_7774"&gt; /u/Dry_Explanation_7774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T12:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgoezb</id>
    <title>What are the cons of MXFP4?</title>
    <updated>2025-12-07T17:43:34+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering that we can make the model FP16 and fine-tune it and then quantize to MXFP4 again,and the model will be robust because it was trained with QAT,what would be the cons? MXFP4 is (almost) virtually lossless,not FP16 but near-lossless,and it cuts training cost into the half compared to FP16? (FP8 won't be exactly the half because some layers will be kept in FP16 or FP32,so usually like 30% less) while MXFP4 still uses layers that are in higher precision the MoE layers are almost always in 4-bit and that's where the bulk of the computation go,so why it's not the new route? Especially it's standardized so it's verified to be in production and we have seen that with GPT-OSS,I found that MXFP4 gets much less loss even when they get upscaled to FP16 and then quantized to something like INT4 (which has wide compatibility with all types of hardware) compared to model that are trained in FP16.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgoezb/what_are_the_cons_of_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgoezb/what_are_the_cons_of_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgoezb/what_are_the_cons_of_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T17:43:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph5m7b</id>
    <title>Nvme offloading possible in mlx or llamacpp?</title>
    <updated>2025-12-08T06:42:23+00:00</updated>
    <author>
      <name>/u/BABA_yaaGa</name>
      <uri>https://old.reddit.com/user/BABA_yaaGa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to run an 80 Qwen 3 Next model (6bit quantized) using lmstudio on my MacBook m4 max with 48gb unified memory. It crashes every time before outputting the first token no matter how small context size I set or use the kv quantization.&lt;/p&gt; &lt;p&gt;Is there any way to offload layers of MOE to nvme during the inference in either mlx or llama cpp? I know it is going to be very slow but still.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BABA_yaaGa"&gt; /u/BABA_yaaGa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5m7b/nvme_offloading_possible_in_mlx_or_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5m7b/nvme_offloading_possible_in_mlx_or_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5m7b/nvme_offloading_possible_in_mlx_or_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T06:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph3avj</id>
    <title>I got tired of my agents losing context on topic shifts, so I hacked together a branch router - thoughts?</title>
    <updated>2025-12-08T04:33:43+00:00</updated>
    <author>
      <name>/u/scotty595</name>
      <uri>https://old.reddit.com/user/scotty595</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been messing with multi-turn agents and kept hitting the same wall: conversation goes A → B → back to A, and the LLM has no idea what &amp;quot;A context&amp;quot; even means anymore because it's buried under B.&lt;/p&gt; &lt;p&gt;So I built a thing that tags each message as STAY/BRANCH/ROUTE and only pulls relevant history per branch. Uses an LLM call to classify (yeah, I know, LLM-to-manage-LLM, but it actually works for this), working on embeddings as the next step.&lt;/p&gt; &lt;p&gt;~2.7k lines, probably over-engineered, definitely has edge cases I haven't hit yet.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DriftOS/driftos-core"&gt;https://github.com/DriftOS/driftos-core&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone else has tried solving this differently - I looked at memGPT but wanted something lighter.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scotty595"&gt; /u/scotty595 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3avj/i_got_tired_of_my_agents_losing_context_on_topic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3avj/i_got_tired_of_my_agents_losing_context_on_topic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3avj/i_got_tired_of_my_agents_losing_context_on_topic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T04:33:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgwznn</id>
    <title>Non agentic uses of LLMs for coding</title>
    <updated>2025-12-07T23:29:01+00:00</updated>
    <author>
      <name>/u/WasteTechnology</name>
      <uri>https://old.reddit.com/user/WasteTechnology</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to answers to this post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;It seems that most people believe that local LLMs for coding are far behind hosted models, at least for agentic coding.&lt;/p&gt; &lt;p&gt;However, there's a question, is there any other case? Do you use them for tab completion, next edit prediction, code review, asking questions about code? Which among these use cases are good enough for local LLMs to be usable? Which tooling do you use for them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WasteTechnology"&gt; /u/WasteTechnology &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwznn/non_agentic_uses_of_llms_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwznn/non_agentic_uses_of_llms_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwznn/non_agentic_uses_of_llms_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T23:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph3z94</id>
    <title>Biggest vision-capable model that can run on a Strix Halo 128 GB?</title>
    <updated>2025-12-08T05:08:43+00:00</updated>
    <author>
      <name>/u/Daniel_H212</name>
      <uri>https://old.reddit.com/user/Daniel_H212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something better than Qwen3-VL-30B-A3B, preferably matching or exceeding Qwen3-VL-32B while being easier to run (say, large MoE, gpt-oss sized or GLM-4.5-air sized). Need strong text reading and document layout understanding capabilities.&lt;/p&gt; &lt;p&gt;Also needs to be relatively smart in text generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daniel_H212"&gt; /u/Daniel_H212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3z94/biggest_visioncapable_model_that_can_run_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3z94/biggest_visioncapable_model_that_can_run_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3z94/biggest_visioncapable_model_that_can_run_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T05:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgxftt</id>
    <title>Does the "less is more" principle apply to AI agents?</title>
    <updated>2025-12-07T23:49:41+00:00</updated>
    <author>
      <name>/u/8ta4</name>
      <uri>https://old.reddit.com/user/8ta4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sketching out an idea for a project. I'm wrestling with this idea: whether &amp;quot;less is more&amp;quot; applies to AI agents.&lt;/p&gt; &lt;p&gt;You see all these demos with agents that can browse the web, use tools, call functions, and all that. But my gut reaction is that it's all function and games until an agent decides to call some tool it doesn't need, poisons its context with irrelevant info, and makes the final output worse.&lt;/p&gt; &lt;p&gt;This is making me lean into constraining the agents for my project. I'm documenting my thinking &lt;a href="https://github.com/8ta4/spam/blob/7f94d397f7777c2f3fdeb557366642f07d7947eb/DONTREADME.md"&gt;here&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;They don't search the web.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;They don't call functions to get more data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Each agent has just one job, so a &lt;code&gt;judge&lt;/code&gt; agent only judges and doesn't edit.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I feel like this will make the whole system more predictable.&lt;/p&gt; &lt;p&gt;But then I can't shake the feeling that this is a shortsighted move. I worry I'm building something that's going to be obsolete the moment a smarter model drops.&lt;/p&gt; &lt;p&gt;With how fast everything is moving, is this constrained approach a form of premature optimization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/8ta4"&gt; /u/8ta4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgxftt/does_the_less_is_more_principle_apply_to_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgxftt/does_the_less_is_more_principle_apply_to_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgxftt/does_the_less_is_more_principle_apply_to_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T23:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdh8q</id>
    <title>[ Removed by Reddit ]</title>
    <updated>2025-12-07T08:35:02+00:00</updated>
    <author>
      <name>/u/NandaVegg</name>
      <uri>https://old.reddit.com/user/NandaVegg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[ Removed by Reddit on account of violating the &lt;a href="/help/contentpolicy"&gt;content policy&lt;/a&gt;. ]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NandaVegg"&gt; /u/NandaVegg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T08:35:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pguel4</id>
    <title>Deepseek R1 671b Q4_K_M</title>
    <updated>2025-12-07T21:39:03+00:00</updated>
    <author>
      <name>/u/I_like_fragrances</name>
      <uri>https://old.reddit.com/user/I_like_fragrances</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"&gt; &lt;img alt="Deepseek R1 671b Q4_K_M" src="https://b.thumbs.redditmedia.com/olNH80zXa9V59_KTFmt1b8Awrp30ktlgeOVQAJqqN8w.jpg" title="Deepseek R1 671b Q4_K_M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was able to run Deepseek R1 671b locally with 384gb of VRAM. Get between 10-15 tok/s. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i1pbettypu5g1.png?width=880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a21fb31c437ea1368541dae4cbb18becb314dc62"&gt;https://preview.redd.it/i1pbettypu5g1.png?width=880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a21fb31c437ea1368541dae4cbb18becb314dc62&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_like_fragrances"&gt; /u/I_like_fragrances &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T21:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph46jn</id>
    <title>[Update] local_faiss_mcp v0.2.0 – I listened to you, r/LocalLLaMA: Added Reranking, CLI, and native PDF support</title>
    <updated>2025-12-08T05:19:15+00:00</updated>
    <author>
      <name>/u/fabiononato</name>
      <uri>https://old.reddit.com/user/fabiononato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I posted my &amp;quot;lazy&amp;quot; local RAG tool here. The consensus was: &lt;em&gt;&amp;quot;Cool start, but for serious use, we need reranking and better ingestion.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I spent the last few days building exactly what you asked for. &lt;strong&gt;v0.2.0 is out now.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s new (based on your feedback):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Re-ranking Support:&lt;/strong&gt; Added a &lt;code&gt;--rerank&lt;/code&gt; flag that uses CrossEncoders (MS MARCO / BGE) to refine results. Precision is significantly higher now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Standalone CLI:&lt;/strong&gt; You no longer need to trigger ingestion via Claude. Just run: &lt;code&gt;local-faiss index &amp;quot;docs/**/*.pdf&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native File Support:&lt;/strong&gt; Now parses PDFs, TXT, and MD natively (plus DOCX/HTML if you have pandoc).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Models:&lt;/strong&gt; You can now bring your own embedding models with &lt;code&gt;--embed [model_name]&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Still the same philosophy:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100% Local (No external APIs)&lt;/li&gt; &lt;li&gt;No Vector DB (Just FAISS + files on disk)&lt;/li&gt; &lt;li&gt;One-line install&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; &lt;code&gt;pip install -U local-faiss-mcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo / Full Release Notes:&lt;/strong&gt; &lt;a href="https://github.com/nonatofabio/local_faiss_mcp"&gt;https://github.com/nonatofabio/local_faiss_mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone who commented on the first thread—keep the requests coming. Next up: Hybrid search?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabiononato"&gt; /u/fabiononato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph46jn/update_local_faiss_mcp_v020_i_listened_to_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph46jn/update_local_faiss_mcp_v020_i_listened_to_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph46jn/update_local_faiss_mcp_v020_i_listened_to_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T05:19:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgyder</id>
    <title>I built a local Privacy Firewall that sanitizes prompts before they hit Claude/ChatGPT</title>
    <updated>2025-12-08T00:32:34+00:00</updated>
    <author>
      <name>/u/Emergency-Glass-6694</name>
      <uri>https://old.reddit.com/user/Emergency-Glass-6694</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a browser extension that intercepts the prompt &lt;em&gt;before&lt;/em&gt; it leaves the browser, sanitizes PII (Names, Emails, IPs, Keys) via a local server, and only then allows the submission.&lt;/p&gt; &lt;p&gt;Uses dslim/bert-base-NER running entirely on localhost - no cloud inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; Chrome Extension (intercepts DOM events on paste/enter).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Python FastAPI running locally (defaulting to &lt;code&gt;dslim/bert-base-NER&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Privacy:&lt;/strong&gt; Inference is 100% localhost. No data leaves your machine until &lt;em&gt;you&lt;/em&gt; confirm the redacted version.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fallback:&lt;/strong&gt; Uses Regex for strict patterns (SSN, API Keys) that models sometimes miss.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I need advice (GLiNER vs BERT):&lt;/strong&gt; Currently, I'm using BERT because it's reliable and I get sub-100ms latency on CPU. However, I keep hearing &lt;code&gt;GLiNER&lt;/code&gt; is the new king for zero-shot performance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has anyone here deployed &lt;strong&gt;GLiNER-small&lt;/strong&gt; or &lt;strong&gt;GLiNER-medium&lt;/strong&gt; in a low-latency production flow?&lt;/li&gt; &lt;li&gt;Is the inference speed hit on CPU worth the accuracy gain over BERT?&lt;/li&gt; &lt;li&gt;My next step is trying to compile GLiNER to ONNX to run purely in-browser (removing the Python backend requirement entirely).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo (MIT Licensed):&lt;/strong&gt; &lt;a href="https://github.com/privacyshield-ai/privacy-firewall"&gt;https://github.com/privacyshield-ai/privacy-firewall&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Constructive roasting of my code or suggestions on the model stack are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Glass-6694"&gt; /u/Emergency-Glass-6694 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgyder/i_built_a_local_privacy_firewall_that_sanitizes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgyder/i_built_a_local_privacy_firewall_that_sanitizes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgyder/i_built_a_local_privacy_firewall_that_sanitizes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T00:32:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8ix9</id>
    <title>My little decentralized Locallama setup, 216gb VRAM</title>
    <updated>2025-12-07T03:47:31+00:00</updated>
    <author>
      <name>/u/Goldkoron</name>
      <uri>https://old.reddit.com/user/Goldkoron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt; &lt;img alt="My little decentralized Locallama setup, 216gb VRAM" src="https://preview.redd.it/o1o7ekxycp5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66f62e54e1c923ea71f1a1d46415562ffdcbc1ba" title="My little decentralized Locallama setup, 216gb VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goldkoron"&gt; /u/Goldkoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o1o7ekxycp5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgnj1q</id>
    <title>Aquif 3.5 Max 1205 (42B-A3B)</title>
    <updated>2025-12-07T17:08:15+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt; &lt;img alt="Aquif 3.5 Max 1205 (42B-A3B)" src="https://b.thumbs.redditmedia.com/NEG8P3hqBPVNg2ugj-hAZZUETF03fHVZR0b3FQKZD7g.jpg" title="Aquif 3.5 Max 1205 (42B-A3B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aquif 3.5 Max 1205 is out and seems much better than the previous one on some work.&lt;/p&gt; &lt;p&gt;No tool call problems so far (Aider or Kilocode) but as usual, early to tell. &lt;/p&gt; &lt;p&gt;It did fix some FE issues I had in a single-shot where Qwen3-Coder-30B or Aquif 3.5 Plus needed a couple turns - Devstral 2507 still managed but slower.&lt;/p&gt; &lt;p&gt;Nice one to Aquif and thanks Noctrex for the GGUF.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/reqwqu4cdt5g1.png?width=1403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35eac71c387b9ebda5e9e2f99e4baa70ac874ab2"&gt;https://preview.redd.it/reqwqu4cdt5g1.png?width=1403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35eac71c387b9ebda5e9e2f99e4baa70ac874ab2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original: &lt;a href="https://huggingface.co/aquif-ai/aquif-3.5-Max-1205"&gt;https://huggingface.co/aquif-ai/aquif-3.5-Max-1205&lt;/a&gt;&lt;br /&gt; MXFP4: &lt;a href="https://huggingface.co/noctrex/aquif-3.5-Max-1205-MXFP4_MOE-GGUF"&gt;https://huggingface.co/noctrex/aquif-3.5-Max-1205-MXFP4_MOE-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;As much I appreciate OSS development, The model seems to be allegedly copied from DavidAU with no credits - as seen on the other Aquif Plus and Max. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The reasoning tokens don't seem to work, unless you use [ironically] DavidAU system prompt, or use the correct chat template. I've done slight adjustment to close the tags properly and ensure all other blocks are also closed. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; REASONING Enable deep thinking subroutine. You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside &amp;lt;thinking&amp;gt;thinking&amp;lt;/thinking&amp;gt; tags, and then provide your solution or response to the problem. Enclose code blocks with triple back ticks. &lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The model seems to be doing ok in small refactors in NextJS, where GPT-OSS-120B and Devstral Small 1.1 2507 succeeds in a single shot in one of my repos. However, it loops reading in larger jobs - will wait tomorrow to check if thinking tags help that or is flop. &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T17:08:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph14do</id>
    <title>dynamic allocation of less used experts to slower memory</title>
    <updated>2025-12-08T02:43:46+00:00</updated>
    <author>
      <name>/u/zqkb</name>
      <uri>https://old.reddit.com/user/zqkb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"&gt; &lt;img alt="dynamic allocation of less used experts to slower memory" src="https://b.thumbs.redditmedia.com/TSEG2N0W9eMlIStr9sCBAh2F6vtoFNU88rMcjRGXhhQ.jpg" title="dynamic allocation of less used experts to slower memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, when Cerebras shared their REAP approach, we had a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/comment/nkjqt91/"&gt;discussion&lt;/a&gt; about offloading less frequently used experts to slower memory. Here's a quick follow-up on testing that (more details + repro steps &lt;a href="https://github.com/okuvshynov/golem?tab=readme-ov-file"&gt;on github&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Coverage of expert activation per layer for two different prompts looks like this (short prompts, 512 tokens generated)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q69bfynq2w5g1.png?width=2100&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09a63ab5f52b1ce41acfb0f217afb1d27173fa84"&gt;Qwen3-235b (6bit, 128 experts total, 8/token)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5jso00333w5g1.png?width=2100&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9eb6cda756742e903169567a6734b29d45e066f9"&gt;GLM 4.6 (4 bit, 160 experts total, 8/token)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Storing a static set of experts/layer will be suboptimal, but we can get some initial seed + implement reasonable allocation/eviction policies and run models which would not fit into fast memory otherwise. Looking at these charts, we can see that first layers and few last layers are more diverse, while the middle part is more likely to benefit from partial allocation.&lt;/p&gt; &lt;p&gt;Here's practical result of running Qwen3-235B @Q6 on M2 Ultra (192GB).&lt;/p&gt; &lt;p&gt;With warm start on some aggregated frequently used expert set, for short prompt + 512 tokens generated, we get hit rate which looks like this, depending on cache size per layer:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/he329uhi4w5g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d18b4c049466618f4abf7079b25c61994934a894"&gt;https://preview.redd.it/he329uhi4w5g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d18b4c049466618f4abf7079b25c61994934a894&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A reasonable thing to do would be to just store less-cacheable layers fully, and be more aggressive in caching the middle layers. &lt;/p&gt; &lt;p&gt;We can make some comparison with t/s for 4bit version, which fits into unified memory:&lt;/p&gt; &lt;p&gt;4bit baseline, model in unified memory:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% mlx_lm.generate --model mlx-community/Qwen3-235B-A22B-4bit-DWQ -p &amp;quot;Write 5 poems about the ocean in different styles&amp;quot; -m 512 ... ========== Prompt: 18 tokens, 48.314 tokens-per-sec Generation: 512 tokens, 28.679 tokens-per-sec Peak memory: 132.397 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;6bit with 96 (out of 128) experts:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% python scripts/generate.py -m ~/projects/llms/Qwen3-235B-A22B-Instruct-2507-6bit -c 96 -p &amp;quot;Write 5 poems about the ocean in different styles&amp;quot; -n 512 -W /tmp/qwen235-6b ... Generation: 512 tokens, 10.4 t/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;6bit with 96 (out of 128) experts + some layers loaded fully:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/generate.py -m ~/projects/llms/Qwen3-235B-A22B-Instruct-2507-6bit -c 96 -p &amp;quot;Write 5 poems about the ocean in different styles&amp;quot; -n 512 -W /tmp/qwen235-6b -f 0-40,90-93 ... Generation: 512 tokens, 14.6 t/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is more information in the repo (including longer prompts, known inefficiencies, etc), but some conclusions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;it's definitely feasible for models which are 'slightly not fitting' for personal usage, where we don't care much about multi-query throughput; &lt;/li&gt; &lt;li&gt;it should work better when secondary memory is faster (say, RAM -&amp;gt; PCIe -&amp;gt; VRAM)&lt;/li&gt; &lt;li&gt;in this experiment, we were bringing experts to fast memory/compute. On different hardware the alternative could be to just decide to keep less frequently experts on slower memory/compute, with periodic prompt-specific reallocation not on critical path. &lt;/li&gt; &lt;li&gt;we can speculatively prefetch experts a few layers in advance and amortize the cost. Current experimental implementation is suboptimal and fetching experts right when we need them, blocking the compute.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zqkb"&gt; /u/zqkb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph14do/dynamic_allocation_of_less_used_experts_to_slower/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T02:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph2aad</id>
    <title>Miles + FSDP2 = Megatron-Level Performance with More Flexibility</title>
    <updated>2025-12-08T03:42:18+00:00</updated>
    <author>
      <name>/u/Expert-Pineapple-740</name>
      <uri>https://old.reddit.com/user/Expert-Pineapple-740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Miles training framework now supports FSDP2 integration, delivering Megatron-level performance with basically zero vendor lock-in.&lt;/p&gt; &lt;p&gt;SGLang team just shipped this and experiments show numerical alignment with Megatron while supporting advanced features like Context Parallelism out of the box.&lt;/p&gt; &lt;p&gt;FSDP2 gives you a flexible, high-performance distributed training backend. Works alongside existing Miles features and scales efficiently for next-gen model training.&lt;/p&gt; &lt;p&gt;Perfect if you're:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training custom models at scale&lt;/li&gt; &lt;li&gt;Looking for Megatron performance without the complexity&lt;/li&gt; &lt;li&gt;Building on SGLang's serving stack and want end-to-end integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Docs: &lt;a href="https://lmsys.org/blog/2025-12-03-miles-fsdp/"&gt;https://lmsys.org/blog/2025-12-03-miles-fsdp/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;X: &lt;a href="https://x.com/lmsysorg/status/1997768901648871925"&gt;https://x.com/lmsysorg/status/1997768901648871925&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Pineapple-740"&gt; /u/Expert-Pineapple-740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph2aad/miles_fsdp2_megatronlevel_performance_with_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph2aad/miles_fsdp2_megatronlevel_performance_with_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph2aad/miles_fsdp2_megatronlevel_performance_with_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T03:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph3js7</id>
    <title>21 Days of Building a Small Language Model.</title>
    <updated>2025-12-08T04:46:45+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"&gt; &lt;img alt="21 Days of Building a Small Language Model." src="https://b.thumbs.redditmedia.com/cWFTudRm4yxp6DIriTfuZ9INDROjEcGR2pdy4_WqgRw.jpg" title="21 Days of Building a Small Language Model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Starting tomorrow, I’m beginning a new series: “21 Days of Building a Small Language Model.”&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bw2jtqnztw5g1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=264ee6545e42bbb39fb7fb9043ad66e8fd6b3c91"&gt;https://preview.redd.it/bw2jtqnztw5g1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=264ee6545e42bbb39fb7fb9043ad66e8fd6b3c91&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As we get close to the end of the year, I want to try something meaningful: help anyone who’s interested build their own small language model by the end of the year.&lt;/p&gt; &lt;p&gt;I’ll be following the structure of my book while keeping everything beginner-friendly and hands-on.&lt;/p&gt; &lt;p&gt;Just to set real expectations: Building AND understanding a small language model in 21 days is definitely challenging.&lt;br /&gt; It won’t be easy. There will be concepts that take time to sink in.&lt;br /&gt; But I’m going to do everything I can to break things down in simple language and make the journey as accessible as possible.&lt;/p&gt; &lt;p&gt;If you want to follow along, I’ll be posting updates every day at 9am PST on LinkedIn &lt;/p&gt; &lt;p&gt;Happy learning, and see you tomorrow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph3js7/21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T04:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgvhal</id>
    <title>mbzuai ifm releases Open 70b model - beats qwen-2.5</title>
    <updated>2025-12-07T22:23:17+00:00</updated>
    <author>
      <name>/u/Powerful-Sail-8826</name>
      <uri>https://old.reddit.com/user/Powerful-Sail-8826</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/LLM360/K2-V2-Instruct"&gt;https://huggingface.co/LLM360/K2-V2-Instruct&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Sail-8826"&gt; /u/Powerful-Sail-8826 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T22:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph5h2q</id>
    <title>RTX 5090 96 GB just popped up on Alibababa</title>
    <updated>2025-12-08T06:33:36+00:00</updated>
    <author>
      <name>/u/RateRoutine2268</name>
      <uri>https://old.reddit.com/user/RateRoutine2268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI Guys,&lt;br /&gt; Just found RTX 5090 96 GB on Alibaba from a verified vendor&lt;br /&gt; :&lt;a href="https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card%5C_1601577163842.html"&gt;https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card\_1601577163842.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I contacted vendor and waiting for reply , anyone tried it yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RateRoutine2268"&gt; /u/RateRoutine2268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T06:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgv2fi</id>
    <title>Unimpressed with Mistral Large 3 675B</title>
    <updated>2025-12-07T22:06:04+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From initial testing (coding related), this seems to be the new llama4.&lt;/p&gt; &lt;p&gt;The accusation from an ex-employee few months ago looks legit now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/suchenzang/status/1954973424486608928"&gt;https://x.com/suchenzang/status/1954973424486608928&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://36kr.com/p/3428277839465857"&gt;https://36kr.com/p/3428277839465857&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No idea whether the new Mistral Large 3 675B was indeed trained from scratch, or &amp;quot;shell-wrapped&amp;quot; on top of DSV3 (i.e. like Pangu: &lt;a href="https://github.com/HW-whistleblower/True-Story-of-Pangu"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt; ). Probably from scratch as it is much worse than DSV3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T22:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgsodd</id>
    <title>ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face</title>
    <updated>2025-12-07T20:28:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"&gt; &lt;img alt="ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face" src="https://external-preview.redd.it/KDS1GGF2jYTqD2RRTZIBI42Bz7Kwl8ZrRXizgMq0fZU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d2bc4dce3f56a2b8d2f8f355fdfb5b4072551a" title="ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Apriel-1.6-15B-Thinker&lt;/strong&gt; is an updated multimodal reasoning model in ServiceNow’s Apriel SLM series, building on &lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;&lt;strong&gt;Apriel-1.5-15B-Thinker&lt;/strong&gt;&lt;/a&gt;. With significantly improved text and image reasoning capabilities, Apriel-1.6 achieves competitive performance against models up to 10x its size. Like its predecessor, it benefits from extensive continual pretraining across both text and image domains. We further perform post-training, focusing on Supervised Finetuning (SFT) and Reinforcement Learning (RL). Apriel-1.6 obtains frontier performance without sacrificing reasoning token efficiency. The model improves or maintains task performance in comparison with Apriel-1.5-15B-Thinker, while &lt;em&gt;reducing reasoning token usage by more than 30%&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves a score of &lt;strong&gt;57&lt;/strong&gt; on the Artificial Analysis index outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient.&lt;/li&gt; &lt;li&gt;Scores &lt;strong&gt;69&lt;/strong&gt; on Tau2 Bench Telecom and &lt;strong&gt;69&lt;/strong&gt; on IFBench, which are key benchmarks for the enterprise domain.&lt;/li&gt; &lt;li&gt;At 15B parameters, the model fits on a single GPU, making it highly memory-efficient.&lt;/li&gt; &lt;li&gt;Based on community feedback on Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (&lt;code&gt;&amp;lt;tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;[BEGIN FINAL RESPONSE]&lt;/code&gt;, &lt;code&gt;&amp;lt;|end|&amp;gt;&lt;/code&gt;) for easier output parsing.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T20:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgza25</id>
    <title>Is this THAT bad today?</title>
    <updated>2025-12-08T01:15:06+00:00</updated>
    <author>
      <name>/u/Normal-Industry-8055</name>
      <uri>https://old.reddit.com/user/Normal-Industry-8055</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgza25/is_this_that_bad_today/"&gt; &lt;img alt="Is this THAT bad today?" src="https://preview.redd.it/3vg1imwjsv5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7a9691b1258cf67a1b8b9b9c4b102035b613ca0" title="Is this THAT bad today?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already bought it. We all know the market... This is special order so not in stock on Provantage but they estimate it should be in stock soon . With Micron leaving us, I don't see prices getting any lower for the next 6-12 mo minimum. What do you all think? For today’s market I don’t think I’m gonna see anything better. Only thing to worry about is if these sticks never get restocked ever.. which I know will happen soon. But I doubt they’re already all completely gone.&lt;/p&gt; &lt;p&gt;link for anyone interested: &lt;a href="https://www.provantage.com/crucial-technology-ct2k64g64c52cu5%7E7CIAL836.htm"&gt;https://www.provantage.com/crucial-technology-ct2k64g64c52cu5~7CIAL836.htm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Industry-8055"&gt; /u/Normal-Industry-8055 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3vg1imwjsv5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgza25/is_this_that_bad_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgza25/is_this_that_bad_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T01:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
