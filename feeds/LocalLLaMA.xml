<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-26T21:05:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nrb73j</id>
    <title>Frontend explicitly designed for stateless "chats"?</title>
    <updated>2025-09-26T19:54:01+00:00</updated>
    <author>
      <name>/u/danielrosehill</name>
      <uri>https://old.reddit.com/user/danielrosehill</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I know that this is a pretty niche use case and it may not seem that useful but I thought I'd ask if anyone's aware of any projects. &lt;/p&gt; &lt;p&gt;I commonly use AI assistants with simple system prompt configurations for doing various text transformation jobs (e.g: convert this text into a well structured email with these guidelines).&lt;/p&gt; &lt;p&gt;Statelessness is desirable for me because I find that local AI performs great on my hardware so long as the trailing context is kept to a minimum. &lt;/p&gt; &lt;p&gt;What I would prefer however is to use a frontend or interface explicitly designed to support this workload: i.e. regardless of whether it looks like there is a conventional chat history being developed, each user turn is treated as a new request and the user and system prompts get sent together for inference.&lt;/p&gt; &lt;p&gt;Anything that does this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielrosehill"&gt; /u/danielrosehill &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrb73j/frontend_explicitly_designed_for_stateless_chats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrb73j/frontend_explicitly_designed_for_stateless_chats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrb73j/frontend_explicitly_designed_for_stateless_chats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T19:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqb3p3</id>
    <title>What? Running Qwen-32B on a 32GB GPU (5090).</title>
    <updated>2025-09-25T16:16:51+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt; &lt;img alt="What? Running Qwen-32B on a 32GB GPU (5090)." src="https://external-preview.redd.it/eGQxejJvNXo1Y3JmMc7C-li4AFXa_Q-5qATmlwGRne0zNSJFPFjYVcktZ0y0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b1e06d42c9067d85e9ec551328755bfdad37895" title="What? Running Qwen-32B on a 32GB GPU (5090)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/01adz6it5crf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T16:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr0lxs</id>
    <title>Anyone else run into LiteLLM breaking down under load?</title>
    <updated>2025-09-26T12:55:30+00:00</updated>
    <author>
      <name>/u/Fabulous_Ad993</name>
      <uri>https://old.reddit.com/user/Fabulous_Ad993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been load testing different LLM gateways for a project where throughput matters. Setup was 1K → 5K RPS with mixed request sizes, tracked using Prometheus/Grafana.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;LiteLLM&lt;/a&gt;: stable up to ~300K RPS, but after that I started seeing latency spikes, retries piling up, and 5xx errors.&lt;/li&gt; &lt;li&gt;&lt;a href="https://portkey.ai/"&gt;Portkey&lt;/a&gt;: handled concurrency a bit better, though I noticed overhead rising at higher loads.&lt;/li&gt; &lt;li&gt;&lt;a href="https://getmax.im/bifr0st"&gt;Bifrost&lt;/a&gt;: didn’t break in the same way under the same tests. Overhead stayed low in my runs, and it comes with decent metrics/monitoring.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone here benchmarked these (TGI, vLLM gateways, custom reverse proxies, etc.) at higher RPS? Also would love to know if anyone has tried &lt;strong&gt;Bifrost&lt;/strong&gt; (found it mentioned on some threads) since it’s relatively new compared to the others; would love to hear your insights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Ad993"&gt; /u/Fabulous_Ad993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0lxs/anyone_else_run_into_litellm_breaking_down_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0lxs/anyone_else_run_into_litellm_breaking_down_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0lxs/anyone_else_run_into_litellm_breaking_down_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr5q4y</id>
    <title>Given the model, context size and number of GPU can you calculate VRAM needed for each GPU?</title>
    <updated>2025-09-26T16:20:16+00:00</updated>
    <author>
      <name>/u/arstarsta</name>
      <uri>https://old.reddit.com/user/arstarsta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is 4x16GB GPU equivalent to a 64GB gpu or is there overhead in memory requirements? Are there some variables that must build duplicated on all GPU? &lt;/p&gt; &lt;p&gt;I was trying to run Qwen next 80B 4bit but it ran out of VRAM on my 2x5090 with tensor parallel = 2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arstarsta"&gt; /u/arstarsta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5q4y/given_the_model_context_size_and_number_of_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5q4y/given_the_model_context_size_and_number_of_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5q4y/given_the_model_context_size_and_number_of_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T16:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nra6d0</id>
    <title>PAR LLAMA v0.7.0 Released - Enhanced Security &amp; Execution Experience</title>
    <updated>2025-09-26T19:13:41+00:00</updated>
    <author>
      <name>/u/probello</name>
      <uri>https://old.reddit.com/user/probello</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nra6d0/par_llama_v070_released_enhanced_security/"&gt; &lt;img alt="PAR LLAMA v0.7.0 Released - Enhanced Security &amp;amp; Execution Experience" src="https://external-preview.redd.it/sAb25lapmd1MlqL5HVNLK1DOrtd-DMHXlE4WZhwm4eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0185f73b4b8ad370e8323f9eedb94f718686a844" title="PAR LLAMA v0.7.0 Released - Enhanced Security &amp;amp; Execution Experience" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8x7edk8d6krf1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b5e5e9634a1fd0af38af7aa6558302de0bfc214"&gt;https://preview.redd.it/8x7edk8d6krf1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b5e5e9634a1fd0af38af7aa6558302de0bfc214&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What It Does&lt;/h1&gt; &lt;p&gt;A powerful Terminal User Interface (TUI) for managing and interacting with Ollama and other major LLM providers — featuring &lt;strong&gt;persistent AI memory&lt;/strong&gt;, &lt;strong&gt;secure code execution&lt;/strong&gt;, &lt;strong&gt;interactive development workflows&lt;/strong&gt;, and &lt;strong&gt;truly personalized conversations&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;PAR LLAMA Chat Interface&lt;/p&gt; &lt;h1&gt;What's New in v0.7.0&lt;/h1&gt; &lt;h1&gt;Improved Execution Experience&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Better Result Formatting&lt;/strong&gt;: Clean, professional display of execution results&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Command Display&lt;/strong&gt;: Shows 'python -c &amp;lt;script&amp;gt;' instead of escaped code for CLI parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Syntax-Highlighted Code Blocks&lt;/strong&gt;: Short scripts (≤10 lines) display with proper syntax highlighting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Language Detection&lt;/strong&gt;: Automatic highlighting for Python, JavaScript, and Bash&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clean Command Truncation&lt;/strong&gt;: Long commands truncated intelligently for better readability&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Previous Major Features (v0.6.0)&lt;/h1&gt; &lt;h1&gt;Memory System&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Persistent User Context&lt;/strong&gt;: AI remembers who you are and your preferences across ALL conversations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Tab Interface&lt;/strong&gt;: Dedicated UI for managing your personal information and context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Memory Updates&lt;/strong&gt;: Use &lt;code&gt;/remember&lt;/code&gt; and &lt;code&gt;/forget&lt;/code&gt; slash commands for intelligent memory management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic Injection&lt;/strong&gt;: Your memory context appears in every new conversation automatically&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Synchronization&lt;/strong&gt;: Memory updates via commands instantly reflect in the Memory tab&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Context Management&lt;/strong&gt;: Never repeat your preferences or background information again&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Template Execution System&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Secure Code Execution&lt;/strong&gt;: Execute code snippets and commands directly from chat messages using &lt;strong&gt;Ctrl+R&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Language Support&lt;/strong&gt;: Python, JavaScript/Node.js, Bash, and shell scripts with automatic language detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Configurable Security&lt;/strong&gt;: Command allowlists, content validation, and comprehensive safety controls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive Development&lt;/strong&gt;: Transform PAR LLAMA into a powerful development companion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Results&lt;/strong&gt;: Execution results appear as chat responses with output, errors, and timing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Enhanced User Experience&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory Slash Commands&lt;/strong&gt;: &lt;code&gt;/remember [info]&lt;/code&gt;, &lt;code&gt;/forget [info]&lt;/code&gt;, &lt;code&gt;/memory.status&lt;/code&gt;, &lt;code&gt;/memory.clear&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Updates&lt;/strong&gt;: AI intelligently integrates new information into existing memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Storage&lt;/strong&gt;: All memory data stored locally with comprehensive file validation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Options Integration&lt;/strong&gt;: Both Memory and Template Execution controls in Options tab&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Settings Persistence&lt;/strong&gt;: All preferences persist between sessions&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory System&lt;/strong&gt;: Persistent user context across all conversations with AI-powered memory management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Template Execution&lt;/strong&gt;: Secure code execution system with configurable safety controls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Provider Support&lt;/strong&gt;: Ollama, OpenAI, Anthropic, Groq, XAI, OpenRouter, Deepseek, LiteLLM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Model Support&lt;/strong&gt;: Chat with images using vision-capable models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session Management&lt;/strong&gt;: Save, load, and organize chat sessions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Prompts&lt;/strong&gt;: Create and manage custom system prompts and Fabric patterns&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theme System&lt;/strong&gt;: Dark/light modes with custom theme support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Management&lt;/strong&gt;: Pull, delete, copy, and create models with native quantization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Caching&lt;/strong&gt;: Intelligent per-provider model caching with configurable durations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Comprehensive file validation and secure operations&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Python&lt;/strong&gt;: Built with Textual and Rich for a beautiful easy to use terminal experience. Dark and Light mode support, plus custom themes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Platform&lt;/strong&gt;: Runs on Windows, macOS, Linux, and WSL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Architecture&lt;/strong&gt;: Non-blocking operations for smooth performance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Type Safe&lt;/strong&gt;: Fully typed with comprehensive type checking&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GitHub &amp;amp; PyPI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/paulrobello/parllama"&gt;https://github.com/paulrobello/parllama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/parllama/"&gt;https://pypi.org/project/parllama/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Comparison:&lt;/h1&gt; &lt;p&gt;I have seen many command line and web applications for interacting with LLM's but have not found any TUI related applications as feature reach as PAR LLAMA&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;If you're working with LLMs and want a powerful terminal interface that &lt;strong&gt;remembers who you are&lt;/strong&gt; and &lt;strong&gt;bridges conversation and code execution&lt;/strong&gt; — PAR LLAMA v0.7.0 is a game-changer. Perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Developers&lt;/strong&gt;: Persistent context about your tech stack + execute code during AI conversations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Scientists&lt;/strong&gt;: AI remembers your analysis preferences + run scripts without leaving chat&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DevOps Engineers&lt;/strong&gt;: Maintains infrastructure context + execute commands interactively&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Researchers&lt;/strong&gt;: Remembers your research focus + test experiments in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consultants&lt;/strong&gt;: Different client contexts persist across sessions + rapid prototyping&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Anyone&lt;/strong&gt;: Who wants truly personalized AI conversations with seamless code execution&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/probello"&gt; /u/probello &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nra6d0/par_llama_v070_released_enhanced_security/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nra6d0/par_llama_v070_released_enhanced_security/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nra6d0/par_llama_v070_released_enhanced_security/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T19:13:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqr5lp</id>
    <title>Kwaipilot/KAT-Dev</title>
    <updated>2025-09-26T03:41:01+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"&gt; &lt;img alt="Kwaipilot/KAT-Dev" src="https://external-preview.redd.it/RFhQxPOdxc2Em-bjhotgIyTCAALVqXONnbv5f8ro8uY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24d680deeef40cd14e1c0a13e00f25c88680f997" title="Kwaipilot/KAT-Dev" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;KAT-Dev-32B&lt;/strong&gt; is an open-source 32B-parameter model for software engineering tasks.&lt;/p&gt; &lt;p&gt;On SWE-Bench Verified, &lt;strong&gt;KAT-Dev-32B&lt;/strong&gt; achieves comparable performance with &lt;strong&gt;62.4%&lt;/strong&gt; resolved and ranks &lt;strong&gt;5th&lt;/strong&gt; among all open-source models with different scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T03:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrblc7</id>
    <title>GPT-1 Revival - Training GPT-1 original architecture + modern features</title>
    <updated>2025-09-26T20:09:07+00:00</updated>
    <author>
      <name>/u/Creative-Ad-2112</name>
      <uri>https://old.reddit.com/user/Creative-Ad-2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I took GPT-1 architecture, firstly updated it to pytorch as is, nothing changed. Secondly, stripped it of its ROCStyle (finetuning?) code portion of it, looks like they finetuned it on a dataset called ROC? I know what you are thinking, if i just modernize GPT-1's architecture, i would just generic sota llm architecture; Qwen, GPTOSS, deepseek, etc. But i decided to try another path to it. I just added MOE to it, keep the conv1d and attention the same. &lt;/p&gt; &lt;h1&gt;training plan (2 stages)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;stage A: pretrain&lt;/strong&gt; on &lt;strong&gt;WikiText-103&lt;/strong&gt;, seq_len=512&lt;/li&gt; &lt;li&gt;&lt;strong&gt;stage B: finetune&lt;/strong&gt; on a small &lt;strong&gt;GPT-OSS distilled reasoning + chat mix&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Params: 166,327,461| vocab=8192&lt;br /&gt; plan on switching to a tiktoken tokenizer soon.&lt;br /&gt; [Stage A] &lt;/p&gt; &lt;p&gt;LM 10.3201 | PPL 29732.62 | LR 1.04e-04&lt;/p&gt; &lt;p&gt;LM 2.7804 | PPL 16.12 | LR 1.00e-04&lt;/p&gt; &lt;p&gt;[Stage B] Reasoning&lt;/p&gt; &lt;p&gt;LM 7.8303 | PPL 2514.93 | LR 1.25e-04&lt;/p&gt; &lt;p&gt;LM 2.0607 | PPL 7.86 | LR 8.00e-05&lt;/p&gt; &lt;h1&gt;quick inference&lt;/h1&gt; &lt;p&gt;&amp;lt;user&amp;gt; Given 2^x = 8, find x.&lt;br /&gt; asnwer x = 3 &lt;/p&gt; &lt;p&gt;&amp;lt;user&amp;gt; If 12x = 36, what is x?&lt;br /&gt; answer x = 3&lt;/p&gt; &lt;p&gt;model output - &lt;/p&gt; &lt;p&gt;&amp;lt;user&amp;gt; Given 2^x=8, find x. &lt;/p&gt; &lt;p&gt;&amp;lt;assistant&amp;gt; &amp;lt;think&amp;gt; We we solve test of region using for that. first of&amp;gt;0,,x is is at 3. We to solve equation the: number of circum h such $,2 and it in in), K:e y y2,. Sincee find all k fori symmetric: xp. Let's that. =d is. We have x= 1 and 1 + k maybe 3. to this formular: it b b = 3 W can also try 3. Thus: sumb = thek 5.. x = 3. each term a =n / +aa &lt;/p&gt; &lt;p&gt;&amp;lt;user&amp;gt; If 12x = 36, what is x? &lt;/p&gt; &lt;p&gt;&amp;lt;think&amp;gt; We be a-it = 12b x +3 -. Then a c from 2ab b + = a s s =a2^ b + 3a ha c = ab + (/ +a)(-)^ -c =1/ +y. So a =a b3aa bb = bbr. Thus sum the (12,12 12). Thus PR: + (y+) +c -) 4 t 2 = a^ + p2 d + 3m = -a - = t an! &amp;lt;think&amp;gt; need a a. Let's compute:d 12/a 3.a a1 3 =0 n -1 (/2)/(-2/) (k1)*(/) =x1 * (^/)(1-)*(+33)). For^4 (m+)/22(x-) =((2)) 3): sqrt(12()=,2]()=63 (5)3 C if sqrt18*(22)/ = 15(1^=. So = 2^2 x/5 = (^/4 =x=3 &amp;lt;think&amp;gt; x =3 x=3 x=3&lt;/p&gt; &lt;p&gt;What do you think? Continue this path?/&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Ad-2112"&gt; /u/Creative-Ad-2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrblc7/gpt1_revival_training_gpt1_original_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrblc7/gpt1_revival_training_gpt1_original_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrblc7/gpt1_revival_training_gpt1_original_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T20:09:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr020h</id>
    <title>InfiniteTalk — open-source sparse-frame video dubbing (lip + head/body sync)</title>
    <updated>2025-09-26T12:30:00+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found a fun open-source project: &lt;strong&gt;InfiniteTalk&lt;/strong&gt;. It does “sparse-frame” video dubbing—so the &lt;strong&gt;lips, head, posture, and expressions&lt;/strong&gt; all track the audio, not just the mouth. It’s built for &lt;strong&gt;infinite-length&lt;/strong&gt; runs and claims fewer hand/body glitches with tighter lip sync than MultiTalk. Also works as &lt;strong&gt;image + audio → talking video&lt;/strong&gt;.&lt;br /&gt; Repo: &lt;a href="https://github.com/MeiGen-AI/InfiniteTalk"&gt;https://github.com/MeiGen-AI/InfiniteTalk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr020h/infinitetalk_opensource_sparseframe_video_dubbing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr020h/infinitetalk_opensource_sparseframe_video_dubbing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr020h/infinitetalk_opensource_sparseframe_video_dubbing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr3kl3</id>
    <title>Isn't there a TTS model just slightly better than Kokoro?</title>
    <updated>2025-09-26T14:58:01+00:00</updated>
    <author>
      <name>/u/TarkanV</name>
      <uri>https://old.reddit.com/user/TarkanV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like its consistency and speed, but I mean, I might sound nitpicky but, it seems like it can fail easily on some relatively common words or names of non-English origin like &amp;quot;Los Angeles&amp;quot;, &amp;quot;Huawei&amp;quot;.&lt;br /&gt; I really wish there was an in-between model or even something that had just a little bit more more parameters than Kokoro.&lt;br /&gt; But to be fair, even ChatGPT Voice Mode seems to fail with names like Siobhan even though Kokoro gets it right...&lt;br /&gt; Otherwise, I'm fine if it's English only and preferably something smaller and faster than Zonos. My main use would be making audiobooks. My build is basically a laptop with a 3060 6GB and and 16gb of ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarkanV"&gt; /u/TarkanV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3kl3/isnt_there_a_tts_model_just_slightly_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3kl3/isnt_there_a_tts_model_just_slightly_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3kl3/isnt_there_a_tts_model_just_slightly_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T14:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr06en</id>
    <title>I built llamactl - Unified management and routing for llama.cpp, MLX and vLLM models with web dashboard.</title>
    <updated>2025-09-26T12:35:37+00:00</updated>
    <author>
      <name>/u/RealLordMathis</name>
      <uri>https://old.reddit.com/user/RealLordMathis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of SSH-ing into servers to manually start/stop different model instances, so I built a control layer that sits on top of llama.cpp, MLX, and vLLM. Great for running multiple models at once or switching models on demand. &lt;/p&gt; &lt;p&gt;I first posted about this almost two months ago and have added a bunch of useful features since. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main features:&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Multiple backend support&lt;/strong&gt;: Native integration with llama.cpp, MLX, and vLLM&lt;br /&gt; - &lt;strong&gt;On-demand instances&lt;/strong&gt;: Automatically start model instances when API requests come in&lt;br /&gt; - &lt;strong&gt;OpenAI-compatible API&lt;/strong&gt;: Drop-in replacement - route by using instance name as model name&lt;br /&gt; - &lt;strong&gt;API key authentication&lt;/strong&gt;: Separate keys for management operations vs inference API access&lt;br /&gt; - &lt;strong&gt;Web dashboard&lt;/strong&gt;: Modern UI for managing instances without CLI&lt;br /&gt; - &lt;strong&gt;Docker support&lt;/strong&gt;: Run backends in isolated containers&lt;br /&gt; - &lt;strong&gt;Smart resource management&lt;/strong&gt;: Configurable instance limits, idle timeout, and LRU eviction &lt;/p&gt; &lt;p&gt;The API lets you route requests to specific model instances by using the instance name as the model name in standard OpenAI requests, so existing tools work without modification. Instance state persists across server restarts, and failed instances get automatically restarted. &lt;/p&gt; &lt;p&gt;Documentation and installation guide: &lt;a href="https://llamactl.org/stable/"&gt;https://llamactl.org/stable/&lt;/a&gt; GitHub: &lt;a href="https://github.com/lordmathis/llamactl"&gt;https://github.com/lordmathis/llamactl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT licensed. Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealLordMathis"&gt; /u/RealLordMathis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr06en/i_built_llamactl_unified_management_and_routing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr06en/i_built_llamactl_unified_management_and_routing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr06en/i_built_llamactl_unified_management_and_routing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:35:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr99vl</id>
    <title>Orchestrate a team of small Local models to do complex stuff with Observer! (Free and Open Source)</title>
    <updated>2025-09-26T18:38:47+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr99vl/orchestrate_a_team_of_small_local_models_to_do/"&gt; &lt;img alt="Orchestrate a team of small Local models to do complex stuff with Observer! (Free and Open Source)" src="https://external-preview.redd.it/56SPQExPI827GpA98kpipwMJSEu04uBeqWtJrHoG4nc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5567fa8755a2e8bb9fdc634d1c9e2f0436b0e05a" title="Orchestrate a team of small Local models to do complex stuff with Observer! (Free and Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; This new &lt;strong&gt;Automatic&lt;/strong&gt; Multi-Agent &lt;strong&gt;Creator and Editor&lt;/strong&gt; makes Observer super super powerful. You can create multiple agents automatically and iterate System Prompts to get your &lt;strong&gt;local&lt;/strong&gt; agents working super fast!&lt;/p&gt; &lt;p&gt;Hey r/LocalLLaMA,&lt;/p&gt; &lt;p&gt;Ever since i started using Local LLMs i've thought about this exact use case. Using &lt;strong&gt;vision + reasoning&lt;/strong&gt; models to do more advanced things, like guiding you while creating a Google account (worked really well for my Mom!), or extracting a LeetCode problem with Gemma and solving it with deepseek automatically.&lt;/p&gt; &lt;p&gt;A while ago I showed you guys how to create them manually but now the Agent Builder can create them &lt;strong&gt;automatically!!&lt;/strong&gt; And better yet, if a model is hallucinating or not triggering your notifications/logging correctly, you just click one button and the &lt;strong&gt;Agent Builder can fix it for you.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This lets you easily have some agent pairs that do the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Monitor &amp;amp; Document&lt;/strong&gt; - One agent describes your screen, another keeps a document of the process.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extract &amp;amp; Solve&lt;/strong&gt; - One agent extracts problems from the screen, another solves them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watch &amp;amp; Guide&lt;/strong&gt; - One agent lists out possible buttons or actions, another provides step-by-step guidance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Of course you can still have simple one-agent configs to get notifications when downloads finish, renders complete, something happens on a video game etc. etc. Everything using your &lt;strong&gt;local models!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can download the app and look at the code right here: &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or try it out without any install (non-local but easy): &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you to everyone who has given it a shot! I hope this App makes more people interested in local models and their possible uses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=6zJh8NmCXYw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr99vl/orchestrate_a_team_of_small_local_models_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr99vl/orchestrate_a_team_of_small_local_models_to_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T18:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr8ohf</id>
    <title>What is the best options currently available for a local LLM using a 24GB GPU?</title>
    <updated>2025-09-26T18:15:04+00:00</updated>
    <author>
      <name>/u/marcoc2</name>
      <uri>https://old.reddit.com/user/marcoc2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My main goals are translation and coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marcoc2"&gt; /u/marcoc2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8ohf/what_is_the_best_options_currently_available_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8ohf/what_is_the_best_options_currently_available_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8ohf/what_is_the_best_options_currently_available_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T18:15:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr5j9j</id>
    <title>Today marks 10 days since IBM uploaded Granite 4 models to HF</title>
    <updated>2025-09-26T16:13:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone have an idea how long we might be waiting for IBM to make them public...? ;)&lt;/p&gt; &lt;p&gt;reference &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5j9j/today_marks_10_days_since_ibm_uploaded_granite_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5j9j/today_marks_10_days_since_ibm_uploaded_granite_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5j9j/today_marks_10_days_since_ibm_uploaded_granite_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T16:13:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqkx7o</id>
    <title>Apparently all third party providers downgrade, none of them provide a max quality model</title>
    <updated>2025-09-25T22:41:03+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"&gt; &lt;img alt="Apparently all third party providers downgrade, none of them provide a max quality model" src="https://preview.redd.it/k5on2q9i2erf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c1c2fb9cea2d9deb0e87434f884c9dc83258dd" title="Apparently all third party providers downgrade, none of them provide a max quality model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k5on2q9i2erf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T22:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqkayx</id>
    <title>I trained an LLM from scratch AMA!</title>
    <updated>2025-09-25T22:14:44+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a few months and I have posted a few times but I am finished!&lt;/p&gt; &lt;p&gt;I used Claude to write my training scripts, and I trained a 960M model on public domain data. It was not fast or easy, but it only cost $500 ( I received free credits from Amazon). It took 3 attempts to get it right. Happy to go into detail&lt;/p&gt; &lt;p&gt;It's a LLama 3 architecture with a 3:1 GQA, flash attention 2, and sink tokens. I have not began post-training yet, so it is NOT VERY USABLE!!!&lt;/p&gt; &lt;p&gt;I am hoping that post turns it into something useful, I have used 1B base models and they all kind of suck.&lt;/p&gt; &lt;p&gt;Post training will be TRL with DPO and the ultrafeedbck dataset. The mdoel is released under the CC0 license, do as you will with it.&lt;/p&gt; &lt;p&gt;Project website: &lt;a href="https://www.libremodel.xyz/"&gt;The LibreModel Project&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face : &lt;a href="https://huggingface.co/jerrimu/libremodel"&gt;jerrimu/libremodel · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github ( GGUF here): &lt;a href="https://github.com/openconstruct/libremodel/releases"&gt;Releases · openconstruct/libremodel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to train more open source models, and am seeking donations for hardware: If you would like to support this cause you may donate here : &lt;a href="https://github.com/sponsors/openconstruct"&gt;Sponsor @openconstruct on GitHub Sponsors&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T22:14:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr1zen</id>
    <title>€5,000 AI server for LLM</title>
    <updated>2025-09-26T13:54:31+00:00</updated>
    <author>
      <name>/u/Slakish</name>
      <uri>https://old.reddit.com/user/Slakish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;We are looking for a solution to run LLMs for our developers. The budget is currently €5000. The setup should be as fast as possible, but also be able to process parallel requests. I was thinking, for example, of a dual RTX 3090TI system with the option of expansion (AMD EPYC platform). I have done a lot of research, but it is difficult to find exact builds. What would be your idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slakish"&gt; /u/Slakish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr1zen/5000_ai_server_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr1zen/5000_ai_server_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr1zen/5000_ai_server_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T13:54:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr9ouo</id>
    <title>Inside GPT-OSS: OpenAI’s Latest LLM Architecture</title>
    <updated>2025-09-26T18:55:10+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr9ouo/inside_gptoss_openais_latest_llm_architecture/"&gt; &lt;img alt="Inside GPT-OSS: OpenAI’s Latest LLM Architecture" src="https://external-preview.redd.it/6U81r9cNlkBV6W3-ZZjd3A_2D4Yg1yntt4LrzKaV2aw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1723d72f6cc258494fbedce2e06f75786d8c3d65" title="Inside GPT-OSS: OpenAI’s Latest LLM Architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/data-science-collective/inside-gpt-oss-openais-latest-llm-architecture-c80e4e6976dc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr9ouo/inside_gptoss_openais_latest_llm_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr9ouo/inside_gptoss_openais_latest_llm_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T18:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr661w</id>
    <title>Tested Qwen 3-Omni as a code copilot with eyes (local H100 run)</title>
    <updated>2025-09-26T16:37:38+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr661w/tested_qwen_3omni_as_a_code_copilot_with_eyes/"&gt; &lt;img alt="Tested Qwen 3-Omni as a code copilot with eyes (local H100 run)" src="https://external-preview.redd.it/a29ycW55enpjanJmMXeJ6owb9CTTQw8BGFXpKLa7dkqrBHj4Ee6shEeE05ca.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=054823c47b8641d321eae9df4fa659211a1bad41" title="Tested Qwen 3-Omni as a code copilot with eyes (local H100 run)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pushing Qwen 3-Omni beyond chat and turned it into a screen-aware code copilot. Super promising.&lt;/p&gt; &lt;p&gt;Overview:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Shared my screen solving a LeetCode problem (it recognized the task + suggested improvements)&lt;/li&gt; &lt;li&gt;Ran on an H100 with FP8 Dynamic Quant&lt;/li&gt; &lt;li&gt;Wired up with &lt;a href="https://github.com/gabber-dev/gabber"&gt;https://github.com/gabber-dev/gabber&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Performance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Logs show throughput was solid. Bottleneck is reasoning depth, not the pipeline.&lt;/li&gt; &lt;li&gt;Latency is mostly from “thinking tokens.” I could disable those for lower latency, but wanted to test with them on to see if the extra reasoning was worth it.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TL;DR Qwen continues to crush it. The stuff you can do with the latest (3) model is impressive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oeuj9vzzcjrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr661w/tested_qwen_3omni_as_a_code_copilot_with_eyes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr661w/tested_qwen_3omni_as_a_code_copilot_with_eyes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T16:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr6f75</id>
    <title>VibeVoice-ComfyUI 1.5.0: Speed Control and LoRA Support</title>
    <updated>2025-09-26T16:47:36+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr6f75/vibevoicecomfyui_150_speed_control_and_lora/"&gt; &lt;img alt="VibeVoice-ComfyUI 1.5.0: Speed Control and LoRA Support" src="https://preview.redd.it/96ikl9gbgjrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b4415d469ed873f34d444c40714c1fa3bc24215" title="VibeVoice-ComfyUI 1.5.0: Speed Control and LoRA Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! 👋&lt;/p&gt; &lt;p&gt;First of all, thank you again for the amazing support, this project has now reached ⭐ &lt;strong&gt;880 stars on GitHub&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;Over the past weeks, VibeVoice-ComfyUI has become more stable, gained powerful new features, and grown thanks to your feedback and contributions.&lt;/p&gt; &lt;h1&gt;✨ Features&lt;/h1&gt; &lt;h1&gt;Core Functionality&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🎤 &lt;strong&gt;Single Speaker TTS&lt;/strong&gt;: Generate natural speech with optional voice cloning&lt;/li&gt; &lt;li&gt;👥 &lt;strong&gt;Multi-Speaker Conversations&lt;/strong&gt;: Support for up to 4 distinct speakers&lt;/li&gt; &lt;li&gt;🎯 &lt;strong&gt;Voice Cloning&lt;/strong&gt;: Clone voices from audio samples&lt;/li&gt; &lt;li&gt;🎨 &lt;strong&gt;LoRA Support&lt;/strong&gt;: Fine-tune voices with custom LoRA adapters (v1.4.0+)&lt;/li&gt; &lt;li&gt;🎚️ &lt;strong&gt;Voice Speed Control&lt;/strong&gt;: Adjust speech rate by modifying reference voice speed (v1.5.0+)&lt;/li&gt; &lt;li&gt;📝 &lt;strong&gt;Text File Loading&lt;/strong&gt;: Load scripts from text files&lt;/li&gt; &lt;li&gt;📚 &lt;strong&gt;Automatic Text Chunking&lt;/strong&gt;: Seamlessly handles long texts with configurable chunk size&lt;/li&gt; &lt;li&gt;⏸️ &lt;strong&gt;Custom Pause Tags&lt;/strong&gt;: Insert silences with &lt;code&gt;[pause]&lt;/code&gt; and &lt;code&gt;[pause:ms]&lt;/code&gt; tags (wrapper feature)&lt;/li&gt; &lt;li&gt;🔄 &lt;strong&gt;Node Chaining&lt;/strong&gt;: Connect multiple VibeVoice nodes for complex workflows&lt;/li&gt; &lt;li&gt;⏹️ &lt;strong&gt;Interruption Support&lt;/strong&gt;: Cancel operations before or between generations&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Model Options&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🚀 &lt;strong&gt;Three Model Variants&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;VibeVoice 1.5B (faster, lower memory)&lt;/li&gt; &lt;li&gt;VibeVoice-Large (best quality, ~17GB VRAM)&lt;/li&gt; &lt;li&gt;VibeVoice-Large-Quant-4Bit (balanced, ~7GB VRAM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance &amp;amp; Optimization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;⚡ &lt;strong&gt;Attention Mechanisms&lt;/strong&gt;: Choose between auto, eager, sdpa, flash_attention_2 or sage&lt;/li&gt; &lt;li&gt;🎛️ &lt;strong&gt;Diffusion Steps&lt;/strong&gt;: Adjustable quality vs speed trade-off (default: 20)&lt;/li&gt; &lt;li&gt;💾 &lt;strong&gt;Memory Management&lt;/strong&gt;: Toggle automatic VRAM cleanup after generation&lt;/li&gt; &lt;li&gt;🧹 &lt;strong&gt;Free Memory Node&lt;/strong&gt;: Manual memory control for complex workflows&lt;/li&gt; &lt;li&gt;🍎 &lt;strong&gt;Apple Silicon Support&lt;/strong&gt;: Native GPU acceleration on M1/M2/M3 Macs via MPS&lt;/li&gt; &lt;li&gt;🔢 &lt;strong&gt;4-Bit Quantization&lt;/strong&gt;: Reduced memory usage with minimal quality loss&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Compatibility &amp;amp; Installation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;📦 &lt;strong&gt;Self-Contained&lt;/strong&gt;: Embedded VibeVoice code, no external dependencies&lt;/li&gt; &lt;li&gt;🔄 &lt;strong&gt;Universal Compatibility&lt;/strong&gt;: Adaptive support for transformers v4.51.3+&lt;/li&gt; &lt;li&gt;🖥️ &lt;strong&gt;Cross-Platform&lt;/strong&gt;: Works on Windows, Linux, and macOS&lt;/li&gt; &lt;li&gt;🎮 &lt;strong&gt;Multi-Backend&lt;/strong&gt;: Supports CUDA, CPU, and MPS (Apple Silicon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;---------------------------------------------------------------------------------------------&lt;/p&gt; &lt;h1&gt;🔥 What’s New in v1.5.0&lt;/h1&gt; &lt;h1&gt;🎨 LoRA Support&lt;/h1&gt; &lt;p&gt;Thanks to the contribution of github user &lt;strong&gt;jpgallegoar&lt;/strong&gt;, I have made a new node to load LoRA adapters for voice customization. The node generates an output that can now be linked directly to both &lt;strong&gt;Single Speaker&lt;/strong&gt; and &lt;strong&gt;Multi Speaker&lt;/strong&gt; nodes, allowing even more flexibility when fine-tuning cloned voices.&lt;/p&gt; &lt;h1&gt;🎚️ Speed Control&lt;/h1&gt; &lt;p&gt;While it’s not possible to force a cloned voice to speak at an exact target speed, a new system has been implemented to slightly alter the input audio speed. This helps the cloning process produce speech closer to the desired pace.&lt;/p&gt; &lt;p&gt;👉 Best results come with &lt;strong&gt;reference samples longer than 20 seconds&lt;/strong&gt;.&lt;br /&gt; It’s not 100% reliable, but in many cases the results are surprisingly good!&lt;/p&gt; &lt;p&gt;🔗 GitHub Repo: &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💡 As always, feedback and contributions are welcome! They’re what keep this project evolving.&lt;br /&gt; Thanks for being part of the journey! 🙏&lt;/p&gt; &lt;p&gt;Fabio&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/96ikl9gbgjrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr6f75/vibevoicecomfyui_150_speed_control_and_lora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr6f75/vibevoicecomfyui_150_speed_control_and_lora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T16:47:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr5h1i</id>
    <title>60% t/s improvement for 30b a3b from upgrading ROCm 6.3 to 7.0 on 7900 XTX</title>
    <updated>2025-09-26T16:10:46+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got around to upgrading ROCm from my February 6.3.3 version to the latest 7.0.1 today. The performance improvements have been massive on my RX 7900 XTX.&lt;/p&gt; &lt;p&gt;This will be highly anecdotal, and I'm sorry about that, but I don't have time to do a better job. I can only give you a very rudimentary look based on top-level numbers. Hopefully someone will make a proper benchmark with more conclusive findings.&lt;/p&gt; &lt;p&gt;All numbers are for unsloth/qwen3-coder-30b-a3b-instruct-IQ4_XS in LMStudio 0.3.25 running on Ubuntu 24.04:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;-&lt;/th&gt; &lt;th align="left"&gt;llama.cpp ROCm&lt;/th&gt; &lt;th align="left"&gt;llama.cpp Vulkan&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;ROCm 6.3.3&lt;/td&gt; &lt;td align="left"&gt;78 t/s&lt;/td&gt; &lt;td align="left"&gt;75 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ROCm 7.0.1&lt;/td&gt; &lt;td align="left"&gt;115 t/s&lt;/td&gt; &lt;td align="left"&gt;125 t/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Of note, previously the ROCm runtime had a slight advantage, but now the Vulkan advantage is significant. Prompt processing is about 30% faster with Vulkan compared to ROCm (both rocm 7) now as well.&lt;/p&gt; &lt;p&gt;I was running on a week older llama.cpp runtime version with ROCm 6.3.3, so that also may be cause for some performance difference, but certainly it couldn't be enough to explain the bulk of the difference.&lt;/p&gt; &lt;p&gt;This was a huge upgrade! I think we need to redo the math on which used GPU is the best to recommend with this change if other people experience the same improvement. It might not be clear cut anymore. What are 3090 users getting on this model with current versions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5h1i/60_ts_improvement_for_30b_a3b_from_upgrading_rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5h1i/60_ts_improvement_for_30b_a3b_from_upgrading_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr5h1i/60_ts_improvement_for_30b_a3b_from_upgrading_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T16:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr0jnz</id>
    <title>ROCM vs Vulkan on IGPU</title>
    <updated>2025-09-26T12:52:35+00:00</updated>
    <author>
      <name>/u/Eden1506</name>
      <uri>https://old.reddit.com/user/Eden1506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0jnz/rocm_vs_vulkan_on_igpu/"&gt; &lt;img alt="ROCM vs Vulkan on IGPU" src="https://b.thumbs.redditmedia.com/oezoSTnfC24x09rIDD1CuCmBAKqbu-BhvixwCYutkyo.jpg" title="ROCM vs Vulkan on IGPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While around the same for text generation vulkan is ahead for prompt processing by a fair margin on the new igpus from AMD now.&lt;/p&gt; &lt;p&gt;Curious considering that it was the other way around before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eden1506"&gt; /u/Eden1506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nr0jnz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0jnz/rocm_vs_vulkan_on_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0jnz/rocm_vs_vulkan_on_igpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqwcsf</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any - (26th Sep)</title>
    <updated>2025-09-26T08:59:24+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks&lt;/p&gt; &lt;p&gt;So many models for this week specially from the &lt;em&gt;Qwen&lt;/em&gt; team who have been super active lately. Please double check my list and update in the comments in case I missed anything worth mentioned this week.&lt;/p&gt; &lt;p&gt;Enjoy :)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit Link&lt;/th&gt; &lt;th align="left"&gt;HF/GH Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Max&lt;/td&gt; &lt;td align="left"&gt;LLM (1TB)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list"&gt;Qwen blog&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code World Model (CWM) 32B&lt;/td&gt; &lt;td align="left"&gt;Code LLM 32B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/facebook/cwm"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen-Image-Edit-2509&lt;/td&gt; &lt;td align="left"&gt;Image edit&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni 30B (A3B variants)&lt;/td&gt; &lt;td align="left"&gt;Omni-modal 30B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner"&gt;Captioner&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking"&gt;Thinking&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3.1-Terminus&lt;/td&gt; &lt;td align="left"&gt;Update 685B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://i.redd.it/729mf2l1xpqf1.jpeg"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qianfan-VL (70B/8B/3B)&lt;/td&gt; &lt;td align="left"&gt;Vision LLMs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-70B"&gt;HF 70B&lt;/a&gt;, &lt;a href="https://huggingface.co/baidu/Qianfan-VL-8B"&gt;HF 8B&lt;/a&gt;, &lt;a href="https://huggingface.co/baidu/Qianfan-VL-3B"&gt;HF 3B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan Image 3.0&lt;/td&gt; &lt;td align="left"&gt;T2I model (TB released)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Stockmark-2-100B-Instruct&lt;/td&gt; &lt;td align="left"&gt;Japanese LLM 100B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-VL-235B A22B (Thinking/Instruct)&lt;/td&gt; &lt;td align="left"&gt;Vision LLM 235B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;Thinking&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;Instruct&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LongCat-Flash-Thinking&lt;/td&gt; &lt;td align="left"&gt;Reasoning MoE 18–31B active&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4B Function Calling&lt;/td&gt; &lt;td align="left"&gt;LLM 4B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Manojb/Qwen3-4B-FunctionCalling"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Isaac 0.1&lt;/td&gt; &lt;td align="left"&gt;Perception LLM 2B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nmiqjh"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PerceptronAI/Isaac-0.1"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral 1.2&lt;/td&gt; &lt;td align="left"&gt;Multi-Modal&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Magistral-Small-2509-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ring-flash-2.0&lt;/td&gt; &lt;td align="left"&gt;Thinking MoE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nl97i5/inclusionairingflash20/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-2.0"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kokoro-82M-FP16-OpenVINO&lt;/td&gt; &lt;td align="left"&gt;TTS 82M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nn45cx/kokoro82mfp16openvino/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Echo9Zulu/Kokoro-82M-FP16-OpenVINO"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Wan2.2-Animate-14B&lt;/td&gt; &lt;td align="left"&gt;Video animate 14B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.2-Animate-14B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniModel-200M-Base&lt;/td&gt; &lt;td align="left"&gt;Tiny LLM 200M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://i.redd.it/clbzeq0i82rf1.png"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/xTimeCrystal/MiniModel-200M-Base"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Other notable mentions&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;K2 Vendor Verifier&lt;/strong&gt; – Open-source tool-call validator for LLM providers (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt;Reddit&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;quelmap + Lightning-4b&lt;/strong&gt; – Local data analysis assistant + LLM (&lt;a href="https://quelmap.com"&gt;quelmap.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.ui&lt;/strong&gt; – Updated privacy-focused LLM web UI (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt;Reddit&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqwcsf/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqwcsf/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqwcsf/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T08:59:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr8acu</id>
    <title>The benchmarks are favouring Qwen3 max</title>
    <updated>2025-09-26T17:59:46+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8acu/the_benchmarks_are_favouring_qwen3_max/"&gt; &lt;img alt="The benchmarks are favouring Qwen3 max" src="https://preview.redd.it/5hyvzvs8tjrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52ee02c3689ab52b47faa094175ba42f13984273" title="The benchmarks are favouring Qwen3 max" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best non thinking model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5hyvzvs8tjrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8acu/the_benchmarks_are_favouring_qwen3_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8acu/the_benchmarks_are_favouring_qwen3_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T17:59:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr4v7e</id>
    <title>Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&lt;15GB VRAM)</title>
    <updated>2025-09-26T15:47:52+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"&gt; &lt;img alt="Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&amp;lt;15GB VRAM)" src="https://preview.redd.it/pq6ej7up5jrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=121b14e94d54780f5a2a7ae8625d9bd2f60d60f6" title="Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&amp;lt;15GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got lots of updates for Reinforcement Learning (RL)! We’re excited to introduce gpt-oss, Vision, and even better RL in Unsloth. Our new gpt-oss RL inference also achieves the fastest token/s vs. any other implementation. Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inference is crucial in RL training. Since gpt-oss RL isn’t vLLM compatible, we rewrote Transformers inference for 3× faster speeds (~21 tok/s). For BF16, Unsloth also delivers the fastest inference (~30 tok/s), especially relative to VRAM use vs. any other implementation.&lt;/li&gt; &lt;li&gt;We made a free &amp;amp; completely new custom notebook showing how RL can automatically create faster matrix multiplication kernels: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;gpt-oss-20b GSPO Colab&lt;/a&gt;-GRPO.ipynb). We also show you how to counteract reward-hacking which is one of RL's biggest challenges.&lt;/li&gt; &lt;li&gt;Unsloth also uses the least VRAM (50% less) and supports the most context length (8x more). gpt-oss-20b RL fits in 15GB VRAM.&lt;/li&gt; &lt;li&gt;As usual, there is no accuracy degradation.&lt;/li&gt; &lt;li&gt;We released &lt;a href="https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl"&gt;Vision RL&lt;/a&gt;, allowing you to train Gemma 3, Qwen2.5-VL with GRPO free in our Colab notebooks.&lt;/li&gt; &lt;li&gt;We also previously introduced more &lt;a href="https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;memory efficient RL&lt;/a&gt; with Standby and extra kernels and algorithms. Unsloth RL now uses 90% less VRAM, and enables 16× longer context lengths than any setup.&lt;/li&gt; &lt;li&gt; ⚠️ Reminder to NOT use Flash Attention 3 for gpt-oss as it'll make your training loss wrong.&lt;/li&gt; &lt;li&gt;We released &lt;a href="https://docs.unsloth.ai/models/deepseek-v3.1-how-to-run-locally"&gt;DeepSeek-V3.1-Terminus&lt;/a&gt; Dynamic GGUFs. We showcased how 3-bit V3.1 scores 75.6% on Aider Polyglot, beating Claude-4-Opus (thinking).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For our new gpt-oss RL release, would recommend you guys to read our blog/guide which details our entire findings and bugs etc.: &lt;a href="https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning"&gt;https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and weekend! 🦥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pq6ej7up5jrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T15:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr3n2r</id>
    <title>How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?</title>
    <updated>2025-09-26T15:00:41+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"&gt; &lt;img alt="How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?" src="https://preview.redd.it/kabtcb5twirf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cfbae1d53a3abc93a95be9789c678d6280c6d58" title="How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is mostly open-weights and open-source discussion and all that jazz but let's be real, unless your name is Achmed Al-Jibani from Qatar or you pi*ss gold you're not getting the SOTA performance with open-weight models like Kimi K2 or DeepSeek because you have to quantize it, your options as an average-wage pleb are either:&lt;/p&gt; &lt;p&gt;a) third party providers&lt;br /&gt; b) running it yourself but quantized to hell&lt;br /&gt; c) spinning up a pod and using a third party providers GPU (expensive) to run your model&lt;/p&gt; &lt;p&gt;I opted for a) most of the time and a recent evaluation done on the accuracy of the Kimi K2 0905 models provided by third party providers has me doubting this decision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kabtcb5twirf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T15:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
