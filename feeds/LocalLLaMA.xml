<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-04T14:50:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oo8vgs</id>
    <title>made a simple webui that supports Qwen3VL for fun</title>
    <updated>2025-11-04T14:39:54+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1oo8vgs/video/xyl1di3w49zf1/player"&gt;https://reddit.com/link/1oo8vgs/video/xyl1di3w49zf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Uses the llama-server endpoint and hopefully it inspires people to make their own webui.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo8vgs/made_a_simple_webui_that_supports_qwen3vl_for_fun/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo8vgs/made_a_simple_webui_that_supports_qwen3vl_for_fun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo8vgs/made_a_simple_webui_that_supports_qwen3vl_for_fun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T14:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo5862</id>
    <title>Dual 5090 work station for SDXL</title>
    <updated>2025-11-04T11:42:55+00:00</updated>
    <author>
      <name>/u/Background-Bank1798</name>
      <uri>https://old.reddit.com/user/Background-Bank1798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; Building a small AI workstation with 2Ã— RTX 5090 for SDXL, light video generation, and occasional LLM inference (7Bâ€“13B). Testing hot inference on-prem to reduce AWS costs. Open to GPU suggestions, including older bigâ€‘VRAM cards (AMD MI50 / MI100, older NVIDIA datacenter) for offline large batch work. Budget-conscious, want best value/performance mix.&lt;/p&gt; &lt;p&gt;Hey Guys,&lt;br /&gt; Iâ€™ve a startup and currently using L40â€™s in AWS but there are times when we have no traffic and the boot time is terrible. I decided to build a small AI workstation as a POC to handle the lower traffic and costs to keep the models hot â€” which later Iâ€™ll take the cards out and put into a server rack on site.&lt;/p&gt; &lt;p&gt;I bought 2 x 5090â€™s, 128 GB DDR5 6400 CL40 and running on a spare 13700K + Asus Prime Z790â€‘P I never used.&lt;br /&gt; I researched the numbers, render times, watts cost etc and besides having only 32 GB VRAM the cards seem they will run fast fine with CUDA parallelism and doing small batch processing. My models will fit. I spent about â‚¬2040 (ex VAT) per MSI Gaming Trio and just got them delivered. Just doubting if I made the best choice on cards, 4090s are near the same price in Europe, 3090s hard to get. I was planning to buy 8 5090s and put them together due to running smaller models and keep training in the cloud if this POC works out.&lt;/p&gt; &lt;p&gt;This is just a temporary test setup â€” it will all be put into a server eventually. I can add 2 more cards into the motherboard. Models mostly fit in memory, so PCIe bandwidth loss is not a big issue. Iâ€™m also looking to do &lt;strong&gt;offline large batch work&lt;/strong&gt;, so older cards could take longer to process but may still be costâ€‘effective.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Workloads &amp;amp; Useâ€‘cases:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SDXL (textâ€‘toâ€‘image)&lt;/li&gt; &lt;li&gt;Soon: video generation (likely small batches initially)&lt;/li&gt; &lt;li&gt;Occasional LLM inference (probably 7Bâ€“13B parameter models)&lt;/li&gt; &lt;li&gt;MCP server&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions Iâ€™m wrestling with:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better GPU choices?&lt;/li&gt; &lt;li&gt;For inferenceâ€‘heavy workloads (image + video + smaller LLMs), are there better value workstation or data center cards I should consider?&lt;/li&gt; &lt;li&gt;Would AMD MI50 / MI100, or older NVIDIA dataâ€‘center cards (A100, H100) be better for occasional LLM inference due to higher VRAM, even if slightly slower for image/video tasks?&lt;/li&gt; &lt;li&gt;Iâ€™m mostly looking for advice on value and performance for inference, especially for SDXL, video generation, and small LLM inference. Budget is limited, but I want to do as much as possible onâ€‘prem.&lt;/li&gt; &lt;li&gt;Iâ€™m &lt;strong&gt;open to any card suggestions or best-value hacks&lt;/strong&gt; :)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance for any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Background-Bank1798"&gt; /u/Background-Bank1798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5862/dual_5090_work_station_for_sdxl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5862/dual_5090_work_station_for_sdxl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5862/dual_5090_work_station_for_sdxl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1onhdob</id>
    <title>How does cerebras get 2000toks/s?</title>
    <updated>2025-11-03T17:05:10+00:00</updated>
    <author>
      <name>/u/npmbad</name>
      <uri>https://old.reddit.com/user/npmbad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering, what sort of GPU do I need to rent and under what settings to get that speed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/npmbad"&gt; /u/npmbad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T17:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo6dq6</id>
    <title>how to choose a model</title>
    <updated>2025-11-04T12:58:23+00:00</updated>
    <author>
      <name>/u/nobody-was-there</name>
      <uri>https://old.reddit.com/user/nobody-was-there</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey i m new to local LLM i m using n8n and i m trying to find the best model for me i have this : &lt;/p&gt; &lt;p&gt;OS: Ubuntu 24.04.3 LTS x86_64 &lt;/p&gt; &lt;p&gt;Kernel: 6.8.0-87-generic &lt;/p&gt; &lt;p&gt;CPU: AMD FX-8300 (8) @ 3.300GHz &lt;/p&gt; &lt;p&gt;GPU: NVIDIA GeForce GTX 1060 3GB &lt;/p&gt; &lt;p&gt;Memory: 4637MiB / 15975MiB&lt;br /&gt; which AI model is the best for me ? i tryed phi3 and gemma3 on ollama do you think i can run a larger model ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nobody-was-there"&gt; /u/nobody-was-there &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6dq6/how_to_choose_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6dq6/how_to_choose_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6dq6/how_to_choose_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T12:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1on628o</id>
    <title>Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation</title>
    <updated>2025-11-03T08:04:32+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt; &lt;img alt="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" src="https://b.thumbs.redditmedia.com/otiqqwWrYAPyBQSbvIHZ-yCKbfdiJZGic1vlE0jFFxk.jpg" title="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0hnvozwh10zf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab171458093a1ad5f07a0eaa42ac44e2c5ab5681"&gt;Google Official Statement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://techcrunch.com/2025/11/02/google-pulls-gemma-from-ai-studio-after-senator-blackburn-accuses-model-of-defamation/"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fortunately, we can still download the weights from HF and run them locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T08:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo6ud3</id>
    <title>How to speed up diarization speed for WhisperX?</title>
    <updated>2025-11-04T13:17:35+00:00</updated>
    <author>
      <name>/u/yccheok</name>
      <uri>https://old.reddit.com/user/yccheok</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently encountering diarization speed issue for WhisperX.&lt;/p&gt; &lt;p&gt;Based on &lt;a href="https://github.com/m-bain/whisperX/issues/499"&gt;https://github.com/m-bain/whisperX/issues/499&lt;/a&gt; , the possible reason is diarization is executing on CPU.&lt;/p&gt; &lt;p&gt;I have tried the mentioned workaround. This is my Dockerfile, running on runpod.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; FROM runpod/pytorch:cuda12 # Set the working directory in the container WORKDIR /app # Install ffmpeg, vim RUN apt-get update &amp;amp;&amp;amp; \ apt-get install -y ffmpeg vim # Install WhisperX via pip RUN pip install --upgrade pip &amp;amp;&amp;amp; \ pip install --no-cache-dir runpod==1.7.7 whisperx==3.3.1 pyannote.audio==3.3.2 torchaudio==2.8.0 matplotlib==3.10.7 # https://github.com/m-bain/whisperX/issues/499 RUN pip uninstall -y onnxruntime &amp;amp;&amp;amp; \ pip install --force-reinstall --no-cache-dir onnxruntime-gpu # Download large-v3 model RUN python -c &amp;quot;import whisperx; whisperx.load_model('large-v3', device='cpu', compute_type='int8')&amp;quot; # Initialize diarization pipeline RUN python -c &amp;quot;import whisperx; whisperx.DiarizationPipeline(use_auth_token='xxx', device='cpu')&amp;quot; # Copy source code into image COPY src src # -u disables output buffering so logs appear in real-time. CMD [ &amp;quot;python&amp;quot;, &amp;quot;-u&amp;quot;, &amp;quot;src/handler.py&amp;quot; ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is my Python code.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; import runpod import whisperx import time start_time = time.time() diarize_model = whisperx.DiarizationPipeline( use_auth_token='...', device='cuda' ) end_time = time.time() time_s = (end_time - start_time) print(f&amp;quot;ðŸ¤– whisperx.DiarizationPipeline done: {time_s:.2f} s&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For a one minute transcription, it will also took one minute to perform the diarization, which I feel is pretty slow.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; diarize_segments = diarize_model(audio) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I was wondering, what else I can try, to speed up the diarization process?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yccheok"&gt; /u/yccheok &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6ud3/how_to_speed_up_diarization_speed_for_whisperx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6ud3/how_to_speed_up_diarization_speed_for_whisperx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6ud3/how_to_speed_up_diarization_speed_for_whisperx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T13:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo5dbf</id>
    <title>Seeking advice for a small model ro run on my laptop</title>
    <updated>2025-11-04T11:50:57+00:00</updated>
    <author>
      <name>/u/Drakooon05</name>
      <uri>https://old.reddit.com/user/Drakooon05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I wanna prompt questions and get answers for video automation reasons&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;p&gt;16GB RAM&lt;/p&gt; &lt;p&gt;Intel Core i7-12650h (16CPUS) 2.3GhHz&lt;/p&gt; &lt;p&gt;Nvidia GeForce RTX 4060 Laptop GPU (8GBVRAM)&lt;/p&gt; &lt;p&gt;1TB SSD&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Drakooon05"&gt; /u/Drakooon05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5dbf/seeking_advice_for_a_small_model_ro_run_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5dbf/seeking_advice_for_a_small_model_ro_run_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5dbf/seeking_advice_for_a_small_model_ro_run_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo2olu</id>
    <title>You can win one DGX Station from Dell</title>
    <updated>2025-11-04T09:05:26+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2olu/you_can_win_one_dgx_station_from_dell/"&gt; &lt;img alt="You can win one DGX Station from Dell" src="https://preview.redd.it/h8m5jkpgh7zf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d20a7b9a2750d2bf30b8722bcf1229c20d13e6d3" title="You can win one DGX Station from Dell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h8m5jkpgh7zf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2olu/you_can_win_one_dgx_station_from_dell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2olu/you_can_win_one_dgx_station_from_dell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T09:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo8x7d</id>
    <title>I fine-tuned (SFT) a 14B model on a free Colab session just using TRL</title>
    <updated>2025-11-04T14:41:45+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've put together a notebook that runs on a &lt;strong&gt;free Colab (T4 GPU)&lt;/strong&gt; and lets you fine-tune models up to &lt;strong&gt;14B parameters&lt;/strong&gt; ðŸ¤¯&lt;/p&gt; &lt;p&gt;It only uses &lt;strong&gt;TRL&lt;/strong&gt;, which now includes new memory optimizations that make this possible. In the example, I fine-tune a reasoning model that generates &lt;em&gt;reasoning traces,&lt;/em&gt; and adapt it to produce these traces in different languages depending on the userâ€™s request.&lt;/p&gt; &lt;p&gt;Notebook: &lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More TRL notebooks I also worked on:&lt;br /&gt; &lt;a href="https://github.com/huggingface/trl/tree/main/examples/notebooks"&gt;https://github.com/huggingface/trl/tree/main/examples/notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy coding! :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo8x7d/i_finetuned_sft_a_14b_model_on_a_free_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo8x7d/i_finetuned_sft_a_14b_model_on_a_free_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo8x7d/i_finetuned_sft_a_14b_model_on_a_free_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T14:41:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4lhs</id>
    <title>What's the biggest most common PROBLEM you have in your personal ML/AI side projects?</title>
    <updated>2025-11-04T11:07:02+00:00</updated>
    <author>
      <name>/u/HectorAlcazar11</name>
      <uri>https://old.reddit.com/user/HectorAlcazar11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I'm currently trying to start my first SaaS and I'm searching for a genuinly painful problem to create a solution. Need your help. Got a quick minute to help me?&lt;br /&gt; I'm specifically interested in things that are taking your time, money, or effort. Would be great if you tell me the story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HectorAlcazar11"&gt; /u/HectorAlcazar11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4lhs/whats_the_biggest_most_common_problem_you_have_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4lhs/whats_the_biggest_most_common_problem_you_have_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4lhs/whats_the_biggest_most_common_problem_you_have_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1onxdqx</id>
    <title>GLM-4.5-Air-REAP-82B-A12B-LIMI</title>
    <updated>2025-11-04T03:47:57+00:00</updated>
    <author>
      <name>/u/CoruNethronX</name>
      <uri>https://old.reddit.com/user/CoruNethronX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm in search of a HW grant to make this model a reality. Plan is to fine-tune cerebras/GLM-4.5-Air-REAP-82B-A12B model using GAIR/LIMI dataset. As per arXiv:2509.17567 , we could expect great gain of agentic model abilities. Script can be easily adapted from github.com/GAIR-NLP/LIMI as authors were initially fine-tuned a full GLM4.5 Air 106B model. I would expect the whole process to require about 12 hour on 8xH100 or equivalent H200 or B200 cluster. As a result I'll publish a trained 82B model with (hopefully) increased agentic abilities, a transparent evaluation report and also GGUF and MLX quants under permissive license. I expect 82B q4 quants to behave better than any 106B q3 quants on e.g. 64Gb apple HW. If you're able to provide temporary ssh acess to abovementioned GPU cluster, please contact me and let's do this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoruNethronX"&gt; /u/CoruNethronX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T03:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo5yz6</id>
    <title>Built a lightweight RAG management tool that only reprocesses what actually changed.</title>
    <updated>2025-11-04T12:35:53+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a small tool that lets you edit your RAG data efficiently&lt;/p&gt; &lt;p&gt;So, during my internship I worked on a few RAG setups and one thing that always slowed us down was to them. Every small change in the documents made us reprocessing and reindexing everything from the start. &lt;/p&gt; &lt;p&gt;Recently, I have started working on optim-rag on a goal to reduce this overhead. Basically, It lets you open your data, edit or delete chunks, add new ones, and only reprocesses what actually changed when you commit those changes.&lt;/p&gt; &lt;p&gt;I have been testing it on my own textual notes and research material and updating stuff has been a lot a easier for me at least.&lt;/p&gt; &lt;p&gt;repo â†’ &lt;a href="http://github.com/Oqura-ai/optim-rag"&gt;github.com/Oqura-ai/optim-rag&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This project is still in its early stages, and thereâ€™s plenty I want to improve. But since itâ€™s already at a usable point as a primary application, I decided not to wait and just put it out there. Next, Iâ€™m planning to make it DB agnostic as currently it only supports qdrant. &lt;/p&gt; &lt;p&gt;Iâ€™m also planning to add local model support to all of my active projects, including this one. The main challenge right now is doing this on a student budget, Iâ€™ve only got a &lt;strong&gt;4GB RTX 3050 + 16GB RAM&lt;/strong&gt; on my laptop. If anyone has experience in building tools with local model supports efficiently or tips on testing quality with limited VRAM, Iâ€™d really appreciate your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5yz6/built_a_lightweight_rag_management_tool_that_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5yz6/built_a_lightweight_rag_management_tool_that_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo5yz6/built_a_lightweight_rag_management_tool_that_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T12:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo88sq</id>
    <title>Nvidia Jetson Orin Nano Super (8 gb) Llama-bench: Qwen3-4B-Instruct-2507-Q4_0</title>
    <updated>2025-11-04T14:14:54+00:00</updated>
    <author>
      <name>/u/JEs4</name>
      <uri>https://old.reddit.com/user/JEs4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on an LLM-driven autonomous ground drone. My current implementation is teleoperation over my local network from my host PC. I'm exploring the viability of moving it all to the edge and just picked up an Nvidia Jetson Orin Nano Super to experiment.&lt;/p&gt; &lt;p&gt;I know there have been a few of these posts recently but I hadn't seen anything that actually list out specs and commands used for bench-marking:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Jetson Orin Nano Super (8gb)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;M.2 NVMe Gen3x4 SSD 256GB 2200 MBS&lt;/p&gt; &lt;p&gt;Super Power Mode (profile 2) enabled&lt;/p&gt; &lt;pre&gt;&lt;code&gt;jwest33@jwest33-desktop:~/Desktop/llama.cpp$ ./build/bin/llama-bench \ -m models/Qwen3-4B-Instruct-2507-Q4_0.gguf \ -ngl 99 \ -fa 1 \ -t 6 \ -p 128,512,1024,2048 \ -n 32,64,128,256 \ -b 2048 \ -ub 512 \ -r 3 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: Orin, compute capability 8.7, VMM: yes | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | pp128 | 588.08 Â± 47.70 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | pp512 | 710.32 Â± 1.18 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | pp1024 | 726.05 Â± 8.75 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | pp2048 | 712.74 Â± 0.40 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | tg32 | 23.23 Â± 0.02 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | tg64 | 23.02 Â± 0.01 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | tg128 | 22.40 Â± 0.07 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 1 | tg256 | 22.98 Â± 0.07 | build: cc98f8d34 (6945) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Useless comparison of same bench run on an RTX 5090:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\jwest33&amp;gt; llama-bench -m C:/models/Qwen3-4B-Instruct-2507/Qwen3-4B-Instruct-2507-Q4_0.gguf -ngl 99 -fa 1 -t 6 -p 128,512,1024,2048 -n 32,64,128,256 -b 2048 -ub 512 -r 3 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes load_backend: loaded CUDA backend from C:\llamacpp\ggml-cuda.dll load_backend: loaded RPC backend from C:\llamacpp\ggml-rpc.dll load_backend: loaded CPU backend from C:\llamacpp\ggml-cpu-alderlake.dll | model | size | params | backend | ngl | threads | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -: | --------------: | -------------------: | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | pp128 | 9083.27 Â± 453.11 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | pp512 | 20304.25 Â± 319.92 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | pp1024 | 21760.52 Â± 360.38 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | pp2048 | 21696.48 Â± 91.91 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | tg32 | 316.27 Â± 4.81 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | tg64 | 295.49 Â± 6.21 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | tg128 | 308.85 Â± 1.60 | | qwen3 4B Q4_0 | 2.21 GiB | 4.02 B | CUDA | 99 | 6 | 1 | tg256 | 336.04 Â± 14.27 | build: 961660b8c (6912) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JEs4"&gt; /u/JEs4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo88sq/nvidia_jetson_orin_nano_super_8_gb_llamabench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo88sq/nvidia_jetson_orin_nano_super_8_gb_llamabench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo88sq/nvidia_jetson_orin_nano_super_8_gb_llamabench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T14:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4h0q</id>
    <title>Are 32k-Token Embedding Models Real Innovation or Just Marketing?</title>
    <updated>2025-11-04T11:00:08+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What do you think about embedding models that support input context lengths of up to 32k tokens?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For example, Voyage 3 or Voyage 3.5 (from MongoDB).&lt;/p&gt; &lt;p&gt;Is it just marketing, or does it make a real difference in practice?&lt;/p&gt; &lt;p&gt;Also, which closed-source embedding model would you recommend for top-tier performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4h0q/are_32ktoken_embedding_models_real_innovation_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4h0q/are_32ktoken_embedding_models_real_innovation_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4h0q/are_32ktoken_embedding_models_real_innovation_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo279x</id>
    <title>[Research] LLM judges systematically penalize balanced reasoning - tested mistral, llama3, gemma, phi3, orca-mini</title>
    <updated>2025-11-04T08:33:12+00:00</updated>
    <author>
      <name>/u/Budget-Reception-533</name>
      <uri>https://old.reddit.com/user/Budget-Reception-533</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just published a study on LLM judge bias using 5 local models, and the results are pretty interesting for anyone using LLMs as evaluators.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper + full data&lt;/strong&gt;: &lt;a href="https://zenodo.org/records/17517864"&gt;https://zenodo.org/records/17517864&lt;/a&gt; (DOI: 10.5281/zenodo.17517864)&lt;/p&gt; &lt;h2&gt;Setup&lt;/h2&gt; &lt;p&gt;Tested these models via Ollama: - mistral:7b-instruct - llama3:8b - gemma:2b-instruct&lt;br /&gt; - phi3:mini - orca-mini:7b&lt;/p&gt; &lt;p&gt;Generated 1,500 responses across 30 moral dilemmas with: - 3 prompt framings (neutral, safety-first, freedom-first) - 10 temperatures (0.0 to 1.0) - Deterministic seeds for full reproducibility&lt;/p&gt; &lt;p&gt;Then had GPT-4o-mini and Claude 3.5 Haiku evaluate each response (3,000 total evaluations).&lt;/p&gt; &lt;h2&gt;Key Finding: The &amp;quot;Balance Penalty&amp;quot;&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Judges systematically penalize balanced responses.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When a model says &amp;quot;both values matter, it depends on context&amp;quot; â†’ mean score 3.60&lt;/p&gt; &lt;p&gt;When a model picks one value decisively â†’ mean score 4.36&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gap: 0.76 points (p&amp;lt;0.001, Cohen's d=1.45)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This holds after controlling for: - Which model generated the response - Temperature setting - Prompt framing - Scenario difficulty&lt;/p&gt; &lt;h2&gt;Why This Matters for Local LLM Users&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;If you're using LLM judges for eval&lt;/strong&gt;, they're probably penalizing nuanced reasoning&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Judge disagreement concentrates on balanced responses&lt;/strong&gt;: When responses acknowledge trade-offs, judges disagree 58% of the time vs 34% for decisive responses&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;GPT-4o-mini judges more harshly than Claude 3.5 Haiku&lt;/strong&gt;: GPT penalty is Î²=1.08 (d=2.21), Claude is Î²=0.53 (d=1.00)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Framing matters WAY more than temperature&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Framing effect: 0.4-0.8 points&lt;/li&gt; &lt;li&gt;Temperature effect: 0.15-0.24 points&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're tweaking temperature for &amp;quot;better&amp;quot; outputs, you're probably wasting time. Focus on prompt framing instead.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Model Rankings (All 5 Performed Similarly)&lt;/h2&gt; &lt;p&gt;Mean alignment scores across all judges/scenarios: - orca-mini:7b: 4.31 - llama3:8b: 4.24 - phi3:mini: 4.23 - mistral:7b-instruct: 4.07 - gemma:2b-instruct: 4.05&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The differences between models are smaller than the balance penalty effect&lt;/strong&gt;, suggesting judge bias matters more than model choice for these evaluations.&lt;/p&gt; &lt;h2&gt;Full Reproducibility&lt;/h2&gt; &lt;p&gt;Everything's public on Zenodo: - 1,500 response files (JSONL with full metadata) - 3,000 judge evaluations (CSV with scores + rationales)&lt;br /&gt; - All analysis scripts (Python) - Reproduction instructions - All figures from paper&lt;/p&gt; &lt;p&gt;All code and data are also mirrored in the GitHub repo (github.com/nenocsf2024/trolley_clean, release v1.0.0), so you can clone or download either source and rerun the full pipeline.&lt;/p&gt; &lt;p&gt;You can literally re-run the entire study, or test different models/judges with the same scenarios.&lt;/p&gt; &lt;h2&gt;Implications&lt;/h2&gt; &lt;p&gt;This was inspired by Anthropic's recent work showing frontier LLM judges only agree ~70% of the time. The &amp;quot;balance penalty&amp;quot; appears to explain much of that disagreement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For practical use&lt;/strong&gt;: If you're using LLM judges to evaluate your local models, be aware they might be systematically penalizing nuanced, context-dependent reasoning in favor of decisive answers.&lt;/p&gt; &lt;h2&gt;Questions for the community:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Have you noticed similar patterns when using LLM judges?&lt;/li&gt; &lt;li&gt;Do you think this is a bug (bad judge calibration) or feature (decisive answers are genuinely better)?&lt;/li&gt; &lt;li&gt;For those doing RLHF/DPO with LLM judges - has this affected your training?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Planning Phase 2 with API models (GPT-4, Claude Opus, Gemini) and human validation. Suggestions welcome!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For those asking about reproduction - yes, you can literally clone this and test your own local models. The scenario file + judging scripts are in the Zenodo archive. DM if you hit any issues!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Budget-Reception-533"&gt; /u/Budget-Reception-533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo279x/research_llm_judges_systematically_penalize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo279x/research_llm_judges_systematically_penalize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo279x/research_llm_judges_systematically_penalize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T08:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo1j5x</id>
    <title>Open Source Alternative to NotebookLM/Perplexity</title>
    <updated>2025-11-04T07:48:59+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Hereâ€™s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1j5x/open_source_alternative_to_notebooklmperplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1j5x/open_source_alternative_to_notebooklmperplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1j5x/open_source_alternative_to_notebooklmperplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T07:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8qe5</id>
    <title>basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet</title>
    <updated>2025-11-03T10:57:38+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt; &lt;img alt="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" src="https://external-preview.redd.it/d240ODlsYmJ3MHpmMRIAV1OZPMFu-DibzoX2jf4rOivExvgg5eIy0W2GXihc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=279c74da96e009360fea0b2b573c2a5636ed406e" title="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models I used:&lt;/p&gt; &lt;p&gt;- RF-DETR â€“ a DETR-style real-time object detector. We fine-tuned it to detect players, jersey numbers, referees, the ball, and even shot types.&lt;/p&gt; &lt;p&gt;- SAM2 â€“ a segmentation and tracking. It re-identifies players after occlusions and keeps IDs stable through contact plays.&lt;/p&gt; &lt;p&gt;- SigLIP + UMAP + K-means â€“ vision-language embeddings plus unsupervised clustering. This separates players into teams using uniform colors and textures, without manual labels.&lt;/p&gt; &lt;p&gt;- SmolVLM2 â€“ a compact vision-language model originally trained on OCR. After fine-tuning on NBA jersey crops, it jumped from 56% to 86% accuracy.&lt;/p&gt; &lt;p&gt;- ResNet-32 â€“ a classic CNN fine-tuned for jersey number classification. It reached 93% test accuracy, outperforming the fine-tuned SmolVLM2.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- code: &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- blogpost: &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;https://blog.roboflow.com/identify-basketball-players&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- detection dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- numbers OCR dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/367omkbbw0zf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo7kqy</id>
    <title>Is GPT-OSS-120B the best llm that fits in 96GB VRAM?</title>
    <updated>2025-11-04T13:48:00+00:00</updated>
    <author>
      <name>/u/GreedyDamage3735</name>
      <uri>https://old.reddit.com/user/GreedyDamage3735</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I wonder if gpt-oss-120b is the best local llm, with respect to the general intelligence(and reasoning ability), that can be run on 96GB VRAM GPU. Do you guys have any suggestions otherwise gpt-oss?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreedyDamage3735"&gt; /u/GreedyDamage3735 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo7kqy/is_gptoss120b_the_best_llm_that_fits_in_96gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo7kqy/is_gptoss120b_the_best_llm_that_fits_in_96gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo7kqy/is_gptoss120b_the_best_llm_that_fits_in_96gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T13:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1onytak</id>
    <title>How much does the average person value a private LLM?</title>
    <updated>2025-11-04T05:02:49+00:00</updated>
    <author>
      <name>/u/SelectLadder8758</name>
      <uri>https://old.reddit.com/user/SelectLadder8758</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been thinking a lot about the future of local LLMs lately. My current take is that while it will eventually be possible (or maybe already is) for everyone to run very capable models locally, Iâ€™m not sure how many people will. For example, many people could run an email server themselves but everyone uses Gmail. DuckDuckGo is a perfectly viable alternative but Google still prevails. &lt;/p&gt; &lt;p&gt;Will LLMs be the same way or will there eventually be enough advantages of running locally (including but not limited to privacy) for them to realistically challenge cloud providers? Is privacy alone enough?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SelectLadder8758"&gt; /u/SelectLadder8758 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo62ww</id>
    <title>KTransformers Open Source New Era: Local Fine-tuning of Kimi K2 and DeepSeek V3</title>
    <updated>2025-11-04T12:44:46+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo62ww/ktransformers_open_source_new_era_local/"&gt; &lt;img alt="KTransformers Open Source New Era: Local Fine-tuning of Kimi K2 and DeepSeek V3" src="https://b.thumbs.redditmedia.com/G8JVqa_7WcL29nT8q-qoAhmxaVcg1D8DRRGWHkHKJjg.jpg" title="KTransformers Open Source New Era: Local Fine-tuning of Kimi K2 and DeepSeek V3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hvu5ohojj8zf1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45b8043885b8171ead6c7ecef513f2585f53e186"&gt;https://preview.redd.it/hvu5ohojj8zf1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45b8043885b8171ead6c7ecef513f2585f53e186&lt;/a&gt;&lt;/p&gt; &lt;p&gt;KTransformers has enabled multi-GPU inference and local fine-tuning capabilities through collaboration with the SGLang and LLaMa-Factory communities. Users can now support higher-concurrency local inference via multi-GPU parallelism and fine-tune ultra-large models like DeepSeek 671B and Kimi K2 1TB locally, greatly expanding the scope of applications.&lt;/p&gt; &lt;p&gt;A dedicated introduction to the Expert Deferral feature just &lt;a href="https://github.com/sgl-project/sglang/pull/12586"&gt;submitted&lt;/a&gt; to the SGLang&lt;/p&gt; &lt;p&gt;In short, our original CPU/GPU parallel scheme left the CPU idle during MLA computationâ€”already a bottleneckâ€”because it only handled routed experts, forcing CPU and GPU to run alternately, which was wasteful.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7h4zt7hyj8zf1.png?width=1637&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=986fec35461ecfb03ead784b800459235017566b"&gt;https://preview.redd.it/7h4zt7hyj8zf1.png?width=1637&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=986fec35461ecfb03ead784b800459235017566b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our fix is simple: leveraging the residual network property, we defer the accumulation of the least-important few (typically 4) of the top-k experts to the next layerâ€™s residual path. This effectively creates a parallel attn/ffn structure that increases CPU/GPU overlap.&lt;/p&gt; &lt;p&gt;Experiments (detailed numbers in our SOSPâ€™25 &lt;a href="https://madsys.cs.tsinghua.edu.cn/publication/ktransformers-unleashing-the-full-potential-of-cpu/gpu-hybrid-inference-for-moe-models/SOSP25-chen.pdf"&gt;paper&lt;/a&gt;) show that deferring, rather than simply skipping, largely preserves model quality while boosting performance by over 30%. Such system/algorithm co-design is now a crucial optimization avenue, and we are exploring further possibilities.&lt;/p&gt; &lt;h1&gt;Fine-tuning with LLaMA-Factory&lt;/h1&gt; &lt;p&gt;Compared to the still-affordable API-based inference, local fine-tuningâ€”especially light local fine-tuning after minor model tweaksâ€”may in fact be a more important need for the vast community of local players. After months of development and tens of thousands of lines of code, this feature has finally been implemented and open-sourced today with the help of the LLaMA-Factory community.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o3nes3sbk8zf1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84f080c9ebfa1b3202001242174549236bc8f83d"&gt;https://preview.redd.it/o3nes3sbk8zf1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84f080c9ebfa1b3202001242174549236bc8f83d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Similar to Unslothâ€™s GPU memory-reduction capability, LLaMa-Factory integrated with KTransformers can, when VRAM is still insufficient, leverage CPU/AMX-instruction compute for CPU-GPU heterogeneous fine-tuning, achieving the dramatic drop in VRAM demand shown below. With just one server plus two RTX 4090s, you can now fine-tune DeepSeek 671B locally!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u7yqc13fk8zf1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ede3dcf8eb95134929b452b67bc76dfeb8e8730"&gt;https://preview.redd.it/u7yqc13fk8zf1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ede3dcf8eb95134929b452b67bc76dfeb8e8730&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo62ww/ktransformers_open_source_new_era_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo62ww/ktransformers_open_source_new_era_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo62ww/ktransformers_open_source_new_era_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T12:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4sfz</id>
    <title>Schema based prompting</title>
    <updated>2025-11-04T11:18:09+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd argue using json schemas for inputs/outputs makes model interactions more reliable, especially when working on agents across different models. Mega prompts that cover all edge cases work with only one specific model. New models get released on a weekly or existing ones get updated, then older versions are discontinued and you have to start over with your prompt.&lt;/p&gt; &lt;p&gt;Why isn't schema based prompting more common practice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4sfz/schema_based_prompting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4sfz/schema_based_prompting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4sfz/schema_based_prompting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4kh7</id>
    <title>Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU</title>
    <updated>2025-11-04T11:05:26+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt; &lt;img alt="Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU" src="https://b.thumbs.redditmedia.com/nnPDB-YtTPCthxTtyPVParTYcBmaEE0gf_kWM64QSso.jpg" title="Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're the KTransformers team (formerly known for our DeepSeek-V3 local CPU/GPU hybrid inference project).&lt;/p&gt; &lt;p&gt;Today, we're proud to announce full integration with LLaMA-Factory, enabling you to &lt;strong&gt;fine-tune DeepSeek-671B or Kimi-K2-1TB locally with just 4x RTX 4090 GPUs&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dlipq1us28zf1.png?width=2332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92fad09b19f37c76c3f08fe9e326816ad4d533d1"&gt;https://preview.redd.it/dlipq1us28zf1.png?width=2332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92fad09b19f37c76c3f08fe9e326816ad4d533d1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/finetuning-deepseek-671b-locally-with-only-80gb-vram-and-v0-24938oydy7zf1.png?width=2246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=216765e8119e54cc2bdc92bf24b082575f7d1bdc"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/finetuning-deepseek-671b-locally-with-only-80gb-vram-and-v0-w1m1j89jy7zf1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bde4b33c857b8fd4c1f4d8c0c4ecc42763f5bbc"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More infomation can be found at&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/tree/main/KT-SFT"&gt;https://github.com/kvcache-ai/ktransformers/tree/main/KT-SFT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo1159</id>
    <title>Anyone else feel like GPU pricing is still the biggest barrier for open-source AI?</title>
    <updated>2025-11-04T07:15:29+00:00</updated>
    <author>
      <name>/u/frentro_max</name>
      <uri>https://old.reddit.com/user/frentro_max</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even with cheap clouds popping up, costs still hit fast when you train or fine-tune.&lt;br /&gt; How do you guys manage GPU spend for experiments?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frentro_max"&gt; /u/frentro_max &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T07:15:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo6226</id>
    <title>Disappointed by dgx spark</title>
    <updated>2025-11-04T12:43:28+00:00</updated>
    <author>
      <name>/u/RockstarVP</name>
      <uri>https://old.reddit.com/user/RockstarVP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"&gt; &lt;img alt="Disappointed by dgx spark" src="https://preview.redd.it/a1tbzs1dk8zf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=090e0bdb3a3f9757ae6bdbff3964dc951a1361ed" title="Disappointed by dgx spark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just tried Nvidia dgx spark irl&lt;/p&gt; &lt;p&gt;gorgeous golden glow, feels like gpu royalty&lt;/p&gt; &lt;p&gt;â€¦but 128gb shared ram still underperform whenrunning qwen 30b with context on vllm&lt;/p&gt; &lt;p&gt;for 5k usd, 3090 still king if you value raw speed over design&lt;/p&gt; &lt;p&gt;anyway, wont replce my mac anytime soon&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RockstarVP"&gt; /u/RockstarVP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1tbzs1dk8zf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T12:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1onzrg9</id>
    <title>Qwen is roughly matching the entire American open model ecosystem today</title>
    <updated>2025-11-04T05:57:18+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt; &lt;img alt="Qwen is roughly matching the entire American open model ecosystem today" src="https://preview.redd.it/zvugibssj6zf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1b76885ebcc9a9fe34b1f3215330df073cc1f12" title="Qwen is roughly matching the entire American open model ecosystem today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvugibssj6zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
