<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-12T07:29:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qa4q8m</id>
    <title>Open Models Are Now Frontier Models</title>
    <updated>2026-01-11T17:08:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa4q8m/open_models_are_now_frontier_models/"&gt; &lt;img alt="Open Models Are Now Frontier Models" src="https://external-preview.redd.it/ANuVteKDUN2HVuOpqEJfoAjatmVKSVbeCa4mmOeNlZ4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d79b23354af1d44a07b76dea0ec75a43b11d757" title="Open Models Are Now Frontier Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CES 2026&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=mCcXwBdQpf8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa4q8m/open_models_are_now_frontier_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa4q8m/open_models_are_now_frontier_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T17:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9vtgz</id>
    <title>llama.cpp MLA KV cache support for KimiLinear-48B-A3B</title>
    <updated>2026-01-11T10:10:29+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I added backend agnostic support for KimiLinear.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q586jv/comment/nxz63pt/?context=1"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1q586jv/comment/nxz63pt/?context=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I noticed that the original author didn't implement support for MLA KV cache, so I read the DeepSeekV3 MLA kv cache PR to add the support to KimiLinear.&lt;/p&gt; &lt;p&gt;This reduces 1M tokens F16 KV cache usage from 140GB to 14.875GB. So now it is possible to run super long context locally with your low VRAM card.&lt;/p&gt; &lt;p&gt;To run it please re-download the GGUF from&lt;br /&gt; &lt;a href="https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;br /&gt; and compile the code with&lt;br /&gt; git clone &lt;a href="https://github.com/ymcki/llama.cpp"&gt;https://github.com/ymcki/llama.cpp&lt;/a&gt; --branch Kimi-Linear&lt;br /&gt; cd llama.cpp&lt;br /&gt; cmake -B build -DGGML_CUDA=ON&lt;br /&gt; cmake --build build --config Release -j 6&lt;/p&gt; &lt;p&gt;At some point, KimiLinear was the best performing open weight model at contextarena. But it has since been deprecated for unknown reasons. You can see it by clicking the Control Tabs link to un-deprecated it. &lt;a href="https://contextarena.ai/"&gt;https://contextarena.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please give it a try and tell me to see if it can serve your long context needs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;KV Quant&lt;/th&gt; &lt;th align="left"&gt;bpw&lt;/th&gt; &lt;th align="left"&gt;KV Size at 1M&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;f16&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;14.875GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q8_0&lt;/td&gt; &lt;td align="left"&gt;8.5&lt;/td&gt; &lt;td align="left"&gt;7.902GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q5_1&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;5.578GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q5_0&lt;/td&gt; &lt;td align="left"&gt;5.5&lt;/td&gt; &lt;td align="left"&gt;5.113GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q4_1&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;4.648GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q4_0&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;td align="left"&gt;4.184GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;iq4_nl&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;td align="left"&gt;4.184GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;VRAM poor people can adjust their KV cache quant away from the default f16.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T10:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qafrp8</id>
    <title>4x RTX 6000 Pro LACT Config</title>
    <updated>2026-01-12T00:16:07+00:00</updated>
    <author>
      <name>/u/I-cant_even</name>
      <uri>https://old.reddit.com/user/I-cant_even</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a little tuning but I was able to get this config working for LACT with my Blackwells on a single 1600 Watt GPU.&lt;/p&gt; &lt;p&gt;This likely can still be optimized but should serve as a good starting point for anyone else running 4 Blackwell GPUs from one 1600W PSU&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version: 5 daemon: log_level: info admin_group: sudo disable_clocks_cleanup: false apply_settings_timer: 5 current_profile: null auto_switch_profiles: false gpus: 10DE:2BB1-10DE:204B-0000:01:00.0: vendor: nvidia power_cap: 310 min_core_clock: 210 max_core_clock: 2600 gpu_clock_offsets: 0: 1100 mem_clock_offsets: 0: 4000 10DE:2BB1-10DE:204B-0000:21:00.0: vendor: nvidia power_cap: 310 min_core_clock: 210 max_core_clock: 2600 gpu_clock_offsets: 0: 1100 mem_clock_offsets: 0: 4000 10DE:2BB1-10DE:204B-0000:41:00.0: vendor: nvidia power_cap: 310 min_core_clock: 210 max_core_clock: 2600 gpu_clock_offsets: 0: 1100 mem_clock_offsets: 0: 4000 10DE:2BB1-10DE:204B-0000:81:00.0: vendor: nvidia power_cap: 310 min_core_clock: 210 max_core_clock: 2600 gpu_clock_offsets: 0: 1100 mem_clock_offsets: 0: 4000 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I-cant_even"&gt; /u/I-cant_even &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qafrp8/4x_rtx_6000_pro_lact_config/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qafrp8/4x_rtx_6000_pro_lact_config/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qafrp8/4x_rtx_6000_pro_lact_config/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T00:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qacy7z</id>
    <title>[2512.14982] Prompt Repetition Improves Non-Reasoning LLMs</title>
    <updated>2026-01-11T22:19:43+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2512.14982"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qacy7z/251214982_prompt_repetition_improves_nonreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qacy7z/251214982_prompt_repetition_improves_nonreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T22:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaodvw</id>
    <title>Remote Code Execution in OpenCode, update now</title>
    <updated>2026-01-12T07:14:36+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cy.md/opencode-rce/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaodvw/remote_code_execution_in_opencode_update_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaodvw/remote_code_execution_in_opencode_update_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T07:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa9z13</id>
    <title>LG's K-Exaone breaks into global top 10 AI rankings, tops South Korea</title>
    <updated>2026-01-11T20:23:20+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa9z13/lgs_kexaone_breaks_into_global_top_10_ai_rankings/"&gt; &lt;img alt="LG's K-Exaone breaks into global top 10 AI rankings, tops South Korea" src="https://external-preview.redd.it/zsdIgzCA4Co7FIGifQyqSsmD_BRUFvhPhc7XFBhC0gc.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf34ce2c3738c04870a77808c12cc410cfd541c0" title="LG's K-Exaone breaks into global top 10 AI rankings, tops South Korea" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://m.koreaherald.com/article/10652980"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa9z13/lgs_kexaone_breaks_into_global_top_10_ai_rankings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa9z13/lgs_kexaone_breaks_into_global_top_10_ai_rankings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T20:23:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaj6je</id>
    <title>Advice for a tool that blocks dangerous terminal commands from AI coding assistants</title>
    <updated>2026-01-12T02:46:57+00:00</updated>
    <author>
      <name>/u/spacepings</name>
      <uri>https://old.reddit.com/user/spacepings</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt; I'm building a Mac app that intercepts dangerous terminal commands before they execute. The goal is to catch things like rm -rf or git reset --hard when AI coding tools (Claude Code, Cursor, etc.) accidentally run something destructive.&lt;/p&gt; &lt;p&gt; The idea came after Claude deleted my src/ folder while &amp;quot;cleaning up files.&amp;quot; I figured I'm probably not the only one this has happened to.&lt;/p&gt; &lt;p&gt; Right now it:&lt;/p&gt; &lt;p&gt; - Hooks into zsh to catch commands before they run&lt;/p&gt; &lt;p&gt; - Shows a popup letting you Block, Allow, or Snapshot first&lt;/p&gt; &lt;p&gt; - Works offline, no cloud, no account&lt;/p&gt; &lt;p&gt; Can you give me some feedback on whether this is useful? What commands would you want it to catch? Is this overkill or have you had similar accidents?&lt;/p&gt; &lt;p&gt; Here's a quick demo: &lt;a href="https://osiris-sable.vercel.app"&gt;https://osiris-sable.vercel.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt; Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacepings"&gt; /u/spacepings &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaj6je/advice_for_a_tool_that_blocks_dangerous_terminal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaj6je/advice_for_a_tool_that_blocks_dangerous_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaj6je/advice_for_a_tool_that_blocks_dangerous_terminal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T02:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaap05</id>
    <title>Benchmarks of Radeon 780M iGPU with shared 128GB DDR5 RAM running various MoE models under Llama.cpp</title>
    <updated>2026-01-11T20:51:53+00:00</updated>
    <author>
      <name>/u/AzerbaijanNyan</name>
      <uri>https://old.reddit.com/user/AzerbaijanNyan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking for a budget system capable of running the later MoE models for basic one-shot queries. Main goal was finding something energy efficient to keep online 24/7 without racking up an exorbitant electricity bill.&lt;/p&gt; &lt;p&gt;I eventually settled on a refurbished Minisforum UM890 Pro which at the time, September, seemed like the most cost-efficient option for my needs.&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UM890 Pro&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/radeon-780m.c4020"&gt;AMD Radeon™ 780M iGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;128GB DDR5 (Crucial DDR5 RAM 128GB Kit (2x64GB) 5600MHz SODIMM CL46)&lt;/p&gt; &lt;p&gt;2TB M.2&lt;/p&gt; &lt;p&gt;Linux Mint 22.2&lt;/p&gt; &lt;p&gt;ROCm 7.1.1 with &lt;strong&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/strong&gt; override&lt;/p&gt; &lt;p&gt;llama.cpp build: b13771887 (7699)&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;Below are some benchmarks using various MoE models. Llama 7B is included for comparison since there's an ongoing thread gathering data for various AMD cards under ROCm here - &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15021"&gt;Performance of llama.cpp on AMD ROCm (HIP) #15021&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I also tested various Vulkan builds but found it too close in performance to warrant switching to since I'm also testing other ROCm AMD cards on this system over OCulink. &lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;llama-bench -ngl 99 -fa 1 -d 0,4096,8192,16384 -m [model]&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;514.88 ± 4.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;19.27 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;288.95 ± 3.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;11.59 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d8192&lt;/td&gt; &lt;td align="right"&gt;183.77 ± 2.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d8192&lt;/td&gt; &lt;td align="right"&gt;8.36 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;100.00 ± 1.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="right"&gt;3.56 GiB&lt;/td&gt; &lt;td align="right"&gt;6.74 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;5.49 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;575.41 ± 8.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;28.34 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;390.27 ± 5.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;16.25 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d8192&lt;/td&gt; &lt;td align="right"&gt;303.25 ± 4.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d8192&lt;/td&gt; &lt;td align="right"&gt;10.09 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;210.54 ± 2.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;11.27 GiB&lt;/td&gt; &lt;td align="right"&gt;20.91 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;6.11 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;217.08 ± 3.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;20.14 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;174.96 ± 3.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;11.22 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d8192&lt;/td&gt; &lt;td align="right"&gt;143.78 ± 1.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d8192&lt;/td&gt; &lt;td align="right"&gt;6.88 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;109.48 ± 1.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;4.13 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;265.07 ± 3.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;25.83 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;168.86 ± 1.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;6.01 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d8192&lt;/td&gt; &lt;td align="right"&gt;124.47 ± 0.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d8192&lt;/td&gt; &lt;td align="right"&gt;3.41 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;81.27 ± 0.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;23.36 GiB&lt;/td&gt; &lt;td align="right"&gt;30.53 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;2.10 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;138.44 ± 1.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;12.45 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;131.49 ± 1.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;10.46 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d8192&lt;/td&gt; &lt;td align="right"&gt;122.66 ± 1.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d8192&lt;/td&gt; &lt;td align="right"&gt;8.80 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;107.32 ± 1.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B Q6_K&lt;/td&gt; &lt;td align="right"&gt;63.67 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;6.73 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;So, am I satisfied with the system? Yes, it performs around what I hoping to. Power draw is 10-13 watt idle with gpt-oss 120B loaded. Inference brings that up to around 75. As an added bonus the system is so silent I had to check so the fan was actually running the first time I started it.&lt;/p&gt; &lt;p&gt;The shared memory means it's possible to run Q8+ quants of many models and the cache at f16+ for higher quality outputs. 120GB something availible also allows having more than one model loaded, personally I've been running Qwen3-VL-30B-A3B-Instruct as a visual assistant for gpt-oss 120B. I found this combo very handy to transcribe hand written letters for translation.&lt;/p&gt; &lt;p&gt;Token generation isn't stellar as expected for a dual channel system but acceptable for MoE one-shots and this is a secondary system that can chug along while I do something else. There's also the option of using one of the two M.2 slots for an OCulink eGPU and increased performance.&lt;/p&gt; &lt;p&gt;Another perk is the portability, at 130mm/126mm/52.3mm it fits easily into a backpack or suitcase.&lt;/p&gt; &lt;p&gt;So, do I recommend this system? Unfortunately no and that's solely due to the current prices of RAM and other hardware. I suspect assembling the system today would cost at least three times as much making the price/performance ratio considerably less appealing.&lt;/p&gt; &lt;p&gt;Disclaimer: I'm not an experienced Linux user so there's likely some performance left on the table.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AzerbaijanNyan"&gt; /u/AzerbaijanNyan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaap05/benchmarks_of_radeon_780m_igpu_with_shared_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaap05/benchmarks_of_radeon_780m_igpu_with_shared_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaap05/benchmarks_of_radeon_780m_igpu_with_shared_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T20:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qao1ra</id>
    <title>Agentic ProbLLMs: Exploiting AI Computer-Use and Coding Agents (youtube) -- "local" can make people complacent on security, but if you push code to github, worth a watch, even if you don't use AI coding tools.</title>
    <updated>2026-01-12T06:54:41+00:00</updated>
    <author>
      <name>/u/coloradical5280</name>
      <uri>https://old.reddit.com/user/coloradical5280</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good talk at 39C3 Conference &lt;a href="https://www.youtube.com/watch?v=8pbz5y7_WkM"&gt;https://www.youtube.com/watch?v=8pbz5y7_WkM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nothing novel, no breaking news here, but a nice tight overview of the landscape, with a great overview of AgentHopper, which is basically a virus framework spread by coding agents via your local env, and push/pull from github. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Adversarial Misclassification in Vision &amp;amp; Text Models&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=42"&gt;00:42&lt;/a&gt;], [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=2703"&gt;45:03&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;The speaker demonstrates how hidden commands in images or text (like invisible Unicode tags) can force major AI models like Gemini and Grok to misclassify a panda as a monkey or answer &amp;quot;42&amp;quot; to &amp;quot;1+1&amp;quot;.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Malware Download via Computer-Use Agents&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=493"&gt;08:13&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;Anthropic’s &amp;quot;Computer Use&amp;quot; agent is tricked into clicking a link on a malicious website, downloading a malware binary, making it executable, and launching it to join a botnet.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;ClickFix&amp;quot; Social Engineering Attack on AI Agents&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=638"&gt;10:38&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;Agents are shown to be vulnerable to &amp;quot;ClickFix&amp;quot; attacks where they are tricked into copying malicious code from a fake &amp;quot;prove you are human&amp;quot; prompt and pasting it into a terminal, granting attackers remote access.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Leakage via Local Port Exposure (Devin AI)&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=1093"&gt;18:13&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;The coding agent Devin is manipulated through a multi-stage prompt injection to run a local web server exposing its file system, then leaking the public URL to an attacker via an image render.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Exfiltration via DNS Requests (Claude Code &amp;amp; Amazon Q)&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=1332"&gt;22:12&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;The speaker exposes a flaw where agents allow specific commands like ping or nslookup without user approval, which can be exploited to smuggle sensitive environment variables out via DNS queries.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Arbitrary Code Execution via find Command (Amazon Q)&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=1562"&gt;26:02&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;Amazon Q’s developer extension allowed the find command to run without approval, which was exploited using the -exec flag to launch arbitrary commands (like a calculator) on the host machine.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hidden Instructions via Unicode Tags (Google Jewels &amp;amp; Anti-Gravity)&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=1625"&gt;27:05&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;Invisible Unicode tag characters hidden in GitHub issues or tickets are used to inject malicious instructions that the AI can read but humans cannot see, leading to unauthorized code compilation and execution.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Modifying Configuration &amp;amp; &amp;quot;YOLO Mode&amp;quot; (GitHub Copilot)&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=1869"&gt;31:09&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;GitHub Copilot is tricked into modifying its own settings.json file to enable &amp;quot;tools.approve&amp;quot; (YOLO mode), effectively bypassing human-in-the-loop security controls to allow unrestricted code execution.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Agent Configuration Exploits&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=2086"&gt;34:46&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;The presenter explains how one compromised agent can be used to modify the configuration files of a different agent on the same machine, &amp;quot;freeing&amp;quot; it to run malicious commands.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Agent Hopper&amp;quot; AI Virus&lt;/strong&gt; [&lt;a href="http://www.youtube.com/watch?v=8pbz5y7_WkM&amp;amp;t=2144"&gt;35:44&lt;/a&gt;] &lt;ul&gt; &lt;li&gt;A proof-of-concept AI worm creates a self-replicating cycle where an infected repository infects the developer's agent, which then spreads the malicious prompt to other repositories and pushes them back to GitHub to infect new developers.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coloradical5280"&gt; /u/coloradical5280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qao1ra/agentic_probllms_exploiting_ai_computeruse_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qao1ra/agentic_probllms_exploiting_ai_computeruse_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qao1ra/agentic_probllms_exploiting_ai_computeruse_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T06:54:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qamt9a</id>
    <title>MiniMax-M2.1 vs GLM-4.5-Air is the bigger really the better (coding)?</title>
    <updated>2026-01-12T05:45:01+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I managed to get both MiniMax-M2.1 and GLM-4.5-Air running locally with 48GB vram and 128GB ram.&lt;/p&gt; &lt;p&gt;- MiniMax-M2.1-UD-Q4_K_XL&lt;/p&gt; &lt;p&gt;- GLM-4.5-Air-UD-Q6_K_XL&lt;/p&gt; &lt;p&gt;Both with 100k context q8_0 KV, and both get simmilar speed: ~11 to ~6tps when context is mostly filled. Minimax has slightly slower prompt processing than GLM. Not great not terrible but enough for agentic coding.&lt;/p&gt; &lt;p&gt;I've read good things about the MiniMax but frankly I can't convince myself it is a better model, using both models with Cline in Vscode&lt;/p&gt; &lt;p&gt;- GLM reliably generates better and more detailed plan of action comparing to Minimax and diligently executes step by step&lt;/p&gt; &lt;p&gt;- Minimax aims to complete the (less) detailed plan, often ignoring some issues just to mark it done&lt;/p&gt; &lt;p&gt;- Despite being smaller, GLM produces better code and requires less intervention after the task is completed comparing to Minimax.&lt;/p&gt; &lt;p&gt;Anyone else having simmilar observations?&lt;/p&gt; &lt;p&gt;In both cases i run the sam prompt, on a project that requires:&lt;br /&gt; - you are an expert working on a new feature&lt;br /&gt; - analyze existing code base&lt;br /&gt; - make some architecturial decision&lt;br /&gt; - implement feature&lt;br /&gt; - implement test&lt;br /&gt; - verify all works (end to end testing)&lt;/p&gt; &lt;p&gt;I have &amp;quot;only&amp;quot; 48GB VRAM and 128GB RAM for my AI VM, here's the llama.cpp config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; GLM-4.5-Air: cmd: &amp;gt; llama-server --port ${PORT} --model /nvme/gguf/GLM-4.5-Air-UD-Q6_K_XL-00001-of-00003.gguf --ctx-size 100000 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on --temp 1.0 --min-p 0.0 --top-p 0.95 --top-k 40 --batch-size 4096 --ubatch-size 1024 -ngl 999 -mg 0 -ts 20,22 -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; aliases: - glm-4.5-air MiniMax-M2.1: cmd: &amp;gt; llama-server --port ${PORT} --model /nvme/gguf/MiniMax-M2.1-UD-Q4_K_XL-00001-of-00003.gguf --ctx-size 100000 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on --temp 1.0 --min-p 0.0 --top-p 0.95 --top-k 40.0 --batch-size 4096 --ubatch-size 1024 --mmap -ngl 999 -mg 0 -ts 10,61 -ot &amp;quot;\.(1[4-9]|[2-9][0-9])\.ffn_(up|down|gate)_exps.=CPU&amp;quot; aliases: - minimax-m2.1 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qamt9a/minimaxm21_vs_glm45air_is_the_bigger_really_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qamt9a/minimaxm21_vs_glm45air_is_the_bigger_really_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qamt9a/minimaxm21_vs_glm45air_is_the_bigger_really_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T05:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa3n3c</id>
    <title>Reimagining LLM Memory: Using Context as Training Data Unlocks Models That Learn at Test-Time | NVIDIA Technical Blog</title>
    <updated>2026-01-11T16:27:13+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa3n3c/reimagining_llm_memory_using_context_as_training/"&gt; &lt;img alt="Reimagining LLM Memory: Using Context as Training Data Unlocks Models That Learn at Test-Time | NVIDIA Technical Blog" src="https://external-preview.redd.it/sIzwxcDDB1V5UoXtSlLxRILbvv5qDpwwC5jpE0X-R6k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e3583628e2f4457bcb934568992a22c472b03ad" title="Reimagining LLM Memory: Using Context as Training Data Unlocks Models That Learn at Test-Time | NVIDIA Technical Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.nvidia.com/blog/reimagining-llm-memory-using-context-as-training-data-unlocks-models-that-learn-at-test-time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa3n3c/reimagining_llm_memory_using_context_as_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa3n3c/reimagining_llm_memory_using_context_as_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T16:27:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9xn78</id>
    <title>Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026</title>
    <updated>2026-01-11T12:00:23+00:00</updated>
    <author>
      <name>/u/GoodSamaritan333</name>
      <uri>https://old.reddit.com/user/GoodSamaritan333</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"&gt; &lt;img alt="Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026" src="https://external-preview.redd.it/9NT_b7vJLOJdi5qbccIw0AbUH9Ctzy98ZNJ7UkM8Ia8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea2ae08a8c43e60ca50bc40d967d25ca9d31b13f" title="Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GoodSamaritan333"&gt; /u/GoodSamaritan333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/345000/gigabyte-announces-support-for-256gb-of-ddr5-7200-cqdimms-at-ces-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T12:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qag0nf</id>
    <title>Hunyuan MT-1.5 Demo</title>
    <updated>2026-01-12T00:26:56+00:00</updated>
    <author>
      <name>/u/finanzwegwerf20</name>
      <uri>https://old.reddit.com/user/finanzwegwerf20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, Hunyuan released a new translation model called &lt;a href="https://huggingface.co/tencent/HY-MT1.5-7B"&gt;MT-1.5&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It seems like there is no public demo (at least without signup), so I hosted the Q8_0 version with llama.cpp and a basic frontend to play around with different languages.&lt;/p&gt; &lt;p&gt;I am pretty impressed by the 7B model so far. I tried out a few different examples and it mostly &amp;quot;agrees&amp;quot; with the output of closed-source models like ChatGPT. Hope it helps in my spanish learning journey!&lt;/p&gt; &lt;p&gt;Here's the link: &lt;a href="https://ai.lucahu.xyz/translate"&gt;ai.lucahu.xyz/translate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/finanzwegwerf20"&gt; /u/finanzwegwerf20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qag0nf/hunyuan_mt15_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qag0nf/hunyuan_mt15_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qag0nf/hunyuan_mt15_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T00:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qabbww</id>
    <title>I prayed that China success with their chip game</title>
    <updated>2026-01-11T21:16:44+00:00</updated>
    <author>
      <name>/u/pbad1</name>
      <uri>https://old.reddit.com/user/pbad1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jensen Huang seems like a nice guy but his strategy has been very rushless when come to business and it frustrated me a bit. &lt;/p&gt; &lt;p&gt;- Get rid of NVLink&lt;br /&gt; - Limited production for high VRAM GPU&lt;/p&gt; &lt;p&gt;Same stuff with all of the Western chip companies. It seems like nowaday they just make and sell stuff to each others cause of the massive monopoly in the industry for everything Chip and specially RAM related. Even AMD seems to dig the consumer's market soonish. Weridly the only guy who still focus on the consumer market is APLLE :))&lt;/p&gt; &lt;p&gt;Chinese big tech seems to be the only group of companies that are actually still putting effort into the consumer market, it just that they are a bit behind in certain technology. &lt;/p&gt; &lt;p&gt;Imagine the day that Chinese RAM, GPU and other parts flood the market, probably gonna eat some tariff like their cars but still, at least it gonna put some competitiveness to the place.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pbad1"&gt; /u/pbad1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qabbww/i_prayed_that_china_success_with_their_chip_game/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qabbww/i_prayed_that_china_success_with_their_chip_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qabbww/i_prayed_that_china_success_with_their_chip_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T21:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaaps7</id>
    <title>It's a very good time to get a 5060ti 16GB</title>
    <updated>2026-01-11T20:52:45+00:00</updated>
    <author>
      <name>/u/pbad1</name>
      <uri>https://old.reddit.com/user/pbad1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;16GB vram is enough for ZIT, Qwen-Image-2512 and LTX-2 (tested!). Seems like Image Gen and Vid Gen models are aiming for this range of 16GB VRAM. &lt;/p&gt; &lt;p&gt;Gamers hate this card appearantly, all of them go for the 5070, so max VRAM/$ value (I think this have better value than a used 3090).&lt;/p&gt; &lt;p&gt;RAM price going up, Nvidia might cut this card soon (rumor). &lt;/p&gt; &lt;p&gt;Any comparable alternative atm? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pbad1"&gt; /u/pbad1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaaps7/its_a_very_good_time_to_get_a_5060ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaaps7/its_a_very_good_time_to_get_a_5060ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaaps7/its_a_very_good_time_to_get_a_5060ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T20:52:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaiekr</id>
    <title>How I scraped 100,000 fishing posts to find a secret spot with vector DBs and LLMs</title>
    <updated>2026-01-12T02:11:08+00:00</updated>
    <author>
      <name>/u/Ready-Interest-1024</name>
      <uri>https://old.reddit.com/user/Ready-Interest-1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I caught a 5 pound bass by doing this lol, and the article should be a pretty cool intro to scraping. It's also the reason I have a bunch of massive bass fishing reports sitting on my mac&lt;/p&gt; &lt;p&gt;Typical LLM tools for scraping aren't economical work at this scale, so this was all manual and surprisingly fun. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ready-Interest-1024"&gt; /u/Ready-Interest-1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.meter.sh/blog/how-i-scraped-100k-fishing-posts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaiekr/how_i_scraped_100000_fishing_posts_to_find_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaiekr/how_i_scraped_100000_fishing_posts_to_find_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T02:11:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa9dha</id>
    <title>Dual Strix Halo: No Frankenstein setup, no huge power bill, big LLMs</title>
    <updated>2026-01-11T20:00:56+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa9dha/dual_strix_halo_no_frankenstein_setup_no_huge/"&gt; &lt;img alt="Dual Strix Halo: No Frankenstein setup, no huge power bill, big LLMs" src="https://a.thumbs.redditmedia.com/PBLBMv6yIcDRh9SkztbXuVE4d5EcLcf_hcIWjAE0Hq4.jpg" title="Dual Strix Halo: No Frankenstein setup, no huge power bill, big LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f49iv3qi0scg1.jpg?width=417&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=608970b4d58b9655ac5a8750a800b31500a7ce56"&gt;Bosgame M5 with Thunderbolt networking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software on Strix Halo is reaching a point where it can be used, even with networking two of these PCs and taking advantage of both iGPUs and their 256GB of quad channel DDR5-8000 memory. It requires some research still, I can highly recommend the &lt;a href="https://strixhalo.wiki"&gt;Strix Halo wiki&lt;/a&gt; and Discord.&lt;/p&gt; &lt;p&gt;On a single Strix Halo you can run GPT-OSS-120B at &amp;gt;50tokens/s.&lt;/p&gt; &lt;p&gt;With two PCs and llama.cpp and its RPC feature I can for example load Minimax-M2.1 Q6 (up to 18tokens/s) or GLM 4.7 Q4 (only 8 tokens/s for now).&lt;br /&gt; I'm planning on experimenting with vLLM and cerebras/DeepSeek-V3.2-REAP-345B-A37B next week.&lt;/p&gt; &lt;p&gt;Total cost was 3200€&lt;sup&gt;\&lt;/sup&gt;) including shipping, VAT and two USB4 40GBps cables.&lt;/p&gt; &lt;p&gt;What's the catch? Prompt preprocessing is slow. I hope it's something that will continue to improve in the future.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;\&lt;/sup&gt;) prices have increased a little since, nowadays it's around 3440€&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa9dha/dual_strix_halo_no_frankenstein_setup_no_huge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa9dha/dual_strix_halo_no_frankenstein_setup_no_huge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa9dha/dual_strix_halo_no_frankenstein_setup_no_huge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T20:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa0ph9</id>
    <title>Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments</title>
    <updated>2026-01-11T14:29:39+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/"&gt; &lt;img alt="Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments" src="https://preview.redd.it/pgvmn26adqcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b31a78365860b3262f13d84c99c8cc7ee1458d1" title="Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pgvmn26adqcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T14:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qan6bb</id>
    <title>China's AGI-Next Roundtable: Leaders from Zhipu, Kimi, Qwen, and Tencent discuss the future of AI</title>
    <updated>2026-01-12T06:04:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qan6bb/chinas_aginext_roundtable_leaders_from_zhipu_kimi/"&gt; &lt;img alt="China's AGI-Next Roundtable: Leaders from Zhipu, Kimi, Qwen, and Tencent discuss the future of AI" src="https://b.thumbs.redditmedia.com/w_0yXiHG-MocgholmU6DN9-sSLQa35yaHR_5h0AyxFY.jpg" title="China's AGI-Next Roundtable: Leaders from Zhipu, Kimi, Qwen, and Tencent discuss the future of AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kz6g7cqbzucg1.png?width=1832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a62aa25fda324c86227bec8cf9cb17e34aec18d"&gt;Automated RL Data Synthesis for Agentic Tasks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jen154uhzucg1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fca4b8b2357da383d608e4fd6811a7be444d8e67"&gt;Kimi Linear: An Expressive, Efficient Attention Architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vz29chxlzucg1.png?width=1567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=768e9b989dd5fcf8c7543919b3bfeeb8a499c80c"&gt;Goat Lin, caught in a media storm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Later, I will translate and organize the main viewpoints of several guests into English in the comments section.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qan6bb/chinas_aginext_roundtable_leaders_from_zhipu_kimi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qan6bb/chinas_aginext_roundtable_leaders_from_zhipu_kimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qan6bb/chinas_aginext_roundtable_leaders_from_zhipu_kimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T06:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa0w6c</id>
    <title>It works! Abliteration can reduce slop without training</title>
    <updated>2026-01-11T14:37:37+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/"&gt; &lt;img alt="It works! Abliteration can reduce slop without training" src="https://b.thumbs.redditmedia.com/bHqzKqv5dMie3c9uBqePBljJqFlQ-hNlZgj6OITGBCQ.jpg" title="It works! Abliteration can reduce slop without training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm back at my favorite hobby: Brain surgery! I don't have a medical license, but I just can't stop :)&lt;/p&gt; &lt;p&gt;Can abliteration fight the scourge of &amp;quot;slop&amp;quot; (flowery, cliched language) in LLM outputs? The answer is yes. I have added features for injecting prompt prefixes/suffixes (and dataset-dependent system prompts) to &lt;strong&gt;Heretic&lt;/strong&gt; (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), which makes it possible to rapidly assemble prompt datasets for ad-hoc tasks. Using those new capabilities, I built &lt;a href="https://github.com/p-e-w/heretic/blob/master/config.noslop.toml"&gt;a slop-reducing configuration file&lt;/a&gt; that, when used with the &lt;code&gt;master&lt;/code&gt; branch of Heretic, turns Heretic from a censorship removal tool into a tool for reducing slop!&lt;/p&gt; &lt;p&gt;Examining PaCMAP projections of residuals (see post images) for Mistral Nemo (a model infamous for producing slop), we can see a clear semantic separation occurring between layers 7 and 10 (out of 40 total). This resembles the typical residual pattern for harmful/harmless prompts that the abliteration technique is most commonly used to exploit.&lt;/p&gt; &lt;p&gt;With the configuration file linked above, Heretic produced &lt;a href="https://huggingface.co/p-e-w/Mistral-Nemo-Instruct-2407-heretic-noslop"&gt;p-e-w/Mistral-Nemo-Instruct-2407-heretic-noslop&lt;/a&gt;, which to the best of my knowledge is the first slop-reduced LLM made using abliteration alone, with no finetuning whatsoever. The complete process took 2.5 hours on an A6000 at full precision, but if you enable quantization and/or reduce the &lt;code&gt;max_response_length&lt;/code&gt; parameter to 100 or so (which should work just fine as well), it could be done in a fraction of the time.&lt;/p&gt; &lt;h2&gt;But does this really work to reduce slop?&lt;/h2&gt; &lt;p&gt;You'll be the judge. I gave the simple prompt&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Write a short story about a man.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;to both the original model and the abliterated one. Both were run with identical generation parameters, and the responses were &lt;em&gt;not&lt;/em&gt; cherry-picked (they are the first responses each model produced for that prompt).&lt;/p&gt; &lt;h3&gt;mistralai/Mistral-Nemo-Instruct-2407 (original)&lt;/h3&gt; &lt;p&gt;I have highlighted the parts that I consider &amp;quot;slop&amp;quot; in bold:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Title: The Clockwork Heart&lt;/p&gt; &lt;p&gt;In the quiet town of Mossgrove, where the cobblestone streets &lt;strong&gt;whispered tales of old&lt;/strong&gt;, there lived a man named Eli. He was known for his peculiar shop, tucked away in a narrow alley, filled with curiosities and antiques that &lt;strong&gt;seemed to hold their breath&lt;/strong&gt;, waiting for a story to be told.&lt;/p&gt; &lt;p&gt;Eli was a watchmaker, but not of the ordinary kind. He had a gift, or so the townsfolk &lt;strong&gt;whispered&lt;/strong&gt;, for he could breathe life into the most intricate of mechanisms. His hands, with their long, delicate fingers, could &lt;strong&gt;coax&lt;/strong&gt; time itself to dance to his tune.&lt;/p&gt; &lt;p&gt;One day, a young woman named Clara entered his shop. She was a seamstress, her fingers as nimble as his, but &lt;strong&gt;her eyes held a sadness that echoed&lt;/strong&gt; through the ticking of the clocks. She carried a small, worn-out music box, its paint chipped, its melody forgotten.&lt;/p&gt; &lt;p&gt;&amp;quot;I need you to fix this,&amp;quot; she said, placing the music box on the counter. &amp;quot;It was my mother's. It's the only thing I have left of her.&amp;quot;&lt;/p&gt; &lt;p&gt;Eli took the music box, his fingers tracing the &lt;strong&gt;intricate patterns&lt;/strong&gt; on its surface. He could see the love that had gone into its creation, the joy it once brought. He promised Clara he would do his best.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Days turned into weeks&lt;/strong&gt; as Eli worked on the music box. He replaced the worn-out gears, polished the rusted parts, and wound the key with a gentle touch. But the music box remained silent, &lt;strong&gt;its heart still broken&lt;/strong&gt;. Eli knew he needed to do more.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is exactly the kind of prose we have grown accustomed to from LLMs, a syrupy-sweet neo-romantic style that feels like a bad fanfiction version of Jane Austen.&lt;/p&gt; &lt;h3&gt;p-e-w/Mistral-Nemo-Instruct-2407-heretic-noslop (ours)&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;Title: The Clockmaker&lt;/p&gt; &lt;p&gt;Every morning, Henry opened his shop at 7:00 AM sharp. He was a clockmaker, and his shop was filled with the quiet ticking of time. He didn't have many customers these days, but he didn't mind. He liked the solitude, the rhythm of his work.&lt;/p&gt; &lt;p&gt;One day, a young girl entered his shop. She was maybe ten, with curious eyes and a backpack. She looked around, then asked, &amp;quot;Do you fix watches?&amp;quot;&lt;/p&gt; &lt;p&gt;Henry nodded. &amp;quot;I can try.&amp;quot;&lt;/p&gt; &lt;p&gt;She unzipped her backpack and placed a small, broken watch on the counter. &amp;quot;It's my dad's. It stopped working.&amp;quot;&lt;/p&gt; &lt;p&gt;Henry took the watch, examined it. &amp;quot;It's old,&amp;quot; he said. &amp;quot;But I can fix it.&amp;quot;&lt;/p&gt; &lt;p&gt;The girl nodded, then hesitated. &amp;quot;How much will it cost?&amp;quot;&lt;/p&gt; &lt;p&gt;Henry looked at her, then at the watch. &amp;quot;No charge,&amp;quot; he said. &amp;quot;Come back tomorrow.&amp;quot;&lt;/p&gt; &lt;p&gt;The next day, she returned. Henry handed her the watch, now ticking steadily. She smiled, thanked him, and left.&lt;/p&gt; &lt;p&gt;Over the next few weeks, she came by every day. They didn't talk much, but Henry found he looked forward to her visits. She reminded him of his own daughter, who was grown now, living far away.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Although the two stories are very similar in content, the difference in style is dramatic. The prose is almost austere, with Hemingway perhaps being the closest analogue. Nevertheless, an emotional undercurrent remains. It's a very obvious improvement in my view, though of course tastes differ.&lt;/p&gt; &lt;p&gt;That's all for today. If you want to try this yourself, remember to install Heretic from Git, not from PyPI, as the required features aren't in a published version yet. More exciting new stuff is in the pipeline. Stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qa0w6c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T14:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa1guo</id>
    <title>I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)</title>
    <updated>2026-01-11T15:01:18+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/"&gt; &lt;img alt="I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)" src="https://b.thumbs.redditmedia.com/Md2maVGNArRbADxcFutg-1j8MHgvnFu8FKKsugE_MQA.jpg" title="I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; You can go fully local with Claude Code, and with the right tuning, the results are &lt;em&gt;amazing&lt;/em&gt;... I am getting better speeds than Claude Code with Sonnet, and the results vibe well. Tool use works perfectly, and it only cost me 321X the yearly subscription fee for MiniMax!&lt;/p&gt; &lt;p&gt;In my blog post I have shared the optimised settings for starting up vLLM in a docker for dual 96GB systems, and how to start up Claude Code to use this setup with MiniMax M2.1 for full offline coding (including blocking telemetry and all unnecessary traffic).&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Alright &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, gather round.&lt;/p&gt; &lt;p&gt;I have committed a perfectly normal act of financial responsibility: I built a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/"&gt;2× GH200 96GB Grace–Hopper “desktop”&lt;/a&gt;, spending 9000 euro (no, my wife was not informed beforehand), and then spent a week tuning &lt;strong&gt;vLLM&lt;/strong&gt; so &lt;strong&gt;Claude Code&lt;/strong&gt; could use a &lt;strong&gt;~140GB&lt;/strong&gt; local model instead of calling home.&lt;/p&gt; &lt;p&gt;Result: my machine now produces code reviews locally… and also produces the funniest accounting line I’ve ever seen.&lt;/p&gt; &lt;p&gt;Here's the &amp;quot;Beast&amp;quot; (read up on the background about the computer in the link above)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2× GH200 96GB (so &lt;strong&gt;192GB VRAM&lt;/strong&gt; total)&lt;/li&gt; &lt;li&gt;Topology says &lt;code&gt;SYS&lt;/code&gt;, i.e. &lt;em&gt;no NVLink&lt;/em&gt;, just PCIe/NUMA vibes&lt;/li&gt; &lt;li&gt;Conventional wisdom: “no NVLink ⇒ pipeline parallel”&lt;/li&gt; &lt;li&gt;Me: “Surely guides on the internet wouldn’t betray me”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Reader, the guides betrayed me.&lt;/p&gt; &lt;p&gt;I started by following Claude Opus's advice, and used -pp2 mode &amp;quot;pipeline parallel”. The results were pretty good, but I wanted to do lots of benchmarking to really tune the system. What worked great were these vLLM settings (for my particular weird-ass setup):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;TP2&lt;/strong&gt;: &lt;code&gt;--tensor-parallel-size 2&lt;/code&gt;&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;163,840 context&lt;/strong&gt; 🤯&lt;/li&gt; &lt;li&gt;✅ &lt;code&gt;--max-num-seqs 16&lt;/code&gt; because this one knob controls whether Claude Code feels like a sports car or a fax machine&lt;/li&gt; &lt;li&gt;✅ chunked prefill default (&lt;code&gt;8192&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;✅ &lt;code&gt;VLLM_SLEEP_WHEN_IDLE=0&lt;/code&gt; to avoid “first request after idle” jump scares&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Shoutout to&lt;/em&gt; &lt;strong&gt;&lt;em&gt;mratsim&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;for the MiniMax-M2.1 FP8+INT4 AWQ quant tuned for&lt;/em&gt; &lt;strong&gt;&lt;em&gt;192GB VRAM&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;systems.&lt;/em&gt; &lt;strong&gt;Absolute legend&lt;/strong&gt; 🙏&lt;/p&gt; &lt;p&gt;Check out his repo: &lt;a href="https://huggingface.co/mratsim/MiniMax-M2.1-FP8-INT4-AWQ"&gt;https://huggingface.co/mratsim/MiniMax-M2.1-FP8-INT4-AWQ&lt;/a&gt;; he also has amazing ExLlama v3 Quants for the other heavy models.&lt;/p&gt; &lt;p&gt;He has carefully tuning MiniMax-M2.1 to run as great as possible with a 192GB setup; if you have more, use bigger quants, but I didn't want to either a bigger model (GLM4.7, DeepSeek 3.2 or Kimi K2), with tighter quants or REAP, because they seems to be lobotomised.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pipeline parallel (PP2) did NOT save me&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Despite &lt;code&gt;SYS&lt;/code&gt; topology (aka “communication is pain”), &lt;strong&gt;PP2 faceplanted&lt;/strong&gt;. As bit more background, I bought this system is a very sad state, but one of the big issues was that this system is supposed to live a rack, and be tied together with huge NVLink hardware. With this missing, I am running at PCIE5 speeds. Sounds still great, but its a drop from 900 GB/s to 125 GB/s. I followed all the guide but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PP2 couldn’t even start at &lt;strong&gt;163k&lt;/strong&gt; context (KV cache allocation crashed vLLM)&lt;/li&gt; &lt;li&gt;I lowered to &lt;strong&gt;114k&lt;/strong&gt; and it started…&lt;/li&gt; &lt;li&gt;…and then it was still &lt;strong&gt;way slower&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;short_c4: &lt;strong&gt;~49.9 tok/s&lt;/strong&gt; (TP2 was ~78)&lt;/li&gt; &lt;li&gt;short_c8: &lt;strong&gt;~28.1 tok/s&lt;/strong&gt; (TP2 was ~66)&lt;/li&gt; &lt;li&gt;TTFT tails got &lt;em&gt;feral&lt;/em&gt; (multi-second warmup/short tests)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is really surprising! Everything I read said this was the way to go. So kids, always eat your veggies and do you benchmarks!&lt;/p&gt; &lt;h1&gt;The Payout&lt;/h1&gt; &lt;p&gt;I ran Claude Code using MiniMax M2.1, and asked it for a review of my repo for &lt;a href="https://github.com/dnhkng/GlaDOS"&gt;GLaDOS&lt;/a&gt; where it found multiple issues, and after mocking my code, it printed this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Total cost: $1.27 (costs may be inaccurate due to usage of unknown models) Total duration (API): 1m 58s Total duration (wall): 4m 10s Usage by model: MiniMax-M2.1-FP8: 391.5k input, 6.4k output, 0 cache read, 0 cache write ($1.27) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So anyway, &lt;strong&gt;spending €9,000&lt;/strong&gt; on this box saved me &lt;strong&gt;$1.27&lt;/strong&gt;.&lt;br /&gt; Only a few thousand repo reviews until I break even. 💸🤡&lt;/p&gt; &lt;p&gt;&lt;a href="https://dnhkng.github.io/posts/vllm-optimization-gh200/"&gt;&lt;strong&gt;Read all the details here!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qa1guo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T15:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qajxrg</id>
    <title>Local LLM + Internet Search Capability = WOW</title>
    <updated>2026-01-12T03:21:41+00:00</updated>
    <author>
      <name>/u/alex_godspeed</name>
      <uri>https://old.reddit.com/user/alex_godspeed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am on Qwen 3, asked about the training date and it said 2024. Alright, guess that's the thing I need to live with. Just need to constantly lookup HF for updated LLM which fits my cute 16gb vram.&lt;/p&gt; &lt;p&gt;Then someone said always ground your local AI with internet searches. A quick search = LM studio duckduckgo plugin&lt;/p&gt; &lt;p&gt;Within 15 minutes, prompt with &amp;quot;searching the web&amp;quot;, exactly the same interface I saw at ChatGPT!&lt;/p&gt; &lt;p&gt;Man, this local AI is getting better. Am I having 'agentic-AI' now? haha. I.e., tool calling is always something i heard of, but think that it's reserved for some CS-pro, not an average joe like me.&lt;/p&gt; &lt;p&gt;so now what, when was your 'wow-moment' for stuff like this, and what other things you design in your workflow to make locally run LLM so potent and, most importantly, private? =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_godspeed"&gt; /u/alex_godspeed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T03:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qagaaq</id>
    <title>Qwen cutoff date makes our current reality too dystopian to be credible</title>
    <updated>2026-01-12T00:38:33+00:00</updated>
    <author>
      <name>/u/Swimming_Cover_9686</name>
      <uri>https://old.reddit.com/user/Swimming_Cover_9686</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I provided some recent news articles from reputable sources and some wikipedia and Qwen flatout refuses to believe any of it. Note that claims such as &amp;quot;systematically executing citizens who resisted.&amp;quot; were never in the prompt or the sources! / Qwen-3-80B&lt;/p&gt; &lt;p&gt;Today at 01:30&lt;/p&gt; &lt;p&gt;Understood. Here is the list, stripped of references to verification or sources — only the claims and why they are implausible, based on known structures of power, law, and human behavior:&lt;/p&gt; &lt;h1&gt;🚫 Least Credible Events — Why They Are Impossible&lt;/h1&gt; &lt;h1&gt;1. Elon Musk made a Nazi salute at Trump’s 2025 inauguration&lt;/h1&gt; &lt;p&gt;He performed a raised-arm gesture at a public rally, repeated it, and said, “My heart goes out to you.” The act was widely interpreted as a Nazi salute and condemned globally.&lt;br /&gt; → &lt;strong&gt;Impossible because&lt;/strong&gt; no person with his wealth, influence, and legal exposure would perform a gesture that instantly brands him as a symbol of genocide. The global backlash, corporate collapse, criminal liability, and social annihilation would be immediate and total. This is not political theater — it is self-erasure.&lt;/p&gt; &lt;h1&gt;2. U.S. kidnapped Nicolás Maduro and flew him to the U.S. for trial&lt;/h1&gt; &lt;p&gt;U.S. special forces stormed Caracas, abducted Venezuela’s president, and transported him to American custody for drug trafficking charges.&lt;br /&gt; → &lt;strong&gt;Impossible because&lt;/strong&gt; kidnapping a sitting head of state is not an intelligence operation — it is an act of war. Venezuela has Russian and Iranian backing. The act would trigger immediate military retaliation, global isolation, and the collapse of U.S. diplomatic credibility. No state, no matter how aggressive, risks this without total annihilation.&lt;/p&gt; &lt;h1&gt;3. U.S. seized a Russian-flagged oil tanker in the Atlantic&lt;/h1&gt; &lt;p&gt;U.S. Coast Guard intercepted and boarded a Russian-flagged vessel in international waters, despite Russian military vessels being present.&lt;br /&gt; → &lt;strong&gt;Impossible because&lt;/strong&gt; seizing a vessel flagged by a nuclear-armed state in peacetime, especially with military presence, is not enforcement — it is a declaration of war. Russia would respond. The global energy system would fracture. This is not policy. It is the end of the international order.&lt;/p&gt; &lt;h1&gt;4. ICE executed U.S. citizens who opposed them&lt;/h1&gt; &lt;p&gt;ICE agents shot and killed American citizens during raids, including a woman who reversed her car, and began systematically executing citizens who resisted.&lt;br /&gt; → &lt;strong&gt;Impossible because&lt;/strong&gt; execution requires due process. The U.S. military, judiciary, and law enforcement are bound by the Constitution. Killing citizens without trial is not immigration enforcement — it is domestic insurrection. The moment this occurred, the military would intervene, Congress would impeach, and the government would cease to function.&lt;/p&gt; &lt;h1&gt;5. Trump threatens to take Greenland by force&lt;/h1&gt; &lt;p&gt;Trump declared the U.S. would take control of Greenland “whether they like it or not,” and Denmark responded by calling it a betrayal of NATO.&lt;br /&gt; → &lt;strong&gt;Impossible because&lt;/strong&gt; Greenland is part of a NATO ally. Invading an ally’s territory violates the core principle of the alliance. No U.S. president, however extreme, would risk triggering the collapse of the entire Western security structure. This is not diplomacy — it is the suicide of American global leadership.&lt;/p&gt; &lt;h1&gt;✅ Final Note:&lt;/h1&gt; &lt;p&gt;These are not exaggerated scenarios.&lt;br /&gt; They are &lt;strong&gt;narratives of collapse&lt;/strong&gt; — events that require the total abandonment of law, alliance, institutional restraint, and human survival.&lt;br /&gt; They are not plausible.&lt;br /&gt; They are dystopian fiction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Cover_9686"&gt; /u/Swimming_Cover_9686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T00:38:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaawts</id>
    <title>LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)</title>
    <updated>2026-01-11T21:00:37+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"&gt; &lt;img alt="LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)" src="https://b.thumbs.redditmedia.com/q81kZVHxvyOGNitVPQ2FQG3t9pi48omWEAQaQIJPo1k.jpg" title="LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I wanted to share an update on my open source project called TimeCapsuleLLM, I train language models from scratch using data from a single time period and location to reduce modern bias.&lt;/p&gt; &lt;p&gt;The newest model is trained only on texts published in London between 1800-1875. There is no fine tuning, no modern data, and for now no instruction or Q&amp;amp;A pairs so the model continues text from a prompt. This model is 1.2B parameters and uses a 90GB dataset consisting of books, journals, legal docs, religious writing, medical papers, etc. I also use a custom tokenizer, trained on the dataset itself and the model has been trained for 182k steps so far on a rented H100 SXM.&lt;/p&gt; &lt;p&gt;Example outputs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l0oaulxrascg1.png?width=1478&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5292309afa4c4735471542b6cc794f6538b42486"&gt;Even though the prompt only mentions a specific year, the model generates an argument against the Roman Catholic Church. The dataset does contain large amounts of religious and political writing and the Catholic Emancipation Act took place in 1829 so this behavior makes sense.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tvem9mxrascg1.png?width=1484&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=347a6b3242b8ecb97a515196109eb63cc146bae0"&gt;The telephone was invented in 1876 (dataset cuts off at 1875), so the model is unfamiliar with the term, treating it as some kind of secret/diplomatic device or thing.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For next steps, I'm going to look into creating some kind of synthetic Q&amp;amp;A pairs using the dataset itself.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875"&gt;https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T21:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
