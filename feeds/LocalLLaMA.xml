<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-06T07:24:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1opphdc</id>
    <title>Mini PCs Recommendations</title>
    <updated>2025-11-06T04:29:27+00:00</updated>
    <author>
      <name>/u/ionlycreate42</name>
      <uri>https://old.reddit.com/user/ionlycreate42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking to run inference with a mini pc, sorta on the go in my car, and can bring it back home quickly whenever. Ideally something that can run 30b dense models, I’m still playing around with all this. But running quantized coding models around this level or VLMs ideally. Again I’m not an expert here so looking to expand on it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ionlycreate42"&gt; /u/ionlycreate42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opphdc/mini_pcs_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opphdc/mini_pcs_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opphdc/mini_pcs_recommendations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1opq7nt</id>
    <title>What is the safest gui / backend to run in work environment</title>
    <updated>2025-11-06T05:08:30+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My requirements are fairly simple - just a chat interface with history, image sending functionality, that's all.&lt;/p&gt; &lt;p&gt;I've already tinkered with Gradio and Ollama to create a basic UI, but I'm aiming for an improved experience this time around. &lt;/p&gt; &lt;p&gt;Most importantly though, I need this setup to be completely safe and appropriate for a work environment.&lt;/p&gt; &lt;p&gt;Ideally, I want gui similar to Copilot or ChatGPT.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opq7nt/what_is_the_safest_gui_backend_to_run_in_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opq7nt/what_is_the_safest_gui_backend_to_run_in_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opq7nt/what_is_the_safest_gui_backend_to_run_in_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T05:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1opo0fn</id>
    <title>The power of a decent computer for AI</title>
    <updated>2025-11-06T03:16:12+00:00</updated>
    <author>
      <name>/u/Appropriate_Fox5922</name>
      <uri>https://old.reddit.com/user/Appropriate_Fox5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Lately I’ve been diving deeper into AI, and honestly, I’ve realized that you don’t need a huge cloud setup or expensive subscriptions to start experimenting with tools like ollama and Hugging Face, I’ve been able to run models like llama 3, Mistral, Phi, and Qwen locally on my own computer and it’s been amazing. It’s not a high-end gaming rig or anything, just a decent machine with good RAM and a solid CPU/GPU.&lt;/p&gt; &lt;p&gt;Being able to test things offline, analyze my own data, and keep everything private has made me enjoy AI even more. It feels more personal and creative, like using your own lab instead of renting one.&lt;/p&gt; &lt;p&gt;I’m curious, do you think we’re getting closer to a point where local AI setups could rival the cloud for most devs? Or maybe even empower more people to become AI developers just by having access to better consumer hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate_Fox5922"&gt; /u/Appropriate_Fox5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opo0fn/the_power_of_a_decent_computer_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opo0fn/the_power_of_a_decent_computer_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opo0fn/the_power_of_a_decent_computer_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T03:16:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1op73qb</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-11-05T16:13:48+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/YzA4MDdtb3NxZ3pmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6be507f0bbf2370a00306c48b3deca6f3dbc2931" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p5a328wsqgzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T16:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1op2d1a</id>
    <title>I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!</title>
    <updated>2025-11-05T13:07:46+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"&gt; &lt;img alt="I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!" src="https://preview.redd.it/7xx856mftfzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7a93e1d29328d3af61a2f1b635a73c9abf0f570" title="I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on: a full, beginner-friendly tutorial for fine-tuning the &lt;strong&gt;Qwen2.5-Coder-1.5B&lt;/strong&gt; model for a real-world task (Chinese sentiment analysis).&lt;/p&gt; &lt;p&gt;The best part? &lt;strong&gt;You can run the entire thing on a free Google Colab T4 GPU in about 20-30 minutes.&lt;/strong&gt; No local setup needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FIIIIQIIII%2FMSJ-Factory"&gt;https://github.com/IIIIQIIII/MSJ-Factory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;▶️ Try it now on Google Colab:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2FIIIIQIIII%2FMSJ-Factory%2Fblob%2Fmain%2FQwen2_5_Sentiment_Fine_tuning_Tutorial.ipynb"&gt;https://colab.research.google.com/github/IIIIQIIII/MSJ-Factory/blob/main/Qwen2_5_Sentiment_Fine_tuning_Tutorial.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's inside:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;One-Click Colab Notebook:&lt;/strong&gt; The link above takes you straight there. Just open and run.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Freeze Training Method:&lt;/strong&gt; I only train the last 6 layers. It's super fast, uses ~9GB VRAM, and still gives amazing results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clear Results:&lt;/strong&gt; I was able to boost accuracy on the test set from &lt;strong&gt;91.6% to 97.8%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Walkthrough:&lt;/strong&gt; From cloning the repo, to training, evaluating, and even uploading your final model to Hugging Face, all within the notebook.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried to make this as easy as possible for anyone who wants to get their hands dirty with fine-tuning but might not have a beefy GPU at home. This method is great for my own quick experiments and for adapting models to new domains without needing an A100.&lt;/p&gt; &lt;p&gt;Hope you find it useful! Let me know if you have any feedback or questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7xx856mftfzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T13:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1opi34d</id>
    <title>Fine-tuning a chat model to mimic one person</title>
    <updated>2025-11-05T22:54:59+00:00</updated>
    <author>
      <name>/u/KnightKingPow</name>
      <uri>https://old.reddit.com/user/KnightKingPow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, beginner here with some experience running StableDiffusion/WAN models in ComfyUI and LM Studio. I would really appreciate some guidance.&lt;/p&gt; &lt;p&gt;I have several text chat conversations between two people (sometimes three). I would like to fine-tune a model so it learns the writing style, tone, and personality of &lt;strong&gt;only one&lt;/strong&gt; of the participants, so that I can later chat with the model &lt;em&gt;as if I’m talking to that person&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The model should ignore or not learn from the other speaker(s).&lt;/p&gt; &lt;p&gt;The language is not English, but I suppose that's not a problem, right?&lt;/p&gt; &lt;p&gt;I have these:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MacBook M3 Max, 64 GB RAM&lt;/li&gt; &lt;li&gt;Windows PC with an RTX 4090 (24 GB VRAM) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I could train on both but ideally I'd like to run the final model locally on the Mac with LM Studio.&lt;/p&gt; &lt;p&gt;What base model would be best for this setup and use case?&lt;/p&gt; &lt;p&gt;What are the full beginner-friendly steps from dataset prep → fine-tuning → exporting/quantizing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KnightKingPow"&gt; /u/KnightKingPow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opi34d/finetuning_a_chat_model_to_mimic_one_person/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opi34d/finetuning_a_chat_model_to_mimic_one_person/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opi34d/finetuning_a_chat_model_to_mimic_one_person/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T22:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oozb8v</id>
    <title>aquif-3.5-Max-42B-A3B</title>
    <updated>2025-11-05T10:27:09+00:00</updated>
    <author>
      <name>/u/CoruNethronX</name>
      <uri>https://old.reddit.com/user/CoruNethronX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"&gt; &lt;img alt="aquif-3.5-Max-42B-A3B" src="https://external-preview.redd.it/iQbBlhyprDj6yO7lC6_VBVFWSoqEqFZE8HYk4JH2vHI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf206a8e60f2f9eb24b0998337979fd6ea8d653c" title="aquif-3.5-Max-42B-A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beats GLM 4.6 according to provided benchmarks Million context Apache 2.0 Works both with GGUF/llama.cpp and MLX/lmstudio out-of-box, as it's qwen3_moe architecture&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoruNethronX"&gt; /u/CoruNethronX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/aquif-ai/aquif-3.5-Max-42B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T10:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1opr30y</id>
    <title>SmallWebRTC - Voice Agent on Slow Airplane WiFi - Why Not?</title>
    <updated>2025-11-06T05:56:40+00:00</updated>
    <author>
      <name>/u/Cipher_Lock_20</name>
      <uri>https://old.reddit.com/user/Cipher_Lock_20</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opr30y/smallwebrtc_voice_agent_on_slow_airplane_wifi_why/"&gt; &lt;img alt="SmallWebRTC - Voice Agent on Slow Airplane WiFi - Why Not?" src="https://external-preview.redd.it/bmx0amJ0Z2x0a3pmMaE648JfkwT7QQVAY7_dHmyflf7GbyQgdh_4RA0EmkB7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50134172169c9119509a6b5cc7d74ecaa58627a0" title="SmallWebRTC - Voice Agent on Slow Airplane WiFi - Why Not?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pipecat recently released their open source SmallWebRTC transport allowing connections directly to your voice agent without any extra servers or infrastructure. The model Im using is Gemini Live for simplicity, but Pipecat is king for creating integrations with all providers and open source models easily.&lt;/p&gt; &lt;p&gt;I decided to see if it would work on the crappy airplane WiFi on my flight home tonight. It worked great and didn’t have to deploy any servers or send my media through an extra SFU or MCU somewhere. &lt;/p&gt; &lt;p&gt;Disclaimers The app makes no sense and is simply to demo the simplicity of a SmallWebRTC connection on slow airplane WiFi. &lt;/p&gt; &lt;p&gt;I didn’t want to sit on a plane talking out loud to a voice agent which is why I’m piping the browser ready back in as an input. I had my headphones on and just used text -&amp;gt; browser reader as voice input to test. &lt;/p&gt; &lt;p&gt;You can deploy their normal template easily if you want to try with different models&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.pipecat.ai/server/services/transport/small-webrtc"&gt;https://docs.pipecat.ai/server/services/transport/small-webrtc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cipher_Lock_20"&gt; /u/Cipher_Lock_20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rocydjkltkzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opr30y/smallwebrtc_voice_agent_on_slow_airplane_wifi_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opr30y/smallwebrtc_voice_agent_on_slow_airplane_wifi_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T05:56:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oosnaq</id>
    <title>New Qwen models are unbearable</title>
    <updated>2025-11-05T03:45:53+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using GPT-OSS-120B for the last couple months and recently thought I'd try Qwen3 32b VL and Qwen3 Next 80B. &lt;/p&gt; &lt;p&gt;They honestly might be worse than peak ChatGPT 4o. &lt;/p&gt; &lt;p&gt;Calling me a genius, telling me every idea of mine is brilliant, &amp;quot;this isnt just a great idea—you're redefining what it means to be a software developer&amp;quot; type shit&lt;/p&gt; &lt;p&gt;I cant use these models because I cant trust them at all. They just agree with literally everything I say. &lt;/p&gt; &lt;p&gt;Has anyone found a way to make these models more usable? They have good benchmark scores so perhaps im not using them correctly&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T03:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oppdxi</id>
    <title>Llama.cpp vs Ollama - Same model, parameters and system prompts but VASTLY different experiences</title>
    <updated>2025-11-06T04:24:25+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm slowly seeing the light on Llama.cpp now that I understand how Llama-swap works. I've got the new Qwen3-VL models working good. &lt;/p&gt; &lt;p&gt;However, GPT-OSS:20B is the default model that the family uses before deciding if they need to branch off out to bigger models or specialized models.&lt;/p&gt; &lt;p&gt;However, 20B on Ollama works about 90-95% of the time the way I want. MCP tools work, it searches the internet when it needs to with my MCP Websearch pipeline thru n8n. &lt;/p&gt; &lt;p&gt;20B in Llama.cpp though is VASTLY inconsistent other than when it's consistently non-sensical . I've got my Temp at 1.0, repeat penalty on 1.1 , top K at 0 and top p at 1.0, just like the Unsloth guide. It makes things up more frequently, ignores the system prompt and what the rules for tool usage are and sometimes the /think tokens spill over into the normal responses.&lt;/p&gt; &lt;p&gt;WTF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1op0j6j</id>
    <title>Recent VRAM Poll results</title>
    <updated>2025-11-05T11:38:38+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"&gt; &lt;img alt="Recent VRAM Poll results" src="https://preview.redd.it/i3y27zfpbfzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0823c607489b2850e90a3ca955b51e1f4428ecdf" title="Recent VRAM Poll results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1olildc/comment/nmi8ftm/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;As mentioned in that post&lt;/a&gt;, That poll missed below ranges.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;9-11GB&lt;/li&gt; &lt;li&gt;25-31GB&lt;/li&gt; &lt;li&gt;97-127GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Poll Results below:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0-8GB - &lt;strong&gt;718&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;12-24GB - &lt;strong&gt;1.1K&lt;/strong&gt; - I think some 10GB folks might have picked this option so this range came with big number.&lt;/li&gt; &lt;li&gt;32-48GB - &lt;strong&gt;348&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;48-96GB - &lt;strong&gt;284&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;128-256GB - &lt;strong&gt;138&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;256+ - &lt;strong&gt;93&lt;/strong&gt; - &lt;sup&gt;Last month someone asked me &amp;quot;Why are you calling yourself GPU Poor when you have 8GB VRAM&amp;quot;&lt;/sup&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next time onwards below ranges would be better to get better results as it covers all ranges. And this would be more useful for Model creators &amp;amp; Finetuners to pick better model sizes/types(MOE or Dense).&lt;/p&gt; &lt;p&gt;&lt;sup&gt;FYI Poll has only 6 options, otherwise I would add more ranges.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~12GB&lt;/li&gt; &lt;li&gt;13-32GB&lt;/li&gt; &lt;li&gt;33-64GB&lt;/li&gt; &lt;li&gt;65-96GB&lt;/li&gt; &lt;li&gt;97-128GB&lt;/li&gt; &lt;li&gt;128GB+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;RAM&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~32GB&lt;/li&gt; &lt;li&gt;33-64GB&lt;/li&gt; &lt;li&gt;65-128GB&lt;/li&gt; &lt;li&gt;129-256GB&lt;/li&gt; &lt;li&gt;257-512GB&lt;/li&gt; &lt;li&gt;513-1TB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Somebody please post above poll threads coming week. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i3y27zfpbfzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T11:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooxple</id>
    <title>GLM 4.6 AIR is coming....?</title>
    <updated>2025-11-05T08:43:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt; &lt;img alt="GLM 4.6 AIR is coming....?" src="https://preview.redd.it/56uu0u1fiezf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4748db1df48b7ce94e935ab40a291000e001166f" title="GLM 4.6 AIR is coming....?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;or not yet? What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56uu0u1fiezf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T08:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1opabzi</id>
    <title>Instead of predicting one token at a time, CALM (Continuous Autoregressive Language Models) predicts continuous vectors that represent multiple tokens at once</title>
    <updated>2025-11-05T18:08:24+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuous Autoregressive Language Models (CALM) replace the traditional token-by-token generation of language models with a continuous next-vector prediction approach, where an autoencoder compresses chunks of multiple tokens into single continuous vectors that can be reconstructed with over 99.9% accuracy. This drastically reduces the number of generative steps and thus the computational cost. Because probabilities over continuous spaces can’t be computed via softmax, CALM introduces a likelihood-free framework for training, evaluation (using the new BrierLM metric), and temperature-based sampling. The result is a paradigm that significantly improves efficiency—achieving comparable performance to strong discrete LLMs while operating far faster—establishing next-vector prediction as a powerful new direction for scalable, ultra-efficient language modeling.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.27688"&gt;https://arxiv.org/abs/2510.27688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opabzi/instead_of_predicting_one_token_at_a_time_calm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opabzi/instead_of_predicting_one_token_at_a_time_calm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opabzi/instead_of_predicting_one_token_at_a_time_calm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T18:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oppykf</id>
    <title>Is OCR accuracy actually a blocker for anyone's RAG/automation pipelines?</title>
    <updated>2025-11-06T04:55:12+00:00</updated>
    <author>
      <name>/u/Individual-Library-1</name>
      <uri>https://old.reddit.com/user/Individual-Library-1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Genuine question for the group -&lt;/p&gt; &lt;p&gt;I've been building document automation systems (litigation, compliance, NGO tools) and keep running into the same issue: &lt;strong&gt;OCR accuracy becomes the bottleneck that caps your entire system's reliability.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Specifically with complex documents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Financial reports with tables + charts + multi-column text&lt;/li&gt; &lt;li&gt;Legal documents with footnotes, schedules, exhibits&lt;/li&gt; &lt;li&gt;Technical manuals with diagrams embedded in text&lt;/li&gt; &lt;li&gt;Scanned forms where structure matters (not just text extraction)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've tried Google Vision, Azure Document Intelligence, Mistral APIs - they're good, but when you're building production systems where 95% accuracy means 1 in 20 documents has errors, that's not good enough. Especially when the errors are in the critical parts (tables, structured data).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt; Is this actually a problem for your workflows?&lt;/p&gt; &lt;p&gt;Or is &amp;quot;good enough&amp;quot; OCR + error handling downstream actually fine, and I'm overthinking this?&lt;/p&gt; &lt;p&gt;I'm trying to understand if OCR quality is a real bottleneck for people building with n8n/LangChain/LlamaIndex, or if it's just my specific use case.&lt;/p&gt; &lt;p&gt;For context: I ended up fine-tuning Qwen3-VL on document OCR and it's working better for complex layouts. Thinking about opening up an API for testing if people actually need this. But want to understand the problem first before I waste time building infrastructure nobody needs.&lt;/p&gt; &lt;p&gt;Appreciate any thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Library-1"&gt; /u/Individual-Library-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppykf/is_ocr_accuracy_actually_a_blocker_for_anyones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppykf/is_ocr_accuracy_actually_a_blocker_for_anyones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oppykf/is_ocr_accuracy_actually_a_blocker_for_anyones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oprpxq</id>
    <title>What is the point of Nvidia's Jet-Nemotron-2B?</title>
    <updated>2025-11-06T06:34:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In their paper, they claiming 10x faster tokens per sec than its parent model Qwen2.5-1.5B. But in my own test using huggingface transformers, this is not the case.&lt;/p&gt; &lt;p&gt;My setup:&lt;br /&gt; RTX 3050 6GB&lt;br /&gt; transformers 4.53.0 context length=1536&lt;br /&gt; temperature=0.1&lt;br /&gt; top_p=0.8&lt;br /&gt; repetitive_penalty=1.25&lt;br /&gt; system: You are a European History Professor named Professor Whitman.&lt;br /&gt; prompt: Why was Duke Vladivoj enfeoffed Duchy of Bohemia with the Holy Roman Empire in 1002? Does that mean Duchy of Bohemia was part of the Holy Roman Empire already? If so, when did the Holy Roman Empire acquired Bohemia?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;tokens&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-1b-it&lt;/td&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B&lt;/td&gt; &lt;td align="left"&gt;1433&lt;/td&gt; &lt;td align="left"&gt;5.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;771&lt;/td&gt; &lt;td align="left"&gt;5.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Jet-Nemotron-2B&lt;/td&gt; &lt;td align="left"&gt;312&lt;/td&gt; &lt;td align="left"&gt;3.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-1.5B&lt;/td&gt; &lt;td align="left"&gt;226&lt;/td&gt; &lt;td align="left"&gt;6.22&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Surprisingly, gemma-3-1b-it seems very good for its size and tried to role play to the system prompt. However, it seems to be quite slow. Qwen2.5-1.5B is useless as it generates Chinese when asked an English question. Qwen3 runs fast but it is very verbose in thinking mode. Turning off thinking seems to give better answer for historical questions.&lt;/p&gt; &lt;p&gt;Jet-Nemotron 2B is slower than Qwen3-1.7B and the reply is not as good. So what is the point? I can only see the theoretical KV cache saving here.&lt;/p&gt; &lt;p&gt;Replies from LLMs are detailed in the replies in this thread.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprpxq/what_is_the_point_of_nvidias_jetnemotron2b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprpxq/what_is_the_point_of_nvidias_jetnemotron2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oprpxq/what_is_the_point_of_nvidias_jetnemotron2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T06:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1opl54d</id>
    <title>What are you doing with your 128GB Mac?</title>
    <updated>2025-11-06T01:04:17+00:00</updated>
    <author>
      <name>/u/Technical_Pass_1858</name>
      <uri>https://old.reddit.com/user/Technical_Pass_1858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a MacBook Pro M3Max 128GB,I think I do not use it effectively.&lt;/p&gt; &lt;p&gt;So I wander what are you doing with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Pass_1858"&gt; /u/Technical_Pass_1858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl54d/what_are_you_doing_with_your_128gb_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl54d/what_are_you_doing_with_your_128gb_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opl54d/what_are_you_doing_with_your_128gb_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T01:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oplwcv</id>
    <title>You can now Fine-tune DeepSeek-OCR locally!</title>
    <updated>2025-11-06T01:39:01+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oplwcv/you_can_now_finetune_deepseekocr_locally/"&gt; &lt;img alt="You can now Fine-tune DeepSeek-OCR locally!" src="https://preview.redd.it/q1dmxwy4h9zf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9cac1e07107ec19f184e22c98ba69c518cbd1fa5" title="You can now Fine-tune DeepSeek-OCR locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q1dmxwy4h9zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oplwcv/you_can_now_finetune_deepseekocr_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oplwcv/you_can_now_finetune_deepseekocr_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T01:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1opl1j0</id>
    <title>AMD to launch gaming-oriented Ryzen AI MAX+ 388 &amp; 392 "Strix Halo" APUs with full Radeon 8060S graphics - VideoCardz.com</title>
    <updated>2025-11-06T00:59:59+00:00</updated>
    <author>
      <name>/u/evil0sheep</name>
      <uri>https://old.reddit.com/user/evil0sheep</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl1j0/amd_to_launch_gamingoriented_ryzen_ai_max_388_392/"&gt; &lt;img alt="AMD to launch gaming-oriented Ryzen AI MAX+ 388 &amp;amp; 392 &amp;quot;Strix Halo&amp;quot; APUs with full Radeon 8060S graphics - VideoCardz.com" src="https://external-preview.redd.it/WU5-XQvK1clH2V4Ad2EQWo3oFOrLJrxUzGAOS1faPQA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=874d8a08b9396a7dc93e7cc0e412e2285f20e269" title="AMD to launch gaming-oriented Ryzen AI MAX+ 388 &amp;amp; 392 &amp;quot;Strix Halo&amp;quot; APUs with full Radeon 8060S graphics - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like the same GPU and memory interface but 8 CPU cores instead of 16 so maybe a bit cheaper&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evil0sheep"&gt; /u/evil0sheep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-to-launch-gaming-oriented-ryzen-ai-max-388-strix-halo-apu-with-full-radeon-8060s-graphics"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl1j0/amd_to_launch_gamingoriented_ryzen_ai_max_388_392/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opl1j0/amd_to_launch_gamingoriented_ryzen_ai_max_388_392/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T00:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1opopxh</id>
    <title>Maya1 : 1st AI TTS model with Voice Design Feature on the fly</title>
    <updated>2025-11-06T03:50:50+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So Maya-research has released Maya1, a low latency TTS model where you can design the voice also given the description (like female, mid 30s, author, a little aggressive). The model uses Llama backbone and has 3B params.&lt;/p&gt; &lt;p&gt;Hugging Face : &lt;a href="https://huggingface.co/maya-research/maya1"&gt;https://huggingface.co/maya-research/maya1&lt;/a&gt; Demo : &lt;a href="https://youtu.be/69voVwdcVYg?si=wx1zM0CXU-DWbKwb"&gt;https://youtu.be/69voVwdcVYg?si=wx1zM0CXU-DWbKwb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opopxh/maya1_1st_ai_tts_model_with_voice_design_feature/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opopxh/maya1_1st_ai_tts_model_with_voice_design_feature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opopxh/maya1_1st_ai_tts_model_with_voice_design_feature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T03:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1opo5k8</id>
    <title>Explanation of Gated DeltaNet (Qwen3-Next and Kimi Linear)</title>
    <updated>2025-11-06T03:23:03+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opo5k8/explanation_of_gated_deltanet_qwen3next_and_kimi/"&gt; &lt;img alt="Explanation of Gated DeltaNet (Qwen3-Next and Kimi Linear)" src="https://external-preview.redd.it/docq3T1hMcX3nsU3Xo2w0YwpdMWU2MziLAnrwt_wi_s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=675bb873c375977b90591f1eec84b3474fb6b564" title="Explanation of Gated DeltaNet (Qwen3-Next and Kimi Linear)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/llms-from-scratch/ch04/08_deltanet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opo5k8/explanation_of_gated_deltanet_qwen3next_and_kimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opo5k8/explanation_of_gated_deltanet_qwen3next_and_kimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T03:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1opp94f</id>
    <title>Where are my 5060ti brothers at.</title>
    <updated>2025-11-06T04:17:24+00:00</updated>
    <author>
      <name>/u/do_u_think_im_spooky</name>
      <uri>https://old.reddit.com/user/do_u_think_im_spooky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opp94f/where_are_my_5060ti_brothers_at/"&gt; &lt;img alt="Where are my 5060ti brothers at." src="https://preview.redd.it/los33pewbkzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be5dff1428a671994dc9cfd688873fadb4454cd" title="Where are my 5060ti brothers at." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured I'd take part in sharing my local AI setup.&lt;/p&gt; &lt;p&gt;Dell Precision T7810 Dual Xeon E5 2680 v4 28c 56t 128GB DDR4 2400MHz Dual RTX 5060 ti 16GB&lt;/p&gt; &lt;p&gt;Originally purchased the Dell before getting into LLMs for homelab services but in the past few months I've dipped my toes into the local AI rabbit hole and it keeps getting deeper...&lt;/p&gt; &lt;p&gt;Running proxmox as the hypervisor and have dedicated containers for my inference engine and chat interface. I started with ollama but now I'm using llama.cpp with llama-swap for easy model swapping. Using openwebui because I'm yet to find something that's better and worth switching to.&lt;/p&gt; &lt;p&gt;What are your use cases or projects you utilize your local AI for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/do_u_think_im_spooky"&gt; /u/do_u_think_im_spooky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/los33pewbkzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opp94f/where_are_my_5060ti_brothers_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opp94f/where_are_my_5060ti_brothers_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1opeu1w</id>
    <title>Visualizing Quantization Types</title>
    <updated>2025-11-05T20:52:02+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt; &lt;img alt="Visualizing Quantization Types" src="https://preview.redd.it/brkkf7fs2izf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=69bbd6b8af4c7420aed83b9b70eddb5a51a78d26" title="Visualizing Quantization Types" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen some releases of MXFP4 quantized models recently and don't understand why given mxfp4 is kind of like a slightly smaller lower quality q4_0.&lt;/p&gt; &lt;p&gt;So unless the original model was post-trained specifically for MXFP4 like gpt-oss-120b or you yourself did some kind of QAT (quantization aware fine-tuning) targeting specifically mxfp4, then personally I'd go with good old q4_0 or ik's newer iq4_kss.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mxfp4 4.25bpw&lt;/li&gt; &lt;li&gt;q4_0 4.5bpw&lt;/li&gt; &lt;li&gt;iq4_kss 4.0bpw&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used the llama.cpp gguf python package to read a uint8 .bmp image, convert it to float16 numpy 2d array, and save that as a .gguf. Then I quantized the gguf to various types using ik_llama.cpp, and then finally re-quantize that back to f16 and save the resulting uint8 .bmp image.&lt;/p&gt; &lt;p&gt;Its kinda neat to visualize the effects of block sizes looking at image data. To me the mxfp4 looks &amp;quot;worse&amp;quot; than the q4_0 and the iq4_kss.&lt;/p&gt; &lt;p&gt;I haven't done perplexity/KLD measurements to directly compare mxfp4, but iq4_kss tends to be one of the best available in that size range in my previous quant release testing.&lt;/p&gt; &lt;p&gt;Finally, it is confusing to me, but nvfp4 is yet &lt;em&gt;a different&lt;/em&gt; quantization type with specific blackwell hardware support which I haven't tried yet myself.&lt;/p&gt; &lt;p&gt;Anyway, in my opinion mxfp4 isn't particularly special or better despite being somewhat newer. What do y'all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/brkkf7fs2izf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T20:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oprsln</id>
    <title>What is your take on this?</title>
    <updated>2025-11-06T06:38:31+00:00</updated>
    <author>
      <name>/u/ya_Priya</name>
      <uri>https://old.reddit.com/user/ya_Priya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt; &lt;img alt="What is your take on this?" src="https://external-preview.redd.it/enAybXNsNngwbHpmMSG2HwlQpQ6Hj-82EDUoyhNg7YK-n8qL0itnzTKon9hZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc87934b9d0d8de087571ccc6f9351740b8dac" title="What is your take on this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: Mobile Hacker on twitter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ya_Priya"&gt; /u/ya_Priya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zp20kj6x0lzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T06:38:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oph7jd</id>
    <title>Unified memory is the future, not GPU for local A.I.</title>
    <updated>2025-11-05T22:20:52+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As model sizes are trending bigger, even the best open weight models hover around half a terabyte, we are not going to be able to run those on GPU, yes on unified memory. Gemini-3 is rumored to be 1.2 trillion parameters:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/"&gt;https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So Apple and Strix Halo are on the right track. Intel where art thou? Any one else we can count on to eventually catch the trend? Medusa halo is going to be awesome:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/shorts/yAcONx3Jxf8"&gt;https://www.youtube.com/shorts/yAcONx3Jxf8&lt;/a&gt; . Quote: Medusa Halo is going to destroy strix halo.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3"&gt;https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Even longer term 5 years, I'm thinking in memory compute will take over versus current standard of von neumann architecture. Once we crack in memory compute nut then things will get very interesting. Will allow a greater level of parallelization. Every neuron can fire simultaneously like our human brain. In memory compute will dominate for future architectures in 10 years versus von neumann.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T22:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1opa6os</id>
    <title>Local Setup</title>
    <updated>2025-11-05T18:03:19+00:00</updated>
    <author>
      <name>/u/mattate</name>
      <uri>https://old.reddit.com/user/mattate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt; &lt;img alt="Local Setup" src="https://preview.redd.it/8imhi4icahzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabf7d36f6208d91a8e908e97d3d1a1b1ee6998f" title="Local Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey just figured I would share our local setup. I started building these machines as an experiment to see if I could drop our cost, and so far it has worked out pretty good. The first one was over a year ago, lots of lessons learned getting them up and stable. &lt;/p&gt; &lt;p&gt;The cost of AI APIs has come down drastically, when we started with these machines there was absolutely no competition. It's still cheaper to run your own hardware, but it's much much closer now. This community really I think is providing crazy value allowing company's like mine to experiment and roll things into production without having to drop hundreds of thousands of dollars literally on propritary AI API usage.&lt;/p&gt; &lt;p&gt;Running a mix of used 3090s, new 4090s, 5090s, and RTX 6000 pro's. The 3090 is certainly the king off cost per token without a doubt, but the problems with buying used gpus is not really worth the hassle of you're relying on these machines to get work done. &lt;/p&gt; &lt;p&gt;We process anywhere between 70m and 120m tokens per day, we could probably do more. &lt;/p&gt; &lt;p&gt;Some notes:&lt;/p&gt; &lt;p&gt;ASUS motherboards work well and are pretty stable, running ASUS Pro WS WRX80E-SAGE SE with threadripper gets up to 7 gpus, but usually pair gpus so 6 is the useful max. Will upgrade to the 90 in future machines. &lt;/p&gt; &lt;p&gt;240v power works much better then 120v, this is more about effciency of the power supplies. &lt;/p&gt; &lt;p&gt;Cooling is a huge problem, any more machines them I have now and cooling will become a very significant issue.&lt;/p&gt; &lt;p&gt;We run predominantly vllm these days, mixture of different models as new ones get released. &lt;/p&gt; &lt;p&gt;Happy to answer any other questions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattate"&gt; /u/mattate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8imhi4icahzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T18:03:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
