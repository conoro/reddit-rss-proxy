<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-14T21:07:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qcsr6h</id>
    <title>Train LoRA over GGUF</title>
    <updated>2026-01-14T17:02:28+00:00</updated>
    <author>
      <name>/u/woct0rdho</name>
      <uri>https://old.reddit.com/user/woct0rdho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've made a proof of concept that we can train LoRA over GGUF rather than bnb 4-bit quantized base model. When using 3-bit rather than 4-bit base model, we can train Qwen-30B-A3B with 16 rather than 24 GB VRAM.&lt;/p&gt; &lt;p&gt;For convenience I'm developing it in my repo &lt;a href="https://github.com/woct0rdho/transformers-qwen3-moe-fused#lora-over-gguf"&gt;https://github.com/woct0rdho/transformers-qwen3-moe-fused#lora-over-gguf&lt;/a&gt; , but it also works with many models that are not Qwen and not MoE.&lt;/p&gt; &lt;p&gt;For now it surely has a lot of rough edges, and we need more experiments to check the quality of such LoRA and optimize the training speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woct0rdho"&gt; /u/woct0rdho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsr6h/train_lora_over_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsr6h/train_lora_over_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsr6h/train_lora_over_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:02:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qct4we</id>
    <title>VectorDBZ update: Pinecone, pgvector, custom embeddings, search stats</title>
    <updated>2026-01-14T17:16:32+00:00</updated>
    <author>
      <name>/u/snirjka</name>
      <uri>https://old.reddit.com/user/snirjka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Hey everyone,&lt;/p&gt; &lt;p&gt;A while ago I shared &lt;strong&gt;VectorDBZ, a desktop GUI for vector databases&lt;/strong&gt;, and the feedback from this community was incredibly useful. Thanks again! üôè&lt;/p&gt; &lt;p&gt;Since then, I‚Äôve added:&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Pinecone&lt;/strong&gt; and &lt;strong&gt;pgvector&lt;/strong&gt; support&lt;br /&gt; ‚Ä¢ Search statistics for queries&lt;br /&gt; ‚Ä¢ Custom embedding functions directly in the search tab&lt;/p&gt; &lt;p&gt;Your earlier feedback helped shape a clear roadmap, and the app feels much more capable now.&lt;/p&gt; &lt;p&gt;I‚Äôd love more ideas and feedback:&lt;br /&gt; ‚Ä¢ What other databases or features would make this essential for your workflows?&lt;br /&gt; ‚Ä¢ Any UI/UX improvements for search or embeddings you‚Äôd suggest?&lt;br /&gt; ‚Ä¢ Is sparse vector worth implementing, and how have you used it?&lt;br /&gt; ‚Ä¢ If you do hybrid search with BM25, check the current search flow and tell me how you‚Äôd implement it UI-wise, since I feel like I might be overthinking it.&lt;br /&gt; ‚Ä¢ Other analytics or visualizations that would be useful?&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; GitHub: &lt;a href="https://github.com/vectordbz/vectordbz?utm_source=chatgpt.com"&gt;https://github.com/vectordbz/vectordbz&lt;/a&gt;&lt;br /&gt; Downloads: &lt;a href="https://github.com/vectordbz/vectordbz/releases"&gt;https://github.com/vectordbz/vectordbz/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you find this useful, a ‚≠ê on GitHub would mean a lot and helps me keep building.&lt;/p&gt; &lt;p&gt;Thanks again for all your input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/snirjka"&gt; /u/snirjka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct4we/vectordbz_update_pinecone_pgvector_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct4we/vectordbz_update_pinecone_pgvector_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qct4we/vectordbz_update_pinecone_pgvector_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcid3l</id>
    <title>Pocket TTS: a 100M-parameter text-to-speech</title>
    <updated>2026-01-14T08:51:14+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"&gt; &lt;img alt="Pocket TTS: a 100M-parameter text-to-speech" src="https://external-preview.redd.it/-wU8cKM1ybBFD4hDGC_AsWfo00bhoyCdexKDfL5kTEQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3304eb1bb7dc20cd79911028958854a3039569d9" title="Pocket TTS: a 100M-parameter text-to-speech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T08:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcm8ds</id>
    <title>"Agent Skills" - The spec unified us. The paths divided us.</title>
    <updated>2026-01-14T12:39:50+00:00</updated>
    <author>
      <name>/u/phoneixAdi</name>
      <uri>https://old.reddit.com/user/phoneixAdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcm8ds/agent_skills_the_spec_unified_us_the_paths/"&gt; &lt;img alt="&amp;quot;Agent Skills&amp;quot; - The spec unified us. The paths divided us." src="https://preview.redd.it/fe2fdwzb8bdg1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce4f4e91626483e0ed89ada9969d0385c9539a83" title="&amp;quot;Agent Skills&amp;quot; - The spec unified us. The paths divided us." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Skills are standardized now. But.....&lt;/p&gt; &lt;p&gt;.github/skills/&lt;/p&gt; &lt;p&gt;.claude/skills/&lt;/p&gt; &lt;p&gt;.codex/skills/&lt;/p&gt; &lt;p&gt;.copilot/skills/&lt;/p&gt; &lt;p&gt;Write once, store‚Ä¶ wherever your agent feels like.&lt;/p&gt; &lt;p&gt;Wish we just also agreed on standardized discovery path for skills (like agents.md). &lt;/p&gt; &lt;p&gt;So Agents Skills are truly interoperable when I am jumping between agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoneixAdi"&gt; /u/phoneixAdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fe2fdwzb8bdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcm8ds/agent_skills_the_spec_unified_us_the_paths/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcm8ds/agent_skills_the_spec_unified_us_the_paths/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T12:39:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc9sw2</id>
    <title>Introducing GLM-Image</title>
    <updated>2026-01-14T01:25:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"&gt; &lt;img alt="Introducing GLM-Image" src="https://preview.redd.it/70ypvyc5w7dg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df4d302e9bb74550a3c16fd5342ab649a5bc3a53" title="Introducing GLM-Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing GLM-Image: A new milestone in open-source image generation.&lt;/p&gt; &lt;p&gt;GLM-Image uses a hybrid auto-regressive plus diffusion architecture, combining strong global semantic understanding with high fidelity visual detail. It matches mainstream diffusion models in overall quality while excelling at text rendering and knowledge intensive generation.&lt;/p&gt; &lt;p&gt;Tech Blog: &lt;a href="http://z.ai/blog/glm-image"&gt;http://z.ai/blog/glm-image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Experience it right now: &lt;a href="http://huggingface.co/zai-org/GLM-Image"&gt;http://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/zai-org/GLM-Image"&gt;http://github.com/zai-org/GLM-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/70ypvyc5w7dg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T01:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcff41</id>
    <title>EXAONE MoE support has been merged into llama.cpp</title>
    <updated>2026-01-14T05:55:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"&gt; &lt;img alt="EXAONE MoE support has been merged into llama.cpp" src="https://external-preview.redd.it/zj2pPBSKKE7hlpLBhVdaJfKDygb15HG1H-ApMccLwl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02a15a93673baf3e7e305c8147197d65844556ed" title="EXAONE MoE support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;K-EXAONE-236B-A23B&lt;/h1&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#introduction"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;K-EXAONE&lt;/strong&gt;, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features &lt;strong&gt;236 billion total&lt;/strong&gt; parameters, with &lt;strong&gt;23 billion active&lt;/strong&gt; during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture &amp;amp; Efficiency:&lt;/strong&gt; Features a 236B fine-grained MoE design (23B active) optimized with &lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context Capabilities:&lt;/strong&gt; Natively supports a &lt;strong&gt;256K context window&lt;/strong&gt;, utilizing a &lt;strong&gt;3:1 hybrid attention&lt;/strong&gt; scheme with a &lt;strong&gt;128-token sliding window&lt;/strong&gt; to significantly minimize memory usage during long-document processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support:&lt;/strong&gt; Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned &lt;strong&gt;150k vocabulary&lt;/strong&gt; with &lt;strong&gt;SuperBPE&lt;/strong&gt;, improving token efficiency by ~30%.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities:&lt;/strong&gt; Demonstrates superior tool-use and search capabilities via &lt;strong&gt;multi-agent strategies.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety &amp;amp; Ethics:&lt;/strong&gt; Aligned with &lt;strong&gt;universal human values&lt;/strong&gt;, the model uniquely incorporates &lt;strong&gt;Korean cultural and historical contexts&lt;/strong&gt; to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18543"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T05:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbw325</id>
    <title>My wishes for 2026</title>
    <updated>2026-01-13T16:35:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt; &lt;img alt="My wishes for 2026" src="https://preview.redd.it/8knck5zv85dg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a8be13989bebb31b688873f7197d169cb43651e" title="My wishes for 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which do you think will happen first? And which won‚Äôt happen in 2026?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8knck5zv85dg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T16:35:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qctseq</id>
    <title>Public coding benchmarks suck, how are you evaluating performance?</title>
    <updated>2026-01-14T17:40:15+00:00</updated>
    <author>
      <name>/u/AvocadoArray</name>
      <uri>https://old.reddit.com/user/AvocadoArray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I feel the need to preface my posts saying this was &lt;strong&gt;entirely written by me with zero help from an LLM&lt;/strong&gt;. A lot of people see a long post w/ headers and automatically think it's AI slop (myself included sometimes). This post might be slop, but it's &lt;em&gt;my&lt;/em&gt; slop.&lt;/p&gt; &lt;h1&gt;Background&lt;/h1&gt; &lt;p&gt;We all know public benchmark scores are becoming less useful as model authors attempt to benchmax everything. To really get a sense of whether a model is viable, I usually just throw a couple of my old one-shot programming problems at it, and if it passes, I give it a complex problem in Roo code on one of my projects at a specific git commit to see how it performs. However, this is process highly subjective and sometimes it's hard to tell if bad results are due to the model itself, a setting I changed, or just a random failure that goes away after retrying.&lt;/p&gt; &lt;p&gt;I wanted to use a more empirical, automated, and repeatable process to evaluate performance of different models / quants / kv quants / settings. I decided to try Aider Polyglot since it seems to be a pretty popular benchmark.&lt;/p&gt; &lt;p&gt;However, I no longer think this is a good option for a few reasons:&lt;/p&gt; &lt;h1&gt;Problem 1: Poorly Written Tests&lt;/h1&gt; &lt;p&gt;I started noticing some of the test failures were not really the model's fault and were instead due to bad/vague instructions, or information the model couldn't have known ahead of time (unless the data was included during training ü§î).&lt;/p&gt; &lt;p&gt;Take the &lt;a href="https://github.com/Aider-AI/polyglot-benchmark/blob/main/python/exercises/practice/two-bucket/.docs/instructions.md"&gt;two-bucket test&lt;/a&gt; for example. From the instructions (emphasis mine):&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Your program will take as input:&lt;br /&gt; - the size of bucket one&lt;br /&gt; - the size of bucket two&lt;br /&gt; - the desired number of liters to reach&lt;br /&gt; - which bucket to fill first, either &lt;strong&gt;bucket one&lt;/strong&gt; or &lt;strong&gt;bucket two&lt;/strong&gt; &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Your program should determine:&lt;br /&gt; - the total number of actions it should take to reach the desired number of liters, including the first fill of the starting bucket&lt;br /&gt; - which bucket should end up with the desired number of liters - either &lt;strong&gt;bucket one&lt;/strong&gt; or &lt;strong&gt;bucket two&lt;/strong&gt;&lt;br /&gt; - how many liters are left in the other bucket&lt;/p&gt; &lt;p&gt;In this case, the model failed the test because it expected an input variable to be either &lt;code&gt;bucket one&lt;/code&gt; or &lt;code&gt;bucket two&lt;/code&gt;, but the the unit test passes bucket names as &lt;code&gt;one&lt;/code&gt; / &lt;code&gt;two&lt;/code&gt; (and expects the return values to be the same). The unit test is not visible to the model during evaluation, so it has no way of knowing exactly how the code will be tested.&lt;/p&gt; &lt;p&gt;(note that by default, Aider gives the model two attempts to pass the test. If the first attempt fails, Aider gives the model the test failure output and gives asks the model to fix the errors.)&lt;/p&gt; &lt;p&gt;As mentioned, the first attempt failed because &lt;code&gt;one&lt;/code&gt; / &lt;code&gt;two&lt;/code&gt; were not valid input variables:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;================================== FAILURES ================================== _ TwoBucketTest.test_measure_one_step_using_bucket_one_of_size_1_and_bucket_two_of_size_3_start_with_bucket_two _ self = &amp;lt;two_bucket_test.TwoBucketTest testMethod=test_measure_one_step_using_bucket_one_of_size_1_and_bucket_two_of_size_3_start_with_bucket_two&amp;gt; def test_measure_one_step_using_bucket_one_of_size_1_and_bucket_two_of_size_3_start_with_bucket_two( self, ): &amp;gt; self.assertEqual(measure(1, 3, 3, &amp;quot;two&amp;quot;), (1, &amp;quot;two&amp;quot;, 0)) ^^^^^^^^^^^^^^^^^^^^^^^ two_bucket_test.py:36: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ bucket_one = 1, bucket_two = 3, goal = 3, start_bucket = 'two' def measure(bucket_one, bucket_two, goal, start_bucket): # Input validation with meaningful error messages if goal == 0: raise ValueError(&amp;quot;Goal cannot be zero&amp;quot;) if goal &amp;gt; bucket_one and goal &amp;gt; bucket_two: raise ValueError(&amp;quot;Goal exceeds both bucket capacities&amp;quot;) if bucket_one &amp;lt;= 0 or bucket_two &amp;lt;= 0: raise ValueError(&amp;quot;Bucket sizes must be positive&amp;quot;) if start_bucket not in (&amp;quot;bucket one&amp;quot;, &amp;quot;bucket two&amp;quot;): &amp;gt; raise ValueError(&amp;quot;Start bucket must be either 'bucket one' or 'bucket two'&amp;quot;) E ValueError: Start bucket must be either 'bucket one' or 'bucket two' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No problem, the model fixed the code to accept either format and normalized the variable before running the rest of the code. But then it failed again because the &lt;em&gt;output&lt;/em&gt; did not match the test case&lt;/p&gt; &lt;pre&gt;&lt;code&gt;================================== FAILURES ================================== _ TwoBucketTest.test_measure_one_step_using_bucket_one_of_size_1_and_bucket_two_of_size_3_start_with_bucket_two _ self = &amp;lt;two_bucket_test.TwoBucketTest testMethod=test_measure_one_step_using_bucket_one_of_size_1_and_bucket_two_of_size_3_start_with_bucket_two&amp;gt; def test_measure_one_step_using_bucket_one_of_size_1_and_bucket_two_of_size_3_start_with_bucket_two( self, ): &amp;gt; self.assertEqual(measure(1, 3, 3, &amp;quot;two&amp;quot;), (1, &amp;quot;two&amp;quot;, 0)) E AssertionError: Tuples differ: (1, 'bucket two', 0) != (1, 'two', 0) E E First differing element 1: E 'bucket two' E 'two' E E - (1, 'bucket two', 0) E ? ------- E E + (1, 'two', 0) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This counts as a strike against the model and lowers its score, but I don't care because the model followed the literal instructions. In fact, I'd almost argue that any model passing this test on the first shot might actually be evidence of cheating / benchmaxing.&lt;/p&gt; &lt;h1&gt;Problem 2: Aider results don't translate to agentic coding&lt;/h1&gt; &lt;p&gt;Most (if not all) Aider tests only involve a editing a single file, but agentic coding involves reading and editing multiple files on top of planning, tool calling, asking the user for clarification etc. That's not really Aider's fault, I just didn't understand that until I looked at the coding problems.&lt;/p&gt; &lt;p&gt;I guess Livebench or SWE-bench might be more relevant to agentic coding?&lt;/p&gt; &lt;h1&gt;Problem 3: Tests take forever&lt;/h1&gt; &lt;p&gt;I run &lt;a href="https://huggingface.co/Intel/Seed-OSS-36B-Instruct-int4-AutoRound"&gt;Seed-OSS 36B INT4 AutoRound&lt;/a&gt; in VLLM across 2x Nvidia L4 24GB cards (tensor parallelism), which gives me about 20 tp/s. It's very usable in Roo Code, as its thinking is usually very short (&amp;lt;512 tokens in most cases). However, with the default system prompt, Aider Polyglot tests often produce 8k+ thinking tokens, and the average duration of each test is over 10 minutes (I actually had to increase the hard-coded 600s timeout to get some tests to complete).&lt;/p&gt; &lt;p&gt;I will probably try using a different system prompt or limit thinking, but I worry that could cause more variance in the results.&lt;/p&gt; &lt;h1&gt;Possible Solutions&lt;/h1&gt; &lt;p&gt;I'll probably start by curating/modifying the Aider problems to fit my taste, as the framework is laid out very logically and it's easy to make changes.&lt;/p&gt; &lt;p&gt;However, I still want a more automated and empirical method of testing agentic performance. Ideally, this process would use the same client that I use in the real world (Roo Code currently, but taking a closer look at OpenCode), and work on actual (past) problems from my project codebases. Maybe I can set something up in n8n/dify, but I haven't played around with those too much.&lt;/p&gt; &lt;p&gt;Anyway, this started as a private note but I thought I'd post here to see if anyone else has any experience with this. If you have an empirical, automated, quick-ish, and repeatable process for benching LLM coding performance, I'd love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AvocadoArray"&gt; /u/AvocadoArray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qctseq/public_coding_benchmarks_suck_how_are_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qctseq/public_coding_benchmarks_suck_how_are_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qctseq/public_coding_benchmarks_suck_how_are_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:40:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcjxex</id>
    <title>Renting "inconvenient" H200 (141 GB), A100 GPUs worth it?</title>
    <updated>2026-01-14T10:30:44+00:00</updated>
    <author>
      <name>/u/Select_Jellyfish9325</name>
      <uri>https://old.reddit.com/user/Select_Jellyfish9325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm a junior research intern at an AI lab. We currently hold a lease on a cluster containing H200s, H100s, and A100s (plus some consumer cards, such as 4090s/5090s, which we have racked ourselves).&lt;/p&gt; &lt;p&gt;While we hit the cluster hard during major training runs, we have periods‚Äîsometimes weeks long‚Äîwhere the high-end capacity sits at 30-40% utilisation.&lt;/p&gt; &lt;p&gt;I‚Äôve been trying to convince the team to open up the idle capacity to the community to recoup some leasing costs. Based on our overhead, we could offer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;H200 (141GB): ~$9 - $10 / hr&lt;/li&gt; &lt;li&gt;A100 (80GB): ~$1.80 / hr&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Catch (and why I‚Äôm asking)&lt;strong&gt;:&lt;/strong&gt;&lt;br /&gt; We are not a cloud provider. We don't have a UI like RunPod or Lambda.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It would be SSH access via a jump host.&lt;/li&gt; &lt;li&gt;You get a Docker container (we can pre-load Unsloth/Axolotl).&lt;/li&gt; &lt;li&gt;No &amp;quot;One-Click Deploy.&amp;quot; Setup is manual.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My Question:&lt;br /&gt; Is that level of &amp;quot;bad UX&amp;quot; a dealbreaker?&lt;/p&gt; &lt;p&gt;I could spend a weekend building a simple web dashboard for reservations, but that might push the price slightly higher (to cover dev time/Stripe fees).&lt;/p&gt; &lt;p&gt;Do you guys prefer the raw, cheapest price with SSH, or is the dashboard worth the extra premium? Just trying to gauge if this is worth setting up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Jellyfish9325"&gt; /u/Select_Jellyfish9325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxex/renting_inconvenient_h200_141_gb_a100_gpus_worth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxex/renting_inconvenient_h200_141_gb_a100_gpus_worth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxex/renting_inconvenient_h200_141_gb_a100_gpus_worth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T10:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc5nml</id>
    <title>Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!</title>
    <updated>2026-01-13T22:32:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt; &lt;img alt="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" src="https://external-preview.redd.it/amZxajFtZXF6NmRnMQO5kEggYbW8-0IppaPjE5mW-pGiD_HSvWQwK_psM6yd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a2ad1d1aac49f10f9a525a08d7b23d8a37a99b3" title="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I‚Äôve been listening to all your feedback on Soprano, and I‚Äôve been working nonstop over these past three weeks to incorporate everything, so I have a TON of updates for you all!&lt;/p&gt; &lt;p&gt;For those of you who haven‚Äôt heard of Soprano before, it is an on-device text-to-speech model I designed to have highly natural intonation and quality with a small model footprint. It can run up to &lt;strong&gt;20x realtime&lt;/strong&gt; on CPU, and up to &lt;strong&gt;2000x&lt;/strong&gt; on GPU. It also supports lossless streaming with &lt;strong&gt;15 ms latency&lt;/strong&gt;, an order of magnitude lower than any other TTS model. You can check out Soprano here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;&lt;strong&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-80M"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-80M&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I am releasing training code for you guys! This was by far the most requested feature to be added, and I am happy to announce that you can now train your own ultra-lightweight, ultra-realistic TTS models like the one in the video with your &lt;strong&gt;own data&lt;/strong&gt; on your &lt;strong&gt;own hardware&lt;/strong&gt; with &lt;strong&gt;Soprano-Factory&lt;/strong&gt;! Using Soprano-Factory, you can add new &lt;strong&gt;voices&lt;/strong&gt;, &lt;strong&gt;styles&lt;/strong&gt;, and &lt;strong&gt;languages&lt;/strong&gt; to Soprano. The entire repository is just 600 lines of code, making it easily customizable to suit your needs.&lt;/p&gt; &lt;p&gt;In addition to the training code, I am also releasing &lt;strong&gt;Soprano-Encoder&lt;/strong&gt;, which converts raw audio into audio tokens for training. You can find both here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Factory:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano-factory"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano-factory&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Encoder:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-Encoder"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-Encoder&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I hope you enjoy it! See you tomorrow,&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;p&gt;Disclaimer: I did not originally design Soprano with finetuning in mind. As a result, I cannot guarantee that you will see good results after training. Personally, I have my doubts that an 80M-parameter model trained on just 1000 hours of data can generalize to OOD datasets, but I have seen bigger miracles on this sub happen, so knock yourself out :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wnuwfpdqz6dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcgviy</id>
    <title>Unique 3.2M-word bilingual (DE-EN) literary erotica corpus available for AI training‚Äîteasers on Hugging Face</title>
    <updated>2026-01-14T07:17:55+00:00</updated>
    <author>
      <name>/u/kardinalzahl</name>
      <uri>https://old.reddit.com/user/kardinalzahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;As an independent author, I've created a large original bilingual erotic fiction corpus (German originals + expanded English adaptations) that's well-suited for training or fine-tuning creative/uncensored models. Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~3.2 million words across 500+ chapters&lt;/li&gt; &lt;li&gt;Long-form, character-driven narrative with progressive consensual kink (e.g., urophilia, period sex), rural/urban Vietnam settings&lt;/li&gt; &lt;li&gt;Sophisticated prose with philosophical references (Kant, Hegel, existential themes)&lt;/li&gt; &lt;li&gt;Bilingual parallel structure (German first, English creatively reworked‚Äîsometimes longer, sometimes shorter)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Three gated teaser datasets (~475k bilingual words total) are now live on Hugging Face:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Profile with all three: &lt;a href="https://huggingface.co/douglasvanwyck"&gt;https://huggingface.co/douglasvanwyck&lt;/a&gt; &lt;ul&gt; &lt;li&gt;With Anna in Saigon (complete mini-series, ~87k words)&lt;/li&gt; &lt;li&gt;&amp;quot;Phung's Quest&amp;quot; (ongoing series, 7 chapters, ~87k words)&lt;/li&gt; &lt;li&gt;&amp;quot;Center of the Universe&amp;quot;‚ÄîFirst 35 chapters (main saga teaser, ~301k words)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kardinalzahl"&gt; /u/kardinalzahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T07:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcjxb4</id>
    <title>Would you watch a channel that builds real AI systems from scratch (local LLMs, CPU/GPU, pipelines)?</title>
    <updated>2026-01-14T10:30:35+00:00</updated>
    <author>
      <name>/u/Few_Tax650</name>
      <uri>https://old.reddit.com/user/Few_Tax650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm considering starting a YouTube channel focused on building production-grade AI systems. Before I invest serious time into this, I want to know if this is something people would actually watch.&lt;/p&gt; &lt;p&gt;I‚Äôm a developer working on AI pipelines and multi-model systems, and I feel there‚Äôs a gap between ‚ÄúAI hype videos‚Äù and real, hands-on system building.&lt;/p&gt; &lt;p&gt;What I‚Äôd cover: ‚Ä¢ Building bots from zero (no fluff, real architecture) ‚Ä¢ CPU vs GPU optimization for local models ‚Ä¢ Multi-model pipelines: routers, fallbacks, model judges ‚Ä¢ Config-driven backends (swap models without rewriting code) ‚Ä¢ Complete workflows: idea ‚Üí architecture ‚Üí working system&lt;/p&gt; &lt;p&gt;Everything would be open-source. You‚Äôd see the code, the mistakes, the refactors, and the final result.&lt;/p&gt; &lt;p&gt;My questions for you: 1. Would you actually watch technical deep-dives like this? 2. What would you personally want more of? (local LLMs, performance benchmarks, agent architecture, deployment, etc.)&lt;/p&gt; &lt;p&gt;I‚Äôm a builder first, not a content creator ‚Äî so I want to make sure this is genuinely useful to real developers before committing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Tax650"&gt; /u/Few_Tax650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxb4/would_you_watch_a_channel_that_builds_real_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxb4/would_you_watch_a_channel_that_builds_real_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxb4/would_you_watch_a_channel_that_builds_real_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T10:30:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcj1lr</id>
    <title>What happened to 1.58bit LLMs?</title>
    <updated>2026-01-14T09:34:54+00:00</updated>
    <author>
      <name>/u/Sloppyjoeman</name>
      <uri>https://old.reddit.com/user/Sloppyjoeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year I remember them being super hyped and largely theoretical. Since then, I understand there‚Äôs a growing body of evidence that larger sparse models outperform smaller denser models, which 1.58bit quantisation seems poised to drastically improve&lt;/p&gt; &lt;p&gt;I haven‚Äôt seen people going ‚Äúoh, the 1.58bit quantisation was overhyped‚Äù - did I just miss it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sloppyjoeman"&gt; /u/Sloppyjoeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T09:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qckjsq</id>
    <title>ZLUDA on llama.cpp -NEWS</title>
    <updated>2026-01-14T11:08:04+00:00</updated>
    <author>
      <name>/u/mossy_troll_84</name>
      <uri>https://old.reddit.com/user/mossy_troll_84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.phoronix.com/news/ZLUDA-Q4-2025-Report"&gt;https://www.phoronix.com/news/ZLUDA-Q4-2025-Report&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mossy_troll_84"&gt; /u/mossy_troll_84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qckjsq/zluda_on_llamacpp_news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qckjsq/zluda_on_llamacpp_news/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qckjsq/zluda_on_llamacpp_news/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T11:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qct6h2</id>
    <title>How does my local LLM rig look?</title>
    <updated>2026-01-14T17:18:09+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"&gt; &lt;img alt="How does my local LLM rig look?" src="https://preview.redd.it/z1xw8usylcdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc831716841f3b411148307f63ee880af80b163" title="How does my local LLM rig look?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In garage/ freezing MN temps are nice!&lt;/p&gt; &lt;p&gt;Key Specs:&lt;/p&gt; &lt;p&gt;Motherboard: ASUS Pro WS W790E-SAGE SE (workstation platform, multi-GPU + tons of PCIe)&lt;/p&gt; &lt;p&gt;CPU: Intel Xeon W9-3495X 56 cores 112 threads, Intel AMX primarily for ktransformers build in mind (moved from an engineering sample to retail)&lt;/p&gt; &lt;p&gt;Memory: 512GB DDR5 ECC (8√ó64GB) 4800 but overclocked to 6000 on an octa-channel platform&lt;/p&gt; &lt;p&gt;GPUs: 2√ó NVIDIA RTX PRO 6000 Blackwell Workstation Edition (96GB VRAM each)&lt;/p&gt; &lt;p&gt;Storage: Samsung 9100 PRO 4TB Gen5 NVMe for models + WD_BLACK SN850X 2TB for OS&lt;/p&gt; &lt;p&gt;Network: 10Gb local + 1Gb internet&lt;/p&gt; &lt;p&gt;Can you spot all other tools except for the server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1xw8usylcdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcv64u</id>
    <title>What‚Äôs the deal with these fake GPU listings on eBay?</title>
    <updated>2026-01-14T18:29:25+00:00</updated>
    <author>
      <name>/u/humandisaster99</name>
      <uri>https://old.reddit.com/user/humandisaster99</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"&gt; &lt;img alt="What‚Äôs the deal with these fake GPU listings on eBay?" src="https://b.thumbs.redditmedia.com/65MpuHjYbWHFf1_tcI0FQmitzKDLLh1kLzUX95wfnoE.jpg" title="What‚Äôs the deal with these fake GPU listings on eBay?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been seeing these around for a while. For most AI GPU searches there will be a couple on the first page. It‚Äôs always a zero review account that was created same-day selling for a third of the normal price. They‚Äôre very clearly scams, but how? eBay buyer protection will always provide a refund if you ask for it basically, so what‚Äôs the scam? Do they just send you a fake GPU and hope you don‚Äôt notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/humandisaster99"&gt; /u/humandisaster99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qcv64u"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:29:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcsmww</id>
    <title>We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.</title>
    <updated>2026-01-14T16:58:22+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an AI agent to localize imported food products for a retail client. The task sounds simple: extract product info, translate it contextually (not Google Translate), calculate nutritional values for local formats, check compliance with local regulations.&lt;/p&gt; &lt;p&gt;First attempt: one detailed prompt. Let the AI figure out the workflow.&lt;/p&gt; &lt;p&gt;Result: chaos. The AI would hallucinate numbers even with clean images. It would skip steps randomly. At scale, we had no idea where things broke. Every error was a mystery to debug.&lt;/p&gt; &lt;p&gt;So we broke it down. Way down. 27 steps.&lt;/p&gt; &lt;p&gt;Each column in our system handles one thing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extract product name&lt;/li&gt; &lt;li&gt;Extract weight&lt;/li&gt; &lt;li&gt;Extract nutritional values per serving&lt;/li&gt; &lt;li&gt;Convert units to local format&lt;/li&gt; &lt;li&gt;Translate product name (contextual, not literal)&lt;/li&gt; &lt;li&gt;Translate description&lt;/li&gt; &lt;li&gt;Check certification requirements&lt;/li&gt; &lt;li&gt;... and so on&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What changed:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Traceability.&lt;/strong&gt; When something fails, we know exactly which step. No more guessing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Fixability.&lt;/strong&gt; Client corrects a number extraction error once, we build a formula that prevents it downstream. Errors get fixed permanently, not repeatedly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Consistency at scale.&lt;/strong&gt; The AI isn't &amp;quot;deciding&amp;quot; what to do. It's executing a defined process. Same input, same process, predictable output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Human oversight actually works.&lt;/strong&gt; The person reviewing outputs learns where the AI struggles. Step 14 always needs checking. Step 22 is solid. They get faster over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The counterintuitive part:&lt;/strong&gt; making the AI &amp;quot;dumber&amp;quot; per step made the overall system smarter. One prompt trying to do everything is one prompt that can fail in infinite ways. 27 simple steps means 27 places where you can inspect, correct, and improve.&lt;/p&gt; &lt;p&gt;We've processed over 10,000 products this way. The manual process used to take 20 minutes per product. Now it's 3 minutes, mostly human review.&lt;/p&gt; &lt;p&gt;The boring truth about reliable AI agents: it's not about prompt engineering magic. It's about architecture that assumes AI will fail and makes failure easy to find and fix.&lt;/p&gt; &lt;p&gt;Happy to answer questions about the approach.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T16:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcy7ug</id>
    <title>meituan-longcat/LongCat-Flash-Thinking-2601 ¬∑ Hugging Face</title>
    <updated>2026-01-14T20:20:00+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcy7ug/meituanlongcatlongcatflashthinking2601_hugging/"&gt; &lt;img alt="meituan-longcat/LongCat-Flash-Thinking-2601 ¬∑ Hugging Face" src="https://external-preview.redd.it/kb1mVOfmWTAvSMxYL_8sXovYFqgoXm6u9Rl74bhEZK8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c6634be30000f0e6f7229b645e18aa7d9cde211" title="meituan-longcat/LongCat-Flash-Thinking-2601 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking-2601"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcy7ug/meituanlongcatlongcatflashthinking2601_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcy7ug/meituanlongcatlongcatflashthinking2601_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T20:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc9m6x</id>
    <title>GLM-Image is released!</title>
    <updated>2026-01-14T01:17:16+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt; &lt;img alt="GLM-Image is released!" src="https://external-preview.redd.it/Ei4JzvCHJGNODl-Xo97JEKHuZJZU81UlEy5iyXWioSw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=251fac1763ed77fdaf4e281f649fddd4555de498" title="GLM-Image is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-Image is an image generation model adopts a hybrid autoregressive + diffusion decoder architecture. In general image generation quality, GLM‚ÄëImage aligns with mainstream latent diffusion approaches, but it shows significant advantages in text-rendering and knowledge‚Äëintensive generation scenarios. It performs especially well in tasks requiring precise semantic understanding and complex information expression, while maintaining strong capabilities in high‚Äëfidelity and fine‚Äëgrained detail generation. In addition to text‚Äëto‚Äëimage generation, GLM‚ÄëImage also supports a rich set of image‚Äëto‚Äëimage tasks including image editing, style transfer, identity‚Äëpreserving generation, and multi‚Äësubject consistency.&lt;/p&gt; &lt;p&gt;Model architecture: a hybrid autoregressive + diffusion decoder design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T01:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcvk9n</id>
    <title>Popularity of DDR3 motherboards is growing rapidly - VideoCardz.com</title>
    <updated>2026-01-14T18:43:26+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcvk9n/popularity_of_ddr3_motherboards_is_growing/"&gt; &lt;img alt="Popularity of DDR3 motherboards is growing rapidly - VideoCardz.com" src="https://external-preview.redd.it/1_qecsYofG5L-GuD7Gh4daRo-sGqgl6aBwxtGGFCAGA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f35147fed27d85c69f96db9aabc5b1ec2622714" title="Popularity of DDR3 motherboards is growing rapidly - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I genuinely hate this timeline.&lt;/p&gt; &lt;p&gt;While I'm in the very lucky position to have bought more than enough RAM and storage for my homelab and local LLM needs before prices went up, my favorite past time and hobby of homelabbing feels completely ruined.&lt;/p&gt; &lt;p&gt;Three months ago, I was looking forward to ECC DDR5 prices coming down to the point of being bale to buy 512GB DDR5 RAM for ~‚Ç¨500 to finally have a Saphire Rapids Xeon in my homelab and play with AMX, I'm now afraid that DDR4 stick I have might fail, and not being able to replace it.&lt;/p&gt; &lt;p&gt;With DDR4 prices through the roof, I guess this was bound to happen, but it doesn't make it sting any less. How long now until DDR3 prices also skyrocket, and with them the motherboards and CPUs that also support it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/popularity-of-ddr3-motherboards-is-growing-rapidly"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcvk9n/popularity_of_ddr3_motherboards_is_growing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcvk9n/popularity_of_ddr3_motherboards_is_growing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcl543</id>
    <title>Which are the top LLMs under 8B right now?</title>
    <updated>2026-01-14T11:42:15+00:00</updated>
    <author>
      <name>/u/Additional_Secret_75</name>
      <uri>https://old.reddit.com/user/Additional_Secret_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I m looking to pick a local LLM and not sure what to go with anymore. There are a lot of ‚Äúbest‚Äù &amp;lt;8B models and every post says something different, even for the same model. What are people using for normal chat, research, or some coding, not super censored and runs well without a ton of VRAM. It doesn t have to be just one LLM, just the best in their category.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Secret_75"&gt; /u/Additional_Secret_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T11:42:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcv304</id>
    <title>NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3</title>
    <updated>2026-01-14T18:26:19+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"&gt; &lt;img alt="NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3" src="https://external-preview.redd.it/eGh0aTBhazZ5Y2RnMTTPucJdRjO2R67S5i-oYJkuLIwhwL3TAJbW3Q2hg2iU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a81ca1f90638b7e5fb617d89b9d1a2abf729cae1" title="NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;The team at Neuphonic is back with a new open-source release: NeuTTS Nano.&lt;/p&gt; &lt;p&gt;After NeuTTS Air trended #1 on HuggingFace last October, we received a lot of requests for something even smaller that could fit into tighter VRAM/RAM constraints for robotics and embedded agents.&lt;/p&gt; &lt;p&gt;Key Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Size: 120M active parameters (3x smaller than NeuTTS Air).&lt;/li&gt; &lt;li&gt;Architecture: Simple LM + codec architecture built off Llama3.&lt;/li&gt; &lt;li&gt;Format: Provided in GGML for easy deployment on mobile, Jetson, and Raspberry Pi.&lt;/li&gt; &lt;li&gt;Capabilities: Instant voice cloning (3s sample) and ultra-realistic prosody.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why use this?&lt;/p&gt; &lt;p&gt;If you are building for smart home devices, robotics, or mobile apps where every MB of RAM matters, Nano is designed for you. It delivers the same &amp;quot;voice magic&amp;quot; but in a much lighter package.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/neuphonic/neutts"&gt;https://github.com/neuphonic/neutts&lt;/a&gt; &lt;/li&gt; &lt;li&gt;HuggingFace: &lt;a href="https://huggingface.co/neuphonic/neutts-nano"&gt;https://huggingface.co/neuphonic/neutts-nano&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Spaces: &lt;a href="https://huggingface.co/spaces/neuphonic/neutts-nano"&gt;https://huggingface.co/spaces/neuphonic/neutts-nano&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Website: &lt;a href="https://www.neuphonic.com/"&gt;https://www.neuphonic.com/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôre curious to see the RTF (Real-Time Factor) benchmarks the community gets on different hardware. What‚Äôs the smallest device you‚Äôre planning to run this on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2nikcyj6ycdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcusnt</id>
    <title>Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M</title>
    <updated>2026-01-14T18:16:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt; &lt;img alt="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" src="https://external-preview.redd.it/NXZ5NDNuYTlzY2RnMX4ZwK1s5ENYxRsvoiSEu3mA0RmAAs2-sAvwRMu-2CtN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2a20f3c8df9af0a0fbced04bbc8dc6ec0450abe" title="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;Today, I am announcing Soprano 1.1! I‚Äôve designed it for massively improved stability and audio quality over the original model. &lt;/p&gt; &lt;p&gt;While many of you were happy with the quality of Soprano, it had a tendency to start, well, &lt;em&gt;Mongolian throat singing&lt;/em&gt;. Contrary to its name, Soprano is &lt;strong&gt;NOT&lt;/strong&gt; supposed to be for singing, so I have reduced the frequency of these hallucinations by &lt;strong&gt;95%&lt;/strong&gt;. Soprano 1.1-80M also has a &lt;strong&gt;50%&lt;/strong&gt; lower WER than Soprano-80M, with comparable clarity to much larger models like Chatterbox-Turbo and VibeVoice. In addition, it now supports sentences up to &lt;strong&gt;30 seconds&lt;/strong&gt; long, up from 15.&lt;/p&gt; &lt;p&gt;The outputs of Soprano could sometimes have a lot of artifacting and high-frequency noise. This was because the model was severely undertrained. I have trained Soprano further to reduce these audio artifacts.&lt;/p&gt; &lt;p&gt;According to a blind study I conducted on my family (against their will), they preferred Soprano 1.1's outputs &lt;strong&gt;63%&lt;/strong&gt; of the time, so these changes have produced a noticeably improved model.&lt;/p&gt; &lt;p&gt;You can check out the new Soprano here:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ekwek/Soprano-1.1-80M"&gt;https://huggingface.co/ekwek/Soprano-1.1-80M&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Try Soprano 1.1 Now: &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt; &lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v0c2rda9scdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:16:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcuerc</id>
    <title>NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency</title>
    <updated>2026-01-14T18:02:19+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen some arguments we‚Äôve reached AGI, it‚Äôs just about putting the separate pieces together in the right context. I think having a relatively small model that knows how to connect with other tools and models is exactly the correct route towards very functional systems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
