<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-13T22:36:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1plkv9n</id>
    <title>Maxun: Free, Open-Source Web Data for AI Agents &amp; Data Pipelines</title>
    <updated>2025-12-13T13:02:04+00:00</updated>
    <author>
      <name>/u/carishmaa</name>
      <uri>https://old.reddit.com/user/carishmaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, everyone&lt;/p&gt; &lt;p&gt;Excited to bring to you &lt;strong&gt;Maxun&lt;/strong&gt; : an open-source, self-hostable web extraction &amp;amp; scraping platform we‚Äôve been building in the open for over a year. &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/getmaxun/maxun"&gt;https://github.com/getmaxun/maxun&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What Maxun Does?&lt;/h1&gt; &lt;p&gt;Maxun uses &lt;strong&gt;web robots&lt;/strong&gt; that emulate real user behavior and return clean, structured data or AI-ready content.&lt;/p&gt; &lt;h1&gt;Extract Robots (Structured Data)&lt;/h1&gt; &lt;p&gt;Build them in two ways&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recorder Mode:&lt;/strong&gt; Browse like a human (click, scroll, paginate). Deterministic and reliable. &lt;ul&gt; &lt;li&gt;Example: Extract 10 Property Listings from Airbnb&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://github.com/user-attachments/assets/c6baa75f-b950-482c-8d26-8a8b6c5382c3"&gt;https://github.com/user-attachments/assets/c6baa75f-b950-482c-8d26-8a8b6c5382c3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Mode&lt;/strong&gt;: Describe what you want in natural language. Works with local LLMs (Ollama) and cloud models. &lt;ul&gt; &lt;li&gt;Example: Extract Names, Rating &amp;amp; Duration of Top 50 Movies from IMDb&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://github.com/user-attachments/assets/f714e860-58d6-44ed-bbcd-c9374b629384"&gt;https://github.com/user-attachments/assets/f714e860-58d6-44ed-bbcd-c9374b629384&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Scrape Robots (Content for AI)&lt;/h1&gt; &lt;p&gt;Built for agent pipelines&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean HTML, &lt;strong&gt;LLM-ready Markdown or capture Screenshots&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Useful for RAG, embeddings, summarization, and indexing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;SDK&lt;/h1&gt; &lt;p&gt;Via the SDK, agents can&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Trigger extract or scrape robots&lt;/li&gt; &lt;li&gt;Use LLM or non-LLM extraction&lt;/li&gt; &lt;li&gt;Handle pagination automatically&lt;/li&gt; &lt;li&gt;Run jobs on schedules or via API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;SDK: &lt;a href="https://github.com/getmaxun/node-sdk"&gt;https://github.com/getmaxun/node-sdk&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://docs.maxun.dev/category/sdk"&gt;https://docs.maxun.dev/category/sdk&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Open Source + Self-Hostable&lt;/h1&gt; &lt;p&gt;Maxun is ~&lt;strong&gt;99% open source&lt;/strong&gt;.&lt;br /&gt; Scheduling, webhooks, robot runs, and management are all available in OSS.&lt;br /&gt; Self-hostable with or without Docker.&lt;/p&gt; &lt;p&gt;Would love feedback, questions and suggestions from folks building agents or data pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carishmaa"&gt; /u/carishmaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkv9n/maxun_free_opensource_web_data_for_ai_agents_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkv9n/maxun_free_opensource_web_data_for_ai_agents_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plkv9n/maxun_free_opensource_web_data_for_ai_agents_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T13:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1plw6ct</id>
    <title>The right Epyc model - making the case for the Turin P-series</title>
    <updated>2025-12-13T21:12:46+00:00</updated>
    <author>
      <name>/u/k0vatch</name>
      <uri>https://old.reddit.com/user/k0vatch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to build an AMD machine for local inference. Started with Threadripper (Zen5) for the cheaper price, then went to the WX/Pro for the better bandwidth, but the higher end models, that seem usable, are pretty expensive. So I'm finally settled on a &lt;strong&gt;single socket Epyc Turin&lt;/strong&gt;. Turin offers the best memory bandwidth and decent motherboard options with 12 DIMM sockets.&lt;/p&gt; &lt;p&gt;There are many SKUs&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Zen_5#Turin"&gt;https://en.wikipedia.org/wiki/Zen_5#Turin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;P-series are limited to single socket systems only&lt;/em&gt;&lt;br /&gt; &lt;em&gt;F-series are juiced up in CCDs or clock&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Looking at the above table, I am questioning why people keep recommending the F-series. There are 5 9x75F models there. To me the Turin P-series seems the best option for a single socket Zen5 system. This is also based on comparing dozens of PassMark scores. I understand 9175F has crazy amount of CCDs, but only 16 cores.&lt;/p&gt; &lt;p&gt;I am leaning towards &lt;strong&gt;9355P&lt;/strong&gt; (street price &amp;lt;$3k ). It has similar performance to 9375F and it's 30% cheaper.&lt;/p&gt; &lt;p&gt;If you want more, go for &lt;strong&gt;9655P&lt;/strong&gt; (street price ~$5k ). It is listed as the 5th fastest by CPU Mark. It has 96 cores, 12 CCDs and about ~750MB/s bandwidth. It is cheaper than both 9475F and 9575F, with similar bandwidth.&lt;/p&gt; &lt;p&gt;Regarding bandwidth scores, I know PassMark exaggerates the numbers, but I was looking at the relative performance. I only considered baselines with 12 RAM modules (mostly Supemicro boards). For 8 CCD models bandwidth was about 600-700MB/s, maybe 750MB/s in some cases. Solid 750MB/s for the 9655/9755 models.&lt;/p&gt; &lt;p&gt;So, yeah - why the F-series?&lt;/p&gt; &lt;p&gt;I say P-series FTW!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k0vatch"&gt; /u/k0vatch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plw6ct/the_right_epyc_model_making_the_case_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plw6ct/the_right_epyc_model_making_the_case_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plw6ct/the_right_epyc_model_making_the_case_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T21:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1plkg6q</id>
    <title>RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs</title>
    <updated>2025-12-13T12:39:09+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;apple briefly published, then quickly removed, a paper on arxiv,&lt;br /&gt; but v1 was already out &lt;a href="https://arxiv.org/pdf/2512.06392v1"&gt;https://arxiv.org/pdf/2512.06392v1&lt;/a&gt; and it‚Äôs interesting.&lt;/p&gt; &lt;p&gt;they introduce &lt;strong&gt;rlax&lt;/strong&gt; ‚Äî a scalable rl framework for llms on tpus.&lt;/p&gt; &lt;p&gt;what rlax looks like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;parameter server architecture&lt;/li&gt; &lt;li&gt;one central trainer updates weights&lt;/li&gt; &lt;li&gt;huge inference fleets pull weights and generate rollouts&lt;/li&gt; &lt;li&gt;built for preemption and extreme parallelism&lt;/li&gt; &lt;li&gt;custom data curation and alignment tricks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;+12.8% pass@8 on qwq-32b&lt;/li&gt; &lt;li&gt;in 12h 48m&lt;/li&gt; &lt;li&gt;using 1024 tpu v5p&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;why this matters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;apple is testing rl at serious scale&lt;/li&gt; &lt;li&gt;tpu-first design = system efficiency focus&lt;/li&gt; &lt;li&gt;gains come from training engineering, not model magic&lt;/li&gt; &lt;li&gt;rl for llms is becoming an industrial pipeline&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkg6q/rlax_largescale_distributed_reinforcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkg6q/rlax_largescale_distributed_reinforcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plkg6q/rlax_largescale_distributed_reinforcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T12:39:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1plohsn</id>
    <title>Local multi agent systems</title>
    <updated>2025-12-13T15:49:56+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have there been any interesting developments in local multi agent systems?&lt;/p&gt; &lt;p&gt;What setup/models do you like for the orchestrator/routers and the agents themselves?&lt;/p&gt; &lt;p&gt;Any interesting repos in this area?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plohsn/local_multi_agent_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plohsn/local_multi_agent_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plohsn/local_multi_agent_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1plgj0p</id>
    <title>What do you think about GLM-4.6V-Flash?</title>
    <updated>2025-12-13T08:25:38+00:00</updated>
    <author>
      <name>/u/lossless-compression</name>
      <uri>https://old.reddit.com/user/lossless-compression</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model seems too good to be true in benchmarks and I found positive reviews but I'm not sure real world tests are comparable,what is your experience?&lt;/p&gt; &lt;p&gt;The model is comparable to the MoE one in activated parameters (9B-12B) but the 12B is much more intelligent because usually a 12B activated MoE behaves more like a 20-30B dense in practice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lossless-compression"&gt; /u/lossless-compression &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plgj0p/what_do_you_think_about_glm46vflash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plgj0p/what_do_you_think_about_glm46vflash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plgj0p/what_do_you_think_about_glm46vflash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T08:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpxss</id>
    <title>Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face</title>
    <updated>2025-12-12T11:49:10+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt; &lt;img alt="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" src="https://preview.redd.it/7r3bnj5ugr6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3d5909063dd5ce912e8ebc203168db53b765be" title="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Xeophon on ùïè: &lt;a href="https://x.com/xeophon_/status/1999394570967089630"&gt;https://x.com/xeophon_/status/1999394570967089630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7r3bnj5ugr6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1plhvuz</id>
    <title>the json parser that automatically repairs your agent's "json-ish" output</title>
    <updated>2025-12-13T09:58:14+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"&gt; &lt;img alt="the json parser that automatically repairs your agent's &amp;quot;json-ish&amp;quot; output" src="https://b.thumbs.redditmedia.com/ULJPg2HwtUiCg7FepRaEpY3mnZCGv4RNbURVDKBnCWA.jpg" title="the json parser that automatically repairs your agent's &amp;quot;json-ish&amp;quot; output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/07r9qxsd2y6g1.png?width=1278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b04c313654e50e327e4d1c718745e9f120a0f2b7"&gt;https://preview.redd.it/07r9qxsd2y6g1.png?width=1278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b04c313654e50e327e4d1c718745e9f120a0f2b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sigridjineth/agentjson"&gt;https://github.com/sigridjineth/agentjson&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLMs are great at structured-ish output, but real pipelines still see markdown fences, extra prose trailing commas/smart quotes, missing commas/closers, etc. In Python, Strict parsers (json, orjson, ‚Ä¶) treat that as a hard failure, so that each agent encounters with delayed retries, latency, and brittle tool/function-calls.&lt;/p&gt; &lt;p&gt;So I made agentjson, which is a Rust-powered JSON repair pipeline with Python bindings. Strict JSON parsers fail while agentjson succeeds end‚Äëto‚Äëend. It does the following stuff.&lt;/p&gt; &lt;p&gt;- Extract the JSON span from arbitrary text&lt;br /&gt; - Repair common errors cheaply first (deterministic heuristics)&lt;br /&gt; - Recover intent via probabilistic Top‚ÄëK parsing + confidence + repair trace&lt;br /&gt; - Optionally ask an LLM for a minimal byte-offset patch only when needed, then re-validate&lt;/p&gt; &lt;p&gt;Try pip install agentjson and give it a shot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T09:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pln8mt</id>
    <title>Llama 3.2 3B fMRI (build update)</title>
    <updated>2025-12-13T14:55:52+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"&gt; &lt;img alt="Llama 3.2 3B fMRI (build update)" src="https://a.thumbs.redditmedia.com/-hmxek3yiq_z5LHZYxo8sbedyvNq3bsGqX6VgcvvnE0.jpg" title="Llama 3.2 3B fMRI (build update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share progress, since it looks like there were a few interested parties yesterday. My goal now is to record turns, and broadcast the individual dims to the rendered space. This lets me identify which individual dimensions activate under different kinds of inputs.&lt;/p&gt; &lt;p&gt;this also allows me to project rotational, grad norm, etc for the same dims and see exactly how the model responds to different kinds of inputs, making AI interp a transparency issue rather than a guessing issue. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/esoaxe75jz6g1.png?width=695&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71fcff0178eb575321bafcaa8aa2f5d613807c6f"&gt;From the bottom: layers 1, 2, 14 / 15, 27, 28&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T14:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1plf33x</id>
    <title>Free Chrome extension to run Kokoro TTS in your browser (local only)</title>
    <updated>2025-12-13T06:53:15+00:00</updated>
    <author>
      <name>/u/Impressive-Sir9633</name>
      <uri>https://old.reddit.com/user/Impressive-Sir9633</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plf33x/free_chrome_extension_to_run_kokoro_tts_in_your/"&gt; &lt;img alt="Free Chrome extension to run Kokoro TTS in your browser (local only)" src="https://preview.redd.it/e6btaupf5x6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07336b177f010ecb6599eddac44de82714049c21" title="Free Chrome extension to run Kokoro TTS in your browser (local only)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My site's traffic shot up when I offered free local Kokoro TTS. Thanks for all the love for &lt;a href="https://freevoicereader.com"&gt;https://freevoicereader.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of the people on &lt;a href="/r/TextToSpeech"&gt;r/TextToSpeech&lt;/a&gt; asked for a chrome extension. Hopefully, this will make it easier to quickly read anything in the browser.&lt;/p&gt; &lt;p&gt;Free, no ads.&lt;/p&gt; &lt;p&gt;&lt;a href="https://chromewebstore.google.com/detail/freevoice-reader-ai-text/bfhihejhhjfocdggkfpeignglimmpoho"&gt;FreeVoiceReader Chrome Extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlight text, right click and select FreeVoiceReader, it starts reading.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The difference from other TTS extensions: everything runs locally in your browser via WebGPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What that means:&lt;/p&gt; &lt;p&gt;‚Ä¢ Your text never leaves your device ‚Ä¢ No character limits or daily quotas ‚Ä¢ Works offline after initial setup (~80MB model download, cached locally) ‚Ä¢ No account required ‚Ä¢ Can export audio as WAV files&lt;/p&gt; &lt;p&gt;Happy to hear feedback or feature requests. There were a couple of UI glitches that people noticed and I have submitted a fix. Waiting for Chrome team to approve it. &lt;/p&gt; &lt;p&gt;(I have been told that the French language doesn't work - sorry to the folks who need French)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Sir9633"&gt; /u/Impressive-Sir9633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e6btaupf5x6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plf33x/free_chrome_extension_to_run_kokoro_tts_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plf33x/free_chrome_extension_to_run_kokoro_tts_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T06:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl4njj</id>
    <title>Running an LLM on a 3DS</title>
    <updated>2025-12-12T22:08:16+00:00</updated>
    <author>
      <name>/u/vreab</name>
      <uri>https://old.reddit.com/user/vreab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"&gt; &lt;img alt="Running an LLM on a 3DS" src="https://external-preview.redd.it/YXA0dDFoZXFqdTZnMayDKB9rDenP9HyWtMAfrMDzC_OwePMKvB7zq1t1dTfu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=799b8c5d268a4ce59bfb456e310307c58099033f" title="Running an LLM on a 3DS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vreab"&gt; /u/vreab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9545t3eqju6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T22:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pluy08</id>
    <title>Those who've deployed a successful self hosted RAG system, what are your hardware specs?</title>
    <updated>2025-12-13T20:18:47+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm working on a &lt;strong&gt;self hosted rag system&lt;/strong&gt; and having a difficult time figuring out the &lt;strong&gt;hardware specs&lt;/strong&gt; for the server. Feeling overwhelmed that i'll either choose a setup that won't be enough or i'll end up choosing something that's an overkill.&lt;/p&gt; &lt;p&gt;So decided it's best to ask others who've been through the same situation, those of you who've deployed a successful self hosted system, &lt;strong&gt;what are your hardware specs ?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current setup and intended use:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The idea is simple, letting the user talk to their files. They'll have the option to upload to upload a bunch of files, and then they could chat with the model about these files (documents and images).&lt;/p&gt; &lt;p&gt;I'm using docling with rapidocr for parsing documents, moondream 2for describing images., bge large embeddings v1.5 for embeddings, weaviate for vector db, and ollama qwen2.5-7b-instruct-q6 for response generation.&lt;/p&gt; &lt;p&gt;Rn i'm using Nvidia A16 (16Gb vram with 64 Gb ram) and 6 cpu cores.&lt;/p&gt; &lt;p&gt;I Would really love to hear what kind of setups others (who've successfully deployed a rag setup) are running , and what sort of latency/token speeds they're getting.&lt;/p&gt; &lt;p&gt;If you don't have an answer but you are just as interested as me to find out more about those hardware specs, please upvote, so that it would get the attention and reach out to more people.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Big&lt;/strong&gt; &lt;strong&gt;thanks&lt;/strong&gt; in advance for your help ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pluy08/those_whove_deployed_a_successful_self_hosted_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pluy08/those_whove_deployed_a_successful_self_hosted_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pluy08/those_whove_deployed_a_successful_self_hosted_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T20:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl0ojb</id>
    <title>The new monster-server</title>
    <updated>2025-12-12T19:23:12+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"&gt; &lt;img alt="The new monster-server" src="https://preview.redd.it/5kas5xaklt6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ebd2b38fa23a6f8f0aca6d1817cac736fa1e6d0" title="The new monster-server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Just wanted to share my upgraded monster-server! I have bought the largest chassi I could reasonably find (Phanteks Enthoo pro 2 server) and filled it to the brim with GPU:s to run local LLM:s alongside my homelab. I am very happy how it has evloved / turned out! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I call it the &amp;quot;Monster server&amp;quot; :)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Based on my trusted old X570 Taichi motherboard (extremely good!) and the Ryzen 3950x that I bought in 2019, that is still PLENTY fast today. I did not feel like spending a lot of money on a EPYC CPU/motherboard and new RAM, so instead I maxed out what I had. &lt;/p&gt; &lt;p&gt;The 24 PCI-e lanes are divided among the following: &lt;/p&gt; &lt;p&gt;3 GPU:s&lt;br /&gt; - 2 x RTX 3090 - both dual slot versions (inno3d RTX 3090 x3 and ASUS turbo RTX 3090)&lt;br /&gt; - 1 x RTX 4090 (an extremely chonky boi, 4 slots! ASUS TUF Gaming OC, that I got for reasonably cheap, around 1300USD equivalent). I run it on the &amp;quot;quiet&amp;quot; mode using the hardware switch hehe.&lt;/p&gt; &lt;p&gt;The 4090 runs off an M2 -&amp;gt; oculink -&amp;gt; PCIe adapter and a second PSU. The PSU is plugged in to the adapter board with its 24-pin connector and it powers on automatically when the rest of the system starts, very handy!&lt;br /&gt; &lt;a href="https://www.amazon.se/dp/B0DMTMJ95J"&gt;https://www.amazon.se/dp/B0DMTMJ95J&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Network: I have 10GB fiber internet for around 50 USD per month hehe...&lt;br /&gt; - 1 x 10GBe NIC - also connected using an M2 -&amp;gt; PCIe adapter. I had to mount this card creatively... &lt;/p&gt; &lt;p&gt;Storage:&lt;br /&gt; - 1 x Intel P4510 8TB U.2 enterprise NVMe. Solid storage for all my VM:s!&lt;br /&gt; - 4 x 18TB Seagate Exos HDD:s. For my virtualised TrueNAS. &lt;/p&gt; &lt;p&gt;RAM: 128GB Corsair Vengeance DDR4. Running at 2100MHz because I cannot get it stable when I try to run it faster, but whatever... LLMs are in VRAM anyway. &lt;/p&gt; &lt;p&gt;So what do I run on it?&lt;br /&gt; - GPT-OSS-120B, fully in VRAM, &amp;gt;100t/s tg. I did not yet find a better model, despite trying many... I use it for research, coding, and generally instead of google sometimes...&lt;br /&gt; I tried GLM4.5 air but it does not seem much smarter to me? Also slower. I would like to find a reasonably good model that I could run alongside FLUX1-dev-fp8 though, so I can generate images on the fly without having to switch. I am evaluating Qwen3-VL-32B for this&lt;/p&gt; &lt;p&gt;- Media server, Immich, Gitea, n8n&lt;/p&gt; &lt;p&gt;- My personal cloud using Seafile&lt;/p&gt; &lt;p&gt;- TrueNAS in a VM&lt;/p&gt; &lt;p&gt;- PBS for backups that is synced to a offsite PBS server at my brothers apartment&lt;/p&gt; &lt;p&gt;- a VM for coding, trying out devcontainers. &lt;/p&gt; &lt;p&gt;-&amp;gt; I also have a second server with a virtualised OPNsense VM as router. It runs other more &amp;quot;essential&amp;quot; services like PiHole, Traefik, Authelia, Headscale/tailscale, vaultwarden, a matrix server, anytype-sync and some other stuff... &lt;/p&gt; &lt;p&gt;---&lt;br /&gt; FINALLY: Why did I build this expensive machine? To make money by vibe-coding the next super-website? To cheat the stock market? To become the best AI engineer at Google? NO! Because I think it is fun to tinker around with computers, it is a hobby... &lt;/p&gt; &lt;p&gt;Thanks Reddit for teaching me all I needed to know to set this up! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5kas5xaklt6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T19:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1plryt0</id>
    <title>HP ZGX Nano G1n (DGX Spark)</title>
    <updated>2025-12-13T18:13:34+00:00</updated>
    <author>
      <name>/u/contactkv</name>
      <uri>https://old.reddit.com/user/contactkv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plryt0/hp_zgx_nano_g1n_dgx_spark/"&gt; &lt;img alt="HP ZGX Nano G1n (DGX Spark)" src="https://preview.redd.it/chxvinhbi07g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9092d56bbe74ffbe57c1e5b75e46bedc59c1f9a" title="HP ZGX Nano G1n (DGX Spark)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If someone is interested, HP's version of DGX Spark can be bought with 5% discount using coupon code: HPSMB524&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/contactkv"&gt; /u/contactkv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/chxvinhbi07g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plryt0/hp_zgx_nano_g1n_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plryt0/hp_zgx_nano_g1n_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T18:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1plv07e</id>
    <title>Optical Context Compression Is Just (Bad) Autoencoding</title>
    <updated>2025-12-13T20:21:25+00:00</updated>
    <author>
      <name>/u/simulated-souls</name>
      <uri>https://old.reddit.com/user/simulated-souls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was some recent excitement here regarding Optical Context Compression models like DeepSeek-OCR. The idea is that rendering text to an image and passing into a vision model uses fewer tokens than regular LLM pipelines, saving compute and potentially increasing context length.&lt;/p&gt; &lt;p&gt;This research shows that optical compression actually lags behind old-school autoencoders. Basically, training a model to directly compress text into fewer tokens significantly outperforms the roundabout image-based method.&lt;/p&gt; &lt;p&gt;The optical compression hype might have been premature.&lt;/p&gt; &lt;p&gt;Abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at this https URL&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simulated-souls"&gt; /u/simulated-souls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2512.03643"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv07e/optical_context_compression_is_just_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plv07e/optical_context_compression_is_just_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T20:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1plqr3q</id>
    <title>Day 6: 21 Days of Building a Small Language Model: Tokenizer</title>
    <updated>2025-12-13T17:23:08+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plqr3q/day_6_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 6: 21 Days of Building a Small Language Model: Tokenizer" src="https://b.thumbs.redditmedia.com/uJ7k-_IlkksSB_S_OoRo8lYT13W2Ee-k5qGz6pFVBJA.jpg" title="Day 6: 21 Days of Building a Small Language Model: Tokenizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you ever wondered how ChatGPT, Claude, or any other language model understands the words you type? The answer lies in a crucial first step called tokenization, a process that transforms human-readable text into something a computer can work with. Think of it as translating between two languages: the language humans speak and the language of numbers that neural networks understand.&lt;/p&gt; &lt;h1&gt;Why text needs processing&lt;/h1&gt; &lt;p&gt;At its core, a language model is a mathematical system. It performs calculations on numbers, not on letters and words. When you type &amp;quot;cat,&amp;quot; your computer sees it as just three characters: 'c', 'a', and 't'. It doesn't inherently know that &amp;quot;cat&amp;quot; refers to a furry animal or that &amp;quot;cat&amp;quot; is more similar to &amp;quot;dog&amp;quot; than to &amp;quot;airplane.&amp;quot;&lt;/p&gt; &lt;p&gt;This fundamental mismatch requires a transformation process. We need to convert text into numeric representations that neural networks can process. The journey goes like this: raw text becomes tokens, tokens become token IDs (numbers), token IDs become embeddings (dense vectors of numbers), and finally these enriched representations enter the language model where the actual understanding happens.&lt;/p&gt; &lt;h1&gt;What is a Token?&lt;/h1&gt; &lt;p&gt;A token is a chunk of text that a language model treats as a single unit. Think of tokens as building blocks that the model uses to understand language. Each token is like a piece that gets combined with others to create meaning.&lt;/p&gt; &lt;p&gt;The interesting part is that tokens can be different sizes. You could break text into individual characters, complete words, or smaller pieces of words. How you choose to break text into tokens is one of the most important decisions when building a language model, and it greatly affects how well the model works.&lt;/p&gt; &lt;p&gt;Let's explore these three main approaches to tokenization and see how each one works&lt;/p&gt; &lt;h1&gt;Three approaches to Tokenization&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s3fr8rkn907g1.png?width=664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=271780260ce5f1c6e44c616a7e810bd3dfcf8005"&gt;https://preview.redd.it/s3fr8rkn907g1.png?width=664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=271780260ce5f1c6e44c616a7e810bd3dfcf8005&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Character-Level Tokenization&lt;/h1&gt; &lt;p&gt;Character-level tokenization treats each individual character as a separate token. This is the most granular approach possible. Every letter, number, punctuation mark, and even spaces become their own tokens.&lt;/p&gt; &lt;p&gt;If you have the sentence &amp;quot;Neural networks learn patterns,&amp;quot; character-level tokenization would break it into 32 separate tokens, one for each character including spaces and punctuation. The word &amp;quot;networks&amp;quot; alone becomes 8 separate tokens.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For example&lt;/strong&gt;: Let's tokenize the sentence &amp;quot;AI learns quickly.&amp;quot;&lt;/p&gt; &lt;p&gt;Character-level tokenization:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[&amp;quot;A&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;l&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;r&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;s&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;q&amp;quot;, &amp;quot;u&amp;quot;, &amp;quot;i&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;k&amp;quot;, &amp;quot;l&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;.&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's 18 tokens for a 3-word sentence. Notice how &amp;quot;learns&amp;quot; is broken into 6 separate characters: 'l', 'e', 'a', 'r', 'n', 's', losing the word's meaning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny vocabulary&lt;/strong&gt;: You only need about 50 to 200 characters for most languages, making the model's vocabulary very small&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No unknown tokens&lt;/strong&gt;: Since you're working at the character level, any text can be tokenized. There are no words that can't be represented.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Language agnostic&lt;/strong&gt;: Works for any language without modification&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Loss of semantic meaning&lt;/strong&gt;: This is the biggest problem. When words are broken into individual characters, the model loses the ability to see words as meaningful units. The word &amp;quot;cat&amp;quot; becomes just three unrelated characters 'c', 'a', and 't' with no inherent meaning. The model must learn from scratch that these character sequences form meaningful words, losing the natural semantic structure of language&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very long sequences&lt;/strong&gt;: A single word becomes multiple tokens, dramatically increasing the length of sequences the model must process&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High computational cost&lt;/strong&gt;: Processing longer sequences requires exponentially more computation, making this approach expensive&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Harder to learn&lt;/strong&gt;: The model must learn to combine many characters into meaningful words, which requires more training data and computation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Character-level tokenization is rarely used in modern language models because of its computational inefficiency. It's mainly useful for research or when dealing with languages that don't have clear word boundaries.&lt;/p&gt; &lt;h1&gt;Word-Level Tokenization&lt;/h1&gt; &lt;p&gt;Word-level tokenization treats each complete word as a separate token. This matches how humans naturally think about language, with each word being a meaningful unit.&lt;/p&gt; &lt;p&gt;The same sentence &amp;quot;Neural networks learn patterns&amp;quot; becomes just 4 tokens, one for each word. Each token represents a complete semantic unit, which makes it easier for the model to understand meaning.&lt;/p&gt; &lt;p&gt;For example: Let's tokenize the sentence &amp;quot;AI learns quickly.&amp;quot;&lt;/p&gt; &lt;p&gt;Word-level tokenization:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[&amp;quot;AI&amp;quot;, &amp;quot;learns&amp;quot;, &amp;quot;quickly&amp;quot;, &amp;quot;.&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's just 4 tokens. Each word is preserved as a complete unit with its meaning intact. However, if the vocabulary doesn't include &amp;quot;learns&amp;quot; or &amp;quot;quickly,&amp;quot; the model cannot represent them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Meaningful units&lt;/strong&gt;: Each token represents a complete word with semantic meaning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Shorter sequences&lt;/strong&gt;: Much fewer tokens per sentence compared to character-level tokenization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient representation&lt;/strong&gt;: Common words are single tokens, making processing faster&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: Aligns with human understanding of language&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The disadvantages&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Large vocabulary&lt;/strong&gt;: Requires tens or hundreds of thousands of tokens to cover common words, proper nouns, technical terms, and domain-specific vocabulary&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The unknown word problem&lt;/strong&gt;: This is a critical limitation. Rare words, misspellings, or new words not in the vocabulary cannot be represented. Even word variations like &amp;quot;learns,&amp;quot; &amp;quot;learned,&amp;quot; or &amp;quot;learning&amp;quot; are treated as completely different words from &amp;quot;learn&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter overhead&lt;/strong&gt;: Large vocabulary means a large embedding layer, consuming significant memory and computation resources&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The biggest challenge with word level tokenization is unknown word problem. Imagine a model trained with a vocabulary that includes &amp;quot;learn&amp;quot; but not &amp;quot;learns,&amp;quot; &amp;quot;learned,&amp;quot; or &amp;quot;learning.&amp;quot; When the model encounters these variations during inference, it cannot represent them, even though they're clearly related to a known word. This means the model would need to see every possible form of every word during training, which is an impossible requirement. This fundamental limitation is why modern models moved away from word-level tokenization.&lt;/p&gt; &lt;h1&gt;Subword-Level Tokenization&lt;/h1&gt; &lt;p&gt;Subword-level tokenization breaks words into smaller units that can be combined to form any word. This approach balances the benefits of word-level (meaningful units) with character-level (comprehensive coverage).&lt;/p&gt; &lt;p&gt;Common words remain as single tokens, while rare or unknown words are broken into multiple subword units. The vocabulary contains both complete words and subword fragments like prefixes, suffixes, and common character sequences.&lt;/p&gt; &lt;p&gt;For example, the word &amp;quot;efficiently&amp;quot; might be split into [&amp;quot;efficient&amp;quot;, &amp;quot;ly&amp;quot;] because &amp;quot;ly&amp;quot; is a common suffix that appears in many words (quickly, slowly, carefully, etc.). The word &amp;quot;unhappiness&amp;quot; might be tokenized as [&amp;quot;un&amp;quot;, &amp;quot;happiness&amp;quot;] or even further decomposed as [&amp;quot;un&amp;quot;, &amp;quot;happy&amp;quot;, &amp;quot;ness&amp;quot;].&lt;/p&gt; &lt;p&gt;A subword tokenizer with 50,000 tokens might contain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Complete common words: &amp;quot;the&amp;quot;, &amp;quot;and&amp;quot;, &amp;quot;machine&amp;quot;, &amp;quot;learning&amp;quot;, &amp;quot;neural&amp;quot;&lt;/li&gt; &lt;li&gt;Common prefixes: &amp;quot;un&amp;quot;, &amp;quot;re&amp;quot;, &amp;quot;pre&amp;quot;, &amp;quot;sub&amp;quot;&lt;/li&gt; &lt;li&gt;Common suffixes: &amp;quot;ly&amp;quot;, &amp;quot;ness&amp;quot;, &amp;quot;ing&amp;quot;, &amp;quot;ed&amp;quot;, &amp;quot;tion&amp;quot;&lt;/li&gt; &lt;li&gt;Common character sequences: &amp;quot;arch&amp;quot;, &amp;quot;itect&amp;quot;, &amp;quot;ure&amp;quot;, &amp;quot;trans&amp;quot;, &amp;quot;form&amp;quot;&lt;/li&gt; &lt;li&gt;Special tokens for formatting and control&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Balanced vocabulary&lt;/strong&gt;: Typically 10,000 to 50,000 tokens, much smaller than word-level but more comprehensive than character-level&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No unknown words&lt;/strong&gt;: Any word can be represented by combining subword units&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient for common words&lt;/strong&gt;: Frequent words remain single tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handles rare words&lt;/strong&gt;: Uncommon words are broken into known subword units&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Language flexibility&lt;/strong&gt;: Works well across different languages and domains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Variable token count&lt;/strong&gt;: Rare words become multiple tokens, increasing sequence length&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Less intuitive&lt;/strong&gt;: Subword units don't always align with linguistic boundaries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Implementation complexity&lt;/strong&gt;: Requires training a tokenizer on large corpora to learn optimal subword units&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Subword tokenization, especially BPE (Byte Pair Encoding), is the standard choice for modern language models. It's used by GPT-3, GPT-4, LLaMA, and virtually all state-of-the-art language model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comparison Summary&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To illustrate the differences, consider tokenizing the technical phrase &amp;quot;backpropagation algorithm&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Character level&lt;/strong&gt;: 22 tokens, one for each character including spaces&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Word level&lt;/strong&gt;: 2 tokens, [&amp;quot;backpropagation&amp;quot;, &amp;quot;algorithm&amp;quot;] (if both words are in vocabulary, otherwise unknown word problem)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Subword level&lt;/strong&gt;: 3 to 4 tokens, [&amp;quot;back&amp;quot;, &amp;quot;propagation&amp;quot;, &amp;quot;algorithm&amp;quot;] or [&amp;quot;backprop&amp;quot;, &amp;quot;agation&amp;quot;, &amp;quot;algorithm&amp;quot;] (depending on learned subword units)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lk28ur2q907g1.png?width=736&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0ab45cb66eb4b56ec73d3f4e91de762949471a7"&gt;https://preview.redd.it/lk28ur2q907g1.png?width=736&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0ab45cb66eb4b56ec73d3f4e91de762949471a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most modern language models use subword tokenization because it provides the best balance: common words remain as single tokens (efficient), while rare words can be represented by combining known subword units (comprehensive).&lt;/p&gt; &lt;p&gt;üí° NOTE: You can visualize this interactively using tools like&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9ushs4lr907g1.png?width=1882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff14bcd7c91b9f798e7a0878164c8ae266bfed02"&gt;https://preview.redd.it/9ushs4lr907g1.png?width=1882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff14bcd7c91b9f798e7a0878164c8ae266bfed02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://tiktokenizer.vercel.app/"&gt;https://tiktokenizer.vercel.app&lt;/a&gt;, which shows exactly how different models tokenize text&lt;/p&gt; &lt;p&gt;‚å®Ô∏è If you want to code along, check out the&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Google Colab notebook: &lt;a href="https://colab.research.google.com/drive/13o8x0AVXUgiMsr85kI9pGGTqLuY4JUOZ?usp=sharing"&gt;https://colab.research.google.com/drive/13o8x0AVXUgiMsr85kI9pGGTqLuY4JUOZ?usp=sharing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub repository: &lt;a href="https://github.com/ideaweaver-ai/Building-Small-Language-Model-from-Scratch-A-Practical-Guide-Book"&gt;https://github.com/ideaweaver-ai/Building-Small-Language-Model-from-Scratch-A-Practical-Guide-Book&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tokenization is the first critical step in the journey from human-readable text to AI understanding. It transforms raw text into discrete units called tokens, which are then mapped to integer token IDs. The choice of tokenization approach, whether character-level, word-level, or subword-level, has profound impacts on model size, performance, and computational efficiency.&lt;/p&gt; &lt;p&gt;Subword-level tokenization, specifically BPE (Byte Pair Encoding), has emerged as the standard approach for modern language models because it provides the optimal balance between vocabulary efficiency and sequence efficiency. By breaking words into subword units, BPE allows common words to remain as single tokens while enabling rare or unknown words to be represented by combining known subword units. This approach eliminates the unknown word problem that plagues word-level tokenization while avoiding the computational inefficiency of character-level tokenization.&lt;/p&gt; &lt;p&gt;Understanding tokenization is essential for anyone working with language models, whether you're building your own model, fine-tuning an existing one, or simply trying to understand how these remarkable systems work. The choices made at the tokenization stage ripple through every aspect of the model, affecting everything from memory usage to computational speed to the model's ability to understand and generate text.&lt;/p&gt; &lt;p&gt;The next time you interact with a language model, remember that behind every word you type, there's a sophisticated tokenization process breaking your text into tokens, converting those tokens into numbers, and transforming those numbers into rich vector representations that capture meaning, context, and relationships. It's this transformation that makes the magic of AI language understanding possible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plqr3q/day_6_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plqr3q/day_6_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plqr3q/day_6_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T17:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pllq0a</id>
    <title>RDMA over Thunderbolt 5 is now possible on MacOS Tahoe 26.2</title>
    <updated>2025-12-13T13:44:37+00:00</updated>
    <author>
      <name>/u/HaAtidChai</name>
      <uri>https://old.reddit.com/user/HaAtidChai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pllq0a/rdma_over_thunderbolt_5_is_now_possible_on_macos/"&gt; &lt;img alt="RDMA over Thunderbolt 5 is now possible on MacOS Tahoe 26.2" src="https://external-preview.redd.it/2uG3mp1grku1teaynrks5JCfpigHSeN1SIYEwOaBaf4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8713859505df19de87039a1230aef3e5deaf349" title="RDMA over Thunderbolt 5 is now possible on MacOS Tahoe 26.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apple quietly released this. This enables Mac clusters to run tensor parallelism over MLX on larger memory pool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HaAtidChai"&gt; /u/HaAtidChai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.apple.com/documentation/macos-release-notes/macos-26_2-release-notes#RDMA-over-Thunderbolt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pllq0a/rdma_over_thunderbolt_5_is_now_possible_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pllq0a/rdma_over_thunderbolt_5_is_now_possible_on_macos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T13:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1plcrg8</id>
    <title>This is how open ai is advertising them selfs on reddit‚Ä¶. They are doomed</title>
    <updated>2025-12-13T04:38:30+00:00</updated>
    <author>
      <name>/u/ThinkExtension2328</name>
      <uri>https://old.reddit.com/user/ThinkExtension2328</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holly god , after months of telling us they are the best and they will achieve agi and how open models are dangerous. This is how open ai is advertising to normies? Yea open ai is doomed &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThinkExtension2328"&gt; /u/ThinkExtension2328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zaedu7ehw6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T04:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1plv12w</id>
    <title>[Idea] Given the leak that was made public before quickly being removed again - CAN a service be built that instantly downloads any upload to HF and seeds it? SHOULD this be done?</title>
    <updated>2025-12-13T20:22:32+00:00</updated>
    <author>
      <name>/u/Competitive_Wait_267</name>
      <uri>https://old.reddit.com/user/Competitive_Wait_267</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See title ;) Further points:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; Models from NVIDIA were uploaded to HF yesterday that very likely were not intended to be made public yet (more precisely: The parent folder was uploaded to hf instead of the model itself, it seems). More context here: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;IANAL, so if in doubt, this is all hypothetical and respecting the law in each relevant country of course. (Although I think you can hardly blame users to download publicly available data. Otherwise, taking it to its logical conclusion, we might not be permitted to store &lt;em&gt;anything&lt;/em&gt; being made public, because &lt;em&gt;every&lt;/em&gt; source might change, get taken down, whatever at &lt;em&gt;some&lt;/em&gt; point in the future...)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I understand and sympathize with the decision of the person who took the model down themselves. At the end of the day, there is at least one human behind every mouse slip. What I want to bring up is more along the lines of establishing automatisms for events like this.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Further points (will edit this section once as long as discussion is ongoing. &lt;strong&gt;Current Edit: 1.&lt;/strong&gt; Grabbing some food after making this edit)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The legal situation of making models available to other for unlicensed models might be a problem, as was pointed in &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/ntvfdzq/"&gt;this comment&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I think the technical question &amp;quot;How can a community of hobbyists store a big amount of LLMs (most of the LLMs being somewhat familiar to each other, i.e. finetunes, newer versions, ...)?&amp;quot; can be viewed independently from &amp;quot;would it be a good idea to mirror models from HF? (if even legal?)&amp;quot;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Wait_267"&gt; /u/Competitive_Wait_267 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T20:22:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1plxdeg</id>
    <title>Mistral 3 llama.cpp benchmarks</title>
    <updated>2025-12-13T22:05:08+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are some benchmarks using a few different GPUs. I'm using unsloth models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ministral 3 14B Instruct 2512 on Hugging Face&lt;/p&gt; &lt;p&gt;HF list &amp;quot; The largest model in the Ministral 3 family, Ministral 3 14B offers frontier capabilities and performance comparable to its larger Mistral Small 3.2 24B counterpart. A powerful and efficient language model with vision capabilities.&amp;quot;&lt;/p&gt; &lt;p&gt;System is Kubuntu OS&lt;/p&gt; &lt;p&gt;All benchmarks done using &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10879"&gt;Vulkan&lt;/a&gt; backend &lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b7273/llama-b7273-bin-ubuntu-vulkan-x64.tar.gz"&gt;build: c4c10bfb8 (7273)&lt;/a&gt; &lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/Ministral-3-14B-Instruct-2512-UD-Q6_K_XL.gguf?download=true"&gt;Q6_K_XL&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model &lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B Q6_K&lt;/td&gt; &lt;td align="left"&gt; 10.62 GiB&lt;/td&gt; &lt;td align="left"&gt;13.51 B&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Ministral-3-14B-Instruct-2512-UD-Q6_K_XL.gguf or Ministral-3-14B-Reasoning-2512-Q6_K_L.gguf&lt;/p&gt; &lt;p&gt;AMD Radeon &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt;RX 7900 GRE&lt;/a&gt; 16GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;766.85 ¬± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;43.51 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Ryzen &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;6800H with 680M &lt;/a&gt;on 64GB DDR5&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;117.81 ¬± 1.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;3.84 ¬± 0.30&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX-1080 Ti &lt;/a&gt;11GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;194.15 ¬± 0.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;26.64 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX1080 Ti&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/p102-101.c3284"&gt;P102-100&lt;/a&gt; 21GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;175.58 ¬± 0.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;25.11 ¬± 0.11&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX-1080 Ti&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; 19GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;147.12 ¬± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;22.00 ¬± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nvidia &lt;a href="https://www.techpowerup.com/gpu-specs/p102-101.c3284"&gt;P102-100&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; 18GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;139.66 ¬± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.84 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080.c2839"&gt;GTX-1080&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; 16GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;132.84 ¬± 2.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.54 ¬± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; x 3 total 24GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;114.89 ¬± 1.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;17.06 ¬± 0.20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Combined sorted by tg128 t/s speed&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AMD Radeon RX 7900 GRE (16GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;766.85&lt;/td&gt; &lt;td align="left"&gt;43.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 Ti (11GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;194.15&lt;/td&gt; &lt;td align="left"&gt;26.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 Ti + P102-100 (21GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;175.58&lt;/td&gt; &lt;td align="left"&gt;25.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 Ti + GTX 1070 (19GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;147.12&lt;/td&gt; &lt;td align="left"&gt;22.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nvidia P102-100 + GTX 1070 (18GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;139.66&lt;/td&gt; &lt;td align="left"&gt;20.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1070 √ó 3 (24GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;114.89&lt;/td&gt; &lt;td align="left"&gt;17.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 + GTX 1070 (16GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;132.84&lt;/td&gt; &lt;td align="left"&gt;15.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ryzen 6800H with 680M iGPU&lt;/td&gt; &lt;td align="left"&gt;117.81&lt;/td&gt; &lt;td align="left"&gt;3.84&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nvidia P102-100 unable to run without using &lt;code&gt;-ngl 39&lt;/code&gt; offload flag&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Nvidia P102-100&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;127.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nvidia P102-100&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plxdeg/mistral_3_llamacpp_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plxdeg/mistral_3_llamacpp_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plxdeg/mistral_3_llamacpp_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T22:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1plewrk</id>
    <title>NVIDIA gpt-oss-120b Eagle Throughput model</title>
    <updated>2025-12-13T06:42:30+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"&gt; &lt;img alt="NVIDIA gpt-oss-120b Eagle Throughput model" src="https://external-preview.redd.it/qdVRXmmV8Rf9W9JXBulc6Wu3niNm-zSxeaMwUkpajKs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a65c4d10380bd77d1bb995e698c64823e1fb437" title="NVIDIA gpt-oss-120b Eagle Throughput model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GPT-OSS-120B-Eagle3-throughput is an &lt;strong&gt;optimized speculative decoding module&lt;/strong&gt; built on top of the &lt;em&gt;OpenAI gpt-oss-120b&lt;/em&gt; base model, designed to improve throughput during text generation. &lt;/li&gt; &lt;li&gt;It uses NVIDIA‚Äôs &lt;strong&gt;Eagle3 speculative decoding&lt;/strong&gt; approach with the Model Optimizer to predict a single draft token efficiently, making it useful for high-concurrency inference scenarios where fast token generation is a priority. &lt;/li&gt; &lt;li&gt;The model is licensed under the &lt;strong&gt;nvidia-open-model-license&lt;/strong&gt; and is intended for commercial and non-commercial use in applications like AI agents, chatbots, retrieval-augmented generation (RAG) systems, and other instruction-following tasks. &lt;a href="https://huggingface.co/nvidia/gpt-oss-120b-Eagle3-throughput"&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/gpt-oss-120b-Eagle3-throughput"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T06:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1plpc6h</id>
    <title>Mistral 3 Large is DeepSeek V3!?</title>
    <updated>2025-12-13T16:24:39+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt; &lt;img alt="Mistral 3 Large is DeepSeek V3!?" src="https://b.thumbs.redditmedia.com/yYDI9PVijp_cl2RjrFgG1_MnObuOyb3iYTmzt8uL1_I.jpg" title="Mistral 3 Large is DeepSeek V3!?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Mistral 3 and DeepSeek V3.2, we got two major open-weight LLMs this month already. I looked into DeepSeek V3.2 last week and just caught up with reading through the config of the Mistral 3 architecture in more detail. &lt;/p&gt; &lt;p&gt;Interestingly, based on &lt;a href="https://mistral.ai/news/mistral-3"&gt;their official announcement post&lt;/a&gt;, Mistral 3 and DeepSeek V3.2 have an almost identical size, 671B and 673B, which makes for an interesting comparison, I thought! &lt;/p&gt; &lt;p&gt;Unfortunately, there is no technical report on Mistral 3 that contains more information about the model development. However, since it‚Äôs an open-weight model, we do have the &lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4/blob/main/params.json"&gt;model weights on the HuggingFace Model Hub&lt;/a&gt;, though. So, l was taking a closer look at Mistral 3 Large yesterday, and it turns out to be exactly the same architecture as DeepSeek V3/V3.1. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/70lznwrbzz6g1.png?width=2846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aca49968a91f54b80594024ab98b9cd968be8bdf"&gt;https://preview.redd.it/70lznwrbzz6g1.png?width=2846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aca49968a91f54b80594024ab98b9cd968be8bdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The only difference is that they increased the size of the experts by a factor of 2 while decreasing the number of experts by the same factor. This keeps the number of expert parameters constant, but it should help a bit with latency (1 big expert is faster than 2 smaller experts since there are fewer operations to deal with). &lt;/p&gt; &lt;p&gt;I think that Mistral 3 reusing the DeepSeek V3 architecture is totally fair in the spirit of open source. I am just surprised by it, because I haven't seen anyone mentioning that yet. &lt;/p&gt; &lt;p&gt;However, while it‚Äôs effectively the same architecture, it is likely the Mistral team trained Mistral 3 from scratch rather than initializing it from DeepSeek V3 and further training it, because Mistral uses its own tokenizer. &lt;/p&gt; &lt;p&gt;Next to Kimi K2, Mistral 3 Large is now the second major model to use the DeepSeek V3 architecture. However, where the Kimi K2 team scaled up the model size from 673B to 1 trillion, the Mistral 3 team only changed the expert size ratio and added a vision encoder for multimodal support. But yes, why not? I think DeepSeek V3 is a pretty solid architecture design, plus it has these nice MoE and MLA efficiency aspects to it. So, why change what ain‚Äôt broke? A lot of the secret sauce these days is in the training pipeline as well as the inference scaling strategies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T16:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1plwgun</id>
    <title>8x RTX Pro 6000 server complete</title>
    <updated>2025-12-13T21:25:43+00:00</updated>
    <author>
      <name>/u/koushd</name>
      <uri>https://old.reddit.com/user/koushd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt; &lt;img alt="8x RTX Pro 6000 server complete" src="https://b.thumbs.redditmedia.com/W2GEGsEUMbENTMcyKk5kW114aYR7rHtS0S5yetky0Lc.jpg" title="8x RTX Pro 6000 server complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: 768 GB VRAM via 8x RTX Pro 6000 (4 Workstation, 4 Max-Q) + Threadripper PRO 9955WX + 384 GB RAM&lt;/p&gt; &lt;p&gt;Longer:&lt;/p&gt; &lt;p&gt;I've been slowly upgrading my GPU server over the past few years. I initially started out using it to train vision models for another project, and then stumbled into my current local LLM obsession.&lt;/p&gt; &lt;p&gt;In reverse order:&lt;/p&gt; &lt;p&gt;Pic 5: Initially was using only a single 3080, which I upgraded to a 4090 + 3080. Running on an older 10900k Intel system.&lt;/p&gt; &lt;p&gt;Pic 4: But the mismatched sizes for training batches and compute was problematic, so I upgraded to double 4090s and sold off the 3080. They were packed in there, and during a training run I ended up actually overheating my entire server closet, and all the equipment in there crashed. When I noticed something was wrong and opened the door, it was like being hit by the heat of an industrial oven.&lt;/p&gt; &lt;p&gt;Pic 3: 2x 4090 in their new home. Due to the heat issue, I decided to get a larger case and a new host that supported PCIe 5.0 and faster CPU RAM, the AMD 9950x. I ended up upgrading this system to dual RTX Pro 6000 Workstation edition (not pictured).&lt;/p&gt; &lt;p&gt;Pic 2: I upgraded to 4x RTX Pro 6000. This is where problems started happening. I first tried to connect them using M.2 risers and it would not POST. The AM5 motherboard I had couldn't allocate enough IOMMU addressing and would not post with the 4th GPU, 3 worked fine. There are consumer motherboards out there that could likely have handled it, but I didn't want to roll the dice on another AM5 motherboard as I'd rather get a proper server platform.&lt;/p&gt; &lt;p&gt;In the meantime, my workaround was to use 2 systems (brought the 10900k out of retirement) with 2 GPUs each in pipeline parallel. This worked, but the latency between systems chokes up token generation (prompt processing was still fast). I tried using 10Gb DAC SFP and also Mellanox cards for RDMA to reduce latency, but gains were minimal. Furthermore, powering all 4 means they needed to be on separate breakers (2400w total) since in the US the max load you can put through 120v 15a is ~1600w.&lt;/p&gt; &lt;p&gt;Pic 1: 8x RTX Pro 6000. I put a lot more thought into this before building this system. There were more considerations, and it became a many months long obsession planning the various components: motherboard, cooling, power, GPU connectivity, and the physical rig.&lt;/p&gt; &lt;p&gt;GPUs: I considered getting 4 more RTX Pro 6000 Workstation Editions, but powering those would, by my math, require a third PSU. I wanted to keep it 2, so I got Max Q editions. In retrospect I should have gotten the Workstation editions as they run much quieter and cooler, as I could have always power limited them.&lt;/p&gt; &lt;p&gt;Rig: I wanted something fairly compact and stackable that I could directly connect 2 cards on the motherboard and use 3 bifurcating risers for the other 6. Most rigs don't support taller PCIe cards on the motherboard directly and assume risers will be used. Options were limited, but I did find some generic &amp;quot;EO3&amp;quot; stackable frames on Aliexpress. The stackable case also has plenty of room for taller air coolers.&lt;/p&gt; &lt;p&gt;Power: I needed to install a 240V outlet; switching from 120V to 240V was the only way to get ~4000W necessary out of a single outlet without a fire. Finding 240V high-wattage PSUs was a bit challenging as there are only really two: the Super Flower Leadex 2800W and the Silverstone Hela 2500W. I bought the Super Flower, and its specs indicated it supports 240V split phase (US). It blew up on first boot. I was worried that it took out my entire system, but luckily all the components were fine. After that, I got the Silverstone, tested it with a PSU tester (I learned my lesson), and it powered on fine. The second PSU is the Corsair HX1500i that I already had.&lt;/p&gt; &lt;p&gt;Motherboard: I kept going back and forth between using a Zen5 EPYC or Threadripper PRO (non-PRO does not have enough PCI lanes). Ultimately, the Threadripper PRO seemed like more of a known quantity (can return to Amazon if there were compatibility issues) and it offered better air cooling options. I ruled out water cooling, because the small chance of a leak would be catastrophic in terms of potential equipment damage. The Asus WRX90 had a lot of concerning reviews, so the Asrock WRX90 was purchased, and it has been great. Zero issues on POST or RAM detection on all 8 RDIMMs, running with the expo profile.&lt;/p&gt; &lt;p&gt;CPU/Memory: The cheapest Pro Threadripper, the 9955wx with 384GB RAM. I won't be doing any CPU based inference or offload on this.&lt;/p&gt; &lt;p&gt;Connectivity: The board has 7 PCIe 5.0 x16 cards. At least 1 bifurcation adapter would be necessary. Reading up on the passive riser situation had me worried there would be signal loss at PCIe 5.0 and possibly even 4.0. So I ended up going the MCIO route and bifurcated 3 5.0 lanes. A PCIe switch was also an option, but compatibility seemed sketchy and it's costs $3000 by itself. The first MCIO adapters I purchased were from ADT Link; however, they had two significant design flaws: The risers are powered via the SATA peripheral power, which is a fire hazard as those cable connectors/pins are only rated for 50W or so safely. Secondly, the PCIe card itself does not have enough clearance for the heat pipe that runs along the back of most EPYC and Threadripper boards just behind the PCI slots on the back of the case. Only 2 slots were usable. I ended up returning the ADT Link risers and buying several Shinreal MCIO risers instead. They worked no problem.&lt;/p&gt; &lt;p&gt;Anyhow, the system runs great (though loud due to the Max-Q cards which I kind of regret). I typically use Qwen3 Coder 480b fp8, but play around with GLM 4.6, Kimi K2 Thinking, and Minimax M2 at times. Personally I find Coder and M2 the best for my workflow in Cline/Roo. Prompt processing is crazy fast, I've seen VLLM hit around ~24000 t/s at times. Generation is still good for these large models, despite it not being HBM, around 45-100 t/s depending on model.&lt;/p&gt; &lt;p&gt;Happy to answer questions in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koushd"&gt; /u/koushd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1plwgun"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T21:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1plng6f</id>
    <title>Qwen3 Next generation optimization</title>
    <updated>2025-12-13T15:04:40+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt; &lt;img alt="Qwen3 Next generation optimization" src="https://external-preview.redd.it/lCjR6IsnTKIxBcWFwTVHflX0Ssz6EaHSVZWuf8jfKVA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb67476532fe0a6cc1d75e633320c04f4f773566" title="Qwen3 Next generation optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of people were requesting dedicated optimizations, so here they are.&lt;/p&gt; &lt;p&gt;I added an optimized autoregressive delta net computation that short-circuits all the recurrect decay calculation because for `n_seq_tokens = 1` it all collapses. I also made sure to specifically optimize out all unneeded reshapes / conts in that version.&lt;/p&gt; &lt;p&gt;The end result is a 40% generation speed upgrade on my box. If you want, you can try it out and tell me how it works on your end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1plnuqu</id>
    <title>OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark.</title>
    <updated>2025-12-13T15:22:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt; &lt;img alt="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." src="https://preview.redd.it/l93slaq9oz6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bab51a5045543fef0a9b8a60e5d6a113bc1f0cef" title="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l93slaq9oz6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
