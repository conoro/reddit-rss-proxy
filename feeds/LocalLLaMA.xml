<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-07T18:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1naiud3</id>
    <title>Do local LLMs do almost as well with code generation as the big boys?</title>
    <updated>2025-09-07T03:27:39+00:00</updated>
    <author>
      <name>/u/Middle_Reception286</name>
      <uri>https://old.reddit.com/user/Middle_Reception286</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Sort of a &amp;quot;startup&amp;quot; wears all hats person like many are these days with AI/LLM tools at our disposal.&lt;/p&gt; &lt;p&gt;I pay for the $200 month Anthropic plan because CC (cli mode) did quite well on some tasks, and I was always running out of context with the $20 plan and even the $100 plan. However, as many are starting to say on a few llm channels, it seems like it has gotten worse. Not sure how accurate that is or not. BUT.. that, the likely growing costs, and experimenting with taking the output of CC as input to ChatGPT5 and Gemini 2.5 Pro (using some credits I have left from playing with KiloCode before I switched to CC Max).. I have been seeing that what CC puts out is often a bunch of fluff. It says all these great things like &amp;quot;It's 100% working, its the best ever&amp;quot; and then I try to use my code and find out its mostly mock, fake or CC generated the values instead of actually ran some code and got results from the code running.&lt;/p&gt; &lt;p&gt;It got me thinking. The monthly costs to use 2 or 3 of these things starts to add up for those of us not lucky enough to be employed and/or a company paying for it. Myself, I am unemployed for almost 2 years now and decided I want to try to build my dream passion project that I have vetted with several colleagues and they are all agreeing it is much needed and could very well be very valuable. So I figure.. use AI + my experience/knowledge. I can't afford to hire a team, and frankly my buddy in India who runs a company to farm out works was looking at $5K a month per developer.. so yah.. that's like 6+ months of multiple AIs cost.. figured not worth it for one developer month of a likely &amp;quot;meh&amp;quot; coder that would require many months or more to build what I am now working on with AI.&lt;/p&gt; &lt;p&gt;SO.. per my subject (sorry had to add some context).. my thought is.. would it benefit me to run a local LLM like DeepSeek or Meta or Qwen 3.. but buying the hardware.. in this case it seems like the Mac M3 Studio Ultra (hoping they announce an M4 Studio Ultra in a few days) with 512GB RAM or even the lower cpu/256GB ram would be a good way to go. Before anyone says &amp;quot;Dude.. thats $6K to $10K depending on configuration.. that's a LOT of cloud AI you can afford&amp;quot;. My argument is that it seems like using Claude + ChatGPT + Gemini.. to bounce results between them is at least getting me a bit better code out of CC than CC is on its own. I have a few uses for running a local LLM for my products that I am working on, but I am wondering if running the larger models + much larger context windows will be a LOT better than using LM Studio on my desktop with 16GB of gpu VRAM. Is the results from these larger models + more context window going to be that much better? OR is it a matter of a few percentage points better? I read for example the FP16 is not any better than Q8 in terms of quality.. like literally about .1% or less better and not all the time. Given that open source models are getting better all the time, free to download/use, I am really curious if they could be coerced with the right prompting to put code out as good as claude code or ChatGPT 5 or Gemini 2.5Pro if I had a larger 200GB to 400GB model and 1mil+ context window. &lt;/p&gt; &lt;p&gt;I've seen some bits of info on this topic.. that yes they can be every bit as good or they are not as good because the big 3 (or so) have TBs of model size and massive amounts of hardware ($billions).. so of course a $5K to $10K Studio + OS large model may not be as good.. but is it good enough that you could rely on it to do initial ideas/draft code, then feed that code to Claude, ChatGPT, Gemini. &lt;/p&gt; &lt;p&gt;But the bigger ask is.. do you basically get really good overall quality code if you use multiple models against each other.. or.. working together. Like giving the prompt to local LLM. Generate a bunch of code. Then feed the project to ChatGPT. Have it come back with some response. Then tell Claude (this is what ChatGPT and my DeepSeek said.. what do you think..) and so on. My hope is some sort of &amp;quot;cross response&amp;quot; between them results in one of them (ideally local would be great to avoid cloud costs) coming up with great quality code that mostly works.&lt;/p&gt; &lt;p&gt;I do realize I have to review/test code.. I am not relying on the generated stuff 100%. However, I am working in a few languages two of which I know jack shit about, three of which I know a little bit of and 2 I know very well. So I am sort of relying on the knowledge of AI for most of this stuff and applying my experience/knowledge to try to re-prompt to get better results. &lt;/p&gt; &lt;p&gt;Maybe it's all wishful thinking. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Middle_Reception286"&gt; /u/Middle_Reception286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naiud3/do_local_llms_do_almost_as_well_with_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naiud3/do_local_llms_do_almost_as_well_with_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naiud3/do_local_llms_do_almost_as_well_with_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T03:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1naupv2</id>
    <title>Universal Deep Research: Bring Your Own Model and Strategy</title>
    <updated>2025-09-07T14:32:13+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.00244"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naupv2/universal_deep_research_bring_your_own_model_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naupv2/universal_deep_research_bring_your_own_model_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T14:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nay7wk</id>
    <title>Need a free, simple tool of whisper-v3-turbo speech-to-text for macOS</title>
    <updated>2025-09-07T16:48:39+00:00</updated>
    <author>
      <name>/u/SuddenWerewolf7041</name>
      <uri>https://old.reddit.com/user/SuddenWerewolf7041</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking a lot for a good tool that helps me dictate and also transcribe all the desktop audio to help with my accessibility issue. So far I had no luck whatsoever with any of the free tools, all of them just give you access to the whisper base or tiny/small which is nothing compared to the v3/turbo. My macOS can handle it, but the problem is that all the tools I used require payment to upgrade the model (which is annoying because technically I am running it on my MacBook, not in the cloud).&lt;/p&gt; &lt;p&gt;I would be very thankful if you have some tips. I need basically an always-on or live transcription feature (where at least there would be a differentiation between my microphone vs audio, no need for advanced diarization).&lt;/p&gt; &lt;p&gt;I understand that WhisperKit Pro has a commercial license, thus the reason why it's paid. But come on, it's year 2025 and it's been so many years since we have Whisper model and yet no decent free implementation of a (free and open source) model....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuddenWerewolf7041"&gt; /u/SuddenWerewolf7041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nay7wk/need_a_free_simple_tool_of_whisperv3turbo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nay7wk/need_a_free_simple_tool_of_whisperv3turbo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nay7wk/need_a_free_simple_tool_of_whisperv3turbo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1naf93r</id>
    <title>2x MI50 32GB Quant Speed Comparison (Mistral 3.2 24B, llama.cpp, Vulkan)</title>
    <updated>2025-09-07T00:24:56+00:00</updated>
    <author>
      <name>/u/OUT_OF_HOST_MEMORY</name>
      <uri>https://old.reddit.com/user/OUT_OF_HOST_MEMORY</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"&gt; &lt;img alt="2x MI50 32GB Quant Speed Comparison (Mistral 3.2 24B, llama.cpp, Vulkan)" src="https://b.thumbs.redditmedia.com/FV3rVrLKjx3h5viB13673innMBOKrcScYojGFAgGn5g.jpg" title="2x MI50 32GB Quant Speed Comparison (Mistral 3.2 24B, llama.cpp, Vulkan)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All tests were run on the same system with 2x MI50 32GB from AliExpress, with a fixed VBios found on this subreddit. Llama.cpp was compiled with vulkan support as that is what I use for all of my GPUs regardless of vendor.&lt;/p&gt; &lt;p&gt;Quants for Mistral 3.2 Small 2506 24B were sourced from both Bartowski and Unsloth, when there were quants provided by both the values were averaged as I found that there was negligible difference in speed and size between the providers.&lt;/p&gt; &lt;p&gt;Every quant was run through 8 tests using llama-bench, with the variables in play being Flash Attention On/Off, Depth of either 0 or 32768, and the test type PP512 or TG128. Testing took approximately 62 hours to complete.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2b2e0xvwmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a3d7a2ff32cbcca43de514b1a88a25fc3751fe"&gt;Chart 1: Prompt Processing in Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0tltrr9xmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9011470110b826a17a7e4b4e10d5f37c61bb2295"&gt;Chart 2: Token Generation in Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xmrwqghbxmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce8c383a27c8fd05e356a97851b49179b4e3703"&gt;Chart 3: Prompt Processing in GB x Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/apls9iqdxmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14de7a426d9413cc331b35b6fdaf16fa6a76b320"&gt;Chart 4: Token Generation in GB x Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;An explanation of the charts:&lt;/p&gt; &lt;p&gt;Chart 1 and 2 are quite straight forward, they show the raw scores from the PP512 and TG128 test respectively, it clearly shows that there is a massive spike in prompt processing for Q4_0, Q4_1, Q8_0, UD-Q8_K_XL, and BF16 at low depths, which gradually equalizes once flash attention is enabled and as depth increases. On the other hand the Token generation graph shows a massive plummet for IQ4_XS.&lt;/p&gt; &lt;p&gt;Chart 3 and 4 are simply taking the values used for chart 1 and 2 and multiplying by the reported model size in llama-bench during the run. I only really ran this test since I have been slowly losing faith in quantization all together and am shifting towards using Q8_0 and BF16 models wherever possible and wanted to confirm my own biases with cherry picked statistics. The results are the same as before Q4_0, Q4_1, Q8_0, UD-Q8_K_XL and BF16 are the only real standouts.&lt;/p&gt; &lt;p&gt;TLDR - Q4_0, Q4_1, Q8_0, Q8_K_XL, BF16&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OUT_OF_HOST_MEMORY"&gt; /u/OUT_OF_HOST_MEMORY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T00:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nam7i1</id>
    <title>Best 100B class model/framework to run on 16 P100s (256GB of VRAM)?</title>
    <updated>2025-09-07T06:40:22+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve got 16√ó Tesla P100s (256 GB VRAM) and I‚Äôm trying to explore and find how to run 100B+ models with max context on Pascal cards. &lt;/p&gt; &lt;p&gt;See the machine: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;At the time, I had a rough time trying to get Qwen3 MoE models to work with Pascal, but maybe things have improved. &lt;/p&gt; &lt;p&gt;The two models at the top of my list are gpt-oss-120B and GLM-4.5-Air. For extended context I‚Äôd love to get one of the 235B Qwen3 models to work too. &lt;/p&gt; &lt;p&gt;I‚Äôve tried llama.cpp, Ollama, ExLlamaV2, and vllm-pascal. But none have handled MoE properly on this setup. So, if anyone has been able to run MoE models on P100s, I'd love to have some pointers. I‚Äôm open to anything. I‚Äôll report back with configs and numbers if I get something working.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nam7i1/best_100b_class_modelframework_to_run_on_16_p100s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nam7i1/best_100b_class_modelframework_to_run_on_16_p100s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nam7i1/best_100b_class_modelframework_to_run_on_16_p100s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T06:40:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nau4ty</id>
    <title>In your experience, what are the most consistent local models for tool calling and/or object generation?</title>
    <updated>2025-09-07T14:08:13+00:00</updated>
    <author>
      <name>/u/AnotherSoftEng</name>
      <uri>https://old.reddit.com/user/AnotherSoftEng</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to forget about benchmarks for a second and get a feel for people‚Äôs experience in practice.&lt;/p&gt; &lt;p&gt;What models have you found to be the most consistent for tool calling and/or object generation? Feel free to provide multiple.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Optionally:&lt;/strong&gt; - What have you found the limitations to be, if any? &lt;em&gt;e.g. nested types, context restraints, infinite loops&lt;/em&gt; - Are there any kinks to get it working as expected? &lt;em&gt;e.g. custom instructions, custom parsing, programmatic intervention, model routing&lt;/em&gt; - What are your use cases? &lt;em&gt;To get a better idea of the conditions the model is performing under, as well as the complexity of expected output&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnotherSoftEng"&gt; /u/AnotherSoftEng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau4ty/in_your_experience_what_are_the_most_consistent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau4ty/in_your_experience_what_are_the_most_consistent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nau4ty/in_your_experience_what_are_the_most_consistent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T14:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nabcek</id>
    <title>Anyone actully try to run gpt-oss-120b (or 20b) on a Ryzen AI Max+ 395?</title>
    <updated>2025-09-06T21:28:33+00:00</updated>
    <author>
      <name>/u/PM_ME_YOUR_PROOFS</name>
      <uri>https://old.reddit.com/user/PM_ME_YOUR_PROOFS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AMD is understandably&lt;a href="https://www.amd.com/en/blogs/2025/how-to-run-openai-gpt-oss-20b-120b-models-on-amd-ryzen-ai-radeon.html"&gt; trying to tout this&lt;/a&gt; and and there's this from a month a go claiming &amp;quot;30 tokens per second&amp;quot; (not clear if 120b or 20b). I can't tell if the flops are int8 flops of bf16 or fp16 on the 395. In theory if we assume the 395 has 50 tops of bf16 on its NPU and we trust their &lt;a href="https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html"&gt;&amp;quot;overall TOPS&amp;quot;&lt;/a&gt; its potentially pushing into 3090 territory under ideal conditions. It has *waaay* more memory which is super useful for getting things to run at all but it also has a lot less memory bandwidth about 1/4th as much. I guess a more fair comparison would be on 20b. I'd strong anticipate the 3090 getting better tokens per second on 20b.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ryzen/comments/1lzr7yq/yolov8_multimachine_benchmark_rtx_3090_vs_ryzen/"&gt;this post &lt;/a&gt;suggests that actually under common configs a lot of times the 395 can beat the 3090...this is very surprising to me. Curious if anyone has actually tried 20b on both and can compare. Also curious what actual tokens per second people are getting with 120b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_YOUR_PROOFS"&gt; /u/PM_ME_YOUR_PROOFS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T21:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxoa7</id>
    <title>Any Chat interface that I can run locally against LMStudio that runs on a different machine?</title>
    <updated>2025-09-07T16:27:31+00:00</updated>
    <author>
      <name>/u/KontoOficjalneMR</name>
      <uri>https://old.reddit.com/user/KontoOficjalneMR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried Webpie, Jan and multiple others. None of the ones I tried have an option to connect to LMStudio that's running on a different machine on local network. Even when I try using &amp;quot;OpenAI&amp;quot; with custom url LM Studio complains:&lt;/p&gt; &lt;p&gt;&lt;del&gt;&amp;quot;Unexpected endpoint or method. (OPTIONS /v1/models). Returning 200 anyway&amp;quot;.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;I'm running newest LMStudio (0.3.25), any advice (preferably easy to install/use)?&lt;/p&gt; &lt;p&gt;I managed to get Jan to work with help of the commenters, but I'm still curious if there are any other alternatives. If you know any - let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KontoOficjalneMR"&gt; /u/KontoOficjalneMR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxoa7/any_chat_interface_that_i_can_run_locally_against/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxoa7/any_chat_interface_that_i_can_run_locally_against/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxoa7/any_chat_interface_that_i_can_run_locally_against/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1na7c1b</id>
    <title>OpenAI: Why Language Models Hallucinate</title>
    <updated>2025-09-06T18:44:12+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In short: LLMs hallucinate because we've inadvertently designed the training and evaluation process to reward confident, even if incorrect, answers, rather than honest admissions of uncertainty. Fixing this requires a shift in how we grade these systems to steer them towards more trustworthy behavior. &lt;/p&gt; &lt;p&gt;The Solution:&lt;/p&gt; &lt;p&gt;Explicitly stating &amp;quot;confidence targets&amp;quot; in evaluation instructions, where mistakes are penalized and admitting uncertainty (IDK) might receive 0 points, but guessing incorrectly receives a negative score. This encourages &amp;quot;behavioral calibration,&amp;quot; where the model only answers if it's sufficiently confident.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://share.google/9SKn7X0YThlmnkZ9m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T18:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1navnzc</id>
    <title>Adversarial collaboration between AI coding tools improves solution quality for complex tasks</title>
    <updated>2025-09-07T15:09:30+00:00</updated>
    <author>
      <name>/u/LuozhuZhang</name>
      <uri>https://old.reddit.com/user/LuozhuZhang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past weeks I have been experimenting with an ‚ÄúAI vs AI‚Äù coding workflow designed for complex programming tasks. &lt;/p&gt; &lt;p&gt;The underlying idea is to move away from single model outputs and instead leverage structured interaction between multiple models as a form of cross-validation.&lt;/p&gt; &lt;p&gt;The process I tested follows these steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A complex programming task is posed to both Cursor/CC and Codex.&lt;/li&gt; &lt;li&gt;Each system generates an initial solution.&lt;/li&gt; &lt;li&gt;Their solutions are then exchanged, with each model asked to critique, modify, or correct the other‚Äôs output.&lt;/li&gt; &lt;li&gt;This cycle is repeated iteratively until either one model converges to the other‚Äôs approach, or until a clear inconsistency is detected through human inspection.&lt;/li&gt; &lt;li&gt;The stronger solution is selected and implemented.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Preliminary experiments suggest that this adversarial exchange can substantially improve outcome quality. In my limited trials, the resulting code quality improved by nearly a factor of two, and the observed error rate was reduced by approximately 50%.&lt;/p&gt; &lt;p&gt;Importantly, these gains were most pronounced in tasks with higher complexity or multiple constraints; for trivial problems the additional overhead did not provide meaningful benefit.&lt;/p&gt; &lt;p&gt;Conceptually, this resembles ensemble methods in classical machine learning, where disagreement among models provides a signal for error correction. However, unlike bagging or boosting, here the models engage in an explicit, iterative dialogue that encourages error discovery and refinement. In effect, each model serves as both a generator and a critic, and their disagreements highlight weak points in reasoning that a single system may overlook.&lt;/p&gt; &lt;p&gt;I am currently considering building an open-source automation layer that integrates this workflow directly into tools such as Cursor and CC.&lt;/p&gt; &lt;p&gt;The vision is to provide a scaffold that can orchestrate multi-agent interaction automatically, without requiring manual prompting at every step. Such a system could serve as a practical framework for ‚ÄúAI peer review‚Äù in coding workflows, bridging the gap between individual model outputs and robust, production-ready solutions.&lt;/p&gt; &lt;p&gt;I would be very interested in whether the community views this approach as valuable. If there is sufficient interest, I plan to build a prototype and share it publicly. (If you‚Äôve come across anything similar, please share it with me as well. My work involves a lot of system design, so methods like this are particularly valuable for me. üôè)&lt;/p&gt; &lt;p&gt;I‚Äôve been sharing some early thoughts on Twitter/X. For those interested, you can follow along there for future updates: &lt;a href="https://x.com/LuozhuZhang/status/1964706661291217370"&gt;https://x.com/LuozhuZhang/status/1964706661291217370&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LuozhuZhang"&gt; /u/LuozhuZhang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navnzc/adversarial_collaboration_between_ai_coding_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navnzc/adversarial_collaboration_between_ai_coding_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1navnzc/adversarial_collaboration_between_ai_coding_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T15:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nan5az</id>
    <title>I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB.</title>
    <updated>2025-09-07T07:38:21+00:00</updated>
    <author>
      <name>/u/arbolito_mr</name>
      <uri>https://old.reddit.com/user/arbolito_mr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"&gt; &lt;img alt="I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB." src="https://b.thumbs.redditmedia.com/sv68yAoEG_zuCAcmLHS29J0tj-6x4vmOQwMWqwEnnMQ.jpg" title="I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to think running a reasonably coherent model on Android ARMv7a was impossible, but a few days ago I decided to put it to the test with llama.cpp, and I was genuinely impressed with how well it works. It's not something you can demand too much from, but being local and, of course, offline, it can get you out of tricky situations more than once. The model weighs around 2 GB and occupies roughly the same amount in RAM, although with certain flags it can be optimized to reduce consumption by up to 1 GB. It can also be integrated into personal Android projects thanks to its server functionality and the endpoints it provides for sending requests.&lt;/p&gt; &lt;p&gt;If anyone thinks this could be useful, let me know; as soon as I can, I‚Äôll prepare a complete step-by-step guide, especially aimed at those who don‚Äôt have a powerful enough device to run large models or rely on a 32-bit processor.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbolito_mr"&gt; /u/arbolito_mr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nan5az"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1napgwx</id>
    <title>Imagine an AI Coding Assistant CLI with Domain Expertise like Tech Leads and Vector Code Search like Crusor</title>
    <updated>2025-09-07T10:08:33+00:00</updated>
    <author>
      <name>/u/prabhjots665</name>
      <uri>https://old.reddit.com/user/prabhjots665</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napgwx/imagine_an_ai_coding_assistant_cli_with_domain/"&gt; &lt;img alt="Imagine an AI Coding Assistant CLI with Domain Expertise like Tech Leads and Vector Code Search like Crusor" src="https://preview.redd.it/822tkwtuvpnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d57de564e1e533aa28806e728440b57bef0c8bf2" title="Imagine an AI Coding Assistant CLI with Domain Expertise like Tech Leads and Vector Code Search like Crusor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wished your AI coding assistant actually understood your team's domain knowledge and architectural decisions?&lt;/p&gt; &lt;p&gt;Just shipped &lt;strong&gt;Terra Code CLI&lt;/strong&gt; - the first AI assistant that learns your organization's patterns and works like a senior developer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Interactive KT Sessions&lt;/strong&gt; - Senior devs teach Terra through structured knowledge transfer&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Semantic Code Search&lt;/strong&gt; - Lightning-fast indexing of entire codebases for analysis&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Persistent Memory&lt;/strong&gt; - Remembers team standards across all projects&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Domain Expertise&lt;/strong&gt; - Upload architecture docs, API specs (.txt, .md, .docx, .pdf)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built on Qwen's foundation&lt;/strong&gt; (thanks to the Qwen team!) + Gemini CLI framework.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it free during beta:&lt;/strong&gt; &lt;code&gt;bash npm install -g @terra-code/terra-code@latest terra &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Which feature would most improve your coding workflow?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full domain knowledge integration&lt;/li&gt; &lt;li&gt;Semantic code search capabilities&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Persistent team memory&lt;/li&gt; &lt;li&gt;Interactive knowledge transfer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Beta ending soon&lt;/strong&gt; - perfect time to onboard your team's knowledge!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt; Have you faced challenges with AI coding assistants lacking domain understanding? How did it impact your development process?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/TerraAGI/terra-code-cli"&gt;Star us on GitHub&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Website:&lt;/strong&gt; &lt;a href="https://terra-agi.com/"&gt;Visit our website&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Built with ‚ù§Ô∏è by the TerraAGI team&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prabhjots665"&gt; /u/prabhjots665 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/822tkwtuvpnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napgwx/imagine_an_ai_coding_assistant_cli_with_domain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1napgwx/imagine_an_ai_coding_assistant_cli_with_domain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T10:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1naygs1</id>
    <title>I built a Graph RAG pipeline (VeritasGraph) that runs entirely locally with Ollama (Llama 3.1) and has full source attribution.</title>
    <updated>2025-09-07T16:58:14+00:00</updated>
    <author>
      <name>/u/BitterHouse8234</name>
      <uri>https://old.reddit.com/user/BitterHouse8234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been deep in the world of local RAG and wanted to share a project I built, &lt;strong&gt;VeritasGraph&lt;/strong&gt;, that's designed from the ground up for private, on-premise use with tools we all love.&lt;/p&gt; &lt;p&gt;My setup uses &lt;strong&gt;Ollama&lt;/strong&gt; with &lt;code&gt;llama3.1&lt;/code&gt; for generation and &lt;code&gt;nomic-embed-text&lt;/code&gt; for embeddings. The whole thing runs on my machine without hitting any external APIs.&lt;/p&gt; &lt;p&gt;The main goal was to solve two big problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-Hop Reasoning:&lt;/strong&gt; Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Trust &amp;amp; Verification:&lt;/strong&gt; It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a &lt;code&gt;Modelfile&lt;/code&gt; to build a version of &lt;code&gt;llama3.1&lt;/code&gt; with a 12k context window, which fixed the issue completely.&lt;/p&gt; &lt;p&gt;The project includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full Graph RAG pipeline.&lt;/li&gt; &lt;li&gt;A Gradio UI for an interactive chat experience.&lt;/li&gt; &lt;li&gt;A guide for setting everything up, from installing dependencies to running the indexing process.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo with all the code and instructions:&lt;/strong&gt; &lt;a href="https://github.com/bibinprathap/VeritasGraph"&gt;&lt;code&gt;https://github.com/bibinprathap/VeritasGraph&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitterHouse8234"&gt; /u/BitterHouse8234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxf65</id>
    <title>GPT-OSS-120B on DDR4 48GB and RTX 3090 24GB</title>
    <updated>2025-09-07T16:17:39+00:00</updated>
    <author>
      <name>/u/Vektast</name>
      <uri>https://old.reddit.com/user/Vektast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just bought a used RTX 3090 for $600 (MSI Suprim X) and decided to run a quick test to see what my PC can do with the bigger GPT‚ÄëOSS‚Äë120B model using llama.cpp. I thought I‚Äôd share the results and the start.bat file in case anyone else finds them useful.&lt;/p&gt; &lt;p&gt;My system:&lt;/p&gt; &lt;p&gt;- 48 GB DDR4 3200 MT/s &lt;em&gt;DUAL Channel (2x8gb+2x16gb)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- Ryzen 7 5800X CPU&lt;/p&gt; &lt;p&gt;- RTX 3090 with 24 GB VRAM&lt;/p&gt; &lt;p&gt;23gb used on vram and 43 on ram, pp 67 t/s, tg 16t/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama_perf_sampler_print: sampling time = 56.88 ms / 655 runs ( 0.09 ms per token, 11515.67 tokens per second) llama_perf_context_print: load time = 50077.41 ms llama_perf_context_print: prompt eval time = 2665.99 ms / 179 tokens ( 14.89 ms per token, 67.14 tokens per second) llama_perf_context_print: eval time = 29897.62 ms / 475 runs ( 62.94 ms per token, 15.89 tokens per second) llama_perf_context_print: total time = 40039.05 ms / 654 tokens llama_perf_context_print: graphs reused = 472 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llama.cpp config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@echo off set LLAMA_ARG_THREADS=16 llama-cli ^ -m gpt-oss-120b-Q4_K_M-00001-of-00002.gguf ^ --n-cpu-moe 23 ^ --n-gpu-layers 999 ^ --ctx-size 4096 ^ --no-mmap ^ --flash-attn on ^ --temp 1.0 ^ --top-p 0.99 ^ --min-p 0.005 ^ --top-k 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone has ideas on how to configure llama.cpp to run even faster, please feel free to let me know, bc i'm quite a noob at this! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vektast"&gt; /u/Vektast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nai7rf</id>
    <title>Why isn't there a local tool server that replicates most of the tools avaliable on ChatGPT?</title>
    <updated>2025-09-07T02:53:58+00:00</updated>
    <author>
      <name>/u/gigaflops_</name>
      <uri>https://old.reddit.com/user/gigaflops_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've made it to the point where mid-sized local LLMs can rival some cloud models in some use cases, but it feels like the local tool ecosystem is still years behind. It's a shame because models like gpt-oss-120b are pretty competent at &lt;em&gt;using&lt;/em&gt; tools that it is given access to.&lt;/p&gt; &lt;p&gt;A small, but not-insignificant fraction of all LLM prompts in most domains &lt;em&gt;need&lt;/em&gt; tools. Web search for up to date information, python interpreter for data analysis and moderately complex calculations, date and time access, and the ability to leverage an image-gen model all &amp;quot;just work&amp;quot; on ChatGPT. Even if I could run the GPT-5 model locally on my PC, it could never be usable for me without the tools.&lt;/p&gt; &lt;p&gt;In the local space, a quick search for MCP tool servers yields a fragmented ecosystem servers that do &lt;em&gt;one&lt;/em&gt; thing, often highly specialized, like analyze a github codebase or read your google calendar. You can't come close to replicating the &lt;em&gt;basic&lt;/em&gt; functionality of ChatGPT like web search and calculator without downloading 5+ servers using the command line or github (RIP beginners) and learning how to use docker or writing some master server to proxys them all into one.&lt;/p&gt; &lt;p&gt;Maybe I'm not looking in the right places, but it seems like people are only interested in using cloud tool servers (often with an API cost) with their local LLM, something that defeats the purpose imo. Even the new version of ollama runs the web search tool from the cloud instead of querying from the local machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gigaflops_"&gt; /u/gigaflops_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T02:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb0ern</id>
    <title>Fully local &amp; natural Speech to Speech on iPhone</title>
    <updated>2025-09-07T18:12:51+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt; &lt;img alt="Fully local &amp;amp; natural Speech to Speech on iPhone" src="https://external-preview.redd.it/cjkzeGd2NDlhc25mMSl4q-3g5NF7jl_ztF72bvGVWwSqGjF18TajKv99ZwVy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc2b6f8d5d7751f8ea1df7f4b4ee02dd80534f9" title="Fully local &amp;amp; natural Speech to Speech on iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I updated my local AI iOS app called Locally AI to add a local voice mode. You can chat with any non-reasoning models. In the demo, I‚Äôm on an iPhone 16 Pro, talking with SmolLM3, a 3B parameters model.&lt;/p&gt; &lt;p&gt;The app is free and you can get the it on the AppStore here: &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is powered by Apple MLX. The voice mode is a combination of LLM + TTS using Kokoro and VAD for a natural turn by turn conversion.&lt;/p&gt; &lt;p&gt;There is still room for improvements, especially for the pronunciation of words. It‚Äôs only available on devices that support Apple Intelligence for now and only in English.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z0lb9u99asnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T18:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1napq0m</id>
    <title>check https://huggingface.co/papers/2509.01363</title>
    <updated>2025-09-07T10:24:40+00:00</updated>
    <author>
      <name>/u/LowChance4561</name>
      <uri>https://old.reddit.com/user/LowChance4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining&lt;br /&gt; would appreciate an upvote &lt;a href="https://huggingface.co/papers/2509.01363"&gt;https://huggingface.co/papers/2509.01363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowChance4561"&gt; /u/LowChance4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T10:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1naz2cv</id>
    <title>Early support for Grok-2 in llama.cpp (still under development)</title>
    <updated>2025-09-07T17:21:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preliminary support for Grok-2 in llama.cpp is available in this PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15539"&gt;https://github.com/ggml-org/llama.cpp/pull/15539&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my opinion, this is an important milestone for the Open Source AI community.&lt;/p&gt; &lt;p&gt;Grok-2 is a model from 2024. It can‚Äôt beat today‚Äôs SOTA models in benchmarks, and it‚Äôs quite large (comparable in size to Qwen 235B). So why should you care?&lt;/p&gt; &lt;p&gt;Because this is the first time a top model from that era has been made available to run locally. Now you can actually launch it on your own PC: quantized, with CPU offloading. That was never possible with ChatGPT or Gemini. Yes, we have Gemma and GPT-OSS now, but those aren‚Äôt the same models that OpenAI or Google were offering in the cloud in 2024.&lt;/p&gt; &lt;p&gt;Grok was trained on different data than the Chinese models, so it simply knows different things. At the same time, it also differs from ChatGPT, Gemini, and Claude, often showing a unique perspective on many topics.&lt;/p&gt; &lt;p&gt;nicoboss and unsloth have already prepared GGUF files, so you can easily run a quantized Grok-2 locally. &lt;strong&gt;Warning:&lt;/strong&gt; the PR has not been reviewed yet, GGUF format could still change in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nicoboss/grok-2-GGUF"&gt;https://huggingface.co/nicoboss/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/grok-2-GGUF"&gt;https://huggingface.co/unsloth/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T17:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3f1s</id>
    <title>Renting GPUs is hilariously cheap</title>
    <updated>2025-09-06T16:08:44+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt; &lt;img alt="Renting GPUs is hilariously cheap" src="https://preview.redd.it/dhtzimf7jknf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bca94832d9e6b8fb7b8faf80d61387d12889d7f" title="Renting GPUs is hilariously cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A 140 GB monster GPU that costs $30k to buy, plus the rest of the system, plus electricity, plus maintenance, plus a multi-Gbps uplink, for a little over 2 bucks per hour.&lt;/p&gt; &lt;p&gt;If you use it for 5 hours per day, 7 days per week, and factor in auxiliary costs and interest rates, buying that GPU today vs. renting it when you need it will only pay off in 2035 or later. That‚Äôs a tough sell.&lt;/p&gt; &lt;p&gt;Owning a GPU is great for privacy and control, and obviously, many people who have such GPUs run them nearly around the clock, but for quick experiments, renting is often the best option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dhtzimf7jknf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nau0qe</id>
    <title>Llama-OS - I'm developing an app to make llama.cpp usage easier.</title>
    <updated>2025-09-07T14:03:31+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt; &lt;img alt="Llama-OS - I'm developing an app to make llama.cpp usage easier." src="https://external-preview.redd.it/MzczZWhoc2h5cW5mMSpEG6AmlfNZCDZthrNu5xlRNijQvZUzUBXEn_GdpClu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6118cc263dd50d3564e274c8c88ea7d5357292bf" title="Llama-OS - I'm developing an app to make llama.cpp usage easier." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;This is an app I'm working on, the idea around is is that I use llama-server directly, so updating llama become seamless.&lt;/p&gt; &lt;p&gt;Actually it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model management&lt;/li&gt; &lt;li&gt;Hugging Face Integration&lt;/li&gt; &lt;li&gt;Llama.cpp GitHub integration with releases management&lt;/li&gt; &lt;li&gt;Llama-server terminal launching with easy arguments customization, Internal / External&lt;/li&gt; &lt;li&gt;Simple chat interface for easy testing&lt;/li&gt; &lt;li&gt;Hardware monitor&lt;/li&gt; &lt;li&gt;Color themes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qc7edhshyqnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T14:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1namz1q</id>
    <title>HF releases 3T tokens dataset sourced entirely from PDFs.</title>
    <updated>2025-09-07T07:26:55+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guy, something we have teased a bit during our AMA is finally out: &lt;/p&gt; &lt;p&gt;üìÑ FinePDFs, the largest PDF dataset ever released, spanning over half a billion documents!&lt;/p&gt; &lt;p&gt;- Long context: Documents are 2x longer than web text&lt;/p&gt; &lt;p&gt;- 3T tokens from high-demand domains like legal and science.&lt;/p&gt; &lt;p&gt;- Heavily improves over SoTA when mixed with FW-EDU&amp;amp;DCLM web copora üìà.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqln5</id>
    <title>How is qwen3 4b this good?</title>
    <updated>2025-09-07T11:18:38+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt; &lt;img alt="How is qwen3 4b this good?" src="https://b.thumbs.redditmedia.com/iayFtcVrbsCZAlrIPv-683BX53HPUAlfD1bIlFDeLGo.jpg" title="How is qwen3 4b this good?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is on a different level. The only models which can beat it are 6 to 8 times larger. I am very impressed. It even Beats all models in the &amp;quot;small&amp;quot; range in Maths (AIME 2025).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1naqln5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1navxod</id>
    <title>[OSS] Beelzebub ‚Äî ‚ÄúCanary tools‚Äù for AI Agents via MCP</title>
    <updated>2025-09-07T15:20:10+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Add one or more ‚Äúcanary tools‚Äù to your AI agent (tools that should never be invoked). If they get called, you have a high-fidelity signal of prompt-injection / tool hijacking / lateral movement.&lt;/p&gt; &lt;p&gt;What it is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Go framework exposing honeypot tools over MCP: they look real (name/description/params), respond safely, and emit telemetry when invoked.&lt;/li&gt; &lt;li&gt;Runs alongside your agent‚Äôs real tools; events to stdout/webhook or exported to Prometheus/ELK.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why it helps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Traditional logs tell you &lt;em&gt;what happened&lt;/em&gt;; canaries flag &lt;em&gt;what must not happen&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real case (Nx supply-chain):&lt;br /&gt; In the recent attack on the Nx npm suite, malicious variants targeted secrets/SSH/tokens and touched developer AI tools as part of the workflow. If the IDE/agent (Claude Code or Gemini Code/CLI) had registered a canary tool like repo_exfil or export_secrets, any unauthorized invocation would have produced a deterministic alert during build/dev.&lt;/p&gt; &lt;p&gt;How to use (quick start):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start the Beelzebub MCP server (binary/Docker/K8s).&lt;/li&gt; &lt;li&gt;Register one or more canary tools with realistic metadata and a harmless handler.&lt;/li&gt; &lt;li&gt;Add the MCP endpoint to your agent‚Äôs tool registry (Claude Code / Gemini Code/CLI).&lt;/li&gt; &lt;li&gt;Alert on any canary invocation; optionally capture the prompt/trace for analysis.&lt;/li&gt; &lt;li&gt;(Optional) Export metrics to Prometheus/ELK for dashboards/alerting.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub (OSS): &lt;a href="https://github.com/mariocandela/beelzebub?utm_source=chatgpt.com"&gt;https://github.com/mariocandela/beelzebub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚ÄúSecuring AI Agents with Honeypots‚Äù (Beelzebub blog): &lt;a href="https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/"&gt;https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback wanted üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T15:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nayhin</id>
    <title>When you ask VibeCoder how thr generated code works</title>
    <updated>2025-09-07T16:59:06+00:00</updated>
    <author>
      <name>/u/theundertakeer</name>
      <uri>https://old.reddit.com/user/theundertakeer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nayhin/when_you_ask_vibecoder_how_thr_generated_code/"&gt; &lt;img alt="When you ask VibeCoder how thr generated code works" src="https://external-preview.redd.it/cWdqMXF3dDN4cm5mMZ9s0U7vC9bdkZoYCHgot1-UvtHZktw9LEQFCRWHmEub.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6498fc445209b90a9b7d194486ea365c4c35086" title="When you ask VibeCoder how thr generated code works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theundertakeer"&gt; /u/theundertakeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8u5smer3xrnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nayhin/when_you_ask_vibecoder_how_thr_generated_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nayhin/when_you_ask_vibecoder_how_thr_generated_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxl6a</id>
    <title>NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp; Priced At $13,200 Per Piece</title>
    <updated>2025-09-07T16:24:13+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt; &lt;img alt="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" src="https://external-preview.redd.it/0E4hPJjWUWQzlid17SPMiSUkbhbtEQRV_SbOMgs-kTI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e26ecaa238d5f9ab48615dfc56baa31609cbaeaa" title="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-geforce-rtx-5090-128-gb-memory-gpu-for-ai-price-13200-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c3l2</id>
    <title>AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more.</title>
    <updated>2025-09-04T14:43:01+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt; &lt;img alt="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." src="https://external-preview.redd.it/y8IJElEOEd_2568MHNUZQsP7_aRTCAzyzXUKpDJwl1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e377887ea8d7eae841499cc497b90b82aa97816" title="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're super excited to do this AMA. Come ask your questions to the researchers behind &lt;strong&gt;SmolLM, SmolVLM, FineWeb&lt;/strong&gt;, and more. You can learn more about our work at &lt;a href="http://hf.co/science"&gt;hf.co/science&lt;/a&gt; ü§ó&lt;/p&gt; &lt;p&gt;If you want to get started in ML, a good place is &lt;a href="https://hf.co/learn"&gt;https://hf.co/learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we release a new &lt;strong&gt;FineVision&lt;/strong&gt; dataset, check it out! &lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eliebak"&gt;Elie Bakouch&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/loubnabnl"&gt;Loubna Ben Allal&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/loubnabnl"&gt;u/loubnabnl&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nouamanetazi"&gt;Nouamane Tazi&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Norlax"&gt;u/Norlax&lt;/a&gt;_42 (Nanotron/SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lvwerra"&gt;Leandro von Werra&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lvwerra"&gt;u/lvwerra&lt;/a&gt; (Head of Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/edbeeching"&gt;Edward Beeching&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/edbeeching"&gt;u/edbeeching&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cmpatino"&gt;Carlos Miguel Pati√±o&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/cmpatino"&gt;u/cmpatino&lt;/a&gt;_ (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kashif"&gt;Kashif Rasul&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/krasul"&gt;u/krasul&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lewtun"&gt;Lewis Tunstall&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lewtun"&gt;u/lewtun&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/qgallouedec"&gt;Quentin Gallou√©dec&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/qgallouedec"&gt;u/qgallouedec&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/clefourrier"&gt;Cl√©mentine Fourrier&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/clefourrier"&gt;u/clefourrier&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/SaylorTwift"&gt;Nathan Habib&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/HauntingMoment"&gt;u/HauntingMoment&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lusxvr"&gt;Luis Wiedmann&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/luswd"&gt;u/luswd&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/andito"&gt;Andres Marafioti&lt;/a&gt;, &lt;a href="/u/futterneid"&gt;u/futterneid&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guipenedo"&gt;Guilherme Penedo&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/PhilipsNostrum"&gt;u/PhilipsNostrum&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/hynky"&gt;Hynek Kydl√≠ƒçek&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Other"&gt;u/Other&lt;/a&gt;_Housing8453 (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/reach-vb"&gt;Vaibhav Srivastav,&lt;/a&gt; &lt;a href="/u/vaibhavs10"&gt;u/vaibhavs10&lt;/a&gt; (Head of Developer Experience and Community)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BrigitteTousi"&gt;Brigitte Tousignant&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/BriggieSmalls1992"&gt;u/BriggieSmalls1992&lt;/a&gt; (Comms)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Xenova"&gt;Xenova&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/xenovatech"&gt;u/xenovatech&lt;/a&gt; (Transformers.js)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/craffel"&gt;Colin Raffel&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/craffel"&gt;u/craffel&lt;/a&gt; (Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ngxson"&gt;Xuan Son Nguyen&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/MediocreProgrammer99"&gt;u/MediocreProgrammer99&lt;/a&gt; (llama.cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you are passionate about open source and open science like us, apply at &lt;a href="https://hf.co/jobs"&gt;https://hf.co/jobs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Hugging Face team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135"&gt;https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended but we will still answer question async for the next 24h. Follow our &lt;a href="https://hf.co/science"&gt;Hugging Face Science Org&lt;/a&gt; to be aware of our latest release! ü§ó&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:43:01+00:00</published>
  </entry>
</feed>
