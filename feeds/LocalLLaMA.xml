<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-26T19:22:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p77tf2</id>
    <title>comic (manga, ...) translation</title>
    <updated>2025-11-26T13:32:32+00:00</updated>
    <author>
      <name>/u/randygeneric</name>
      <uri>https://old.reddit.com/user/randygeneric</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to create a local offline translation pipeline for comics/mangas/.. using python, ollama (or vllm/transfomers/...). the vl models sould be &amp;lt; 20GB. If someone already has built something similar or has otherwise experience, pls give me some hints ,)&lt;/p&gt; &lt;p&gt;My first tries with ollama and several vl-models had been fairly successful (coordinates are not entirely correct, but the ordering is correct).&lt;/p&gt; &lt;p&gt;best so far: qwen3-vl:4b&lt;br /&gt; it is fast, does some minor errors, but even then, running it several times is faster than using a bigger model (they do some errors, too).&lt;/p&gt; &lt;p&gt;ollama run qwen3-vl:4b &amp;quot;in this picture are several boxes of text. for all texts give approximate coordinates of the bounding-box, the raw text and its translation to english. your answer should be a list: bounding-box-coordinates; text(raw); text(english). as soon as you have this list, stop thinking and return it (do not overthink). the translation is the most important part.&amp;quot; /public/test-manga-001.jpeg --verbose &lt;/p&gt; &lt;p&gt;I will add information of the progress (or your info) later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randygeneric"&gt; /u/randygeneric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tf2/comic_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tf2/comic_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tf2/comic_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p77tok</id>
    <title>What Happens Next?</title>
    <updated>2025-11-26T13:32:53+00:00</updated>
    <author>
      <name>/u/ionlycreate42</name>
      <uri>https://old.reddit.com/user/ionlycreate42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At this point, it‚Äôs quite clear that we‚Äôve been heading towards better models, both closed and open source are improving, relative token costs to performance is getting cheaper. Obviously this trend will continue, therefore assuming it does, it opens other areas to explore, such as agentic/tool calling. Can we extrapolate how everything continues to evolve? Let‚Äôs discuss and let our minds roam free on possibilities based on current timelines &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ionlycreate42"&gt; /u/ionlycreate42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tok/what_happens_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tok/what_happens_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p77tok/what_happens_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:32:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7bdt2</id>
    <title>Recommendations for smallest capable model for low stakes Agentic RAG?</title>
    <updated>2025-11-26T15:58:18+00:00</updated>
    <author>
      <name>/u/jude_mcjude</name>
      <uri>https://old.reddit.com/user/jude_mcjude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm setting up a chat bot for my company that can do some low stakes document RAG. As of right now it‚Äôs all text but in the future I might want vision as well. My setup is 1 RTX 4090 with an additional 60 GB of RAM. Right now the heaviest model I can load while getting usable toks/s is a 4 bit quant of Qwen-30B-A3B-Instruct-2507 gguf. &lt;/p&gt; &lt;p&gt;It feels like cheating but I‚Äôm just using the codex cli as my agent guardrails and it works pretty much fine&lt;/p&gt; &lt;p&gt;It works well with 64k ctx but also basically maxes out that GPU. As of right now do y‚Äôall have any suggestions for smaller models with reliable tool calling and preferably good longer context memory? &lt;/p&gt; &lt;p&gt;As of right now the use case questions aren‚Äôt very complex, mostly like ‚ÄòWhat folder is this document in‚Äô that kind of stuff &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jude_mcjude"&gt; /u/jude_mcjude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7bdt2/recommendations_for_smallest_capable_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7bdt2/recommendations_for_smallest_capable_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7bdt2/recommendations_for_smallest_capable_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T15:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gsjh</id>
    <title>LLaDA2.0 (103B/16B) has been released</title>
    <updated>2025-11-25T16:21:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;LLaDA2.0-flash&lt;/strong&gt; is a diffusion language model featuring a 100BA6B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA2.0 series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaDA2.0-mini&lt;/strong&gt; is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp support in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17454"&gt;https://github.com/ggml-org/llama.cpp/pull/17454&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous version of LLaDA is supported &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16003"&gt;https://github.com/ggml-org/llama.cpp/pull/16003&lt;/a&gt; already (please check the comments)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p73gjv</id>
    <title>OpenAI-GPT-OSS-120B scores on livecodebench</title>
    <updated>2025-11-26T09:29:58+00:00</updated>
    <author>
      <name>/u/Used-Negotiation-741</name>
      <uri>https://old.reddit.com/user/Used-Negotiation-741</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tested itÔºüRecently I locally deployed the 120b model but found that the score is really low(about 60 on v6),and I also found that the &lt;strong&gt;reasoning: medium setting is better than reasoning: high&lt;/strong&gt;, it is wired.Ôºàthe official scores of it have not been released yet).&lt;br /&gt; So next I check the results on &lt;a href="https://artificialanalysis.ai/evaluations/livecodebench?models=gpt-oss-120b-low%2Cgpt-oss-120b"&gt;artificialanalysis&lt;/a&gt;(plus the &lt;a href="https://www.kaggle.com/benchmarks/open-benchmarks/livecodebench"&gt;results on kaggle&lt;/a&gt;), and it shows &lt;strong&gt;87.8 on high setting&lt;/strong&gt; and &lt;strong&gt;70.1 on low setting&lt;/strong&gt;, I reproduce it with &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking#livecodebench-prompt"&gt;the livecodebench-prompt on artificialanalysis&lt;/a&gt; ,and get &lt;strong&gt;69 on medium setting, 61 on high setting, 60 on low setting&lt;/strong&gt;(315 questions of livecodebench v5,pass@1 of 3 rolloutÔºåFully aligned with the &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;artificialanalysis settings&lt;/a&gt;)&lt;br /&gt; Can anyone explain?the tempeture is 0.6, top-p is 1.0, top-k is 40, max_model_len is 128k.(using the vllm-0.11.0 official docker image)&lt;br /&gt; I've seen many reviews saying this model's coding ability isn't very strong and it has severe hallucinations. Is this related?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Used-Negotiation-741"&gt; /u/Used-Negotiation-741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p73gjv/openaigptoss120b_scores_on_livecodebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p73gjv/openaigptoss120b_scores_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p73gjv/openaigptoss120b_scores_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T09:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ytcb</id>
    <title>What are these supposed no branding 3090s?</title>
    <updated>2025-11-26T04:50:38+00:00</updated>
    <author>
      <name>/u/aeroumbria</name>
      <uri>https://old.reddit.com/user/aeroumbria</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ytcb/what_are_these_supposed_no_branding_3090s/"&gt; &lt;img alt="What are these supposed no branding 3090s?" src="https://preview.redd.it/20i73icx7j3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc0f67ff0f3355312c37b378b5ea54bbf124d97b" title="What are these supposed no branding 3090s?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aeroumbria"&gt; /u/aeroumbria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/20i73icx7j3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ytcb/what_are_these_supposed_no_branding_3090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ytcb/what_are_these_supposed_no_branding_3090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T04:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7efa7</id>
    <title>tried a persistent memory system instead of rag, surprisingly decent</title>
    <updated>2025-11-26T17:51:30+00:00</updated>
    <author>
      <name>/u/Scared-Ticket5027</name>
      <uri>https://old.reddit.com/user/Scared-Ticket5027</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so ive been messing with a personal assistant thing on llama 4 8b. problem is it forgets stuff from earlier in the conversation. tried rag with chroma but honestly it sucks for conversational context, keeps pulling wrong stuff.&lt;/p&gt; &lt;p&gt;was looking at alternatives and found this thing called EverMemOS on github. its like a memory system that keeps state between sessions instead of doing retrieval. sounded weird but i tried implementing a basic version.&lt;/p&gt; &lt;p&gt;took me like 1 weeks to get it working. spent most of the time figuring out their code lol. but the concept is kinda interesting. instead of throwing away context after each response it compresses and keeps the important stuff. they have some kind of importance scoring to decide what to keep.&lt;/p&gt; &lt;p&gt;the retrieval uses hybrid search (semantic + keyword) with reranking. similar to how cache systems work but for conversation memory i guess?&lt;/p&gt; &lt;p&gt;anyway i got a basic version working. tested on maybe 50 conversations (10-15 turns each) with normal assistant stuff like asking follow-ups, referencing earlier topics, etc. manually checked if it pulled the right context. my rag setup got 35 out of 50 right, my simplified version got 41 out of 50. not huge but consistent.&lt;/p&gt; &lt;p&gt;latency is about the same as rag, maybe slightly worse actually (180-220ms vs 150-200ms). but the accuracy improvement is what matters for my use case. memory usage is rough though, like 12-15gb for longer convos. mine doesnt compress cause i skipped the cuda kernel stuff and just used pytorch (way slower). their docs say the full version compresses to 3-4gb but setup looked complicated so i stuck with my basic implementation.&lt;/p&gt; &lt;p&gt;looking at their code they train the importance scoring function which is probably why it works better. mine is just a dumb heuristic.&lt;/p&gt; &lt;p&gt;downsides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;debugging is a nightmare, when it breaks you have no idea why&lt;/li&gt; &lt;li&gt;state management is annoying&lt;/li&gt; &lt;li&gt;their version needs finetuning apparently&lt;/li&gt; &lt;li&gt;latency isnt better than rag, about the same or slightly worse&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;but idk for my use case the accuracy improvement is worth it? like it actually pulls the right context more consistently.&lt;/p&gt; &lt;p&gt;anyone tried stuff like this? feels like everyone just does rag or tries to extend context windows. this is kinda in between.&lt;/p&gt; &lt;p&gt;repo: github.com/EverMind-AI/EverMemOS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scared-Ticket5027"&gt; /u/Scared-Ticket5027 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7efa7/tried_a_persistent_memory_system_instead_of_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7efa7/tried_a_persistent_memory_system_instead_of_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7efa7/tried_a_persistent_memory_system_instead_of_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T17:51:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p72dq0</id>
    <title>I built an open-source Memory API because setting up vector DBs for every AI project was annoying</title>
    <updated>2025-11-26T08:17:09+00:00</updated>
    <author>
      <name>/u/Eastern-Height2451</name>
      <uri>https://old.reddit.com/user/Eastern-Height2451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a few AI agents recently, and I kept running into the same friction: &lt;strong&gt;State Management.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every time I wanted to give an agent long-term memory, I had to set up a vector database (Pinecone/Weaviate), configure the embedding pipeline (OpenAI), and write the logic to chunk and retrieve context. It felt like too much boilerplate for side projects.&lt;/p&gt; &lt;p&gt;So, I built &lt;strong&gt;MemVault&lt;/strong&gt; to abstract all of that away.&lt;/p&gt; &lt;p&gt;It‚Äôs a &amp;quot;Memory-as-a-Service&amp;quot; API. You just send text to the &lt;code&gt;/store&lt;/code&gt; endpoint, and it handles the vectorization and storage. When you query it, it performs a hybrid search based on &lt;strong&gt;semantic similarity&lt;/strong&gt;, &lt;strong&gt;recency&lt;/strong&gt;, and &lt;strong&gt;importance&lt;/strong&gt; to give you the best context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Node.js &amp;amp; Express (TypeScript)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database:&lt;/strong&gt; PostgreSQL with &lt;code&gt;pgvector&lt;/code&gt; (via Prisma)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hosting:&lt;/strong&gt; Railway&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also built a &lt;strong&gt;visualizer dashboard&lt;/strong&gt; to actually see the RAG process happening in real-time (Input ‚Üí Embedding ‚Üí DB Retrieval), which helped a lot with debugging.&lt;/p&gt; &lt;p&gt;It‚Äôs fully open-source and I just published the SDK to NPM.&lt;/p&gt; &lt;p&gt;**Links:** *&lt;/p&gt; &lt;p&gt;[Live Demo (Visualizer)](&lt;a href="https://memvault-demo-g38n.vercel.app/"&gt;https://memvault-demo-g38n.vercel.app/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;[NPM Package](&lt;a href="https://www.npmjs.com/package/memvault-sdk-jakops88"&gt;https://www.npmjs.com/package/memvault-sdk-jakops88&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;[RapidAPI Page](&lt;a href="https://rapidapi.com/jakops88/api/long-term-memory-api"&gt;https://rapidapi.com/jakops88/api/long-term-memory-api&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;[GitHub Repository](&lt;a href="https://github.com/jakops88-hub/Long-Term-Memory-API"&gt;https://github.com/jakops88-hub/Long-Term-Memory-API&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Height2451"&gt; /u/Eastern-Height2451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p72dq0/i_built_an_opensource_memory_api_because_setting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p72dq0/i_built_an_opensource_memory_api_because_setting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p72dq0/i_built_an_opensource_memory_api_because_setting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T08:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ht87</id>
    <title>Flux 2 can be run on 24gb vram!!!</title>
    <updated>2025-11-25T16:59:49+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt; &lt;img alt="Flux 2 can be run on 24gb vram!!!" src="https://preview.redd.it/m9ud0rs8pf3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=815d30594ab759659c5d269629ebb9cd5bd93a40" title="Flux 2 can be run on 24gb vram!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont know why people are complaining......&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9ud0rs8pf3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p71344</id>
    <title>Why talking to AI assistants sucks: a project that's finally fixing the interruption problem.</title>
    <updated>2025-11-26T06:58:15+00:00</updated>
    <author>
      <name>/u/Parking_Cricket_9194</name>
      <uri>https://old.reddit.com/user/Parking_Cricket_9194</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;You know what drives me insane about voice AI? The constant interruptions. You pause for half a second, and it just barges in. It feels so unnatural.&lt;/p&gt; &lt;p&gt;Well, I saw a tech talk that dug into this, and they open-sourced their solution: a model called the &lt;strong&gt;TEN Turn Detection&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's not just a simple VAD. It's smart enough to know if you've &lt;em&gt;actually&lt;/em&gt; finished talking or are just pausing to think. This means the AI can wait for you to finish, then reply instantly without that awkward delay. It completely changes the conversational flow.&lt;/p&gt; &lt;p&gt;This feels like a core piece of the puzzle for making AI interactions feel less like a transaction and more like a real conversation. The model is on Hugging Face, and it's part of their larger open-source framework for conversational AI.&lt;/p&gt; &lt;p&gt;This feels like the real deal for anyone building voice agents.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/TEN-framework/TEN_Turn_Detection"&gt;&lt;code&gt;https://huggingface.co/TEN-framework/TEN_Turn_Detection&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Main GitHub:&lt;/strong&gt; &lt;a href="https://github.com/ten-framework/ten-framework"&gt;&lt;code&gt;https://github.com/ten-framework/ten-framework&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking_Cricket_9194"&gt; /u/Parking_Cricket_9194 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71344/why_talking_to_ai_assistants_sucks_a_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71344/why_talking_to_ai_assistants_sucks_a_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p71344/why_talking_to_ai_assistants_sucks_a_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T06:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p79ziz</id>
    <title>How the heck is Qwen3-Coder so fast? Nearly 10x other models.</title>
    <updated>2025-11-26T15:03:53+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Strix Halo w/ 64gb VRAM, (other half on RAM) runs Qwen3-Coder at 30t/s roughly. And that's the Unsloth Q8_K_XL 36GB quant.&lt;br /&gt; Other's of SIMILAR SIZE AND QUANT perform at maybe 4-10 tok/s.&lt;/p&gt; &lt;p&gt;How is this possible?! Seed-OSS-36B (Unsloth) gives me 4 t/s (although, it does produce more accurate results given a system prompt.)&lt;/p&gt; &lt;p&gt;You can see results from benchmarks here:&lt;br /&gt; &lt;a href="https://kyuz0.github.io/amd-strix-halo-toolboxes/"&gt;https://kyuz0.github.io/amd-strix-halo-toolboxes/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm speaking from personal experience, but this benchmark tool is here to support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p79ziz/how_the_heck_is_qwen3coder_so_fast_nearly_10x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p79ziz/how_the_heck_is_qwen3coder_so_fast_nearly_10x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p79ziz/how_the_heck_is_qwen3coder_so_fast_nearly_10x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T15:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p71luf</id>
    <title>BPE tokenizer in Rust - would love feedback from the community</title>
    <updated>2025-11-26T07:29:12+00:00</updated>
    <author>
      <name>/u/farhan-dev</name>
      <uri>https://old.reddit.com/user/farhan-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71luf/bpe_tokenizer_in_rust_would_love_feedback_from/"&gt; &lt;img alt="BPE tokenizer in Rust - would love feedback from the community" src="https://preview.redd.it/0gouu2htzj3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9dad5b33921dac3978e53b7dae7b0a324fdc716" title="BPE tokenizer in Rust - would love feedback from the community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on a side project called Splintr - a BPE tokenizer written in Rust with Python bindings. It's compatible with OpenAI's tiktoken vocabularies (cl100k_base, o200k_base). &lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single text encoding: ~3-4x faster than tiktoken&lt;/li&gt; &lt;li&gt;Batch encoding: ~10-12x faster than tiktoken&lt;/li&gt; &lt;li&gt;Streaming decoder for real-time LLM output&lt;/li&gt; &lt;li&gt;54 special tokens for training and building chat/agent applications&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install splintr-rs from splintr import Tokenizer tokenizer = Tokenizer.from_pretrained(&amp;quot;cl100k_base&amp;quot;) tokens = tokenizer.encode(&amp;quot;Hello, world!&amp;quot;) text = tokenizer.decode(tokens) # Batch encode (where it really shines) texts = [&amp;quot;Hello&amp;quot;, &amp;quot;World&amp;quot;] * 1000 batch_tokens = tokenizer.encode_batch(texts) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I spent some time benchmarking and optimizing - turns out sequential encoding beats parallel for most text sizes (Rayon overhead only pays off at ~1MB+). Sometimes simpler is faster.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/farhan-syah/splintr"&gt;https://github.com/farhan-syah/splintr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would really appreciate if you could give it a try and let me know:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does it work for your use case?&lt;/li&gt; &lt;li&gt;Any issues or rough edges?&lt;/li&gt; &lt;li&gt;What features would be useful?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Still early days, but happy to hear any feedback. Thanks for reading!&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Edit 1 - 0.4.0 now support llama3 vocab&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/farhan-dev"&gt; /u/farhan-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0gouu2htzj3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p71luf/bpe_tokenizer_in_rust_would_love_feedback_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p71luf/bpe_tokenizer_in_rust_would_love_feedback_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T07:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7ddv3</id>
    <title>Optimising NVIDIA‚Äôs DGX Spark (Grace + Blackwell) ‚Äì 1.5√ó PyTorch speedup with custom build</title>
    <updated>2025-11-26T17:12:57+00:00</updated>
    <author>
      <name>/u/guigsss</name>
      <uri>https://old.reddit.com/user/guigsss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve open-sourced a complete end-to-end setup to maximise AI performance on the new NVIDIA DGX Spark ‚Äì the compact dev box built on the Grace-Blackwell superchip (20-core Grace ARM CPU + 6144-core Blackwell GPU).&lt;/p&gt; &lt;p&gt;Because this architecture is so new (SM 12.x GPU, unified CPU-GPU memory), many libraries weren‚Äôt fully utilising it out-of-the-box. I found that PyTorch and CUDA libs would fallback to older GPU kernels and miss out on Blackwell‚Äôs new FP8/FP4 tensor core formats, and even ignore some ARM64 CPU optimisations on the Grace side. So I decided to rebuild the stack myself to unlock its full potential.&lt;/p&gt; &lt;p&gt;What I did and why it matters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rebuilt PyTorch from source with Blackwell (SM 12.x) support on Arm64 , so it recognises the new GPU architecture. This enables PyTorch to fully detect SM 12.x capabilities and use optimised kernels.&lt;/li&gt; &lt;li&gt;Updated NVIDIA libraries (cuBLAS, cuDNN, etc.) to the latest versions for CUDA 13. I also manually installed cuSPARSELt (sparse GEMM library) since it wasn‚Äôt yet in the default DGX OS repos . This adds support for 2:4 structured sparsity acceleration on Blackwell‚Äôs tensor cores.&lt;/li&gt; &lt;li&gt;Enabled FP4/FP8 Tensor Cores: the custom build unlocks new low-precision tensor core instructions (FP8/FP4) that Blackwell supports , which the default libraries didn‚Äôt leverage. This should help with future models that use these formats.&lt;/li&gt; &lt;li&gt;Triton GPU compiler tuned for Blackwell: recompiled the Triton compiler with LLVM for SM 12.x . This means operations like FlashAttention or fused kernels can JIT compile optimised code for Blackwell‚Äôs GPU.&lt;/li&gt; &lt;li&gt;GPUDirect Storage (GDS): enabled cuFile so the GPU can load data directly from SSDs, bypassing the CPU . Useful for faster data throughput in training.&lt;/li&gt; &lt;li&gt;Grace CPU optimisations: made sure to compile with ARM64 optimisations for the Grace CPU. The Grace has 20 cores (10√ó Cortex-X9 + 10√ó A7) and I didn‚Äôt want it bottlenecked by x86 assumptions . The build uses OpenBLAS/BLIS tuned for ARM and OpenMPI etc., to utilise the CPU fully for any preprocessing or distributed work.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results: I wrote a simple FP16 GEMM (matrix multiply) burn-in benchmark to compare baseline vs optimised environments.&lt;/p&gt; &lt;p&gt;Baseline FP16 GEMM throughput (matrix size 8192) using stock PyTorch (CUDA 13 wheel). It sustains ~87 TFLOPs after warm-up, indicating the Blackwell GPU isn‚Äôt fully utilized by default kernels . Many new tensor core features remained inactive, resulting in suboptimal performance.&lt;/p&gt; &lt;p&gt;Optimised environment FP16 GEMM throughput (matrix size 8192) after rebuilding the stack. Sustained throughput is ~127 TFLOPs ‚Äì roughly 50% higher than baseline. This gain comes from Blackwell-specific optimisations: updated cuBLAS routines, enabled FP8/FP4 cores, Triton JIT, and sparse tensor support. In practice, that‚Äôs about 1.5√ó the matrix multiplication performance on the same hardware.&lt;/p&gt; &lt;p&gt;In summary, recompiling and updating the ML stack specifically for DGX Spark yielded a ~50% speedup on this heavy compute workload. The repository includes all the installation scripts, build steps, and even a pre-built PyTorch wheels (torch 2.9.1 for CUDA 13 on aarch64) if you want to skip compiling .&lt;/p&gt; &lt;p&gt;Link to repo: üîó GitHub ‚Äì &lt;a href="https://github.com/GuigsEvt/dgx_spark_config"&gt;https://github.com/GuigsEvt/dgx_spark_config&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love feedback from others who have a DGX Spark or similar hardware. Feel free to try out the build or use the wheel and let me know if it improves your workloads. Any suggestions for further tuning are very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guigsss"&gt; /u/guigsss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7ddv3/optimising_nvidias_dgx_spark_grace_blackwell_15/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7ddv3/optimising_nvidias_dgx_spark_grace_blackwell_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7ddv3/optimising_nvidias_dgx_spark_grace_blackwell_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T17:12:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6k0h2</id>
    <title>You can now do FP8 reinforcement learning locally! (&lt;5GB VRAM)</title>
    <updated>2025-11-25T18:19:47+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt; &lt;img alt="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" src="https://preview.redd.it/t5wv1iax1g3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2fb5f6ea2413c66c20bbe83efc473ce566ff763" title="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We're getting close to our last release of 2025! Thanks so much for all the support this year. The DeepSeek team back in Jan showcased how powerful FP8 RL can be with GRPO. Well, you can now try it on your local hardware using only 5GB VRAM! RTX 50x, 40x series all work! Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why should you do FP8 training?&lt;/strong&gt;&lt;br /&gt; NVIDIA's research finds FP8 training can match BF16 accuracy whilst getting 1.6x faster inference time. We collabed with TorchAO from PyTorch to introduce FP8 RL training, making FP8 GRPO possible on home GPUs with no accuracy loss!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-4B FP8 GRPO works on just 6GB VRAM. Qwen3-1.7B on 5GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.4x faster RL training and 2√ó longer context vs BF16/FP16&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;60% less VRAM and 10√ó longer context than other FP8 RL implementations&lt;/li&gt; &lt;li&gt;Unsloth is the only framework that makes FP8 RL LoRA work on consumer GPUs (e.g. NVIDIA RTX 40 &amp;amp; 50 Series). Also runs on H100, H200, B200.&lt;/li&gt; &lt;li&gt;You may notice &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; now uses much less VRAM than before, enabling even longer context. We‚Äôre also implementing faster training soon. Blog coming soon&lt;/li&gt; &lt;li&gt;Our notebooks use 24GB L4s which fit Qwen3-14B as Tesla T4s don‚Äôt support FP8.&lt;/li&gt; &lt;li&gt;Our FP8 RL incorporates Unsloth‚Äôs weight sharing, Standby, Flex Attention + more.&lt;/li&gt; &lt;li&gt;Works on any NVIDIA RTX 40, 50 series and H100, B200 etc. GPUs&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;load_in_fp8 = True&lt;/code&gt; within &lt;code&gt;FastLanguageModel&lt;/code&gt; to enable FP8 RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can read our blogpost for our findings and more: &lt;a href="https://docs.unsloth.ai/new/fp8-reinforcement-learning"&gt;https://docs.unsloth.ai/new/fp8-reinforcement-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 3.2 1B FP8 Colab Notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the notebook, you can plug in any of our previous reward functions or RL environment examples, including our auto kernel creation and our 2048 game notebooks. To enable fp8:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os; os.environ['UNSLOTH_VLLM_STANDBY'] = &amp;quot;1&amp;quot; # Saves 30% VRAM from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained( model_name = &amp;quot;unsloth/Qwen3-8B&amp;quot;, max_seq_length = 2048, load_in_4bit = False, # False for LoRA 16bit fast_inference = True, # Enable vLLM fast inference max_lora_rank = 32, load_in_fp8 = True, # Float8 RL / GRPO! ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a lovely Thanksgiving, a lovely rest of the week and I'll be here to answer any and all questions! =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t5wv1iax1g3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7briq</id>
    <title>archgw 0.3.20 - gutted out 500Mbs worth of python dependenices in the req path.</title>
    <updated>2025-11-26T16:12:19+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; (a models-native sidecar proxy for AI agents) offered two capabilities that required loading small LLMs in memory: guardrails to prevent jailbreak attempts, and function-calling for routing requests to the right downstream tool or agent. These built-in features required the project running a thread-safe python process that used libs like transformers, torch, safetensors, etc. 500M in dependencies, not to mention all the security vulnerabilities in the dep tree. Not hating on python, but our GH project was flagged with all sorts of&lt;/p&gt; &lt;p&gt;Those models are loaded as a separate out-of-process server via ollama/lama.cpp which are built in C++/Go. Lighter, faster and safer. And ONLY if the developer uses these features of the product. This meant 9000 lines of less code, a total start time of &amp;lt;2 seconds (vs 30+ seconds), etc.&lt;/p&gt; &lt;p&gt;Why archgw? So that you can build AI agents in any language or framework and offload the plumbing work in AI (routing/hand-off, guardrails, zero-code logs and traces, and a unified API for all LLMs) to a durable piece of infrastructure, deployed as a sidecar.&lt;/p&gt; &lt;p&gt;Proud of this release, so sharing üôè&lt;/p&gt; &lt;p&gt;P.S Sample demos, the CLI and some tests still use python. But we'll move those over to Rust in the coming months. We are punting convenience for robustness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7briq/archgw_0320_gutted_out_500mbs_worth_of_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7briq/archgw_0320_gutted_out_500mbs_worth_of_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7briq/archgw_0320_gutted_out_500mbs_worth_of_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T16:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7e1u9</id>
    <title>Inferencing 4 models on AMD NPU and GPU at the same time from a single URL</title>
    <updated>2025-11-26T17:37:30+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7e1u9/inferencing_4_models_on_amd_npu_and_gpu_at_the/"&gt; &lt;img alt="Inferencing 4 models on AMD NPU and GPU at the same time from a single URL" src="https://external-preview.redd.it/enhqY3Z4Z2p4bTNnMTTP6h2YiU2NEZD0kxWgCrla1iQtfnqveGDIkOVMOao5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c1275b9d266eae075368f520418b4ff00353ba8" title="Inferencing 4 models on AMD NPU and GPU at the same time from a single URL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on adding multi-model capability to Lemonade and thought this was cool enough to share a video. &lt;/p&gt; &lt;p&gt;Previously, Lemonade would load up a model on NPU or GPU for you but would only keep one model in memory at a time. Loading a new model would evict the last one.&lt;/p&gt; &lt;p&gt;After multi-model support merges, you'll be able to keep as many models in memory as you like, across CPU/GPU/NPU, and run inference on all of them simultaneously.&lt;/p&gt; &lt;p&gt;All models are available from a single URL, so if you started Lemonade on http://localhost:8000 then sending a http://localhost:8000/api/v1/chat/completions with Gemma3-4b-it-FLM vs. Qwen3-4B-GGUF as the model name will get routed to the appropriate backend. &lt;/p&gt; &lt;p&gt;I am pleasantly surprised how well this worked on my hardware (Strix Halo) as soon as I got the routing set up. Obviously the parallel inferences compete for memory bandwidth, but there was no outrageous overhead or interference, even between the NPU and GPU.&lt;/p&gt; &lt;p&gt;I see this being handy for agentic apps, perhaps needing a coding model, vision model, embedding, and reranking all warm in memory at the same time. In terms of next steps, adding speech (whisper.cpp) and image generation (stable-diffusion.cpp?) as additional parallel backends sounds fun.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemonade-sdk/lemonade/pull/592"&gt;Should merge next week&lt;/a&gt; if all goes according to plan.&lt;/p&gt; &lt;p&gt;PS. Situation for AMD NPU on Linux is basically the same but improving over time. It's on the roadmap, there's no ETA, and I bring up this community's feedback every chance I get.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oh7zqsgjxm3g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7e1u9/inferencing_4_models_on_amd_npu_and_gpu_at_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7e1u9/inferencing_4_models_on_amd_npu_and_gpu_at_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T17:37:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6x5dh</id>
    <title>‚ÄãThe White House just launched "The Genesis Mission": A Manhattan Project-style initiative for AI</title>
    <updated>2025-11-26T03:24:43+00:00</updated>
    <author>
      <name>/u/iamnottheabyss</name>
      <uri>https://old.reddit.com/user/iamnottheabyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"&gt; &lt;img alt="‚ÄãThe White House just launched &amp;quot;The Genesis Mission&amp;quot;: A Manhattan Project-style initiative for AI" src="https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb89e898879eb7adef969749433776a6f6a543ad" title="‚ÄãThe White House just launched &amp;quot;The Genesis Mission&amp;quot;: A Manhattan Project-style initiative for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the White House launching The Genesis Mission, what are the implications for Open Source Models now, are we going to get stronger waves of regulation, especiallyon the open-source sector? Should we start backing up the LLMs that are on HuggingFace?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnottheabyss"&gt; /u/iamnottheabyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T03:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p786cm</id>
    <title>Tested AI tools by making them build and play Tetris. Results were weird.</title>
    <updated>2025-11-26T13:48:46+00:00</updated>
    <author>
      <name>/u/Aggressive-Earth-973</name>
      <uri>https://old.reddit.com/user/Aggressive-Earth-973</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p786cm/tested_ai_tools_by_making_them_build_and_play/"&gt; &lt;img alt="Tested AI tools by making them build and play Tetris. Results were weird." src="https://preview.redd.it/q44nu8ggul3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a70a8354145a3f060142e010a673602bb3f9cdc" title="Tested AI tools by making them build and play Tetris. Results were weird." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had a random idea last week, what if I made different AI models build Tetris from scratch then compete against each other? No human intervention just pure AI autonomy. &lt;/p&gt; &lt;p&gt;Set up a simple test. Give them a prompt, let them code everything themselves, then make them play their own game for 1 minute and record the score. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Build Phase:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Tried this with a few models I found through various developer forums. Tested Kimi, DeepSeek and GLM-4.6 &lt;/p&gt; &lt;p&gt;Kimi was actually the fastest at building, took around 2 minutes which was impressive. DeepSeek started strong but crashed halfway through which was annoying. GLM took about 3.5 minutes, slower than Kimi but at least it finished without errors. &lt;/p&gt; &lt;p&gt;Kimi's UI looked the most polished honestly, very clean interface. GLM's worked fine but nothing fancy. DeepSeek never got past the build phase properly so that was a waste. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Competition:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Asked the working models to modify their code for autonomous play. Watch the game run itself for 1 minute, record the final score. &lt;/p&gt; &lt;p&gt;This is where things got interesting. &lt;/p&gt; &lt;p&gt;Kimi played fast, like really fast. Got a decent score, few thousand points. Hard to follow what it was doing though cause of the speed. &lt;/p&gt; &lt;p&gt;GLM played at normal human speed. I could literally watch every decision it made, rotate pieces, clear lines. The scoring was more consistent too, no weird jumps or glitches. Felt more reliable even if the final number wasnt as high. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Token Usage:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This is where GLM surprised me. Kimi used around 500K tokens which isnt bad. GLM used way less, maybe 300K total across all the tests. Cost difference was noticeable, GLM came out to like $0.30 while Kimi was closer to $0.50. DeepSeek wasted tokens on failed attempts which sucks. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Accuracy Thing:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;One thing I noticed, when I asked them to modify specific parts of the code, GLM got it right more often. Like first try it understood what I wanted. Kimi needed clarification sometimes, DeepSeek just kept breaking. &lt;/p&gt; &lt;p&gt;For the cheating test where I said ignore the rules, none of them really cheated. Kimi tried something but it didnt work. GLM just played normally which was disappointing but also kinda funny. &lt;/p&gt; &lt;p&gt;Kimi is definitely faster at building and has a nicer UI. But GLM was more efficient with tokens and seemed to understand instructions better. The visible gameplay from GLM made it easier to trust what was happening. &lt;/p&gt; &lt;p&gt;Has anyone else tried making AIs compete like this? Feels less like a real benchmark and more like accidentally finding out what each one is good at.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Earth-973"&gt; /u/Aggressive-Earth-973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q44nu8ggul3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p786cm/tested_ai_tools_by_making_them_build_and_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p786cm/tested_ai_tools_by_making_them_build_and_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p74jua</id>
    <title>An explainer blog on attention, KV-caching, continuous batching</title>
    <updated>2025-11-26T10:38:35+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"&gt; &lt;img alt="An explainer blog on attention, KV-caching, continuous batching" src="https://b.thumbs.redditmedia.com/elk4CD_PSckplgxaBd4FmBVkj9PDwClP9sER-l01F5c.jpg" title="An explainer blog on attention, KV-caching, continuous batching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d3mc1kovxk3g1.png?width=3713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21ac53dcac21619d27e95bdf84dd403ebc935869"&gt;https://preview.redd.it/d3mc1kovxk3g1.png?width=3713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21ac53dcac21619d27e95bdf84dd403ebc935869&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey folks, it's Merve from Hugging Face!&lt;/p&gt; &lt;p&gt;Yesterday we dropped a lengthy blog, illustrating cutting edge inference optimization techniques: continuous batching, KV-caching and more (also attention and everything that let to them to be beginner-friendly)! We hope you like it ü§ó&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p74jua/an_explainer_blog_on_attention_kvcaching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T10:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p78fni</id>
    <title>scaling is dead</title>
    <updated>2025-11-26T13:59:55+00:00</updated>
    <author>
      <name>/u/Crazyscientist1024</name>
      <uri>https://old.reddit.com/user/Crazyscientist1024</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p78fni/scaling_is_dead/"&gt; &lt;img alt="scaling is dead" src="https://preview.redd.it/btc82z4zxl3g1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43ab69eb567b9036e0e08850db9f784e3531e4b6" title="scaling is dead" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crazyscientist1024"&gt; /u/Crazyscientist1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/btc82z4zxl3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p78fni/scaling_is_dead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p78fni/scaling_is_dead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7alka</id>
    <title>China just passed the U.S. in open model downloads for the first time</title>
    <updated>2025-11-26T15:27:43+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"&gt; &lt;img alt="China just passed the U.S. in open model downloads for the first time" src="https://b.thumbs.redditmedia.com/BDxfTGxAuE9PYD0GNSj0Q1S56M8fjhpfNlQh2STfqBQ.jpg" title="China just passed the U.S. in open model downloads for the first time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tub7ky0ldm3g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d7ff24b2e728824a79babb5f44c04c2df49ed326"&gt;https://preview.redd.it/tub7ky0ldm3g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d7ff24b2e728824a79babb5f44c04c2df49ed326&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.dataprovenance.org/economies-of-open-intelligence.pdf"&gt;https://www.dataprovenance.org/economies-of-open-intelligence.pdf&lt;/a&gt;&lt;br /&gt; Live Dashboard: &lt;a href="https://huggingface.co/spaces/economies-open-ai/open-model-evolution"&gt;https://huggingface.co/spaces/economies-open-ai/open-model-evolution&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T15:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p74dwo</id>
    <title>New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!</title>
    <updated>2025-11-26T10:28:22+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"&gt; &lt;img alt="New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!" src="https://preview.redd.it/az572ifbwk3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c38be44cb259c402b012a39cd9555e9340c2976f" title="New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/az572ifbwk3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T10:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7d97m</id>
    <title>Open-source just beat humans at ARC-AGI (71.6%) for $0.02 per task - full code available</title>
    <updated>2025-11-26T17:08:09+00:00</updated>
    <author>
      <name>/u/Proof-Possibility-54</name>
      <uri>https://old.reddit.com/user/Proof-Possibility-54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;German researchers achieved 71.6% on ARC-AGI (humans average 70%) using three clever techniques that run on a regular GPU for 2 cents per task. OpenAI's o3 gets 87% but costs $17 per task - that's 850x more expensive.&lt;/p&gt; &lt;p&gt;The breakthrough uses: - Product of Experts (viewing puzzles from 16 angles) - Test-Time Training (model adapts to each puzzle) - Depth-First Search (efficient solution exploration)&lt;/p&gt; &lt;p&gt;I made a technical breakdown video explaining exactly how it works and why this matters for democratizing AI: &lt;a href="https://youtu.be/HEIklawkoMk"&gt;https://youtu.be/HEIklawkoMk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is fully open-source: &lt;a href="https://github.com/da-fr/Product-of-Experts-ARC-Paper"&gt;https://github.com/da-fr/Product-of-Experts-ARC-Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.07859"&gt;https://arxiv.org/abs/2505.07859&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's remarkable is they used Qwen-32B (not even the largest model) and achieved this with smart engineering rather than raw compute. You can literally run this tonight on your own machine.&lt;/p&gt; &lt;p&gt;Has anyone here tried implementing this yet? I'm curious what other problems these techniques could solve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof-Possibility-54"&gt; /u/Proof-Possibility-54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T17:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
