<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-09T04:49:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q79n6x</id>
    <title>I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out.</title>
    <updated>2026-01-08T11:42:44+00:00</updated>
    <author>
      <name>/u/ikergarcia1996</name>
      <uri>https://old.reddit.com/user/ikergarcia1996</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt; &lt;img alt="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out." src="https://b.thumbs.redditmedia.com/Pou7U9wzorYlQFrD5KCYo6fxr8k5_S3OwV7_B5qbW8s.jpg" title="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I now feel bad seeing the model realize it was losing its mind and struggling with it, it feels like I was torturing it :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikergarcia1996"&gt; /u/ikergarcia1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q79n6x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T11:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7p09i</id>
    <title>Free, open source adventure RP app (AGPL 3) | Aventura</title>
    <updated>2026-01-08T21:42:38+00:00</updated>
    <author>
      <name>/u/AuYsI</name>
      <uri>https://old.reddit.com/user/AuYsI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Over these last couple of weeks, I've been working on a frontend called &lt;a href="https://github.com/unkarelian/Aventura"&gt;Aventura&lt;/a&gt;. It's 100% free and open source, under AGPL 3.&lt;/p&gt; &lt;h1&gt;What is Aventura?&lt;/h1&gt; &lt;p&gt;Simply put, it's a frontend purpose built for adventure RP and creative writing. While the original release only had support for openrouter, I have added the ability to add &lt;em&gt;any&lt;/em&gt; openai compatible source, as well as the ability to manually change the parameters you send to a model. While I have limited testing myself due to my poor GPU, it should work just fine with local models (: . (I hope)&lt;/p&gt; &lt;h1&gt;So what does it do?&lt;/h1&gt; &lt;p&gt;It has a built in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tracker, for events, characters, plot points, inventory, etc&lt;/li&gt; &lt;li&gt;Multiple choice options, for both creative writing and adventure mode, allowing for good reference points on what to do next&lt;/li&gt; &lt;li&gt;Long term memory(!!!) using the exact same system as timeline-memory (a SillyTavern extension I made), but with several optimizations. It runs &lt;strong&gt;much&lt;/strong&gt; faster than it does with timeline-memory, due to being able to run several queries in parallel.&lt;/li&gt; &lt;li&gt;Lorebook management, completely automatic and in the background, not requiring any user input and not interrupting the flow&lt;/li&gt; &lt;li&gt;LLM based lorebook retrieval, massively increasing accuracy over using embedding models&lt;/li&gt; &lt;li&gt;Anti-slop automation, taking inspiration from my fork of Prose Polisher, I have ditched the programmatic way of determining it, and instead use an LLM, which is much more accurate&lt;/li&gt; &lt;li&gt;Setup wizard for creating new scenarios, with the assistance of AI&lt;/li&gt; &lt;li&gt;Built in spell checker using harper&lt;/li&gt; &lt;li&gt;Lorebook classification using LLM's Note: This was made with parallel requests in mind, and as such it at times makes several generations at once. Make sure you have some sort of way to handle that, or alternatively, disable the features that do make multiple requests. You're also going to have to set up the models for each feature yourself if you do run locally, as it only has pre-configurations for api aggregators (for the sake of my own sanity).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical details of the memory system&lt;/h1&gt; &lt;p&gt;Since this is &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; , I figured I should also share how the memory system here works. It's not a system I've really seen anywhere else, though I may be wrong.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;In every message, the 'time' is either advanced or kept the same. Either way, the 'current time' is saved to each message. When a token threshold is passed (default 24k), a summary is automatically triggered. In this automatic summary, the 'starting time' (the time of the first message in the summary) and the 'ending time' (the time of the last message of the summary) are saved as part of the data, alongside the characters and locations visited. This gives the summary itself a stable sense of in-universe 'time' that helps maintain coherence. But that's just a modification of the summary, and not really anything that different.&lt;/p&gt; &lt;h1&gt;The slightly different part&lt;/h1&gt; &lt;p&gt;What actually matters here is that we don't get rid of the messages within the summary. Instead, while we hide them from the 'visible' chat history to the AI, before every message after a summary is made, multiple 'queries' are run on those summarized 'chapters'. When a query is made, a separate AI is given the &lt;strong&gt;entirety&lt;/strong&gt; of that chapter alongside the query, and, crucially, it passes back an answer to that query. That way, we can keep even the smallest details of a chapter &lt;em&gt;without&lt;/em&gt; overloading the context of the 'main narrative ai'. It's basically trading pure inference for accuracy. All of this comes together to make a very coherent 'timeline' of events. It also has a separate agentic mode after each chapter is created, where an AI will run in the background and make tool calls after querying chapters, and actively update the lorebooks for you. You don't really have to maintain the world yourself at all with this, it just does it for you.&lt;/p&gt; &lt;h1&gt;Contributing&lt;/h1&gt; &lt;p&gt;Contributions are very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AuYsI"&gt; /u/AuYsI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7p09i/free_open_source_adventure_rp_app_agpl_3_aventura/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7p09i/free_open_source_adventure_rp_app_agpl_3_aventura/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7p09i/free_open_source_adventure_rp_app_agpl_3_aventura/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T21:42:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7k754</id>
    <title>toy model</title>
    <updated>2026-01-08T18:46:43+00:00</updated>
    <author>
      <name>/u/Eduard_T</name>
      <uri>https://old.reddit.com/user/Eduard_T</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If anyone is interested in creating, training, and chatting with a toy model, I‚Äôve created &lt;a href="https://github.com/EduardTalianu/toygpt"&gt;https://github.com/EduardTalianu/toygpt&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a model script to create a model&lt;/li&gt; &lt;li&gt;a training script to train it on a&lt;code&gt;.txt&lt;/code&gt; file&lt;/li&gt; &lt;li&gt;a chat script to interact with the trained model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs a PyTorch research implementation of a Manifold-Constrained Hyper-Connection Transformer (mHC), combining Mixture-of-Experts efficiency, Sinkhorn-based routing, and architectural stability enhancements.&lt;/p&gt; &lt;p&gt;Slower per step than a vanilla Transformer ‚Äî but &lt;em&gt;much&lt;/em&gt; more sample-efficient. At &amp;lt;1 epoch it already learns grammar, structure, and style instead of collapsing into mush.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eduard_T"&gt; /u/Eduard_T &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7y717</id>
    <title>Resume Helper AI: Privacy-first resume tailor &amp; application tracker (Ollama + APIs)</title>
    <updated>2026-01-09T04:09:05+00:00</updated>
    <author>
      <name>/u/OpeningSad323</name>
      <uri>https://old.reddit.com/user/OpeningSad323</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôm a solo dev working on an experimental tool called &lt;a href="https://github.com/gibbenergy/Resume_Helper"&gt;Resume Helper AI&lt;/a&gt;. It‚Äôs designed to automate resume tailoring and manage the full job application lifecycle while prioritizing data privacy. It‚Äôs a work in progress, and I‚Äôm looking for architectural and model-related feedback from the community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical Overview:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Privacy-First Multi-LLM Support: Supports local inference via &lt;strong&gt;Ollama&lt;/strong&gt; and hosted APIs (OpenAI/Anthropic/Gemini/etc...) via LiteLLM + (remove personal info before sending to API service)&lt;/li&gt; &lt;li&gt;No Langchain ( I think MoE is good enough). Langchain is overkill &lt;/li&gt; &lt;li&gt;Full Application Tracking: Manages the entire lifecycle of a job hunt, beyond simple document generation.&lt;/li&gt; &lt;li&gt;The Stack: Built with Gradio, LiteLLM, Ollama.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Looking for Opinions on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Document Quality:&lt;/strong&gt; For those using LLMs to generate or tailor resumes/cover letter/ skill gap analysis , how are you finding the quality of the output compared to manual writing or other app? Are there specific prompting techniques that help maintain a professional, non-&amp;quot;AI-sounding&amp;quot; tone?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Recommendations:&lt;/strong&gt; Which specific LLMs (local or API) have you found most effective for document tailoring? I‚Äôm looking for models that excel at following strict formatting constraints and matching resume bullet points to job descriptions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Efficiency:&lt;/strong&gt; Are there any specific tools, programming trick or logic flows you‚Äôd suggest to make the transition from &amp;quot;Raw Job Description&amp;quot; to &amp;quot;Tailored Resume/Cover Letter/Skill Gap&amp;quot; more efficient?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm looking to improve the utility of this tool while keeping it local-first. Would love to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OpeningSad323"&gt; /u/OpeningSad323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7y717/resume_helper_ai_privacyfirst_resume_tailor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7y717/resume_helper_ai_privacyfirst_resume_tailor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7y717/resume_helper_ai_privacyfirst_resume_tailor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T04:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7m2eh</id>
    <title>Built a blind benchmark for coding models - which local models should I add?</title>
    <updated>2026-01-08T19:54:08+00:00</updated>
    <author>
      <name>/u/Equivalent-Yak2407</name>
      <uri>https://old.reddit.com/user/Equivalent-Yak2407</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/"&gt; &lt;img alt="Built a blind benchmark for coding models - which local models should I add?" src="https://preview.redd.it/6ocf1gbxj6cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4dc23f03c864b314bc32210b4178f4a8e53727f4" title="Built a blind benchmark for coding models - which local models should I add?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3 AI judges score each output blind. Early results from 10 coding tasks - Deepseek V3.2 at #9. GLM 4.7 at #6, beating Claude Opus 4.5.&lt;/p&gt; &lt;p&gt;Some open-source models are free to evaluate. Which local models should I evaluate and add to the leaderboard?&lt;/p&gt; &lt;p&gt;&lt;a href="http://codelens.ai/leaderboard"&gt;codelens.ai/leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT: Tested community suggestions! Results now live on the leaderboard: &lt;/p&gt; &lt;p&gt;- GPT-OSS-120B, Qwen3 Next 80B, Devstral 2, Nemotron Nano 30B, and more &lt;/p&gt; &lt;p&gt;Keep the suggestions coming - we'll keep adding models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Yak2407"&gt; /u/Equivalent-Yak2407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6ocf1gbxj6cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T19:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7wkaz</id>
    <title>Just finished Chip Huyen‚Äôs "AI Engineering" (O‚ÄôReilly) ‚Äî I have 534 pages of theory and 0 lines of code. What's the "Indeed-Ready" bridge?</title>
    <updated>2026-01-09T02:53:50+00:00</updated>
    <author>
      <name>/u/Substantial_Sky_8167</name>
      <uri>https://old.reddit.com/user/Substantial_Sky_8167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just finished a cover-to-cover grind of Chip Huyen‚Äôs &lt;em&gt;AI Engineering&lt;/em&gt; (the new O'Reilly release). Honestly? The book is a masterclass. I actually understand &amp;quot;AI-as-a-judge,&amp;quot; RAG evaluation bottlenecks, and the trade-offs of fine-tuning vs. prompt strategy now.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; I am currently the definition of &amp;quot;book smart.&amp;quot; I haven't actually built a single repo yet. If a hiring manager asked me to spin up a production-ready LangGraph agent or debug a vector DB latency issue right now, I‚Äôd probably just stare at them and recite the preface.&lt;/p&gt; &lt;p&gt;I want to spend the next 2-3 months getting &amp;quot;Job-Ready&amp;quot; for a US-based AI Engineer role. I have full access to O'Reilly (courses, labs, sandbox) and a decent budget for API credits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you were hiring an AI Engineer today, what is the FIRST &amp;quot;hands-on&amp;quot; move you'd make to stop being a theorist and start being a candidate?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm currently looking at these three paths on O'Reilly/GitHub:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Agentic&amp;quot; Route:&lt;/strong&gt; Skip the basic &amp;quot;PDF Chatbot&amp;quot; (which feels like a 2024 project) and build a Multi-Agent Researcher using &lt;strong&gt;LangGraph&lt;/strong&gt; or &lt;strong&gt;CrewAI&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Ops/Eval&amp;quot; Route:&lt;/strong&gt; Focus on the &amp;quot;boring&amp;quot; stuff Chip talks about‚Äîbuilding an automated &lt;strong&gt;Evaluation Pipeline&lt;/strong&gt; for an existing model to prove I can measure accuracy/latency properly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Deployment&amp;quot; Route:&lt;/strong&gt; Focus on serving models via &lt;strong&gt;FastAPI&lt;/strong&gt; and &lt;strong&gt;Docker&lt;/strong&gt; on a cloud service, showing I can handle the &amp;quot;Engineering&amp;quot; part of AI Engineering.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm basically looking for the shortest path from &amp;quot;I read the book&amp;quot; to &amp;quot;I have a GitHub that doesn't look like a collection of tutorial forks.&amp;quot; Are certifications like &lt;strong&gt;Microsoft AI-102&lt;/strong&gt; or &lt;strong&gt;Databricks&lt;/strong&gt; worth the time, or should I just ship a complex system?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I know the theory thanks to Chip Huyen, but I‚Äôm a total fraud when it comes to implementation. How do I fix this before the 2026 hiring cycle passes me by?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Sky_8167"&gt; /u/Substantial_Sky_8167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7wkaz/just_finished_chip_huyens_ai_engineering_oreilly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7wkaz/just_finished_chip_huyens_ai_engineering_oreilly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7wkaz/just_finished_chip_huyens_ai_engineering_oreilly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T02:53:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7uo7u</id>
    <title>SimpleLLM ‚Äî a minimal (~950 LOC) LLM inference engine built from scratch</title>
    <updated>2026-01-09T01:30:58+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uo7u/simplellm_a_minimal_950_loc_llm_inference_engine/"&gt; &lt;img alt="SimpleLLM ‚Äî a minimal (~950 LOC) LLM inference engine built from scratch" src="https://external-preview.redd.it/eW56MWo5OGo3OGNnMQt6mXHkLBiOyVm9E_-7IBj4RKtoglrz47V6J4dn3Gg-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac2113c19b230d70c00cd56b79a99cc4aa1f0903" title="SimpleLLM ‚Äî a minimal (~950 LOC) LLM inference engine built from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SimpleLLM's engine is async by default. Every request goes through a background inference loop that continuously batches work to keep the GPU saturated &amp;amp; prioritizing throughput.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;SimpleLLM&lt;/th&gt; &lt;th align="left"&gt;vLLM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;batch_size = 1&lt;/td&gt; &lt;td align="left"&gt;135 tok/s&lt;/td&gt; &lt;td align="left"&gt;138 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;batch_size = 64&lt;/td&gt; &lt;td align="left"&gt;4,041 tok/s&lt;/td&gt; &lt;td align="left"&gt;3,846 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note: Currently, this repository ONLY supports OpenAI/gpt-oss-120b on a single NVIDIA H100.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from llm import LLM&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;engine = LLM(&amp;quot;./gpt-oss-120b&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;outputs = engine.generate([&amp;quot;What is the meaning of life?&amp;quot;], max_tokens=100).result()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(outputs[0].text)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Github Repo - &lt;a href="https://github.com/naklecha/simple-llm"&gt;https://github.com/naklecha/simple-llm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/twqirt3j78cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uo7u/simplellm_a_minimal_950_loc_llm_inference_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uo7u/simplellm_a_minimal_950_loc_llm_inference_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T01:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7hywi</id>
    <title>How do you manage quality when AI agents write code faster than humans can review it?</title>
    <updated>2026-01-08T17:28:30+00:00</updated>
    <author>
      <name>/u/lostsoul8282</name>
      <uri>https://old.reddit.com/user/lostsoul8282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are shifting to an agentic workflow. My thesis is &amp;quot;Code at Inference Speed.&amp;quot; My CTO's counter-argument is that &lt;strong&gt;reviewing code is harder than writing it&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;His concern is simple: If AI increases code volume by 10x, human review becomes a fatal bottleneck. He predicts technical debt will explode because humans can‚Äôt mentally verify that much logic that quickly.&lt;/p&gt; &lt;p&gt;How do handle this? I know one option is to slow down releases but is there any other approaches people are taking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostsoul8282"&gt; /u/lostsoul8282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:28:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7hikw</id>
    <title>Qwen3-4B-Instruct-2507 multilingual FT with upscaled Polish language</title>
    <updated>2026-01-08T17:12:08+00:00</updated>
    <author>
      <name>/u/Significant_Focus134</name>
      <uri>https://old.reddit.com/user/Significant_Focus134</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Just wanted to share a preview of my latest finetuned model based on Qwen3-4B-Instruct-2507.&lt;/p&gt; &lt;p&gt;Languages ratio:&lt;/p&gt; &lt;p&gt;Polish - high&lt;br /&gt; English - medium&lt;br /&gt; Chinese - medium&lt;br /&gt; Czech - medium/low&lt;br /&gt; Ukrainian - medium/low&lt;br /&gt; Russian - medium/low&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf"&gt;https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Focus134"&gt; /u/Significant_Focus134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7c0pd</id>
    <title>AI21 releases Jamba2 3B and Jamba2 Mini, built for grounding and instruction following</title>
    <updated>2026-01-08T13:38:34+00:00</updated>
    <author>
      <name>/u/zennaxxarion</name>
      <uri>https://old.reddit.com/user/zennaxxarion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for AI21, creator of the Jamba model family.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre excited to announce the public release of Jamba2 3B and Jamba2 Mini.&lt;/p&gt; &lt;p&gt;The Jamba2 family aims to give enterprises cost-effective models that will integrate well into production agent stacks.&lt;/p&gt; &lt;p&gt;These models are designed for reliable instruction following and grounded outputs, working well over long documents and avoiding drifting once context becomes large.&lt;/p&gt; &lt;p&gt;They perform best for precise question answering over internal policies, technical manuals and knowledge bases, without the overhead of thinking tokens which can become costly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key performance data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B and Jamba2 Mini outperform peers due to their hybrid SSM-Transformer architecture and KV cache innovations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outpaces Ministral3 14B and Qwen3 30B A3B across FACTS, IFBench and IFEval. &lt;/li&gt; &lt;li&gt;Beats Ministral3 3B and Qwen3 4B on IFEval and IFBench, tying with Qwen3 4B as category leader on FACTS.&lt;/li&gt; &lt;li&gt;At context lengths of 100K, Jamba2 Mini delivers 2.7X greater throughput than Ministral3 14B and 1.4X greater throughout than Qwen3 30B A3B.&lt;/li&gt; &lt;li&gt;At context lengths of 100K, Jamba2 3B delivers 1.7X greater throughout than Ministral3 3B and 2.7X greater throughput than Qwen 3 14B.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs available today in AI21‚Äôs SaaS and from Hugging Face.&lt;/p&gt; &lt;p&gt;Happy to answer questions or dig into benchmarks if people want more detail.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="http://www.ai21.com/blog/introducing-jamba2"&gt;http://www.ai21.com/blog/introducing-jamba2&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/collections/ai21labs/jamba2"&gt;https://huggingface.co/collections/ai21labs/jamba2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zennaxxarion"&gt; /u/zennaxxarion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T13:38:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q71sbe</id>
    <title>Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)</title>
    <updated>2026-01-08T04:08:39+00:00</updated>
    <author>
      <name>/u/ManavTheWorld</name>
      <uri>https://old.reddit.com/user/ManavTheWorld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt; &lt;img alt="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" src="https://b.thumbs.redditmedia.com/6NWh2JOC5gMK_IIgH5CXHBl--P730mcKTIYRJh91W0w.jpg" title="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079"&gt;https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MVPandey/DTS"&gt;https://github.com/MVPandey/DTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Motivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:&lt;/p&gt; &lt;p&gt;(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generates N diverse strategies&lt;/li&gt; &lt;li&gt;Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)&lt;/li&gt; &lt;li&gt;Rolls out full multi-turn conversations down each branch&lt;/li&gt; &lt;li&gt;Has 3 independent LLM judges score each trajectory, takes the median&lt;/li&gt; &lt;li&gt;Prunes branches below threshold, backpropagates scores&lt;/li&gt; &lt;li&gt;Repeats for however many rounds you configure&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4"&gt;https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Three judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.&lt;/p&gt; &lt;p&gt;Main additions over CAE:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;user intent forking (strategies get stress-tested against different personas)&lt;/li&gt; &lt;li&gt;deep research integration via GPT-Researcher for domain context&lt;/li&gt; &lt;li&gt;proper visualization with conversation playback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Only supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls&lt;/p&gt; &lt;p&gt;It's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;p&gt;BTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManavTheWorld"&gt; /u/ManavTheWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T04:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7o8kl</id>
    <title>GLM-4.7 on 4x RTX 3090 with ik_llama.cpp</title>
    <updated>2026-01-08T21:14:19+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the help of Opus 4.5 I got unsloth/GLM-4.7-GGUF (Q4_K_M) running on my 4x RTX 3090 setup using ik_llama.cpp in Docker. I wanted to share my benchmark results and configuration, and ask if these numbers are what I should expect - or if there's room for improvement.&lt;/p&gt; &lt;h1&gt;My Setup&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Specs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Motherboard&lt;/td&gt; &lt;td align="left"&gt;Supermicro H12SSL-i&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD EPYC 7282&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPUs&lt;/td&gt; &lt;td align="left"&gt;4x NVIDIA RTX 3090 (96GB VRAM total, all at PCIe x16)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;256GB DDR4-2133&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Storage&lt;/td&gt; &lt;td align="left"&gt;2 TB NVMe SSD&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Benchmark Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Context&lt;/th&gt; &lt;th align="left"&gt;n-cpu-moe&lt;/th&gt; &lt;th align="left"&gt;Batch&lt;/th&gt; &lt;th align="left"&gt;VRAM/GPU&lt;/th&gt; &lt;th align="left"&gt;Prompt&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Generation&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Initial (mmap)&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;all&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;~5 GB&lt;/td&gt; &lt;td align="left"&gt;2.8 t/s&lt;/td&gt; &lt;td align="left"&gt;3.1 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;split-mode layer&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;partial&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;~17 GB&lt;/td&gt; &lt;td align="left"&gt;2.8 t/s&lt;/td&gt; &lt;td align="left"&gt;‚ö†Ô∏è 0.29 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;+ no-mmap&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;all&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;~10 GB&lt;/td&gt; &lt;td align="left"&gt;8.5 t/s&lt;/td&gt; &lt;td align="left"&gt;3.45 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;+ n-cpu-moe 72&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;72&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;~17 GB&lt;/td&gt; &lt;td align="left"&gt;9.9 t/s&lt;/td&gt; &lt;td align="left"&gt;4.12 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Best 8K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;4096&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;~21 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;12.0 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;4.48 t/s&lt;/strong&gt; ‚≠ê&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Best 16K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;16K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;~19 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;10.5 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;4.28 t/s&lt;/strong&gt; ‚≠ê&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Benchmark Methodology&lt;/h1&gt; &lt;p&gt;All tests were performed using the same simple request via curl:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl http://localhost:8080/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;GLM-4.7-GUFF&amp;quot;, &amp;quot;messages&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Write a short Haiku.&amp;quot;}], &amp;quot;temperature&amp;quot;: 0.7, &amp;quot;max_tokens&amp;quot;: 100 }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The response includes timing information:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;timings&amp;quot;: { &amp;quot;prompt_n&amp;quot;: 17, &amp;quot;prompt_ms&amp;quot;: 1419.902, &amp;quot;prompt_per_second&amp;quot;: 11.97, &amp;quot;predicted_n&amp;quot;: 100, &amp;quot;predicted_ms&amp;quot;: 22301.81, &amp;quot;predicted_per_second&amp;quot;: 4.48 } } &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;prompt_per_second&lt;/strong&gt;: How fast the input tokens are processed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;predicted_per_second&lt;/strong&gt;: How fast new tokens are generated (this is what matters most for chat)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each configuration was tested with a fresh server start (cold start) and the first request after warmup. Note that GLM-4.7 has a &amp;quot;thinking/reasoning&amp;quot; mode enabled by default, so the 100 generated tokens include internal reasoning tokens.&lt;/p&gt; &lt;h1&gt;My Current Configuration&lt;/h1&gt; &lt;h1&gt;Best for 8K Context (fastest):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;llama-server \ --model &amp;quot;/models/GLM-4-Q4_K_M-00001-of-00005.gguf&amp;quot; \ --host 0.0.0.0 --port 8080 \ --ctx-size 8192 \ --n-gpu-layers 999 \ --split-mode graph \ --flash-attn on \ --no-mmap \ -b 4096 -ub 4096 \ --cache-type-k q4_0 --cache-type-v q4_0 \ --k-cache-hadamard \ --jinja \ --n-cpu-moe 65 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Best for 16K Context:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;llama-server \ --model &amp;quot;/models/GLM-4-Q4_K_M-00001-of-00005.gguf&amp;quot; \ --host 0.0.0.0 --port 8080 \ --ctx-size 16384 \ --n-gpu-layers 999 \ --split-mode graph \ --flash-attn on \ --no-mmap \ -b 2048 -ub 2048 \ --cache-type-k q4_0 --cache-type-v q4_0 \ --k-cache-hadamard \ --jinja \ --n-cpu-moe 68 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Key Findings:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;--no-mmap&lt;/code&gt; &lt;strong&gt;is crucial&lt;/strong&gt; - Loading the model into RAM instead of memory-mapping from SSD &lt;strong&gt;tripled&lt;/strong&gt; my prompt processing speed (2.8 ‚Üí 12 t/s)&lt;/li&gt; &lt;li&gt;&lt;code&gt;--split-mode graph&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; &lt;code&gt;layer&lt;/code&gt; - Layer mode gave me only 0.29 t/s because GPUs process sequentially. Graph mode enables true tensor parallelism.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--n-cpu-moe X&lt;/code&gt; - This flag controls how many MoE layers stay on CPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size matters&lt;/strong&gt; - Smaller batches (2048) allowed more MoE layers on GPU for 16K context.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Docker Setup&lt;/h1&gt; &lt;p&gt;I'm running this in Docker. Here's my &lt;code&gt;docker-compose.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;services: glm-4: build: context: . dockerfile: Dockerfile container_name: glm-4-server deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - /path/to/models:/models:ro ports: - &amp;quot;8080:8080&amp;quot; environment: - CTX_MODE=${CTX_MODE:-8k} # Switch between 8k/16k - NO_MMAP=true - KV_CACHE_K=q4_0 - KV_CACHE_V=q4_0 - K_CACHE_HADAMARD=true shm_size: '32gb' ipc: host restart: unless-stopped &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And my &lt;code&gt;Dockerfile&lt;/code&gt; builds ik_llama.cpp with CUDA support:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM nvidia/cuda:12.4.0-devel-ubuntu22.04 # Install dependencies RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \ git cmake build-essential curl \ &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* # Clone and build ik_llama.cpp WORKDIR /opt RUN git clone https://github.com/ikawrakow/ik_llama.cpp.git WORKDIR /opt/ik_llama.cpp RUN cmake -B build \ -DGGML_CUDA=ON \ -DGGML_CUDA_FA_ALL_QUANTS=ON \ -DCMAKE_CUDA_ARCHITECTURES=&amp;quot;86&amp;quot; \ -DCMAKE_BUILD_TYPE=Release \ &amp;amp;&amp;amp; cmake --build build --config Release -j$(nproc) \ &amp;amp;&amp;amp; cmake --install build EXPOSE 8080 COPY entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh ENTRYPOINT [&amp;quot;/entrypoint.sh&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Are these speeds (4.48 t/s generation) normal for this setup?&lt;/strong&gt; I've seen some posts mentioning 5-6 t/s with 2x RTX 5090, but they had 64GB VRAM total vs my 96GB.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Any other flags I should try?&lt;/strong&gt; I tested &lt;code&gt;--run-time-repack&lt;/code&gt; but it didn't help much.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is there a better MoE offloading strategy?&lt;/strong&gt; I'm using &lt;code&gt;--n-cpu-moe&lt;/code&gt; but I know there's also the &lt;code&gt;-ot&lt;/code&gt; regex approach.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Would a different quantization help?&lt;/strong&gt; Currently using Q4_K_M. Would IQ4_XS or Q5_K_M be faster/better?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low GPU power usage during inference?&lt;/strong&gt; My cards are power-limited to 275W each, but during inference they only draw ~100-120W. Could this be a bottleneck limiting my token/s?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would love to hear your thoughts and any optimization tips.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T21:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7wdxd</id>
    <title>Curious Why Model File Transfers Are Slow. Moving From One SATA SSD to Another.</title>
    <updated>2026-01-09T02:45:54+00:00</updated>
    <author>
      <name>/u/Five9Fine</name>
      <uri>https://old.reddit.com/user/Five9Fine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7wdxd/curious_why_model_file_transfers_are_slow_moving/"&gt; &lt;img alt="Curious Why Model File Transfers Are Slow. Moving From One SATA SSD to Another." src="https://preview.redd.it/4cyrf3xy78cg1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82c8792e2c3c1bba872175fafe070f4625dd1740" title="Curious Why Model File Transfers Are Slow. Moving From One SATA SSD to Another." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm transferring my models folder (250GB) from one hard drive to another. Both are new SATA SSDS rated at around ~500MB/s. I am getting very slow transfer speeds, around 5MB/s with sporadic bursts of up to 312MB. I know that transfer speed can be very dependent on the structure of the data being transferred but I'm curious if this is normal, is there is something inherent about model file structures that make them slow to transfer? Maybe the issue is with my drives? Both drives are less than a month old but storage on them is at about 80% capacity. All my other files and folders transfer at expected speeds. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Five9Fine"&gt; /u/Five9Fine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4cyrf3xy78cg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7wdxd/curious_why_model_file_transfers_are_slow_moving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7wdxd/curious_why_model_file_transfers_are_slow_moving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T02:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77rxh</id>
    <title>Z-image base model is being prepared for release</title>
    <updated>2026-01-08T09:51:33+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt; &lt;img alt="Z-image base model is being prepared for release" src="https://preview.redd.it/038zb25ok3cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2cbdfd4fbd53811cd0fc218bed6e466b49ff678" title="Z-image base model is being prepared for release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08"&gt;https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/038zb25ok3cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ysj2</id>
    <title>We benchmarked every 4-bit quantization method in vLLM üëÄ</title>
    <updated>2026-01-09T04:38:29+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt; &lt;img alt="We benchmarked every 4-bit quantization method in vLLM üëÄ" src="https://b.thumbs.redditmedia.com/m8O7xkrgA45EVdPp2UmUufoulHAEZRrosao1Uv_SFws.jpg" title="We benchmarked every 4-bit quantization method in vLLM üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just published a deep dive on vLLM quantization. Tested AWQ, GPTQ, Marlin, GGUF, and BitsandBytes on Qwen2.5-32B using an H200.&lt;/p&gt; &lt;p&gt;Stuff we found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Marlin hits 712 tok/s, baseline FP16 does 461. Quantized and faster.&lt;/li&gt; &lt;li&gt;GPTQ without Marlin kernel is actually slower than FP16 (276 tok/s)&lt;/li&gt; &lt;li&gt;BitsandBytes had the smallest quality drop and doesn't need pre-quantized weights&lt;/li&gt; &lt;li&gt;GGUF had the worst perplexity but best HumanEval score among quantized methods&lt;/li&gt; &lt;li&gt;AWQ was weirdly slow in vLLM (67 tok/s)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Blog covers how each technique actually works under the hood if you want the details.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t4212ygj59cg1.png?width=3169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97eff0fcb212924355a7feb7262b25895de5603a"&gt;https://preview.redd.it/t4212ygj59cg1.png?width=3169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97eff0fcb212924355a7feb7262b25895de5603a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://docs.jarvislabs.ai/blog/vllm-quantization-complete-guide-benchmarks"&gt;https://docs.jarvislabs.ai/blog/vllm-quantization-complete-guide-benchmarks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T04:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7a62a</id>
    <title>AI21 Labs releases Jamba2</title>
    <updated>2026-01-08T12:10:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt; &lt;img alt="AI21 Labs releases Jamba2" src="https://b.thumbs.redditmedia.com/Il111fZ012O0JrQgLsZklbi8sbSvI68SHycPLPcigNc.jpg" title="AI21 Labs releases Jamba2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb"&gt;https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-Mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Superior reliability-to-throughput ratio:&lt;/strong&gt; Maintains high performance at 100K+ token contexts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Category-leading benchmarks:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Statistically significant quality wins:&lt;/strong&gt; Outperforms comparable models on real-world enterprise tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes technical manuals, research papers, and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-optimized:&lt;/strong&gt; Lean memory footprint for scalable deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d"&gt;https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devices‚ÄîiPhones, Androids, Macs, and PCs‚Äîwhile maintaining the grounding and instruction-following capabilities required for production use.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;On-device deployment:&lt;/strong&gt; Runs efficiently on iPhones, Androids, Macs, and PCs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-compact footprint:&lt;/strong&gt; 3B parameters enabling edge deployments with minimal resources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark leadership:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes long documents and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SSM-Transformer architecture:&lt;/strong&gt; Memory-efficient design for resource-constrained environments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it works in llama.cpp, tested on my Windows desktop:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71"&gt;https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;fixed blog post &lt;a href="https://www.ai21.com/blog/introducing-jamba2/"&gt;https://www.ai21.com/blog/introducing-jamba2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs are in progress &lt;a href="https://huggingface.co/mradermacher/model_requests/discussions/1683"&gt;https://huggingface.co/mradermacher/model_requests/discussions/1683&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous generation of Jamba models&lt;/p&gt; &lt;p&gt;399B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7xd96</id>
    <title>Gemma-3-4b (null-space) abliteration &amp; RP fine-tune</title>
    <updated>2026-01-09T03:30:31+00:00</updated>
    <author>
      <name>/u/JEs4</name>
      <uri>https://old.reddit.com/user/JEs4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7xd96/gemma34b_nullspace_abliteration_rp_finetune/"&gt; &lt;img alt="Gemma-3-4b (null-space) abliteration &amp;amp; RP fine-tune" src="https://external-preview.redd.it/8MsDm6oseUFMBQKroxuYj3kQ8ddgGPXg7n46GwYAb90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a93da97e2ba4d4ae0a33f7ba1f40c4fc8cc75c24" title="Gemma-3-4b (null-space) abliteration &amp;amp; RP fine-tune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been branching out from research to actually building models recently, and this is my first attempt at applying a lora adapter on top of my abliterations.&lt;/p&gt; &lt;p&gt;I used my null-space abliteration &lt;a href="https://huggingface.co/jwest33/gemma-3-4b-it-null-space-abliterated"&gt;Gemma-3-4B-IT&lt;/a&gt; model with an adapter trained from a subset of the &lt;a href="https://huggingface.co/datasets/lemonilia/LimaRP"&gt;lemonilia/LimaRP&lt;/a&gt; roleplaying dataset. I plan on removing the step limit and reducing the learning rate but wanted to start here.&lt;/p&gt; &lt;p&gt;The model card should have all the information needed to know how I trained it but I'm happy to share anything else if I missed anything. Looking for any feedback before I start on larger models. Thanks! &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer"&gt;https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer-GGUF"&gt;https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JEs4"&gt; /u/JEs4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7xd96/gemma34b_nullspace_abliteration_rp_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7xd96/gemma34b_nullspace_abliteration_rp_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T03:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7dlkn</id>
    <title>Qwen3-VL-Reranker - a Qwen Collection</title>
    <updated>2026-01-08T14:45:00+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt; &lt;img alt="Qwen3-VL-Reranker - a Qwen Collection" src="https://external-preview.redd.it/p_EUBuVnZfgcYfu2zAo996Hix2TFsBWGTVl7mQyY9Tk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b4e2c1310fa6d24d7fb43df7672f7329a04cfbc" title="Qwen3-VL-Reranker - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-reranker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7nqxl</id>
    <title>llama.cpp has Out-of-bounds Write in llama-server</title>
    <updated>2026-01-08T20:56:15+00:00</updated>
    <author>
      <name>/u/radarsat1</name>
      <uri>https://old.reddit.com/user/radarsat1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe good to know for some of you that might be running llama.cpp on a regular basis.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;llama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and prior, the n_discard parameter is parsed directly from JSON input in the llama.cpp server's completion endpoints without validation to ensure it's non-negative. When a negative value is supplied and the context fills up, llama_memory_seq_rm/add receives a reversed range and negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic memory corruption can crash the process or enable remote code execution (RCE). There is no fix at the time of publication.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Also reported &lt;a href="https://security-tracker.debian.org/tracker/CVE-2026-21869"&gt;for Debian&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radarsat1"&gt; /u/radarsat1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cve.org/CVERecord?id=CVE-2026-21869"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:56:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7jd1a</id>
    <title>LFM2.5 1.2B Instruct is amazing</title>
    <updated>2026-01-08T18:17:04+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model punches way above its weight. It outperforms every other model I've tried in this size range and runs smoothly on basically any hardware. If you haven't tried it yet, you definitely should.&lt;/p&gt; &lt;p&gt;Important note:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; We recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7uuxo</id>
    <title>OK I get it, now I love llama.cpp</title>
    <updated>2026-01-09T01:39:13+00:00</updated>
    <author>
      <name>/u/vulcan4d</name>
      <uri>https://old.reddit.com/user/vulcan4d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just made the switch from Ollama to llama.cpp. Ollama is fantastic for the beginner because it lets you super easily run LLMs and switch between them all. Once you realize what you truly want to run, llama.cpp is really the way to go.&lt;/p&gt; &lt;p&gt;My hardware ain't great, I have a single 3060 12GB GPU and three P102-100 GPUs for a total of 42GB. My system ram is 96GB along with an Intel i7-9800x. It blows my mind that with some tuning what difference it can make. You really need to understand each of the commands for llama.cpp to get the most out of it especially with uneven vram like mine. I used Chatgpt, Perplexity and suprisingly only Google AI studio could optimize my settings while teaching me along the way.&lt;/p&gt; &lt;p&gt;Crazy how these two commands both fill up the ram but one is twice as fast as the other. Chatgpt helped me with the first one, Google AI with the other ;). Now I'm happy running local lol.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;11t/s:&lt;/strong&gt;&lt;br /&gt; sudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo CUDA_VISIBLE_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf --n-gpu-layers 21 --main-gpu 0 --flash-attn off --cache-type-k q8_0 --cache-type-v f16 --ctx-size 30000 --port 8080 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --mmap --numa distribute --batch-size 384 --ubatch-size 256 --jinja --threads $(nproc) --parallel 2 --tensor-split 12,10,10,10 --mlock&lt;/p&gt; &lt;p&gt;&lt;strong&gt;21t/s&lt;/strong&gt;&lt;br /&gt; sudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo GGML_CUDA_ENABLE_UNIFIED_MEMORY=0 CUDA_VISIBLE_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf --n-gpu-layers 99 --main-gpu 0 --split-mode layer --tensor-split 5,5,6,20 -ot &amp;quot;blk\.(2[1-9]|[3-9][0-9])\.ffn_.*_exps\.weight=CPU&amp;quot; --ctx-size 30000 --port 8080 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --batch-size 512 --ubatch-size 256 --threads 8 --parallel 1 --mlock&lt;/p&gt; &lt;p&gt;Nothing here is worth copying and pasting as it is unique to my config but the moral of the story is, if you tune llama.cpp this thing will FLY!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vulcan4d"&gt; /u/vulcan4d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T01:39:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7mvuf</id>
    <title>Z.ai (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange</title>
    <updated>2026-01-08T20:23:59+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Zai_org/status/2009290783678239032"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:23:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7d8bj</id>
    <title>Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt</title>
    <updated>2026-01-08T14:29:47+00:00</updated>
    <author>
      <name>/u/Prior-Arm-6705</name>
      <uri>https://old.reddit.com/user/Prior-Arm-6705</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt; &lt;img alt="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" src="https://external-preview.redd.it/M2cyNzBqaHB4NGNnMeuNas4_kS8fQc08s_eqp1ss4JB4szq45v23OyPEbFog.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a660094eff764f4c2c968c5de87ba1bafcb35b9" title="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone had to count it. Turns out Jensen said &amp;quot;AI&amp;quot; exactly 121 times in the CES 2025 keynote.&lt;/p&gt; &lt;p&gt;I used &lt;a href="https://github.com/OpenAgentPlatform/Dive"&gt;https://github.com/OpenAgentPlatform/Dive&lt;/a&gt; (open-source MCP client) + two MCPs I made:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/kevinwatt/yt-dlp-mcp"&gt;https://github.com/kevinwatt/yt-dlp-mcp&lt;/a&gt; - YouTube download&lt;br /&gt; - &lt;a href="https://github.com/kevinwatt/ffmpeg-mcp-lite"&gt;https://github.com/kevinwatt/ffmpeg-mcp-lite&lt;/a&gt; - video editing&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One prompt:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Task: Create a compilation video of every exact moment Jensen Huang says &amp;quot;AI&amp;quot;.&lt;br /&gt; Video source: &lt;a href="https://www.youtube.com/watch?v=0NBILspM4c4"&gt;https://www.youtube.com/watch?v=0NBILspM4c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instructions:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)&lt;/p&gt; &lt;p&gt;Parse JSON3 to find every &amp;quot;AI&amp;quot; instance with precise start/end times&lt;/p&gt; &lt;p&gt;Use ffmpeg to cut clips (~50-100ms padding for natural sound)&lt;/p&gt; &lt;p&gt;Concatenate all clips chronologically&lt;/p&gt; &lt;p&gt;Output: Jensen_CES_AI.mp4&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Dive chained the two MCPs together - download ‚Üí parse timestamps ‚Üí cut 121 clips ‚Üí merge. All local, no cloud.&lt;/p&gt; &lt;p&gt;If you want to see how it runs: &lt;a href="https://www.youtube.com/watch?v=u_7OtyYAX74"&gt;https://www.youtube.com/watch?v=u_7OtyYAX74&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The result is... hypnotic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Arm-6705"&gt; /u/Prior-Arm-6705 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hein55gpx4cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7qcux</id>
    <title>The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.</title>
    <updated>2026-01-08T22:33:33+00:00</updated>
    <author>
      <name>/u/PostEasy7183</name>
      <uri>https://old.reddit.com/user/PostEasy7183</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, ‚ÄãI‚Äôve been reading the text of the &amp;quot;NO FAKES Act&amp;quot; currently in Congress, and it‚Äôs worse than I thought. ‚ÄãThe Tldr: It creates a &amp;quot;digital replica right&amp;quot; for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who &amp;quot;makes available&amp;quot; a tool that is primarily used for replicas.&lt;br /&gt; ‚ÄãThe Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation). ‚ÄãThere is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.&lt;/p&gt; &lt;p&gt;What I did: I contacted my reps email to flag this as an &amp;quot;innovation killer.&amp;quot; If you run a repo or care about open weights, you might want to do the same. We need them to add a &amp;quot;Safe Harbor&amp;quot; for tool devs.&lt;/p&gt; &lt;p&gt;S.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress &lt;a href="https://share.google/u6dpy7ZQDvZWUrlfc"&gt;https://share.google/u6dpy7ZQDvZWUrlfc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;UPDATE: ACTION ITEMS (How to actually stop this) ‚ÄãIf you don't want to go to jail for hosting a repo, you need to make noise now. ‚Äã1. The &amp;quot;Lazy&amp;quot; Email (Takes 30 seconds): Go to Democracy.io or your Senator‚Äôs contact page. ‚ÄãSubject: Opposition to NO FAKES Act (H.R. 2794 / S. 1367) - Open Source Liability ‚ÄãMessage: &amp;quot;I am a constituent and software engineer. I oppose the NO FAKES Act unless it includes a specific Safe Harbor for Open Source Code Repositories. The current 'Digital Fingerprinting' requirement (Section 3) is technically impossible for raw model weights to comply with. This bill effectively bans open-source AI hosting in the US and hands a monopoly to Big Tech. Please amend it to protect tool developers.&amp;quot; ‚Äã2. The &amp;quot;Nuclear&amp;quot; Option (Call them): ‚ÄãCall the Capitol Switchboard: (202) 224-3121 ‚ÄãAsk for Senators Wyden (D) or Massie (R) if you want to thank them for being tech-literate, or call your own Senator to complain. ‚ÄãScript: &amp;quot;The NO FAKES Act kills open-source innovation. We need a Safe Harbor for developers who write code, separate from the bad actors who use it.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PostEasy7183"&gt; /u/PostEasy7183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T22:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
