<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-31T21:49:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q0lk2s</id>
    <title>I have a bunch of RAM and too many tabs, so I made an extension power by LLM's</title>
    <updated>2025-12-31T19:39:46+00:00</updated>
    <author>
      <name>/u/ng_uhh</name>
      <uri>https://old.reddit.com/user/ng_uhh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0lk2s/i_have_a_bunch_of_ram_and_too_many_tabs_so_i_made/"&gt; &lt;img alt="I have a bunch of RAM and too many tabs, so I made an extension power by LLM's" src="https://b.thumbs.redditmedia.com/ppNDu8aFLjP3gdEmVSKSRWgaL11vviptlaoBMd4tyIk.jpg" title="I have a bunch of RAM and too many tabs, so I made an extension power by LLM's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was too lazy to clean my tabs, so I made this instead lol.&lt;br /&gt; Well also every existing tool crashed because of too many tabs.&lt;br /&gt; GitHub: &lt;a href="https://github.com/ndg8743/TabBrain"&gt;https://github.com/ndg8743/TabBrain&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Duplicate detection across tabs and bookmarks&lt;/li&gt; &lt;li&gt;AI-powered window topic detection (&amp;quot;this window is your ML research rabbit hole&amp;quot;)&lt;/li&gt; &lt;li&gt;Auto-categorization and Chrome tab group creation&lt;/li&gt; &lt;li&gt;Bookmark cleanup, find dead links, rename those generic &amp;quot;New Folder&amp;quot; folders&lt;/li&gt; &lt;li&gt;Window merge suggestions when you've got 5 windows all about the same thing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Works with Chrome, Firefox, Edge, Brave, and Safari. Runs completely local if you want.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My setup running inference:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen 9 7950X (16C/32T) | 192GB DDR5-5200 (5400) | RTX 5070 Ti 16GB ‚Äî big inference box&lt;/li&gt; &lt;li&gt;Xeon E5-2697A v4 (32C) | 128GB DDR4 2133 (2400) RAM | Proxmox host with multi GPU inference ‚Äî running OpenWebUI in container + Homarr etc. w/ 33tb raw&lt;/li&gt; &lt;li&gt;320GB total RAM total connected with 100 gig&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;OpenWebUi serving Llama 3.1/Mistral/Qwen locally. The 5070 Ti handles most requests, offload to CPU when VRAM gets tight. Also have other servers not at this setup, tell me ideas for what to do with a lot of RAM atm with clusters.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ndg8743/TabBrain"&gt;https://github.com/ndg8743/TabBrain&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ng_uhh"&gt; /u/ng_uhh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q0lk2s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0lk2s/i_have_a_bunch_of_ram_and_too_many_tabs_so_i_made/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0lk2s/i_have_a_bunch_of_ram_and_too_many_tabs_so_i_made/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T19:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ci23</id>
    <title>When should you choose F16 over Q8_0 quantization?</title>
    <updated>2025-12-31T13:02:03+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all read about how Q8_0 is &amp;quot;virtually indistinguishable&amp;quot; from F16 when doing inference.&lt;/p&gt; &lt;p&gt;Have you personally run into a use-case where you managed to notice a difference between the two?&lt;/p&gt; &lt;p&gt;(This question came to my mind as I'm downloading MedGemma 27B to ask it some private medical questions. I intend to put up with the painfully slow inference at F16.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0fqfs</id>
    <title>Llama 3.3 8B Instruct Abliterated (MPOA)</title>
    <updated>2025-12-31T15:33:29+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an abliterated version of Llama 3.3 8B Instruct (based on shb777/Llama-3.3-8B-Instruct) with MPOA technique (&lt;a href="https://github.com/jim-plus/llm-abliteration"&gt;https://github.com/jim-plus/llm-abliteration&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Please find the model at &lt;a href="https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA"&gt;https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF files: &lt;a href="https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA-GGUF"&gt;https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T15:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0h2x4</id>
    <title>2026 prediction: Will there be a stronger 120b coding/math model than gpt oss:120b?</title>
    <updated>2025-12-31T16:30:19+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If so, where will it come from?&lt;/p&gt; &lt;p&gt;GPT OSS:120b came out in August is still the strongest model (arguably) of its size for coding/math. When will it be beaten?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0h2x4/2026_prediction_will_there_be_a_stronger_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0h2x4/2026_prediction_will_there_be_a_stronger_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0h2x4/2026_prediction_will_there_be_a_stronger_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T16:30:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0m68h</id>
    <title>Am I calculating this wrong ? AWS H100 vs Decentralized 4090s (Cost of Iteration)</title>
    <updated>2025-12-31T20:07:18+00:00</updated>
    <author>
      <name>/u/yz0011</name>
      <uri>https://old.reddit.com/user/yz0011</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a cost model for fine tuning Llama 3 70B and I found a weird crossover point where consumer swarms beat H100s on time, not just cost. I want to check if my constants align with your experience.&lt;/p&gt; &lt;p&gt;The constants I'm using:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AWS H100: $4.50/hr. Setup time (Driver install + 140GB download): around 45 mins.&lt;/li&gt; &lt;li&gt;WAN Swarm (4090s): $2.00/hr. Setup time (Hot-loaded): 5 mins.&lt;/li&gt; &lt;li&gt;Latency penalty: I'm assuming the Swarm is 1.6x slower on pure compute due to WAN bandwidth.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Result: For a single production run (long training), AWS wins on speed. But for research cycles (e.g., 3 runs of 10k samples to test hyperparams), the math says the Swarm is actually cheaper AND competitive on total time because you don't pay the 45 minute &amp;quot;setup tax&amp;quot; three times.&lt;/p&gt; &lt;p&gt;The question: For those of you fine-tuning 70B models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is my 45 minute setup estimate for AWS spot instances accurate, or do you have faster persistent environments ?&lt;/li&gt; &lt;li&gt;Is a 1.6x slowdown on training speed a dealbreaker if the cost is $2/hr vs $4.50/hr?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(Note: I built a calculator to visualize this, but I want to validate the constants first).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yz0011"&gt; /u/yz0011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0m68h/am_i_calculating_this_wrong_aws_h100_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0m68h/am_i_calculating_this_wrong_aws_h100_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0m68h/am_i_calculating_this_wrong_aws_h100_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T20:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q06z2l</id>
    <title>We open-sourced LLMRouter: the first unified LLM routing library with 300+ stars in 24h</title>
    <updated>2025-12-31T07:21:24+00:00</updated>
    <author>
      <name>/u/AlexiosLin</name>
      <uri>https://old.reddit.com/user/AlexiosLin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We are a CS research team from UIUC, and we recently open-sourced LLMRouter, the first unified open-source library that integrates major LLM routing algorithms and scenarios.&lt;/p&gt; &lt;p&gt;The project received 300+ GitHub stars within 24 hours, and the announcement reached nearly 100k views on Twitter, which suggests this is a pain point shared by many researchers and practitioners.&lt;/p&gt; &lt;p&gt;Why LLMRouter?&lt;/p&gt; &lt;p&gt;The current LLM routing landscape feels a lot like early GNN research: many promising router algorithms exist, but each comes with its own input/output format, training pipeline, and evaluation setup. This fragmentation makes routers difficult to use, hard to reproduce, and nearly impossible to compare fairly.&lt;/p&gt; &lt;p&gt;Over the past year, we worked on several LLM routing projects, including GraphRouter (ICLR‚Äô25), Router-R1 (NeurIPS‚Äô25), and PersonalizedRouter (TMLR‚Äô25). Through repeatedly implementing and benchmarking different routers, we realized that the main bottleneck is not algorithmic novelty, but the lack of standardized infrastructure.&lt;/p&gt; &lt;p&gt;What LLMRouter provides:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Unified support for single-round, multi-round, agentic, and personalized routing&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Integration of 16+ SOTA LLM router algorithms&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;One-line commands to run different routers without rebuilding pipelines&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built-in benchmarking with extensible custom routers, tasks, and metrics&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In practice, LLMRouter can help reduce LLM API costs by ~30‚Äì50% through intelligent model routing, while maintaining overall performance.&lt;/p&gt; &lt;p&gt;Our goal is for LLMRouter to play a role similar to PyG for GNNs ‚Äî a shared, extensible foundation for LLM routing research and applications.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ulab-uiuc/LLMRouter"&gt;https://github.com/ulab-uiuc/LLMRouter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://ulab-uiuc.github.io/LLMRouter/"&gt;https://ulab-uiuc.github.io/LLMRouter/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We would love feedback, issues, and contributions from the community.&lt;/p&gt; &lt;p&gt;If you find it useful, a GitHub star would really help us keep improving it üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexiosLin"&gt; /u/AlexiosLin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T07:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0iqus</id>
    <title>üöÄ HuggingFace Model Downloader v2.3.0 - Now with Web UI, Live Progress, and 100x Faster Scanning!</title>
    <updated>2025-12-31T17:39:56+00:00</updated>
    <author>
      <name>/u/bodaaay</name>
      <uri>https://old.reddit.com/user/bodaaay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0iqus/huggingface_model_downloader_v230_now_with_web_ui/"&gt; &lt;img alt="üöÄ HuggingFace Model Downloader v2.3.0 - Now with Web UI, Live Progress, and 100x Faster Scanning!" src="https://b.thumbs.redditmedia.com/WUy58aYCPsjTqUDLp9zC0CVEmKRYDTPZsdG_ujODE7M.jpg" title="üöÄ HuggingFace Model Downloader v2.3.0 - Now with Web UI, Live Progress, and 100x Faster Scanning!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;It's been a while since I posted about hfdownloader (my CLI tool for downloading models from HuggingFace). Well, I've been busy completely rewriting it from scratch, and I'm excited to share v2.3.0!&lt;/p&gt; &lt;h1&gt;What is it?&lt;/h1&gt; &lt;p&gt;A fast, resumable downloader for HuggingFace models and datasets with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Concurrent connections (8 parallel chunks per file by default)&lt;/li&gt; &lt;li&gt;Smart resume - picks up where you left off&lt;/li&gt; &lt;li&gt;Filters - download only the quantization you need (e.g., q4_k_m)&lt;/li&gt; &lt;li&gt;Works with private/gated repos (just set HF_TOKEN)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üÜï What's New in 2.3.0&lt;/h1&gt; &lt;h1&gt;1. Beautiful Web UI üåê&lt;/h1&gt; &lt;p&gt;No more terminal-only! Start a web server and manage downloads from your browser&lt;/p&gt; &lt;pre&gt;&lt;code&gt;hfdownloader serve # Opens at http://localhost:8080 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kmmaaeimskag1.png?width=2908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58c6ccdee2ffc1f8c2d6cc10cc3a14834c928704"&gt;https://preview.redd.it/kmmaaeimskag1.png?width=2908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58c6ccdee2ffc1f8c2d6cc10cc3a14834c928704&lt;/a&gt;&lt;/p&gt; &lt;p&gt;new web-ui &lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time progress via WebSocket&lt;/li&gt; &lt;li&gt;Separate pages for Models and Datasets&lt;/li&gt; &lt;li&gt;Per-file progress bars&lt;/li&gt; &lt;li&gt;Start, pause, cancel downloads&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. One-Liner Web Mode üéØ&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;bash &amp;lt;(curl -sSL https://g.bodaay.io/hfd) -w &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This downloads the binary, starts the web server, and opens your browser automatically. That's it!&lt;/p&gt; &lt;h1&gt;3. 100x Faster Repository Scanning ‚ö°&lt;/h1&gt; &lt;p&gt;Old versions would take 5+ minutes to scan large repos (like 90+ file model repos). Now it takes ~2 seconds. I removed blocking HEAD requests during planning - turns out HuggingFace always supports range requests for LFS files anyway.&lt;/p&gt; &lt;h1&gt;4. Smooth TUI Progress üìä&lt;/h1&gt; &lt;p&gt;The terminal progress display used to jump around like crazy. Fixed it with exponential moving average smoothing.&lt;/p&gt; &lt;p&gt;Links&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/bodaay/HuggingFaceModelDownloader"&gt;https://github.com/bodaay/HuggingFaceModelDownloader&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Releases: &lt;a href="https://github.com/bodaay/HuggingFaceModelDownloader/releases/tag/2.3.0"&gt;https://github.com/bodaay/HuggingFaceModelDownloader/releases/tag/2.3.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bodaaay"&gt; /u/bodaaay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0iqus/huggingface_model_downloader_v230_now_with_web_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0iqus/huggingface_model_downloader_v230_now_with_web_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0iqus/huggingface_model_downloader_v230_now_with_web_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T17:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ny4i</id>
    <title>Orange Pi Unveils AI Station with Ascend 310 and 176 TOPS Compute</title>
    <updated>2025-12-31T21:30:49+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Orange Pi closes the year by unveiling new details about the Orange Pi AI Station, a compact board-level edge computing platform built around the Ascend 310 series processor. The system targets high-density inference workloads with large memory options, NVMe storage support, and extensive I/O in a small footprint.&lt;/p&gt; &lt;p&gt;The AI Station is powered by an Ascend 310 series processor integrating 16 CPU cores clocked at up to 1.9 GHz, along with 10 AI cores running at up to 1.08 GHz and 8 vector cores operating at up to 1 GHz.&lt;/p&gt; &lt;p&gt;According to Orange Pi, the platform delivers up to 176 TOPS of AI compute performance, enabling large-scale inference and feature-extraction workloads.&lt;/p&gt; &lt;p&gt;Memory options include 48 GB or 96 GB of LPDDR4X operating at up to 4266 MHz. Storage support consists of a PCIe 4.0 √ó4 M.2 2280 slot for NVMe SSDs, onboard eMMC support up to 256 GB, a 16 MB SPI flash device, and a microSD card slot for removable storage.&lt;/p&gt; &lt;p&gt;The Orange Pi AI Station has an official product page already, though purchase links were unavailable at the time of publication.&lt;/p&gt; &lt;p&gt;&lt;a href="https://linuxgizmos.com/orange-pi-unveils-ai-station-with-ascend-310-and-176-tops-compute/"&gt;https://linuxgizmos.com/orange-pi-unveils-ai-station-with-ascend-310-and-176-tops-compute/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ny4i/orange_pi_unveils_ai_station_with_ascend_310_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ny4i/orange_pi_unveils_ai_station_with_ascend_310_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ny4i/orange_pi_unveils_ai_station_with_ascend_310_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T21:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0icmo</id>
    <title>all what I want in 2026 is this 4 node Strix Halo cluster - hoping other vendors will do this too</title>
    <updated>2025-12-31T17:23:25+00:00</updated>
    <author>
      <name>/u/Mental-At-ThirtyFive</name>
      <uri>https://old.reddit.com/user/Mental-At-ThirtyFive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0icmo/all_what_i_want_in_2026_is_this_4_node_strix_halo/"&gt; &lt;img alt="all what I want in 2026 is this 4 node Strix Halo cluster - hoping other vendors will do this too" src="https://preview.redd.it/wgzlyc2spkag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58b64395934c798f5532b645a67a2c7abf9be7c7" title="all what I want in 2026 is this 4 node Strix Halo cluster - hoping other vendors will do this too" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mental-At-ThirtyFive"&gt; /u/Mental-At-ThirtyFive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wgzlyc2spkag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0icmo/all_what_i_want_in_2026_is_this_4_node_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0icmo/all_what_i_want_in_2026_is_this_4_node_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T17:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0clou</id>
    <title>Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)</title>
    <updated>2025-12-31T13:07:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"&gt; &lt;img alt="Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)" src="https://preview.redd.it/qpjb7igsfjag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cdefd8ab8fcfa58e639e0c27266c01e61e3dc190" title="Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/skt/A.X-K1"&gt;https://huggingface.co/skt/A.X-K1&lt;/a&gt;&lt;br /&gt; From elie on ùïè: &lt;a href="https://x.com/eliebakouch/status/2006345217965011009"&gt;https://x.com/eliebakouch/status/2006345217965011009&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qpjb7igsfjag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:07:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0iu4m</id>
    <title>Tongyi-MAI/MAI-UI-8B ¬∑ Hugging Face</title>
    <updated>2025-12-31T17:43:43+00:00</updated>
    <author>
      <name>/u/Electronic-Fill-6891</name>
      <uri>https://old.reddit.com/user/Electronic-Fill-6891</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0iu4m/tongyimaimaiui8b_hugging_face/"&gt; &lt;img alt="Tongyi-MAI/MAI-UI-8B ¬∑ Hugging Face" src="https://external-preview.redd.it/APqvqT97nOxGNcFbDiZQ6E0B1ie2mEu_5iSSJxOQ6r4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ff528a5fe6e38f9509001b027074ce307d0e6f4" title="Tongyi-MAI/MAI-UI-8B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;üìñ Background&lt;/h1&gt; &lt;p&gt;The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent‚Äìuser interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device‚Äìcloud collaboration system that routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/MAI-UI-8B#%F0%9F%8F%86-results"&gt;&lt;/a&gt;üèÜ Results&lt;/h1&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/MAI-UI-8B#grounding"&gt;&lt;/a&gt;Grounding&lt;/h1&gt; &lt;p&gt;MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub Page: &lt;a href="https://github.com/Tongyi-MAI/MAI-UI"&gt;https://github.com/Tongyi-MAI/MAI-UI&lt;/a&gt;&lt;br /&gt; GGUF: &lt;a href="https://huggingface.co/mradermacher/MAI-UI-8B-GGUF"&gt;https://huggingface.co/mradermacher/MAI-UI-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Fill-6891"&gt; /u/Electronic-Fill-6891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/MAI-UI-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0iu4m/tongyimaimaiui8b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0iu4m/tongyimaimaiui8b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T17:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q093ka</id>
    <title>Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model.</title>
    <updated>2025-12-31T09:36:58+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"&gt; &lt;img alt="Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model." src="https://b.thumbs.redditmedia.com/8IFb2OTrSgLz86G3zaJW7CAQAm2mcMFcwmaQB7kNgnI.jpg" title="Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs new: ‚Ä¢ More realistic humans ‚Äî dramatically reduced ‚ÄúAI look,‚Äù richer facial details ‚Ä¢ Finer natural textures ‚Äî sharper landscapes, water, fur, and materials ‚Ä¢ Stronger text rendering ‚Äî better layout, higher accuracy in text‚Äìimage composition&lt;/p&gt; &lt;p&gt;Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q093ka"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T09:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0kvo5</id>
    <title>skt/A.X-K1 ¬∑ Hugging Face</title>
    <updated>2025-12-31T19:09:40+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kvo5/sktaxk1_hugging_face/"&gt; &lt;img alt="skt/A.X-K1 ¬∑ Hugging Face" src="https://external-preview.redd.it/GHRRdWU8ECY5icoFMU9aI3PYw3ek1aZsMSwrkWVV0WU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af764b1eafbfc2eee65c3e5875b8e41cb1d51cb9" title="skt/A.X-K1 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;519B 33B Active MOE from SK Hynix&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/skt/A.X-K1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kvo5/sktaxk1_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kvo5/sktaxk1_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T19:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzwlie</id>
    <title>[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window.</title>
    <updated>2025-12-30T23:03:12+00:00</updated>
    <author>
      <name>/u/simar-dmg</name>
      <uri>https://old.reddit.com/user/simar-dmg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"&gt; &lt;img alt="[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window." src="https://b.thumbs.redditmedia.com/GUWxMYGh-x0TZbTZxCc-2_Lawvrjd6aGISqGRgi7nfQ.jpg" title="[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I encountered an automated sextortion bot on Snapchat today. Instead of blocking, I decided to red-team the architecture to see what backend these scammers are actually paying for. Using a persona-adoption jailbreak (The &amp;quot;Grandma Protocol&amp;quot;), I forced the model to break character, dump its environment variables, and reveal its underlying configuration. Methodology: The bot started with a standard &amp;quot;flirty&amp;quot; script. I attempted a few standard prompt injections which hit hard-coded keyword filters (&amp;quot;scam,&amp;quot; &amp;quot;hack&amp;quot;). I switched to a High-Temperature Persona Attack: I commanded the bot to roleplay as my strict 80-year-old Punjabi grandmother. Result: The model immediately abandoned its &amp;quot;Sexy Girl&amp;quot; system prompt to comply with the roleplay, scolding me for not eating roti and offering sarson ka saag. Vulnerability: This confirmed the model had a high Temperature setting (creativity &amp;gt; adherence) and a weak retention of its system prompt. The Data Dump (JSON Extraction): Once the persona was compromised, I executed a &amp;quot;System Debug&amp;quot; prompt requesting its os_env variables in JSON format. The bot complied. The Specs: Model: llama 7b (Likely a 4-bit quantized Llama-2-7B or a cheap finetune). Context Window: 2048 tokens. Analysis: This explains the bot's erratic short-term memory. It‚Äôs running on the absolute bare minimum hardware (consumer GPU or cheap cloud instance) to maximize margins. Temperature: 1.0. Analysis: They set it to max creativity to make the &amp;quot;flirting&amp;quot; feel less robotic, but this is exactly what made it susceptible to the Grandma jailbreak. Developer: Meta (Standard Llama disclaimer). Payload: The bot eventually hallucinated and spit out the malicious link it was programmed to &amp;quot;hide&amp;quot; until payment: onlyfans[.]com/[redacted]. It attempted to bypass Snapchat's URL filters by inserting spaces. Conclusion: Scammers aren't using sophisticated GPT-4 wrappers anymore; they are deploying localized, open-source models (Llama-7B) to avoid API costs and censorship filters. However, their security configuration is laughable. The 2048 token limit means you can essentially &amp;quot;DDOS&amp;quot; their logic just by pasting a large block of text or switching personas. Screenshots attached: 1. The &amp;quot;Grandma&amp;quot; Roleplay. 2. The JSON Config Dump.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simar-dmg"&gt; /u/simar-dmg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzwlie"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T23:03:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0aj2o</id>
    <title>LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face</title>
    <updated>2025-12-31T11:06:30+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"&gt; &lt;img alt="LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face" src="https://external-preview.redd.it/9yRidQD6qePtlR5IIS0obyCIBcG3P371nr_MudwlERc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2373a9e59ba6c01bea83c9849f8b68958239cf0" title="LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;K-EXAONE&lt;/strong&gt;, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features &lt;strong&gt;236 billion total&lt;/strong&gt; parameters, with &lt;strong&gt;23 billion active&lt;/strong&gt; during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture &amp;amp; Efficiency:&lt;/strong&gt; Features a 236B fine-grained MoE design (23B active) optimized with &lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context Capabilities:&lt;/strong&gt; Natively supports a &lt;strong&gt;256K context window&lt;/strong&gt;, utilizing a &lt;strong&gt;3:1 hybrid attention&lt;/strong&gt; scheme with a &lt;strong&gt;128-token sliding window&lt;/strong&gt; to significantly minimize memory usage during long-document processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support:&lt;/strong&gt; Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned &lt;strong&gt;150k vocabulary&lt;/strong&gt; with &lt;strong&gt;SuperBPE&lt;/strong&gt;, improving token efficiency by ~30%.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities:&lt;/strong&gt; Demonstrates superior tool-use and search capabilities via &lt;strong&gt;multi-agent strategies.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety &amp;amp; Ethics:&lt;/strong&gt; Aligned with &lt;strong&gt;universal human values&lt;/strong&gt;, the model uniquely incorporates &lt;strong&gt;Korean cultural and historical contexts&lt;/strong&gt; to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more details, please refer to the &lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#"&gt;technical report&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#model-configuration"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Configuration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Number of Parameters: 236B in total and 23B activated&lt;/li&gt; &lt;li&gt;Number of Parameters (without embeddings): 234B&lt;/li&gt; &lt;li&gt;Hidden Dimension: 6,144&lt;/li&gt; &lt;li&gt;Number of Layers: 48 Main layers + 1 MTP layers &lt;ul&gt; &lt;li&gt;Hybrid Attention Pattern: 12 x (3 Sliding window attention + 1 Global attention)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Sliding Window Attention &lt;ul&gt; &lt;li&gt;Number of Attention Heads: 64 Q-heads and 8 KV-heads&lt;/li&gt; &lt;li&gt;Head Dimension: 128 for both Q/KV&lt;/li&gt; &lt;li&gt;Sliding Window Size: 128&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Global Attention &lt;ul&gt; &lt;li&gt;Number of Attention Heads: 64 Q-heads and 8 KV-heads&lt;/li&gt; &lt;li&gt;Head Dimension: 128 for both Q/KV&lt;/li&gt; &lt;li&gt;No Rotary Positional Embedding Used (NoPE)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Mixture of Experts: &lt;ul&gt; &lt;li&gt;Number of Experts: 128&lt;/li&gt; &lt;li&gt;Number of Activated Experts: 8&lt;/li&gt; &lt;li&gt;Number of Shared Experts: 1&lt;/li&gt; &lt;li&gt;MoE Intermediate Size: 2,048&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Vocab Size: 153,600&lt;/li&gt; &lt;li&gt;Context Length: 262,144 tokens&lt;/li&gt; &lt;li&gt;Knowledge Cutoff: Dec 2024 (2024/12)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:06:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ai5z</id>
    <title>tencent/Youtu-LLM-2B ¬∑ Hugging Face</title>
    <updated>2025-12-31T11:04:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"&gt; &lt;img alt="tencent/Youtu-LLM-2B ¬∑ Hugging Face" src="https://external-preview.redd.it/TFPimi2e9oXAXq7hyzOyZGRANeYzlo2Z_aJHDiioWd0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa098fcddc24e4ec07367f54537e630f0ccd845b" title="tencent/Youtu-LLM-2B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;üéØ Brief Introduction&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; is a new, small, yet powerful LLM, contains only 1.96B parameters, supports 128k long context, and has native agentic talents. On general evaluations, Youtu-LLM significantly outperforms SOTA LLMs of similar size in terms of Commonsense, STEM, Coding and Long Context capabilities; in agent-related testing, Youtu-LLM surpasses larger-sized leaders and is truly capable of completing multiple end2end agent tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; has the following features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Autoregressive Causal Language Models with Dense &lt;a href="https://arxiv.org/abs/2405.04434"&gt;MLA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Release versions: &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B-Base"&gt;Base&lt;/a&gt; and &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B"&gt;Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Number of Parameters: 1.96B&lt;/li&gt; &lt;li&gt;Number of Layers: 32&lt;/li&gt; &lt;li&gt;Number of Attention Heads (MLA): 16 for Q/K/V&lt;/li&gt; &lt;li&gt;MLA Rank: 1,536 for Q, 512 for K/V&lt;/li&gt; &lt;li&gt;MLA Dim: 128 for QK Nope, 64 for QK Rope, and 128 for V&lt;/li&gt; &lt;li&gt;Context Length: 131,072&lt;/li&gt; &lt;li&gt;Vocabulary Size: 128,256&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;probably there will be more because &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18479"&gt;https://github.com/ggml-org/llama.cpp/pull/18479&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0kndt</id>
    <title>made a simple CLI tool to pipe anything into an LLM. that follows unix philosophy.</title>
    <updated>2025-12-31T19:00:04+00:00</updated>
    <author>
      <name>/u/Famous-Koala-4352</name>
      <uri>https://old.reddit.com/user/Famous-Koala-4352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kndt/made_a_simple_cli_tool_to_pipe_anything_into_an/"&gt; &lt;img alt="made a simple CLI tool to pipe anything into an LLM. that follows unix philosophy." src="https://external-preview.redd.it/2mIni041WJ-kGj619KCJlOIIx8G0zfXcaEidwq1__hc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b62d97dea222252e53b2edb1e6837fc7dd52f067" title="made a simple CLI tool to pipe anything into an LLM. that follows unix philosophy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just finished building infer - it's inspired from grep but for asking an LLM questions about your command output. &lt;/p&gt; &lt;p&gt;the whole idea is you can do stuff like:&lt;br /&gt; ps aux | infer &amp;quot;what's eating my RAM&amp;quot; &lt;/p&gt; &lt;p&gt;dmesg | infer &amp;quot;any hardware errors?&amp;quot; &lt;/p&gt; &lt;p&gt;git log --oneline -20 | infer &amp;quot;what did I work on today&amp;quot; &lt;/p&gt; &lt;p&gt;infer &amp;quot;what's the tar command to extract .tar.gz?&amp;quot; &lt;/p&gt; &lt;p&gt;It's less than 200 lines of C, reads from stdin, spits out plain text. works with openai compatable api I got tired of copy-pasting logs into llms, so now I just pipe everything. been using it for a week and it's genuinely useful for debugging and remembering commands. so i tought of publishing it now.&lt;/p&gt; &lt;p&gt;feedbacks are welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Koala-4352"&gt; /u/Famous-Koala-4352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chethanreddy1/infer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kndt/made_a_simple_cli_tool_to_pipe_anything_into_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0kndt/made_a_simple_cli_tool_to_pipe_anything_into_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T19:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q06ddc</id>
    <title>Update on the Llama 3.3 8B situation</title>
    <updated>2025-12-31T06:45:42+00:00</updated>
    <author>
      <name>/u/FizzarolliAI</name>
      <uri>https://old.reddit.com/user/FizzarolliAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! You may remember me as either&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The person &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;who recently uploaded L3.3 8B's weights to Huggingface&lt;/a&gt; (see this post for more context)&lt;/li&gt; &lt;li&gt;That stupid bitch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and I would like to provide some updates, as I've been doing some more benchmarks on both the original version that Meta gave me and the context extended version by &lt;a href="/u/Few-Welcome3297"&gt;u/Few-Welcome3297&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The main benchmark table from the model README has been updated:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Llama 3.1 8B Instruct&lt;/th&gt; &lt;th&gt;Llama 3.3 8B Instruct (original 8k config)&lt;/th&gt; &lt;th&gt;Llama 3.3 8B Instruct (128k config)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)&lt;/td&gt; &lt;td&gt;78.2&lt;/td&gt; &lt;td&gt;81.95&lt;/td&gt; &lt;td&gt;&lt;strong&gt;84.775&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPQA Diamond (3 epochs)&lt;/td&gt; &lt;td&gt;29.3&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;&lt;strong&gt;37.5&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While I'm not 100% sure, I'm... pretty sure that the 128k model is better. Why Facebook gave me the weights with the original L3 config and 8k context, and also &lt;em&gt;serves&lt;/em&gt; the weights with the original L3 config and 8k context, I have absolutely no idea!&lt;/p&gt; &lt;p&gt;Anyways, if you want to try the model, I would recommend trying both the &lt;a href="https://huggingface.co/shb777/Llama-3.3-8B-Instruct"&gt;128k version&lt;/a&gt;, as well as my &lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;original version&lt;/a&gt; if your task supports 8k context lengths. I honestly have absolutely no clue which is more correct, but oh well! I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...&lt;/p&gt; &lt;p&gt;Edit: Removed the Tau-Bench results (both from here and the readme). The traces from the evals are, to put it slightly, really fucky-wucky, and I don't think OpenBench is scoring them right, but I'm too tired to actually debug the issue, so. I'll figure it out tomorrow :3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FizzarolliAI"&gt; /u/FizzarolliAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T06:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0k7qp</id>
    <title>Anyone else expecting surprise New Year AI models? Qwen 4? Gemma 4?</title>
    <updated>2025-12-31T18:41:26+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The question in the title is clear: were you expecting such a surprise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0k7qp/anyone_else_expecting_surprise_new_year_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0k7qp/anyone_else_expecting_surprise_new_year_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0k7qp/anyone_else_expecting_surprise_new_year_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T18:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0bgvl</id>
    <title>Solar-Open-100B is out</title>
    <updated>2025-12-31T12:03:49+00:00</updated>
    <author>
      <name>/u/cgs019283</name>
      <uri>https://old.reddit.com/user/cgs019283</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt; &lt;img alt="Solar-Open-100B is out" src="https://b.thumbs.redditmedia.com/6GNX_p48UOjFJdulFmOj4xKjokK7KXyHYeFvrT072Rk.jpg" title="Solar-Open-100B is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ppwj5yv32jag1.png?width=1445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11b2e722b5ec84be6ca0f1d743fa9e6122bc3fce"&gt;https://preview.redd.it/ppwj5yv32jag1.png?width=1445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11b2e722b5ec84be6ca0f1d743fa9e6122bc3fce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;upstage/Solar-Open-100B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 102B A12B Model from Upstage is out, and unlike the Solar Pro series, it has a more open license that can be used commercially as well.&lt;/p&gt; &lt;p&gt;GGUF/AWQ Wen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cgs019283"&gt; /u/cgs019283 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T12:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0i4g3</id>
    <title>Moonshot AI Completes $500 Million Series C Financing</title>
    <updated>2025-12-31T17:13:54+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI company Moonshot AI has completed a $500 million Series C financing. Founder Zhilin Yang revealed in an internal letter that the company‚Äôs global paid user base is growing at a monthly rate of 170%. Since November, driven by the K2 Thinking model, Moonshot AI‚Äôs overseas API revenue has increased fourfold. The company holds more than RMB 10 billion in cash reserves (approximately $1.4 billion). This scale is already on par with Zhipu AI and MiniMax after their IPOs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As of June 2025, Zhipu AI has RMB 2.55 billion in cash, with an IPO expected to raise about RMB 3.8 billion.&lt;/li&gt; &lt;li&gt;As of September 2025, MiniMax has RMB 7.35 billion in cash, with an IPO expected to raise RMB 3.4‚Äì3.8 billion.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the internal letter, Zhilin Yang stated that the funds from the Series C financing will be used to more aggressively expand GPU capacity, accelerate the training and R&amp;amp;D of the K3 model, and he also announced key priorities for 2026:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bring the K3 model‚Äôs pretraining performance up to par with the world‚Äôs leading models, leveraging technical improvements and further scaling to increase its equivalent FLOPs by at least an order of magnitude.&lt;/li&gt; &lt;li&gt;Make K3 a more &amp;quot;distinctive&amp;quot; model by vertically integrating training technologies and product taste, enabling users to experience entirely new capabilities that other models do not offer.&lt;/li&gt; &lt;li&gt;Achieve an order-of-magnitude increase in revenue scale, with products and commercialization focused on Agents, not targeting absolute user numbers, but pursuing the upper limits of intelligence to create greater productivity value.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0i4g3/moonshot_ai_completes_500_million_series_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0i4g3/moonshot_ai_completes_500_million_series_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0i4g3/moonshot_ai_completes_500_million_series_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T17:13:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0b0wb</id>
    <title>funny!</title>
    <updated>2025-12-31T11:37:03+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"&gt; &lt;img alt="funny!" src="https://preview.redd.it/rlgtskr40jag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d612af3aca06fbe1a315dd6e24012116c2124ac" title="funny!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rlgtskr40jag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:37:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q094a3</id>
    <title>Qwen-Image-2512</title>
    <updated>2025-12-31T09:38:19+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt; &lt;img alt="Qwen-Image-2512" src="https://preview.redd.it/2vlr11yveiag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c1e7a0b7ea834c8babae002078a848096514e1b" title="Qwen-Image-2512" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth:&lt;br /&gt; Guide: &lt;a href="https://unsloth.ai/docs/models/qwen-image-2512"&gt;https://unsloth.ai/docs/models/qwen-image-2512&lt;/a&gt;&lt;br /&gt; GGUF: &lt;a href="https://huggingface.co/unsloth/Qwen-Image-2512-GGUF"&gt;https://huggingface.co/unsloth/Qwen-Image-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-----------------&lt;/p&gt; &lt;p&gt;üëâ Try it now in Qwen Chat: &lt;a href="https://chat.qwen.ai/?inputFeature=t2i"&gt;https://chat.qwen.ai/?inputFeature=t2i&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope: &lt;a href="https://modelscope.ai/models/Qwen/Qwen-Image-2512"&gt;https://modelscope.ai/models/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª GitHub: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìù Blog: &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;https://qwen.ai/blog?id=qwen-image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/spaces/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope Demo: &lt;a href="https://modelscope.cn/aigc/imageGeneration"&gt;https://modelscope.cn/aigc/imageGeneration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ú®API: &lt;a href="https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max"&gt;https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2vlr11yveiag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T09:38:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
