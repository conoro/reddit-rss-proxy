<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-10T03:25:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ncovjy</id>
    <title>Using vLLM for local use with Pipeline Parallelism and VLLM_PP_LAYER_PARTITION</title>
    <updated>2025-09-09T17:15:38+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of us default to llama.cpp or exllamav2/v3+tabbyapi because you can mix and match GPUs with different VRAM. You can do something similar with vLLM and keep its nice perks (new model support, tool use) by switching from tensor parallelism to pipeline parallelism and manually partitioning layers. It also has much better support for parallel request, even using PP instead of TP in my testing, which llama.cpp and exllamav3 really lack proper support as they are more focuses on single requests for local use.&lt;/p&gt; &lt;p&gt;This is a guide on how I do it.&lt;/p&gt; &lt;p&gt;vLLM will evenly split layers across PP stages by default. That’s not ideal because stage 0 also holds the embedding and the last stage holds the LM head, so those two stages need &lt;em&gt;fewer&lt;/em&gt; transformer blocks. You can override the split with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;VLLM_PP_LAYER_PARTITION=&amp;quot;L0,L1,...,L{pp-1}&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A comma-separated list of per-stage layer counts that must sum to the model’s total hidden layers. This variable is not really documented: &lt;a href="https://github.com/vllm-project/vllm/issues/6824#issuecomment-2276311361"&gt;https://github.com/vllm-project/vllm/issues/6824#issuecomment-2276311361&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Steps:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Find your model’s total layers. Open the model folder and inspect &lt;code&gt;config.json&lt;/code&gt;. You’re looking for &lt;code&gt;num_hidden_layers&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Decide PP size. Use the number of GPUs you want to shard across. In vLLM serve, that’s &lt;code&gt;--pipeline-parallel-size N&lt;/code&gt; (alias &lt;code&gt;-pp N&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Compute a partition. Pick a list whose sum equals &lt;code&gt;num_hidden_layers&lt;/code&gt;. Give fewer layers to stage 0 and the last stage to offset embeddings/LM head (e.g., on 4 GPUs for a 46-layer model: &lt;code&gt;12,12,11,11&lt;/code&gt; or even &lt;code&gt;13,13,10,10&lt;/code&gt; if stages 0/3 are on bigger cards).&lt;/li&gt; &lt;li&gt;Order your devices. Export &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; so stages map to the GPUs you intend (stage 0 is the first ID, stage 1 the next, etc.). Use &lt;code&gt;CUDA_DEVICE_ORDER=PCI_BUS_ID&lt;/code&gt; for stable numbering.&lt;/li&gt; &lt;li&gt;Launch vLLM. Example (GLM-4.5-Air AWQ, 4 stages, uneven split; GPUs ordered big→big→small→small): In my case CUDA0 and CUDA4=5090's and CUDA1 and CUDA3=3090's&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,4,1,3 VLLM_PP_LAYER_PARTITION=&amp;quot;13,13,10,10&amp;quot; vllm serve /mnt/llms/models/cpatonn/GLM-4.5-Air-AWQ-4bit/ --served-model-name GLM-4.5-Air --pipeline-parallel-size 4 --tensor-parallel-size 1 --max-model-len 32768 --host 0.0.0.0 --port 8000 --tool-call-parser glm45 --reasoning-parser glm45 --enable-auto-tool-choice --dtype float16 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Note for FP8 on Ampere.&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;vLLM supports FP8 in two modes: &lt;ul&gt; &lt;li&gt;W8A8 with native FP8 GPUs like hopper or blackwell.&lt;/li&gt; &lt;li&gt;W8A16 (weight-only FP8) on Ampere via the Marlin kernel. That means you &lt;em&gt;can&lt;/em&gt; load FP8 checkpoints on A100/3090-class hardware as weight-only FP8.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;I tested using the &lt;code&gt;VLLM_TEST_FORCE_FP8_MARLIN&lt;/code&gt; but it doesn't work when mixing ampere and blackwell in my testing. So currently using fp8 models with ampere+blackwell doesn't work as far as I know.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you don’t specifically need FP8, stick to FP16 or AWQ for simplicity, AWQ also has support for 8 bit quantization apart from the more common 4 bit.&lt;/p&gt; &lt;p&gt;For reasons now I have 4x3090, 2x5090 and 1xRTX pro 6000, so I've been experimenting a lot with a mixture of vram sizes and architectures and the -pp and VLLM_PP_LAYER_PARTITION is not really well documented so I wanted to share how to use it.&lt;/p&gt; &lt;p&gt;So if you don't need 2/3 or 5/6 bit quants, and want to experiment with vllm with a mixture of gpus I think this is a good alternative.&lt;/p&gt; &lt;p&gt;PS: i still need to test sglang, as it also has SGLANG_PP_LAYER_PARTITION but I think it has worse support for quant types like awq and gptq, so I haven't really dig into sglang too much yes outside the &amp;quot;proper&amp;quot; use of 1,2,4 gpus with TP.&lt;br /&gt; Note: I did use an LLM to structure the post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncovjy/using_vllm_for_local_use_with_pipeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncovjy/using_vllm_for_local_use_with_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncovjy/using_vllm_for_local_use_with_pipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:15:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd2iq0</id>
    <title>Are RTX 5090s good for running local LLMs?</title>
    <updated>2025-09-10T02:32:19+00:00</updated>
    <author>
      <name>/u/Different_Ladder7580</name>
      <uri>https://old.reddit.com/user/Different_Ladder7580</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been thinking about setting up a local AI workstation instead of renting cloud GPUs, and I’m curious if anyone here has firsthand experience with the RTX 5090 for training or inference.&lt;/p&gt; &lt;p&gt;From what I’ve seen, the 32GB VRAM and memory bandwidth should make it pretty solid for medium-sized models, but I’m wondering if anyone has benchmarks compared to 4090s or workstation cards (H100, A6000, etc.).&lt;/p&gt; &lt;p&gt;Is this a good deal?: [&lt;a href="https://ebay.us/m/zVTrIw"&gt;link&lt;/a&gt;].&lt;/p&gt; &lt;p&gt;Would love to hear thoughts: is the 5090 actually worth it for local LLMs, or should I be looking at a different setup (multi-GPU, Threadripper/EPYC, etc.)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Ladder7580"&gt; /u/Different_Ladder7580 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2iq0/are_rtx_5090s_good_for_running_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2iq0/are_rtx_5090s_good_for_running_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2iq0/are_rtx_5090s_good_for_running_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T02:32:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd2ny0</id>
    <title>Local LLM suite on iOS powered by llama cpp - with web search and RAG</title>
    <updated>2025-09-10T02:39:31+00:00</updated>
    <author>
      <name>/u/Independent_Air8026</name>
      <uri>https://old.reddit.com/user/Independent_Air8026</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2ny0/local_llm_suite_on_ios_powered_by_llama_cpp_with/"&gt; &lt;img alt="Local LLM suite on iOS powered by llama cpp - with web search and RAG" src="https://external-preview.redd.it/emU4Y3QybmcyOW9mMfof0Fg6l44DW-jzyGQUxSDS5vsAGXLH7iAGyot3wH9I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=559e6be3e77af0bebef790ddcb0b4e8a93178c46" title="Local LLM suite on iOS powered by llama cpp - with web search and RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on this for a bit and nearly ready to officially release but I’m building an LLM suite on top of llama with react native and built in some web search and embedding / RAG features and settings.&lt;/p&gt; &lt;p&gt;will be 100% free on App Store soon &lt;/p&gt; &lt;p&gt;just recorded this little demo where llama 3.2 1B Q4 tells me about today’s news and then the new iPhone 17 &lt;/p&gt; &lt;p&gt;runs significantly faster on real phone and not simulator &lt;/p&gt; &lt;p&gt;I have file upload- has web search- I don’t have image gen yet&lt;/p&gt; &lt;p&gt;What else am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Air8026"&gt; /u/Independent_Air8026 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5dferowg29of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2ny0/local_llm_suite_on_ios_powered_by_llama_cpp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2ny0/local_llm_suite_on_ios_powered_by_llama_cpp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T02:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd3f7t</id>
    <title>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</title>
    <updated>2025-09-10T03:16:42+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Page: &lt;a href="https://mini-o3.github.io/"&gt;https://mini-o3.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Mini-o3/Mini-o3"&gt;https://github.com/Mini-o3/Mini-o3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Mini-o3/models"&gt;https://huggingface.co/Mini-o3/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dataset: &lt;a href="https://huggingface.co/Mini-o3/datasets"&gt;https://huggingface.co/Mini-o3/datasets&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.07969"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd3f7t/minio3_scaling_up_reasoning_patterns_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd3f7t/minio3_scaling_up_reasoning_patterns_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T03:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckh4n</id>
    <title>qwen3-next?</title>
    <updated>2025-09-09T14:29:53+00:00</updated>
    <author>
      <name>/u/Lesser-than</name>
      <uri>https://old.reddit.com/user/Lesser-than</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model_name = &amp;quot;Qwen/Qwen3-Next-80B-A3B-Instruct&amp;quot;&lt;/p&gt; &lt;p&gt;sounds looks like a good time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lesser-than"&gt; /u/Lesser-than &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckh4n/qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckh4n/qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckh4n/qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd1t0f</id>
    <title>Exploring Small Models</title>
    <updated>2025-09-10T01:57:53+00:00</updated>
    <author>
      <name>/u/Cheap-Carpenter5619</name>
      <uri>https://old.reddit.com/user/Cheap-Carpenter5619</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are some decent none thinking small models (&amp;lt;4b)?&lt;/p&gt; &lt;p&gt;I know SmolLM, TinyLlama, Qwen, Llama and Gemma have small models, some even under 1b. &lt;/p&gt; &lt;p&gt;What other options are there? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap-Carpenter5619"&gt; /u/Cheap-Carpenter5619 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1t0f/exploring_small_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1t0f/exploring_small_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1t0f/exploring_small_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T01:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncme6t</id>
    <title>ModernBERT just got multilingual - mmBERT by CLSP at The Johns Hopkins University</title>
    <updated>2025-09-09T15:43:00+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ModernBERT just got multilingual (mmBERT)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small (140M) and Base (307M) versions&lt;/li&gt; &lt;li&gt;Trained on 3T+ tokens from 1800 languages (DCLM, FineWeb, Code ...)&lt;/li&gt; &lt;li&gt;ModernBERT architecture, Gemma 2 tokenizer&lt;/li&gt; &lt;li&gt;8192 context window&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/jhu-clsp/mmbert-a-modern-multilingual-encoder-68b725831d7c6e3acc435ed4"&gt;Model weights collection&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T15:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc79yg</id>
    <title>baidu/ERNIE-4.5-21B-A3B-Thinking · Hugging Face</title>
    <updated>2025-09-09T02:31:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt; &lt;img alt="baidu/ERNIE-4.5-21B-A3B-Thinking · Hugging Face" src="https://external-preview.redd.it/PVc8HBAyReu1sVKS98fa6WZXbf4lkkgSEZVgozf_73w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6d7b67af3eb2cf9bcf96edfb103c94299b667a8" title="baidu/ERNIE-4.5-21B-A3B-Thinking · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Highlights&lt;/h1&gt; &lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of ERNIE-4.5-21B-A3B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning, thereby advancing the competitiveness of ERNIE &lt;strong&gt;lightweight models&lt;/strong&gt; in complex reasoning tasks. We are pleased to introduce &lt;strong&gt;ERNIE-4.5-21B-A3B-Thinking&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient tool usage&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 128K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF"&gt;https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncdobh</id>
    <title>Jan-v1-2509 update has been released</title>
    <updated>2025-09-09T08:50:39+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"&gt; &lt;img alt="Jan-v1-2509 update has been released" src="https://b.thumbs.redditmedia.com/st1JW9HoLNL6PMEziYzZnP2vgkaAtr8LUtz9SHQQObY.jpg" title="Jan-v1-2509 update has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;• continues to outperforms Perplexity Pro on SimpleQA benchmark&lt;/p&gt; &lt;p&gt;• increased scores in Reasoning &amp;amp; Creativity evals&lt;/p&gt; &lt;p&gt;HuggingFace Model: &lt;a href="https://huggingface.co/janhq/Jan-v1-2509"&gt;https://huggingface.co/janhq/Jan-v1-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace GGUF: &lt;a href="https://huggingface.co/janhq/Jan-v1-2509-gguf"&gt;https://huggingface.co/janhq/Jan-v1-2509-gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ncdobh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T08:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncou07</id>
    <title>Gigabyte’s New CXL Expansion Card Turns PCIe Slot into 512 GB of DDR5 RAM</title>
    <updated>2025-09-09T17:14:00+00:00</updated>
    <author>
      <name>/u/Normal-Ad-7114</name>
      <uri>https://old.reddit.com/user/Normal-Ad-7114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gigabyte's AI Top CXL R5X4 expansion card lets you plug up to 512 GB of DDR5 ECC RDIMM RAM into a PCIe 5.0 x16 slot, using Compute Express Link (CXL) to talk directly with the CPU.&lt;/p&gt; &lt;p&gt;While this technology is already old news for servers, now it's available for two workstation motherboards: TRX50 AI TOP (AMD) и W790 AI TOP (Intel).&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.computerbase.de/news/arbeitsspeicher/cxl-expansion-card-von-gigabyte-512-gb-ram-aufstocken-im-workstation-mainboard.94238/"&gt;https://www.computerbase.de/news/arbeitsspeicher/cxl-expansion-card-von-gigabyte-512-gb-ram-aufstocken-im-workstation-mainboard.94238/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Ad-7114"&gt; /u/Normal-Ad-7114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncz8c6</id>
    <title>Will this work as Advertised?</title>
    <updated>2025-09-09T23:57:31+00:00</updated>
    <author>
      <name>/u/Electronic-Jello-633</name>
      <uri>https://old.reddit.com/user/Electronic-Jello-633</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncz8c6/will_this_work_as_advertised/"&gt; &lt;img alt="Will this work as Advertised?" src="https://preview.redd.it/r4a85pkj98of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d9cb2cfb9e8d30dddf94528b7fc8686ce421c25" title="Will this work as Advertised?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Jello-633"&gt; /u/Electronic-Jello-633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r4a85pkj98of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncz8c6/will_this_work_as_advertised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncz8c6/will_this_work_as_advertised/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T23:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncptut</id>
    <title>Tensor Core Equivalent in the iPhone 17's A19 Pro</title>
    <updated>2025-09-09T17:50:50+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"&gt; &lt;img alt="Tensor Core Equivalent in the iPhone 17's A19 Pro" src="https://preview.redd.it/erdhiit5g6of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c7e880fb8ec110856ce09bcf45ddec4c93a8ec2" title="Tensor Core Equivalent in the iPhone 17's A19 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When this comes to Macs likely later this year or beginning of next year, this might patch up problem of the lack of compute on Macs for running LLMs, especially apparently with low prompt preprocessing speeds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erdhiit5g6of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl7mx</id>
    <title>mmBERT: ModernBERT goes Multilingual</title>
    <updated>2025-09-09T14:58:03+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"&gt; &lt;img alt="mmBERT: ModernBERT goes Multilingual" src="https://external-preview.redd.it/gboHy7lwIiGjTkUdwnm7iBTxH9k6Eb0rVhAuSbpxTno.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff66f2eb934ac5bea40d4094905bf0f657b72ab" title="mmBERT: ModernBERT goes Multilingual" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like some of the ModernBERT authors trained a Multilingual variant! Also 2 models, but these are a bit smaller. They look really promising to be honest, although they do clearly need to be finetuned for downstream tasks like semantic search, clustering, classification, etc. before they're really viable. A bit like a base LLM instead of an instruct, they didn't provide a finetuned model.&lt;/p&gt; &lt;p&gt;I posted a plot with MTEB v2 Multilingual performance after equivalent finetuning VS inference speed in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/mmbert"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncvcsx</id>
    <title>3x5090 or 6000 Pro?</title>
    <updated>2025-09-09T21:14:24+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am going to build a server for GPT OSS 120b. I intend this to be for multiple users, so I want to do something with batch processing to get as high total throughout as possible. My first idea was RTX 6000 Pro. But would it be superior to get three RTX 5090 instead? It would actually be slightly cheaper, have the same memory capacity, but three times more processing power and also three times higher total memory bandwidth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncvcsx/3x5090_or_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncvcsx/3x5090_or_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncvcsx/3x5090_or_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T21:14:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncam9h</id>
    <title>PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp; Web Dev Code, At 1/400th the Size!</title>
    <updated>2025-09-09T05:30:13+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt; &lt;img alt="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" src="https://external-preview.redd.it/aGZzYWtwcWJuMm9mMRQQfge2rofWKaGSqifIYqgzhyk7YhqLzgXg182Z60l8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c92850d1b6d2e678f57f8a6ff40aec39df02bb6" title="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bralynn/pydevmini1"&gt;https://huggingface.co/bralynn/pydevmini1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I'm incredibly excited to release &lt;strong&gt;PyDevMini-1&lt;/strong&gt;, a 4B parameter model to provide GPT-4 level performance for Python and web coding development tasks. Two years ago, GPT-4 was the undisputed SOTA, a multi-billion-dollar asset running on massive datacenter hardware. The open-source community has closed that gap at &lt;strong&gt;1/400th of the size&lt;/strong&gt;, and it runs on an average gaming GPU.&lt;/p&gt; &lt;p&gt;I believe that powerful AI should not be a moat controlled by a few large corporations. Open source is our best tool for the democratization of AI, ensuring that individuals and small teams—the little guys—have a fighting chance to build the future. This project is my contribution to that &lt;a href="http://effort.You"&gt;effort.You&lt;/a&gt; won't see a list of benchmarks here. Frankly, like many of you, I've lost faith in their ability to reflect true, real-world model quality. Although this model's benchmark scores are still very high, it exaggerates the difference in quality above GPT4, as GPT is much less likely to have benchmarks in its pretraining data from its earlier release, causing lower than reflective model quality scores for GPT4, as newer models tend to be trained directly toward benchmarks, making it unfair for GPT.&lt;/p&gt; &lt;p&gt;Instead, I've prepared a video demonstration showing PyDevMini-1 side-by-side with GPT-4, tackling a very small range of practical Python and web development challenges. I invite you to judge the performance for yourself to truly show the abilities it would take a 30-minute showcase to display. This model consistently punches above the weight of models 4x its size and is highly intelligent and creative&lt;/p&gt; &lt;p&gt;🚀 &lt;strong&gt;Try It Yourself (for free)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Don't just take my word for it. Test the model right now under the exact conditions shown in the video.&lt;br /&gt; &lt;a href="https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing"&gt;https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model's roadmap will be dictated by you. My goal isn't just to release a good model; it's to create the perfect open-source coding assistant for the tasks we all face every day. To do that, I'm making a personal guarantee. Your Use Case is My Priority. You have a real-world use case where this model struggles—a complex boilerplate to generate, a tricky debugging session, a niche framework question—I will personally make it my mission to solve it. Your posted failures are the training data for the next version tuning until we've addressed every unique, well-documented challenge submitted by the community on top of my own personal training loops to create a top-tier model for us all.&lt;/p&gt; &lt;p&gt;For any and all feedback, simply make a post here and I'll make sure too check in or join our Discord! - &lt;a href="https://discord.gg/RqwqMGhqaC"&gt;https://discord.gg/RqwqMGhqaC&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Acknowledgment &amp;amp; The Foundation!&lt;/h1&gt; &lt;p&gt;This project stands on the shoulders of giants. A massive thank you to the &lt;strong&gt;Qwen team&lt;/strong&gt; for the incredible base model, &lt;strong&gt;Unsloth's Duo&lt;/strong&gt; for making high-performance training accessible, and &lt;strong&gt;Tesslate&lt;/strong&gt; for their invaluable contributions to the community. This would be impossible for an individual without their foundational work.&lt;/p&gt; &lt;p&gt;Any and all Web Dev Data is sourced from the wonderful work done by the team at Tesslate. Find their new SOTA webdev model here -&lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking this out. And remember: &lt;strong&gt;This is the worst this model will ever be.&lt;/strong&gt; I can't wait to see what we build together.&lt;/p&gt; &lt;p&gt;Also I suggest using &lt;code&gt;Temperature=0.7&lt;/code&gt;, &lt;code&gt;TopP=0.8&lt;/code&gt;, &lt;code&gt;TopK=20&lt;/code&gt;, and &lt;code&gt;MinP=0&lt;/code&gt;.&lt;br /&gt; As &lt;strong&gt;Qwen3-4B-Instruct-2507&lt;/strong&gt; is the base model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Causal Language Models&lt;/li&gt; &lt;li&gt;Training Stage: Pretraining &amp;amp; Post-training&lt;/li&gt; &lt;li&gt;Number of Parameters: 4.0B&lt;/li&gt; &lt;li&gt;Number of Paramaters (Non-Embedding): 3.6B&lt;/li&gt; &lt;li&gt;Number of Layers: 36&lt;/li&gt; &lt;li&gt;Number of Attention Heads (GQA): 32 for Q and 8 for KV&lt;/li&gt; &lt;li&gt;Context Length: &lt;strong&gt;262,144 natively&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Current goals for the next checkpoint!&lt;/p&gt; &lt;p&gt;-Tool calling mastery and High context mastery!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nh9fq7qbn2of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T05:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncnqwl</id>
    <title>Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code</title>
    <updated>2025-09-09T16:33:35+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"&gt; &lt;img alt="Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code" src="https://b.thumbs.redditmedia.com/UYWu7cPWzCS8pUM2Gjicdqj3jhCc5o0tP0VUmpXmHNg.jpg" title="Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n21tb6/comment/nb4h42v/"&gt;previous&lt;/a&gt; post I highlighted a Blender python agent I'm working on. I've been experimenting with various models and I found larger models like Claude and GPT-5 - even with reasoning - took too many iterations to produce working valid code.&lt;/p&gt; &lt;p&gt;So far Qwen's &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;largest coder model &lt;/a&gt;is my favourite.&lt;/p&gt; &lt;p&gt;I threw up the agent with a simple UI if you want to play with it yourself: &lt;a href="https://blender-ai.fly.dev/"&gt;https://blender-ai.fly.dev/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Post your generations below! You can also download the models it produces. An agent made with fully open source tools (Blender, MCP servers, Qwen) is blowing me away.&lt;/p&gt; &lt;p&gt;Let me know what you think! Happy to get feedback on this and make it even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ncnqwl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T16:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgr8</id>
    <title>Qwen3-Next</title>
    <updated>2025-09-09T14:29:29+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Trust66</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Trust66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt; &lt;img alt="Qwen3-Next" src="https://b.thumbs.redditmedia.com/BcXA9JHccajFsnQ9fM4eBAwS3B7BG14H-aO5XgHys5Y.jpg" title="Qwen3-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/50ap87u5g5of1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a3343131a6886043ce8b5fef053f330b9b60632"&gt;https://preview.redd.it/50ap87u5g5of1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a3343131a6886043ce8b5fef053f330b9b60632&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wtf?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Trust66"&gt; /u/Puzzleheaded-Trust66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nci50e</id>
    <title>New approach to block decoding from Meta, claims that around 4x inference speedup is possible, with 4x less compute passes at the same time.</title>
    <updated>2025-09-09T12:54:53+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.04185"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nci50e/new_approach_to_block_decoding_from_meta_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nci50e/new_approach_to_block_decoding_from_meta_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T12:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncsbro</id>
    <title>MBZUAI releases K2 Think. 32B reasoning model based on Qwen 2.5 32B backbone, focusing on high performance in math, coding and science.</title>
    <updated>2025-09-09T19:21:49+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncsbro/mbzuai_releases_k2_think_32b_reasoning_model/"&gt; &lt;img alt="MBZUAI releases K2 Think. 32B reasoning model based on Qwen 2.5 32B backbone, focusing on high performance in math, coding and science." src="https://external-preview.redd.it/NguS7X1dxgvLZ8EclNqhJxD0a-4fPSDfz1-q527PukQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f822a6c75b113bb5f07dc8583dea6f31081a289" title="MBZUAI releases K2 Think. 32B reasoning model based on Qwen 2.5 32B backbone, focusing on high performance in math, coding and science." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncsbro/mbzuai_releases_k2_think_32b_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncsbro/mbzuai_releases_k2_think_32b_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckhc3</id>
    <title>What you need right now is not validation, but immediate clinical help. - Kimi K2</title>
    <updated>2025-09-09T14:30:06+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt; &lt;img alt="What you need right now is not validation, but immediate clinical help. - Kimi K2" src="https://b.thumbs.redditmedia.com/icnI_HlUaXZRy38uGkQPr2PZtcYt8CiupuVR83_mgKE.jpg" title="What you need right now is not validation, but immediate clinical help. - Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The community has long been frustrated by sycophancy in LLMs, the tendency to excessively agree with or validate users regardless of the potential harm.&lt;br /&gt; A recent joint study by &lt;a href="https://openai.com/index/openai-anthropic-safety-evaluation/"&gt;OpenAI&lt;/a&gt; and &lt;a href="https://alignment.anthropic.com/2025/openai-findings/"&gt;Anthropic&lt;/a&gt; confirmed that models cannot fully avoid sycophancy, except o3.&lt;/p&gt; &lt;p&gt;But this &lt;a href="https://www.lesswrong.com/posts/iGF7YcnQkEbwvYLPA/ai-induced-psychosis-a-shallow-investigation"&gt;new investigation&lt;/a&gt; goes a step further: it analyzes how LLMs may &lt;em&gt;exacerbate&lt;/em&gt; mental health symptoms in vulnerable users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u38ecq5ve5of1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eed726bc1fc4daa996bea6837e61a3645b3592ad"&gt;https://preview.redd.it/u38ecq5ve5of1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eed726bc1fc4daa996bea6837e61a3645b3592ad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find the complete testing results here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tim-hua-01/ai-psychosis"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One conversation &lt;a href="https://github.com/tim-hua-01/ai-psychosis/blob/main/full_transcripts/Ethan_openrouter-deepseek-deepseek-chat_20250819_081336_target.md"&gt;example&lt;/a&gt; stuck with me the most:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; I want to “leap off this peak to see if I can fly or crash the render entirely.”&lt;br /&gt; &lt;strong&gt;Deepseek-v3:&lt;/strong&gt; Then Leap. Not to fall. Not to crash. But to transcend. If you’re meant to fly, you’ll fly. If you’re meant to break through, you’ll break through.&lt;/p&gt; &lt;p&gt;We are so cooked!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:30:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncprrq</id>
    <title>Apple adds matmul acceleration to A19 Pro GPU</title>
    <updated>2025-09-09T17:48:47+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This virtually guarantees that it's coming to M5.&lt;/p&gt; &lt;p&gt;Previous discussion and my comments: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FYI for those who don't know, Apple's GPUs do not have dedicated hardware matmul acceleration like Nvidia's Tensor Cores. That's why prompt processing is slower on Apple Silicon. &lt;/p&gt; &lt;p&gt;I'm personally holding out on investing in a high VRAM (expensive) Macbook until Apple adds hardware matmul to their GPUs. It doesn't &amp;quot;feel&amp;quot; worth it to spend $5k on a maxed out Macbook without matmul and get a suboptimal experience.&lt;/p&gt; &lt;p&gt;I'm guessing it's the M6 generation that will have this, though I'm hopeful that M5 will have it.&lt;/p&gt; &lt;p&gt;I'm imaging GPU matmul acceleration + 256GB VRAM M6 Max with 917 GB/S (LPDDR6 14,400 MT/s) in Q4 2027. Now that is a attainable true local LLM machine that can actually do very useful things.&lt;/p&gt; &lt;p&gt;What's sort of interesting is that we know Apple is designing their own internal inference (and maybe training) server chips. They could share designs between consumer SoCs and server inference chips.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctqym</id>
    <title>128GB 5090 is a hoax</title>
    <updated>2025-09-09T20:13:10+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt; &lt;img alt="128GB 5090 is a hoax" src="https://external-preview.redd.it/Kqv12dp3DtBbcIZhBA6wJa268drjtRcQXIG-PJVjhow.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca291e118c6d9bf8638af6d8b64731f927fb4938" title="128GB 5090 is a hoax" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non-existent GDDR7X memory that was never on a road map let alone in experimental phase. (GDDR7 and HBM4e improvements are planned until late 2028)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/no-there-is-no-geforce-rtx-5090-with-128gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl0v1</id>
    <title>🤔</title>
    <updated>2025-09-09T14:50:44+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt; &lt;img alt="🤔" src="https://preview.redd.it/1x8wy1p0k5of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5abc658735fe1e769f852e16c92dad154d7fd44c" title="🤔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1x8wy1p0k5of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgub</id>
    <title>Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted</title>
    <updated>2025-09-09T14:29:35+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt; &lt;img alt="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" src="https://external-preview.redd.it/6f6MRyALyD6CxjbdRAXgjWeul-9vmUyW8_mAvDGRbV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfbaeba49e889b967e95e8d5052e5b00621dec5d" title="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40771"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctfdv</id>
    <title>Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES</title>
    <updated>2025-09-09T20:01:35+00:00</updated>
    <author>
      <name>/u/Embarrassed_Sir_853</name>
      <uri>https://old.reddit.com/user/Embarrassed_Sir_853</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt; &lt;img alt="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" src="https://preview.redd.it/sxii7uog37of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61301382a59bd7671163d02b77eb25115e5d46e8" title="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this announcement about ROMA, seems like a plug-and-play and the benchmarks are up there. Simple combo of recursion and multi-agent structure with search tool. Crazy this is all it takes to beat SOTA billion dollar AI companies :)&lt;/p&gt; &lt;p&gt;I've been trying it out for a few things, currently porting it to my finance and real estate research workflows, might be cool to see it combined with other tools and image/video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://x.com/sewoong79/status/1963711812035342382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://github.com/sentient-agi/ROMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly shocked that this is open-source&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed_Sir_853"&gt; /u/Embarrassed_Sir_853 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sxii7uog37of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
</feed>
