<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-24T02:43:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p4wvzj</id>
    <title>Can GLM-4.5-air run on a single 3090 (24gb vram) with 48gb ram at above 10t/s?</title>
    <updated>2025-11-23T20:22:02+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can‚Äôt find a straight answer! I‚Äôve checked the vram calculator and it says that a Q1 can fit into 21GB vram? So I‚Äôm not sure? Anyone know if a Q4 is possible with this setup? Etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wvzj/can_glm45air_run_on_a_single_3090_24gb_vram_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wvzj/can_glm45air_run_on_a_single_3090_24gb_vram_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wvzj/can_glm45air_run_on_a_single_3090_24gb_vram_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:22:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p52a67</id>
    <title>Crush AI Inference Power by 1/3 on Your Local Rig ‚Äì SlimeTree's Non-Commutative Rings for Efficient Graphs [P] (Patent Pending Teaser)</title>
    <updated>2025-11-24T00:09:03+00:00</updated>
    <author>
      <name>/u/Alarmed_Ad4718</name>
      <uri>https://old.reddit.com/user/Alarmed_Ad4718</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, first post here after lurking for ages ‚Äì super excited to share something I've been tinkering with for local AI setups. Running big graphs on your home GPU? Cycles killing your inference speed and jacking up power draw? (Yeah, that 90% waste on recursion loops sucks.) Enter SlimeTree: my open-source-ish framework blending operator algebra, philosophy, and physics to &amp;quot;crystallize&amp;quot; those messy knowledge graphs into lean, mean machines. Patent pending (2025-183827), but the core math is dropping as a teaser soon ‚Äì think 7x throughput without melting your rig.&lt;/p&gt; &lt;h1&gt;Why Local Folks Will Dig This&lt;/h1&gt; &lt;p&gt;Local LLMs thrive on efficiency, right? SlimeTree uses the commutator &lt;strong&gt;[a,b] = ab - ba ‚â† 0&lt;/strong&gt; to model non-commutative rings, smashing cyclic dependencies with Union-Find compression. No more endless loops in your RAG pipelines or edge models ‚Äì just finite &amp;quot;time crystals of meaning&amp;quot; that run buttery smooth on consumer hardware.&lt;/p&gt; &lt;h1&gt;Quick Code Snippet (Python + SymPy for the Math Nerds)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from sympy import symbols, Matrix a, b = symbols('a b') commutator = a*b - b*a # [a,b] ‚â† 0 ‚Äì non-commutativity FTW # Toy graph matrix (scale to 1M nodes for real fun) A = Matrix([[1, 0], [0, 1]]) # Identity B = Matrix([[0, 1], [1, 0]]) # Swap C = A * B - B * A # Commutator computes the &amp;quot;spark&amp;quot; def compress_cycle(graph_nodes): parent = {node: node for node in graph_nodes} # Loop detection + Union-Find squash... return len(set(parent.values())) / len(graph_nodes) # Compression ratio: ~7x speedup # Bench: 100TB sim data ‚Üí 14h/300W to 2h/100W (1/3 power on your local setup!) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Pair it with Semantic Area Sampling (SAS) for 12x data crunching ‚Äì perfect for privacy-focused local runs (GDPR baked in via MetaGene Slots). Tested on FHIR medical graphs; now optimizing for broadcast HLS and IoT bots too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://example.com/slime-tree-efficiency.png"&gt;Before/After Power Chart&lt;/a&gt;&lt;br /&gt; &lt;em&gt;(Legacy: Power hog. SlimeTree: Efficient AF. Try the sim at&lt;/em&gt; &lt;a href="http://slimetree.ai"&gt;&lt;em&gt;slimetree.ai&lt;/em&gt;&lt;/a&gt; &lt;em&gt;‚Äì runs local!)&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Local AI Wins&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Power Slash&lt;/strong&gt;: From 250W inference to ~83W ‚Äì your laptop lasts 3x longer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph Taming&lt;/strong&gt;: ms ‚Üí Œºs on cycles; ideal for RAG/agents without cloud crutches.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ethical Edge&lt;/strong&gt;: Forget post-training hacks; ethics embedded at the silicon level.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Teaser repo incoming ‚Äì wanna collab on integrating with Ollama or Llama.cpp? How do you handle graph cycles in your local stacks? Overkill math or game-changer? Spill your thoughts! üòé&lt;/p&gt; &lt;p&gt;Url: &lt;a href="https://slimetree.ai"&gt;slimetree.ai&lt;/a&gt;&lt;br /&gt; Hit me up for the alpha code. #SlimeTree #NonCommutativeRings #LocalAI #AIEfficiency&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Mod heads-up: [P] for project ‚Äì math/code heavy, no hard sell. LMK if it vibes wrong!)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarmed_Ad4718"&gt; /u/Alarmed_Ad4718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p52a67/crush_ai_inference_power_by_13_on_your_local_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p52a67/crush_ai_inference_power_by_13_on_your_local_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p52a67/crush_ai_inference_power_by_13_on_your_local_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T00:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p52fb0</id>
    <title>RAG follow-ups not working ‚Äî Qwen2.5 ignores previous context and gives unrelated answers</title>
    <updated>2025-11-24T00:15:31+00:00</updated>
    <author>
      <name>/u/NoBlackberry3264</name>
      <uri>https://old.reddit.com/user/NoBlackberry3264</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a &lt;strong&gt;RAG-based chat system&lt;/strong&gt; using FastAPI + &lt;strong&gt;Qwen/Qwen2.5-7B-Instruct&lt;/strong&gt;, and I‚Äôm running into an issue with follow-up queries.&lt;/p&gt; &lt;p&gt;The first query works fine, retrieving relevant documents from my knowledge base. But when the user asks a follow-up question, the model completely ignores previous context and fetches unrelated information. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;User:&lt;/strong&gt; ‚Äúgold loan‚Äù ‚Üí retrieves correct documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User:&lt;/strong&gt; ‚Äúhow to create account?‚Äù ‚Üí model ignores previous context, fetches unrelated info.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Example Payload (Client Request)&lt;/h1&gt; &lt;p&gt;Here‚Äôs the structure of the payload my client sends:&lt;br /&gt; {&lt;/p&gt; &lt;p&gt;&amp;quot;system_persona&amp;quot;: &amp;quot;KB&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;system_prompt&amp;quot;: { ... }, &lt;/p&gt; &lt;p&gt;&amp;quot;context&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;chat_history&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;nabil bank ko baryama bhana?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;Any advice or real examples for &lt;strong&gt;handling follow-ups in RAG with Qwen2.5&lt;/strong&gt; would be super helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoBlackberry3264"&gt; /u/NoBlackberry3264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p52fb0/rag_followups_not_working_qwen25_ignores_previous/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p52fb0/rag_followups_not_working_qwen25_ignores_previous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p52fb0/rag_followups_not_working_qwen25_ignores_previous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T00:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4k9is</id>
    <title>LLMSnap - fast model swapping for vLLM using sleep mode</title>
    <updated>2025-11-23T11:15:48+00:00</updated>
    <author>
      <name>/u/Camvizioneer</name>
      <uri>https://old.reddit.com/user/Camvizioneer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I saw the release of vLLM sleep mode providing second-ish swap times, I was very intrigued - it was exactly what I needed. Previous non-sleep vLLM model swapping was unusable for frequent model swaps, with startup times around 1 minute each.&lt;/p&gt; &lt;p&gt;I started looking for an existing lightweight model router with vLLM sleep mode support but couldn't find any. I found what seemed like a perfect project to add this functionality - llama-swap. I implemented vLLM sleep support and opened a PR, but it was closed with the reasoning that most llama-swap users use llama.cpp and don't need this feature. That's how &lt;a href="https://github.com/napmany/llmsnap"&gt;llmsnap&lt;/a&gt;, a fork of llama-swap, was born! :)&lt;/p&gt; &lt;p&gt;I'm going to continue working on llmsnap with a focus on making LLM model swapping faster and more resource-effective, without limiting or tight coupling to any one inference server - even though only vLLM took its spot in the title for now :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/napmany/llmsnap"&gt;https://github.com/napmany/llmsnap&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can install and use it with brew, docker, release binaries, or from source.&lt;/p&gt; &lt;p&gt;Questions and feedback are very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Camvizioneer"&gt; /u/Camvizioneer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T11:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p533ne</id>
    <title>New build, CPU question: would there be a meaningful difference in local inference / hosting between a Ryzen 7 9800x3d and a Ryzen 9 9950x3d?</title>
    <updated>2025-11-24T00:46:38+00:00</updated>
    <author>
      <name>/u/Sad_Yam6242</name>
      <uri>https://old.reddit.com/user/Sad_Yam6242</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX 5090&lt;/p&gt; &lt;p&gt;Lots of ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Yam6242"&gt; /u/Sad_Yam6242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p533ne/new_build_cpu_question_would_there_be_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p533ne/new_build_cpu_question_would_there_be_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p533ne/new_build_cpu_question_would_there_be_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T00:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4wm42</id>
    <title>Exploring non-standard LLM architectures - is modularity worth pursuing on small GPUs?</title>
    <updated>2025-11-23T20:11:04+00:00</updated>
    <author>
      <name>/u/lukatu10</name>
      <uri>https://old.reddit.com/user/lukatu10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I‚Äôm working on some experimental LLM ideas that go &lt;strong&gt;beyond the usual ‚Äútrain one big model‚Äù approach&lt;/strong&gt;.&lt;br /&gt; Without going into specific techniques, the general direction is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;not a normal monolithic LLM&lt;/li&gt; &lt;li&gt;not just fine-tuning existing checkpoints&lt;/li&gt; &lt;li&gt;more of a modular / multi-component system&lt;/li&gt; &lt;li&gt;where different parts handle different functions&lt;/li&gt; &lt;li&gt;and the overall structure is &lt;em&gt;not&lt;/em&gt; something conventional LLMs typically use&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All experiments are done on a small consumer GPU (a 3060), so efficiency matters a lot.&lt;/p&gt; &lt;p&gt;My question for people who have built unconventional or custom LLM setups:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Is it actually realistic to get better task-specific performance from a modular system (multiple small cooperating components) than from one larger dense model of the same total size?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Not asking for theory - more for practical experience:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Did modularity help?&lt;/li&gt; &lt;li&gt;Any major pitfalls?&lt;/li&gt; &lt;li&gt;Any scaling limits on consumer hardware?&lt;/li&gt; &lt;li&gt;Any ‚ÄúI tried something similar, here‚Äôs what I learned‚Äù?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm trying to see if this direction is worth pushing further,&lt;br /&gt; or if modular setups rarely outperform dense models in practice.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lukatu10"&gt; /u/lukatu10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wm42/exploring_nonstandard_llm_architectures_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wm42/exploring_nonstandard_llm_architectures_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wm42/exploring_nonstandard_llm_architectures_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4xe0a</id>
    <title>Estimating the Size of Gemini-3, GPT-5.1, and Magistral Medium Using Open LLMs on the Omniscience Bench (ROUGH!)</title>
    <updated>2025-11-23T20:42:27+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Artificialanalysis discovered that the &amp;quot;AA-Omniscience Accuracy&amp;quot; value strongly correlates with model size. Therefore, I used the open LLMs captured by the benchmark, whose parameter counts are known, to establish a relationship between the accuracy value and the number of parameters for each model. Out of pure curiosity, I wanted to see if this relationship could be used to roughly estimate the parameter counts of Gemini-3, GPT-5.1 (think), and Magistral Medium 1.2.&lt;/p&gt; &lt;p&gt;Tests showed that the accuracy values of the 13 open reasoning models can be very well modeled using a power regression:&lt;/p&gt; &lt;p&gt;x: Number of parameters&lt;/p&gt; &lt;p&gt;f(x): Omniscience Bench accuracy value&lt;/p&gt; &lt;p&gt;f(x) = a * x^b&lt;/p&gt; &lt;p&gt;a = 7.73862&lt;/p&gt; &lt;p&gt;b = 0.192839&lt;/p&gt; &lt;p&gt;r¬≤ = 0.954166&lt;/p&gt; &lt;p&gt;The r¬≤ value is very close to 1, meaning the function describes the relationship relatively well.&lt;/p&gt; &lt;p&gt;Gemini-3 achieves an accuracy value of 53. The idea is to estimate the number of parameters by solving the equation f(x) = 53. The assumption here is that the power function derived from the open models also applies to commercial models.&lt;/p&gt; &lt;p&gt;However, this requires extending the power function well beyond the range of accuracy values obtained from open models, which increases inaccuracies. Therefore, I had Kimi-K2-Thinking write a program to calculate the confidence intervals in which the actual model size lies with 90% probability.&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Estimated Parameters&lt;/th&gt; &lt;th align="left"&gt;90% Confidence Interval&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GEMINI-3&lt;/td&gt; &lt;td align="left"&gt;21,538.35 billion&lt;/td&gt; &lt;td align="left"&gt;8,380 to 55,358 billion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.1&lt;/td&gt; &lt;td align="left"&gt;2,504 billion&lt;/td&gt; &lt;td align="left"&gt;1,130 to 5,553 billion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Medium&lt;/td&gt; &lt;td align="left"&gt;138 billion&lt;/td&gt; &lt;td align="left"&gt;68 to 278 billion&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The confidence intervals show that only a rough estimate is possible.&lt;/p&gt; &lt;p&gt;Mistral AI introduced Mistral Medium with the slogan &amp;quot;Medium is the new large.&amp;quot; Combined with the above estimate, it seems to confirm that Medium has 123 billion parameters, similar to the previous Mistral Large 2.&lt;/p&gt; &lt;p&gt;The estimate for GPT-5.1 seems realistic to me. But is Gemini-3 really that enormous?&lt;/p&gt; &lt;p&gt;(Text translated via Le Chat)&lt;/p&gt; &lt;p&gt;EDIT: Source &lt;a href="https://artificialanalysis.ai/evaluations/omniscience"&gt;https://artificialanalysis.ai/evaluations/omniscience&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xe0a/estimating_the_size_of_gemini3_gpt51_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xe0a/estimating_the_size_of_gemini3_gpt51_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xe0a/estimating_the_size_of_gemini3_gpt51_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:42:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4lovv</id>
    <title>Qwen3-VL Computer Using Agent works extremely well</title>
    <updated>2025-11-23T12:36:50+00:00</updated>
    <author>
      <name>/u/Money-Coast-3905</name>
      <uri>https://old.reddit.com/user/Money-Coast-3905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"&gt; &lt;img alt="Qwen3-VL Computer Using Agent works extremely well" src="https://external-preview.redd.it/Sq30vujKBtYvLAsqvjch2NpcJnBTRk8SnxTPeB4cRLU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1199d59ca5d3099ce2eabd236c89b5eaa3bb0080" title="Qwen3-VL Computer Using Agent works extremely well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/h4ici013403g1.gif"&gt;https://i.redd.it/h4ici013403g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôve been using &lt;strong&gt;Qwen3-VL&lt;/strong&gt; as a real &lt;em&gt;computer-using&lt;/em&gt; agent ‚Äì it moves the mouse, clicks, types, scrolls, and reads the screen from screenshots, pretty much like a human.&lt;/p&gt; &lt;p&gt;I open-sourced a tiny driver that exposes a &lt;code&gt;computer_use&lt;/code&gt; tool over an OpenAI-compatible API and uses &lt;code&gt;pyautogui&lt;/code&gt; to control the desktop. The GIF shows it &lt;strong&gt;resolving a GitHub issue end-to-end fully autonomously&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Repo (code + minimal loop):&lt;br /&gt; üëâ &lt;a href="https://github.com/SeungyounShin/qwen3_computer_use"&gt;https://github.com/SeungyounShin/qwen3_computer_use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next I‚Äôm planning to try &lt;strong&gt;RL tuning&lt;/strong&gt; on top of this Would love feedback or ideas‚Äîhappy to discuss in the comments or DMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Money-Coast-3905"&gt; /u/Money-Coast-3905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T12:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4zk6v</id>
    <title>Is it worth buying RTX 5060Ti 16Gb for a regular gaming + AI cheap PC and moving 3060 12Gb to x8 slot?</title>
    <updated>2025-11-23T22:11:18+00:00</updated>
    <author>
      <name>/u/Global_Impression470</name>
      <uri>https://old.reddit.com/user/Global_Impression470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current specs:&lt;/p&gt; &lt;p&gt;- 5700X&lt;br /&gt; - 2x16Gb 3200Mhz (2 more slots available)&lt;br /&gt; - RTX 3060 12Gb (x16 slot)&lt;br /&gt; - 750W Gold Cougar Gex PSU&lt;/p&gt; &lt;p&gt;I want to try 28Gb of combined VRAM with Ollama, Vllm, OpenWebUI and mb some other software (thinking about ComfyUI as soon as I get rid of my laziness). Is it worth upgrading just in order to have better local LLM experience and slightly better gaming (I don't play much, just sometimes)? Never tried Cloud inference btw, using LLMs for RAG experiments, Continue plugin in IntelliJ IDEs and OCR tasks&lt;/p&gt; &lt;p&gt;Prices in my region:&lt;br /&gt; 5060Ti: 450‚Ç¨ (the only new option)&lt;br /&gt; 3060 12Gb: 200‚Ç¨&lt;br /&gt; 3090: ~500-550‚Ç¨&lt;br /&gt; 4060Ti 16Gb: ~350-400‚Ç¨&lt;/p&gt; &lt;p&gt;And what models it will be able to handle that current build can't / does slow enough to call it unusable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Global_Impression470"&gt; /u/Global_Impression470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4zk6v/is_it_worth_buying_rtx_5060ti_16gb_for_a_regular/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4zk6v/is_it_worth_buying_rtx_5060ti_16gb_for_a_regular/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4zk6v/is_it_worth_buying_rtx_5060ti_16gb_for_a_regular/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T22:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4hkaf</id>
    <title>Qwen3-2B-VL for OCR is actually insane. Dockerized Set Up + GitHub</title>
    <updated>2025-11-23T08:26:46+00:00</updated>
    <author>
      <name>/u/exaknight21</name>
      <uri>https://old.reddit.com/user/exaknight21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to find an efficient model to perform OCR for my use case for a while. I created &lt;a href="https://github.com/ikantkode/exaOCR"&gt;exaOCR&lt;/a&gt; - and when I pushed the code, I can swear on all that is holy that it was working. BUT, for some reason, I simply cannot fix it anymore. It uses OCRMyPDF and the error is literally unsolvable by any models (ChatGPT, DeepSeek, Claude, Grok) and I threw in the towel until I guess I can make enough friends that are actual coders. (If you are able to contribute, please do.)&lt;/p&gt; &lt;p&gt;My entire purpose in using AI to create these crappy streamlit apps is to test the usability for my use case and then essentially go from there. As such, I could never get DeepSeek OCR to work, but someone posted about their project (ocrarena.ai) and I was able to try the models. Not very impressed + the general chatter around it.&lt;/p&gt; &lt;p&gt;I am a huge fan of the Qwen Team and not because they publish everything Open Source, but the fact that they are working towards an efficient AI model that *some* of us peasants can run.&lt;/p&gt; &lt;p&gt;Brings me to the main point. I got a T5610 for $239, I had a 3060 12 GB laying around and I got another for $280 also 12 GB, I threw them both together and they are able to help me experiment. The Qwen3-2B-VL for OCR is actually insane... I mean, deploy it and look for yourself. Just a heads up, my friend tried it on his 10 GB 3080, and vLLM threw an error, you will want to reduce the **--max-model-len from 16384 to probably 8000 **. Remember, I am using dual 3060s giving me more VRAM to play with.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ikantkode/qwen3-2b-ocr-app"&gt;https://github.com/ikantkode/qwen3-2b-ocr-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In any event, here is a short video of it working: &lt;a href="https://youtu.be/anjhfOc7RqA"&gt;https://youtu.be/anjhfOc7RqA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exaknight21"&gt; /u/exaknight21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T08:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ijay</id>
    <title>Making an offline STS (speech to speech) AI that runs under 2GB RAM. But do people even need offline AI now?</title>
    <updated>2025-11-23T09:29:08+00:00</updated>
    <author>
      <name>/u/Automatic_Finish8598</name>
      <uri>https://old.reddit.com/user/Automatic_Finish8598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a full speech to speech AI that runs totally offline. Everything stays on the device. STT, LLM inference and TTS all running locally in under 2GB RAM. I already have most of the architecture working and a basic MVP.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The part I‚Äôm thinking a lot about is the bigger question. With models like Gemini, ChatGPT and Llama becoming cheaper and extremely accessible, why would anyone still want to use something fully offline?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My reason is simple. I want an AI that can work completely on personal or sensitive data without sending anything outside. Something you can use in hospitals, rural government centers, developer setups, early startups, labs, or places where internet isn‚Äôt stable or cloud isn‚Äôt allowed. Basically an AI you own fully, with no external calls.&lt;/p&gt; &lt;p&gt;My idea is to make a proper offline autonomous assistant that behaves like a personal AI layer. It should handle voice, do local reasoning, search your files, automate stuff, summarize documents, all of that, without depending on the internet or any external service.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I‚Äôm curious what others think about this direction. Is offline AI still valuable when cloud AI is getting so cheap? Are there use cases I‚Äôm not thinking about or is this something only a niche group will ever care about?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic_Finish8598"&gt; /u/Automatic_Finish8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T09:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p552bw</id>
    <title>[Update] Epstein Files dataset stays open and ungated on Hugging Face</title>
    <updated>2025-11-24T02:20:19+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you to everyone who provided feedback on our previous post. We agree with your comments - public data should stay public.&lt;/p&gt; &lt;p&gt;As for maintaining the data, we kindly request you to go through this &lt;a href="https://huggingface.co/blog/tensonaut/the-epstein-files"&gt;data usage article&lt;/a&gt; and contribute as volunteer in any way you can. Every small contribution is valuable - priority wise adding additional data from official sources while performing data integrity is of utmost importance&lt;/p&gt; &lt;p&gt;We're creating a central hub for all the investigative tools being built on this dataset. We already have 5 projects from this sub. If you are working on any tool to help journalists to search through the documents efficiently or share findings you've made, we request you to submit a PR &lt;a href="https://github.com/EF20K/Projects"&gt;here&lt;/a&gt; so we can update our documentation and have a central index of all the tools that journalists can use.&lt;/p&gt; &lt;p&gt;Thank you again to everyone who provided feedback and support. This dataset exists because of your feedbacks and suggestions, and we look forward to continuing to build this resource with this sub &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T02:20:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4o9lt</id>
    <title>Olmo 3 from scratch</title>
    <updated>2025-11-23T14:38:20+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"&gt; &lt;img alt="Olmo 3 from scratch" src="https://b.thumbs.redditmedia.com/6yYfJoLy6qtGGUYvc12cn1_9w9dgwwAi7lOyBfzvTxs.jpg" title="Olmo 3 from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lots of interesting LLM releases last week. My favorite was actually the Olmo 3 release. (I love the Olmo series because there's always so much useful info in their technical reports.)&lt;/p&gt; &lt;p&gt;I coded the Olmo 3 architecture in a standalone notebook here if you are interested: &lt;a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb"&gt;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here's the side-by-side architecture comparison with Qwen3: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pmeozowxp03g1.jpg?width=5000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6a5d3528d32dc999681af9017b3dc00613606b34"&gt;https://preview.redd.it/pmeozowxp03g1.jpg?width=5000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6a5d3528d32dc999681af9017b3dc00613606b34&lt;/a&gt;&lt;/p&gt; &lt;p&gt;1) As we can see, the Olmo 3 architecture is relatively similar to Qwen3. However, it's worth noting that this is essentially likely inspired by the Olmo 2 predecessor, not Qwen3. &lt;/p&gt; &lt;p&gt;2) Similar to Olmo 2, Olmo 3 still uses a post-norm flavor instead of pre-norm, as they found in the Olmo 2 paper that it stabilizes the training. &lt;/p&gt; &lt;p&gt;3) Interestingly, the 7B model still uses multi-head attention similar to Olmo 2.&lt;br /&gt; However, to make things more efficient and reduce the KV cache size, they now use sliding-window attention (e.g., similar to Gemma 3). &lt;/p&gt; &lt;p&gt;Next, the 32B model (the figure is not shown here due to space reasons, but you can find it in my &lt;a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison"&gt;The Big LLM Architecture Comparison&lt;/a&gt; article or my &lt;a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb"&gt;Olmo 3 from-scratch notebook&lt;/a&gt;): &lt;/p&gt; &lt;p&gt;4) Overall, it's the same architecture but just scaled up. Also, the proportions (e.g., going from the input to the intermediate size in the feed-forward layer, and so on) roughly match the ones in Qwen3. &lt;/p&gt; &lt;p&gt;5) My guess is the architecture was initially somewhat smaller than Qwen3 due to the smaller vocabulary, and they then scaled up the intermediate size expansion from 5x in Qwen3 to 5.4 in Olmo 3 to have a 32B model for a direct comparison. &lt;/p&gt; &lt;p&gt;6) Also, note that the 32B model (finally!) uses grouped query attention. &lt;/p&gt; &lt;p&gt;And yes, I also did a from-scratch implementation. It was still a lot of work, but since I had already implemented Qwen3 from scratch, as well as Gemma 3 (for the sliding-window attention component), it wasn't too bad!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T14:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5092p</id>
    <title>what do we think of Tenstorrent Blackhole p150a's capabilities as we move into 2026?</title>
    <updated>2025-11-23T22:39:59+00:00</updated>
    <author>
      <name>/u/starkruzr</name>
      <uri>https://old.reddit.com/user/starkruzr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://tenstorrent.com/hardware/blackhole"&gt;https://tenstorrent.com/hardware/blackhole&lt;/a&gt;&lt;/p&gt; &lt;p&gt;spoke to a couple of their folks at some length at Supercomputing last week and 32GB &amp;quot;VRAM&amp;quot; (not exactly, but still) plus the strong connectivity capabilities for ganging cards together for training seems interesting, plus it's less than half as expensive as a 5090. with advancements in software over the last six-ish months, I'm curious how it's benching today vs. other options from Nvidia. about 4 months ago I think it was doing about half the performance of a 5090 at tg.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starkruzr"&gt; /u/starkruzr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5092p/what_do_we_think_of_tenstorrent_blackhole_p150as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5092p/what_do_we_think_of_tenstorrent_blackhole_p150as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5092p/what_do_we_think_of_tenstorrent_blackhole_p150as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T22:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4r0ol</id>
    <title>I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo</title>
    <updated>2025-11-23T16:30:05+00:00</updated>
    <author>
      <name>/u/Glass-Ant-6041</name>
      <uri>https://old.reddit.com/user/Glass-Ant-6041</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"&gt; &lt;img alt="I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo" src="https://external-preview.redd.it/bHBhb2J3bDJhMTNnMe_d2tHulvaS54ITJ5YIpl3vKGq8IwT_QpQcAhaljqVu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44cfc0fb49b1efd73232aa38ace208b1d2c96406" title="I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glass-Ant-6041"&gt; /u/Glass-Ant-6041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wfgc0yl2a13g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T16:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4wlej</id>
    <title>Ai2's Olmo 3 now on OpenRouter üëÄ</title>
    <updated>2025-11-23T20:10:17+00:00</updated>
    <author>
      <name>/u/ghostderp</name>
      <uri>https://old.reddit.com/user/ghostderp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wlej/ai2s_olmo_3_now_on_openrouter/"&gt; &lt;img alt="Ai2's Olmo 3 now on OpenRouter üëÄ" src="https://external-preview.redd.it/ET-WVr4mXHD1An075CPHuJQ0Di6PZSiO2KfryrF_HGA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=792f34f5ea050c95ee3f1033fc94a23dcaca3d68" title="Ai2's Olmo 3 now on OpenRouter üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Parasail added Ai2's Olmo 3 to OpenRouter‚ÄîThink (32B and 7B) and Instruct (7B). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghostderp"&gt; /u/ghostderp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://openrouter.ai/allenai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wlej/ai2s_olmo_3_now_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4wlej/ai2s_olmo_3_now_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ftd5</id>
    <title>Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT.</title>
    <updated>2025-11-23T06:36:53+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt; &lt;img alt="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." src="https://preview.redd.it/94nizo5acy2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a7cc2846512c02a0c4119a338567cbcbcc8574" title="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94nizo5acy2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T06:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4xscg</id>
    <title>Can an expert chime in and explain what is holding Vulkan back from becoming the standard API for ML?</title>
    <updated>2025-11-23T20:59:05+00:00</updated>
    <author>
      <name>/u/A_Chungus</name>
      <uri>https://old.reddit.com/user/A_Chungus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm just getting into GPGPU programming, and my knowledge is limited. I‚Äôve only written a handful of code and mostly just read examples. I‚Äôm trying to understand whether there are any major downsides or roadblocks to writing or contributing to AI/ML frameworks using Vulkan, or whether I should just stick to CUDA or others.&lt;/p&gt; &lt;p&gt;My understanding is that Vulkan is primarily a graphics-focused API, while CUDA, ROCm, and SYCL are more compute-oriented. However, Vulkan has recently been shown to match or even beat CUDA in performance in projects like llama.cpp. With features like &lt;a href="https://www.vulkan.org/user/pages/09.events/vulkanised-2025/T47-Jeff-Bolz-NVIDIA.pdf"&gt;Vulkan Cooperative Vectors&lt;/a&gt;, it seems it possible to squeeze the most performance out of the hardware and only limited by architecture tuning. The only times I see Vulkan lose to CUDA are in a few specific workloads on Linux or when the model exceeds VRAM. In those cases, Vulkan tends to fail or crash, while CUDA still finishes generation, although very slowly.&lt;/p&gt; &lt;p&gt;Since Vulkan can already reach this level of performance and is improving quickly, it seems like a serious contender to challenge CUDA‚Äôs moat and to offer true cross-vendor, cross-platform support unlike the rest. Even if Vulkan never fully matches CUDA‚Äôs performance in every framework, I can still see it becoming the default backend for many applications. For example, Electron dominates desktop development despite its sub-par performance because it makes cross-platform development so easy.&lt;/p&gt; &lt;p&gt;Setting aside companies‚Äô reluctance to invest in Vulkan as part of their AI/ML ecosystems in order to protect their proprietary platforms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are vendors actively doing anything to limit its capabilities?&lt;/li&gt; &lt;li&gt;Could we see more frameworks like &lt;a href="https://docs.pytorch.org/tutorials/unstable/vulkan_workflow.html"&gt;PyTorch&lt;/a&gt; adopting it and eventually making Vulkan a go-to cross-vendor solution?&lt;/li&gt; &lt;li&gt;If more contributions were made to Vulkan ecosystem, could it eventually reach the ecosystem that of CUDA has with libraries and tooling, or will Vulkan always be limited as a permanent ‚Äúsecond source‚Äù backend?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even with the current downsides, I don't think they‚Äôre significant enough to prevent Vulkan from gaining wider adoption in the AI/ML space. Could I be wrong here?&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;I guess what I'm really asking is if there are any CUDA/Vulkan devs that can provide some input on where they think Vulkan is lacking other than what I mentioned and if it its doable eventually to be feature parity with CUDA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A_Chungus"&gt; /u/A_Chungus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4s7nt</id>
    <title>Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!</title>
    <updated>2025-11-23T17:17:31+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" src="https://external-preview.redd.it/QHcTMS_GK1SpPCsYVSA_d521aSr77tuQOduExaTV8io.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=260671c6b499b9df6965e93453782a12c31d98d9" title="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I have your attention, I'd like to ask: Does anyone here honestly bother with models below 12B? Like 8B, 4B, or 2B? I feel like I might have neglected smaller model sizes for far too long.&lt;/p&gt; &lt;p&gt;Also: &amp;quot;Air 4.6 in two weeks!&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Snowpiercer v4 is part of the Gen 4.0 series I'm working on that puts more focus on character adherence. YMMV. You might want to check out Gen 3.5/3.0 if Gen 4.0 isn't doing it for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TheDrummer/directory"&gt;https://huggingface.co/spaces/TheDrummer/directory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ln6s</id>
    <title>Computer Manufacturer threw my $ 20000 rig down the stairs and now says everything is fine</title>
    <updated>2025-11-23T12:34:18+00:00</updated>
    <author>
      <name>/u/phwlarxoc</name>
      <uri>https://old.reddit.com/user/phwlarxoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought a custom built Threadripper Pro water-cooled dual RTX 4090 workstation from a builder and had it updated a couple of times with new hardware so that finally it became a rig worth about $20000.&lt;/p&gt; &lt;p&gt;Upon picking up the machine last week from the builder after another upgrade I asked staff that we check together the upgrade before paying and confirming the order fulfilled.&lt;/p&gt; &lt;p&gt;They lifted the machine (still in its box and secured with two styrofoam blocks), on a table, but the heavy box (30kg) slipped from their hands, the box fell on the floor and from there down a staircase where it cartwheeled several times until it stopped at the end of the stairs.&lt;/p&gt; &lt;p&gt;They sent a mail saying they checked the machine and everything is fine.&lt;/p&gt; &lt;p&gt;Who wouldn't expect otherwise.&lt;/p&gt; &lt;p&gt;Can anyone comment on possible damages such an incident can have on the electronics, PCIe Slots, GPUs, watercooling, mainboard etc, ‚Äî also on what damages might have occurred that are not immediately evident, but could e.g. impact signal quality and therefore speed? Would you accept back such a machine?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phwlarxoc"&gt; /u/phwlarxoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T12:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jyrv</id>
    <title>No way kimi gonna release new model !!</title>
    <updated>2025-11-23T10:57:51+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt; &lt;img alt="No way kimi gonna release new model !!" src="https://preview.redd.it/1ezldlbumz2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ad6c7d5d0f6a6e2b160c885a08a82d80d71ef81" title="No way kimi gonna release new model !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ezldlbumz2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4t5ix</id>
    <title>I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!</title>
    <updated>2025-11-23T17:54:06+00:00</updated>
    <author>
      <name>/u/Inv1si</name>
      <uri>https://old.reddit.com/user/Inv1si</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt; &lt;img alt="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" src="https://external-preview.redd.it/YWZjazBjYjBwMTNnMfl_KE3bRLUmxUrgo6sq7iH5IJtc0qUYB-cQv58tKBaC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59e7a57c308ef96e98b98c0b7dd565ffc1a5aa0c" title="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inv1si"&gt; /u/Inv1si &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9r0ixbb0p13g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4zjmd</id>
    <title>vibe coding at its finest</title>
    <updated>2025-11-23T22:10:38+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4zjmd/vibe_coding_at_its_finest/"&gt; &lt;img alt="vibe coding at its finest" src="https://preview.redd.it/qxnrtbquy23g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1527e83e1947277781cd098addc6cd3adc91fb6d" title="vibe coding at its finest" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qxnrtbquy23g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4zjmd/vibe_coding_at_its_finest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4zjmd/vibe_coding_at_its_finest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T22:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
