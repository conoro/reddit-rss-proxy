<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-15T11:36:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qc9m6x</id>
    <title>GLM-Image is released!</title>
    <updated>2026-01-14T01:17:16+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt; &lt;img alt="GLM-Image is released!" src="https://external-preview.redd.it/Ei4JzvCHJGNODl-Xo97JEKHuZJZU81UlEy5iyXWioSw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=251fac1763ed77fdaf4e281f649fddd4555de498" title="GLM-Image is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-Image is an image generation model adopts a hybrid autoregressive + diffusion decoder architecture. In general image generation quality, GLM‚ÄëImage aligns with mainstream latent diffusion approaches, but it shows significant advantages in text-rendering and knowledge‚Äëintensive generation scenarios. It performs especially well in tasks requiring precise semantic understanding and complex information expression, while maintaining strong capabilities in high‚Äëfidelity and fine‚Äëgrained detail generation. In addition to text‚Äëto‚Äëimage generation, GLM‚ÄëImage also supports a rich set of image‚Äëto‚Äëimage tasks including image editing, style transfer, identity‚Äëpreserving generation, and multi‚Äësubject consistency.&lt;/p&gt; &lt;p&gt;Model architecture: a hybrid autoregressive + diffusion decoder design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T01:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qct6h2</id>
    <title>How does my local LLM rig look?</title>
    <updated>2026-01-14T17:18:09+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"&gt; &lt;img alt="How does my local LLM rig look?" src="https://preview.redd.it/z1xw8usylcdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc831716841f3b411148307f63ee880af80b163" title="How does my local LLM rig look?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In garage/ freezing MN temps are nice!&lt;/p&gt; &lt;p&gt;Key Specs:&lt;/p&gt; &lt;p&gt;Motherboard: ASUS Pro WS W790E-SAGE SE (workstation platform, multi-GPU + tons of PCIe)&lt;/p&gt; &lt;p&gt;CPU: Intel Xeon W9-3495X 56 cores 112 threads, Intel AMX primarily for ktransformers build in mind (moved from an engineering sample to retail)&lt;/p&gt; &lt;p&gt;Memory: 512GB DDR5 ECC (8√ó64GB) 4800 but overclocked to 6000 on an octa-channel platform&lt;/p&gt; &lt;p&gt;GPUs: 2√ó NVIDIA RTX PRO 6000 Blackwell Workstation Edition (96GB VRAM each)&lt;/p&gt; &lt;p&gt;Storage: Samsung 9100 PRO 4TB Gen5 NVMe for models + WD_BLACK SN850X 2TB for OS&lt;/p&gt; &lt;p&gt;Network: 10Gb local + 1Gb internet&lt;/p&gt; &lt;p&gt;Can you spot all other tools except for the server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1xw8usylcdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcsmww</id>
    <title>We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.</title>
    <updated>2026-01-14T16:58:22+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an AI agent to localize imported food products for a retail client. The task sounds simple: extract product info, translate it contextually (not Google Translate), calculate nutritional values for local formats, check compliance with local regulations.&lt;/p&gt; &lt;p&gt;First attempt: one detailed prompt. Let the AI figure out the workflow.&lt;/p&gt; &lt;p&gt;Result: chaos. The AI would hallucinate numbers even with clean images. It would skip steps randomly. At scale, we had no idea where things broke. Every error was a mystery to debug.&lt;/p&gt; &lt;p&gt;So we broke it down. Way down. 27 steps.&lt;/p&gt; &lt;p&gt;Each column in our system handles one thing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extract product name&lt;/li&gt; &lt;li&gt;Extract weight&lt;/li&gt; &lt;li&gt;Extract nutritional values per serving&lt;/li&gt; &lt;li&gt;Convert units to local format&lt;/li&gt; &lt;li&gt;Translate product name (contextual, not literal)&lt;/li&gt; &lt;li&gt;Translate description&lt;/li&gt; &lt;li&gt;Check certification requirements&lt;/li&gt; &lt;li&gt;... and so on&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What changed:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Traceability.&lt;/strong&gt; When something fails, we know exactly which step. No more guessing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Fixability.&lt;/strong&gt; Client corrects a number extraction error once, we build a formula that prevents it downstream. Errors get fixed permanently, not repeatedly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Consistency at scale.&lt;/strong&gt; The AI isn't &amp;quot;deciding&amp;quot; what to do. It's executing a defined process. Same input, same process, predictable output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Human oversight actually works.&lt;/strong&gt; The person reviewing outputs learns where the AI struggles. Step 14 always needs checking. Step 22 is solid. They get faster over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The counterintuitive part:&lt;/strong&gt; making the AI &amp;quot;dumber&amp;quot; per step made the overall system smarter. One prompt trying to do everything is one prompt that can fail in infinite ways. 27 simple steps means 27 places where you can inspect, correct, and improve.&lt;/p&gt; &lt;p&gt;We've processed over 10,000 products this way. The manual process used to take 20 minutes per product. Now it's 3 minutes, mostly human review.&lt;/p&gt; &lt;p&gt;The boring truth about reliable AI agents: it's not about prompt engineering magic. It's about architecture that assumes AI will fail and makes failure easy to find and fix.&lt;/p&gt; &lt;p&gt;Happy to answer questions about the approach.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T16:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdgtny</id>
    <title>Microsoft releases FrogMini on HF. Built on Qwen3-14B</title>
    <updated>2026-01-15T11:13:17+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/microsoft/FrogMini-14B-2510"&gt;https://huggingface.co/microsoft/FrogMini-14B-2510&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Achieving state-of-the-art performance on SWE-Bench Verified (Pass@1: 45.0%)&lt;/p&gt; &lt;p&gt;Employs supervised fine-tuning (SFT) on successful debugging trajectories generated by a strong teacher model (e.g., Claude), obtained from a mix of real-world and synthetic bug datasets&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgtny/microsoft_releases_frogmini_on_hf_built_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgtny/microsoft_releases_frogmini_on_hf_built_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgtny/microsoft_releases_frogmini_on_hf_built_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T11:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcl543</id>
    <title>Which are the top LLMs under 8B right now?</title>
    <updated>2026-01-14T11:42:15+00:00</updated>
    <author>
      <name>/u/Additional_Secret_75</name>
      <uri>https://old.reddit.com/user/Additional_Secret_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I m looking to pick a local LLM and not sure what to go with anymore. There are a lot of ‚Äúbest‚Äù &amp;lt;8B models and every post says something different, even for the same model. What are people using for normal chat, research, or some coding, not super censored and runs well without a ton of VRAM. It doesn t have to be just one LLM, just the best in their category.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Secret_75"&gt; /u/Additional_Secret_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T11:42:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdh5l5</id>
    <title>I built a 100% Rust orchestrator to chain local models (Ollama, Whisper) without Python or LangChain. Runs entirely offline.</title>
    <updated>2026-01-15T11:32:37+00:00</updated>
    <author>
      <name>/u/stxrmcrypt</name>
      <uri>https://old.reddit.com/user/stxrmcrypt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh5l5/i_built_a_100_rust_orchestrator_to_chain_local/"&gt; &lt;img alt="I built a 100% Rust orchestrator to chain local models (Ollama, Whisper) without Python or LangChain. Runs entirely offline." src="https://preview.redd.it/uzq44ca1xhdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72539543c1f46574256c1d988775f0e7d56e5d30" title="I built a 100% Rust orchestrator to chain local models (Ollama, Whisper) without Python or LangChain. Runs entirely offline." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Like many of you, I got tired of the &amp;quot;modern&amp;quot; AI stack. I wanted to build complex workflows (like &amp;quot;watch folder -&amp;gt; transcribe audio -&amp;gt; summarize text -&amp;gt; save to obsidian&amp;quot;), but every tool out there felt like overkill. They were either wrappers around the OpenAI API or massive Python frameworks that required a venv just to say &amp;quot;hello.&amp;quot;&lt;/p&gt; &lt;p&gt;I wanted the &amp;quot;Unix pipes&amp;quot; philosophy, but for local intelligence. So I built &lt;strong&gt;LAO (Local AI Orchestrator)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Pitch:&lt;/strong&gt; It‚Äôs a desktop app (in alpha so lower ur expectations) written in &lt;strong&gt;Rust&lt;/strong&gt; (backend + native egui frontend) that lets you chain local models into Directed Acyclic Graphs (DAGs). It runs completely offline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;No Python Required:&lt;/strong&gt; It's a single binary. No &lt;code&gt;pip install&lt;/code&gt;, no CUDA version conflicts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Stack:&lt;/strong&gt; It connects to &lt;strong&gt;Ollama&lt;/strong&gt; for LLMs and uses native Rust bindings for tools like Whisper.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Builder:&lt;/strong&gt; I built a node-based graph editor in &lt;code&gt;egui&lt;/code&gt; so you can visually drag-and-drop steps (or just write YAML if you prefer).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt-to-Workflow:&lt;/strong&gt; You can literally type &amp;quot;Summarize this audio file and tag action items,&amp;quot; and it uses a local model (like Llama 3) to generate the execution graph for you.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plugin System:&lt;/strong&gt; I implemented a dynamic loading system (&lt;code&gt;.dll&lt;/code&gt;/&lt;code&gt;.so&lt;/code&gt;) so you can write your own high-performance plugins in Rust and drop them in.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I built it:&lt;/strong&gt; I realized that if we want &amp;quot;Edge AI&amp;quot; to be real, we need tools that respect system resources. Python is great for prototyping, but I wanted something that could run in the background without eating 4GB of RAM just for the orchestrator itself.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://www.github.com/abendrothj/lao"&gt;https://www.github.com/abendrothj/lao&lt;/a&gt; &lt;/p&gt; &lt;p&gt;It‚Äôs open source (MIT). I‚Äôd love to hear what kind of local workflows you all are running and if this would be useful for your setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stxrmcrypt"&gt; /u/stxrmcrypt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uzq44ca1xhdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh5l5/i_built_a_100_rust_orchestrator_to_chain_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh5l5/i_built_a_100_rust_orchestrator_to_chain_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T11:32:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd8vpj</id>
    <title>Claude Code or OpenCode which one do you use and why?</title>
    <updated>2026-01-15T03:42:21+00:00</updated>
    <author>
      <name>/u/Empty_Break_8792</name>
      <uri>https://old.reddit.com/user/Empty_Break_8792</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm curious what people here are using more for coding: &lt;strong&gt;Claude Code&lt;/strong&gt; or &lt;strong&gt;OpenCode&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Which one do you personally prefer, and &lt;em&gt;why&lt;/em&gt;?&lt;br /&gt; Is it better reasoning, speed, pricing, rate limits, editor integration, or something else?&lt;/p&gt; &lt;p&gt;Would love to hear real-world experiences and tradeoffs. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Break_8792"&gt; /u/Empty_Break_8792 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd8vpj/claude_code_or_opencode_which_one_do_you_use_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd8vpj/claude_code_or_opencode_which_one_do_you_use_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd8vpj/claude_code_or_opencode_which_one_do_you_use_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T03:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdh28f</id>
    <title>RTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured</title>
    <updated>2026-01-15T11:27:15+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia has essentially killed off supply for the RTX 5070 Ti. Also supply of RTX 5060 Ti 16 GB has been significantly reduced. This happened partially due to memory supply shortages. This means that most AIBs will no longer manufacture these GPUs. Prices are already jumping significantly. The 5070 Ti has risen ~$100 over MSRP, and retailers expect further hikes. 8 GB configuration of RTX 5060 Ti remains unaffected. &lt;/p&gt; &lt;p&gt;Credit: Hardware Unboxed &lt;/p&gt; &lt;p&gt;&lt;a href="https://m.youtube.com/watch?v=yteN21aJEvE"&gt;https://m.youtube.com/watch?v=yteN21aJEvE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T11:27:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdfosg</id>
    <title>Anyone finetuned the OLMocr 2 on custom data?</title>
    <updated>2026-01-15T10:04:47+00:00</updated>
    <author>
      <name>/u/nightwing_2</name>
      <uri>https://old.reddit.com/user/nightwing_2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need help in fine tuning OLMocr on custom dataset including data preparation pipeline&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nightwing_2"&gt; /u/nightwing_2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdfosg/anyone_finetuned_the_olmocr_2_on_custom_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdfosg/anyone_finetuned_the_olmocr_2_on_custom_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdfosg/anyone_finetuned_the_olmocr_2_on_custom_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T10:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcv64u</id>
    <title>What‚Äôs the deal with these fake GPU listings on eBay?</title>
    <updated>2026-01-14T18:29:25+00:00</updated>
    <author>
      <name>/u/humandisaster99</name>
      <uri>https://old.reddit.com/user/humandisaster99</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"&gt; &lt;img alt="What‚Äôs the deal with these fake GPU listings on eBay?" src="https://b.thumbs.redditmedia.com/65MpuHjYbWHFf1_tcI0FQmitzKDLLh1kLzUX95wfnoE.jpg" title="What‚Äôs the deal with these fake GPU listings on eBay?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been seeing these around for a while. For most AI GPU searches there will be a couple on the first page. It‚Äôs always a zero review account that was created same-day selling for a third of the normal price. They‚Äôre very clearly scams, but how? eBay buyer protection will always provide a refund if you ask for it basically, so what‚Äôs the scam? Do they just send you a fake GPU and hope you don‚Äôt notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/humandisaster99"&gt; /u/humandisaster99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qcv64u"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:29:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcy7ug</id>
    <title>meituan-longcat/LongCat-Flash-Thinking-2601 ¬∑ Hugging Face</title>
    <updated>2026-01-14T20:20:00+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcy7ug/meituanlongcatlongcatflashthinking2601_hugging/"&gt; &lt;img alt="meituan-longcat/LongCat-Flash-Thinking-2601 ¬∑ Hugging Face" src="https://external-preview.redd.it/kb1mVOfmWTAvSMxYL_8sXovYFqgoXm6u9Rl74bhEZK8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c6634be30000f0e6f7229b645e18aa7d9cde211" title="meituan-longcat/LongCat-Flash-Thinking-2601 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking-2601"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcy7ug/meituanlongcatlongcatflashthinking2601_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcy7ug/meituanlongcatlongcatflashthinking2601_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T20:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdf6h7</id>
    <title>Raspberry Pi AI HAT+ 2 launch</title>
    <updated>2026-01-15T09:33:04+00:00</updated>
    <author>
      <name>/u/nicolash33</name>
      <uri>https://old.reddit.com/user/nicolash33</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Raspberry Pi AI HAT+ 2 is available now at $130, with 8 GB onboard LPDDR4X-4267 SDRAM, with the Hailo-10H accelerator &lt;/p&gt; &lt;p&gt;Since it uses the only pcie express port, there's no easy way to have both the accelerator and an nvme at the same time I presume.&lt;/p&gt; &lt;p&gt;What do you guys this about this for edge LLMs ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicolash33"&gt; /u/nicolash33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.raspberrypi.com/products/ai-hat-plus-2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdf6h7/raspberry_pi_ai_hat_2_launch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdf6h7/raspberry_pi_ai_hat_2_launch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T09:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd3jk9</id>
    <title>llama.cpp has incredible performance on Ubuntu, i'd like to know why</title>
    <updated>2026-01-14T23:46:56+00:00</updated>
    <author>
      <name>/u/Deep_Traffic_7873</name>
      <uri>https://old.reddit.com/user/Deep_Traffic_7873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.phoronix.com/review/ubuntu-2604-jan-amd-epyc/4"&gt;&lt;strong&gt;https://www.phoronix.com/review/ubuntu-2604-jan-amd-epyc/4&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Traffic_7873"&gt; /u/Deep_Traffic_7873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd3jk9/llamacpp_has_incredible_performance_on_ubuntu_id/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd3jk9/llamacpp_has_incredible_performance_on_ubuntu_id/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd3jk9/llamacpp_has_incredible_performance_on_ubuntu_id/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T23:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdd1l7</id>
    <title>Step-Audio-R1.1 (Open Weight) by StepFun just set a new SOTA on the Artificial Analysis Speech Reasoning leaderboard</title>
    <updated>2026-01-15T07:20:11+00:00</updated>
    <author>
      <name>/u/Inevitable_Sea8804</name>
      <uri>https://old.reddit.com/user/Inevitable_Sea8804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"&gt; &lt;img alt="Step-Audio-R1.1 (Open Weight) by StepFun just set a new SOTA on the Artificial Analysis Speech Reasoning leaderboard" src="https://b.thumbs.redditmedia.com/VlFNErXfdwuwoj22ZqGSh4EmErOD8Zi59CLceVgOUFM.jpg" title="Step-Audio-R1.1 (Open Weight) by StepFun just set a new SOTA on the Artificial Analysis Speech Reasoning leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post: &lt;a href="https://x.com/ModelScope2022/status/2011687986338136089"&gt;https://x.com/ModelScope2022/status/2011687986338136089&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-R1.1"&gt;https://huggingface.co/stepfun-ai/Step-Audio-R1.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://modelscope.cn/studios/stepfun-ai/Step-Audio-R1"&gt;https://modelscope.cn/studios/stepfun-ai/Step-Audio-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It outperforms Grok, Gemini, and GPT-Realtime with a 96.4% accuracy rate.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native Audio Reasoning (End-to-End)&lt;/li&gt; &lt;li&gt;Audio-native CoT (Chain of Thought)&lt;/li&gt; &lt;li&gt;Real-time streaming inference&lt;/li&gt; &lt;li&gt;FULLY OPEN SOURCE&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wln8b464sgdg1.png?width=6507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc83bc19c38b1c973fe264d3f32ca1b0ee860fbc"&gt;https://preview.redd.it/wln8b464sgdg1.png?width=6507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc83bc19c38b1c973fe264d3f32ca1b0ee860fbc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nxnh1w35sgdg1.png?width=3960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=303afc1cca072ad309af2b75944f675d033da76c"&gt;https://preview.redd.it/nxnh1w35sgdg1.png?width=3960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=303afc1cca072ad309af2b75944f675d033da76c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/14mu93p5sgdg1.png?width=6008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403110a80e8d15fc1e4a48362ab28c34f6a42042"&gt;https://preview.redd.it/14mu93p5sgdg1.png?width=6008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403110a80e8d15fc1e4a48362ab28c34f6a42042&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Sea8804"&gt; /u/Inevitable_Sea8804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T07:20:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdgeak</id>
    <title>MiniMax-M2.1 REAP models (0xSero) are fixed!</title>
    <updated>2026-01-15T10:48:34+00:00</updated>
    <author>
      <name>/u/AdamDhahabi</name>
      <uri>https://old.reddit.com/user/AdamDhahabi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, some experts where mistakenly left out and that caused loops, new GGUF uploads happening right now.&lt;br /&gt; - REAP-20 Deprecated&lt;br /&gt; - REAP-30 &lt;strong&gt;Fixed&lt;/strong&gt;&lt;br /&gt; - REAP-40 &lt;strong&gt;Fixed&lt;/strong&gt;&lt;br /&gt; - REAP-50 Deprecated&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-30-GGUF"&gt;https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-30-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-40-GGUF"&gt;https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-40-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdamDhahabi"&gt; /u/AdamDhahabi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgeak/minimaxm21_reap_models_0xsero_are_fixed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgeak/minimaxm21_reap_models_0xsero_are_fixed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgeak/minimaxm21_reap_models_0xsero_are_fixed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T10:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcvk9n</id>
    <title>Popularity of DDR3 motherboards is growing rapidly - VideoCardz.com</title>
    <updated>2026-01-14T18:43:26+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcvk9n/popularity_of_ddr3_motherboards_is_growing/"&gt; &lt;img alt="Popularity of DDR3 motherboards is growing rapidly - VideoCardz.com" src="https://external-preview.redd.it/1_qecsYofG5L-GuD7Gh4daRo-sGqgl6aBwxtGGFCAGA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f35147fed27d85c69f96db9aabc5b1ec2622714" title="Popularity of DDR3 motherboards is growing rapidly - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I genuinely hate this timeline.&lt;/p&gt; &lt;p&gt;While I'm in the very lucky position to have bought more than enough RAM and storage for my homelab and local LLM needs before prices went up, my favorite past time and hobby of homelabbing feels completely ruined.&lt;/p&gt; &lt;p&gt;Three months ago, I was looking forward to ECC DDR5 prices coming down to the point of being bale to buy 512GB DDR5 RAM for ~‚Ç¨500 to finally have a Saphire Rapids Xeon in my homelab and play with AMX, I'm now afraid that DDR4 stick I have might fail, and not being able to replace it.&lt;/p&gt; &lt;p&gt;With DDR4 prices through the roof, I guess this was bound to happen, but it doesn't make it sting any less. How long now until DDR3 prices also skyrocket, and with them the motherboards and CPUs that also support it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/popularity-of-ddr3-motherboards-is-growing-rapidly"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcvk9n/popularity_of_ddr3_motherboards_is_growing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcvk9n/popularity_of_ddr3_motherboards_is_growing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcv304</id>
    <title>NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3</title>
    <updated>2026-01-14T18:26:19+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"&gt; &lt;img alt="NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3" src="https://external-preview.redd.it/eGh0aTBhazZ5Y2RnMTTPucJdRjO2R67S5i-oYJkuLIwhwL3TAJbW3Q2hg2iU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a81ca1f90638b7e5fb617d89b9d1a2abf729cae1" title="NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;The team at Neuphonic is back with a new open-source release: NeuTTS Nano.&lt;/p&gt; &lt;p&gt;After NeuTTS Air trended #1 on HuggingFace last October, we received a lot of requests for something even smaller that could fit into tighter VRAM/RAM constraints for robotics and embedded agents.&lt;/p&gt; &lt;p&gt;Key Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Size: 120M active parameters (3x smaller than NeuTTS Air).&lt;/li&gt; &lt;li&gt;Architecture: Simple LM + codec architecture built off Llama3.&lt;/li&gt; &lt;li&gt;Format: Provided in GGML for easy deployment on mobile, Jetson, and Raspberry Pi.&lt;/li&gt; &lt;li&gt;Capabilities: Instant voice cloning (3s sample) and ultra-realistic prosody.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why use this?&lt;/p&gt; &lt;p&gt;If you are building for smart home devices, robotics, or mobile apps where every MB of RAM matters, Nano is designed for you. It delivers the same &amp;quot;voice magic&amp;quot; but in a much lighter package.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/neuphonic/neutts"&gt;https://github.com/neuphonic/neutts&lt;/a&gt; &lt;/li&gt; &lt;li&gt;HuggingFace: &lt;a href="https://huggingface.co/neuphonic/neutts-nano"&gt;https://huggingface.co/neuphonic/neutts-nano&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Spaces: &lt;a href="https://huggingface.co/spaces/neuphonic/neutts-nano"&gt;https://huggingface.co/spaces/neuphonic/neutts-nano&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Website: &lt;a href="https://www.neuphonic.com/"&gt;https://www.neuphonic.com/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôre curious to see the RTF (Real-Time Factor) benchmarks the community gets on different hardware. What‚Äôs the smallest device you‚Äôre planning to run this on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2nikcyj6ycdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcusnt</id>
    <title>Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M</title>
    <updated>2026-01-14T18:16:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt; &lt;img alt="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" src="https://external-preview.redd.it/NXZ5NDNuYTlzY2RnMX4ZwK1s5ENYxRsvoiSEu3mA0RmAAs2-sAvwRMu-2CtN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2a20f3c8df9af0a0fbced04bbc8dc6ec0450abe" title="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;Today, I am announcing Soprano 1.1! I‚Äôve designed it for massively improved stability and audio quality over the original model. &lt;/p&gt; &lt;p&gt;While many of you were happy with the quality of Soprano, it had a tendency to start, well, &lt;em&gt;Mongolian throat singing&lt;/em&gt;. Contrary to its name, Soprano is &lt;strong&gt;NOT&lt;/strong&gt; supposed to be for singing, so I have reduced the frequency of these hallucinations by &lt;strong&gt;95%&lt;/strong&gt;. Soprano 1.1-80M also has a &lt;strong&gt;50%&lt;/strong&gt; lower WER than Soprano-80M, with comparable clarity to much larger models like Chatterbox-Turbo and VibeVoice. In addition, it now supports sentences up to &lt;strong&gt;30 seconds&lt;/strong&gt; long, up from 15.&lt;/p&gt; &lt;p&gt;The outputs of Soprano could sometimes have a lot of artifacting and high-frequency noise. This was because the model was severely undertrained. I have trained Soprano further to reduce these audio artifacts.&lt;/p&gt; &lt;p&gt;According to a blind study I conducted on my family (against their will), they preferred Soprano 1.1's outputs &lt;strong&gt;63%&lt;/strong&gt; of the time, so these changes have produced a noticeably improved model.&lt;/p&gt; &lt;p&gt;You can check out the new Soprano here:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ekwek/Soprano-1.1-80M"&gt;https://huggingface.co/ekwek/Soprano-1.1-80M&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Try Soprano 1.1 Now: &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt; &lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v0c2rda9scdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:16:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdax6z</id>
    <title>LFM 2.5 is insanely good</title>
    <updated>2026-01-15T05:22:51+00:00</updated>
    <author>
      <name>/u/guiopen</name>
      <uri>https://old.reddit.com/user/guiopen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's the first model at ~1b that I find not just useful, but altright good and comparable to models 3x larger&lt;/p&gt; &lt;p&gt;Everytime a ultra small model launches with impressive benchmark numbers , it's always the same thing: infinite loops, breaking in multi turn conversations, doesn't know basic facts like the size of an elephant, etc etc... And it is very good at my native language (Portuguese) despite it not being officially supported&lt;/p&gt; &lt;p&gt;But this is different, the benchmarks seem to reflect it's performance really well, and it feels somewhere in between llama 2 7b and llama 3 8b&lt;/p&gt; &lt;p&gt;You should try it. I am running at Q6 and having excelent results for simple tasks like basic QA and summarization.&lt;/p&gt; &lt;p&gt;The jump from lfm2 makes me excited about the 8b-a1b moe model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guiopen"&gt; /u/guiopen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdax6z/lfm_25_is_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdax6z/lfm_25_is_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdax6z/lfm_25_is_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T05:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd92pm</id>
    <title>stepfun-ai/Step3-VL-10B ¬∑ Hugging Face</title>
    <updated>2026-01-15T03:51:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd92pm/stepfunaistep3vl10b_hugging_face/"&gt; &lt;img alt="stepfun-ai/Step3-VL-10B ¬∑ Hugging Face" src="https://preview.redd.it/88t4oaa3rfdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88fb05f58d5a4493756c36b2bd8d44c42fe3366b" title="stepfun-ai/Step3-VL-10B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;stepfun-ai/Step3-VL-10B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/88t4oaa3rfdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd92pm/stepfunaistep3vl10b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd92pm/stepfunaistep3vl10b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T03:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdbxei</id>
    <title>Mistral releases Ministral 3 paper</title>
    <updated>2026-01-15T06:16:31+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;details: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2601.08584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdbxei/mistral_releases_ministral_3_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdbxei/mistral_releases_ministral_3_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T06:16:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd88v2</id>
    <title>I trained a model to 'unslop' AI prose</title>
    <updated>2026-01-15T03:12:29+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"&gt; &lt;img alt="I trained a model to 'unslop' AI prose" src="https://b.thumbs.redditmedia.com/9LRUV10qj7Dvr1f-NPfxLbXNLOlOCDxfXyYG6Tn3zms.jpg" title="I trained a model to 'unslop' AI prose" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran passages from Project Gutenberg through GPT-4o-mini 10 times over, each time telling it to &amp;quot;make it read far better, adding superior prose, etc.&amp;quot;. This lead to classic literary passages being enslopped. I then reversed this pipeline, and trained a model to go from [slop] -&amp;gt; [original]. The resulting model is capable enough to fool Pangram (a fairly robust AI detector - I take this as a metric of how 'human-sounding' the output is), at very little overall quality cost:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/go88234vifdg1.png?width=2817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed2c84e748f4441648e9f53c891258d78ccbb0a"&gt;While quality decreases slightly, humanness jumps from 0 to 0.481. The unslopped version stays firmly above Mistral Large 3 and close to the original GPT-5.2 baseline.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, the model is OSS: &lt;a href="https://huggingface.co/N8Programs/Unslopper-30B-A3B-bf16"&gt;https://huggingface.co/N8Programs/Unslopper-30B-A3B-bf16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And there is a twitter thread (complete with 'thread below üëá', ironic in a thread about slop): &lt;a href="https://x.com/N8Programs/status/2011591738591494625"&gt;https://x.com/N8Programs/status/2011591738591494625&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The goal here is &lt;strong&gt;not&lt;/strong&gt; to fool Pangram, deceive/cheat, etc. I only use Pangram as a proxy for the prose being more readable - ideally, you'd use this model to make your own AI-generated passages more palatable to read, or as part of a larger pipeline in automated writing generation for training, for instance.&lt;/p&gt; &lt;p&gt;Example (GPT-5.2):&lt;br /&gt; The wind rose all at once, as if the mountain itself had inhaled.&lt;/p&gt; &lt;p&gt;She stood at the edge of the pass, cloak snapping like a torn banner behind her, staring down at the valley where the last lights were going out one by one. Each flame extinguished felt deliberate‚Äîan answer to a question she had been too afraid to ask. Somewhere below, bells began to ring, not in warning, but in mourning. They already knew.&lt;/p&gt; &lt;p&gt;‚ÄúDon‚Äôt,‚Äù he said, breathless, reaching for her arm. His fingers brushed fabric, then air. ‚ÄúIf you cross that line, there‚Äôs no coming back.‚Äù&lt;/p&gt; &lt;p&gt;She didn‚Äôt turn. The cold burned her lungs, sharpened her thoughts. ‚ÄúThere was never a way back,‚Äù she replied softly. ‚ÄúOnly a way forward you were willing to pretend didn‚Äôt exist.‚Äù&lt;/p&gt; &lt;p&gt;Thunder rolled, low and ancient. For a moment, the sky fractured into white fire, and in that instant he saw her face‚Äîresolved, terrified, incandescent with purpose. Not the girl he had sworn to protect, but the woman the world had demanded she become.&lt;/p&gt; &lt;p&gt;‚ÄúYou‚Äôll die,‚Äù he said.&lt;/p&gt; &lt;p&gt;She smiled then, finally facing him, eyes bright with unshed tears. ‚ÄúSo will everyone,‚Äù she answered. ‚ÄúThe difference is what we leave burning when we‚Äôre gone.‚Äù&lt;/p&gt; &lt;p&gt;She stepped forward.&lt;/p&gt; &lt;p&gt;The mountain exhaled.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pangram Verdict:&lt;/strong&gt; High Confidence AI Generated&lt;/p&gt; &lt;p&gt;Unslopped (Temp=0.8, Rep Penalty=1.1, 6-bit MLX quant):&lt;br /&gt; And then it just happened. Like the whole of reality had been held breathless for a moment, and suddenly sprang into focus.&lt;/p&gt; &lt;p&gt;She stood at the pass's edge. Her cloak flapped behind her like a banner carried to its last stop. She was looking down into the valley, at lights being snuffed out one by one. Each flame extinguished was a point of answer to questions she had dared not ask. And now bells were ringing, not a signal but a dirge, because they already knew.&lt;/p&gt; &lt;p&gt;&amp;quot;Please,&amp;quot; he said. He reached for her; his hand closed on the snaking line of fabric that trailed behind her, then nothing. &amp;quot;You can't cross over.&amp;quot;&lt;/p&gt; &lt;p&gt;She didn't turn to him. It was cold enough to bite at her throat; she breathed it in, sharp and full of poison. &amp;quot;There was never a way out,&amp;quot; she said softly. &amp;quot;Only the road in that you did not want to remember.&amp;quot;&lt;/p&gt; &lt;p&gt;A low rumble of thunder, older than mountains, filled the air. A white flash lit up the sky for a moment, a vivid static song of fire that illuminated her face in an instant. It was no longer the girl he'd sworn to protect, but the woman she had become.&lt;/p&gt; &lt;p&gt;&amp;quot;You'll die.&amp;quot;&lt;/p&gt; &lt;p&gt;She smiled at him and turned toward him. There were tears on her face, but they did not fall. &amp;quot;We all do,&amp;quot; she said. &amp;quot;The difference is what we leave behind us.&amp;quot;&lt;/p&gt; &lt;p&gt;She started walking toward the edge.&lt;/p&gt; &lt;p&gt;And it all happened at once. The mountain exhaled itself, and took her with it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pangram Verdict:&lt;/strong&gt; High Confidence Human Written&lt;/p&gt; &lt;p&gt;Note that there are some local coherence issues w/ the Unslopper - that's why I'd recommend integrating it into a larger pipeline or editing its output yourself. It's definitely not production ready.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T03:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcuerc</id>
    <title>NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency</title>
    <updated>2026-01-14T18:02:19+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen some arguments we‚Äôve reached AGI, it‚Äôs just about putting the separate pieces together in the right context. I think having a relatively small model that knows how to connect with other tools and models is exactly the correct route towards very functional systems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd6nho</id>
    <title>Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)</title>
    <updated>2026-01-15T02:01:03+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/"&gt; &lt;img alt="Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)" src="https://external-preview.redd.it/67JUXSnUreB8wTlODdM32UrKgKSfJgeIROoAbEyBScs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d29623e3dc2c928508ca7ce7d10f296f2a1d15a" title="Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.scmp.com/tech/tech-war/article/3339869/zhipu-ai-breaks-us-chip-reliance-first-major-model-trained-huawei-stack"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T02:01:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
