<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-08T16:25:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nb3b8l</id>
    <title>Aquif-3-moe (17B) Thinking</title>
    <updated>2025-09-07T20:04:49+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt; &lt;img alt="Aquif-3-moe (17B) Thinking" src="https://b.thumbs.redditmedia.com/PG2DZom31Ip8OdlIQP2-poMryul3rQ0LN3lLRyE7SAA.jpg" title="Aquif-3-moe (17B) Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A high-performance mixture-of-experts language model optimized for efficiency, coding, science, and general use. With 17B total parameters and 2.8B active parameters, aquif-3-moe delivers competitive performance across multiple domains while maintaining computational efficiency.&lt;/p&gt; &lt;p&gt;Is this true? A MOE 17B better than Gemini. I am testing it asap. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nb3b8l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbbgab</id>
    <title>~$15K Inference Workstation for a 250+ Gov Org</title>
    <updated>2025-09-08T02:02:10+00:00</updated>
    <author>
      <name>/u/reughdurgem</name>
      <uri>https://old.reddit.com/user/reughdurgem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I saw a post on here asking for an idea of an inference setup for a school and figured I'd also see what this community thinks of the setup I've been tasked with building. &lt;/p&gt; &lt;p&gt;For some context I work for a local county government clerk of about 250 employees and considering the information we deal with has lots of sensitivities we want to explore on-prem AI solutions for things like LLM chatbots for the public and VLMs for extracting structured JSON data from scanned images. &lt;/p&gt; &lt;p&gt;I have approximately $15K budgeted for hardware which essentially will be a dedicated AI server and/or workstation box that our employees would interact with via various tools over our network and it would directly integrate with some of our court management software. &lt;/p&gt; &lt;p&gt;I've been in the AI community since the OG DALL-E days and use models like GPT-OSS:20B and Qwen3 4B regularly via Ollama hooked into GitHub Copilot Chat in VSCode on my A5500 laptop for testing precision and accuracy when editing JavaScript files or light agentic tasks but I've never gotten into the distributed computing space. &lt;/p&gt; &lt;p&gt;From my research it seems like either VLLM or SGLang would be the optimal engines to run on a CLI Linux environment with hardware similar to the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: NVIDIA RTX 6000 PRO Blackwell 96GB (Server or Workstation Edition is better?)&lt;/li&gt; &lt;li&gt;CPU: AMD RYZEN Thread ripper Pro 7965WX (Overkill?)&lt;/li&gt; &lt;li&gt;MOBO: ASUS Pro WRX90E&lt;/li&gt; &lt;li&gt;SSD: 4TB NVME (brand agnostic)&lt;/li&gt; &lt;li&gt;RAM: 256GB ECC (8 sticks probably?)&lt;/li&gt; &lt;li&gt;Network: 10Gb NIC but probably 25Gb is preferred?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm curious what you all think of this approach since it seems like used 3090s is a more cost effective method to get lots of VRAM - however the gains from newer architectures seem to be worth it in terms of response tokens per second? I believe the A5500 is similarish to a 3080 and running GPT-OSS 20B on that and my 5070Ti at home the speed difference is noticable. Also I read that speed is better with one GPU versus multiple if all else is equal but idk if that's true in practice.&lt;/p&gt; &lt;p&gt;My current goal would be to run a vision model like Pixtral 12B which another county is using on dual L40Ss and just that model alone is using all 96GB of their VRAM - idk if that's just an insane context length because the model isn't &lt;em&gt;that&lt;/em&gt; huge on its own I don't believe. And if that is the case then something like GPT-OSS 120B for general text inference would be great too if it could all fit on the 6000 Pro. &lt;/p&gt; &lt;p&gt;I also read about offloading tasks like RAG and potentially smaller models (7b range) to the CPU and RAM to cut costs for &amp;quot;less essential&amp;quot; tasks so I'm considering that as well. Let me know your thoughts and any improvements I can make to the setup. &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reughdurgem"&gt; /u/reughdurgem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T02:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nau0qe</id>
    <title>Llama-OS - I'm developing an app to make llama.cpp usage easier.</title>
    <updated>2025-09-07T14:03:31+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt; &lt;img alt="Llama-OS - I'm developing an app to make llama.cpp usage easier." src="https://external-preview.redd.it/MzczZWhoc2h5cW5mMSpEG6AmlfNZCDZthrNu5xlRNijQvZUzUBXEn_GdpClu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6118cc263dd50d3564e274c8c88ea7d5357292bf" title="Llama-OS - I'm developing an app to make llama.cpp usage easier." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;This is an app I'm working on, the idea around is is that I use llama-server directly, so updating llama become seamless.&lt;/p&gt; &lt;p&gt;Actually it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model management&lt;/li&gt; &lt;li&gt;Hugging Face Integration&lt;/li&gt; &lt;li&gt;Llama.cpp GitHub integration with releases management&lt;/li&gt; &lt;li&gt;Llama-server terminal launching with easy arguments customization, Internal / External&lt;/li&gt; &lt;li&gt;Simple chat interface for easy testing&lt;/li&gt; &lt;li&gt;Hardware monitor&lt;/li&gt; &lt;li&gt;Color themes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qc7edhshyqnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T14:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8wys</id>
    <title>My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)</title>
    <updated>2025-09-08T00:00:22+00:00</updated>
    <author>
      <name>/u/incrediblediy</name>
      <uri>https://old.reddit.com/user/incrediblediy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt; &lt;img alt="My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)" src="https://b.thumbs.redditmedia.com/a7Ufbae-YySckctbCfshnCtZWyYRgOg4l50M5Kdrmkg.jpg" title="My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found a cheap HP DL380 G9 from a local eWaste place and decided to build an inference server. I will keep all equivalent prices in US$, including shipping, but I paid for everything in local currency (AUD). The fan speed is ~20% or less and quite silent for a server.&lt;/p&gt; &lt;p&gt;Parts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;HP DL380 G9 = $150 (came with dual Xeon 2650 v3 + 64GB RDIMM (I had to remove these), no HDD, both PCIe risers: this is important)&lt;/li&gt; &lt;li&gt;512 GB LRDIMM (8 sticks, 64GB each from an eWaste place), I got LRDIMM as they are cheaper than RDIMM for some reason = $300&lt;/li&gt; &lt;li&gt;My old RTX3060 (was a gift in 2022 or so)&lt;/li&gt; &lt;li&gt;AMD MI50 32GB from AliExpress = $235 including shipping + tax&lt;/li&gt; &lt;li&gt;GPU power cables from Amazon (2 * HP 10pin to EPS + 2 * EPS to PCIe)&lt;/li&gt; &lt;li&gt;NVMe to PCIe adapters * 2 from Amazon&lt;/li&gt; &lt;li&gt;SN5000 1TB ($55) + 512GB old Samsung card, which I had&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ovsl3hkl0unf1.jpg?width=1663&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9ff5c0c0f034a1e2f3ae60bc2a86f7a4a5bf4230"&gt;https://preview.redd.it/ovsl3hkl0unf1.jpg?width=1663&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9ff5c0c0f034a1e2f3ae60bc2a86f7a4a5bf4230&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ubuntu 24.04.3 LTS&lt;/li&gt; &lt;li&gt;NVIDIA 550 drivers were automatically installed with Ubuntu&lt;/li&gt; &lt;li&gt;AMD drivers + ROCm 6.4.3&lt;/li&gt; &lt;li&gt;Ollama (curl -fsSL &lt;a href="https://ollama.com/install.sh"&gt;https://ollama.com/install.sh&lt;/a&gt; | sh)&lt;/li&gt; &lt;li&gt;Drivers: &lt;ol&gt; &lt;li&gt;amdgpu-install -y --usecase=graphics,rocm,hiplibsdk&lt;/li&gt; &lt;li&gt;&lt;a href="https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html"&gt;https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ROCm (need to copy DFX906 files from ArchLinux AUR as below):&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/drivers_for_radeon_instinct_mi50_16gb/"&gt;https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/drivers_for_radeon_instinct_mi50_16gb/&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977"&gt;https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://archlinux.org/packages/extra/x86_64/rocblas/"&gt;https://archlinux.org/packages/extra/x86_64/rocblas/&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I noticed that Ollama automatically selects a GPU or a combination of targets, depending on the model size. Ex: if the model is smaller than 12GB, it selects RTX3060, if larger than that MI50 (I tested with Qwen different size models). For a very large model like DeepSeek R1:671B, it used both GPU + RAM automatically. It used n_ctx_per_seq (4096) by default; I haven't done extensive testing yet.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = true) load_tensors: offloading 3 repeating layers to GPU load_tensors: offloaded 3/62 layers to GPU load_tensors: ROCm0 model buffer size = 21320.01 MiB load_tensors: CPU_Mapped model buffer size = 364369.62 MiB time=2025-09-06T04:49:32.151+10:00 level=INFO source=server.go:1284 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; time=2025-09-06T04:49:32.405+10:00 level=INFO source=server.go:1284 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot; llama_context: constructing llama_context llama_context: n_seq_max = 1 llama_context: n_ctx = 4096 llama_context: n_ctx_per_seq = 4096 llama_context: n_batch = 512 llama_context: n_ubatch = 512 llama_context: causal_attn = 1 llama_context: flash_attn = 0 llama_context: kv_unified = false llama_context: freq_base = 10000.0 llama_context: freq_scale = 0.025 llama_context: n_ctx_per_seq (4096) &amp;lt; n_ctx_train (163840) -- the full capacity of the model will not be utilized llama_context: CPU output buffer size = 0.52 MiB llama_kv_cache_unified: ROCm0 KV buffer size = 960.00 MiB llama_kv_cache_unified: CPU KV buffer size = 18560.00 MiB llama_kv_cache_unified: size = 19520.00 MiB ( 4096 cells, 61 layers, 1/1 seqs), K (f16): 11712.00 MiB, V (f16): 7808.00 MiB llama_context: CUDA0 compute buffer size = 3126.00 MiB llama_context: ROCm0 compute buffer size = 1250.01 MiB llama_context: CUDA_Host compute buffer size = 152.01 MiB llama_context: graph nodes = 4845 llama_context: graph splits = 1092 (with bs=512), 3 (with bs=1) time=2025-09-06T04:49:51.514+10:00 level=INFO source=server.go:1288 msg=&amp;quot;llama runner started in 63.85 seconds&amp;quot; time=2025-09-06T04:49:51.514+10:00 level=INFO source=sched.go:473 msg=&amp;quot;loaded runners&amp;quot; count=1 time=2025-09-06T04:49:51.514+10:00 level=INFO source=server.go:1250 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; time=2025-09-06T04:49:51.515+10:00 level=INFO source=server.go:1288 msg=&amp;quot;llama runner started in 63.85 seconds&amp;quot; [GIN] 2025/09/06 - 04:49:51 | 200 | 1m5s | 127.0.0.1 | POST &amp;quot;/api/generate&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Memory usage:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gpu@gpu:~/ollama$ free -h total used free shared buff/cache available Mem: 503Gi 28Gi 65Gi 239Mi 413Gi 475Gi Swap: 4.7Gi 256Ki 4.7Gi gpu@gpu:~/ollama$ =========================================== ROCm System Management Interface =========================================== ===================================================== Concise Info ===================================================== Device Node IDs Temp Power Partitions SCLK MCLK Fan Perf PwrCap VRAM% GPU% (DID, GUID) (Edge) (Socket) (Mem, Compute, ID) ======================================================================================================================== 0 2 0x66a1, 5947 36.0°C 16.0W N/A, N/A, 0 925Mhz 350Mhz 14.51% auto 225.0W 75% 0% ======================================================================================================================== ================================================= End of ROCm SMI Log ================================================== Sat Sep 6 04:51:46 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.163.01 Driver Version: 550.163.01 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3060 Off | 00000000:84:00.0 Off | N/A | | 0% 36C P8 15W / 170W | 3244MiB / 12288MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | 0 N/A N/A 12196 G /usr/lib/xorg/Xorg 4MiB | | 0 N/A N/A 33770 C /usr/local/bin/ollama 3230MiB | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;DeepSeek R1:671B output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gpu@gpu:~/ollama$ ollama run deepseek-r1:671b &amp;gt;&amp;gt;&amp;gt; hello Thinking... Hmm, the user just said &amp;quot;hello&amp;quot;. That's a simple greeting but I should respond warmly to start off on a good note. I notice they didn't include any specific question or context - could be testing me out, might be shy about asking directly, or maybe just being polite before diving into something else. Their tone feels neutral from this single word. Since it's such an open-ended opener, I'll keep my reply friendly but leave room for them to steer the conversation wherever they want next. A smiley emoji would help make it feel welcoming without overdoing it. Important not to overwhelm them with options though - &amp;quot;how can I help&amp;quot; is better than listing possibilities since they clearly haven't decided what they need yet. The ball's in their court now. ...done thinking. Hello! 😊 How can I assist you today? &amp;gt;&amp;gt;&amp;gt; Send a message (/? for help) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incrediblediy"&gt; /u/incrediblediy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T00:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqln5</id>
    <title>How is qwen3 4b this good?</title>
    <updated>2025-09-07T11:18:38+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt; &lt;img alt="How is qwen3 4b this good?" src="https://b.thumbs.redditmedia.com/iayFtcVrbsCZAlrIPv-683BX53HPUAlfD1bIlFDeLGo.jpg" title="How is qwen3 4b this good?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is on a different level. The only models which can beat it are 6 to 8 times larger. I am very impressed. It even Beats all models in the &amp;quot;small&amp;quot; range in Maths (AIME 2025).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1naqln5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbg2zk</id>
    <title>MiniPC options are escalating, which one would you get?</title>
    <updated>2025-09-08T06:12:50+00:00</updated>
    <author>
      <name>/u/SmokingHensADAN</name>
      <uri>https://old.reddit.com/user/SmokingHensADAN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was going to buy a framework desktop but each day a new one is popping up, released or teased. I think there are around 25 AI 395hx versions already. FEVM has some interesting ones too, just wanted to see what you guys thought. They got one with an ai chip for $500 barebone that they say it, &amp;quot;connects a 3090 via oculink directly to cpu so your not losing that much latency&amp;quot;&lt;/p&gt; &lt;p&gt;Dell has a SFF 45% off, that you can max out a cpu and 4000ada for like $2300, It was gen 4 mobo though so not interested but you could part it out for prob $3k.&lt;/p&gt; &lt;p&gt; The MS-S1 beast workstation is where it's at, though,. With a PCIE 16 slot or discrete GPU option, clustering and 320watt, etc &lt;a href="https://www.techradar.com/pro/this-mini-pc-is-the-first-computer-ever-to-have-a-revolutionary-new-tech-that-allows-usb-to-finally-match-thunderbolt-minisforum-ms-s1-max-has-usb-4-0-v2-ports"&gt;https://www.techradar.com/pro/this-mini-pc-is-the-first-computer-ever-to-have-a-revolutionary-new-tech-that-allows-usb-to-finally-match-thunderbolt-minisforum-ms-s1-max-has-usb-4-0-v2-ports&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Geekom also has a preorder that uses the pro version of the chip&lt;/p&gt; &lt;p&gt;GEEKOM A9 Mega-The Most Powerful Mini PC on Earth, via &lt;a href="/u/Kickstarter"&gt;u/Kickstarter&lt;/a&gt; &lt;a href="https://www.kickstarter.com/projects/1906688106/geekom-a9-mega-the-most-powerful-mini-pc-on-earth"&gt;https://www.kickstarter.com/projects/1906688106/geekom-a9-mega-the-most-powerful-mini-pc-on-earth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The FEVM FA65G mini PC comes with a choice of high-end, MXM-form-factor graphics processing units (GPUs). The manufacturer, FEVM, has shown models equipped with both the NVIDIA GeForce RTX 4080 LP and the professional NVIDIA RTX 5000 Ada. Key features of the GPU options include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RTX 4080 LP (Laptop):&lt;/strong&gt; This version of the GPU is limited to a power usage of 115 W. According to FEVM's internal testing, its performance is comparable to or slightly faster than a desktop RTX 3080 or RTX 4070.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RTX 5000 Ada (Mobile):&lt;/strong&gt; For even higher performance, some FA65G builds feature the powerful RTX 5000 Ada mobile graphics card. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both GPU options are rare, high-performance units for a mini PC, allowing the FA65G to deliver desktop-class graphics power in a compact chassis. &lt;br /&gt; That one is interesting, I have 2x64gb ddr5 128gb crucial sodimm and 2x2tb 1x4tb WD black 2280 nvme SN850X sitting on my desk. I need to find it a home.&lt;/p&gt; &lt;p&gt;This is old benchmarks and there are already much better minipc since this was wrote 6 months ago. Any suggestions which way to go&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.hardware-corner.net/guides/mini-pc-with-oculink/"&gt;https://www.hardware-corner.net/guides/mini-pc-with-oculink/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmokingHensADAN"&gt; /u/SmokingHensADAN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbg2zk/minipc_options_are_escalating_which_one_would_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbg2zk/minipc_options_are_escalating_which_one_would_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbg2zk/minipc_options_are_escalating_which_one_would_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb4lka</id>
    <title>Inference for 24 people with a 5000€ budget</title>
    <updated>2025-09-07T20:55:52+00:00</updated>
    <author>
      <name>/u/HyperHyper15</name>
      <uri>https://old.reddit.com/user/HyperHyper15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a teacher at an informatics school (16 years and above) and we want to build a inference server to run small llm's for our lessons. Mainly we want to teach how prompting works, mcp servers, rag pipelines and how to create system prompts.&lt;br /&gt; I know the budget is not a lot for something like this, but is it reasonable to host something like Qwen3-Coder-30B-A3B-Instruct with an okayish speed?&lt;br /&gt; I thougt about getting an 5090 and maybe add an extra gpu in a year or two (when we have a new budget).&lt;br /&gt; But what CPU/Mainboard/Ram should we buy?&lt;br /&gt; Has someone built a system in a simmilar environment and give me some thoughts what worked good / bad?&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Local is not a strict requirement, but since we have 4 classes with each 24 people, cloud services could get expensive quickly. Another &amp;quot;Painpoint&amp;quot; of cloud is, that students have a budget on their api key. But what if an oopsie happens and the burn through their budget? &lt;/p&gt; &lt;p&gt;On used hardware: I have to look what regulatories apply here. What i know is that we need an invoice when we buy something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HyperHyper15"&gt; /u/HyperHyper15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbmhi4</id>
    <title>Episodic Memory Bank and local voice to voice using Cline.</title>
    <updated>2025-09-08T12:33:06+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"&gt; &lt;img alt="Episodic Memory Bank and local voice to voice using Cline." src="https://external-preview.redd.it/YXpvdGx0OGFxeG5mMamjuXx1p5ZbwAaKmxghv-BQMSuAGY3X-aSJMyvQWEDF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e89a36a762194aef2c481b8bea340ca919158822" title="Episodic Memory Bank and local voice to voice using Cline." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a new memory bank framework called the episodic memory bank. Here I demo that in action and show off the new kokoro and Apple Intelligence powered voice to voice in Cline.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ymgdjt8aqxnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqa34</id>
    <title>🚀 What model should we build next? YOU DECIDE! 🚀</title>
    <updated>2025-09-08T15:08:12+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA!&lt;/p&gt; &lt;p&gt;After the amazing support we received in our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;last post&lt;/a&gt; with Art-0-8B, we're ready to tackle our next project and want YOU to decide what it should be! (Art-1 8B and 20B versions are coming soon btw)&lt;/p&gt; &lt;p&gt;For those who missed it, we're AGI-0 Labs - a decentralized research lab building open-source AGI through democratic community input. Our mission is simple: create AI that belongs to everyone, developed openly and guided by the community. Check us out at &lt;a href="http://AGI-0.com"&gt;AGI-0.com&lt;/a&gt; if you want to learn more about our approach.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's how this works:&lt;/strong&gt; The most upvoted comment below describing a model idea will be our next development target. Whether it's a specialized fine-tune, a novel architecture experiment, or something completely wild - if the community wants it, we'll build it.&lt;/p&gt; &lt;p&gt;We're also open to collaborating with any sponsors who'd like to help us get more compute resources - feel free to reach out if you're interested in supporting open-source AI development!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Drop your model ideas below and let's see what the community wants most! The highest upvoted suggestion gets built. 🗳️&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Looking forward to seeing what creative ideas you all come up with!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbllnt</id>
    <title>Folks who are fine-tuning SLMs, where do you acquire datasets?</title>
    <updated>2025-09-08T11:51:11+00:00</updated>
    <author>
      <name>/u/CrescendollsFan</name>
      <uri>https://old.reddit.com/user/CrescendollsFan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed a lot of folks interested in unsloth and fine-tuning and with a few of the colab notebooks pulling in a genetic dataset. I am just curious if anyone is replicating this approach outside of a demo / how to - where people acquire or curate datasets and then fine-tune&lt;/p&gt; &lt;p&gt;For example deepseeks distillation method was from pulling data from OpenAI models , and I heard phi4 had &lt;a href="https://arxiv.org/html/2412.08905v1"&gt;synthetics&lt;/a&gt; as a bulk of the training data . Are many people training SLMs in the same way, and where do you get or curate your own specialised data - or you find over-fitting is too much of a problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CrescendollsFan"&gt; /u/CrescendollsFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbq3y7</id>
    <title>New research preprint: Evolving Transformers with NEMoE</title>
    <updated>2025-09-08T15:01:47+00:00</updated>
    <author>
      <name>/u/Desperate_Contact102</name>
      <uri>https://old.reddit.com/user/Desperate_Contact102</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I just uploaded a new research preprint called NEMoE (Neuro-Evolutionary Mixture of Experts Transformer).&lt;/p&gt; &lt;p&gt;Instead of using a standard Transformer with fixed experts, NEMoE applies ideas from evolutionary algorithms (mutation, crossover, selection) to improve how experts are chosen and combined.&lt;/p&gt; &lt;p&gt;🔹 Early results show:&lt;/p&gt; &lt;p&gt;Lower perplexity (better language modeling performance)&lt;/p&gt; &lt;p&gt;More stable training compared to Switch Transformer&lt;/p&gt; &lt;p&gt;Better use of experts without adding compute cost&lt;/p&gt; &lt;p&gt;Here’s the preprint (open access on Zenodo): 👉 &lt;a href="https://doi.org/10.5281/zenodo.17073715"&gt;https://doi.org/10.5281/zenodo.17073715&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Contact102"&gt; /u/Desperate_Contact102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:01:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqb3o</id>
    <title>Will you use this new Qwen3-ASR?</title>
    <updated>2025-09-08T15:09:21+00:00</updated>
    <author>
      <name>/u/LuozhuZhang</name>
      <uri>https://old.reddit.com/user/LuozhuZhang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"&gt; &lt;img alt="Will you use this new Qwen3-ASR?" src="https://preview.redd.it/1ytcylkfiynf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c83369e0e599a5317d04c7536881ab848342f5d" title="Will you use this new Qwen3-ASR?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Supporting 11 languages &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LuozhuZhang"&gt; /u/LuozhuZhang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ytcylkfiynf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxl6a</id>
    <title>NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp; Priced At $13,200 Per Piece</title>
    <updated>2025-09-07T16:24:13+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt; &lt;img alt="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" src="https://external-preview.redd.it/0E4hPJjWUWQzlid17SPMiSUkbhbtEQRV_SbOMgs-kTI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e26ecaa238d5f9ab48615dfc56baa31609cbaeaa" title="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-geforce-rtx-5090-128-gb-memory-gpu-for-ai-price-13200-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbpmcn</id>
    <title>UIGEN-FX-4B-Preview is your Frontend Engineer across frameworks</title>
    <updated>2025-09-08T14:43:45+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbpmcn/uigenfx4bpreview_is_your_frontend_engineer_across/"&gt; &lt;img alt="UIGEN-FX-4B-Preview is your Frontend Engineer across frameworks" src="https://b.thumbs.redditmedia.com/X609xqcZmHm-OaKsFOItR0OTU1gO7SoIJIjZWlytKBE.jpg" title="UIGEN-FX-4B-Preview is your Frontend Engineer across frameworks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-FX-4B-Preview"&gt;UIGEN-FX-4B-Preview Link&lt;/a&gt; UIGEN-FX series is a Frontend engineer that is a drop in replacement to your coding models but better at frontend tasks. As a 4B model, its still a work in progress (hence, Preview!) We're looking for additional finetuners or developers who may be interested in helping out with this project - Send me a PM!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nbpmcn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbpmcn/uigenfx4bpreview_is_your_frontend_engineer_across/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbpmcn/uigenfx4bpreview_is_your_frontend_engineer_across/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T14:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbq1n0</id>
    <title>Switched to LobeChat from OpenWebUI because of crappy web search and no reasoning level support: a review</title>
    <updated>2025-09-08T14:59:32+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For people who use OpenWebUI, I could not get web search working properly. Also, it's been weeks since OpenAI Harmony was released, and it still doesn't support configuring reasoning level for gpt-oss. &lt;/p&gt; &lt;p&gt;I gave up on OpenWebUI being useful, and switched to LobeChat. One &lt;code&gt;docker compose up -d&lt;/code&gt; later, it's running on my server.&lt;/p&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Native web search &lt;a href="https://i.imgur.com/7mmS43O.png"&gt;actually works&lt;/a&gt;! It calls gpt-5 api correctly using the &lt;a href="https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses"&gt;web_search tool&lt;/a&gt; built-in search engine. It's a lot faster than running searxng on my local machine, doesn't seem to run into cloudflare issues, and &lt;a href="https://i.imgur.com/zoakfFJ.png"&gt;gives high quality results quickly&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You can &lt;a href="https://i.imgur.com/4dpL9jp.png"&gt;set gpt-oss/gpt-5 reasoning effort&lt;/a&gt;! &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The icons are really really ugly. Not my style at all. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's written by a team in china, and it shows sometimes in the translations. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Setting up the server with a custom domain/SSL/custom ports is a bit harder. The default config assumes that you have port 8000, 9000, 9001, and 5432 (postgres port) available. You need to tweak the config a bit if you don't want to use those ports if they are already in use. You can change some of the ports in the &lt;code&gt;.env&lt;/code&gt; file, but the port 9001 and 5432 are hardcoded so you need to change that. This is not a big deal though, just takes a few mins to configure.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, I rate it 4/5 stars. Would be a higher score if we could change the icons.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq1n0/switched_to_lobechat_from_openwebui_because_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq1n0/switched_to_lobechat_from_openwebui_because_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq1n0/switched_to_lobechat_from_openwebui_because_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T14:59:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbmz92</id>
    <title>NotebookLM is amazing - how can I replicate it locally and keep data private?</title>
    <updated>2025-09-08T12:55:27+00:00</updated>
    <author>
      <name>/u/Hot-Independence-197</name>
      <uri>https://old.reddit.com/user/Hot-Independence-197</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like how &lt;strong&gt;NotebookLM&lt;/strong&gt; works - I just upload a file, ask any question, and it provides high-quality answers. How could one build a similar system locally? Would this be considered a RAG (Retrieval-Augmented Generation) pipeline, or something else? Could you recommend good &lt;strong&gt;open-source&lt;/strong&gt; versions that can be run locally, while keeping data secure and private?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Independence-197"&gt; /u/Hot-Independence-197 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo0bz</id>
    <title>KittenML released a mini version (80M) of their text to speech model.</title>
    <updated>2025-09-08T13:39:19+00:00</updated>
    <author>
      <name>/u/Yorn2</name>
      <uri>https://old.reddit.com/user/Yorn2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt; &lt;img alt="KittenML released a mini version (80M) of their text to speech model." src="https://external-preview.redd.it/6tEU3HFyV9wrAIlbgWYDqTicViQ2PFk-H0trsfrB-TE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae1cdc19684b4f8a1d7922e2097495effc92e03" title="KittenML released a mini version (80M) of their text to speech model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yorn2"&gt; /u/Yorn2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqa1p</id>
    <title>Qwen released API (only) Qwen3-ASR — the all-in-one speech recognition model!</title>
    <updated>2025-09-08T15:08:10+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt; &lt;img alt="Qwen released API (only) Qwen3-ASR — the all-in-one speech recognition model!" src="https://preview.redd.it/et1syg58iynf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ab48cb0e9692e8d94764a7f031fd34d0db1ae95" title="Qwen released API (only) Qwen3-ASR — the all-in-one speech recognition model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🎙️ Meet Qwen3-ASR — the all-in-one speech recognition model!&lt;/p&gt; &lt;p&gt;✅ High-accuracy EN/CN + 9 more languages: ar, de, en, es, fr, it, ja, ko, pt, ru, zh&lt;/p&gt; &lt;p&gt;✅ Auto language detection&lt;/p&gt; &lt;p&gt;✅ Songs? Raps? Voice with BGM? No problem. &amp;lt;8% WER&lt;/p&gt; &lt;p&gt;✅ Works in noise, low quality, far-field&lt;/p&gt; &lt;p&gt;✅ Custom context? Just paste ANY text — names, jargon, even gibberish 🧠&lt;/p&gt; &lt;p&gt;✅ One model. Zero hassle.Great for edtech, media, customer service &amp;amp; more.&lt;/p&gt; &lt;p&gt;API: &lt;a href="https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031"&gt;https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Modelscope Demo: &lt;a href="https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo"&gt;https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/et1syg58iynf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbly7o</id>
    <title>MiniCPM4.1-8B</title>
    <updated>2025-09-08T12:08:09+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: &lt;a href="https://huggingface.co/openbmb/MiniCPM4.1-8B"&gt;https://huggingface.co/openbmb/MiniCPM4.1-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8B hybrid reasoning model (/think vs /no_think)&lt;/li&gt; &lt;li&gt;InfLLM v2 sparse attention, natively supports 65K, RoPE scaling validated to 131K&lt;/li&gt; &lt;li&gt;BitCPM ternary quantization, FP8 and multi-token prediction&lt;/li&gt; &lt;li&gt;Eagle3 speculative decoding integrated in vLLM, SGLang, and CPM .cu with up to 3x faster reasoning&lt;/li&gt; &lt;li&gt;On Jetson Orin achieves approximately 7x faster decoding compared to Qwen3-8B and 3x reasoning speedup over MiniCPM4&lt;/li&gt; &lt;li&gt;Available in GPTQ, AutoAWQ, Marlin, GGUF, MLX, and Eagle3 draft variants&lt;/li&gt; &lt;li&gt;Apache 2.0&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbgosx</id>
    <title>Apocalyptic scenario: If you could download only one LLM before the internet goes down, which one would it be?</title>
    <updated>2025-09-08T06:50:43+00:00</updated>
    <author>
      <name>/u/sado361</name>
      <uri>https://old.reddit.com/user/sado361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, a thought crossed my mind and I've been thinking about it for a few days. Let's say we have an apocalyptic scenario, like a zombie apocalypse. You have a Mac Studio with an M3 chip and 512 GB of RAM (it uses little power and can run large models). If such an apocalypse happened today, which local LLM would you download before the internet disappears? You only have a chance to download one. Electricity is not a problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sado361"&gt; /u/sado361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbfy60</id>
    <title>Finishing touches on dual RTX 6000 build</title>
    <updated>2025-09-08T06:04:43+00:00</updated>
    <author>
      <name>/u/ikkiyikki</name>
      <uri>https://old.reddit.com/user/ikkiyikki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt; &lt;img alt="Finishing touches on dual RTX 6000 build" src="https://preview.redd.it/sez83piasvnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599c86ee050439c49f07c7056ff67e6c48ef8381" title="Finishing touches on dual RTX 6000 build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a dream build: 192 gigs of fast VRAM (and another 128 of RAM) but worried I'll burn the house down because of the 15A breakers.&lt;/p&gt; &lt;p&gt;Downloading Qwen 235B q4 :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikkiyikki"&gt; /u/ikkiyikki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sez83piasvnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbi95c</id>
    <title>Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages</title>
    <updated>2025-09-08T08:32:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt; &lt;img alt="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" src="https://external-preview.redd.it/aoPAPmODv59RqOF8q1zUghKheD5cO88KxVLhosHPVZE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d56ba07b04f6a26463fb99f2d29054bf135f506a" title="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TildeOpen LLM is an open-source foundational language model built to serve underrepresented Nordic and Eastern European languages. Developed with European Commission funding and trained on the LUMI supercomputer, this 30B+ parameter model addresses the performance gaps that speakers of 19 focus languages—representing over 165 million people—face with existing AI systems.&lt;/p&gt; &lt;p&gt;The model employs an equitable tokeniser and curriculum-learning approach to ensure fair representation across less-resourced languages, moving beyond the typical English-centric design of most language models. As an open-source project, TildeOpen LLM enables transparent research and community-driven development while maintaining European technological independence.&lt;/p&gt; &lt;p&gt;This foundational model is not yet adapted to follow instructions or aligned with safety features. The next version being built on top of this model will be a specialised translation model, leveraging TildeOpen LLM's multilingual foundation to provide high-quality translation capabilities across the supported European language pairs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt; Albanian, Bosnian, Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Hungarian, Icelandic, Irish, Italian, Latgalian, Latvian, Lithuanian, Macedonian, Maltese, Montenegrin, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovene, Spanish, Swedish, Turkish, Ukrainian as well of mathematical proofs, programming code and XML documents containing translation data&lt;/p&gt; &lt;p&gt;GGUF:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/TildeOpen-30b-GGUF"&gt;https://huggingface.co/mradermacher/TildeOpen-30b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TildeAI/TildeOpen-30b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T08:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbkxnm</id>
    <title>Introducing IndexTTS-2.0: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</title>
    <updated>2025-09-08T11:16:22+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce the official open-sourcing of IndexTTS-2.0 - an emotionally rich and duration-controllable autoregressive zero-shot text-to-speech system. &lt;/p&gt; &lt;p&gt;- We innovatively propose a &amp;quot;time encoding&amp;quot; mechanism applicable to autoregressive systems, solving for the first time the challenge of precise speech duration control in traditional autoregressive models. &lt;/p&gt; &lt;p&gt;- The system also introduces a timbre-emotion decoupling modeling mechanism, offering diverse and flexible emotional control methods. Beyond single-audio reference, it enables precise adjustment of synthesized speech's emotional expression through standalone emotional reference audio, emotion vectors, or text descriptions, significantly enhancing the expressiveness and adaptability of generated speech. &lt;/p&gt; &lt;p&gt;The architecture of IndexTTS-2.0 makes it widely suitable for various creative and application scenarios, including but not limited to: AI voiceovers, audiobooks, dynamic comics, video translation, voice dialogues, podcasts, and more. We believe this system marks a crucial milestone in advancing zero-shot TTS technology toward practical applications. &lt;/p&gt; &lt;p&gt;Currently, the project paper, full code, model weights, and online demo page are all open-sourced. We warmly invite developers, researchers, and content creators to explore and provide valuable feedback. In the future, we will continue optimizing model performance and gradually release more resources and tools, looking forward to collaborating with the developer community to build an open and thriving technology ecosystem. &lt;/p&gt; &lt;p&gt;👉 Repository: &lt;a href="https://github.com/index-tts/index-tts"&gt;https://github.com/index-tts/index-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;👉 Paper: &lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt; &lt;/p&gt; &lt;p&gt;👉 Demo: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbr45v</id>
    <title>Poor man’s FlashAttention: Llama.cpp-gfx906 fork!</title>
    <updated>2025-09-08T15:39:53+00:00</updated>
    <author>
      <name>/u/CornerLimits</name>
      <uri>https://old.reddit.com/user/CornerLimits</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt; &lt;img alt="Poor man’s FlashAttention: Llama.cpp-gfx906 fork!" src="https://external-preview.redd.it/PecTYmNSbm5tb-T9OW67-xyMoNn-SzofgAKif5I3sUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e84ff63d5e5bbf73f86b2641f6f74955f0a20dbe" title="Poor man’s FlashAttention: Llama.cpp-gfx906 fork!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just released a fork of llama.cpp that implements some strong optimizations for the MI50/MI60/Vega7 series. &lt;/p&gt; &lt;p&gt;Thanks to the outstanding work of open source community I made a final effort to actually make flash attention FASTER than no flash attention in almost every case. Yeah… almost.&lt;/p&gt; &lt;p&gt;The goal is to run ~30B models with ~30K ctx on a single card at decent speed.&lt;/p&gt; &lt;p&gt;You can find benchmarks, compile/launch/bench scripts, references to the original works and explanations of my new kernel in the repo.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CornerLimits"&gt; /u/CornerLimits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo33p</id>
    <title>UAE Preparing to Launch K2 Think, "the world’s most advanced open-source reasoning model"</title>
    <updated>2025-09-08T13:42:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt; &lt;img alt="UAE Preparing to Launch K2 Think, &amp;quot;the world’s most advanced open-source reasoning model&amp;quot;" src="https://external-preview.redd.it/3A4olwwXC7kAmitvVkfkfzLywUYc6IvJ9He-QlxgRLY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2afac7f2d1366e35e6945533e06a6756d060e202" title="UAE Preparing to Launch K2 Think, &amp;quot;the world’s most advanced open-source reasoning model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;In the coming week, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and G42 will release K2 Think, the world’s most advanced open-source reasoning model. &lt;strong&gt;Designed to be leaner and smarter, K2 Think delivers frontier-class performance in a remarkably compact form&lt;/strong&gt; – often matching, or even surpassing, the results of models an order of magnitude larger. The result: greater efficiency, more flexibility, and broader real-world applicability.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wam.ae/en/article/bll7llv-recognition-sheikh-khalifa%E2%80%99s-contribution"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
