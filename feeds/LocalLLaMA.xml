<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-12T04:36:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o4fjfx</id>
    <title>Â¿What open-source models that run locally are the most commonly used?</title>
    <updated>2025-10-12T04:10:09+00:00</updated>
    <author>
      <name>/u/Mister_X-16</name>
      <uri>https://old.reddit.com/user/Mister_X-16</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I'm about to start exploring the world of local Al, and I'd love to know which models you use. I just want to get an idea of what's popular or worth trying - any category is fine!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mister_X-16"&gt; /u/Mister_X-16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fjfx/what_opensource_models_that_run_locally_are_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fjfx/what_opensource_models_that_run_locally_are_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fjfx/what_opensource_models_that_run_locally_are_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T04:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4fkin</id>
    <title>LLM advice</title>
    <updated>2025-10-12T04:11:48+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To start things out i'll give you my hardware first:&lt;/p&gt; &lt;p&gt;AMD Ryzen 5 4500&lt;br /&gt; 32GB DDR4&lt;br /&gt; 1TB NVMe SSD&lt;br /&gt; RX 7900 XT 20GB&lt;br /&gt; RX 6800 16 GB&lt;br /&gt; OS Ubuntu 22.04 LTS&lt;br /&gt; Ollama&lt;br /&gt; LM Studio&lt;br /&gt; just downloaded llama.cpp not set up yet&lt;/p&gt; &lt;p&gt;What models could I conceivably run on this hardware? I'm fine with slower t/s so long as i'm not waiting 5 minutes till first token. I'm a lslow reader anyway lol&lt;/p&gt; &lt;p&gt;Any help would be greatly appreciated. I&amp;quot;m currently using GPT-OSS-20B and while it's useful for talking to, it seems as if it's not the best for the small coding tasks I give it. And I'm very interested in something bigger. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fkin/llm_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fkin/llm_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fkin/llm_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T04:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4fxer</id>
    <title>PSA: Ollama no longer supports the Mi50 or Mi60</title>
    <updated>2025-10-12T04:32:11+00:00</updated>
    <author>
      <name>/u/TechEnthusiastx86</name>
      <uri>https://old.reddit.com/user/TechEnthusiastx86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ollama/ollama/pull/12481"&gt;https://github.com/ollama/ollama/pull/12481&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama recently upgraded its ROCM version and therefore no longer supports the Mi50 or Mi60.&lt;/p&gt; &lt;p&gt;Their most recent release notes states that &amp;quot;AMD gfx900 and gfx906 (MI50, MI60, etc) GPUs are no longer supported via ROCm. We're working to support these GPUs via Vulkan in a future release.&amp;quot; &lt;/p&gt; &lt;p&gt;This means if you pull the latest version of Ollama you won't be able to use the Mi50 even though Ollama docs still list it as being supported.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechEnthusiastx86"&gt; /u/TechEnthusiastx86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T04:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o43hyn</id>
    <title>Recommendation for a local Japanese -&gt; English vision model</title>
    <updated>2025-10-11T18:49:43+00:00</updated>
    <author>
      <name>/u/Confident-Willow5457</name>
      <uri>https://old.reddit.com/user/Confident-Willow5457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per the title I'm looking for a multimodal model that can perform competent JP to ENG translations from images. Ideally it'd fit in 48 gb of VRAM but I'm not opposed to doing a bit of CPU offloading for significantly higher quality translation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident-Willow5457"&gt; /u/Confident-Willow5457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43hyn/recommendation_for_a_local_japanese_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43hyn/recommendation_for_a_local_japanese_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o43hyn/recommendation_for_a_local_japanese_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3vnt3</id>
    <title>Tooling+Model recommendations for base (16G) mac Mini M4 as remote server?</title>
    <updated>2025-10-11T13:28:29+00:00</updated>
    <author>
      <name>/u/Valuable-Question706</name>
      <uri>https://old.reddit.com/user/Valuable-Question706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Intel laptop as my main coding machine. Recently got myself a base model mac Mini and got surprised how fast it is for inference.&lt;/p&gt; &lt;p&gt;I'm still very new at using AI for coding. Not trying to be lazy, but want to get an advice in a large and quickly developing field from knowledgeable people.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;What I already tried: &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; in VS studio + ollama with qwen2.5-coder:7B. It works, but is there a better, more efficient way? I'm quite technical so I won't mind running more complex software stack if it brings significant improvements.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I'd like to automate some routine, boring programming tasks, for example: writing boilerplate html/js, writing bash scripts (yes, I very carefully check them before running), writing basic, boring python code. Nothing too complex, because I still prefer using my brain for actual work, plus even paid edge models are still not good at my area.&lt;/p&gt; &lt;p&gt;So I need a model that is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;is good at tasks specified above (should I use a specially optimized model or generic ones are OK?)&lt;/li&gt; &lt;li&gt;outputs at least 15+ tokens/sec&lt;/li&gt; &lt;li&gt;would integrate nicely with tooling on my work machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, what does a proper, modern VS code setup looks nowadays? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Question706"&gt; /u/Valuable-Question706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T13:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3zz30</id>
    <title>Poor GPU Club : Anyone use Q3/Q2 quants of 20-40B Dense models? How's it?</title>
    <updated>2025-10-11T16:27:46+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FYI My System Info: &lt;sup&gt;Intel(R&lt;/sup&gt; Core(TM) i7-14700HX 2.10 GHz |) &lt;strong&gt;&lt;sup&gt;32 GB RAM&lt;/sup&gt;&lt;/strong&gt; &lt;sup&gt;| 64-bit OS, x64-based processor | NVIDIA GeForce RTX 4060 Laptop GPU (&lt;/sup&gt;&lt;strong&gt;&lt;sup&gt;8GB VRAM&lt;/sup&gt;&lt;/strong&gt; |) &lt;strong&gt;&lt;sup&gt;Cores - 20&lt;/sup&gt;&lt;/strong&gt; &lt;sup&gt;|&lt;/sup&gt; &lt;strong&gt;&lt;sup&gt;Logical Processors - 28&lt;/sup&gt;&lt;/strong&gt;&lt;sup&gt;.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;Unfortunately I can't use Q4 or above quants of 20-40B Dense models, it'll be slower with single digit t/s.&lt;/p&gt; &lt;p&gt;How is Q3/Q2 quants of 20-40B Dense models? Talking about Perplexity, KL divergence, etc., metrics. Are they worthy enough to use? Wish there's a portal for such metrics for all models &amp;amp; with all quants.&lt;/p&gt; &lt;p&gt;List of models I want to use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Magistral-Small-2509 ( &lt;strong&gt;IQ3_XXS&lt;/strong&gt; - 9.41GB | Q3_K_S - 10.4GB | Q3_K_M - 11.5GB )&lt;/li&gt; &lt;li&gt;Devstral-Small-2507 ( &lt;strong&gt;IQ3_XXS&lt;/strong&gt; - 9.41GB | Q3_K_S - 10.4GB | Q3_K_M - 11.5GB )&lt;/li&gt; &lt;li&gt;reka-flash-3.1 ( &lt;strong&gt;IQ3_XXS&lt;/strong&gt; - 9.2GB )&lt;/li&gt; &lt;li&gt;Seed-OSS-36B-Instruct ( IQ3_XXS - 14.3GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 10.2GB )&lt;/li&gt; &lt;li&gt;GLM-4-32B-0414 ( IQ3_XXS - 13GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.26GB )&lt;/li&gt; &lt;li&gt;Gemma-3-27B-it ( IQ3_XXS - 10.8GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 7.85GB )&lt;/li&gt; &lt;li&gt;Qwen3-32B ( IQ3_XXS - 13GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.3GB )&lt;/li&gt; &lt;li&gt;KAT-V1-40B ( &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 11.1GB )&lt;/li&gt; &lt;li&gt;KAT-Dev ( IQ3_XXS - 12.8GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.1GB )&lt;/li&gt; &lt;li&gt;EXAONE-4.0.1-32B ( IQ3_XXS - 12.5GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 8.7GB )&lt;/li&gt; &lt;li&gt;Falcon-H1-34B-Instruct ( IQ3_XXS - 13.5GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.8GB )&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share your thoughts. Thanks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;BTW I'm able to run ~30B MOE models &amp;amp; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;posted a thread recently&lt;/a&gt;. Here my above list contains some models without MOE or small size choices. It seems I can skip Gemma &amp;amp; Qwen from the list since we have low size models from them. But for other few models, I don't have choice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T16:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o42jx1</id>
    <title>Optimize my environment for GLM 4.5 Air</title>
    <updated>2025-10-11T18:11:28+00:00</updated>
    <author>
      <name>/u/Former-Tangerine-723</name>
      <uri>https://old.reddit.com/user/Former-Tangerine-723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there people. For the last month I am using GLM air (4 K S quant) and I really like it! It's super smart and always to the point! I only have one problem, the t/s are really low (6-7 tk/s) So im looking for a way to upgrade my local rig, that's why I call you, the smart people! âºï¸ My current setup is AMD 7600 cpu, 64 gb ddr5 6000, and two cpus, 1 5060ti 16gb and 1 4060ti 16gb. My backend is LM Studio. So, should I change backend? Should I get a third GPU? What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Former-Tangerine-723"&gt; /u/Former-Tangerine-723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42jx1/optimize_my_environment_for_glm_45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42jx1/optimize_my_environment_for_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o42jx1/optimize_my_environment_for_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:11:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3l5zs</id>
    <title>GLM just blow up, or have I been in the dark?</title>
    <updated>2025-10-11T03:24:01+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like this community is ever moving, did GLM just blow up? like, I did not realise so many people talked about it.... What kinda system are you guys on 4.6 running? Because it looks like I would essential need 4x48gb Quadro 8000s/a6000s/6000 ada GPUs or at least 2x96gb RTX Pro 6000s... I may can afford 4 quadros but not 2 rtx pro 6000s, for the price of a car. lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T03:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4da60</id>
    <title>Appreciate advice on labeling sound files</title>
    <updated>2025-10-12T02:09:35+00:00</updated>
    <author>
      <name>/u/seoulsrvr</name>
      <uri>https://old.reddit.com/user/seoulsrvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâd like to automate the process of labeling a large catalog of music files - bpm, chords, etc. What tools work best for this?&lt;br /&gt; Thanks in advance for any suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seoulsrvr"&gt; /u/seoulsrvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4da60/appreciate_advice_on_labeling_sound_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4da60/appreciate_advice_on_labeling_sound_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4da60/appreciate_advice_on_labeling_sound_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T02:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3zgy7</id>
    <title>The LLM running on my local PC is too slow.</title>
    <updated>2025-10-11T16:07:43+00:00</updated>
    <author>
      <name>/u/Glanble</name>
      <uri>https://old.reddit.com/user/Glanble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm getting really slow speeds and need a sanity check.&lt;br /&gt; I'm only getting 1.0 t/s running a C4AI 111B model (63GB Q4_GGUF) on an RTX 5090 with 128GB of RAM.&lt;br /&gt; this normal, or is something wrong with my config?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glanble"&gt; /u/Glanble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T16:07:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o394p3</id>
    <title>Here we go again</title>
    <updated>2025-10-10T18:36:34+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt; &lt;img alt="Here we go again" src="https://preview.redd.it/b2abfaikwbuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db3949cb0def07809e7a9ba9a730d1582083844" title="Here we go again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2abfaikwbuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T18:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4dwj6</id>
    <title>Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp</title>
    <updated>2025-10-12T02:41:51+00:00</updated>
    <author>
      <name>/u/rtsov</name>
      <uri>https://old.reddit.com/user/rtsov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dwj6/optimized_docker_image_for_unsloth_finetuning/"&gt; &lt;img alt="Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp" src="https://external-preview.redd.it/iRMYKPaKkEEDms46u7jhdX3Sb7NwqrocaVgHjkYHszM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87f388a35515f2a499f727fef68a4423768197b0" title="Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;ð³ unsloth-docker&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This Docker image seamlessly integrates &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; â the ultra-fast LLM fine-tuning library â with &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; to enable end-to-end training and quantized GGUF model export in a single, GPU-accelerated environment.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;â¨ Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pre-installed Unsloth&lt;/strong&gt; with FlashAttention, xformers, and custom CUDA kernels for blazing-fast training&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full llama.cpp toolchain&lt;/strong&gt;, including &lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt; for easy GGUF conversion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; pre-configured for interactive development&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU-accelerated&lt;/strong&gt; (CUDA 12.1 + cuDNN)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization-ready&lt;/strong&gt;: supports all standard GGUF quant types (&lt;code&gt;q4_k_m&lt;/code&gt;, &lt;code&gt;q5_k_m&lt;/code&gt;, &lt;code&gt;q8_0&lt;/code&gt;, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;ð Quick Start&lt;/h2&gt; &lt;h3&gt;1. Build &amp;amp; Launch&lt;/h3&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Build the image&lt;/h1&gt; &lt;p&gt;docker compose build&lt;/p&gt; &lt;h1&gt;Start the container (Jupyter Lab runs on port 38888)&lt;/h1&gt; &lt;p&gt;docker compose up -d ```&lt;/p&gt; &lt;h3&gt;2. Access Jupyter Lab&lt;/h3&gt; &lt;p&gt;Open your browser at &lt;strong&gt;&lt;a href="http://127.0.0.1:38888"&gt;http://127.0.0.1:38888&lt;/a&gt;&lt;/strong&gt; and log in with your password.&lt;/p&gt; &lt;p&gt;Create a new notebook to fine-tune your model using Unsloth.&lt;/p&gt; &lt;p&gt;After training, save and convert your model directly inside the notebook:&lt;/p&gt; &lt;p&gt;```python&lt;/p&gt; &lt;h1&gt;Save merged model (Unsloth syntax)&lt;/h1&gt; &lt;p&gt;model.save_pretrained_merged(&amp;quot;your-new-model&amp;quot;, tokenizer)&lt;/p&gt; &lt;h1&gt;Convert to GGUF using pre-installed llama.cpp&lt;/h1&gt; &lt;p&gt;!python /workspace/llama.cpp/convert_hf_to_gguf.py \ --outfile your-new-model-gguf \ --outtype q8_0 \ your-new-model ``` &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Train fast. Quantize smarter. Run anywhere. ð&lt;/p&gt; &lt;p&gt;ð &lt;strong&gt;Star the repo if you find it useful!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/covrom/unsloth-docker"&gt;https://github.com/covrom/unsloth-docker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rtsov"&gt; /u/rtsov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/covrom/unsloth-docker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dwj6/optimized_docker_image_for_unsloth_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dwj6/optimized_docker_image_for_unsloth_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T02:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o478xe</id>
    <title>auditlm: dirt simple self-hostable code review</title>
    <updated>2025-10-11T21:24:51+00:00</updated>
    <author>
      <name>/u/ellenhp</name>
      <uri>https://old.reddit.com/user/ellenhp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up from &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1gdp9/im_seeking_alternatives_to_coderabbit_cli_for/"&gt;this thread&lt;/a&gt;, I implemented a very basic &lt;a href="https://github.com/ellenhp/auditlm"&gt;self-hostable code review tool&lt;/a&gt; for when I want a code review but don't have any humans available to help with that. It is an extremely cavewoman-brained piece of software, I basically just give an agent free reign inside of a docker container and ask it to run any commands it needs to get context about the codebase before providing a review of the diff. There's no forge integration yet so it's not usable as a copilot alternative, but perhaps I'll get to that in due time :)&lt;/p&gt; &lt;p&gt;I don't know if I'd recommend anyone actually &lt;em&gt;use&lt;/em&gt; this at least in its current state, especially without additional sandboxing, but I'm hoping either this project or something else will grow to fill this need.&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ellenhp"&gt; /u/ellenhp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o478xe/auditlm_dirt_simple_selfhostable_code_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o478xe/auditlm_dirt_simple_selfhostable_code_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o478xe/auditlm_dirt_simple_selfhostable_code_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T21:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o48ufw</id>
    <title>50-series and pro 6000s sm120 cards. supported models in vllm, exl3, sglang etc. thread</title>
    <updated>2025-10-11T22:35:05+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys I'm starting this thread so people like me with sm120 cards can share with each other which models they get working how they got them working in vllm, sglang, exl3 etc. If you have one or more of these cards please share your experiences and what works and what doesn't etc. I will post too. For now I have gpt-oss working both 20b and 120b and will be trying GLM-4.6 soon&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o48ufw/50series_and_pro_6000s_sm120_cards_supported/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o48ufw/50series_and_pro_6000s_sm120_cards_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o48ufw/50series_and_pro_6000s_sm120_cards_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T22:35:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ez13</id>
    <title>I made a plugin to run LLMs on phones</title>
    <updated>2025-10-12T03:39:04+00:00</updated>
    <author>
      <name>/u/Dragneel_passingby</name>
      <uri>https://old.reddit.com/user/Dragneel_passingby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I've been working on a side project to get LLMs (GGUF models) running locally on Android devices using Flutter.&lt;/p&gt; &lt;p&gt;The result is a plugin I'm calling Llama Flutter. It uses llama.cpp under the hood and lets you load any GGUF model from Hugging Face. I built a simple chat app as an example to test it.&lt;/p&gt; &lt;p&gt;I'm sharing this here because I'm looking for feedback from the community. Has anyone else tried building something similar? I'd be curious to know your thoughts on the approach, or any suggestions for improvement.&lt;/p&gt; &lt;p&gt;Video Demo: &lt;a href="https://files.catbox.moe/xrqsq2.mp4"&gt;https://files.catbox.moe/xrqsq2.mp4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example APK: &lt;a href="https://github.com/dragneel2074/Llama-Flutter/blob/master/example-app/app-release.apk"&gt;https://github.com/dragneel2074/Llama-Flutter/blob/master/example-app/app-release.apk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are some of the technical details / features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Uses the latest llama.cpp (as of Oct 2025) with ARM64 optimizations.&lt;/li&gt; &lt;li&gt; Provides a simple Dart API with real-time token streaming.&lt;/li&gt; &lt;li&gt; Supports a good range of generation parameters and several built-in chat templates.&lt;/li&gt; &lt;li&gt; For now, it's Android-only and focused on text generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're interested in checking it out to provide feedback or contribute, the links are below. If you find it useful, a star on GitHub would help me gauge interest.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;* GitHub Repo: &lt;a href="https://github.com/dragneel2074/Llama-Flutter"&gt;https://github.com/dragneel2074/Llama-Flutter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Plugin on pub.dev: &lt;a href="https://pub.dev/packages/llama%5C_flutter%5C_android"&gt;https://pub.dev/packages/llama\_flutter\_android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think? Is local execution of LLMs on mobile something you see a future for in Flutter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragneel_passingby"&gt; /u/Dragneel_passingby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T03:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o47di4</id>
    <title>Running a large model overnight in RAM, use cases?</title>
    <updated>2025-10-11T21:30:21+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3945wx with 512gb of ddr4 2666mhz. Work is tossing out a few old servers so I am getting my hands on 1TB of ram for free. I have 2x3090 currently.&lt;/p&gt; &lt;p&gt;But was thinking of doing some scraping and analysis, particularly for stocks. My pricing goes to 7p per kw overnight and was thinking of using a night model in RAM that is slow, but fast and using the GPUs during the day. &lt;/p&gt; &lt;p&gt;Surely Iâm not the only one who has thought about this? &lt;/p&gt; &lt;p&gt;Perplexity has started to throttle labs queries so this could be my replacement for deep research. It might be slow, but it will be cheaper than a GPU furnace!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o47di4/running_a_large_model_overnight_in_ram_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o47di4/running_a_large_model_overnight_in_ram_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o47di4/running_a_large_model_overnight_in_ram_use_cases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T21:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3xluf</id>
    <title>Fighting Email Spam on Your Mail Server with LLMs â Privately</title>
    <updated>2025-10-11T14:52:20+00:00</updated>
    <author>
      <name>/u/unixf0x</name>
      <uri>https://old.reddit.com/user/unixf0x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing a blog post I wrote: &lt;a href="https://cybercarnet.eu/posts/email-spam-llm/"&gt;https://cybercarnet.eu/posts/email-spam-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's about how to use local LLMs on your own mail server to identify and fight email spam.&lt;/p&gt; &lt;p&gt;This uses Mailcow, Rspamd, Ollama and a custom proxy in python.&lt;/p&gt; &lt;p&gt;Give your opinion, what you think about the post. If this could be useful for those of you that self-host mail servers.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixf0x"&gt; /u/unixf0x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T14:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o48x07</id>
    <title>What is the most you can do to scale the inference of a model? Specifically looking for lesser known tricks and optimization you have found while tinkering with models</title>
    <updated>2025-10-11T22:38:20+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Scenario: Assuming I have the Phi 4 14b model hosted on a A100 40GB machine, and I can run it for a single data. If i have 1 million legal text documents, what is the best way to scale the inference such that I can process the 1 million text (4000 million words) and extract information out of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o48x07/what_is_the_most_you_can_do_to_scale_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o48x07/what_is_the_most_you_can_do_to_scale_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o48x07/what_is_the_most_you_can_do_to_scale_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T22:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o42ch4</id>
    <title>Choosing a code completion (FIM) model</title>
    <updated>2025-10-11T18:03:11+00:00</updated>
    <author>
      <name>/u/Zc5Gwu</name>
      <uri>https://old.reddit.com/user/Zc5Gwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fill-in-the-middle (FIM) models don't necessarily get all of the attention that coder models get but they work great with llama.cpp and &lt;a href="https://github.com/ggml-org/llama.vim"&gt;llama.vim&lt;/a&gt; or &lt;a href="https://github.com/ggml-org/llama.vscode"&gt;llama.vscode&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Generally, when picking an FIM model, &lt;strong&gt;&lt;em&gt;speed&lt;/em&gt;&lt;/strong&gt; is absolute priority because no one wants to sit waiting for the completion to finish. Choosing models with few active parameters and running GPU only is key. Also, counterintuitively, &amp;quot;base&amp;quot; models work just as well as instruct models. Try to aim for &amp;gt;70 t/s.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Note that only some models support FIM.&lt;/em&gt;&lt;/strong&gt; Sometimes, it can be hard to tell from model cards whether they are supported or not.&lt;/p&gt; &lt;p&gt;Recent models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; (the larger variant might also be FIM, I don't have the hardware to try it)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KwaiCoder-23B-A4B-v1"&gt;Kwaipilot/KwaiCoder-23B-A4B-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KwaiCoder-DS-V2-Lite-Base"&gt;Kwaipilot/KwaiCoder-DS-V2-Lite-Base&lt;/a&gt; (16b 2.4b active)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Slightly older but reliable small models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-3B"&gt;Qwen/Qwen2.5-Coder-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B"&gt;Qwen/Qwen2.5-Coder-1.5B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Untested, new models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;Salesforce/CoDA-v0-Instruct&lt;/a&gt; (I'm unsure if this is FIM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What models am I missing? What models are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zc5Gwu"&gt; /u/Zc5Gwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:03:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4av71</id>
    <title>LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</title>
    <updated>2025-10-12T00:08:28+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: &lt;a href="https://github.com/rbalestr-lab/llm-jepa"&gt;this https URL&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Limitations &lt;/p&gt; &lt;p&gt;Despite its strong accuracy gains, LLM-JEPA introduces two additional hyperparameters. As shown in fig. 7, the optimal configuration may occur at any point in a grid (Î», k), which imposes a significant cost for hyperparameter tuning. While we have not identified an efficient method to explore this space, we empirically observe that adjacent grid points often yield similar accuracy, suggesting the potential for a more efficient tuning algorithm.&lt;/p&gt; &lt;p&gt;The primary bottleneck at present is the 2-fold increase in compute cost during training, which is mitigated by random loss dropout.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.14252"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4av71/llmjepa_large_language_models_meet_joint/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4av71/llmjepa_large_language_models_meet_joint/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T00:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ahfi</id>
    <title>How do you discover &amp; choose right models for your agents? (genuinely curious)</title>
    <updated>2025-10-11T23:49:59+00:00</updated>
    <author>
      <name>/u/Curious-Engineer22</name>
      <uri>https://old.reddit.com/user/Curious-Engineer22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how people actually find the right model for their use case.&lt;/p&gt; &lt;p&gt;If you've recently picked a model for a project, how did you do it?&lt;/p&gt; &lt;p&gt;A few specific questions: 1. Where did you start your search? (HF search, Reddit, benchmarks, etc.) 2. How long did it take? (minutes, hours, days?) 3. What factors mattered most? (accuracy, speed, size?) 4. Did you test multiple models or commit to one? 5. How confident were you in your choice?&lt;/p&gt; &lt;p&gt;Also curious: what would make this process easier?&lt;/p&gt; &lt;p&gt;My hypothesis is that most of us are winging it more than we'd like to admit. Would love to hear if others feel the same way or if I'm just doing it wrong!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious-Engineer22"&gt; /u/Curious-Engineer22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ahfi/how_do_you_discover_choose_right_models_for_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ahfi/how_do_you_discover_choose_right_models_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ahfi/how_do_you_discover_choose_right_models_for_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T23:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o44u78</id>
    <title>We know the rule of thumbâ¦ large quantized models outperform smaller less quantized models, but is there a level where that breaks down?</title>
    <updated>2025-10-11T19:43:54+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ask because Iâve also heard quants below 4 bit are less effective, and that rule of thumb always seemed to compare 4bit large vs 8bit small.&lt;/p&gt; &lt;p&gt;As an example letâs take the large GLM 4.5 vs GLM 4.5 Air. You can have a much higher bitrate with GLM 4.5 Airâ¦ butâ¦ even with a 2bit quant made by unsloth, GLM 4.5 does quite well for me. &lt;/p&gt; &lt;p&gt;I havenât figured out a great way to have complete confidence though so I thought Iâd ask you all. Whatâs your rule of thumb when having to weigh a smaller model vs larger model at different quants? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T19:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3opq5</id>
    <title>What the sub feels like lately</title>
    <updated>2025-10-11T06:47:33+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt; &lt;img alt="What the sub feels like lately" src="https://preview.redd.it/92s8znbxifuf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4866bff0d572386ea47fc19d643a6b2261fbdb" title="What the sub feels like lately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92s8znbxifuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o43qhn</id>
    <title>What rig are you running to fuel your LLM addiction?</title>
    <updated>2025-10-11T18:59:15+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post your shitboxes, H100's, nvidya 3080ti's, RAM-only setups, MI300X's, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4dswr</id>
    <title>HuggingFace storage is no longer unlimited - 12TB public storage max</title>
    <updated>2025-10-12T02:36:39+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case youâve missed the memo like me, HuggingFace is no longer unlimited.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Type of account&lt;/th&gt; &lt;th&gt;Public storage&lt;/th&gt; &lt;th&gt;Private storage&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Free user or org&lt;/td&gt; &lt;td&gt;Best-effort* usually up to 5 TB for impactful work&lt;/td&gt; &lt;td&gt;100 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;PRO&lt;/td&gt; &lt;td&gt;Up to 10 TB included* â grants available for impactful workâ &lt;/td&gt; &lt;td&gt;1 TB + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Team Organizations&lt;/td&gt; &lt;td&gt;12 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Enterprise Organizations&lt;/td&gt; &lt;td&gt;500 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As seen on &lt;a href="https://huggingface.co/docs/hub/en/storage-limits"&gt;https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, they started enforcing it.&lt;/p&gt; &lt;p&gt;â-&lt;/p&gt; &lt;p&gt;For ref. &lt;a href="https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits"&gt;https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T02:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
