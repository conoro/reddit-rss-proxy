<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-01T18:25:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ol9cai</id>
    <title>Gerbil: An open source desktop app for running LLMs locally</title>
    <updated>2025-10-31T23:24:38+00:00</updated>
    <author>
      <name>/u/i_got_the_tools_baby</name>
      <uri>https://old.reddit.com/user/i_got_the_tools_baby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt; &lt;img alt="Gerbil: An open source desktop app for running LLMs locally" src="https://external-preview.redd.it/eno0eDlyNTQ3anlmMYROUy6ynC042_ngFZye_M2RHtEEKYtBWVHWeI_XUuyY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4340ab3f6ed93d4f4c1334de594505f6406eaea" title="Gerbil: An open source desktop app for running LLMs locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_got_the_tools_baby"&gt; /u/i_got_the_tools_baby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/096u8qj06jyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1olisc5</id>
    <title>Best models for open ended text based role play games? Advice appreciated!</title>
    <updated>2025-11-01T08:38:01+00:00</updated>
    <author>
      <name>/u/seoulsrvr</name>
      <uri>https://old.reddit.com/user/seoulsrvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a long time programmer and I'm familiar with deploying and training LLM's for research in other areas but I know nothing about game development.&lt;br /&gt; I have some ideas about applying rpg to other areas.&lt;br /&gt; Please let me know if you have any suggestions on the best LLM's and/or related tools. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seoulsrvr"&gt; /u/seoulsrvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olisc5/best_models_for_open_ended_text_based_role_play/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olisc5/best_models_for_open_ended_text_based_role_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olisc5/best_models_for_open_ended_text_based_role_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1olr7jo</id>
    <title>[R] TempoPFN: Synthetic Pretraining of Linear RNNs for Zero-Shot Timeseries Forecasting</title>
    <updated>2025-11-01T15:50:09+00:00</updated>
    <author>
      <name>/u/Yossarian_1234</name>
      <uri>https://old.reddit.com/user/Yossarian_1234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olr7jo/r_tempopfn_synthetic_pretraining_of_linear_rnns/"&gt; &lt;img alt="[R] TempoPFN: Synthetic Pretraining of Linear RNNs for Zero-Shot Timeseries Forecasting" src="https://b.thumbs.redditmedia.com/ypraYG_dAialEk-Utw7xurIYL-EFsDwiWvc3rCTTJFo.jpg" title="[R] TempoPFN: Synthetic Pretraining of Linear RNNs for Zero-Shot Timeseries Forecasting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yffxoi5v2oyf1.png?width=2806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=106219fb98e9e490fb7e6b9eb89c5dbee1afbe6d"&gt;https://preview.redd.it/yffxoi5v2oyf1.png?width=2806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=106219fb98e9e490fb7e6b9eb89c5dbee1afbe6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Github:&lt;/em&gt; &lt;a href="https://github.com/automl/TempoPFN"&gt;https://github.com/automl/TempoPFN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Paper:&lt;/em&gt; &lt;a href="https://arxiv.org/abs/2510.25502"&gt;https://arxiv.org/abs/2510.25502&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Authors:&lt;/em&gt; Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter&lt;/p&gt; &lt;p&gt;TempoPFN is a univariate time series foundation model based on linear RNNs that is pre-trained exclusively on synthetic data and achieves competitive zero-shot forecasting performance while maintaining efficient, fully parallelizable training and inference. The model uses a GatedDeltaProduct architecture with state-weaving and outperforms all existing synthetic-only approaches on the Gift-Eval benchmark, with open-sourced code and data pipeline for reproducibility.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yossarian_1234"&gt; /u/Yossarian_1234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olr7jo/r_tempopfn_synthetic_pretraining_of_linear_rnns/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olr7jo/r_tempopfn_synthetic_pretraining_of_linear_rnns/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olr7jo/r_tempopfn_synthetic_pretraining_of_linear_rnns/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1olobn4</id>
    <title>How to improve LLM's creativity and randomness?</title>
    <updated>2025-11-01T13:50:15+00:00</updated>
    <author>
      <name>/u/KairosJS</name>
      <uri>https://old.reddit.com/user/KairosJS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, &lt;/p&gt; &lt;p&gt;As most of you probably already know, it's not really possible to have truly random generations with LLMs due to structural reasons. If you ask an LLM to choose a random color or number, you'll notice that it tends to give the same answer most of the time, as expected. &lt;/p&gt; &lt;p&gt;However, I'm interested in finding ways to increase creativity and randomness. For example, if I ask an LLM to create a character persona and description, how could I make it generate less predictable and more diverse results? &lt;/p&gt; &lt;p&gt;Here's what I've tried so far, with varying degrees of success:&lt;br /&gt; - Increasing the temperature/top_k (obvious)&lt;br /&gt; - Programmatically picking a random theme from a list and adding it to the prompt (works, but it limits creativity since it never looks beyond the provided themes)&lt;br /&gt; - Combining multiple random themes to create unique combinations&lt;br /&gt; - Injecting random noise (nonsensical sentences, etc.) to disrupt the probability chain (it just decreases output quality)&lt;br /&gt; - Generating multiple responses within the same conversation, later generations sometimes pull from less probable tokens &lt;/p&gt; &lt;p&gt;I've combined some of these approaches with mild results so far. &lt;/p&gt; &lt;p&gt;Are there any tools or techniques that could help me push this further and get the model to produce much more creative or unpredictable outputs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KairosJS"&gt; /u/KairosJS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olobn4/how_to_improve_llms_creativity_and_randomness/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olobn4/how_to_improve_llms_creativity_and_randomness/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olobn4/how_to_improve_llms_creativity_and_randomness/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T13:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ollh15</id>
    <title>What is the difference between qwen3-vl-4b &amp; qwen3-4b-2507 ?</title>
    <updated>2025-11-01T11:28:45+00:00</updated>
    <author>
      <name>/u/Champignac1</name>
      <uri>https://old.reddit.com/user/Champignac1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ollh15/what_is_the_difference_between_qwen3vl4b/"&gt; &lt;img alt="What is the difference between qwen3-vl-4b &amp;amp; qwen3-4b-2507 ?" src="https://preview.redd.it/apu8a637smyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60444033edfc59f374faede297398d56cc1f995e" title="What is the difference between qwen3-vl-4b &amp;amp; qwen3-4b-2507 ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it just like an addition of a vision feature or does it also has an effect on its general capabilities ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Champignac1"&gt; /u/Champignac1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/apu8a637smyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ollh15/what_is_the_difference_between_qwen3vl4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ollh15/what_is_the_difference_between_qwen3vl4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T11:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1olitlk</id>
    <title>Want to apply all the great llama.cpp quantization methods to your vector store? Then check this out: full support for GGML vectors and GGUF!</title>
    <updated>2025-11-01T08:40:26+00:00</updated>
    <author>
      <name>/u/davidmezzetti</name>
      <uri>https://old.reddit.com/user/davidmezzetti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olitlk/want_to_apply_all_the_great_llamacpp_quantization/"&gt; &lt;img alt="Want to apply all the great llama.cpp quantization methods to your vector store? Then check this out: full support for GGML vectors and GGUF!" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="Want to apply all the great llama.cpp quantization methods to your vector store? Then check this out: full support for GGML vectors and GGUF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davidmezzetti"&gt; /u/davidmezzetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://colab.research.google.com/github/neuml/txtai/blob/master/examples/78_Accessing_Low_Level_Vector_APIs.ipynb#scrollTo=89abb301"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olitlk/want_to_apply_all_the_great_llamacpp_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olitlk/want_to_apply_all_the_great_llamacpp_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oltmre</id>
    <title>NVIDIA Nemotron Nano 12B V2 VL, vision and other models</title>
    <updated>2025-11-01T17:27:03+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled across &lt;a href="https://developer.nvidia.com/blog/develop-specialized-ai-agents-with-new-nvidia-nemotron-vision-rag-and-guardrail-models/"&gt;this&lt;/a&gt; the other day. Apparently one of these models has launched:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16"&gt;Nemotron Nano 12B V2 VL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...and others are on the way. &lt;/p&gt; &lt;p&gt;Anyone played around with these new vision models yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oltmre/nvidia_nemotron_nano_12b_v2_vl_vision_and_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oltmre/nvidia_nemotron_nano_12b_v2_vl_vision_and_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oltmre/nvidia_nemotron_nano_12b_v2_vl_vision_and_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T17:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1olrebz</id>
    <title>Tuning local RAG workflows — floating UI + system prompts (feedback welcome)</title>
    <updated>2025-11-01T15:57:57+00:00</updated>
    <author>
      <name>/u/Zealousideal-Fox-76</name>
      <uri>https://old.reddit.com/user/Zealousideal-Fox-76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olrebz/tuning_local_rag_workflows_floating_ui_system/"&gt; &lt;img alt="Tuning local RAG workflows — floating UI + system prompts (feedback welcome)" src="https://external-preview.redd.it/_g7MxTDjiIWeWKTKuKZXglQRW6EdboZ0ViXwGjT4zqE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3a35d2cb24eeffc8ac9b7477450487d55ab5efc" title="Tuning local RAG workflows — floating UI + system prompts (feedback welcome)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been building &lt;a href="https://hyperlink.nexa.ai/?utm_id=cl1101&amp;amp;utm_source=rd=po&amp;amp;utm_campaign=feat_fb&amp;amp;utm_term=fl_ui_syst_prmpt"&gt;Hyperlink&lt;/a&gt;, a fully local doc-QA tool that runs offline, handles multi-PDF data, and gives line-level cites.&lt;/p&gt; &lt;p&gt;Two features I’ve just added:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Floating UI:&lt;/strong&gt; summon the model from anywhere.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System prompt + top-k/top-p tuning:&lt;/strong&gt; experiment quickly with retrieval depth and response creativity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The aim is to make local inference feel more integrated into real work, less like isolated testing.&lt;/p&gt; &lt;p&gt;I’d love to hear from others:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how you tweak prompts or retrieval settings for smoother local use&lt;/li&gt; &lt;li&gt;what bottlenecks you hit building local agents&lt;/li&gt; &lt;li&gt;what would make local RAG setups feel “production-ready”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Always happy to share if anyone’s curious. &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1olrebz/video/byo54rga3oyf1/player"&gt;HR's resume grooming on-device with system prompt and sampling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1olrebz/video/bbw6hwg04oyf1/player"&gt;Floating UI recall&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Fox-76"&gt; /u/Zealousideal-Fox-76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olrebz/tuning_local_rag_workflows_floating_ui_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olrebz/tuning_local_rag_workflows_floating_ui_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olrebz/tuning_local_rag_workflows_floating_ui_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:57:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1olkcw3</id>
    <title>Lora finetuning on a single 3090</title>
    <updated>2025-11-01T10:20:56+00:00</updated>
    <author>
      <name>/u/NikolaTesla13</name>
      <uri>https://old.reddit.com/user/NikolaTesla13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have a few questions for the folks who tried to finetune LLMs on a single RTX 3090. I am ok with lower scale finetunes and with lower speeds, I am open to learn.&lt;/p&gt; &lt;p&gt;Does gpt oss 20b or qwen3 30b a3b work within the 24gb vram? I read on unsloth they claim 14gb vram is enough for gpt oss 20b, and 18gb vram for qwen3 30b.&lt;/p&gt; &lt;p&gt;However I am worried about the conversion to 4bit for the qwen3 MoE, does that require much vram/ram? Are there any fixes?&lt;/p&gt; &lt;p&gt;Also since gpt oss 20b is only mxfp4, does that even work to finetune at all, without bfp16? Are there any issues afterwards if I want to use with vLLM?&lt;/p&gt; &lt;p&gt;Also please share any relevant knowledge from your experience. Thank you very much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikolaTesla13"&gt; /u/NikolaTesla13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkcw3/lora_finetuning_on_a_single_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkcw3/lora_finetuning_on_a_single_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olkcw3/lora_finetuning_on_a_single_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T10:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7ri8</id>
    <title>support for Minimax M2 has been merged into llama.cpp</title>
    <updated>2025-10-31T22:11:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt; &lt;img alt="support for Minimax M2 has been merged into llama.cpp" src="https://external-preview.redd.it/I_x1QIcREivfRZWw6RyYObzeaj8mdE6DXTQR3kx1F5I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47afb9f21e4fd695dd9279346c35a27194d0369b" title="support for Minimax M2 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16831"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1olsh0j</id>
    <title>Best setup for running local LLMs? Budget up to $4,000</title>
    <updated>2025-11-01T16:40:51+00:00</updated>
    <author>
      <name>/u/Future_Inventor</name>
      <uri>https://old.reddit.com/user/Future_Inventor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I’m looking to build or buy a setup for running language models locally and could use some advice.&lt;/p&gt; &lt;p&gt;More about my requirements: - Budget: up to $4,000 USD (but fine with cheaper if it’s enough). - I'm open to Windows, macOS, or Linux. - Laptop or desktop, whichever makes more sense. - I'm an experienced software engineer, but new to working with local LLMs. - I plan to use it for testing, local inference, and small-scale app development, maybe light fine-tuning later on.&lt;/p&gt; &lt;p&gt;What would you recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Future_Inventor"&gt; /u/Future_Inventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsh0j/best_setup_for_running_local_llms_budget_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsh0j/best_setup_for_running_local_llms_budget_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olsh0j/best_setup_for_running_local_llms_budget_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T16:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1olqwnj</id>
    <title>Making AI agent reasoning visible, feedback welcome on this first working trace view 🙌</title>
    <updated>2025-11-01T15:38:10+00:00</updated>
    <author>
      <name>/u/AdVivid5763</name>
      <uri>https://old.reddit.com/user/AdVivid5763</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqwnj/making_ai_agent_reasoning_visible_feedback/"&gt; &lt;img alt="Making AI agent reasoning visible, feedback welcome on this first working trace view 🙌" src="https://preview.redd.it/masbqpvs0oyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c7e11a0bc3f98ef8935b77fcfddcb8d5f50f912" title="Making AI agent reasoning visible, feedback welcome on this first working trace view 🙌" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been hacking on a small visual layer to understand how an agent thinks step by step. Basically every box here is one reasoning step (parse → decide → search → analyze → validate → respond).&lt;/p&gt; &lt;p&gt;Each node shows:&lt;/p&gt; &lt;p&gt;1- the action type (input/action/validation/. output)&lt;/p&gt; &lt;p&gt;2- success status + confidence %&lt;/p&gt; &lt;p&gt;3- and color-coded links showing how steps connect (loops = retries, orange = validation passes).&lt;/p&gt; &lt;p&gt;If a step fails, it just gets a red border (see the validation node).&lt;/p&gt; &lt;p&gt;Not trying to build anything fancy yet — just want to know:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. When you’re debugging agent behavior, what info do you actually want on screen? 2. Do confidence bands (green/yellow/red) help or just clutter? 3. Anything about the layout that makes your eyes hurt or your brain happy? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Still super rough, I’m posting here to sanity check the direction before I overbuild it. Appreciate any blunt feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdVivid5763"&gt; /u/AdVivid5763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/masbqpvs0oyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqwnj/making_ai_agent_reasoning_visible_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olqwnj/making_ai_agent_reasoning_visible_feedback/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7vwv</id>
    <title>For any LLM enthusiast in Finland you have decommission Super Computer equipped with 96 Nvidia A100 40Gb Pcie , if you live nearby Kajaani try contact company maybe you get them on discount ;)</title>
    <updated>2025-10-31T22:16:56+00:00</updated>
    <author>
      <name>/u/DeathRabit86</name>
      <uri>https://old.reddit.com/user/DeathRabit86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/"&gt;https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;“CSC is preparing the end-of-life plans for Mahti and Puhti in line with scientific needs and sustainability principles. In practice, we’ll donate the systems to suitable recipients for continued use or spare parts”, says&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Sebastian von Alfthan&lt;/em&gt;&lt;/strong&gt;*, Development Manager at CSC.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathRabit86"&gt; /u/DeathRabit86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:16:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oluay3</id>
    <title>Part 3: Building LLMs from Scratch – Model Architecture &amp; GPU Training [Follow-up to Part 1 and 2]</title>
    <updated>2025-11-01T17:54:11+00:00</updated>
    <author>
      <name>/u/amitbahree</name>
      <uri>https://old.reddit.com/user/amitbahree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to share &lt;strong&gt;Part 3&lt;/strong&gt; of my series on building an LLM &lt;em&gt;from scratch&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;This installment dives into the guts of model architecture, multi-GPU training, memory-precision tricks, checkpointing &amp;amp; inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you’ll find inside:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Two model sizes (117M &amp;amp; 354M parameters) and how we designed the architecture.&lt;/li&gt; &lt;li&gt;Multi-GPU training setup: how to handle memory constraints, fp16/bf16 precision, distributed training.&lt;/li&gt; &lt;li&gt;Experiment tracking (thanks Weights &amp;amp; Biases), checkpointing strategies, resume logic for long runs.&lt;/li&gt; &lt;li&gt;Converting PyTorch checkpoints into a deployable format for inference / sharing.&lt;/li&gt; &lt;li&gt;Real-world mistakes and learnings: out-of-memory errors, data-shape mismatches, GPU tuning headaches.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt;&lt;br /&gt; Even if your data pipeline and tokenizer (see Part 2) are solid, your model architecture &lt;em&gt;and&lt;/em&gt; infrastructure matter just as much — otherwise you’ll spend more time debugging than training. This post shows how to build a &lt;em&gt;robust&lt;/em&gt; training pipeline that actually scales.&lt;/p&gt; &lt;p&gt;If you’ve followed along from Part 1 and Part 2, thanks for sticking with it — and if you’re just now jumping in, you can catch up on those earlier posts (links below).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;🔗 &lt;a href="https://blog.desigeek.com/post/2025/11/building-llm-from-scratch-part3-model-architecture-gpu-training/"&gt;Blog post&lt;/a&gt; &lt;/li&gt; &lt;li&gt;🔗 &lt;a href="https://github.com/bahree/helloLondon"&gt;GitHub codebase&lt;/a&gt;&lt;/li&gt; &lt;li&gt;🔗&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o562l3/part_2_building_llms_from_scratch_data_collection/"&gt;Part 2: Data Collection &amp;amp; Custom Tokenizers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;🔗&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1npzstw/a_step_by_step_guide_on_how_to_build_a_llm_from/"&gt;Part 1: Quick Start &amp;amp; Overview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;🔗 &lt;a href="https://www.linkedin.com/posts/amitbahree_ai-llm-generativeai-activity-7390442713931767808-xSfS"&gt;LinkedIn Post&lt;/a&gt; - If that is your thing.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitbahree"&gt; /u/amitbahree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oluay3/part_3_building_llms_from_scratch_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oluay3/part_3_building_llms_from_scratch_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oluay3/part_3_building_llms_from_scratch_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T17:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1olildc</id>
    <title>How much VRAM do you have?</title>
    <updated>2025-11-01T08:24:47+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: sorry guys i missed the 10gb range and the view results option. Pls don’t crucify me too much &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1olildc"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1olrjvc</id>
    <title>Google's new AI model (C2S-Scale 27B) - innovation or hype</title>
    <updated>2025-11-01T16:03:56+00:00</updated>
    <author>
      <name>/u/Emergency-Loss-5961</name>
      <uri>https://old.reddit.com/user/Emergency-Loss-5961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, Google introduced a new AI model (C2S-Scale 27B) that helped identify a potential combination therapy for cancer, pairing silmitasertib with interferon to make “cold” tumors more visible to the immune system. &lt;/p&gt; &lt;p&gt;On paper, that sounds incredible. An AI model generating new biological hypotheses that are then experimentally validated. But here’s a thought I couldn’t ignore. If the model simply generated hundreds or thousands of possible combinations and researchers later found one that worked, is that truly intelligence or just statistical luck? &lt;/p&gt; &lt;p&gt;If it actually narrowed down the list through meaningful biological insight, that’s a real step forward. But if not, it risks being a “shotgun” approach, flooding researchers with possibilities they still need to manually validate. &lt;/p&gt; &lt;p&gt;So, what do you think? Does this kind of result represent genuine AI innovation in science or just a well-packaged form of computational trial and error?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Loss-5961"&gt; /u/Emergency-Loss-5961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olrjvc/googles_new_ai_model_c2sscale_27b_innovation_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olrjvc/googles_new_ai_model_c2sscale_27b_innovation_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olrjvc/googles_new_ai_model_c2sscale_27b_innovation_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T16:03:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol30e5</id>
    <title>qwen2.5vl:32b is saving me $1400 from my HOA</title>
    <updated>2025-10-31T18:53:04+00:00</updated>
    <author>
      <name>/u/jedsk</name>
      <uri>https://old.reddit.com/user/jedsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over this year I finished putting together my local LLM machine with a quad 3090 setup. Built a few workflows with it but like most of you, just wanted to experiment with local models and for the sake of burning tokens lol.&lt;/p&gt; &lt;p&gt;Then in July, my ceiling got damaged from an upstairs leak. HOA says &amp;quot;not our problem.&amp;quot; I'm pretty sure they're wrong, but proving it means reading their governing docs (20 PDFs, +1,000 pages total).&lt;/p&gt; &lt;p&gt;Thought this was the perfect opportunity to create an actual useful app and do bulk PDF processing with vision models. Spun up qwen2.5vl:32b on Ollama and built a pipeline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF → image conversion → markdown&lt;/li&gt; &lt;li&gt;Vision model extraction&lt;/li&gt; &lt;li&gt;Keyword search across everything&lt;/li&gt; &lt;li&gt;Found 6 different sections proving HOA was responsible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Took about 3-4 hours to process everything locally. Found the proof I needed on page 287 of their Declaration. Sent them the evidence, but ofc still waiting to hear back.&lt;/p&gt; &lt;p&gt;Finally justified the purpose of this rig lol.&lt;/p&gt; &lt;p&gt;Anyone else stumble into unexpectedly practical uses for their local LLM setup? Built mine for experimentation, but turns out it's perfect for sensitive document processing you can't send to cloud services.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jedsk"&gt; /u/jedsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol8bfx</id>
    <title>New AI workstation</title>
    <updated>2025-10-31T22:36:32+00:00</updated>
    <author>
      <name>/u/faileon</name>
      <uri>https://old.reddit.com/user/faileon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt; &lt;img alt="New AI workstation" src="https://b.thumbs.redditmedia.com/lEmf1RVi5jrp-eyNGLmi5QJsRATbD0Vj35rKsMda9_Q.jpg" title="New AI workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to fit in 4x RTX 3090 to a Phantek Server/Workstation case. Scores each card for roughly 800$. The PCIE riser on picture was too short (30cm) and had to be replaced with a 60cm one. The vertical mount is for Lian LI case, but manages to hook it up in the Phantek too. Mobo is ASRock romed8-2t, CPU is EPYC 7282 from eBay for 75$. So far it's a decent machine especially considering the cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faileon"&gt; /u/faileon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol8bfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz8qz</id>
    <title>pewdiepie dropped a video about running local ai</title>
    <updated>2025-10-31T16:28:56+00:00</updated>
    <author>
      <name>/u/topfpflanze187</name>
      <uri>https://old.reddit.com/user/topfpflanze187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt; &lt;img alt="pewdiepie dropped a video about running local ai" src="https://external-preview.redd.it/WddxiFHLc3dMB9LBPGHmNWXXrzglB78uxpSOk1Y4d6E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d41e205151bfdcec37d1be377abc09d05a02773e" title="pewdiepie dropped a video about running local ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topfpflanze187"&gt; /u/topfpflanze187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=qw4fDU18RcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1olr4uq</id>
    <title>Optimizations using llama.cpp command?</title>
    <updated>2025-11-01T15:47:12+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;sup&gt;Why are we not seeing threads like this frequently? Most of the time we see threads related to Big Hardware, Large GPU, etc., I really want to see more threads related to Optimizations, Tips/Tricks, Performance, CPU Only inference, etc., which are more useful for low config systems and more importantly we could get 100% performance benchmarks(Like what's the maximum t/s possible from 8GB model without any GPU&lt;/sup&gt;) with low level systems first by using those stuff. To put simply, we must try &lt;strong&gt;&lt;sup&gt;extreme possibilities from limited hardware&lt;/sup&gt;&lt;/strong&gt; &lt;sup&gt;first before buying new or additional rigs.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;All right, here my questions related to title.&lt;/p&gt; &lt;p&gt;1] &lt;strong&gt;-ot vs -ncmoe&lt;/strong&gt; .... I still see some people do use -ot even after -ncmoe. For Dense models, -ot is the way. But any reasons for -ot with MOE models when we have -ncmoe?(&lt;strong&gt;EDIT&lt;/strong&gt;: Exception - Multi GPUs case) Please share sample command examples.&lt;/p&gt; &lt;p&gt;2] Anyone use both -ot &amp;amp; -ncmoe &lt;strong&gt;together&lt;/strong&gt;? Will both work together first of all? If it is, what are possibilities to get more performance?&lt;/p&gt; &lt;p&gt;3] &lt;strong&gt;What else&lt;/strong&gt; can give us more performance? Apart from quantized KVCache, Flash Attention, threads. Am I missing &lt;strong&gt;any other important parameters&lt;/strong&gt;? or should I change value of existing parameters?&lt;/p&gt; &lt;p&gt;I'm hoping to get 50 t/s (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;Currently getting 33 t/s without context&lt;/a&gt;) from Q4 of Qwen3-30B-A3B with my 8GB VRAM + 32GB RAM if possible. Expecting some experts/legends in this sub share their secret stash. My current command is below.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 160.45 ± 18.06 | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 33.73 ± 0.74 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The reason I'm trying to squeeze this more, so I could get decent 20-30 t/s after adding 32-64K context(which is mandatory for agentic coding tools such as Roo code). Thanks a lot.&lt;/p&gt; &lt;p&gt;One other reason for this thread is, still some people not aware of both -ot &amp;amp; -ncmoe. Use it folks, don't leave any tokens at the table. You welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olr4uq/optimizations_using_llamacpp_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olr4uq/optimizations_using_llamacpp_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olr4uq/optimizations_using_llamacpp_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1olsliw</id>
    <title>MiniMax-M2-exl3 - now with CatBench™</title>
    <updated>2025-11-01T16:45:44+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt; &lt;img alt="MiniMax-M2-exl3 - now with CatBench™" src="https://b.thumbs.redditmedia.com/hZFUwp417AAuD17RUalBhIzjpyePeIINpbOg8MJu3Xo.jpg" title="MiniMax-M2-exl3 - now with CatBench™" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/turboderp/MiniMax-M2-exl3"&gt;https://huggingface.co/turboderp/MiniMax-M2-exl3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;⚠️ Requires ExLlamaV3 v0.0.11 &lt;code&gt;dev&lt;/code&gt; branch or v0.0.12 master when released (soon?)&lt;/p&gt; &lt;p&gt;Use the optimized quants if you can fit them!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3hxgebenboyf1.jpg?width=836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=339786475037e8109c89de298db8c14dfd6bbb45"&gt;https://preview.redd.it/3hxgebenboyf1.jpg?width=836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=339786475037e8109c89de298db8c14dfd6bbb45&lt;/a&gt;&lt;/p&gt; &lt;p&gt;True AGI will make the best cat memes. You'll see it here first ;)&lt;/p&gt; &lt;p&gt;Exllama discord: &lt;a href="https://discord.gg/GJmQsU7T"&gt;https://discord.gg/GJmQsU7T&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T16:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1olkx65</id>
    <title>Gaming PC converted to AI Workstation</title>
    <updated>2025-11-01T10:56:09+00:00</updated>
    <author>
      <name>/u/highdefw</name>
      <uri>https://old.reddit.com/user/highdefw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt; &lt;img alt="Gaming PC converted to AI Workstation" src="https://preview.redd.it/z55xgdghmmyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58f471128678bd5884598fa5d1393664c1759fe5" title="Gaming PC converted to AI Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro 5000 and 4000 just arrived. NVME expansion slot on the bottom. 5950x with 128gb ram. Future upgrade will be a cpu upgrade.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/highdefw"&gt; /u/highdefw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z55xgdghmmyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T10:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1olqjxj</id>
    <title>Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B</title>
    <updated>2025-11-01T15:23:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt; &lt;img alt="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" src="https://external-preview.redd.it/_7BYCEiuSe8H_fldVM7chLfCb5j0ciz_pk_F5HpmBuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9f337f24ae40e158287e6812e9408e53add8ae" title="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1olouiw</id>
    <title>TIL: For long-lived LLM sessions, swapping KV Cache to RAM is ~10x faster than recalculating it. Why isn't this a standard feature?</title>
    <updated>2025-11-01T14:12:57+00:00</updated>
    <author>
      <name>/u/Shoddy-Tutor9563</name>
      <uri>https://old.reddit.com/user/Shoddy-Tutor9563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was diving into how vLLM and similar inference servers work and had a thought about optimizing memory for long-lived but inactive chat sessions. The standard approach seems to be either keeping the KV Cache in precious VRAM or evicting it and recalculating from scratch when the user returns. I think there might be a better way.&lt;/p&gt; &lt;p&gt;Here's the core idea: Implement a swapping mechanism for the KV Cache of inactive sessions, moving it from VRAM to system RAM (and back), instead of deleting it.&lt;/p&gt; &lt;p&gt;We always focus on the high cost of moving data between CPU and GPU, but we often forget the cost of recalculating that data. Let's do a quick back-of-the-napkin comparison for a Qwen3-4B-like model with a 16k token context:&lt;/p&gt; &lt;p&gt;Scenario: A user's session becomes inactive. Their 16k-token KV Cache is evicted. Later, they return. We need to restore their context.&lt;/p&gt; &lt;p&gt;· Option A: Recalculate the KV Cache (Standard Approach) · This requires a full &amp;quot;prefill&amp;quot; pass over the entire 16k token prompt. · Estimated Time: ~1.5 to 3 seconds on a modern GPU. · Option B: Swapping (Proposed Approach) · We simply copy the ~4 GB of KV Cache data from system RAM back to VRAM over PCIe. · Estimated Time: ~200-400 ms (on PCIe 4.0).&lt;/p&gt; &lt;p&gt;The math is pretty compelling. Swapping is roughly 7-15x faster than a full recalculation. For a user, waiting 200ms for their chat history to &amp;quot;wake up&amp;quot; is a much better experience than waiting 2+ seconds.&lt;/p&gt; &lt;p&gt;This wouldn't be for high-throughput, always-online inference, but specifically for managing many long-lived sessions (e.g., support chatbots, document analysis with breaks, multi-user systems with intermittent activity). It's a classic space-time tradeoff, but in this case, using slightly more &amp;quot;space&amp;quot; (system RAM) saves a huge amount of &amp;quot;time&amp;quot; (latency on reactivation).&lt;/p&gt; &lt;p&gt;So, I have two main questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Did I mess up my calculations or reasoning anywhere? Are there hidden costs or architectural limitations (e.g., in vLLM, PyTorch, or CUDA) that make this swapping idea less practical than it seems on paper?&lt;/li&gt; &lt;li&gt;Has anyone seen or heard of implementations doing this? I know vLLM's PagedAttention is genius for VRAM management, but I haven't found anything about spilling over to CPU RAM. Are there any forks, research papers, or other inference engines exploring this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keen to hear your thoughts and correct any misunderstandings I might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy-Tutor9563"&gt; /u/Shoddy-Tutor9563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T14:12:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1olmj9z</id>
    <title>Bought MI50 32 Gb from Alibaba. Did I get scammed?</title>
    <updated>2025-11-01T12:26:47+00:00</updated>
    <author>
      <name>/u/Moist_Toto</name>
      <uri>https://old.reddit.com/user/Moist_Toto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt; &lt;img alt="Bought MI50 32 Gb from Alibaba. Did I get scammed?" src="https://preview.redd.it/v3w8clon2nyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ef938653e71635a81d9e3e2eaf625cfbf73033e" title="Bought MI50 32 Gb from Alibaba. Did I get scammed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I bought 8 MI50 32Gb units from someone on Alibaba.&lt;/p&gt; &lt;p&gt;After spending some time to figure out Linux and the software stack, I entered the 'amd-smi static' command in the terminal.&lt;/p&gt; &lt;p&gt;The result is quite frightening, here it is: &lt;/p&gt; &lt;p&gt;especially the bottom part product name saying &amp;quot;16GB&amp;quot;, my heart skipped a beat. Is this something driver related or am I screwed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist_Toto"&gt; /u/Moist_Toto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v3w8clon2nyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T12:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
