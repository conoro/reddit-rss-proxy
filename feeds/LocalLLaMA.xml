<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-04T14:06:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qvg844</id>
    <title>RAG accuracy plateau - anyone else stuck around 70-75%?</title>
    <updated>2026-02-04T05:40:36+00:00</updated>
    <author>
      <name>/u/GlitteringWay7289</name>
      <uri>https://old.reddit.com/user/GlitteringWay7289</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been iterating on a RAG setup for internal docs for about 3 months now. Tried different chunking sizes, overlap strategies, switched from ada-002 to text-embedding-3-large. Still hovering around 70-75% on our eval set.&lt;/p&gt; &lt;p&gt;Starting to think vector similarity alone just has a ceiling. The retrieved chunks are &amp;quot;related&amp;quot; but not always what actually answers the question.&lt;/p&gt; &lt;p&gt;Anyone break through this? What actually moved the needle for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlitteringWay7289"&gt; /u/GlitteringWay7289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg844/rag_accuracy_plateau_anyone_else_stuck_around_7075/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg844/rag_accuracy_plateau_anyone_else_stuck_around_7075/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg844/rag_accuracy_plateau_anyone_else_stuck_around_7075/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvommg</id>
    <title>[Release] Eva-4B-V2: Updated Financial Evasion Detection Model. Now #1, beating Claude Opus 4.5 &amp; Gemini 3 Flash.</title>
    <updated>2026-02-04T13:31:29+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvommg/release_eva4bv2_updated_financial_evasion/"&gt; &lt;img alt="[Release] Eva-4B-V2: Updated Financial Evasion Detection Model. Now #1, beating Claude Opus 4.5 &amp;amp; Gemini 3 Flash." src="https://preview.redd.it/46zsrxh2chhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ead613ff0faafb52ac64b2b37a3497d7c4ddd29" title="[Release] Eva-4B-V2: Updated Financial Evasion Detection Model. Now #1, beating Claude Opus 4.5 &amp;amp; Gemini 3 Flash." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Quick update on Eva-4B â€” we've released &lt;strong&gt;Eva-4B-V2&lt;/strong&gt;, an improved version that now outperforms all frontier LLMs on EvasionBench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new in V2:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 84.9% Macro-F1, beating Gemini 3 Flash (84.6%), Claude Opus 4.5 (84.4%), and GPT-5.2 (80.9%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Two-stage fine-tuning on 84K samples (60K consensus + 24K three-judge majority voting)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Dataset&lt;/strong&gt;: We've released EvasionBench dataset on HuggingFace&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; Classifies earnings call Q&amp;amp;A into &lt;code&gt;direct&lt;/code&gt;, &lt;code&gt;intermediate&lt;/code&gt;, or &lt;code&gt;fully_evasive&lt;/code&gt;. Helps identify when executives are sidestepping analysts' questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why use this over a general LLM?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A 4B model running locally that beats models 100x+ its size on this task&lt;/li&gt; &lt;li&gt;Try it instantly in Colab â€” no setup needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/FutureMa/Eva-4B-V2"&gt;https://huggingface.co/FutureMa/Eva-4B-V2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset: &lt;a href="https://huggingface.co/datasets/FutureMa/EvasionBench"&gt;https://huggingface.co/datasets/FutureMa/EvasionBench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab: &lt;a href="https://colab.research.google.com/github/IIIIQIIII/EvasionBench/blob/main/scripts/eva4b_inference.ipynb"&gt;https://colab.research.google.com/github/IIIIQIIII/EvasionBench/blob/main/scripts/eva4b_inference.ipynb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/IIIIQIIII/EvasionBench"&gt;https://github.com/IIIIQIIII/EvasionBench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Project Page: &lt;a href="https://iiiiqiiii.github.io/EvasionBench/"&gt;https://iiiiqiiii.github.io/EvasionBench/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/46zsrxh2chhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvommg/release_eva4bv2_updated_financial_evasion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvommg/release_eva4bv2_updated_financial_evasion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvp0hm</id>
    <title>model: (qwen3next) correct vectorized key_gdiff calculation by ngxson Â· Pull Request #19324 Â· ggml-org/llama.cpp</title>
    <updated>2026-02-04T13:47:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"&gt; &lt;img alt="model: (qwen3next) correct vectorized key_gdiff calculation by ngxson Â· Pull Request #19324 Â· ggml-org/llama.cpp" src="https://external-preview.redd.it/Dqgg7ZvrLWPUlWr_lQFMlLvrUGKt4Wjs_hNwPvpf-8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cecc3e5f53db33a22bb927dfd55b97e94627ad38" title="model: (qwen3next) correct vectorized key_gdiff calculation by ngxson Â· Pull Request #19324 Â· ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(First?) Fix for Qwen Next Coder&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19324"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvg14v</id>
    <title>GGML implementation of Qwen3-ASR</title>
    <updated>2026-02-04T05:30:22+00:00</updated>
    <author>
      <name>/u/redditgivingmeshit</name>
      <uri>https://old.reddit.com/user/redditgivingmeshit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg14v/ggml_implementation_of_qwen3asr/"&gt; &lt;img alt="GGML implementation of Qwen3-ASR" src="https://external-preview.redd.it/sbrvrkybCIXzPoOhitnG--AeU0R3tNgehTBWYD1GgTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ea5fcdd7dc654a047c01962678ce8793e450c54" title="GGML implementation of Qwen3-ASR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently been experimenting with agent loops, and I got it to work somewhat reliably with minimal guidance from me.&lt;/p&gt; &lt;p&gt;As I have a side project that needs high ASR accuracy, I thought &lt;strong&gt;implementing Qwen3-ASR-0.6B in pure ggml&lt;/strong&gt; would be the perfect real-world test, and surprisingly, it worked!&lt;/p&gt; &lt;p&gt;Anyways, I hope this will be of help to anyone who wanted to use the Qwen3-ASR-0.6B model with forced alignment on their devices.&lt;/p&gt; &lt;p&gt;It supports Q8 quantization for now, which lowers the ram usage under 2 gigs, even including the forced aligner model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditgivingmeshit"&gt; /u/redditgivingmeshit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/predict-woo/qwen3-asr.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg14v/ggml_implementation_of_qwen3asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvg14v/ggml_implementation_of_qwen3asr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvganp</id>
    <title>Step 3.5 Flash is janky af</title>
    <updated>2026-02-04T05:44:24+00:00</updated>
    <author>
      <name>/u/tharsalys</name>
      <uri>https://old.reddit.com/user/tharsalys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using it in Opencode since yesterday. When it works, it's excellent. It's like a much much faster GLM 4.7. But after a few turns, it starts to hallucinate tool calls. &lt;/p&gt; &lt;p&gt;At this point not sure if its a harness issue or a model issue but looking at the reasoning traces which are also full of repetitive lines and jank, it's probably LLM.&lt;/p&gt; &lt;p&gt;Anyone else tried it? Any way to get it working well because I'm really enjoying the speed here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tharsalys"&gt; /u/tharsalys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvganp/step_35_flash_is_janky_af/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvganp/step_35_flash_is_janky_af/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvganp/step_35_flash_is_janky_af/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvvtv</id>
    <title>Qwen3-Coder-Next</title>
    <updated>2026-02-03T16:03:56+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt; &lt;img alt="Qwen3-Coder-Next" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next is out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:03:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvdcz4</id>
    <title>Why is GPT-OSS extremely restrictive</title>
    <updated>2026-02-04T03:21:56+00:00</updated>
    <author>
      <name>/u/sayamss</name>
      <uri>https://old.reddit.com/user/sayamss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the response it returns when trying to make home automation work: &lt;/p&gt; &lt;p&gt;**Security &amp;amp; Privacy** â€“ The script would need to log into your camera and send data over the local network. Running that from this chat would mean Iâ€™d be accessing your private devices, which isnâ€™t allowed. 2. **Policy** â€“ The OpenAI policy says the assistant must not act as a tool that can directly control a userâ€™s device or network.&lt;/p&gt; &lt;p&gt;Why would they censor the model to this extent? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sayamss"&gt; /u/sayamss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvdcz4/why_is_gptoss_extremely_restrictive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvdcz4/why_is_gptoss_extremely_restrictive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvdcz4/why_is_gptoss_extremely_restrictive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T03:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1quxtkj</id>
    <title>The open-source version of Suno is finally here: ACE-Step 1.5</title>
    <updated>2026-02-03T17:13:53+00:00</updated>
    <author>
      <name>/u/AppropriateGuava6262</name>
      <uri>https://old.reddit.com/user/AppropriateGuava6262</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt; &lt;img alt="The open-source version of Suno is finally here: ACE-Step 1.5" src="https://b.thumbs.redditmedia.com/Wc9W0Y1pM9FgVsLGL_nSRSARq7eVx7wAjk_hkOIqGfE.jpg" title="The open-source version of Suno is finally here: ACE-Step 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ACE-Step 1.5 is an open-source music model that can generate a full song in about 2 seconds on an A100, runs locally on a typical PC (around 4GB VRAM), and beats Suno on common evaluation scores.&lt;/p&gt; &lt;p&gt;Key traits of ACE-Step 1.5:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quality: beats Suno on common eval scores&lt;/li&gt; &lt;li&gt;Speed: full song under 2s on A100&lt;/li&gt; &lt;li&gt;Local: ~4GB VRAM, under 10s on RTX 3090&lt;/li&gt; &lt;li&gt;LoRA: train your own style with a few songs&lt;/li&gt; &lt;li&gt;License: MIT, free for commercial use&lt;/li&gt; &lt;li&gt;Data: fully authorized plus synthetic&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ace-step/ACE-Step-1.5"&gt;https://github.com/ace-step/ACE-Step-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights/Training code/LoRA code/Paper are all open.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppropriateGuava6262"&gt; /u/AppropriateGuava6262 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1quxtkj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvm0ft</id>
    <title>Mixture-of-Models routing beats single LLMs on SWE-Bench via task specialization</title>
    <updated>2026-02-04T11:24:04+00:00</updated>
    <author>
      <name>/u/botirkhaltaev</name>
      <uri>https://old.reddit.com/user/botirkhaltaev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been looking at per-task results on SWE-Bench Verified and noticed something that leaderboard averages hide: different models consistently solve &lt;em&gt;different&lt;/em&gt; subsets of tasks.&lt;/p&gt; &lt;p&gt;Even the top overall model on the leaderboard fails a non-trivial number of tasks that other models reliably solve, and the reverse is also true. This suggests strong task-level specialization rather than one model being strictly better.&lt;/p&gt; &lt;p&gt;To test this, I built a &lt;strong&gt;Mixture-of-Models architecture&lt;/strong&gt;, which is different from traditional routing that just defaults to the strongest aggregate model most of the time. The goal isnâ€™t to route to a single model as often as possible, but to exploit complementary strengths between models.&lt;/p&gt; &lt;p&gt;Concretely:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The problem description is embedded&lt;/li&gt; &lt;li&gt;Itâ€™s assigned to a semantic cluster (learned from general coding data, not SWE-Bench)&lt;/li&gt; &lt;li&gt;Each cluster has learned per-model success statistics&lt;/li&gt; &lt;li&gt;The task is routed to the historically strongest model for that &lt;em&gt;type&lt;/em&gt; of problem&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Importantly, this does &lt;strong&gt;not&lt;/strong&gt; route the top aggregate model for the majority of tasks. Several clusters consistently route to other models where they outperform it, even though it has the highest overall score.&lt;/p&gt; &lt;p&gt;Thereâ€™s no new foundation model, no test-time search, and no repo execution, just a lightweight gating mechanism over multiple models.&lt;/p&gt; &lt;p&gt;Using this Mixture-of-Models setup, the system reaches 75.6% on SWE-Bench, exceeding single-model baselines (~74%). The takeaway isnâ€™t the absolute number, but the mechanism: leaderboard aggregates hide complementary strengths, and mixture architectures can capture a higher ceiling than any single model.&lt;/p&gt; &lt;p&gt;Blog with details and methodology here: &lt;a href="https://nordlyslabs.com/blog/hypernova"&gt;https://nordlyslabs.com/blog/hypernova&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: the framework is open source ! &lt;a href="https://github.com/Nordlys-Labs/nordlys"&gt;https://github.com/Nordlys-Labs/nordlys&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/botirkhaltaev"&gt; /u/botirkhaltaev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm0ft/mixtureofmodels_routing_beats_single_llms_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm0ft/mixtureofmodels_routing_beats_single_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm0ft/mixtureofmodels_routing_beats_single_llms_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T11:24:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv5d1k</id>
    <title>Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge</title>
    <updated>2026-02-03T21:47:26+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt; &lt;img alt="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" src="https://external-preview.redd.it/3bIaBnDXu08CXhELxk4__N-qsOVuqLC1ZUdzCxFB0Fo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00daa4c0505c069dbac679c0b3ae6151aa6f7543" title="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-Coder tech report is super interesting on a number of items:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They specifically tested on various tool chat templates to make sure the model stays flexible no matter where you use it. From their own data, only DeepSeek-v3.2 is close - even a bit better - (which suggests they do the same) and they're both quite a bit ahead of other models.&lt;/li&gt; &lt;li&gt;As the model gets smarter and smarter, it gets better and better at finding loopholes in the test environment to find the solution by cheating (&lt;a href="https://github.com/SWE-bench/SWE-bench/pull/471"&gt;https://github.com/SWE-bench/SWE-bench/pull/471&lt;/a&gt;), which they have to combat.&lt;/li&gt; &lt;li&gt;They trained several specialized submodels (UI dev, webdev, software engineering, ...) and the final model is a distillation of those.&lt;/li&gt; &lt;li&gt;It's similar in performance to the base (non-Coder) model on general benchmarks, and quite a bit better at math.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T21:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvlh5n</id>
    <title>Qwen Coders Visual Benchmark</title>
    <updated>2026-02-04T10:53:25+00:00</updated>
    <author>
      <name>/u/loadsamuny</name>
      <uri>https://old.reddit.com/user/loadsamuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to compare the new Qwen Coders so I ran various gguf (IQ1 vs Q3 vs Q4) quants of Qwen Coder Next, along with Coder 30B and VL 32B just to compare vs non coder.&lt;/p&gt; &lt;p&gt;The lightshow test is the one most fail and only the 30B passed it. &lt;/p&gt; &lt;p&gt;All code and prompts are up at&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/electricazimuth/LocalLLM%5C_VisualCodeTest"&gt;https://github.com/electricazimuth/LocalLLM\_VisualCodeTest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/loadsamuny"&gt; /u/loadsamuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://electricazimuth.github.io/LocalLLM_VisualCodeTest/results/2026.02.04/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvlh5n/qwen_coders_visual_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvlh5n/qwen_coders_visual_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T10:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvhc3o</id>
    <title>Yuan 3.0 Flash 40B - 3.7b parameter multimodal foundation model. Does anyone know these or have tried the model?</title>
    <updated>2026-02-04T06:41:33+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/YuanLabAI/Yuan3.0-Flash-4bit"&gt;https://huggingface.co/YuanLabAI/Yuan3.0-Flash-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://yuanlab.ai"&gt;https://yuanlab.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was looking for optimized models for RAG data retrieval and found this. I've never heard of it. I wonder if the architecture is supported by llama.cpp (it's probably something derived from existing models).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T06:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvp2hg</id>
    <title>internlm/Intern-S1-Pro Â· Hugging Face</title>
    <updated>2026-02-04T13:50:20+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"&gt; &lt;img alt="internlm/Intern-S1-Pro Â· Hugging Face" src="https://external-preview.redd.it/YxAPCHfyx1X69aAa5eRKFFzDTrzC_SvUlWSg_aGoYn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1a22c338f7ffdcfdd1ac0da2068e064b078cc48" title="internlm/Intern-S1-Pro Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1000B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qva5gk</id>
    <title>How to get more tok/s?</title>
    <updated>2026-02-04T00:59:54+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt; &lt;img alt="How to get more tok/s?" src="https://external-preview.redd.it/ZnpvY2wyN3BtZGhnMX3C4bhSrcOBtwpO2ghilluKqvqoK5kABDx37kIjqzIp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fd51fdcf7f7597f9ade8d8e5db8eb03b2cb2d80" title="How to get more tok/s?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not OC! [Source](&lt;a href="https://x.com/climate%5C_ben/status/2000636466117193866?s=61"&gt;https://x.com/climate\_ben/status/2000636466117193866?s=61&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l8lk0xapmdhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T00:59:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv65ed</id>
    <title>Got Qwen-Coder-Next running on ROCm on my Strix Halo!</title>
    <updated>2026-02-03T22:17:18+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt; &lt;img alt="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" src="https://external-preview.redd.it/dzdscnFjbDZ0Y2hnMarG5pOoEfpz9JksRMChe8rZdrijqwmTF4wbigP7RjX-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8e501a14c67b4224883973e411e9b24a9f6bcf8" title="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to see the new model, 80B with 3B active seems perfect for Strix Halo. Video is running on &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm/releases/tag/b1170"&gt;llamacpp-rocm b1170&lt;/a&gt; with context size 16k and &lt;code&gt;--flash-attn on --no-mmap&lt;/code&gt;. Let me know what you want me to try and I'll run it later tonight!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hnso57l6tchg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T22:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvax2n</id>
    <title>Qwen3-Coder-Next-NVFP4 quantization is up, 45GB</title>
    <updated>2026-02-04T01:33:48+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/Qwen3-Coder-Next-NVFP4"&gt;GadflyII/Qwen3-Coder-Next-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All experts were calibrated with ultrachat_200k dataset, 1.63% accuracy loss in MMLU Pro+, 149GB to 45GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T01:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvqs9</id>
    <title>Qwen/Qwen3-Coder-Next Â· Hugging Face</title>
    <updated>2026-02-03T15:58:52+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-Coder-Next Â· Hugging Face" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen/Qwen3-Coder-Next Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T15:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvm388</id>
    <title>Qwen3-Coder-Next is available on HuggingChat</title>
    <updated>2026-02-04T11:28:26+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"&gt; &lt;img alt="Qwen3-Coder-Next is available on HuggingChat" src="https://external-preview.redd.it/ts3qmqwhhBSKiMfaD-GP4qTCSy4zry7pFJqkPo5wT7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b618deb783becca7cdc00ba44e4ab3a6dfaf36bd" title="Qwen3-Coder-Next is available on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/models/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T11:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvox18</id>
    <title>Intern-S1-Pro (1T/A22B)</title>
    <updated>2026-02-04T13:43:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt; &lt;img alt="Intern-S1-Pro (1T/A22B)" src="https://preview.redd.it/kobet850fhhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3ca84ca0879baf4bbb204fc239f5b6087ee3a57" title="Intern-S1-Pro (1T/A22B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ðŸš€Introducing Intern-S1-Pro, an advanced 1T MoE open-source multimodal scientific reasoning model.&lt;/p&gt; &lt;p&gt;- SOTA scientific reasoning, competitive with leading closed-source models across AI4Science tasks.&lt;/p&gt; &lt;p&gt;- Top-tier performance on advanced reasoning benchmarks, strong general multimodal performance on various benchmarks.&lt;/p&gt; &lt;p&gt;- 1T-A22B MoE training efficiency with STE routing (dense gradient for router training) and grouped routing for stable convergence and balanced expert parallelism.&lt;/p&gt; &lt;p&gt;- Fourier Position Encoding (FoPE) + upgraded time-series modeling for better physical signal representation; supports long, heterogeneous time-series (10^0â€“10^6 points).&lt;/p&gt; &lt;p&gt;- Intern-S1-Pro is now supported by vLLM @vllm_project and SGLang @sgl_project @lmsysorg â€” more ecosystem integrations are on the way.&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/InternLM/Intern-S1"&gt;https://github.com/InternLM/Intern-S1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kobet850fhhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1quzwjf</id>
    <title>ACE-Step-1.5 has just been released. Itâ€™s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno</title>
    <updated>2026-02-03T18:26:58+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt; &lt;img alt="ACE-Step-1.5 has just been released. Itâ€™s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" src="https://external-preview.redd.it/ZDNiNm9lcXduYmhnMXNUFTz1lD2uwrlR8i5n8_uV8Hgq6zjqVqa04fhxxOUs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b143554ca8c67bc465c8e39d15ee68486eaeef36" title="ACE-Step-1.5 has just been released. Itâ€™s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://xcancel.com/acemusicAI/status/2018731205546684678"&gt;https://xcancel.com/acemusicAI/status/2018731205546684678&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ace-step.github.io/ace-step-v1.5.github.io/"&gt;https://ace-step.github.io/ace-step-v1.5.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Itâ€™s already supported in Comfy. MIT license. HuggingFace Demo is also available! Pretty much the whole package - LoRAs are supported, multiple different models to tailor to different needs, cover and repainting features. This is the closest open-source has gotten to Suno and similar top-slop platforms. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r7v6v6qwnbhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T18:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvo91g</id>
    <title>Intern-S1-Pro</title>
    <updated>2026-02-04T13:14:55+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another 1T-ish VLM. Looks like a Qwen3-235B scaled to 512 experts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvgbhs</id>
    <title>Context rot is killing my agent - how are you handling long conversations?</title>
    <updated>2026-02-04T05:45:40+00:00</updated>
    <author>
      <name>/u/i_m_dead_</name>
      <uri>https://old.reddit.com/user/i_m_dead_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a support agent that needs to maintain context across a full customer session (sometimes 20+ turns). Model starts contradicting itself or forgetting key details around turn 15.&lt;/p&gt; &lt;p&gt;Using GPT-4o with a sliding window but that throws away potentially important early context. Tried summarization but it loses nuance.&lt;/p&gt; &lt;p&gt;Anyone found a practical solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_m_dead_"&gt; /u/i_m_dead_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgbhs/context_rot_is_killing_my_agent_how_are_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgbhs/context_rot_is_killing_my_agent_how_are_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvgbhs/context_rot_is_killing_my_agent_how_are_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T05:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvjonm</id>
    <title>First Qwen3-Coder-Next REAP is out</title>
    <updated>2026-02-04T09:04:09+00:00</updated>
    <author>
      <name>/u/Dany0</name>
      <uri>https://old.reddit.com/user/Dany0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt; &lt;img alt="First Qwen3-Coder-Next REAP is out" src="https://external-preview.redd.it/j98XKqoJ3UOGeW66Etg0lVtFqPsaabyeyZuH8PQVb-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=234ec5f7ffcda5d2272c5b48c2652755e36ad2b9" title="First Qwen3-Coder-Next REAP is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;40% REAP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dany0"&gt; /u/Dany0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lovedheart/Qwen3-Coder-Next-REAP-48B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T09:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. Weâ€™re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM â€“ 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
