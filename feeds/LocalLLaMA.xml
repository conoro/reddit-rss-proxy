<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-18T08:09:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ozfsr7</id>
    <title>Local rig, back from the dead.</title>
    <updated>2025-11-17T13:07:31+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfsr7/local_rig_back_from_the_dead/"&gt; &lt;img alt="Local rig, back from the dead." src="https://a.thumbs.redditmedia.com/WXgLTfXX1MnIbSGT1ih5QPJGn_3KwKX6WjGYpGkrqn0.jpg" title="Local rig, back from the dead." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt;this post&lt;/a&gt; I thought I'd update since I last &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt;posted my setup&lt;/a&gt;. As a few people pointed out, cooling was... suboptimal. It was fine in cool weather but a hot summer meant I burned out some VRAM on one of the A6000s. &lt;/p&gt; &lt;p&gt;JoshiLabs were able to repair it (replace the chip, well done him) and I resolved to watercool. You can get reasonably priced Bykski A6000 blocks from Aliexpress, it turns out. Unfortunately, while building the watercooling loop, I blew up my motherboard (X299) with a spillage. It was very fiddly and difficult in a confined space. There is a 240x60mm rad in the front as well. The build was painful and expensive.&lt;/p&gt; &lt;p&gt;I ended up on a ROMED8-2T like many others here, and an Epyc. Sourcing eight sticks of matched RAM was difficult (I did eventually).&lt;/p&gt; &lt;p&gt;Temps depend on ambient, but are about 25C idle and settle at about 45C with full fans (I ended up on Noctua industrial) and a dynamic power limit at 200W each card. Beefy fans make a huge difference.&lt;/p&gt; &lt;p&gt;I'm running GLM 4.5 Air AWQ FP8 or 4.6 REAP AWQ 4bit on vLLM. It's good. I'm hoping for 4.6 Air or a new Mistral Large. You'll notice the gaps between the cards. I'm pondering a passively cooled A2 (16GB, single slot) for speech or embeddings. If anyone has experience with those, I'd be curious.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ozfsr7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfsr7/local_rig_back_from_the_dead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfsr7/local_rig_back_from_the_dead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T13:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz9vs3</id>
    <title>Apple is considering putting miniHBM on iPhones in 2027</title>
    <updated>2025-11-17T07:15:05+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This news was reported on Macrumor, Apple Insider.&lt;a href="https://www.macrumors.com/2025/05/14/2027-iphones-advanced-ai-memory-tech/?utm_source=chatgpt.com"&gt;https://www.macrumors.com/2025/05/14/2027-iphones-advanced-ai-memory-tech/?utm_source=chatgpt.com&lt;/a&gt; If Apple puts minihbm( high bandwdith memory) on the iphone, then macs will also have minihbm soon… Crazy bandwidths are coming, I hope HBM comes to macs before the iphone! Maybe some people have to wait even longer to upgrade then. Hbm4e will have 2.8 -3.25TB/s per stack ,, and the mac studio can fit up to 3 stacks, we are talking about 8.4-9.75 TB/s on the mac studio. suppose minihbm4e is 20% less than that, that is still 6.8-7.8TB/s.. and up to 2 stacks for the macbook pro, so 5.6-6.5 TB/s but realistically probably lower due to thermal and power constraints , so 3-4 TB/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T07:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p04u8c</id>
    <title>Ordered an RTX 5090 for my first LLM build , skipped used 3090s. Curious if I made the right call?</title>
    <updated>2025-11-18T06:34:18+00:00</updated>
    <author>
      <name>/u/AdventurousAgency371</name>
      <uri>https://old.reddit.com/user/AdventurousAgency371</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just ordered RTX 5090 (Galax), this might have been an impulsive purchase.&lt;/p&gt; &lt;p&gt;My main goal is to have the ability to run largest possible local LLMs on a consumer gpu/gpus that I can afford, around 3k.&lt;/p&gt; &lt;p&gt;Originally, I seriously considered buying &lt;strong&gt;used 3090s&lt;/strong&gt; because the price/VRAM seemed great. But I’m not an experienced builder and was worried possible trouble that may come with them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Is it a much better idea to buy 4 3090s, or just starting with two of them? Still have time to regret and cancel the order of 5090.&lt;/p&gt; &lt;p&gt;Are used 3090/3090 Ti cards &lt;em&gt;more&lt;/em&gt; trouble and risk than they’re worth for beginners?&lt;/p&gt; &lt;p&gt;Also open to suggestions for the rest of the build (budget around ~$1,000–$1,400 USD excluding 5090, as long as it's sufficient to support the 5090 and function an ai workstation. I'm not a gamer, for now).&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousAgency371"&gt; /u/AdventurousAgency371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p04u8c/ordered_an_rtx_5090_for_my_first_llm_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p04u8c/ordered_an_rtx_5090_for_my_first_llm_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p04u8c/ordered_an_rtx_5090_for_my_first_llm_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T06:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozofpd</id>
    <title>[30 Trillion token dataset] "HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models", Oepen et al. 2025</title>
    <updated>2025-11-17T18:40:25+00:00</updated>
    <author>
      <name>/u/RecmacfonD</name>
      <uri>https://old.reddit.com/user/RecmacfonD</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RecmacfonD"&gt; /u/RecmacfonD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2511.01066"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozofpd/30_trillion_token_dataset_hplt_30_very_largescale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozofpd/30_trillion_token_dataset_hplt_30_very_largescale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozfhjg</id>
    <title>cerebras/MiniMax-M2-REAP-162B-A10B · Hugging Face</title>
    <updated>2025-11-17T12:53:42+00:00</updated>
    <author>
      <name>/u/maroule</name>
      <uri>https://old.reddit.com/user/maroule</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfhjg/cerebrasminimaxm2reap162ba10b_hugging_face/"&gt; &lt;img alt="cerebras/MiniMax-M2-REAP-162B-A10B · Hugging Face" src="https://external-preview.redd.it/pZNcDARkPPYS1XPZ3DSC6Cog6lLWkjR2LNUrD7vyKjM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac99bec61621b284cbf7697c5666d42696ab91f4" title="cerebras/MiniMax-M2-REAP-162B-A10B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maroule"&gt; /u/maroule &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2-REAP-162B-A10B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfhjg/cerebrasminimaxm2reap162ba10b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfhjg/cerebrasminimaxm2reap162ba10b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T12:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozq14d</id>
    <title>Comparing Unsloth's GLM-4.6 IQ2_M -vs- GLM-4.6-REAP-268B Q2_K_XL</title>
    <updated>2025-11-17T19:39:11+00:00</updated>
    <author>
      <name>/u/Feedback_Loopy</name>
      <uri>https://old.reddit.com/user/Feedback_Loopy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GLM 4.6 Quantization Trade-offs:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Full&lt;/strong&gt; &lt;strong&gt;IQ2_M&lt;/strong&gt; &lt;strong&gt;(Pervasive Degradation) vs. REAP&lt;/strong&gt; &lt;strong&gt;Q2_K_XL&lt;/strong&gt; &lt;strong&gt;(Structural Removal)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;These 2 are at the limits of what will fit in 128GB and the best local models in this size bracket.&lt;/p&gt; &lt;p&gt;The core of this is comparing the error profiles of pervasive quantization damage versus the structural damage from expert pruning while keeping more of the core preserved from quant damage.&lt;/p&gt; &lt;p&gt;Unsloth's quantization strategies, specifically the _M vs. _XL suffixes - dictate the resource allocation for mitigating quant damage.&lt;/p&gt; &lt;p&gt; _M (Medium) quant applies moderate preservation to core components like the attention mechanism&lt;/p&gt; &lt;p&gt;_XL (Extra Large) quant aggressively preserves the entire reasoning engine and a significant subset of high-magnitude &amp;quot;outlier&amp;quot; weights within the MLP/expert layers.&lt;/p&gt; &lt;p&gt;This is pitted against Cerebras's REAP, which structurally removes entire expert layers, a process whose &amp;quot;near-lossless&amp;quot; claim on benchmarks often conflicts with reports of brittle, domain-specific failures.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Two Philosophies of Compression:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM 4.6 IQ2_M - The &amp;quot;Pervasive Degradation&amp;quot; Model:&lt;/strong&gt; This is the complete 357B parameters. The IQ2 baseline introduces significant precision degradation across more weights. The _M(Medium) preservation strategy is a compromise: it allocates its limited budget to &lt;strong&gt;partially shield the attention mechanism&lt;/strong&gt;, but this leaves the reasoning core still impacted by quantization noise and provides &lt;strong&gt;no remaining budget to preserve critical, high-magnitude &amp;quot;outlier&amp;quot; weights&lt;/strong&gt; in the MLP/expert layers. The result is a model with its full knowledge base intact, but with a systemic, low-level degradation affecting both its reasoning and its recall of specific patterns.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.6 REAP Q2_K_XL - The &amp;quot;Structural Deficit&amp;quot; Model:&lt;/strong&gt; This is a structurally altered 268B parameter version where ~25% of expert layers have been permanently amputated. The key difference is the _XL preservation strategy. It allocates its much larger budget to first &lt;strong&gt;fully preserve the entire remaining attention mechanism at a high precision&lt;/strong&gt; - effectively insulating more of the model's &amp;quot;brain&amp;quot; from quantization damage. It then uses its remaining budget to &lt;strong&gt;surgically preserve a significant subset of critical knowledge outliers&lt;/strong&gt; in the remaining experts. The result should be a model with a sharp, high-fidelity reasoning core and more critical weights better preserved but with permanent, irreparable gaps in its knowledge and complex glitches.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Core Technical Debate for Coding:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The choice between these models seems a choice between two distinct types of risk.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;Full IQ2_M&lt;/strong&gt; risks a &lt;strong&gt;consistent lack of sharpness&lt;/strong&gt;. Its partially degraded reasoning core may lead to subtle but critical logical flaws, less optimal code, and a failure to grasp nuance in complex, multi-step instructions. It's a &amp;quot;known unknown&amp;quot; that its performance ceiling is lowered across the board.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;REAP Q2_K_XL&lt;/strong&gt; risks &lt;strong&gt;brittle, domain-specific failures&lt;/strong&gt;. Its well-preserved core should, in theory, provide superior logical fidelity and more precise code generation. However, this is entirely contingent on the REAP process not having pruned an expert critical to your tasks and next token. This is an &amp;quot;unknown unknown&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Theoretically, for high-precision tasks like coding, the REAP Q2_K_XL seems superior, as its insulated brain should be more reliable. But this hypothesis falls apart if the pruning damage is more significant than benchmarks suggest.&lt;/p&gt; &lt;p&gt;During my limited coding testing I'm seeing:&lt;br /&gt; REAP_Q2_K_XL sometimes perform better but fail more often, including sometimes looping and some broken code outputs.&lt;br /&gt; Full_IQ2_M retains more general and contextual knowledge and seems more consistent, but perhaps less chance of a great output.&lt;/p&gt; &lt;p&gt;Could not find any benchmarks comparing these versions and didn't expect to find any yet.&lt;/p&gt; &lt;p&gt;I've not run proper A-B testing and benchmarking yet either, plus such benchmarking is not reliable anyway.&lt;/p&gt; &lt;p&gt;Have any of you compared them much?&lt;br /&gt; Especially interested in coders who've tried both: what are you seeing so far?&lt;br /&gt; Also experts weighing in on the trade offs of a full _M vs REAPed _XL?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feedback_Loopy"&gt; /u/Feedback_Loopy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozq14d/comparing_unsloths_glm46_iq2_m_vs_glm46reap268b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozq14d/comparing_unsloths_glm46_iq2_m_vs_glm46reap268b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozq14d/comparing_unsloths_glm46_iq2_m_vs_glm46reap268b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T19:39:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz5rsw</id>
    <title>ChatGPT understands its creator</title>
    <updated>2025-11-17T03:27:05+00:00</updated>
    <author>
      <name>/u/mtmttuan</name>
      <uri>https://old.reddit.com/user/mtmttuan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"&gt; &lt;img alt="ChatGPT understands its creator" src="https://preview.redd.it/wkig4aaykq1g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b12bd9e65204d4231dda523ee5d070e5e906c6b" title="ChatGPT understands its creator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even ChatGPT knows &amp;quot;Open Source&amp;quot; seems unlikely when it comes to OpenAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtmttuan"&gt; /u/mtmttuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wkig4aaykq1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T03:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p04mf6</id>
    <title>Guide: Setting up llama-swap on Strix Halo with Bazzite Linux</title>
    <updated>2025-11-18T06:21:31+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got my Framework Desktop last week and spent some time over the weekend setting up llama-swap. This is my quick set up instructions for configuring llama-swap with Bazzite Linux. Why Bazzite? As a gaming focused distro things just worked out of the box with GPU drivers and decent performance. &lt;/p&gt; &lt;p&gt;After spending a couple of days and trying different distros I'm pretty happy with this set up. It's easy to maintain and relatively easy to get going. I would recommend Bazzite as everything I needed worked out of the box where I can run LLMs and maybe the occational game. I have the Framework Desktop but I expect these instructions to work for Bazzite on other Strix Halo platforms. &lt;/p&gt; &lt;h2&gt;Installing llama-swap&lt;/h2&gt; &lt;p&gt;First create the directories for storing the config and models in /var/llama-swap:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh $ sudo mkdir -p /var/llama-swap/models $ sudo chown -R $USER /var/llama-swap &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Create /var/llama-swap/config.yaml. &lt;/p&gt; &lt;p&gt;Here's a starter one: &lt;/p&gt; &lt;p&gt;```yaml logLevel: debug sendLoadingState: true&lt;/p&gt; &lt;p&gt;macros: &amp;quot;default_strip_params&amp;quot;: &amp;quot;temperature, min_p, top_k, top_p&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;server-latest&amp;quot;: | /app/llama-server --host 0.0.0.0 --port ${PORT} -ngl 999 -ngld 999 --no-mmap --no-warmup --jinja&lt;/p&gt; &lt;p&gt;&amp;quot;gptoss-server&amp;quot;: | /app/llama-server --host 127.0.0.1 --port ${PORT} -ngl 999 -ngld 999 --no-mmap --no-warmup --model /models/gpt-oss-120b-mxfp4-00001-of-00003.gguf --ctx-size 65536 --jinja --temp 1.0 --top-k 100 --top-p 1.0&lt;/p&gt; &lt;p&gt;models: gptoss-high: name: &amp;quot;GPT-OSS 120B high&amp;quot; filters: strip_params: &amp;quot;${default_strip_params}&amp;quot; cmd: | ${gptoss-server} --chat-template-kwargs '{&amp;quot;reasoning_effort&amp;quot;: &amp;quot;high&amp;quot;}'&lt;/p&gt; &lt;p&gt;gptoss-med: name: &amp;quot;GPT-OSS 120B med&amp;quot; filters: strip_params: &amp;quot;${default_strip_params}&amp;quot; cmd: | ${gptoss-server} --chat-template-kwargs '{&amp;quot;reasoning_effort&amp;quot;: &amp;quot;medium&amp;quot;}'&lt;/p&gt; &lt;p&gt;gptoss-20B: name: &amp;quot;GPT-OSS 20B&amp;quot; filters: strip_params: &amp;quot;${default_strip_params}&amp;quot; cmd: | ${server-latest} --model /models/gpt-oss-20b-mxfp4.gguf --temp 1.0 --top-k 0 --top-p 1.0 --ctx-size 65536 ```&lt;/p&gt; &lt;p&gt;Now create the &lt;a href="https://docs.bazzite.gg/Installing_and_Managing_Software/Quadlet/"&gt;Quadlet&lt;/a&gt; service file in &lt;code&gt;$HOME/.config/containers/systemd&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;``` [Container] ContainerName=llama-swap Image=ghcr.io/mostlygeek/llama-swap:vulkan AutoUpdate=registry PublishPort=8080:8080 AddDevice=/dev/dri&lt;/p&gt; &lt;p&gt;Volume=/var/llama-swap/models:/models:z,ro Volume=/var/llama-swap/config.yaml:/app/config.yaml:z,ro&lt;/p&gt; &lt;p&gt;[Install] WantedBy=default.target ```&lt;/p&gt; &lt;p&gt;Then start up llama-swap: &lt;/p&gt; &lt;p&gt;``` $ systemctl --user daemon-reload $ systemctl --user restart llama-swap&lt;/p&gt; &lt;h1&gt;run services even if you're not logged in&lt;/h1&gt; &lt;p&gt;$ loginctl enable-linger $USER ```&lt;/p&gt; &lt;p&gt;llama-swap should now be running on port 8080 on your host. When you edit your config.yaml you will have to restart llama-swap with: &lt;/p&gt; &lt;p&gt;``` $ systemctl --user restart llama-swap&lt;/p&gt; &lt;h1&gt;tail llama-swap's logs&lt;/h1&gt; &lt;p&gt;$ journalctl --user -fu llama-swap&lt;/p&gt; &lt;h1&gt;update llama-swap:vulkan&lt;/h1&gt; &lt;p&gt;$ podman pull ghcr.io/mostlygeek/llama-swap:vulkan ```&lt;/p&gt; &lt;h2&gt;Performance Tweaks&lt;/h2&gt; &lt;p&gt;The general recommendation is to allocate the lowest amount of memory (512MB) in BIOS. On Linux it's possible to use up almost all of the 128GB but I haven't tested beyond gpt-oss 120B at this point.&lt;/p&gt; &lt;p&gt;There are three kernel params to add:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ttm.pages_limit=27648000&lt;/li&gt; &lt;li&gt;ttm.page_pool_size=27648000&lt;/li&gt; &lt;li&gt;amd_iommu=off&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;```sh $ sudo rpm-ostree kargs --editor&lt;/p&gt; &lt;h1&gt;add ttm.pages_limit, ttm.page_pool_size - use all the memory availble in the framework&lt;/h1&gt; &lt;h1&gt;add amd_iommu=off - increases memory speed&lt;/h1&gt; &lt;p&gt;rhgb quiet root=UUID=&amp;lt;redacted&amp;gt; rootflags=subvol=root rw iomem=relaxed bluetooth.disable_ertm=1 ttm.pages_limit=27648000 ttm.page_pool_size=27648000 amd_iommu=off ```&lt;/p&gt; &lt;p&gt;After rebooting you can run a memory speed test. Here's what mine look like after the tweaks: &lt;/p&gt; &lt;p&gt;``` $ curl -LO &lt;a href="https://github.com/GpuZelenograd/memtest_vulkan/releases/download/v0.5.0/memtest_vulkan-v0.5.0_DesktopLinux_X86_64.tar.xz"&gt;https://github.com/GpuZelenograd/memtest_vulkan/releases/download/v0.5.0/memtest_vulkan-v0.5.0_DesktopLinux_X86_64.tar.xz&lt;/a&gt; $ tar -xf memtest_vulkan-v0.5.0_DesktopLinux_X86_64.tar.xz $ ./memtest_vulkan &lt;a href="https://github.com/GpuZelenograd/memtest_vulkan"&gt;https://github.com/GpuZelenograd/memtest_vulkan&lt;/a&gt; v0.5.0 by GpuZelenograd To finish testing use Ctrl+C&lt;/p&gt; &lt;p&gt;1: Bus=0xC2:00 DevId=0x1586 71GB Radeon 8060S Graphics (RADV GFX1151) 2: Bus=0x00:00 DevId=0x0000 126GB llvmpipe (LLVM 21.1.4, 256 bits) (first device will be autoselected in 8 seconds) Override index to test: ...testing default device confirmed Standard 5-minute test of 1: Bus=0xC2:00 DevId=0x1586 71GB Radeon 8060S Graphics (RADV GFX1151) 1 iteration. Passed 0.5851 seconds written: 63.8GB 231.1GB/sec checked: 67.5GB 218.3GB/sec 3 iteration. Passed 1.1669 seconds written: 127.5GB 231.0GB/sec checked: 135.0GB 219.5GB/sec 12 iteration. Passed 5.2524 seconds written: 573.8GB 230.9GB/sec checked: 607.5GB 219.5GB/sec 64 iteration. Passed 30.4095 seconds written: 3315.0GB 230.4GB/sec checked: 3510.0GB 219.1GB/sec 116 iteration. Passed 30.4793 seconds written: 3315.0GB 229.8GB/sec checked: 3510.0GB 218.7GB/sec ```&lt;/p&gt; &lt;p&gt;Here are some things I really like about the Strix Halo: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;It very low power, it idle at about 16W. My nvidia server (2x3090, 2xP40), 128GB DDR4, X99 with 22-core xeon idles at ~150W.&lt;/li&gt; &lt;li&gt;It's good for MoE models. Qwen3 series, gpt-oss, etc are good. &lt;/li&gt; &lt;li&gt;It's not so good for dense models. llama-3 70B Q4_K_M w/ speculative decoding gets about 5.5tok/sec. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this helps you set up your own Strix Halo LLM server quickly! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p04mf6/guide_setting_up_llamaswap_on_strix_halo_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p04mf6/guide_setting_up_llamaswap_on_strix_halo_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p04mf6/guide_setting_up_llamaswap_on_strix_halo_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T06:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozx8h1</id>
    <title>Taught a Local LLM to play Cartpole from OpenAI Gym</title>
    <updated>2025-11-18T00:20:18+00:00</updated>
    <author>
      <name>/u/viewmodifier</name>
      <uri>https://old.reddit.com/user/viewmodifier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozx8h1/taught_a_local_llm_to_play_cartpole_from_openai/"&gt; &lt;img alt="Taught a Local LLM to play Cartpole from OpenAI Gym" src="https://external-preview.redd.it/N2ZzaGlibGdzdzFnMULuo3MR4pX1LbW-4KQh_y6FbG5qa-Bh-MCJW06VtRB9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d858dee74d61ee74ff8d29a3f3e24020dfbf99f" title="Taught a Local LLM to play Cartpole from OpenAI Gym" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/viewmodifier"&gt; /u/viewmodifier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uu4k3clgsw1g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozx8h1/taught_a_local_llm_to_play_cartpole_from_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozx8h1/taught_a_local_llm_to_play_cartpole_from_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T00:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozjz15</id>
    <title>Reactive Agents: AI agents that self-optimize after every interaction</title>
    <updated>2025-11-17T15:57:22+00:00</updated>
    <author>
      <name>/u/No_Heart_159</name>
      <uri>https://old.reddit.com/user/No_Heart_159</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozjz15/reactive_agents_ai_agents_that_selfoptimize_after/"&gt; &lt;img alt="Reactive Agents: AI agents that self-optimize after every interaction" src="https://b.thumbs.redditmedia.com/hlnfwE1W_ah7kgZ3Z9lk-Z0jsv7mAo4-lU5Me6BX5vM.jpg" title="Reactive Agents: AI agents that self-optimize after every interaction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have developed an actual reactive agent that continuously learns and adapts based on its own performance, without requiring code changes or human intervention. To make them easy to deploy, observe, and manage, we also built a server and app. All of our work is open source under the Apache 2.0 license. You can find it here: &lt;a href="https://github.com/idkhub-com/reactive-agents"&gt;https://github.com/idkhub-com/reactive-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After setting up the server, you don't need to make many changes to migrate a normal agent to a reactive agent. The server understands the OpenAI API standard, so you can continue to use the OpenAI library from Python, JS, Rust, or whatever language you use.&lt;/p&gt; &lt;p&gt;Each agent can perform the following changes in real-time:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Choose different LLM providers and models&lt;/li&gt; &lt;li&gt;Optimize system prompts&lt;/li&gt; &lt;li&gt;Change hyperparameters&lt;/li&gt; &lt;li&gt;Choose different configurations for conversations on different topics&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You set up your agents in the UI. The most work you will have to do is to provide 1 or 2 sentences describing what each agent does, as well as 1 or 2 sentences describing what each skill (node) does.&lt;/li&gt; &lt;li&gt;Select the LLM models you want each skill to use.&lt;/li&gt; &lt;li&gt;Select what you want the agent to improve based on (task completion, conversation completeness, latency, etc).&lt;/li&gt; &lt;li&gt;Send regular requests to the Reactive Agents server with a header that specifies which agent and skill to use.&lt;/li&gt; &lt;li&gt;For every request you send, you can see its input, output, the system prompt that was used, how the agent evaluated itself, and other information.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We have achieved remarkable results in many scenarios, but we still need to do considerable work. Things to look out for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Streaming is not supported yet. (Top priority right now)&lt;/li&gt; &lt;li&gt;We support over 30 different AI providers, but we have only truly tested OpenAI, Ollama, OpenRouter, and Google (Gemini).&lt;/li&gt; &lt;li&gt;You may need to periodically check how the agent is evaluating itself to ensure it is not being too strict or lenient.&lt;/li&gt; &lt;li&gt;The algorithms used internally will continue to evolve and may cause issues.&lt;/li&gt; &lt;li&gt;Please don't expose the server to the public. Although we have security implementations in place, the server is currently intended to be run locally only.&lt;/li&gt; &lt;li&gt;Please refrain from using it for requests that you can't afford to lose. We haven't pushed things past their breaking points yet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We welcome feedback, discussions, and contributions. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Heart_159"&gt; /u/No_Heart_159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ozjz15"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozjz15/reactive_agents_ai_agents_that_selfoptimize_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozjz15/reactive_agents_ai_agents_that_selfoptimize_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T15:57:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozh8py</id>
    <title>MXFP4 Hybrid Dense Models (Ready to share - Near Lossless Precision, Faster, Smaller)</title>
    <updated>2025-11-17T14:09:14+00:00</updated>
    <author>
      <name>/u/crossivejoker</name>
      <uri>https://old.reddit.com/user/crossivejoker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created 10+ hybrid MXFP4 GGUF of the top models available today. Many of these models often have faster TPS than a Q4_K_M, ~10% smaller than a Q8_0 model, and much less precision loss than Q6_K (very near Q8, sometimes better) . I'll provide links to the models, all the benchmarks, and my process.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;If you don't care about the details and just want to play with the fun experiment models, just go the last section of the post.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I kept hearing “MXFP4 is bad on dense models,” but nobody showed numbers that satisfied my curiosity. So I ran my own tests. The first MXFP4 dense run was a total disaster, but I didn’t stop.&lt;/p&gt; &lt;p&gt;I kept protecting different parts of the model. The changes I thought would help made things worse. The ones I didn’t expect to matter suddenly did. So I kept digging… and something genuinely exciting started to appear.&lt;/p&gt; &lt;h1&gt;What is a MXFP4 Hybrid Model?&lt;/h1&gt; &lt;p&gt;An MXFP4 hybrid is the process of discovering the AI's architecture preference of which quantization most protects the models sanity to prevent noise. The goal is to detect which of these area's MXFP4 most damages while leaving as much quantized as MXFP4 as possible. The following are the most critical to protect from MXFP4 in different combinations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Output weights&lt;/li&gt; &lt;li&gt;Token embd weights&lt;/li&gt; &lt;li&gt;router&lt;/li&gt; &lt;li&gt;gate&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Between each of those 4 critical aspects that must be protected from noise, a combination of MXFP4, Q5_K, Q6_K, Q8_0, and F16 must be discovered to reduce noise as much as possible. Note I never found a combination with Q4 that supported MXFP4.&lt;/p&gt; &lt;p&gt;When proper combinations are discovered, I've found magic will occur. I created an evolution process that creates, destroys, and discovers the patterns per model to find optimal hybrid MXFP4 variants.&lt;/p&gt; &lt;h1&gt;Examples&lt;/h1&gt; &lt;p&gt;Please note that I will showcase here some hand picked examples that're some of the best results achieved. But it's important to remember that NOT all models achieved these results. Many models were out right allergic to MXFP4 no matter the variants. A future &lt;a href="https://github.com/magiccodingman"&gt;GitHub repository&lt;/a&gt; I'll be making will showcase benchmarks of models that couldn't achieve a single successful variant, or models that achieved, &amp;quot;ehhh&amp;quot; results, that simply weren't good enough to write home about.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Unsloth Qwen3 4B Thinking 2507&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;12% smaller than the Q8 model, while achieving only 0.0007% precision loss (basically F16 precision). It also hit ~423 tok/s in testing, which was faster than the Q8, Q6, Q5, and Q4.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;output + tensors were MXFP4. The router, gate, and text embed was Q6_k.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Unsloth Granite 4.0 H 350M MXFP4&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This tiny 350 million parameter model found a variant that had only a 0.04959% precision drop, and reduce the size by 30% compared to the F16 model. But for a tiny model like this, you need this small of a precision drop to not lobotomize the model. For models this size, even a Q8_0 rarely achieves precision drops that don't cause brain damage.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Used F16 router, gate, and embed. Output was Q6_k. The rest of the tensors were MXFP4.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Unsloth - Seed OSS 36B Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Seed OSS had 2 winners. One variant was 8.8% smaller than Q8, though basically the same precision and TPS to the Q8.&lt;/p&gt; &lt;p&gt;But this model was an outlier and the MFXP4_MOE pure was 11.7% smaller than the Q4_K_M, while achieving slightly better precision than the Q4_K_M! A 36B model that's not full blown stupid at 17.9 GB? I'll take that win.&lt;/p&gt; &lt;h1&gt;Top Patterns Variant?&lt;/h1&gt; &lt;p&gt;Honestly I wish I could say there's patterns that I see. I noticed a lot of models really loved Q6_K. And you'll see through my benchmarks that on many occasions the Q6_K outperforms a Q8 in precision, speed, and file size. Which honestly is just a reminder to all of us to STOP posting quantized models without benchmarks (seriously it's part of llama.cpp, it's easy, please do this).&lt;/p&gt; &lt;p&gt;There was a time I thought MXFP4 plus Q6_K were best friends until Apriel 1.5 15B thinker came out and said, &amp;quot;hey, you know how not a single model likes Q5_K? Well, I do!&amp;quot;&lt;/p&gt; &lt;p&gt;When no model had variations with Q8 that worked, the Granite 4.0 H 1B was apparently best friends with Q8 and MXFP4. Qwen3 VL 8B Instruct strictly only liked Q6, but the thinker variant.. Well it was cool with both Q6 and Q8.&lt;/p&gt; &lt;p&gt;Some models like F16 and Q6_k, some liked super weird combinations. Every time I recorded patterns, another model would break my theory.&lt;/p&gt; &lt;p&gt;In the end, I learned only 1 truth. That every models architecture works different and you must find what quantization the models speaks too without noise.&lt;/p&gt; &lt;p&gt;But one thing is clear from my experiment. MXFP4 isn't &amp;quot;bad&amp;quot;, it's simply different. And the community hasn't had enough fun playing with it yet.&lt;/p&gt; &lt;h1&gt;The Models &amp;amp; Benchmarks&lt;/h1&gt; &lt;p&gt;I’ve bundled everything into a Hugging Face collection here:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/magiccodingman/mxfp4-hybrid-gguf"&gt;&lt;strong&gt;https://huggingface.co/collections/magiccodingman/mxfp4-hybrid-gguf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far there's like 10+ models I've uploaded.&lt;/p&gt; &lt;p&gt;Model parameters tested ranged from 350M, 1B, 4B, 8B, 15B, 32B, 36B. There's more still uploading as well. Vision models included, but benchmarks on images are untested. If you test this before me, please let me know your results!&lt;/p&gt; &lt;p&gt;Every repo includes &lt;strong&gt;organized benchmark tables&lt;/strong&gt; and the raw logs, so you can see exactly how I got my numbers. If something looks off, tell me, seriously, I don’t bite.&lt;/p&gt; &lt;p&gt;I've been utilizing these models without issue so far. And I worked really hard to build a benchmark suite to validate accuracy. But that doesn't mean the model is not quirky! I may not have found the weirdness MXFP4 hybrids are causing yet. Maybe there's none? Maybe there's some or a lot?&lt;/p&gt; &lt;p&gt;Either way. Enjoy my really weird MXFP4 hybrid models I created with a barbaric evolution algorithm.&lt;/p&gt; &lt;p&gt;And if you test these models, I would love to hear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Did it outperform the base model for your use case?&lt;/li&gt; &lt;li&gt;Did it fall apart in some domain the benchmarks didn’t catch?&lt;/li&gt; &lt;li&gt;Would you actually use a hybrid like this long-term?&lt;/li&gt; &lt;li&gt;Are you tempted to run your own batch experiments to see which hybrid format becomes “king” on other architectures?&lt;/li&gt; &lt;li&gt;Does any of the results surprise you? Why?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hope you find this as fun and weird as I do.&lt;br /&gt; If you’ve got questions, hit me.&lt;br /&gt; If you understand the “why” behind some of these bizarre patterns, &lt;em&gt;definitely&lt;/em&gt; speak up!&lt;/p&gt; &lt;p&gt;Hope you enjoy these experimental models as much as I have :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Answers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm still refining my batch evolution scripts, but I will share them on &lt;a href="https://github.com/magiccodingman"&gt;GitHub at magiccodingman&lt;/a&gt; soon enough. I fine tuned my algorithm last night and found even better optimizations that I'm not sharing here yet. So, I'm still in the process of optimizing before I share my dirty code.&lt;/li&gt; &lt;li&gt;I'm putting together all my benchmarks of bad batches.&lt;/li&gt; &lt;li&gt;I still have many more models I'm working on that I will upload in the coming weeks on my Hugging Face repo.&lt;/li&gt; &lt;li&gt;I'm still uploading models right now lol. I swear my upload bandwidth is the only thing holding me back! Apriel 1.5B has a better variant found from last night still uploading. Qwen3 VL 32B still uploading as well. Should be done uploading this afternoon post 12 PM EST 11/17/25.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossivejoker"&gt; /u/crossivejoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh8py/mxfp4_hybrid_dense_models_ready_to_share_near/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh8py/mxfp4_hybrid_dense_models_ready_to_share_near/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh8py/mxfp4_hybrid_dense_models_ready_to_share_near/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T14:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p039e3</id>
    <title>Epstein emails graph relationship extraction and visualizer</title>
    <updated>2025-11-18T05:04:27+00:00</updated>
    <author>
      <name>/u/madmax_br5</name>
      <uri>https://old.reddit.com/user/madmax_br5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"&gt; &lt;img alt="Epstein emails graph relationship extraction and visualizer" src="https://b.thumbs.redditmedia.com/5MRfJ06E2zDuBlR4SudLQG-kry1JtdebITN8KKt2jKc.jpg" title="Epstein emails graph relationship extraction and visualizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o5pig6lx5y1g1.png?width=3434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85d3e0204d9eb558e49791cc41b6789a62c6296"&gt;https://preview.redd.it/o5pig6lx5y1g1.png?width=3434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85d3e0204d9eb558e49791cc41b6789a62c6296&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this visualizer with the help of claude code: &lt;a href="https://github.com/maxandrews/Epstein-doc-explorer"&gt;https://github.com/maxandrews/Epstein-doc-explorer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a hosted version linked in the repo, I can't paste it here because reddit inexplicably banned the link sitewide (see my post history for details if you're interested). &lt;/p&gt; &lt;p&gt;It uses the claude agents framework (so you can use your MAX plan inference budget if you have one) to extract relationships triple, tags, and other metadata from the documents, then clusters tags with qwen instruct embeddings, dedupes actor names into an alias table, and serves it all in a nice UI. If you don't have a max plan, you can fork and refactor to use any other capable LLM. &lt;/p&gt; &lt;h1&gt;Analysis Pipeline Features&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/maxandrews/Epstein-doc-explorer#analysis-pipeline-features"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Extraction:&lt;/strong&gt; Uses Claude to extract entities, relationships, and events from documents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Tagging:&lt;/strong&gt; Automatically tags triples with contextual metadata (legal, financial, travel, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tag Clustering:&lt;/strong&gt; Groups 28,000+ tags into 30 semantic clusters using K-means for better filtering&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Entity Deduplication:&lt;/strong&gt; Merges duplicate entities using LLM-based similarity detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Incremental Processing:&lt;/strong&gt; Supports analyzing new documents without reprocessing everything&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Top-3 Cluster Assignment:&lt;/strong&gt; Each relationship is assigned to its 3 most relevant tag clusters&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Visualization Features&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/maxandrews/Epstein-doc-explorer#visualization-features"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interactive Network Graph:&lt;/strong&gt; Force-directed graph with 15,000+ relationships&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Actor-Centric Views:&lt;/strong&gt; Click any actor to see their specific relationships&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Filtering:&lt;/strong&gt; Filter by 30 content categories (Legal, Financial, Travel, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Timeline View:&lt;/strong&gt; Chronological relationship browser with document links&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Viewer:&lt;/strong&gt; Full-text document display with highlighting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Responsive Design:&lt;/strong&gt; Works on desktop and mobile devices&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Optimized:&lt;/strong&gt; Uses materialized database columns for fast filtering&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madmax_br5"&gt; /u/madmax_br5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T05:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozbzpx</id>
    <title>MemLayer, a Python package that gives local LLMs persistent long-term memory (open-source)</title>
    <updated>2025-11-17T09:34:08+00:00</updated>
    <author>
      <name>/u/MoreMouseBites</name>
      <uri>https://old.reddit.com/user/MoreMouseBites</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"&gt; &lt;img alt="MemLayer, a Python package that gives local LLMs persistent long-term memory (open-source)" src="https://b.thumbs.redditmedia.com/BjTeAd0Z3JtiJfDbzczZ1J4L20ond_itkXXAaGvGO-A.jpg" title="MemLayer, a Python package that gives local LLMs persistent long-term memory (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What Memlayer Does&lt;/h1&gt; &lt;p&gt;MemLayer is an open-source &lt;strong&gt;Python package&lt;/strong&gt; that adds persistent, long-term memory to &lt;strong&gt;local LLMs&lt;/strong&gt; and embedding pipelines.&lt;/p&gt; &lt;p&gt;Local models are powerful, but they’re stateless. Every prompt starts from zero.&lt;br /&gt; This makes it difficult to build assistants or agents that remember anything from one interaction to the next.&lt;/p&gt; &lt;p&gt;MemLayer provides a lightweight memory layer that works entirely &lt;strong&gt;offline&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;captures key information from conversations&lt;/li&gt; &lt;li&gt;stores it persistently using &lt;strong&gt;local&lt;/strong&gt; vector + graph memory&lt;/li&gt; &lt;li&gt;retrieves relevant context automatically on future calls&lt;/li&gt; &lt;li&gt;works with any local embedding model (BGE, Instructor, SentenceTransformers, etc.)&lt;/li&gt; &lt;li&gt;does not require OpenAI / cloud APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The workflow:&lt;br /&gt; you send a message → MemLayer saves what matters → later, when you ask something related, the local model answers correctly because the memory layer retrieved the earlier information.&lt;/p&gt; &lt;p&gt;Everything happens locally. No servers, no internet, no external dependencies.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wmmp2zhzis1g1.png?width=3128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=470f1584de96b0f1efef3e7876d43e14760d5a37"&gt;Example workflow for Memlayer&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;MemLayer is perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Users building offline LLM apps or assistants&lt;/li&gt; &lt;li&gt;Developers who want &lt;strong&gt;persistent recall&lt;/strong&gt; across sessions&lt;/li&gt; &lt;li&gt;People running &lt;strong&gt;GGUF models&lt;/strong&gt;, &lt;strong&gt;local embeddings&lt;/strong&gt;, or &lt;strong&gt;on-device inference&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Anyone who wants a memory system without maintaining vector databases or cloud infra&lt;/li&gt; &lt;li&gt;Researchers exploring long-term memory architectures for local models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s lightweight, works with CPU or GPU, and requires no online services.&lt;/p&gt; &lt;h1&gt;Comparison With Existing Alternatives&lt;/h1&gt; &lt;p&gt;Some frameworks include memory components, but MemLayer differs in key ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local-first:&lt;/strong&gt; Designed to run with offline LLMs and embedding models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pure Python + open-source:&lt;/strong&gt; Easy to inspect, modify, or extend.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured memory:&lt;/strong&gt; Combines semantic vector recall with optional graph memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Noise-aware:&lt;/strong&gt; Includes an optional ML-based “is this worth saving?” gate to avoid storing junk.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infrastructure-free:&lt;/strong&gt; No cloud APIs, storage is all local files.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to offer a memory layer you can drop into any &lt;strong&gt;local LLM workflow&lt;/strong&gt; without adopting a large framework or setting up servers.&lt;/p&gt; &lt;p&gt;If anyone has feedback, ideas, or wants to try it with their own local models, I’d love to hear it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/divagr18/memlayer"&gt;https://github.com/divagr18/memlayer&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;PyPI:&lt;/strong&gt; &lt;code&gt;pip install memlayer&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreMouseBites"&gt; /u/MoreMouseBites &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T09:34:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozf9al</id>
    <title>Embedding models have converged</title>
    <updated>2025-11-17T12:42:57+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"&gt; &lt;img alt="Embedding models have converged" src="https://b.thumbs.redditmedia.com/etPbD7zBgdTQ8601uBTw1uabNY5lgHrnCBsAqYFvlYk.jpg" title="Embedding models have converged" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are so many embedding models out there that it’s hard to know which one is actually “the best.” I kept seeing different recommendations, so I got curious and tested them myself. &lt;/p&gt; &lt;p&gt;I ran 13 models on 8 datasets and checked latency, accuracy, and an LLM-judged ELO score. Honestly, the results were not what I expected - most models ended up clustered pretty tightly.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~85% are inside a 50-ELO band&lt;/li&gt; &lt;li&gt;top 4 are ~23.5 ELO apart&lt;/li&gt; &lt;li&gt;rank 1 → 10 is around a 3% gap&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q2e21in1ct1g1.png?width=1810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbef3263bd735ab4bd5eeb7b8cd1d4a057f0ecfd"&gt;https://preview.redd.it/q2e21in1ct1g1.png?width=1810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbef3263bd735ab4bd5eeb7b8cd1d4a057f0ecfd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So now I’m thinking the embedding choice isn’t the thing that moves quality the most. The bigger differences seem to come from other parts of the pipeline: chunking, hybrid search, and reranking.&lt;/p&gt; &lt;p&gt;Full breakdown if you want to look at the numbers: &lt;a href="https://agentset.ai/embeddings"&gt;https://agentset.ai/embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T12:42:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozk1rh</id>
    <title>MiniMax-M2-REAP-172B-A10B-GGUF</title>
    <updated>2025-11-17T16:00:18+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozk1rh/minimaxm2reap172ba10bgguf/"&gt; &lt;img alt="MiniMax-M2-REAP-172B-A10B-GGUF" src="https://external-preview.redd.it/yC7porbEEX7QYKHPcLygWYv7HiHQvUYKMqU4aFgmwiA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdf4b22ec08094ccf73f74b8c2cc5cc286e38a84" title="MiniMax-M2-REAP-172B-A10B-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As in topic. Since Cerebras published the reap, I decided I'd try to get some GGUFs going (since I wanted to use them too).&lt;/p&gt; &lt;p&gt;It has been kind of annoying since apparently Cerebras messed up the tokenizer files (I think they uploaded the GLM tokenizer files by mistake, but I've been to lazy to actually check). Anyways, I restored the tokenizer and the model works quite decently.&lt;/p&gt; &lt;p&gt;Can't do an imatrix right now, so just publishing Q5_K_M quants since it seems like a general use case (and fits in 128 GB RAM). I'm collecting demands if someone wants some specific quants :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ilintar/MiniMax-M2-REAP-172B-A10B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozk1rh/minimaxm2reap172ba10bgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozk1rh/minimaxm2reap172ba10bgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T16:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p02zed</id>
    <title>Built using local Mini-Agent with MiniMax-M2-Thrift on M3 Max 128GB</title>
    <updated>2025-11-18T04:49:55+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p02zed/built_using_local_miniagent_with_minimaxm2thrift/"&gt; &lt;img alt="Built using local Mini-Agent with MiniMax-M2-Thrift on M3 Max 128GB" src="https://external-preview.redd.it/NHQyaGFtemk0eTFnMYxPYLMDTd64wJbXomG_2CQqPnCBfCTtQBpK6YmQSGj_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28dd5cc5018d37e5a756487a2f723a53de3c1446" title="Built using local Mini-Agent with MiniMax-M2-Thrift on M3 Max 128GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/?f=flair_name%3A%22Resources%22"&gt;&lt;/a&gt;Just wanted to bring awareness to &lt;a href="https://github.com/MiniMax-AI/Mini-Agent"&gt;MiniMax-AI/Mini-Agent&lt;/a&gt;, which can be configured to work with a local API endpoint for inference and works really well with, yep you guessed it, &lt;a href="https://huggingface.co/mradermacher/MiniMax-M2-THRIFT-i1-GGUF"&gt;MiniMax-M2&lt;/a&gt;. Here is a guide on how to set it up &lt;a href="https://github.com/latent-variable/minimax-agent-guide"&gt;https://github.com/latent-variable/minimax-agent-guide&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xo6bqlzi4y1g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p02zed/built_using_local_miniagent_with_minimaxm2thrift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p02zed/built_using_local_miniagent_with_minimaxm2thrift/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T04:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozy72c</id>
    <title>Baguettotron, a 321 million parameters generalist Small Reasoning Model (80-layers deep)</title>
    <updated>2025-11-18T01:02:19+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozy72c/baguettotron_a_321_million_parameters_generalist/"&gt; &lt;img alt="Baguettotron, a 321 million parameters generalist Small Reasoning Model (80-layers deep)" src="https://external-preview.redd.it/Y2tKjEjozUln8VtzJeRPc_zDKjaxfbVqC-L3XOBXmQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=153a2353c4c45555147bd60bb8b6ef7ddf0c6d9d" title="Baguettotron, a 321 million parameters generalist Small Reasoning Model (80-layers deep)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baguettotron is a 321 million parameters generalist Small Reasoning Model, trained on 200 billions tokens from SYNTH, a fully open generalist dataset.&lt;/p&gt; &lt;p&gt;Despite being trained on consideraly less data, Baguettotron outperforms most SLM of the same size range on non-code industry benchmarks, providing an unprecedented balance between memory, general reasoning, math and retrieval performance.&lt;/p&gt; &lt;p&gt;The name is both a nod to French origins and to the unusual shape of the model: with 80 layers, Baguettotron is currently the deepest SLM in its size range.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PleIAs/Baguettotron"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozy72c/baguettotron_a_321_million_parameters_generalist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozy72c/baguettotron_a_321_million_parameters_generalist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T01:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozsdbe</id>
    <title>Cerebras REAPs: MiniMax-M2 (25, 30, 40%), Kimi-Linear 30%, more on the way!</title>
    <updated>2025-11-17T21:06:21+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just dropped REAP'd MiniMax-M2 in 3 sizes:&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/MiniMax-M2-REAP-172B-A10B"&gt;https://hf.co/cerebras/MiniMax-M2-REAP-172B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/MiniMax-M2-REAP-162B-A10B"&gt;https://hf.co/cerebras/MiniMax-M2-REAP-162B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/MiniMax-M2-REAP-139B-A10B"&gt;https://hf.co/cerebras/MiniMax-M2-REAP-139B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're running more agentic benchmarks for MiniMax-M2 REAPs, so far we're seeing good accuracy retention, especially at 25 and 30% compression.&lt;/p&gt; &lt;p&gt;We also recently released a Kimi-Linear REAP@30% and it works well for coding and for long-context QA:&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/Kimi-Linear-REAP-35B-A3B-Instruct"&gt;https://hf.co/cerebras/Kimi-Linear-REAP-35B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Meanwhile, folks over at Unsloth were kind to provide GGUFs for a couple REAPs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/unsloth/GLM-4.6-REAP-268B-A32B-GGUF"&gt;https://hf.co/unsloth/GLM-4.6-REAP-268B-A32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/unsloth/Qwen3-Coder-REAP-363B-A35B-GGUF"&gt;https://hf.co/unsloth/Qwen3-Coder-REAP-363B-A35B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working to get a Kimi-K2-Think REAP out, so stay tuned. Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozsdbe/cerebras_reaps_minimaxm2_25_30_40_kimilinear_30/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozsdbe/cerebras_reaps_minimaxm2_25_30_40_kimilinear_30/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozsdbe/cerebras_reaps_minimaxm2_25_30_40_kimilinear_30/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T21:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozobsy</id>
    <title>I miss when it looked like community fine-tunes were the future</title>
    <updated>2025-11-17T18:36:31+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else? There was a hot moment, maybe out of naivety, where fine-tunes of Llama 2 significantly surpassed the original and even began chasing down ChatGPT3. This sub was a flurry of ideas and datasets and had its own minor celebrities with access to impressive but modest GPU farms.&lt;/p&gt; &lt;p&gt;Today it seems like the sub is still enjoying local LLMs but has devolved into begging 6 or 7 large companies into giving us more free stuff, the smallest of which is still worth billions, and celebrating like fanatics when we're thrown a bone.&lt;/p&gt; &lt;p&gt;The harsh reality was that Llama2 was weaker out the box and very easy to improve upon and fine tunes on Llama3 and beyond yielded far less exciting results.&lt;/p&gt; &lt;p&gt;Does anyone else feel the vibe change or am I nostalgic for a short-lived era that never really existed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozobsy/i_miss_when_it_looked_like_community_finetunes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozobsy/i_miss_when_it_looked_like_community_finetunes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozobsy/i_miss_when_it_looked_like_community_finetunes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oziszl</id>
    <title>How come Qwen is getting popular with such amazing options in the open source LLM category?</title>
    <updated>2025-11-17T15:12:06+00:00</updated>
    <author>
      <name>/u/Puzzleheaded_Toe5074</name>
      <uri>https://old.reddit.com/user/Puzzleheaded_Toe5074</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oziszl/how_come_qwen_is_getting_popular_with_such/"&gt; &lt;img alt="How come Qwen is getting popular with such amazing options in the open source LLM category?" src="https://preview.redd.it/ue6rw77n1u1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4b2c8f55f693519a0cd98ab0ba4b27693c9518b" title="How come Qwen is getting popular with such amazing options in the open source LLM category?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To be fair, apart from Qwen, there is also Kimi K2. Why is this uptick in their popularity? Openrouters shows a 20% share of Qwen. The different evaluations certainly favor the Qwen models when compared with Claude and Deepseek. &lt;/p&gt; &lt;p&gt;The main points I feel like working in Qwen's favor are its cheap prices and the open source models. This model doesn't appear to be sustainable however. This will require masssive inflow of resources and talent to keep up with giants like Anthropic and OpenAI or Qwen will fast become a thing of the past very fast. The recent wave of frontier model updates means Qwen must show sustained progress to maintain market relevance. &lt;/p&gt; &lt;p&gt;What's your take on Qwen's trajectory? I'm curious how it stacks up against Claude and ChatGPT in your real-world use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded_Toe5074"&gt; /u/Puzzleheaded_Toe5074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ue6rw77n1u1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oziszl/how_come_qwen_is_getting_popular_with_such/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oziszl/how_come_qwen_is_getting_popular_with_such/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T15:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozo2v8</id>
    <title>Do we rely too much on huggingface? Do you think they’ll eventually regulate open source models? Is there any way to distribute them elsewhere?</title>
    <updated>2025-11-17T18:27:54+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know torrenting may be a thing, but I’m also just curious if anyone knows anything or has any insight.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozo2v8/do_we_rely_too_much_on_huggingface_do_you_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozo2v8/do_we_rely_too_much_on_huggingface_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozo2v8/do_we_rely_too_much_on_huggingface_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozre2i</id>
    <title>NanoGPT 124m from scratch using a 4090 and a billion tokens of Fineweb in a cave with a box of scraps.</title>
    <updated>2025-11-17T20:29:18+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozre2i/nanogpt_124m_from_scratch_using_a_4090_and_a/"&gt; &lt;img alt="NanoGPT 124m from scratch using a 4090 and a billion tokens of Fineweb in a cave with a box of scraps." src="https://external-preview.redd.it/Xkp-uBD1eaeELSsY4T0RZUFyVZGTUIdapJjtKGQFbjY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19349af1d39c198a50fecbc1f1e139ec105a188f" title="NanoGPT 124m from scratch using a 4090 and a billion tokens of Fineweb in a cave with a box of scraps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need a buddy and only have a few hours to make one? &lt;/p&gt; &lt;p&gt;I was recently doing some digging into NanoGPT, Karpathy's couple years old &lt;a href="https://github.com/karpathy/nanoGPT"&gt;repo&lt;/a&gt; to recreate GPT-2 124m using 10 billion tokens of fineweb and 8xA100 40gb over the course of four days.&lt;/p&gt; &lt;p&gt;More recently, I saw that they've started &lt;a href="https://github.com/KellerJordan/modded-nanogpt"&gt;speedrunning efforts&lt;/a&gt; to train the same model to 3.28 loss as fast as possible with 8xH100, and currently the speed record on that setup is less than 3 minutes to train from scratch.&lt;/p&gt; &lt;p&gt;That led me to think... with all of the advancements that have been made in the last few years, how fast could I train the same model to that 3.28 loss range on a single 4090?&lt;/p&gt; &lt;p&gt;The answer? 115 minutes flat. It ran through 0.92 billion tokens in the process, with 130-140k t/s speeds during training.&lt;/p&gt; &lt;p&gt;What does this mean?&lt;/p&gt; &lt;p&gt;If you ever find yourself lonely in a cave with a box of scraps, a 4090, and a billion fineweb tokens... you can build your own teeny-jarvis in a couple hours flat then chat with it. I've provided training code and inference code, and the trained model if you want to mess with it for some odd reason. I set up a little github repo as well, so if you feel like trying your hands at modifying my training run and beating it, drop a PR with your results/log/training run and I'll add it to the speedrun chart:&lt;br /&gt; &lt;a href="https://github.com/Deveraux-Parker/nanoGPT_1GPU_SPEEDRUN"&gt;https://github.com/Deveraux-Parker/nanoGPT_1GPU_SPEEDRUN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I haven't bothered with any posttraining/finetuning/etc etc etc, this is just the base model trained up from nothing. I might go through and add a little instruct tune on top of it so that I can create a teeny little chatgpt. &lt;/p&gt; &lt;p&gt;Here's the list of things it's implementing:&lt;br /&gt; &lt;strong&gt;Computation &amp;amp; Precision Optimizations&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;FP8 Quantization&lt;/strong&gt; - 8-bit floating-point numbers (float8) for matrix multiplications instead of the usual 16 or 32-bit. This cuts memory use and speeds up math operations dramatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mixed Precision Training (bfloat16)&lt;/strong&gt; - Most computations happen in bfloat16, which is faster than float32 while maintaining good numerical stability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Triton Kernels&lt;/strong&gt; - Hand-written GPU kernels for specific operations like symmetric matrix multiplication (X·X^T), which are faster than PyTorch's default implementations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;torch.compile&lt;/strong&gt; - PyTorch 2.0's JIT compilation that fuses operations and optimizes the computational graph.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flash Attention&lt;/strong&gt; - Ultra-fast attention implementation that reduces memory usage and speeds up the attention mechanism.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Novel Optimizer &amp;amp; Training Techniques&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Muon Optimizer&lt;/strong&gt; - A custom momentum-based optimizer that uses orthogonalization (keeping gradient directions independent) for better convergence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Polar Express Orthogonalization&lt;/strong&gt; - A specific algorithm to maintain orthogonality in the Muon optimizer's updates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NorMuon Variance Estimator&lt;/strong&gt; - Adaptive second moment estimation that helps Muon scale gradients appropriately.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Optimizers&lt;/strong&gt; - Using Adam for embeddings/scalars and Muon for weight matrices, each optimized for their parameter type.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alternating Optimizer Steps&lt;/strong&gt; - Muon runs every other step, both optimizers on odd steps, reducing computational overhead.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gradient Accumulation&lt;/strong&gt; - Accumulating gradients over 32 micro-batches to simulate larger batch sizes without running out of memory.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architecture Innovations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;YaRN (Yet another RoPE extensioN)&lt;/strong&gt; - Extends the context length capability of Rotary Position Embeddings beyond what the model was trained on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RoPE (Rotary Position Embeddings)&lt;/strong&gt; - More efficient positional encoding than absolute positions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RMS Normalization&lt;/strong&gt; - Simpler and faster than LayerNorm while being equally effective.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Squared ReLU Activation&lt;/strong&gt; - Using ReLU(x)² instead of GELU, which is faster and works well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skip Connections with Learnable Gates&lt;/strong&gt; - U-Net-style architecture where early layers connect to later layers through learned gates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value Embeddings&lt;/strong&gt; - Separate embedding tables that inject information directly into attention values.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smear Gating&lt;/strong&gt; - Mixes each token with the previous token using a learned gate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backout Connections&lt;/strong&gt; - Subtracts certain layer outputs to prevent feature redundancy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attention Gating&lt;/strong&gt; - Per-head gates that learn to selectively use attention outputs.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Learning Rate &amp;amp; Schedule Optimizations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Custom LR Multipliers&lt;/strong&gt; - Different learning rates for embeddings (75x), scalars (5x), etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Weight Decay Multipliers&lt;/strong&gt; - Different regularization strength for different parameter types.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Warmup-Stable-Decay Schedule&lt;/strong&gt; - Linear warmup (100 steps), stable plateau (80% of training), then cosine decay.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Muon Momentum&lt;/strong&gt; - Momentum coefficient that changes during training (0.85→0.95→0.85).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive Hyperparameter Tuning&lt;/strong&gt; - Automatically adjusts learning rate and weight decay based on train/val loss dynamics.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Memory &amp;amp; Data Optimizations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Expandable Memory Segments&lt;/strong&gt; - PyTorch memory allocator setting that reduces fragmentation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kernel Warmup&lt;/strong&gt; - Pre-compiling and warming up kernels before actual training to avoid first-step slowdown.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asynchronous Data Loading&lt;/strong&gt; - Background threads preload the next data shard while training continues.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BOS-Aligned Batching&lt;/strong&gt; - Sequences are aligned to document boundaries (BOS tokens) for more natural training.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pin Memory&lt;/strong&gt; - Keeps data in page-locked memory for faster CPU→GPU transfers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Non-Blocking Transfers&lt;/strong&gt; - Async GPU transfers that overlap with computation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;set_to_none=True&lt;/strong&gt; - More efficient way to zero gradients than setting them to zero tensors.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Training Efficiency Tricks&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Variable Attention Window Sizes&lt;/strong&gt; - Different layers use different block masking sizes (some see more context, some less).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logit Capping&lt;/strong&gt; - Applies 30·sigmoid(logits/7.5) to prevent extreme values.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vocabulary Size Rounding&lt;/strong&gt; - Rounds vocab to multiples of 128 for better GPU utilization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strategic Initialization&lt;/strong&gt; - Zero initialization for output projections, uniform bounded for inputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checkpoint Resumption&lt;/strong&gt; - Can pause and resume training without losing progress.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early Stopping&lt;/strong&gt; - Automatically stops when target validation loss is reached.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frequent Checkpointing&lt;/strong&gt; - Saves model every validation step to prevent data loss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Gradient Zeroing&lt;/strong&gt; - Only zeroes gradients after they're used, not before.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/DevParker/NanoGPT-124m-In-A-Cave-With-A-Box-Of-Scraps/blob/main/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozre2i/nanogpt_124m_from_scratch_using_a_4090_and_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozre2i/nanogpt_124m_from_scratch_using_a_4090_and_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T20:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozu5v4</id>
    <title>20,000 Epstein Files in a single text file available to download (~100 MB)</title>
    <updated>2025-11-17T22:14:12+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've processed all the text and image files (~25,000 document pages/emails) within individual folders released last friday into a two column text file. I used Googles tesseract OCR library to convert jpg to text. &lt;/p&gt; &lt;p&gt;You can download it here: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I uploaded it yesterday, but some of files were incomplete. This version is full. For each document, I've included the full path to the original google drive folder from House oversight committee so you can link and verify contents.&lt;/p&gt; &lt;p&gt;I used mistral 7b to extract entities and relationships and build a basic Graph RAG. There are some new &amp;quot;associations&amp;quot; that have not been reported in the news but couldn't find any breakthrough content. Also my entity/relationship extraction was quick and dirty. Sharing this dataset for people interested in getting into RAG and digging deeper to get more insight that what meets the eye.&lt;/p&gt; &lt;p&gt;In using this dataset, please be sensitive to the privacy of the people involved (and remember that many of these people were certainly not involved in any of the actions which precipitated the investigation.) - Quoted from Enron Email Dataset release&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T22:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
</feed>
