<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-29T19:15:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qqgf00</id>
    <title>This Week In AI Agents: Open Source Edition</title>
    <updated>2026-01-29T18:18:03+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqgf00/this_week_in_ai_agents_open_source_edition/"&gt; &lt;img alt="This Week In AI Agents: Open Source Edition" src="https://b.thumbs.redditmedia.com/ZJUXEDFCX3Qr_QHS0cmwatek2MSoWewvJGxrmbP55SY.jpg" title="This Week In AI Agents: Open Source Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on AI agents. Here are the local highlights from this week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EvoCUA - #1 open-source computer use agent on OSWorld (56.7%)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Evolutionary framework: synthetic task generation + sandbox rollouts + learning from failures&lt;/p&gt; &lt;p&gt;- Available in 32B and 8B variants under Apache 2.0&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/meituan/EvoCUA-32B-20260105"&gt;Model Weights&lt;/a&gt; | &lt;a href="https://huggingface.co/papers/2601.15876"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/meituan/EvoCUA"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4et6pg9yxbgg1.png?width=906&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbbeb0508417fc42777bebc37646772927178542"&gt;https://preview.redd.it/4et6pg9yxbgg1.png?width=906&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbbeb0508417fc42777bebc37646772927178542&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-TTS - Open-source TTS with voice cloning and design&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 3-second voice cloning, 10 languages, 97ms first-packet latency&lt;/p&gt; &lt;p&gt;- 0.6B and 1.7B variants under Apache 2.0&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts?spm=a2ty_o06.30285417.0.0.2994c921a3PoQo"&gt;Model&lt;/a&gt;s | &lt;a href="https://qwen.ai/blog?id=qwen3tts-0115"&gt;Writeup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ecra7nlzxbgg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f70266a19af6aa34090c6960fe25efd2ceebfb71"&gt;https://preview.redd.it/ecra7nlzxbgg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f70266a19af6aa34090c6960fe25efd2ceebfb71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Moltbot - Open-source personal AI assistant that runs locally&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Persistent memory, WhatsApp/Telegram/Discord integration, extensible skills&lt;/p&gt; &lt;p&gt;- Runs on your machine with Anthropic/OpenAI/local models&lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.molt.bot/"&gt;Moltbot&lt;/a&gt; | &lt;a href="https://x.com/omooretweets/status/2015618038088024164"&gt;Discussion&lt;/a&gt;(Video Source) | &lt;a href="https://x.com/0xsammy/status/2015562918151020593"&gt;Major Security Issue&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qqgf00/video/oqxlsgwixbgg1/player"&gt;https://reddit.com/link/1qqgf00/video/oqxlsgwixbgg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VIGA - Vision-as-inverse-graphics agent for 3D reconstruction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Converts images to editable Blender code through multimodal reasoning&lt;/p&gt; &lt;p&gt;- +124.70% improvement on BlenderBench&lt;/p&gt; &lt;p&gt;- &lt;a href="https://fugtemypt123.github.io/VIGA-website/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2601.11109"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/Fugtemypt123/VIGA"&gt;Code&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/DietCoke4671/BlenderBench"&gt;Benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qqgf00/video/a901q7okxbgg1/player"&gt;https://reddit.com/link/1qqgf00/video/a901q7okxbgg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LingBot-VLA - VLA foundation model with 20k hours of real robot data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- First empirical evidence VLA models scale with massive real-world data&lt;/p&gt; &lt;p&gt;- 261 samples/sec/GPU throughput, open weights&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/papers/2601.18692"&gt;Paper&lt;/a&gt; | &lt;a href="https://technology.robbyant.com/lingbot-vla"&gt;Project Page&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/robbyant/lingbot-vla"&gt;Models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qqgf00/video/17j9dlblxbgg1/player"&gt;https://reddit.com/link/1qqgf00/video/17j9dlblxbgg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PersonaPlex - NVIDIA's full-duplex conversational AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Persona control through text prompts + voice conditioning&lt;/p&gt; &lt;p&gt;- Built on Moshi architecture, MIT license&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/NVIDIA/personaplex"&gt;GitHub&lt;/a&gt; | &lt;a href="https://research.nvidia.com/labs/adlr/personaplex/"&gt;Project Page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qqgf00/video/38mq0tfmxbgg1/player"&gt;https://reddit.com/link/1qqgf00/video/38mq0tfmxbgg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/autopiloteverything/p/the-agentic-edge-2-power-without?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more agent demos, research, tools, and more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqgf00/this_week_in_ai_agents_open_source_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqgf00/this_week_in_ai_agents_open_source_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqgf00/this_week_in_ai_agents_open_source_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T18:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpfse6</id>
    <title>Run Kimi K2.5 Locally</title>
    <updated>2026-01-28T16:17:45+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt; &lt;img alt="Run Kimi K2.5 Locally" src="https://preview.redd.it/rxqfj5os74gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2606f30079a77f14bb28c31413be651c092abaa9" title="Run Kimi K2.5 Locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-K2.5 achieves SOTA performance in vision, coding, agentic and chat tasks. &lt;/p&gt; &lt;p&gt;The 1T parameter hybrid reasoning model requires 600GB of disk space, while the quantized &lt;strong&gt;Unsloth Dynamic 1.8-bit&lt;/strong&gt; version reduces this to &lt;strong&gt;240GB (-60% size).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Kimi-K2.5-GGUF"&gt;&lt;strong&gt;Kimi-K2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide:&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/kimi-k2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/kimi-k2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rxqfj5os74gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T16:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8z7z</id>
    <title>Field Report: What leadership actually *treats AI as (Notes from a Dev)</title>
    <updated>2026-01-29T13:46:29+00:00</updated>
    <author>
      <name>/u/MitsotakiShogun</name>
      <uri>https://old.reddit.com/user/MitsotakiShogun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Hype. Hype &amp;gt; Substance. In order to woo stockholders. That's it.&lt;/p&gt; &lt;p&gt;Hi fellow llamas,&lt;/p&gt; &lt;p&gt;I read &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qpsgzr/field_report_what_leadership_actually_thinks_ai/"&gt;this pretty decent post&lt;/a&gt; and while I do agree with lots of the views in that post (even though it's not meant for hobbyists), I thought I'd chime in with a few more thoughts about leadership, and stuff. But before that, let me share some background.&lt;/p&gt; &lt;p&gt;I work at a big company (top 500 by market cap, world), one that actually used AI (which its different names, like statistical/machine learning, NLP, etc) from the early '90s in high-impact domains (adjacent to finance or law, but not quite). The first department head had a published paper on Bayesian statistics for NLP before I was born, and I don't think I understand all of it even now. Decades of NLP work created quite a few useful products, most of which had narrow scope for the AI parts, and the rest was mostly engineering effort and human expert work (reviewing/fixing stuff). We had text-generation models in production at least 4-5 months before ChatGPT (not sure how much more, that's when I transferred from a different business unit).&lt;/p&gt; &lt;p&gt;Fast-forward to today, and management is basically a joke. The last capable (aka engineer/scientist) department head was fired ~3 years ago by the young CTO (who was a Consulting Boy‚Ñ¢), and the interim department heads were also incapable and had short tenures. The current CTO does seem capable and knowledgeable (another engineer), but the middle layers of management are still the same, with most capable people leaving to the bigger firms, and the less capable getting promoted. So let's view &lt;em&gt;how&lt;/em&gt; this happens.&lt;/p&gt; &lt;p&gt;Last year I've been in probably a thousand meetings (like most tech folk, I guess) with managers of all levels, from CTO to managers-in-name only (e.g. directors without any (in)direct reports), to talk about our ongoing AI projects, planned projects, project proposals. The proposals that went through were all about &amp;quot;agents&amp;quot;. If something contained the word, it's probability of getting approved was 418967936.71% higher. I remember a meeting when a scientist and an engineer presented what was essentially an LLM-assisted exhaustive search (multiple data sources) and generation implementation with planning, refinement, draft, human feedback, and final output... and management (CTO, department head, and a couple director) was asking why they didn't use &amp;quot;deep search&amp;quot; and how it can be made agentic. Zero questions about potential issues, zero questions about costs, zero questions about quality. The scientist was so perplexed with those questions, not understand why you would let the LLM decide &lt;em&gt;if it wants&lt;/em&gt; to use search or which databases to query (rather than being forced to use it, and query all databases).&lt;/p&gt; &lt;p&gt;Of course, the problem doesn't stop with management not understanding, and thus promoting the wrong projects and focusing on the wrong metrics (&amp;quot;AI adoption&amp;quot; instead of &amp;quot;revenue increase&amp;quot; / &amp;quot;cost reduction&amp;quot; / ...). This also enables a culture that lets engineers give in to their bad habits and temptations. I know because I've been there too, and it basically boils down to: &amp;quot;Oh look, a shiny new framework! Let's replace all our battle-tested, well-documented tools with this thingy that a single person created in a few months, because it's popular and might be in demand for new jobs and I can put it on my CV&amp;quot;. The newest CTO is trying to curb this trend with a bigger focus on products (which sadly disproportionately affected research output, e.g. publications, open-sourcing), but the middle managers are also trying to showcase the work their teams are doing and thus aim for the flashy stuff that they don't really understand. I've lost track of how many times I've heard my manager speak of using AI in ways that simply don't make any sense.&lt;/p&gt; &lt;p&gt;Perhaps the easiest way to tell is the number of new projects that were started versus what made it in production versus what has &amp;gt;10 users after a year. All AI/ML projects had low success rates (at least for individual experiments, if you hacked at a problem for months and collected data then the rate was much higher), but last year the number of employees trended downwards, the number of projects shot up, and the number of projects that get discarded (decommissioned, merged into others, etc) is also higher than ever.&lt;/p&gt; &lt;p&gt;So when that other post said to not over-engineer solutions when &amp;quot;a script will do&amp;quot;, it wasn't just fluff, it's a real issue that in the past was kept in check by management that &lt;del&gt;didn't butt in too much&lt;/del&gt; trusted its experts, and senior engineers that were too &lt;del&gt;grumpy&lt;/del&gt; uhm... &lt;del&gt;lazy to try to anything new&lt;/del&gt; no, wait... focused on what mattered. You don't need a fucking observability platform and AI code reviews / automated PRs when you cannot even use the &lt;code&gt;logging&lt;/code&gt; library. You don't need the most expensive LLM agents when your prompts writer doesn't even know what templating is, and instead of using structured generation or function calling he asks the LLM to reply with &lt;code&gt;&amp;lt;answer&amp;gt;yes|no&amp;lt;/answer&amp;gt;&lt;/code&gt; which is then parsed without even using regex. And I don't need to come back after a two week vacation to see half my code &amp;quot;refactored&amp;quot; by a dude vibe-coding everything four weeks before the production release deadline.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Sorry, this turned into a rant quicker than I realize. To re-iterate: * upper management tries to appeal to stockholders with hype chasing * middle management tries to appeal to upper management with hype chasing * all management focuses on wrong metrics (e.g. usage of AI copilot, how many products had AI integrated into them) * engineers try to appeal to middle management with hype chasing and also play with new fancy tech * talented folks are leaving for bigger/better companies while the &amp;quot;meh&amp;quot; people remain and get promoted to higher roles and management * proper engineering culture takes a back seat because nobody cares anymore since no incentives promote it&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;AI disclaimer: 100% of this post was hand-typed. Because &lt;del&gt;I'm stupid and like to waste my time on Reddit&lt;/del&gt; thoughts matter more than formatting, but I know how much y'all love your emojis, so here's your daily dosage: ‚úÖüåàü¶Ñüå∏üå∫üåªüåºüå∑üåπüçÄüå¥üåµüå≤üå≥üçéüçèüçêüçäüçãüçåüçâüçáüçìü´êüçàüçíüçëü•≠üççü••ü•ùüçÖüçÜü•ëü•¶ü•¨ü•íüå∂Ô∏èü´ëüåΩü•ïü´íüßÑüßÖü•îüç†ü•êü•Øüçûü•ñü•®üßÄü•ö‚ú®&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MitsotakiShogun"&gt; /u/MitsotakiShogun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8z7z/field_report_what_leadership_actually_treats_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8z7z/field_report_what_leadership_actually_treats_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8z7z/field_report_what_leadership_actually_treats_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T13:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqffrz</id>
    <title>Any good open source of project Genie?</title>
    <updated>2026-01-29T17:44:11+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqffrz/any_good_open_source_of_project_genie/"&gt; &lt;img alt="Any good open source of project Genie?" src="https://preview.redd.it/vyvj84sesbgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba26b08643bbdf3a3366873bff375487a5ee2bc8" title="Any good open source of project Genie?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyvj84sesbgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqffrz/any_good_open_source_of_project_genie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqffrz/any_good_open_source_of_project_genie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T17:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpqlfj</id>
    <title>768Gb "Mobile" AI Server Follow-Up Part 1, Look Inside</title>
    <updated>2026-01-28T22:44:03+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"&gt; &lt;img alt="768Gb &amp;quot;Mobile&amp;quot; AI Server Follow-Up Part 1, Look Inside" src="https://external-preview.redd.it/b3IzbXRvY3BwNWdnMU2mkuU7oHD8qNQyskshMOF3z-mD-pRqGUpyoVD2VUXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b6cba6ca164006c230852989fb885c68feec072" title="768Gb &amp;quot;Mobile&amp;quot; AI Server Follow-Up Part 1, Look Inside" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Y'all,&lt;/p&gt; &lt;p&gt;The post I made about the AI server got a lot of buzz, so I decided to do a follow up with some video on the project. Because of reddit's video upload restrictions, I'll have to upload them in separate posts with slightly different focuses, but I've uploaded the full (and higher quality) version to Youtube. Taking the video from 1080p to 720p to meet reddit's video size requirements kinda messed up visibility on the screen record in one of the later parts, so I'll leave a link to the full video here for convenience, otherwise the other parts should get posted here shortly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/TJOKEFdCkv0"&gt;https://youtu.be/TJOKEFdCkv0&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This part primarily focuses on providing some background context on how we came to the W200 in the first place, what it solved for us, and a look inside the unit.&lt;/p&gt; &lt;p&gt;Spec summary:&lt;/p&gt; &lt;p&gt;512Gb DDR4, 256GB VRAM (8x3090+2x5090), 64 core Threadripper Pro 3995WX&lt;/p&gt; &lt;p&gt;Case: Core W200&lt;/p&gt; &lt;p&gt;Appreciate all of the comments and responses on the last post, I've never done anything like this before so I apologize if things are not more polished, attention normally isn't my thing so while the volume of feedback was a little overwhelming the interest was very much encouraging. It seems like every other day we see people post builds here composed of top of the line enterprise hardware with sunken costs reaching tens of thousands of dollars, so I think it can make a difference to just highlight what can be possible with a little ingenuity, consumer grade components, and a more relatively &amp;quot;realistic&amp;quot; budget (in this case, around ~17k usd). Keep this figure in mind when comparing cost:value to these other workstations and their specs/performance capability/creative potential, because I do think this illustrates that effective AI hosting can be more than just throwing money at the problem. Whether someone is working with 100$ or 100k$, focusing on innovative problem solving, pushing optimization limits, and just seeing what can be possible with what's currently available is an order of magnitude more exciting and interesting to see than a squeaky clean $50,000 supercomputer with specialized hardware that very few people will ever get to see in-person within their lifetime posted by someone asking the same question asked since the dawn of time, &amp;quot;what should I do with this?&amp;quot;. Ultimately the interest for experimentation and trying new approaches is what keeps this hobby (local AI) alive and relevant, and imo will be our best counterbalance to the complications that closed-model AI companies impose as we move forward.&lt;/p&gt; &lt;p&gt;Questions welcome.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/trvmg2cpp5gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T22:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpyxfk</id>
    <title>Reasoning Devstral 2</title>
    <updated>2026-01-29T04:42:29+00:00</updated>
    <author>
      <name>/u/Front_Eagle739</name>
      <uri>https://old.reddit.com/user/Front_Eagle739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fun fact! You can actually make devstral 2 123B &amp;amp; Devstral 24B reason! Accidently had a reasoning forcing jinja template on for another model when I started testing the mlx version of this thing with a couple of reasoning effot = extra high statements in my system prompt because I really wanted more reasoning out of the last model I was using and havving forgotten about that tried devstral 2 and got 2 minutes of reasoning before it answered my test question.&lt;/p&gt; &lt;p&gt;Turns out they are both hybrid reasoners if you put {%- set reasoning_content = 'High' %} in the jinja. Nice clean logical reasoning as well. That's actually fixed my main issue with these models, sometimes you just really need that extra consistency.&lt;/p&gt; &lt;p&gt;Did everybody else know this and I just missed it somehow?&lt;/p&gt; &lt;p&gt;Edit. Seems the smaller one may have some difficulty exiting the thinking, at least with some sampler settings. Big one seems fine though. Quality of response is definitely going way up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Front_Eagle739"&gt; /u/Front_Eagle739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T04:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqakid</id>
    <title>Anyone see the new Acree models?</title>
    <updated>2026-01-29T14:49:48+00:00</updated>
    <author>
      <name>/u/EuphoricPenguin22</name>
      <uri>https://old.reddit.com/user/EuphoricPenguin22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Trinity-Large-Preview"&gt;https://huggingface.co/arcee-ai/Trinity-Large-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;400B w/ 13B active for the large preview model. Free right now via API on OpenRouter (or the Apache 2.0 weights on HuggingFace).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EuphoricPenguin22"&gt; /u/EuphoricPenguin22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqakid/anyone_see_the_new_acree_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqakid/anyone_see_the_new_acree_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqakid/anyone_see_the_new_acree_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T14:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq0qut</id>
    <title>I built an open-source, multi-agent alternative to OpenAI Prism for research workflows (Verification Agent + LaTeX + PDF)</title>
    <updated>2026-01-29T06:14:18+00:00</updated>
    <author>
      <name>/u/Inside-Scratch4</name>
      <uri>https://old.reddit.com/user/Inside-Scratch4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on an open-source project called &lt;strong&gt;Prismer&lt;/strong&gt; to tackle the mess that is the current academic workflow.&lt;/p&gt; &lt;p&gt;Like many of you, I found that using generic LLMs for research often leads to hallucinations, especially with citations. And relying on closed ecosystems like OpenAI‚Äôs Prism wasn‚Äôt ideal for privacy or customization.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Prismer&lt;/strong&gt;, an all-in-one platform that integrates: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Native PDF Reader&lt;/strong&gt;: With bi-directional citation graphs. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Citation Verification Agent&lt;/strong&gt;: Uses multiple agents to cross-check references against real databases (arXiv, etc.) to prevent LLM hallucinations. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jupyter Integration&lt;/strong&gt;: For data analysis right next to your writing. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;LaTeX Editor&lt;/strong&gt;: With real-time preview.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs completely open-source (MIT License). The goal is to have a modular system where you can swap in your own models or agents.&lt;/p&gt; &lt;p&gt;I‚Äôd love to get some feedback from this community on the agent orchestration part specifically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Prismer-AI/Prismer"&gt;https://github.com/Prismer-AI/Prismer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inside-Scratch4"&gt; /u/Inside-Scratch4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T06:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqc1fx</id>
    <title>Run Local LLMs with Claude Code &amp; OpenAI Codex</title>
    <updated>2026-01-29T15:43:54+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqc1fx/run_local_llms_with_claude_code_openai_codex/"&gt; &lt;img alt="Run Local LLMs with Claude Code &amp;amp; OpenAI Codex" src="https://preview.redd.it/46s35meo6bgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a5c4d381bee94911400914067a46722f3b88a4a" title="Run Local LLMs with Claude Code &amp;amp; OpenAI Codex" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This step-by-step guide shows you how to connect open LLMs to Claude Code and Codex entirely locally. &lt;/p&gt; &lt;p&gt;Run using any open model like DeepSeek, Qwen, Gemma etc.&lt;/p&gt; &lt;p&gt;Official Blog post - &lt;a href="https://unsloth.ai/docs/basics/claude-codex"&gt;https://unsloth.ai/docs/basics/claude-codex&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/46s35meo6bgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqc1fx/run_local_llms_with_claude_code_openai_codex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqc1fx/run_local_llms_with_claude_code_openai_codex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T15:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq29ab</id>
    <title>Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description.</title>
    <updated>2026-01-29T07:40:59+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"&gt; &lt;img alt="Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description." src="https://external-preview.redd.it/NGZpZjMyeWJwOGdnMSyVzMY88rGIMLP8wkCsphE6OdlDVcwcn9ECGq-UAL8f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fc37bfffb290e2fd1af99cd3cb5f3e90281514f" title="Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The system works by having a pool of 200 spell components like explosive or change color. A LLM then converts each word into a set of component instructions.&lt;/p&gt; &lt;p&gt;For example &amp;quot;explode&amp;quot; = explosive + change color + apply force.&lt;/p&gt; &lt;p&gt;This means we can have a system that can generate a spell for literally any word.&lt;/p&gt; &lt;p&gt;Stick based music was made with Suno.&lt;/p&gt; &lt;p&gt;It's still early Alpha, but if you want to help me break it or try to find hidden spells, come join the Discord: &lt;a href="https://discord.com/invite/VjZQcjtfDq"&gt;https://discord.com/invite/VjZQcjtfDq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hbq4wsxbp8gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T07:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqhhtx</id>
    <title>Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù</title>
    <updated>2026-01-29T18:56:08+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"&gt; &lt;img alt="Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù" src="https://external-preview.redd.it/NW03ZGMyazI1Y2dnMWh2gxSpyeR6q2IEmV4jHAJM791DDo_e5MvHim0gQe4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43d352ce0d8764709982770e551c498fa8279ecc" title="Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wd12dl725cgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T18:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqf86g</id>
    <title>New 96GB Rig, Would Like Advice</title>
    <updated>2026-01-29T17:36:44+00:00</updated>
    <author>
      <name>/u/DonkeyBonked</name>
      <uri>https://old.reddit.com/user/DonkeyBonked</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqf86g/new_96gb_rig_would_like_advice/"&gt; &lt;img alt="New 96GB Rig, Would Like Advice" src="https://preview.redd.it/rueq6u13rbgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01620eaa8815dbe2e487298e3ceaa16013ce8d3f" title="New 96GB Rig, Would Like Advice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, I know some people are not fans of these kinds of posts, but I am asking for this advice in all sincerity. I have done tons of research myself, I did not by hardware with no idea what to do with it, I would just like some advice from more experienced people to hopefully get on the right track sooner, maybe avoid mistakes I'm not aware of.&lt;/p&gt; &lt;p&gt;First, my past experience: I've been running my laptop with an eGPU to get to 40GB VRAM for a while, and I have found for my personal use cases, this has let me run 30B models at decent speeds with decent results, but nothing too serious because it seemed to be a sweet spot where I could get a 30B model to code with a decent context window, but if I started adding agents to it, I lost context, lost model quality, and had to sacrifice to fit even a decent amount into my VRAM. Plus, my laptop GPU (Turing RTX 5000 16GB) was decent, but a bottleneck. I pretty much have stuck to llama.cpp and ComfyUI, nothing exceptional.&lt;/p&gt; &lt;p&gt;Today, I just finally brought the machine I've been working on for months to life! I'm waiting on a few last cables to clean it up so I can add the last GPU, but that should be here in a couple of days.&lt;/p&gt; &lt;p&gt;My new system isn't exactly the GOAT or anything, I know it's kind of older but, it's new and good for me. My setup will run 4x RTX 3090 24GB and I have an old RX 570 4GB as the actual display driver for now. I got 3 of the 3090s running but like I said, the 4th will be added in a couple of days. I needed to order a different riser and I'm still waiting on my OCuLink adapter so I can move the display card out of my PCI-E x16 slot. I have 128GB of DDR4 and an AMD EPYC 7502 CPU. I managed to score some cheap 4TB Samsung EVO 990 Plus for $180 each before prices went insane, so I'll have plenty of storage I think, I could put 12TB in the dedicated NVME slots on my motherboard.&lt;/p&gt; &lt;p&gt;I'm building this on the Huananzhi H12D-8D with the AST2500 BCM Module. I &amp;quot;think&amp;quot; I've got the board setup correctly, Re-Size BAR and IOMMU Enabled, etc., though I am still combining through and learning this board. I don't have any NVLink adapters.&lt;/p&gt; &lt;p&gt;So here's where I need advice:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I would like to run a multi-agent, multi-model stack. Something like Nemotron 3 Nano 30B + Qwen 3 Coder 30B Instruct + multiple agents tasked to make sure the models follow the workflow, and I'd like to know if anyone has experience running such a setup, and if so, what agents worked best together?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The end goal is primarily autonomous coding, where I can create a flow chart, design an app, give it a layout, and have the AI build it autonomously without me needing to keep prompting it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I plan to run this like a private LLM server, and that got me thinking ü§î (dangerous). I would like to learn how to build multi-user LLM servers where there's a que system for prompts and the system can keep VRAM clear between users. I have a friend who really likes some if the models I've customized and wants to use them, but this will get into model switching and VRAM management that I'm not familiar with, so I was wondering if I should be looking at a different framework? Would vLLM be better or faster for this? I heard it can support pipeline parallelism now, but I'm not even sure how necessary that is with this kind of setup. I've been using an eGPU so it was necessary before, but would this setup be fine without NVLink now?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I would like to make my own LoRAs and fine tune smaller models myself, but I'm not sure how viable my hardware is for this and was wondering if anyone here has experience with this and could advise? I did some research, but didn't get too deep into it because I lacked the hardware (still might?)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If I want to just straight run an LLM, one that maximizes use of the new hardware, I was wondering what people's experience was with the best coding model available that would run with at least 256K context on 96GB of VRAM?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;A lot of new models have dropped recently that I haven't had much time to test and I feel like I'm falling behind. I've never run much more than 30B models at Q8 quants, so I really don't know what models have lower quants that are actually viable for coding. I've pretty much stuck to Q8 models and Q8 KV, so I have little experience beyond that.&lt;/p&gt; &lt;p&gt;Also, I can add more GPUs. I plan to add at least 3 more and switch to USB for my display at some point. So before I need to start getting creative, I think I can get a bit more VRAM depending on what cards I can manage. I'm not sure I can pull off anymore of the 3090s, they're getting hard to find deals on. If there's a sweet spot I can pull off without slowing down the performance, I'm definitely open to suggestions on possible cards to add.&lt;/p&gt; &lt;p&gt;Thanks in advance for anyone who is willing to give advice on this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonkeyBonked"&gt; /u/DonkeyBonked &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rueq6u13rbgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqf86g/new_96gb_rig_would_like_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqf86g/new_96gb_rig_would_like_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T17:36:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq64rx</id>
    <title>Kimi K2.5, a Sonnet 4.5 alternative for a fraction of the cost</title>
    <updated>2026-01-29T11:31:03+00:00</updated>
    <author>
      <name>/u/Grand-Management657</name>
      <uri>https://old.reddit.com/user/Grand-Management657</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes you read the title correctly. Kimi K2.5 is THAT good.&lt;/p&gt; &lt;p&gt;I would place it around Sonnet 4.5 level quality. It‚Äôs great for agentic coding and uses structured to-do lists similar to other frontier models, so it‚Äôs able to work autonomously like Sonnet or Opus.&lt;/p&gt; &lt;p&gt;It's thinking is very methodical and highly logical, so its not the best at creative writing but the tradeoff is that it is very good for agentic use.&lt;/p&gt; &lt;p&gt;The move from K2 -&amp;gt; K2.5 brought multimodality, which means that you can drive it to self-verify changes. Prior to this, I used antigravity almost exclusively because of its ability to drive the browser agent to verify its changes. This is now a core agentic feature of K2.5. It can build the app, open it in a browser, take a screenshot to see if it rendered correctly, and then loop back to fix the UI based on what it &amp;quot;saw&amp;quot;. Hookup playwright or vercel's browser-agent and you're good to go.&lt;/p&gt; &lt;p&gt;Now like I said before, I would still classify Opus 4.5 as superior outside of JS or TS environments. If you are able to afford it you should continue using Opus, especially for complex applications. &lt;/p&gt; &lt;p&gt;But for many workloads the best economical and capable pairing would be Opus as an orchestrator/planner + Kimi K2.5 as workers/subagents. This way you save a ton of money while getting 99% of the performance (depending on your workflow).&lt;/p&gt; &lt;p&gt;+ You don't have to be locked into a single provider for it to work.&lt;/p&gt; &lt;p&gt;+ Screw closed source models.&lt;/p&gt; &lt;p&gt;+ Spawn hundreds of parallel agents like you've always wanted WITHOUT despawning your bank account.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Btw this is coming from someone who very much disliked GLM 4.7 and thought it was benchmaxxed to the moon&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grand-Management657"&gt; /u/Grand-Management657 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq64rx/kimi_k25_a_sonnet_45_alternative_for_a_fraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq64rx/kimi_k25_a_sonnet_45_alternative_for_a_fraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq64rx/kimi_k25_a_sonnet_45_alternative_for_a_fraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq401x</id>
    <title>I built an open-source, local-first voice cloning studio (Qwen3-TTS + Whisper)</title>
    <updated>2026-01-29T09:26:48+00:00</updated>
    <author>
      <name>/u/jamiepine</name>
      <uri>https://old.reddit.com/user/jamiepine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on an open-source project called Voicebox.&lt;/p&gt; &lt;p&gt;Qwen3-TTS blew my mind when it dropped, crazy good cloning from seconds of audio, low latency, and open. I started playing around, but got annoyed re-cloning the same voices every session. So I built a quick saver for profiles... and it snowballed into &lt;strong&gt;Voicebox&lt;/strong&gt;, my attempt at the &amp;quot;Ollama for voice.&amp;quot;&lt;/p&gt; &lt;p&gt;It's a native desktop app (Tauri/Rust/Python, super lightweight‚Äîno Electron bloat or Python setup for users). Everything local, private, offline.&lt;/p&gt; &lt;p&gt;Main bits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clone voices instantly with Qwen3-TTS (single or multi-sample for better quality)&lt;/li&gt; &lt;li&gt;DAW-like multi-track timeline to compose conversations/podcasts/narratives&lt;/li&gt; &lt;li&gt;In-app system audio/mic recording + Whisper transcription&lt;/li&gt; &lt;li&gt;REST API + one-click local server for integrating into games/apps/agents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MIT open-source, early stage (v0.1.x).&lt;br /&gt; Repo: &lt;a href="https://github.com/jamiepine/voicebox"&gt;https://github.com/jamiepine/voicebox&lt;/a&gt;&lt;br /&gt; Downloads: &lt;a href="https://voicebox.sh/"&gt;https://voicebox.sh&lt;/a&gt; (macOS/Windows now; Linux soon)&lt;/p&gt; &lt;p&gt;Planning XTTS, Bark, etc. next. What models do you want most? Any feedback if you try it‚Äîbugs, missing features, workflow pains?&lt;/p&gt; &lt;p&gt;Give it a spin and lmk what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamiepine"&gt; /u/jamiepine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T09:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqbkk4</id>
    <title>GLM 4.7 flash Q6 thought for 1400 minutes. 2000 lines of thoughts, had to be stopped.</title>
    <updated>2026-01-29T15:27:00+00:00</updated>
    <author>
      <name>/u/regjoe13</name>
      <uri>https://old.reddit.com/user/regjoe13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqbkk4/glm_47_flash_q6_thought_for_1400_minutes_2000/"&gt; &lt;img alt="GLM 4.7 flash Q6 thought for 1400 minutes. 2000 lines of thoughts, had to be stopped." src="https://b.thumbs.redditmedia.com/JgX3rA4cwLQskKgGIifQhm7pes24EQPx7ALGs87m29A.jpg" title="GLM 4.7 flash Q6 thought for 1400 minutes. 2000 lines of thoughts, had to be stopped." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tryed this model for the first time. Asked a simple question, and forgot about it. Today morning I still see it thinking. Thankfully I stopped it before it became sentient.&lt;br /&gt; 3090, 3060 dual, 96GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regjoe13"&gt; /u/regjoe13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qqbkk4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqbkk4/glm_47_flash_q6_thought_for_1400_minutes_2000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqbkk4/glm_47_flash_q6_thought_for_1400_minutes_2000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T15:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqeudu</id>
    <title>Why don‚Äôt we have more distilled models?</title>
    <updated>2026-01-29T17:23:15+00:00</updated>
    <author>
      <name>/u/GreedyWorking1499</name>
      <uri>https://old.reddit.com/user/GreedyWorking1499</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen 8B DeepSeek R1 distill genuinely blew me away when it dropped. You had reasoning capabilities that punched way above the parameter count, running on consumer (GPU poor) hardware. &lt;/p&gt; &lt;p&gt;So where are the rest of them? Why aren‚Äôt there more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreedyWorking1499"&gt; /u/GreedyWorking1499 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqeudu/why_dont_we_have_more_distilled_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqeudu/why_dont_we_have_more_distilled_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqeudu/why_dont_we_have_more_distilled_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T17:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8rpu</id>
    <title>[News] ACE-Step 1.5 Preview - Now requires &lt;4GB VRAM, 100x faster generation</title>
    <updated>2026-01-29T13:37:50+00:00</updated>
    <author>
      <name>/u/ExcellentTrust4433</name>
      <uri>https://old.reddit.com/user/ExcellentTrust4433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8rpu/news_acestep_15_preview_now_requires_4gb_vram/"&gt; &lt;img alt="[News] ACE-Step 1.5 Preview - Now requires &amp;lt;4GB VRAM, 100x faster generation" src="https://preview.redd.it/9x51tk85kagg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=040668466faf2131a4ff169fa2fe22f761361864" title="[News] ACE-Step 1.5 Preview - Now requires &amp;lt;4GB VRAM, 100x faster generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fresh from the ACE-Step Discord - preview of the v1.5 README!&lt;/p&gt; &lt;p&gt;Key improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;**&amp;lt;4GB VRAM** (down from 8GB in v1!) - true consumer hardware&lt;/li&gt; &lt;li&gt;**100x faster** than pure LM architectures&lt;/li&gt; &lt;li&gt;Hybrid LM + DiT architecture with Chain-of-Thought&lt;/li&gt; &lt;li&gt;10-minute compositions, 50+ languages&lt;/li&gt; &lt;li&gt;Cover generation, repainting, vocal-to-BGM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Release should be imminent!&lt;/p&gt; &lt;p&gt;Also check &lt;a href="/r/ACEStepGen"&gt;r/ACEStepGen&lt;/a&gt; for dedicated discussions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcellentTrust4433"&gt; /u/ExcellentTrust4433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9x51tk85kagg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8rpu/news_acestep_15_preview_now_requires_4gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8rpu/news_acestep_15_preview_now_requires_4gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T13:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqadna</id>
    <title>My humble GLM 4.7 Flash appreciation post</title>
    <updated>2026-01-29T14:42:32+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"&gt; &lt;img alt="My humble GLM 4.7 Flash appreciation post" src="https://preview.redd.it/jh83y5tqqagg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84c7118a2d6ca354bab71459f8fd90766a909deb" title="My humble GLM 4.7 Flash appreciation post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was impressed by GLM 4.7 Flash performance, but not surprised, because I knew they could make an outstanding model that will leave most of the competitor models around the same size in the dust.&lt;/p&gt; &lt;p&gt;However I was wondering how good it really is, so I got an idea to use Artificial Analysis to put together all the similar sized open weight models I could think of at that time (or at least the ones available there for selection) and check out their benchmarks against each other to see how are they all doing.&lt;/p&gt; &lt;p&gt;To make things more interesting, I decided to throw in some of the best Gemini models for comparison and well... I knew the model was good, but this good? I don't think we can appreciate this little gem enough, just look who's there daring to get so close to the big guys. üòâ&lt;/p&gt; &lt;p&gt;This graph makes me wonder - Could it be that 30B-A3B or similar model sizes might eventually be enough to compete with today's big models? Because to me it looks that way and I have a strong belief that ZAI has what it takes to get us there and I think it's amazing that we have a model of this size and quality at home now.&lt;/p&gt; &lt;p&gt;Thank you, ZAI! ‚ù§&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jh83y5tqqagg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T14:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq5zdr</id>
    <title>I built an 80M parameter LLM from scratch using the same architecture as Llama 3 - here's what I learned</title>
    <updated>2026-01-29T11:22:46+00:00</updated>
    <author>
      <name>/u/Routine-Thanks-572</name>
      <uri>https://old.reddit.com/user/Routine-Thanks-572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share Mini-LLM, a complete implementation of a modern transformer language model built entirely from scratch.&lt;/p&gt; &lt;h1&gt;What makes this different from most educational projects?&lt;/h1&gt; &lt;p&gt;Most tutorials use outdated techniques (learned position embeddings, LayerNorm, character-level tokenization). Mini-LLM implements the &lt;strong&gt;exact same components as Llama 3&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RoPE&lt;/strong&gt; (Rotary Position Embeddings) - scales to longer sequences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RMSNorm&lt;/strong&gt; - faster and more stable than LayerNorm&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SwiGLU&lt;/strong&gt; - state-of-the-art activation function&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grouped Query Attention&lt;/strong&gt; - efficient inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SentencePiece BPE&lt;/strong&gt; - real-world tokenization with 32K vocab&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Complete Pipeline&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Custom tokenizer ‚Üí Data processing ‚Üí Training ‚Üí Inference&lt;/li&gt; &lt;li&gt;Memory-mapped data loading (TB-scale ready)&lt;/li&gt; &lt;li&gt;Mixed precision training with gradient accumulation&lt;/li&gt; &lt;li&gt;KV caching for fast generation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;80M parameters trained on 361M tokens&lt;/li&gt; &lt;li&gt;5 hours on single A100, final loss ~3.25&lt;/li&gt; &lt;li&gt;Generates coherent text with proper grammar&lt;/li&gt; &lt;li&gt;200-500 tokens/sec inference speed&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ashx098/Mini-LLM"&gt;https://github.com/Ashx098/Mini-LLM&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/Ashx098/Mini-LLM"&gt;https://huggingface.co/Ashx098/Mini-LLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is clean, well-documented, and designed for learning. Every component has detailed explanations of the &amp;quot;why&amp;quot; not just the &amp;quot;how&amp;quot;.&lt;/p&gt; &lt;p&gt;Perfect for students wanting to understand modern LLM architecture without drowning in billion-parameter codebases!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Routine-Thanks-572"&gt; /u/Routine-Thanks-572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:22:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq67io</id>
    <title>OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion</title>
    <updated>2026-01-29T11:35:08+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"&gt; &lt;img alt="OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion" src="https://external-preview.redd.it/anhiOGswbTh5OWdnMQLRfJQU73a9pcHTIBGMkMlYp-rLlT5zwChrnU104y5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75356aa649008b072137cfe5ab634a7b54be4fd5" title="OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: MOVA: Towards Scalable and Synchronized Video‚ÄìAudio Generation: &lt;a href="https://github.com/OpenMOSS/MOVA"&gt;https://github.com/OpenMOSS/MOVA&lt;/a&gt;&lt;br /&gt; MOVA-360: &lt;a href="https://huggingface.co/OpenMOSS-Team/MOVA-360p"&gt;https://huggingface.co/OpenMOSS-Team/MOVA-360p&lt;/a&gt;&lt;br /&gt; MOVA-720p: &lt;a href="https://huggingface.co/OpenMOSS-Team/MOVA-720p"&gt;https://huggingface.co/OpenMOSS-Team/MOVA-720p&lt;/a&gt;&lt;br /&gt; From OpenMOSS on ùïè: &lt;a href="https://x.com/Open_MOSS/status/2016820157684056172"&gt;https://x.com/Open_MOSS/status/2016820157684056172&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6n89xfl8y9gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8e2x</id>
    <title>Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face</title>
    <updated>2026-01-29T13:21:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face" src="https://external-preview.redd.it/7bBjSbi8Jb_ZIxPLdQlxsAX41TayP_Nw4jr5gGuqpXw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=807a024098d7c29e7df6eb6cac205fdc9b7cdeb4" title="Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All-in-one&lt;/strong&gt;: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent and Fast&lt;/strong&gt;: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel and strong forced alignment Solution&lt;/strong&gt;: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive inference toolkit&lt;/strong&gt;: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-ASR-1.7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T13:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqfe1k</id>
    <title>Kimi AI team sent me this appreciation mail</title>
    <updated>2026-01-29T17:42:26+00:00</updated>
    <author>
      <name>/u/mehulgupta7991</name>
      <uri>https://old.reddit.com/user/mehulgupta7991</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"&gt; &lt;img alt="Kimi AI team sent me this appreciation mail" src="https://preview.redd.it/0ztj2mk3sbgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cb8fb8a83de79c13b7ca310a250afa85f95fe79" title="Kimi AI team sent me this appreciation mail" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I covered Kimi K2.5 on my YT channel and the team sent me this mail with a premium access to agent swarm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehulgupta7991"&gt; /u/mehulgupta7991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ztj2mk3sbgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T17:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq6n3t</id>
    <title>GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.</title>
    <updated>2026-01-29T11:58:22+00:00</updated>
    <author>
      <name>/u/Distinct-Expression2</name>
      <uri>https://old.reddit.com/user/Distinct-Expression2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"&gt; &lt;img alt="GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week." src="https://preview.redd.it/uf2m03ak2agg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02bc2552e04fbb090e9eb5df2979a536c39ef524" title="GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It this the js framework hell moment of ai?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Expression2"&gt; /u/Distinct-Expression2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uf2m03ak2agg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp46za</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)</title>
    <updated>2026-01-28T06:54:28+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/y2qj7ancf1gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bb1df11c9d46ca94be0db3438449dc28e2dd48e" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Wednesday's guests, &lt;strong&gt;The Moonshot AI Lab Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Wednesday, Jan. 28th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2qj7ancf1gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
