<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-13T17:06:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o5ke8m</id>
    <title>Kind of amazed?</title>
    <updated>2025-10-13T13:56:08+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ke8m/kind_of_amazed/"&gt; &lt;img alt="Kind of amazed?" src="https://b.thumbs.redditmedia.com/ylMz8UU6jso8RwSKrJLNEHunvVVlRTR0b0fd5K9oeLo.jpg" title="Kind of amazed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u2fw0g6kvvuf1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1cdc5a7df10ba4a51137e9075fb4c7953ad3dc9b"&gt;https://preview.redd.it/u2fw0g6kvvuf1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1cdc5a7df10ba4a51137e9075fb4c7953ad3dc9b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yqt9jt6lwvuf1.png?width=224&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ba88d6f5d0a8593ec014fb11b3282b04965bec1"&gt;https://preview.redd.it/yqt9jt6lwvuf1.png?width=224&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ba88d6f5d0a8593ec014fb11b3282b04965bec1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have been using OpenWebUI for a bit now to chat with gpt-oss-20b, but I tested it's generation in the webollama little generation section, and the t/s surprised me. I was not aware that my speeds were that good with my tiny machine. the WebOllama screenshot is first, and the second is the generation information from asking the exact same question in OpenWebUI. Something seems like OpenWebUI takes more time to get a response? Could that be overhead of running OpenWebUI? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ke8m/kind_of_amazed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ke8m/kind_of_amazed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ke8m/kind_of_amazed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T13:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5euxz</id>
    <title>GLM-4.6-FP8 on single GH200</title>
    <updated>2025-10-13T09:15:44+00:00</updated>
    <author>
      <name>/u/Normal-Phone7762</name>
      <uri>https://old.reddit.com/user/Normal-Phone7762</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;I have full access to GH200 96 GB during some periods of a day, so I wanted to use zai-org/GLM-4.6-FP8 model. I am new to local LLM. I run GLM 4.5-Air before using lama.cpp, but since GH200 has 480RAM and 96GB VRAM I tought i sholud try GLM-4.6-FP8. I would like to use vllm, because I saw that fp8 calculations are actually faster then int8 on G200. &lt;/p&gt; &lt;p&gt;I have so many questions and if someone has time it would be nice for someone to answer them (questions are at the end of the post), BUT main question is &amp;quot;how can I run this model?&amp;quot;. &lt;/p&gt; &lt;p&gt;I tried this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -it --rm \ --gpus all \ --ipc=host \ --shm-size=64g \ -p 8000:8000 \ -e HF_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \ -e HUGGING_FACE_HUB_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \ -e MALLOC_ARENA_MAX=2 \ -v /opt/vllm/models:/models \ -v /home/admin/.cache/huggingface:/root/.cache/huggingface \ -v /home/admin/.cache/vllm:/root/.cache/vllm \ vllm/vllm-openai:latest-aarch64 \ --model zai-org/GLM-4.6-FP8 \ --download-dir /models \ --tensor-parallel-size 1 \ --cpu-offload-gb 350 \ --kv-cache-dtype fp8_e4m3 \ --gpu-memory-utilization 0.95 \ --max-model-len 4098 \ --max-num-batched-tokens 1024 \ --max-num-seqs 1 \ --served-model-name glm-4.6-fp8 \ --api-key sk-local-jan \ --trust-remote-code \ --enforce-eager &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Sometimes it fails after loading shards. Sometimes before loading shards.&lt;/p&gt; &lt;p&gt;‚ÄúModel loading took ~29.8 GiB‚Äù &lt;/p&gt; &lt;p&gt;‚ÄúAvailable KV cache memory: 0.81 GiB / -0.27 GiB‚Äù &lt;/p&gt; &lt;p&gt;‚ÄúNo available memory for the cache blocks‚Ä¶ Try increasing gpu_memory_utilization or decreasing max_model_len‚Äù&lt;/p&gt; &lt;p&gt;I‚Äôm confused about a few things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why is GPU memory utilization always at &lt;strong&gt;100%&lt;/strong&gt;, even when I set &lt;code&gt;--gpu-memory-utilization 0.9&lt;/code&gt; or &lt;code&gt;0.98&lt;/code&gt;? It always shows &lt;code&gt;97277MiB / 97871MiB&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;It loads ~30 GB of weights to the GPU. Does that mean the problem is that it can‚Äôt load the KV cache into VRAM?&lt;/li&gt; &lt;li&gt;What exactly gets loaded to the GPU first, the weights or the KV cache?&lt;/li&gt; &lt;li&gt;Since I just want to test the model, is there a way to explicitly tell vLLM to load only ~10 GB of weights to GPU and keep the rest on CPU? I‚Äôm always short by less than 1 GB before it fails.&lt;/li&gt; &lt;li&gt;If I have 96 GB VRAM and only ~30 GB of weights are loaded, what is taking up the other 66 GB?&lt;/li&gt; &lt;li&gt;Is it even possible to run this model on a single GH200?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Phone7762"&gt; /u/Normal-Phone7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5euxz/glm46fp8_on_single_gh200/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5euxz/glm46fp8_on_single_gh200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5euxz/glm46fp8_on_single_gh200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T09:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mwet</id>
    <title>GPU Poor LLM Arena is BACK! üéâüéäü•≥</title>
    <updated>2025-10-12T11:43:02+00:00</updated>
    <author>
      <name>/u/kastmada</name>
      <uri>https://old.reddit.com/user/kastmada</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt; &lt;img alt="GPU Poor LLM Arena is BACK! üéâüéäü•≥" src="https://external-preview.redd.it/xnvppfD8q4Rvrqs00KT2LLxfAKmO_ypt1REhqFgxlVw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49fb1bf881d9a00b0e731a0269d44b4ea6c31968" title="GPU Poor LLM Arena is BACK! üéâüéäü•≥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;üöÄ GPU Poor LLM Arena is BACK! New Models &amp;amp; Updates!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;First off, a massive apology for the extended silence. Things have been a bit hectic, but the GPU Poor LLM Arena is officially back online and ready for action! Thanks for your patience and for sticking around.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üöÄ Newly Added Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Granite 4.0 Small Unsloth (32B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Tiny Unsloth (7B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Micro Unsloth (3B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Thinking 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (30B, 4-bit)&lt;/li&gt; &lt;li&gt;OpenAI gpt-oss Unsloth (20B, 4-bit)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;üö® Important Notes for GPU-Poor Warriors:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Please be aware that Granite 4.0 Small, Qwen 3 30B, and OpenAI gpt-oss models are quite bulky. Ensure your setup can comfortably handle them before diving in to avoid any performance issues.&lt;/li&gt; &lt;li&gt;I've decided to default to Unsloth GGUFs for now. In many cases, these offer valuable bug fixes and optimizations over the original GGUFs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to see you back in the arena, testing out these new additions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kastmada"&gt; /u/kastmada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5e3zy</id>
    <title>With ROCm support on the RX9060xt 16gb do we have a cheap alternative to 64gb of Vram?</title>
    <updated>2025-10-13T08:27:27+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"&gt; &lt;img alt="With ROCm support on the RX9060xt 16gb do we have a cheap alternative to 64gb of Vram?" src="https://external-preview.redd.it/2pEp6h9DVo4Sdr9wjgy_p89eQv-zFl1B4zrMdKUtXdM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de95310c04e2ea4306b5a78e4f6935e9c2389359" title="With ROCm support on the RX9060xt 16gb do we have a cheap alternative to 64gb of Vram?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jbbtsazy9uuf1.png?width=1310&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4739761e7cf8822ba0ff7df6139e1f5d74252f8e"&gt;from https://videocardz.com/newz/amd-releases-rocm-7-0-2-with-radeon-rx-9060-support&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reading the news and considering that a card costs ‚Ç¨300 + VAT, with ‚Ç¨1200 + VAT you can get 4 cards for a total of 64GB of VRAM. I don't know the performance of the new drivers and I hope someone here tests them soon, but it seems like good news. Opinions? Also 160W x 4 = 640W. Cheap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T08:27:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4uagn</id>
    <title>Traning Llama3.2:3b on my whatsapp chats with wife</title>
    <updated>2025-10-12T16:56:53+00:00</updated>
    <author>
      <name>/u/jayjay_1996</name>
      <uri>https://old.reddit.com/user/jayjay_1996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;So my wife and I have been dating since 2018. ALL our chats are on WhatsApp. &lt;/p&gt; &lt;p&gt;I am an LLM noob but I wanted to export it as a txt. And then feed it into an LLM so I could ask questions like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;who has said I love you more? &lt;/li&gt; &lt;li&gt;who apologises more? &lt;/li&gt; &lt;li&gt;what was discussed during our Japan trip? &lt;/li&gt; &lt;li&gt;how many times did we fight in July 2023? &lt;/li&gt; &lt;li&gt;who is more sarcastic in 2025? &lt;/li&gt; &lt;li&gt;list all the people we‚Äôve talked about&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Etc&lt;/p&gt; &lt;p&gt;So far - the idea was to chunk them and store them in a vector DB. And then use llama to interact with it. But the results have been quite horrible. Temp - 0.1 to 0.5, k=3 to 25. Broke the chat into chunks of 4000 with overlap 100 &lt;/p&gt; &lt;p&gt;Any better ideas out there? Would love to hear! And if it works I could share the ingestion script!&lt;/p&gt; &lt;p&gt;Edit - I‚Äôve reduced the chunk size to 250. And ingesting it via llama3.2:3b. Currently - 14 hours out of 34 done! Another 20 hours and I could let you know how that turns out ‚ò†Ô∏è &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jayjay_1996"&gt; /u/jayjay_1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T16:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5auc8</id>
    <title>Meta Superintelligence group publishes paper on new RAG technique</title>
    <updated>2025-10-13T05:04:45+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5auc8/meta_superintelligence_group_publishes_paper_on/"&gt; &lt;img alt="Meta Superintelligence group publishes paper on new RAG technique" src="https://external-preview.redd.it/JCvftI08SHl-gSbgzIC77Ii1UGjMaLNJCQ7drY7JAIc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b567d36377fcc16b8bef230712988e58520db703" title="Meta Superintelligence group publishes paper on new RAG technique" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://paddedinputs.substack.com/p/meta-superintelligences-surprising"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5auc8/meta_superintelligence_group_publishes_paper_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5auc8/meta_superintelligence_group_publishes_paper_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T05:04:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5p4ed</id>
    <title>I wrote a 2025 deep dive on why long system prompts quietly hurt context windows, speed, and cost</title>
    <updated>2025-10-13T16:49:51+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there!&lt;/p&gt; &lt;p&gt;I just published a new article that breaks down what a context window is, how transformers actually process long inputs, and why bloated system prompts can lower accuracy and raise latency and spend. I talk about long context limits, prefill vs decode, KV cache pressure, prompt caching caveats, and practical guardrails for keeping prompts short without losing control.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key ideas&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every system token displaces conversation history and user input inside the fixed window.&lt;/li&gt; &lt;li&gt;Longer inputs increase prefill time and KV cache size, which hits time to first token and throughput.&lt;/li&gt; &lt;li&gt;Instruction dilution and lost-in-the-middle effects are real on very long inputs.&lt;/li&gt; &lt;li&gt;Prompt caching helps cost and sometimes latency, but it does not fix noisy instructions.&lt;/li&gt; &lt;li&gt;Sensible target: keep the system prompt to roughly 5 to 10 percent of the total window for most apps.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also maintain a repo that contains real system prompts from closed-source tools. It is a handy reference for how others structure roles, output formats and more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full article with more analysis: &lt;a href="https://medium.com/@lucknitelol/why-long-system-prompts-hurt-context-windows-and-how-to-fix-it-7a3696e1cdf9"&gt;Why long system prompts hurt context windows and how to fix it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The GitHub repo to grab prompts: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5dly2</id>
    <title>Gemini 2.5 pro / Deep Think VS local LLM</title>
    <updated>2025-10-13T07:54:36+00:00</updated>
    <author>
      <name>/u/Dumperandumper</name>
      <uri>https://old.reddit.com/user/Dumperandumper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm on ¬´ Ultra ¬ª plan with google since 3 months now and while I was cool with their discovery offer (149‚Ç¨/ month) I have now 3 days left to cancel before they start charging me 279‚Ç¨/ month. I did heavily use 2.5 pro and Deep Think for creative writing, brainstorming critical law related questions. I do not code. I have to admit Gemini has been a huge gain in productivity but 279‚Ç¨/ month is such a heavy price just to have access to Deep Think. My question is : are there any local LLM that I can run, even slowly, on my hardware that are good enough compared to what I have been used to ? I‚Äôve got a macbook pro M3 max 128gb ram. How well can I do ? Any pointer greatly appreciated. Apologies for my english. Frenchman here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dumperandumper"&gt; /u/Dumperandumper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dly2/gemini_25_pro_deep_think_vs_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dly2/gemini_25_pro_deep_think_vs_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dly2/gemini_25_pro_deep_think_vs_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T07:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5l4m1</id>
    <title>Tweaked Mistral-Small-3.2-24B-Instruct-2506 repo to better work with HF Transformers</title>
    <updated>2025-10-13T14:24:18+00:00</updated>
    <author>
      <name>/u/grimjim</name>
      <uri>https://old.reddit.com/user/grimjim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a small thing, but I've put together an updated repo For Mistral Small 3.2 24B Instruct, restoring various transformers-related files which were present in 3.1, and splicing in a generic tokenizer chat template based on the Tekken v7 format from Mistral Small 24B Instruct. Hopes this saves people the time I spent figuring out what was needed. The model loads with AutoModelForImageTextToText, not AutoModelForCausalLM. This should enable use as a plain text LLM. I left out the consolidated safetensors file to save space.&lt;br /&gt; &lt;a href="https://huggingface.co/grimjim/Mistral-Small-3.2-24B-Instruct-2506"&gt;https://huggingface.co/grimjim/Mistral-Small-3.2-24B-Instruct-2506&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grimjim"&gt; /u/grimjim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l4m1/tweaked_mistralsmall3224binstruct2506_repo_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l4m1/tweaked_mistralsmall3224binstruct2506_repo_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l4m1/tweaked_mistralsmall3224binstruct2506_repo_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T14:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5gbbf</id>
    <title>LM Studio + Snapdragon Laptops = Bad experience</title>
    <updated>2025-10-13T10:44:07+00:00</updated>
    <author>
      <name>/u/Andrew_C0</name>
      <uri>https://old.reddit.com/user/Andrew_C0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I've been running into this issue recently that I'm unable to debug or fix whatsoever.&lt;/p&gt; &lt;p&gt;Using the latest version of LM Studio (0.3.30) on my Snapdragon Laptop (a Slim 7X - the 32GB RAM version), I get pretty great experience first time I run LM Studio. I tried recently Qwen3 1.7B model just to test it out, and I get around 50 tokens/s, which is great.&lt;/p&gt; &lt;p&gt;However, that only works the first time the model is loaded. Afterwards, if I want to eject the model and use another one (let's say, Qwen3 4B), I get somewhat arount 0.02 tokens/s. I just don't get why. If I want to reload the same 1.7B model, I get the same token performance.&lt;/p&gt; &lt;p&gt;What I've noticed is that rebooting the laptop and loading the model again, it fixes the issue (in regards to whatever model I load first, including Qwen3 Coder 30B), but as soon as I eject and load another model, until I reboot, the tokens/s is always under 1 t/s.&lt;/p&gt; &lt;p&gt;I haven't altered any settings, so I just downloaded the model, loaded it in, and that's it.&lt;/p&gt; &lt;p&gt;I had the same experience using a Surface Laptop 7 in the past, with an older version of LM Studio, but after some updates, it was somehow fixed.&lt;/p&gt; &lt;p&gt;Any help is greatly appreciated to fix this.&lt;/p&gt; &lt;p&gt;LE: Solved by changing the power plan to `Best Performance`, since `Better power efficiency` greatly handicapped the CPU and LM Studio performance, it seems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Andrew_C0"&gt; /u/Andrew_C0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5gbbf/lm_studio_snapdragon_laptops_bad_experience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5gbbf/lm_studio_snapdragon_laptops_bad_experience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5gbbf/lm_studio_snapdragon_laptops_bad_experience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T10:44:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o54wbu</id>
    <title>Did you create a new benchmark? Good, keep it to yourself, don't release how it works until something beats it.</title>
    <updated>2025-10-13T00:05:35+00:00</updated>
    <author>
      <name>/u/EmirTanis</name>
      <uri>https://old.reddit.com/user/EmirTanis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only release leaderboards / charts. This is the only way to avoid pollution / interference from the AI companies. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmirTanis"&gt; /u/EmirTanis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o54wbu/did_you_create_a_new_benchmark_good_keep_it_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o54wbu/did_you_create_a_new_benchmark_good_keep_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o54wbu/did_you_create_a_new_benchmark_good_keep_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T00:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5lio5</id>
    <title>At least now I can follow what it is doing</title>
    <updated>2025-10-13T14:39:16+00:00</updated>
    <author>
      <name>/u/bomxacalaka</name>
      <uri>https://old.reddit.com/user/bomxacalaka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5lio5/at_least_now_i_can_follow_what_it_is_doing/"&gt; &lt;img alt="At least now I can follow what it is doing" src="https://preview.redd.it/dzhzxy1y4wuf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbd1ad34dd23416e1cc75c044b5716973b13e196" title="At least now I can follow what it is doing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bomxacalaka"&gt; /u/bomxacalaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dzhzxy1y4wuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5lio5/at_least_now_i_can_follow_what_it_is_doing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5lio5/at_least_now_i_can_follow_what_it_is_doing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T14:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5dh3v</id>
    <title>What's the missing piece in the LLaMA ecosystem right now?</title>
    <updated>2025-10-13T07:45:58+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLaMA model ecosystem is exploding with new variants and fine-tunes. &lt;/p&gt; &lt;p&gt;But what's the biggest gap or most underdeveloped area still holding it back? &lt;/p&gt; &lt;p&gt;For me, it's the &lt;strong&gt;&lt;em&gt;data prep and annotation tools&lt;/em&gt;&lt;/strong&gt;. The models are getting powerful, but cleaning and structuring quality training data for fine-tuning is still a major, manual bottleneck. &lt;/p&gt; &lt;p&gt;What do you think is the most missing piece? &lt;/p&gt; &lt;p&gt;Better/easier fine-tuning tools?&lt;br /&gt; More accessible hardware solutions?&lt;br /&gt; Something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T07:45:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4v880</id>
    <title>New Unhinged NSFW Reasoning Model - Satyr-V0.1-4B</title>
    <updated>2025-10-12T17:32:33+00:00</updated>
    <author>
      <name>/u/ThePantheonUnbound</name>
      <uri>https://old.reddit.com/user/ThePantheonUnbound</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This version is an unpredictable experiment and may produce vulgar, explicit, or graphic content. Please use it at your own risk. More multifaceted versions will be released soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePantheonUnbound"&gt; /u/ThePantheonUnbound &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4v880/new_unhinged_nsfw_reasoning_model_satyrv014b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4v880/new_unhinged_nsfw_reasoning_model_satyrv014b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T17:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5ec73</id>
    <title>Seeking for a uncensored NSFW Dutch language.</title>
    <updated>2025-10-13T08:42:12+00:00</updated>
    <author>
      <name>/u/jobbie1973</name>
      <uri>https://old.reddit.com/user/jobbie1973</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i am seeking for a local uncensored nsfw Dutch dataset for KoboltCPP and SillyTavern locally on my pc, AMD 7-5800/32GB Ram/RTX3060Ti/8GB.&lt;/p&gt; &lt;p&gt;i am planned to write some VN-Story.&lt;/p&gt; &lt;p&gt;Friendly greetings from Netherlands.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jobbie1973"&gt; /u/jobbie1973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ec73/seeking_for_a_uncensored_nsfw_dutch_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ec73/seeking_for_a_uncensored_nsfw_dutch_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ec73/seeking_for_a_uncensored_nsfw_dutch_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T08:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5mr9j</id>
    <title>Do you guys personally notice a difference between Q4 - Q8 or higher?</title>
    <updated>2025-10-13T15:24:43+00:00</updated>
    <author>
      <name>/u/XiRw</name>
      <uri>https://old.reddit.com/user/XiRw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It feels like for me I can see the differences between parameters of higher numbers easily compared to quantizations between models which feels a lot harder to notice any benefits between them. &lt;/p&gt; &lt;p&gt;To be fair I haven‚Äôt worked with Q4 too much but Q6 and Q8 of the same model I don‚Äôt really notice a difference. Even when it comes to Q8 or F16-32 but again I have limited experience with floating point numbers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiRw"&gt; /u/XiRw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mr9j/do_you_guys_personally_notice_a_difference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mr9j/do_you_guys_personally_notice_a_difference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mr9j/do_you_guys_personally_notice_a_difference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4wg6q</id>
    <title>Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp; try the demo</title>
    <updated>2025-10-12T18:18:58+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"&gt; &lt;img alt="Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp;amp; try the demo" src="https://external-preview.redd.it/WYNRuaBJiIIoNSwO7SSTGPP2ITAUNljSMCnTROkhRdg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11110ecc44cc993c095ca5cc3a864cd7384ecf18" title="Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp;amp; try the demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/AgentFlow/agentflow"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T18:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5n4fu</id>
    <title>Fully functional native FP4 training finally released</title>
    <updated>2025-10-13T15:37:51+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been eagerly watching the development of FP4 training, as it would enable anyone with a Blackwell device to train models with 2x the parameters that we can currently fit with FP8, and 4x BF16, which most people are still training in (get with the times people).&lt;/p&gt; &lt;p&gt;There have been many papers previously showing that FP4 is effective:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.19115"&gt;https://arxiv.org/abs/2505.19115&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2501.17116"&gt;https://arxiv.org/abs/2501.17116&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.14669"&gt;https://arxiv.org/abs/2505.14669&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.20586"&gt;https://arxiv.org/abs/2502.20586&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And one of them has also been working on public versions of the training kernels... but they have only released the forward pass kernels: &lt;a href="https://github.com/huggingface/transformers/pull/38696"&gt;https://github.com/huggingface/transformers/pull/38696&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a comparison of the 4 papers by Gemini, if you're interested in the details: &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565"&gt;https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT-OSS was also trained in FP4, but released no code, though I would bet that NVidia's in house solution was used.&lt;/p&gt; &lt;p&gt;Now, finally, NVidia has published their own FP4 training recipe. It's not well documented or tested yet, and apparently one of the techniques required for stable quantization (stochastic rounding) &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/2255#issuecomment-3387759788"&gt;simply doesn't work on the consumer RTX 50 series&lt;/a&gt;, only the datacenter cards, but still, it's here and we can use it. The use of Hadamard transforms should still allow consumer cards to train with some stability.&lt;/p&gt; &lt;p&gt;Here's some documentation which touches on their FP4 recipe: &lt;a href="https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb"&gt;https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and here's their paper which goes into detail: &lt;a href="https://arxiv.org/abs/2509.25149v1"&gt;https://arxiv.org/abs/2509.25149v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5l62w</id>
    <title>Dolphin X1 8B (Llama3.1 8B decensor) live on HF</title>
    <updated>2025-10-13T14:25:53+00:00</updated>
    <author>
      <name>/u/dphnAI</name>
      <uri>https://old.reddit.com/user/dphnAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, we have released Dolphin X1 8B - a finetune of Llama3.1 8B Instruct with the goal of de-censoring the model as much as possible without harming other abilities&lt;/p&gt; &lt;p&gt;It scored a 96% pass rate on our internal refusals eval, only refusing 181 of 4483 prompts&lt;/p&gt; &lt;p&gt;Using the same formula that we used on dphn/Dolphin-Mistral-24B-Venice-Edition - X1 is the new name for this latest series of models (more coming very soon)&lt;/p&gt; &lt;p&gt;X1 Apertus + seedOSS coming soon&lt;/p&gt; &lt;p&gt;Feel free to request any other models you would like us to train&lt;/p&gt; &lt;p&gt;We hope you enjoy it&lt;/p&gt; &lt;p&gt;Benchmarks were equal or higher to Llama3.1 8B Instruct all except ifeval&lt;/p&gt; &lt;p&gt;No abliteration was used in the making of this model - purely SFT + RL&lt;/p&gt; &lt;p&gt;Many thanks to Deepinfra for the sponsorship on this model - they offer B200's at $2.5 per hour which is amazing value for training&lt;/p&gt; &lt;p&gt;Full size model = dphn/Dolphin-X1-8B&lt;/p&gt; &lt;p&gt;GGUF + FP8 + exl2 all uploaded on our HF - exl3 coming soon&lt;/p&gt; &lt;p&gt;It is hosted for free in both our Chat UI &amp;amp; Telegram bot which you can find on our website&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dphnAI"&gt; /u/dphnAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l62w/dolphin_x1_8b_llama31_8b_decensor_live_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l62w/dolphin_x1_8b_llama31_8b_decensor_live_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l62w/dolphin_x1_8b_llama31_8b_decensor_live_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T14:25:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5mlng</id>
    <title>Anyone think openAI will create a sequel of GPT-OSS?</title>
    <updated>2025-10-13T15:19:12+00:00</updated>
    <author>
      <name>/u/BothYou243</name>
      <uri>https://old.reddit.com/user/BothYou243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean they should right? because gpt-oss (not biased or just have some grudge) is a nice model, and the rprobelm is it's just nice, so creating somethign better is still needed, anyone got any leaks about it?&lt;/p&gt; &lt;p&gt;what about anthropic, wont they drop something open, and xAI?&lt;br /&gt; xAI have poteential to outpace everyone, i am not. a fan of open sorucing some 1 year old model trend, but if they create soemthign from scracth to open source just like openAI did, it will be Absolutely Incredible! (yes taken from tim cook)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BothYou243"&gt; /u/BothYou243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5jbqp</id>
    <title>Who is waiting for the m5 max and the 2026 mac studio?</title>
    <updated>2025-10-13T13:11:50+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The m5 max will probably have 256 gb of unified ram, i hope they lower the price for the 128 gb m5 max and m6 max ‚Ä¶ The high ram (128 gb) macbooks are a little too expensive , if it was 1200 bucks cheaper , it would be great, but i know they almost never lower price, but i think they will give more ram for the default model‚Ä¶.&lt;/p&gt; &lt;p&gt;M5/4 ultra will probably have 1tb of ram‚Ä¶.Who is gonna get it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5jbqp/who_is_waiting_for_the_m5_max_and_the_2026_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5jbqp/who_is_waiting_for_the_m5_max_and_the_2026_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5jbqp/who_is_waiting_for_the_m5_max_and_the_2026_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T13:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5o8z1</id>
    <title>Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!</title>
    <updated>2025-10-13T16:18:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt; &lt;img alt="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" src="https://external-preview.redd.it/TovPswR4pl93bt0GT2q9uuik1XMY41ZSblXtMnDzdsU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed07f9c93dd6fdbb3800e12511f49c15a31923c3" title="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hot Take: Many models today are 'too smart' in a creative sense - trying too hard to be sensible and end up limiting their imagination to the user's prompt. Rerolls don't usually lead to different outcomes, and every gen seems catered to the user's expectations. Worst of all, there's an assistant bias that focuses on serving you (the user) instead of the story. All of these stifle their ability to express characters in a lively way. (inb4 skill issue)&lt;/p&gt; &lt;p&gt;Given the success of 22B and 123B ReduX v1.0, I revisited the old models and brought out a flavorful fusion of creativity and smarts through my latest tuning. 22B may not be as smart and sensible as the newer 24B, but ReduX makes it (more than) serviceable for users hoping for broader imagination and better immersion in their creative uses.&lt;/p&gt; &lt;h1&gt;Cydonia ReduX 22B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Behemoth ReduX 123B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1"&gt;https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;Enjoy! (Please note that this is a dual release: 123B and 22B. Notice the two links in this post.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5h18a</id>
    <title>Has anyone gotten hold of DGX Spark for running local LLMs?</title>
    <updated>2025-10-13T11:24:16+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt; &lt;img alt="Has anyone gotten hold of DGX Spark for running local LLMs?" src="https://preview.redd.it/ombg19hz5vuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0724eec16512ad1132ec21657234daac0040c74" title="Has anyone gotten hold of DGX Spark for running local LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DGX Spark is apparently one of the Time's Best Invention of 2025!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ombg19hz5vuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T11:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o58klk</id>
    <title>I rue the day they first introduced "this is not X, this is &lt;unearned superlative&gt;' to LLM training data</title>
    <updated>2025-10-13T03:05:58+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- This isn't just a bug, this is a fundamental design flaw&lt;/p&gt; &lt;p&gt;- This isn't just a recipe, this is a culinary journey&lt;/p&gt; &lt;p&gt;- This isn't a change, this is a seismic shift&lt;/p&gt; &lt;p&gt;- This isn't about font choice, this is about the very soul of design&lt;/p&gt; &lt;p&gt;- This isn't a refactor, this is a fundamental design overhaul&lt;/p&gt; &lt;p&gt;- This isn't a spreadsheet, this is a blueprint of a billion dollar business&lt;/p&gt; &lt;p&gt;And it seems to have spread to all LLMs now, to the point that you have to consciously avoid this phrasing everywhere if you're a human writer&lt;/p&gt; &lt;p&gt;Perhaps the idea of Model Collapse (&lt;a href="https://en.wikipedia.org/wiki/Model_collapse"&gt;https://en.wikipedia.org/wiki/Model_collapse&lt;/a&gt;) is not unreasonable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T03:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5nlli</id>
    <title>Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp; More</title>
    <updated>2025-10-13T15:55:32+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt; &lt;img alt="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" src="https://b.thumbs.redditmedia.com/53soxOPlO99qPaIqPE4FAHhMXBhvv7QgMGAh0UoqSHU.jpg" title="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to share &lt;strong&gt;Nanonets-OCR2&lt;/strong&gt;, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).&lt;/p&gt; &lt;p&gt;üîç &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LaTeX Equation Recognition:&lt;/strong&gt; Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (&lt;code&gt;$...$&lt;/code&gt;) and display (&lt;code&gt;$$...$$&lt;/code&gt;) equations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Image Description:&lt;/strong&gt; Describes images within documents using structured &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Signature Detection &amp;amp; Isolation:&lt;/strong&gt; Identifies and isolates signatures from other text, outputting them within a &lt;code&gt;&amp;lt;signature&amp;gt;&lt;/code&gt; tag. This is crucial for processing legal and business documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watermark Extraction:&lt;/strong&gt; Detects and extracts watermark text from documents, placing it within a &lt;code&gt;&amp;lt;watermark&amp;gt;&lt;/code&gt; tag.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Checkbox Handling:&lt;/strong&gt; Converts form checkboxes and radio buttons into standardized Unicode symbols (&lt;code&gt;‚òê&lt;/code&gt;, &lt;code&gt;‚òë&lt;/code&gt;, &lt;code&gt;‚òí&lt;/code&gt;) for consistent and reliable processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Table Extraction:&lt;/strong&gt; Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flow charts &amp;amp; Organisational charts:&lt;/strong&gt; Extracts flow charts and organisational as &lt;a href="https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org"&gt;mermaid&lt;/a&gt; code.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handwritten Documents:&lt;/strong&gt; The model is trained on handwritten documents across multiple languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Question Answering (VQA):&lt;/strong&gt; The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with &amp;quot;Not mentioned.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://docstrange.nanonets.com/"&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://nanonets.com/research/nanonets-ocr-2"&gt;üì¢ Blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NanoNets/docstrange"&gt;‚å®Ô∏è GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó &lt;a href="https://huggingface.co/collections/nanonets/nanonets-ocr2-68ed207f17ee6c31d226319e"&gt;Huggingface models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea00f9623db4529514533820223b2fb53be4767d"&gt;Document with equation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4a1316e250f7f244f6e253d66c8ebf1ba105313"&gt;Document with complex checkboxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8bcc88b138a553c7760d6e46319b864802339913"&gt;Quarterly Report (Please use the Markdown(Financial Docs) for best result in docstrange demo)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9"&gt;Signatures&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=008fce272c2979b00e0033c34ffcd2b0d69cb24c"&gt;mermaid code for flowchart&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jytsym6eiwuf1.png?width=2462&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c"&gt;Visual Question Answering&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try it out and share your feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
