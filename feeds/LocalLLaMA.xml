<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-04T11:35:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pd9tgj</id>
    <title>A Technical Tour of the DeepSeek Models from V3 to V3.2</title>
    <updated>2025-12-03T17:03:17+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt; &lt;img alt="A Technical Tour of the DeepSeek Models from V3 to V3.2" src="https://external-preview.redd.it/Oy9W7OYOeVO8Z6Sl3EWWZR-9AbREkAwoyEei1XJ7yeY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a5effd7f132f71b2efdd47cc12daa448023c0bf" title="A Technical Tour of the DeepSeek Models from V3 to V3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/technical-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T17:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd5yxy</id>
    <title>My experiences with the new Ministral 3 14B Reasoning 2512 Q8</title>
    <updated>2025-12-03T14:41:03+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt; &lt;img alt="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" src="https://b.thumbs.redditmedia.com/YQNWxn03P5a0Q35GBj3cSIS0Oa0a8pdRn0Pkkl0sUGM.jpg" title="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;45 minutes and 33K tokens of thinking about making html tetris (1 line prompt):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819"&gt;https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tool calling breaks all the time:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184"&gt;https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also at some point it stopped using the [think] tags altogether and just started thinking out loud. I'll leave it running for a couple of hours and see if it eventually manages to build the HTML Tetris.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T14:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3xyp</id>
    <title>Why don't Google and Openai release their old models?</title>
    <updated>2025-12-03T13:19:51+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPt 4 and gemini 2 pro are dated, they should release it... Are they afraid of releasing their data and architecture? They released gemma and gpt oss already. Gemini 2 has a large context window, but the quality degrades when it gets large though and it is replicable.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdv179</id>
    <title>Small models are solving the "Computer Use" data bottleneck. üñ±Ô∏è</title>
    <updated>2025-12-04T08:43:29+00:00</updated>
    <author>
      <name>/u/buntyshah2020</name>
      <uri>https://old.reddit.com/user/buntyshah2020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdv179/small_models_are_solving_the_computer_use_data/"&gt; &lt;img alt="Small models are solving the &amp;quot;Computer Use&amp;quot; data bottleneck. üñ±Ô∏è" src="https://preview.redd.it/96yomp2og55g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77ff2b591fc04c4158cac3642bd1f0cb5c892bbb" title="Small models are solving the &amp;quot;Computer Use&amp;quot; data bottleneck. üñ±Ô∏è" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Small models are solving the &amp;quot;Computer Use&amp;quot; data bottleneck. üñ±Ô∏è &lt;/p&gt; &lt;p&gt;As AI Architects, we've been waiting for the moment when agentic models could reliably interact with GUIs without massive, brittle scaffolding (like accessibility trees or DOM parsers). &lt;/p&gt; &lt;p&gt;Microsoft just released Fara-7B, and it‚Äôs a significant leap for two reasons:&lt;br /&gt; Pixel-In, Action-Out: It doesn‚Äôt rely on HTML parsing or accessibility trees at inference time. It perceives the screen directly via screenshots and predicts coordinates. This is the robust, &amp;quot;human-like&amp;quot; interaction model we need for resilience against dynamic UI changes. &lt;/p&gt; &lt;p&gt;The Data Engine (FaraGen): The real breakthrough isn't the model; it's the data. They built a scalable synthetic data engine that generates high-quality trajectories for ~$1 per task. This solves the scarcity problem for agent training data. &lt;/p&gt; &lt;p&gt;Benchmarks:&lt;br /&gt; WebVoyager: 73.5% success rate (beating UI-TARS-7B at 66.4% and even GPT-4o SoM).&lt;br /&gt; Cost: ~$0.025 per task. This makes on-device, local agents economically viable for the first time. &lt;/p&gt; &lt;p&gt;If you are designing local-first agentic systems or privacy-sensitive automation, this is the model to test.&lt;/p&gt; &lt;p&gt;Paper Link: &lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use.pdf"&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/microsoft/fara"&gt;https://github.com/microsoft/fara&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/microsoft/Fara-7B"&gt;https://huggingface.co/microsoft/Fara-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buntyshah2020"&gt; /u/buntyshah2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/96yomp2og55g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdv179/small_models_are_solving_the_computer_use_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdv179/small_models_are_solving_the_computer_use_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T08:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdrist</id>
    <title>Epyc setup + 1/2 Pro 6000 that can run Qwen coder 480b at 20 tps?</title>
    <updated>2025-12-04T05:13:58+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the old rig that I have been using to experiment with the Pro 6000 - I am finally going to replace it with a comparable SOTA setup (minus the GPU). I would like a working setup that could achieve 20 tps with my favourite model. If that is unrealistic, 10+ tps could work too. I already know 5 tps is fairly achievable (but not useful)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrist/epyc_setup_12_pro_6000_that_can_run_qwen_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrist/epyc_setup_12_pro_6000_that_can_run_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrist/epyc_setup_12_pro_6000_that_can_run_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T05:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdi8o7</id>
    <title>DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp; CPU Support</title>
    <updated>2025-12-03T22:13:40+00:00</updated>
    <author>
      <name>/u/Dogacel</name>
      <uri>https://old.reddit.com/user/Dogacel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt; &lt;img alt="DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" src="https://external-preview.redd.it/9aiVAPD5fnnzElgE74nQoRx4aoPflI7CwyCPycl2BLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f593757a501d49b05111d714ec689f7acf643d1" title="DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently updated DeepSeek-OCR to support Apple Metal (MPS) and CPU acceleration. I wanted to share this in case anyone else has been looking to run it efficiently on macOS.&lt;/p&gt; &lt;p&gt;To make it easier to use, I also forked an existing desktop client and applied the patch. You can check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Dogacel/deepseek-ocr-client-macos"&gt;https://github.com/Dogacel/deepseek-ocr-client-macos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogacel"&gt; /u/Dogacel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Dogacel/DeepSeek-OCR-Metal-MPS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdtj6c</id>
    <title>entered a memory competition with my local llama setup, results were weird</title>
    <updated>2025-12-04T07:07:11+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this long term memory competition thing on twitter a few weeks back and decided to enter with my local setup. Llama 3.1 8B Instruct + some memory hacks i've been working on.&lt;/p&gt; &lt;p&gt;Competition had 3 main tasks:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Long-term dialogue (50+ turns, reference stuff from turn 5 at turn 45)&lt;/li&gt; &lt;li&gt;Multi-person conversation tracking (track who said what when) &lt;/li&gt; &lt;li&gt;Causal reasoning (if X happened because of Y, remember the connection)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My approach was pretty basic. Used transformers library, monkey patched the generate() function to not reset past_key_values between conversation turns. Added some janky importance scoring - basically tracked which tokens got high attention scores and tried to keep those when hitting memory limits. Nothing fancy, just hacked together over a weekend.&lt;/p&gt; &lt;p&gt;Results were all over the place:&lt;/p&gt; &lt;p&gt;Task 1 (long conversations): 72.3% - not bad Task 2 (multi person): 43.8% - terrible Task 3 (causal reasoning): 81.7% - surprisingly good&lt;/p&gt; &lt;p&gt;The weird part is task 3. My system somehow got causal connections way better than conversation tracking. No clue why that worked.&lt;/p&gt; &lt;p&gt;Looking at other entries, most people did RAG stuff. Vector DBs, embeddings, retrieval, you know. Standard approach. My KV cache thing was kinda different.&lt;/p&gt; &lt;p&gt;Top scorer got 92.3% overall using some open source memory system. Way better than my 65.9% average but their approach was completely different from mine. From the leaderboard description, they used hybrid retrieval with multiple databases instead of just KV cache hacks. Found the repo later: github.com/EverMind-AI/EverMemOS. Seemed like a proper memory framework with MongoDB, Elasticsearch, and vector databases vs my simple KV cache approach.&lt;/p&gt; &lt;p&gt;Couple things i figured out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache stuff works but eats memory like crazy (hit 22.8GB on my 3090 for the 50+ turn conversations, had to restart multiple times)&lt;/li&gt; &lt;li&gt;importance scoring is key, otherwise you run out of space fast&lt;/li&gt; &lt;li&gt;multi person chats are a nightmare, way harder than i expected. spent most time debugging this&lt;/li&gt; &lt;li&gt;causal reasoning was surprisingly ok, not sure why. maybe got lucky?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Might look into other approaches. My hack was fun but obviously not great lol. The winning approach looked more serious but setup seemed complicated from what i could see. Maybe worth checking out if i have time.&lt;/p&gt; &lt;p&gt;Competition was actually useful tho. Made me test things properly instead of just &amp;quot;eh seems to work&amp;quot;. Realized my approach had way more issues than i thought.&lt;/p&gt; &lt;p&gt;Anyone else tried these memory challenge things? Curious what approaches worked for you. Mine was obviously not great but learned a lot about the limitations of simple KV cache approaches.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdwzgb</id>
    <title>Deepseek 3.2 just does not seem to perform (for me)</title>
    <updated>2025-12-04T10:49:38+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using RooCode and just not feeling DeepSeek 3.2 at all. &lt;/p&gt; &lt;p&gt;Is it just me? Any tips? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T10:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2wjt</id>
    <title>DeepSeek V3.2 Technical Report</title>
    <updated>2025-12-03T12:31:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt; &lt;img alt="DeepSeek V3.2 Technical Report" src="https://preview.redd.it/q3rjrhs0gz4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2e078ce099142771b5d3999cbb9670fbfc18d8" title="DeepSeek V3.2 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a brief summary of &lt;strong&gt;key breakthroughs of DeepSeek V3.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. DeepSeek Sparse Attention (DSA)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new efficient attention mechanism that dramatically reduces computational complexity while preserving performance in long-context scenarios. &lt;/p&gt; &lt;p&gt;It uses a lightning indexer with fine-grained top-k token selection to achieve sparse but effective attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Scalable and Stable Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Implements a heavily scaled post-training RL pipeline, with compute exceeding 10% of pretraining cost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Large-Scale Agentic Task Synthesis Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Provides a novel pipeline that programmatically generates large numbers of tool-use environments (1,800+ environments, 85,000+ complex prompts). &lt;/p&gt; &lt;p&gt;This boosts generalization, tool-use ability, and instruction-following in interactive settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Unified Reasoning + Agentic RL Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Merges reasoning, tool-use, and human-alignment RL into a single stage rather than multi-stage pipelines. &lt;/p&gt; &lt;p&gt;This avoids catastrophic forgetting and improves cross-domain performance simultaneously.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A high-compute variant trained with relaxed length penalties and enhanced mathematical-reasoning rewards. &lt;/p&gt; &lt;p&gt;This model even surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.02556"&gt;Arxiv paper &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3rjrhs0gz4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdswk2</id>
    <title>Spec-Kit with Ministral 3 14b</title>
    <updated>2025-12-04T06:30:23+00:00</updated>
    <author>
      <name>/u/International_Quail8</name>
      <uri>https://old.reddit.com/user/International_Quail8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had been fighting with a few models to get spec-kit working locally. gpt-oss-20b, qwen3-coder-30b and qwen3-next all failed me! Used lmstudio for local inference and qwen code as the codegen cli. &lt;/p&gt; &lt;p&gt;I gave the new ministral 3 14b reasoning model a shot and was very impressed that it was able to follow the spec-kit process, work with the templates and generate my feature as spec‚Äôd! It also did it with reasonably good speed. Not perfect, but got through an entire complex feature from start to finish where the other models failed. Mistral did it again! Was a huge fan of Mixtral 8x7B from ‚Äúback in the day‚Äù&lt;/p&gt; &lt;p&gt;Nice one Mistral AI!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/International_Quail8"&gt; /u/International_Quail8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T06:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdk4zx</id>
    <title>[Resource] 20,000+ Pages of U.S. House Oversight Epstein Estate Docs (OCR'd &amp; Cleaned for RAG/Analysis)</title>
    <updated>2025-12-03T23:30:44+00:00</updated>
    <author>
      <name>/u/Ok-District-1330</name>
      <uri>https://old.reddit.com/user/Ok-District-1330</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve reuploaded the recent release of 20,000+ pages of documents regarding the Epstein Estate from the U.S. House Oversight Committee. The goal is to make these scattered government files accessible for journalists and researchers using open-source tools.&lt;/p&gt; &lt;p&gt;Note, this was originally shared here, by another user who's account has now been deleted, then uploaded to huggingface. The original huggingface repo has since been removed, as well as the original uploaders account. Credit for the original dataset goes to him/her. This is simply a clone, hosted on Github and my huggingface account, with a gradio app I built for interacting/searching it.&lt;/p&gt; &lt;p&gt;The original release contained mixed file formats and nested folders. This dataset converts images/PDFs to text (via Tesseract OCR) and standardizes them into a single CSV format.&lt;/p&gt; &lt;p&gt;Searchable App: A Gradio browser to search the corpus without downloading the full set.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/theelderemo/epstein-files"&gt;Hugging Face Gradio App and Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/theelderemo/Epstein-files"&gt;Github Mirror&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-District-1330"&gt; /u/Ok-District-1330 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T23:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdin3b</id>
    <title>The Best Open Weights Coding Models of 2025</title>
    <updated>2025-12-03T22:29:44+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt; &lt;img alt="The Best Open Weights Coding Models of 2025" src="https://external-preview.redd.it/wxWktbYwfvy3j0Y1BhifZGPwnHUnY7JGZEyykx4N_HM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48a8af816e33acba1ab83ce8086f21346740c37d" title="The Best Open Weights Coding Models of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm back with uncontaminated evals for DeepSeek-V3.2, Kimi K2 Thinking, and MiniMax M2. (We caught GLM 4.6 last time around.) &lt;/p&gt; &lt;p&gt;If you just want the numbers, you can find them for the finalists &lt;a href="https://brokk.ai/power-ranking?models=dsv3.2%2Cglm4.6-fp8%2Ck2-thinking%2Cm2"&gt;here&lt;/a&gt; and for everyone else &lt;a href="https://brokk.ai/power-ranking?dataset=openround"&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.brokk.ai/the-best-open-weights-coding-models-of-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlfu3</id>
    <title>Frozen networks show usable early-layer intent: 1370√ó fewer FLOPs and 10√ó faster inference (code + weights)9</title>
    <updated>2025-12-04T00:26:15+00:00</updated>
    <author>
      <name>/u/anima-core</name>
      <uri>https://old.reddit.com/user/anima-core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with whether a frozen network‚Äôs early activations contain enough ‚Äúsemantic intent‚Äù to skip most of the compute.&lt;/p&gt; &lt;p&gt;I used a standard ResNet-18 trained on CIFAR-10 (87.89 percent accuracy), pulled a single 64-dimensional vector from an early layer, and trained a tiny decoder on top of it.&lt;/p&gt; &lt;p&gt;Results on the same hardware: ‚Ä¢ 72.57 percent accuracy from that early-layer vector ‚Ä¢ ~10√ó faster real latency ‚Ä¢ 1370√ó fewer FLOPs ‚Ä¢ No pruning, distillation, quantization, early exit tricks, or sparsity ‚Ä¢ The full model stayed completely frozen&lt;/p&gt; &lt;p&gt;This means 99.93 percent of the original network‚Äôs compute was not required to recover 82.6 percent of its performance.&lt;/p&gt; &lt;p&gt;Code + one-click run script: &lt;a href="https://github.com/Anima-Core/an1-meaning-engine"&gt;https://github.com/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF demo + pretrained weights: &lt;a href="https://huggingface.co/Anima-Core/an1-meaning-engine"&gt;https://huggingface.co/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Runs end to end on almost any GPU or CPU in a few minutes.&lt;/p&gt; &lt;p&gt;Dedicated to my late father, Asad Shamim, whose loss opened the path that led me here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anima-core"&gt; /u/anima-core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdtht0</id>
    <title>Do you think we‚Äôll get GLM 4.6 Air one day?</title>
    <updated>2025-12-04T07:04:52+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was about to forget, it‚Äôs probably not the priority of zai but hope remain!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdm268</id>
    <title>How Attention Got So Efficient [GQA/MLA/DSA]</title>
    <updated>2025-12-04T00:53:59+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt; &lt;img alt="How Attention Got So Efficient [GQA/MLA/DSA]" src="https://external-preview.redd.it/4QixmEzxJtTr5ZgAjR4FoJjK4qVPLU4zAuNo-fsPzgM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f8badcadc6197f760be212560f8188dc2793fa6" title="How Attention Got So Efficient [GQA/MLA/DSA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone trying to understand why Deepseek 3.2 DSA is a milestone in terms of solving long context, I really recommend this video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Y-o545eYjXM?si=pt-SxR5anfLNSN8j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdupdg</id>
    <title>Deepseek's progress</title>
    <updated>2025-12-04T08:21:16+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt; &lt;img alt="Deepseek's progress" src="https://preview.redd.it/zpkzyrrxc55g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88e4dbc74aac37f16270f4775ec470f375eab2f5" title="Deepseek's progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's fascinating that DeepSeek has been able to make all this progress with the same pre-trained model since the start of the year, and has just improved post-training and attention mechanisms. It makes you wonder if other labs are misusing their resources by training new base models so often.&lt;/p&gt; &lt;p&gt;Also, what is going on with the Mistral Large 3 benchmarks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zpkzyrrxc55g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T08:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdfk0o</id>
    <title>Hermes 4.3 - 36B Model released</title>
    <updated>2025-12-03T20:30:22+00:00</updated>
    <author>
      <name>/u/crazeum</name>
      <uri>https://old.reddit.com/user/crazeum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt; &lt;img alt="Hermes 4.3 - 36B Model released" src="https://external-preview.redd.it/thAQxjbw3fpc9fgR1nrJDb-3cDeZ9f7TtJWveW5lCQ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49b96ff1b32dfa841362b8c2a0d4449fdd83b1f0" title="Hermes 4.3 - 36B Model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes uncensored line models with apache 2 license. Post trained from Seed-OSS-36B-Base on their psyche network. The cool bit is they also trained it centralized and the distributed psyche trained version outperformed the centrally trained one.&lt;/p&gt; &lt;p&gt;GGUF links: &lt;a href="https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF"&gt;https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazeum"&gt; /u/crazeum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdcytv</id>
    <title>Micron Announces Exit from Crucial Consumer Business</title>
    <updated>2025-12-03T18:54:47+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technically speaking, we're screwed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdvupp</id>
    <title>Cruxy: Train 1.5B models on 4GB VRAM - new optimiser just released</title>
    <updated>2025-12-04T09:37:58+00:00</updated>
    <author>
      <name>/u/National_Control4101</name>
      <uri>https://old.reddit.com/user/National_Control4101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've just released Cruxy - an adaptive optimiser that lets you fine-tune billion-parameter models on consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; - Drop-in replacement for AdamW - Meta-Lion mode uses 1/3 the memory of AdamW - Automatic stability control - no scheduler tuning needed - Verified on TinyLlama 1.1B and Qwen 2.5 1.5B on a GTX 1650 (4GB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (Shakespeare GPT):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Optimiser&lt;/th&gt; &lt;th&gt;Final Loss&lt;/th&gt; &lt;th&gt;Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AdamW&lt;/td&gt; &lt;td&gt;1.6843&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta3&lt;/td&gt; &lt;td&gt;1.6413&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta-Lion&lt;/td&gt; &lt;td&gt;1.6633&lt;/td&gt; &lt;td&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pip install Cruxy&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/christophergardner-star/Crux1"&gt;https://github.com/christophergardner-star/Crux1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions. Built this on evenings and weekends because cloud GPUs are expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National_Control4101"&gt; /u/National_Control4101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T09:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu5pe</id>
    <title>WTF are these AI companies doing where they supposedly are the cause of the ram price spike?</title>
    <updated>2025-12-04T07:46:40+00:00</updated>
    <author>
      <name>/u/Red_Redditor_Reddit</name>
      <uri>https://old.reddit.com/user/Red_Redditor_Reddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand what could justify that much investment. Maybe I'm way out of the loop, but what huge application are they expecting that would have this kind of payout? Why is there all of the sudden this spike instead of a slower increase in demand? Like I kinda get the overall GPU demand, but this sudden dramatic change in RAM demand doesn't make sense to me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Redditor_Reddit"&gt; /u/Red_Redditor_Reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu46s</id>
    <title>New model, microsoft/VibeVoice-Realtime-0.5B</title>
    <updated>2025-12-04T07:43:58+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt; &lt;img alt="New model, microsoft/VibeVoice-Realtime-0.5B" src="https://external-preview.redd.it/yC3RHTaiptQZaDONKxzLP6lQoJh8pT8uDk6mruPADNY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b568bf1e3f993edb57eab9f43241d593fd7c1c2" title="New model, microsoft/VibeVoice-Realtime-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VibeVoice: A Frontier Open-Source Text-to-Speech Model&lt;/p&gt; &lt;p&gt;VibeVoice-Realtime is a lightweight real‚Äëtime text-to-speech model supporting streaming text input. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long before a full answer is generated. It produces initial audible speech in ~300 ms (hardware dependent).&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;Parameter size: 0.5B (deployment-friendly) Realtime TTS (~300 ms first audible latency) Streaming text input Robust long-form speech generation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqxbw</id>
    <title>Chinese CXMT unveils DDR5-8000 RAM</title>
    <updated>2025-12-04T04:43:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory"&gt;https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chinese RAM might be the way to buck the trend of rising prices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdh0sm</id>
    <title>8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich</title>
    <updated>2025-12-03T21:26:43+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt; &lt;img alt="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" src="https://external-preview.redd.it/ZzlmajZ6b3gzMjVnMYyMXOA9G9iEfbHd4uR1YsqLbApEsnv66h0V49mXIA5l.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f3aaa8ebe40ec05d81117c0df8ce6a792a1bd" title="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o8n25oox325g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
