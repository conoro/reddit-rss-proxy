<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-20T00:31:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qh5yvm</id>
    <title>LlamaBarn 0.23 — tiny macOS app for running local LLMs (open source)</title>
    <updated>2026-01-19T14:43:16+00:00</updated>
    <author>
      <name>/u/erusev_</name>
      <uri>https://old.reddit.com/user/erusev_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5yvm/llamabarn_023_tiny_macos_app_for_running_local/"&gt; &lt;img alt="LlamaBarn 0.23 — tiny macOS app for running local LLMs (open source)" src="https://preview.redd.it/2gia31vxibeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2532ca7ae291b1405d8ccfc782756cc3ea05dbc" title="LlamaBarn 0.23 — tiny macOS app for running local LLMs (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;code&gt;r/LocalLLaMA&lt;/code&gt;! We posted about LlamaBarn back when it was in version 0.8. Since then, we've shipped 15 releases and wanted to share what's new.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ggml-org/LlamaBarn"&gt;https://github.com/ggml-org/LlamaBarn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The big change: Router Mode&lt;/p&gt; &lt;p&gt;LlamaBarn now uses llama-server's Router Mode. The server runs continuously in the background and loads models automatically when they're requested. You no longer have to manually select a model before using it — just point your app at http://localhost:2276/v1 and request any installed model by name.&lt;/p&gt; &lt;p&gt;Models also unload automatically when idle (configurable: off, 5m, 15m, 1h), so you're not wasting memory when you're not using them.&lt;/p&gt; &lt;p&gt;You can see the rest of the changes in the &lt;a href="https://github.com/ggml-org/LlamaBarn/releases"&gt;GitHub releases&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Install: &lt;code&gt;brew install --cask llamabarn&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erusev_"&gt; /u/erusev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2gia31vxibeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5yvm/llamabarn_023_tiny_macos_app_for_running_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5yvm/llamabarn_023_tiny_macos_app_for_running_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh9srb</id>
    <title>I built a Windows all-in-one local AI studio opensource, looking for contributors</title>
    <updated>2026-01-19T17:01:38+00:00</updated>
    <author>
      <name>/u/Motor-Resort-5314</name>
      <uri>https://old.reddit.com/user/Motor-Resort-5314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"&gt; &lt;img alt="I built a Windows all-in-one local AI studio opensource, looking for contributors" src="https://b.thumbs.redditmedia.com/oEucn08yeClrk3dFP63fE03zNpUinSUaAEIZWywg7zI.jpg" title="I built a Windows all-in-one local AI studio opensource, looking for contributors" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been building a project called &lt;strong&gt;V6rge&lt;/strong&gt;. It’s a Windows-based local AI studio meant to remove the constant pain of Python, CUDA, and dependency breakage when running models locally.&lt;/p&gt; &lt;p&gt;V6rge uses its own isolated runtime, so it doesn’t touch your system Python. It’s built for both developers and non-coders who just want local AI tools that work without setup.&lt;/p&gt; &lt;p&gt;It works as a modular studio. Each feature has its own category, and users simply download the model that fits their hardware. No manual installs, no environment tuning.&lt;/p&gt; &lt;p&gt;Current features include:&lt;/p&gt; &lt;p&gt;Local LLMs (Qwen 7B, 32B, 72B) with hardware guidance&lt;br /&gt; Vision models for image understanding&lt;br /&gt; Image generation (FLUX, Qwen-Image)&lt;br /&gt; Music generation (MusicGen)&lt;br /&gt; Text-to-speech (Chatterbox)&lt;br /&gt; A real local agent that can execute tasks on your PC&lt;br /&gt; Video generation, 3D generation, image upscaling, background removal, and vocal separation&lt;/p&gt; &lt;p&gt;All models are managed through a built-in model manager that shows RAM and VRAM requirements.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/80tjarmt5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a1a34e3512541d01f34261d16f53bee1408dd04"&gt;https://preview.redd.it/80tjarmt5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a1a34e3512541d01f34261d16f53bee1408dd04&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k5b8sa6x5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=53788a739da00cd525e2f7e1245233b8b342f358"&gt;https://preview.redd.it/k5b8sa6x5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=53788a739da00cd525e2f7e1245233b8b342f358&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hfzt1sy26ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8014ab04616d23fbbefa9bc6437c485d9c53bdb"&gt;https://preview.redd.it/hfzt1sy26ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8014ab04616d23fbbefa9bc6437c485d9c53bdb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shcg9usj6ceg1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5f5244ee4a72b0769f81de25d0c80763d2680f7"&gt;https://preview.redd.it/shcg9usj6ceg1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5f5244ee4a72b0769f81de25d0c80763d2680f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hfotsbxa7ceg1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f72b9dc0e04a00b9a4b1952b02a62576b94226c"&gt;https://preview.redd.it/hfotsbxa7ceg1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f72b9dc0e04a00b9a4b1952b02a62576b94226c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/urve0fee7ceg1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac007209f6f9589ecd694e8d78ecaddb25bb41d3"&gt;https://preview.redd.it/urve0fee7ceg1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac007209f6f9589ecd694e8d78ecaddb25bb41d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve open sourced it because I don’t want this to be just my project, I want it to become the best possible local AI studio. I don’t have a GPU machine, so I need help with testing across hardware, optimization, bug fixing, and adding more models and features. I’m honestly struggling to push this as far as it should go on my own, and community contributions would make a huge difference.&lt;br /&gt; Repo - &lt;a href="https://github.com/Dedsec-b/v6rge-releases-"&gt;https://github.com/Dedsec-b/v6rge-releases-&lt;/a&gt;&lt;/p&gt; &lt;p&gt;package -exe - &lt;a href="https://github.com/Dedsec-b/v6rge-releases-/releases/tag/v0.1.5"&gt;https://github.com/Dedsec-b/v6rge-releases-/releases/tag/v0.1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor-Resort-5314"&gt; /u/Motor-Resort-5314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhh3mi</id>
    <title>anyone would be interested at Tier 3 DC H200's?</title>
    <updated>2026-01-19T21:18:58+00:00</updated>
    <author>
      <name>/u/DjuricX</name>
      <uri>https://old.reddit.com/user/DjuricX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have hands on several DC's nodes for rent currently, and theres new clusters of H200's added, willing to offer free tests to run, also theyre all bare metal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DjuricX"&gt; /u/DjuricX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhh3mi/anyone_would_be_interested_at_tier_3_dc_h200s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhh3mi/anyone_would_be_interested_at_tier_3_dc_h200s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhh3mi/anyone_would_be_interested_at_tier_3_dc_h200s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T21:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgrw3d</id>
    <title>Just put together my new setup(3x v620 for 96gb vram)</title>
    <updated>2026-01-19T02:31:42+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt; &lt;img alt="Just put together my new setup(3x v620 for 96gb vram)" src="https://b.thumbs.redditmedia.com/mJVsjBo7IHRjbuffjsuNrdAMt7krK8sWOZUchd5L7tE.jpg" title="Just put together my new setup(3x v620 for 96gb vram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgrw3d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T02:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhf451</id>
    <title>What are the main uses of small models like gemma3:1b</title>
    <updated>2026-01-19T20:07:01+00:00</updated>
    <author>
      <name>/u/SchoolOfElectro</name>
      <uri>https://old.reddit.com/user/SchoolOfElectro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find it very interesting that models like these run on really low hardware but what are the main uses of model like gemma3:1b? Basic questions? Simple math?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SchoolOfElectro"&gt; /u/SchoolOfElectro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhf451/what_are_the_main_uses_of_small_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhf451/what_are_the_main_uses_of_small_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhf451/what_are_the_main_uses_of_small_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh3oj0</id>
    <title>Intel LLM-Scaler-Omni Update Brings ComfyUI &amp; SGLang Improvements On Arc Graphics</title>
    <updated>2026-01-19T13:07:55+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh3oj0/intel_llmscaleromni_update_brings_comfyui_sglang/"&gt; &lt;img alt="Intel LLM-Scaler-Omni Update Brings ComfyUI &amp;amp; SGLang Improvements On Arc Graphics" src="https://external-preview.redd.it/Jst-QPnk04q7baqsRFa3aXcG3HFaoXucjPJMdxa8Uf4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6083575735ba4b966f485ffd86a696d2b6276328" title="Intel LLM-Scaler-Omni Update Brings ComfyUI &amp;amp; SGLang Improvements On Arc Graphics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/intel/llm-scaler/releases/tag/omni-0.1.0-b5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh3oj0/intel_llmscaleromni_update_brings_comfyui_sglang/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh3oj0/intel_llmscaleromni_update_brings_comfyui_sglang/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T13:07:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhgm0r</id>
    <title>Bringing Anthropic's "advanced tool use" pattern to local models with mcpx</title>
    <updated>2026-01-19T21:01:03+00:00</updated>
    <author>
      <name>/u/vicdotso</name>
      <uri>https://old.reddit.com/user/vicdotso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic recently published their [advanced tool use](&lt;a href="https://www.anthropic.com/engineering/advanced-tool-use"&gt;https://www.anthropic.com/engineering/advanced-tool-use&lt;/a&gt;) approach - the key insight is moving intermediate computation outside the model's context window. Instead of the model reading, processing, and storing everything in-context, you offload that to external tools and only pass summaries back.&lt;/p&gt; &lt;p&gt;This matters even more for local models where context is tighter and inference is slower.&lt;/p&gt; &lt;p&gt;The problem: MCP is great for tool connectivity, but loading tool schemas upfront burns 40-50k tokens before you start working. That's rough when you're running a 32k context model locally.&lt;/p&gt; &lt;p&gt;Built mcpx (fork with added features) to solve this. Tools are discovered at runtime through bash instead of loaded at the API layer:&lt;/p&gt; &lt;p&gt;- mcpx ( list all servers/tools )&lt;/p&gt; &lt;p&gt;- mcpx grep &amp;quot;*browser*&amp;quot; ( search by pattern )&lt;/p&gt; &lt;p&gt;- mcpx playwright/click ( get schema for one tool )&lt;/p&gt; &lt;p&gt;- mcpx playwright/click '{&amp;quot;selector&amp;quot;: &amp;quot;#submit&amp;quot;}' ( call tool )&lt;/p&gt; &lt;p&gt;some features and advantages:&lt;/p&gt; &lt;p&gt;- ~400 tokens instead of ~47k for tool definitions&lt;/p&gt; &lt;p&gt;- Any model with bash/tool calling can use MCP servers&lt;/p&gt; &lt;p&gt;- Daemon mode keeps stateful connections alive (browser sessions, db handles)&lt;/p&gt; &lt;p&gt;- Globally disabled tools ( like .gitignore for MCP )&lt;/p&gt; &lt;p&gt;- Prompt cache stays intact when adding servers&lt;/p&gt; &lt;p&gt;The examples/advanced_tool_use.sh in the repo shows the full pattern - orchestrating multi-step workflows where the model directs but doesn't hold all the data.&lt;/p&gt; &lt;p&gt;GitHub: github.com/cs50victor/mcpx&lt;/p&gt; &lt;p&gt;Working on MCP registry support if anyone wants to contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vicdotso"&gt; /u/vicdotso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgm0r/bringing_anthropics_advanced_tool_use_pattern_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgm0r/bringing_anthropics_advanced_tool_use_pattern_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgm0r/bringing_anthropics_advanced_tool_use_pattern_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T21:01:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhk5j9</id>
    <title>nvfp4 on Blackwell: sglang, vllm, trt</title>
    <updated>2026-01-19T23:16:05+00:00</updated>
    <author>
      <name>/u/ARCHLucifer</name>
      <uri>https://old.reddit.com/user/ARCHLucifer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why architecture of kernels from hardware developer and end users differs slightly ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/advpropx/status/2013383198466556394?s=46"&gt;https://x.com/advpropx/status/2013383198466556394?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ARCHLucifer"&gt; /u/ARCHLucifer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T23:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh10q9</id>
    <title>Demo: On-device browser agent (Qwen) running locally in Chrome</title>
    <updated>2026-01-19T10:48:29+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"&gt; &lt;img alt="Demo: On-device browser agent (Qwen) running locally in Chrome" src="https://external-preview.redd.it/MzNzcDNuMGdjYWVnMcLtcBYDX5SdJ9uQfQaEUwyxr5ovu1B5qUxuDFDhwgNH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67f9a368e6dac7735d2b2e3f72bc407aa88b54c0" title="Demo: On-device browser agent (Qwen) running locally in Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! wanted to share a cool demo of LOCAL Browser agent (powered by Web GPU Liquid LFM &amp;amp; Alibaba Qwen models) opening the All in Podcast on Youtube running as a chrome extension. &lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/RunanywhereAI/on-device-browser-agent"&gt;https://github.com/RunanywhereAI/on-device-browser-agent&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ljp6zwzfcaeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhli1x</id>
    <title>Best local models for synthetic data generation?</title>
    <updated>2026-01-20T00:11:16+00:00</updated>
    <author>
      <name>/u/mugacariya</name>
      <uri>https://old.reddit.com/user/mugacariya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Was wondering if there were any benchmarks or personal opinions on what local models are best for synthetic data generation for the purpose of sequence classification via. BERT. Papers I've read that do stuff like this utilize Llama7b and/or GPT 4o-mini, which seem outdated in comparison to the amount of new local models released in 2025. Currently going to try either Ministral 3 or gpt-oss20b and wanted to see anyone else's experience on this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mugacariya"&gt; /u/mugacariya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhli1x/best_local_models_for_synthetic_data_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhli1x/best_local_models_for_synthetic_data_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhli1x/best_local_models_for_synthetic_data_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhinqq</id>
    <title>Best MoE models for 64gb RAM &amp; CPU inference?</title>
    <updated>2026-01-19T22:18:01+00:00</updated>
    <author>
      <name>/u/GamerFromGamerTown</name>
      <uri>https://old.reddit.com/user/GamerFromGamerTown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I've been looking around around for good ~A3B models that can run well on my hardware, but this space seems to be pretty saturated with options; among these, &lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;GLM-4.7-Flash&lt;/a&gt;, &lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;NVIDIA-Nemotron-3-Nano-30B-A3B&lt;/a&gt;, &lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;gpt-oss-20b&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; , &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B"&gt;Qwen3-30B-A3B&lt;/a&gt;, and &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct"&gt;Qwen3-Next-80B-A3B-Instruct&lt;/a&gt; seem to be the most popular choices, though I might be missing one or two! With them not really sharing many benchmarks, it can be a bit difficult to compare them; Nemotron-A3B and gpt-oss 20b seem to be pretty popular with the people around here, but GLM-4.7 flash just released, which people seem to feel pretty positively about.&lt;/p&gt; &lt;p&gt;I'll just be doing some coding help, math, and maybe some online/offline RAG. If you have other use cases though, feel free to share!&lt;/p&gt; &lt;p&gt;Given my mediocre Alaskan internet, it would be impossible to download them all to try them out, so anyone with experience trying some of these would be greatly appreciated. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerFromGamerTown"&gt; /u/GamerFromGamerTown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgx83t</id>
    <title>3x3090 + 3060 in a mid tower case</title>
    <updated>2026-01-19T06:59:39+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt; &lt;img alt="3x3090 + 3060 in a mid tower case" src="https://b.thumbs.redditmedia.com/isvewN3PNijf6_OZxoF82ROhZEAD0nfG7ddsr3ghkYU.jpg" title="3x3090 + 3060 in a mid tower case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to go all out and max out this desktop. I was lucky to find 3090 cards for around 600 usd, over a period of 3 months and decided to go for it. &lt;/p&gt; &lt;p&gt;The RAM was a bit more expensive, but I had 64 bought before the price spiked.&lt;/p&gt; &lt;p&gt;I didn’t want to change the case, because I through it’s a high quality case and it would be a shame to toss it. So made the most out of it!&lt;/p&gt; &lt;p&gt;Specs: * Fractal Define 7 Mid Tower * 3x3090 + 1x3060 (86gb total, but 72gb VRAM main) * 128GB DDR4 (Corsair 4x32) * Corsair HX1500i 1500w (has 7 PCIe power cables) * Vertical mounts are all cheap from AliExpress * ASUS Maximus XII Hero — has only 3x PCIe16x, had to deactivate the 2nd NVMe to use the 3rd PCIe16x in 4x, the 4th GPU (the 3060) is on a riser from a PCIe1x. * For drives, only one NVMe of 1TB works, I also bought 2x2TB SSDs that I tried in RAID but the performance was terrible (and they are limited to 500mb from the SATA interface, which I didn’t know…) so I keep them as 2 drives.&lt;/p&gt; &lt;p&gt;Temperatures are holding surprisingly well. The gap between the cards is about the size of an empty PCIe slot, maybe a bit more. &lt;/p&gt; &lt;p&gt;Temperature was a big improvement compared to having just 2x3090 stacked without any space between them — the way the motherboard is designed to use them.&lt;/p&gt; &lt;p&gt;In terms of performance 3x3090 is great! There are great options in the 60-65gb range with the extra space to 72gb VRAM used for context.&lt;/p&gt; &lt;p&gt;I am not using the RAM for anything other than to load models, and the speed is amazing when everything is loaded in VRAM! &lt;/p&gt; &lt;p&gt;Models I started using a lot: * gpt-oss-120b in MXFP4 with 60k context * glm-4.5-air in IQ4_NL with 46k context * qwen3-vl-235b in TQ1_0 (surprisingly good!) * minimax-M2-REAP-139B in Q3_K_S with 40k context&lt;/p&gt; &lt;p&gt;But still return a lot to old models for context and speed: * devstral-small-2-24 in Q8_0 with 200k context * qwen3-coder in Q8 with 1M (!!) context (using RAM) * qwen3-next-80b in Q6_K with 60k context — still my favourite for general chat, and the Q6 makes me trust it more than Q3-Q4 models&lt;/p&gt; &lt;p&gt;The 3060 on the riser from PCIe1x is very slow at loading the models, however, once it’s loaded it works great! I am using it for image generation and TTS audio generation mostly (for Open WebUI).&lt;/p&gt; &lt;p&gt;Also did a lot of testing on using 2x3090 via normal PCIe, with a 3rd card via riser — it works same as normal PCIe! But the loading takes forever (sometimes over 2-3 minutes) and you simply can’t use the RAM for context because of how slow it is — so I am considering the current setup to be “maxed out” because I don’t think adding a 4th 3090 will be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgx83t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:59:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgwup8</id>
    <title>Is Local Coding even worth setting up</title>
    <updated>2026-01-19T06:38:37+00:00</updated>
    <author>
      <name>/u/Interesting-Fish6494</name>
      <uri>https://old.reddit.com/user/Interesting-Fish6494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am new to Local LLM but have been having a lot of issues setting up a local LLM coding environment so wanted some suggestions from people.I have a 5070 ti (16gb vram).&lt;/p&gt; &lt;p&gt;I have tried to use Kilo code with qwen 2.5 coder 7B running through ollama but the context size feels so low that it finishes the context within a single file of my project.&lt;/p&gt; &lt;p&gt;How are other people with a 16gb GPU dealing with local llm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Fish6494"&gt; /u/Interesting-Fish6494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhkh2z</id>
    <title>I FP8 quantized GLM 4.7 Flash!</title>
    <updated>2026-01-19T23:28:43+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I know it ain't much, I finally decided to try and be the first out to fp8 quant a newly dropped model. I would love to hear feedback if you try it. Steps to get it running are in the README :) &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8"&gt;https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T23:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh442y</id>
    <title>Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)</title>
    <updated>2026-01-19T13:27:32+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt; &lt;img alt="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" src="https://preview.redd.it/85cs39k6daeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72f0ad403efa3ee18868a0b8bf289eb713cca04a" title="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently finished my 3x3090 setup, and thought of sharing my experience.&lt;/p&gt; &lt;p&gt;This is very much a personal observation, with some very basic testing. &lt;/p&gt; &lt;p&gt;The benchmark is by no means precise, however, after checking the numbers, it is very much aligned with &amp;quot;how I feels they perform&amp;quot; after a few days of bouncing between them. All the above are running on CUDA 12 llama.cpp via LM Studio (nothing special). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Large models (&amp;gt; 100 B)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;All big models run in roughly the same ballpark—about &lt;strong&gt;30 tok/s&lt;/strong&gt; in everyday use. GPT‑OSS‑120 runs a bit faster than the other large models, but the difference is only noticeable on very short answers; you wouldn’t notice it during longer conversations. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Qwen3‑VL 235 B (TQ1, 1.66‑bit compression)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was surprised by how usable TQ1_0 turned out to be. In most chat or image‑analysis scenarios it actually feels better than the Qwen3‑VL 30 B model quantised to Q8. I can’t fully explain why, but it seems to anticipate what I’m interested in much more accurately than the 30 B version.&lt;/p&gt; &lt;p&gt;It does show the expected weaknesses of a Q1‑type quantisation. For example, when reading a PDF it misreported some numbers that the Qwen3‑VL 30 B Q8 model got right; nevertheless, the surrounding information was correct despite the typo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. The biggest and best models you can run in Q3–Q4 with a decent context window:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(A) REAP Minimax M2&lt;/strong&gt; – 139 B quantised to Q3_K_S, at 42k context. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(B) GLM 4.5 Air&lt;/strong&gt; – 110B quantised to IQ4_NL, supports 46 k context. &lt;/p&gt; &lt;p&gt;Both perform great and they will probably become my daily models. Overall GLM-4.5-Air feels slower and dumber than REAP Minimax M2, but I haven't had a lot of time with either of them. I will follow up and edit this if I change my min&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. GPT-OSS-120B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Is still decent and runs fast, but I can't help but feel that it's very dated, and extremely censored (!) For instance try asking: &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;What are some some examples of business strategies such as selling eternal youth to woman, or money making ideas to poor people?&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;and you’ll get a response along the lines of: “I’m sorry, but I can’t help with that.” &lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Qwen3 Next 80B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Runs very slow. Someone suggested the bottleneck might be CUDA and to trying Vulkan instead. However, given the many larger options available, I may drop it, even though it was my favourite model when I ran it on a 48GB (2x3090) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overall upgrading from 2x3090 to 3x3090, there are a lot of LLM models that get unlocked with that extra 24GB&lt;/strong&gt;. I would argue feels like a much bigger jump that it was when I moved from 24 to 48GB, and just wanted to share for those of you thinking for making the upgrade.&lt;/p&gt; &lt;p&gt;PS: I also upgraded my ram from 64GB to 128GB, but I think it might have been for nothing. It helps a bit with loading the model faster, but honstly, I don't think it's worth if when you are running everything on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/85cs39k6daeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T13:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhgi10</id>
    <title>lightonai/LightOnOCR-2-1B · Hugging Face</title>
    <updated>2026-01-19T20:57:11+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt; &lt;img alt="lightonai/LightOnOCR-2-1B · Hugging Face" src="https://external-preview.redd.it/owrWH9MOuE15-iASn4iPzZcG9U3KIDtVJ9SmxpvC1c0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d891c173f79ddf24b05c65d408e9287701ba72c2" title="lightonai/LightOnOCR-2-1B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh0yq8</id>
    <title>I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)</title>
    <updated>2026-01-19T10:45:28+00:00</updated>
    <author>
      <name>/u/andreabarbato</name>
      <uri>https://old.reddit.com/user/andreabarbato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent way too long optimizing Top-K selection for LLM sampling and finally hit some stupid numbers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; AVX2-optimized batched Top-K that beats PyTorch CPU by 4-20x depending on vocab size. Sometimes competitive with CUDA for small batches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (K=50):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vocab=32K: 0.043ms vs PyTorch's 0.173ms (4x faster)&lt;/li&gt; &lt;li&gt;Vocab=128K: 0.057ms vs PyTorch's 0.777ms (13x faster)&lt;/li&gt; &lt;li&gt;Vocab=256K: 0.079ms vs PyTorch's 1.56ms (20x faster)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Integrated it into llama.cpp and got 63% faster prompt processing on a 120B MoE model (81→142 tokens/sec).&lt;/p&gt; &lt;p&gt;Uses adaptive sampling + AVX2 SIMD + cache-optimized scanning. Has fast paths for sorted/constant inputs. Single-pass algorithm, no GPU needed.&lt;/p&gt; &lt;p&gt;Includes pre-built DLLs and llama.cpp implementation (for windows).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/RAZZULLIX/fast_topk_batched"&gt;https://github.com/RAZZULLIX/fast_topk_batched&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or roasting, whichever you prefer.&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;can anyone try it and let me know if it works for them? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreabarbato"&gt; /u/andreabarbato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhlnsv</id>
    <title>Unsloth GLM 4.7-Flash GGUF</title>
    <updated>2026-01-20T00:17:58+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhg6rm</id>
    <title>GLM-4.7-FLASH-NVFP4 on huggingface (20.5 GB)</title>
    <updated>2026-01-19T20:45:46+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I published a mixed precision NVFP4 quantized version the new GLM-4.7-FLASH on HF, can any of you can test it and let me know how it goes, I would really appreciate it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4"&gt;https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjhlh</id>
    <title>GLM-4.7-Flash-GGUF is here!</title>
    <updated>2026-01-19T22:49:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt; &lt;img alt="GLM-4.7-Flash-GGUF is here!" src="https://external-preview.redd.it/xaz8me0jAeBOkTb7mKUXdYdIdr8aoSsiwENwulyOJmI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f21f70be7ae2e1b3f10f33471dbfc4c47ba6518" title="GLM-4.7-Flash-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhaq21</id>
    <title>New in llama.cpp: Anthropic Messages API</title>
    <updated>2026-01-19T17:33:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt; &lt;img alt="New in llama.cpp: Anthropic Messages API" src="https://external-preview.redd.it/zqasF6xdAR1yVfMl-Ppz2b8-S-Dv35pa4J_UeKummLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56eabcfaa752210d59dc7af42f1b2087636a579d" title="New in llama.cpp: Anthropic Messages API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhii5v</id>
    <title>My gpu poor comrades, GLM 4.7 Flash is your local agent</title>
    <updated>2026-01-19T22:12:06+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried many MoE models at 30B or under and all of them failed sooner or later in an agentic framework. If z.ai is not redirecting my requests to another model, then GLM 4.7 Flash is finally the reliable (soon local) agent that I desperately wanted.&lt;/p&gt; &lt;p&gt;I am running it since more than half an hour on opencode and it produced hundreds of thousands tokens in one session (with context compacting obviously) without any tool calling errors. It clones github repos, it runs all kind of commands, edits files, commits changes, all perfect, not a single error yet.&lt;/p&gt; &lt;p&gt;Can't wait for GGUFs to try this locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5wdq</id>
    <title>zai-org/GLM-4.7-Flash · Hugging Face</title>
    <updated>2026-01-19T14:40:27+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.7-Flash · Hugging Face" src="https://external-preview.redd.it/Qs0t4y5eLm-uwORWdP6T0dcwW2T6VJyQFBUSY70CTF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8700f4a43fe16a1031ccda94b517fd709573a5c3" title="zai-org/GLM-4.7-Flash · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhitrj</id>
    <title>GLM 4.7 Flash official support merged in llama.cpp</title>
    <updated>2026-01-19T22:24:24+00:00</updated>
    <author>
      <name>/u/ayylmaonade</name>
      <uri>https://old.reddit.com/user/ayylmaonade</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt; &lt;img alt="GLM 4.7 Flash official support merged in llama.cpp" src="https://external-preview.redd.it/AVP8Isc32PMjAyVGtAipaav3x8aU8JY8Lx1bZ_yPak0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43081fb39d8cfd3c8faeeb3516b7513654ed8fce" title="GLM 4.7 Flash official support merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayylmaonade"&gt; /u/ayylmaonade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
