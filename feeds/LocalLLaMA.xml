<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-12T00:57:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lx5n8c</id>
    <title>FYI Qwen3 235B A22B IQ4_XS works with 128 GB DDR5 + 8GB VRAM in Windows</title>
    <updated>2025-07-11T12:30:36+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Disclaimers: Nothing new here especially given the recent posts, but was supposed to report back at &lt;a href="/u/Evening_Ad6637"&gt;u/Evening_Ad6637&lt;/a&gt; et al. Furthermore, i am a total noob and do local LLM via LM Studio on Windows 11, so no fancy ik_llama.cpp etc., as it is just so convenient.)&lt;/p&gt; &lt;p&gt;I finally received 2x64 GB DDR5 5600 MHz Sticks (Kingston &lt;a href="https://www.kingston.com/datasheets/KF556C36BBE-8.pdf"&gt;Datasheet&lt;/a&gt;) giving me 128 GB RAM on my ITX Build. I did load the EXPO0 timing profile giving CL36 etc.&lt;br /&gt; This is complemented by a Low Profile RTX 4060 with 8 GB, all controlled by a Ryzen 9 7950X (any CPU would do).&lt;/p&gt; &lt;p&gt;Through LM Studio, I downloaded and ran both unsloth's 128K Q3_K_XL quant (103.7 GB) as well as managed to run the &lt;strong&gt;IQ4_XS&lt;/strong&gt; quant (125.5 GB) on a freshly restarted windows machine. (Haven't tried crashing or stress testing it yet, it currently works without issues).&lt;br /&gt; I left all model settings untouched and increased the context to ~17000. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Time to first token&lt;/strong&gt; on a prompt about a Berlin neighborhood took &lt;strong&gt;around 10 sec, then 3.3-2.7 tps.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I can try to provide any further information or run prompts for you and return the response as well as times. Just wanted to update you that this works. Cheers! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:30:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2dw4</id>
    <title>Is a heavily quantised Q235b any better than Q32b?</title>
    <updated>2025-07-11T09:24:06+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've come to the conclusion that Qwen's 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen's 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?&lt;/p&gt; &lt;p&gt;I'm in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it'll POST.&lt;/p&gt; &lt;p&gt;Is anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:24:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxbynb</id>
    <title>How much do you use your local model on average on a day?</title>
    <updated>2025-07-11T16:50:11+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In terms of minutes/hours or number of query/response?&lt;/p&gt; &lt;p&gt;I'm averaging around 90 minutes on good days and 30 minutes on bad days.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T16:50:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmldq</id>
    <title>LiquidAI LFM2 Model Released</title>
    <updated>2025-07-12T00:10:33+00:00</updated>
    <author>
      <name>/u/Federal-Effective879</name>
      <uri>https://old.reddit.com/user/Federal-Effective879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LiquidAI released their &lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38"&gt;LFM2 model family&lt;/a&gt;, and support for it was just &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14620"&gt;merged into llama.cpp&lt;/a&gt; a few hours ago. I haven't yet tried it locally, but I was quite impressed by their online demo of the 1.2B model. It had excellent world knowledge and general conversational coherence and intelligence for its size. I found it much better than SmolLM2 at everything, and similar in intelligence to Qwen 3 1.7B but with better world knowledge. Seems SOTA for its size. Context length is 32k tokens. The license disallows commercial use over $10M revenue, but for personal use or small commercial use it should be fine. In general the license didn't seem too bad.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal-Effective879"&gt; /u/Federal-Effective879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxgm02</id>
    <title>An alternative to semantic or benchmark-based routing: A preference-aligned router model</title>
    <updated>2025-07-11T19:53:20+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgm02/an_alternative_to_semantic_or_benchmarkbased/"&gt; &lt;img alt="An alternative to semantic or benchmark-based routing: A preference-aligned router model" src="https://preview.redd.it/dji5sexqsacf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15a563260f80a5416899522519ec64978ae98380" title="An alternative to semantic or benchmark-based routing: A preference-aligned router model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am one of the core maintainers of Arch (&lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;), an open-source proxy for LLMs written in Rust. A few days ago we launched Arch-Router (&lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;) on HuggingFace, a 1.5B router model designed for preference-aligned routing (and of course integrated in the proxy server). Full paper: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt; &lt;/p&gt; &lt;p&gt;As teams integrate multiple LLMs - each with different strengths, styles, or cost/latency profiles ‚Äî routing the right prompt to the right model becomes a critical part of the application design. But it‚Äôs still an open problem. Existing routing systems fall into two camps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Embedding-based or semantic routers&lt;/strong&gt; map the user‚Äôs prompt to a dense vector and route based on similarity ‚Äî but they struggle in practice: they lack context awareness (so follow-ups like ‚ÄúAnd Boston?‚Äù are misrouted), fail to detect negation or logic (‚ÄúI don‚Äôt want a refund‚Äù vs. ‚ÄúI want a refund‚Äù), miss rare or emerging intents that don‚Äôt form clear clusters, and can‚Äôt handle short, vague queries like ‚Äúcancel‚Äù without added context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance-based routers&lt;/strong&gt; pick models based on benchmarks like MMLU or MT-Bench, or based on latency or cost curves. But benchmarks often miss what matters in production: domain-specific quality or subjective preferences especially as developers evaluate the effectiveness of their prompts against selected models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Arch-Router takes a different approach: route by preferences written in plain language. You write rules like ‚Äúcontract clauses ‚Üí GPT-4o‚Äù or ‚Äúquick travel tips ‚Üí Gemini Flash.‚Äù The router maps the prompt (and conversation context) to those rules using a lightweight 1.5B autoregressive model. No retraining, no fragile if/else chains. We built this with input from teams at Twilio and Atlassian. It handles intent drift, supports multi-turn conversations, and lets you swap in or out models with a one-line change to the routing policy. Full details are in our paper (&lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;), but here‚Äôs a snapshot:&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1.5B parameters ‚Äî runs on a single GPU (or CPU for testing) &lt;/li&gt; &lt;li&gt;No retraining needed ‚Äî point it at any mix of LLMs&lt;/li&gt; &lt;li&gt;Outperforms larger closed models on conversational routing benchmarks (details in the paper)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you enjoy the paper, the model and the usage integrated via the proxy &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dji5sexqsacf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgm02/an_alternative_to_semantic_or_benchmarkbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgm02/an_alternative_to_semantic_or_benchmarkbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T19:53:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxgi3j</id>
    <title>People with a Mac Studio 512G: what are you doing with it?</title>
    <updated>2025-07-11T19:48:54+00:00</updated>
    <author>
      <name>/u/Dangerous-Yak3976</name>
      <uri>https://old.reddit.com/user/Dangerous-Yak3976</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sure, the full Deepseek R1 model loads, but the tokens per second are still way too slow to be useful. &lt;/p&gt; &lt;p&gt;So I‚Äôm just curious: for those of you who spent $10K+ on that nice little box, what are you actually doing with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Yak3976"&gt; /u/Dangerous-Yak3976 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T19:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwta86</id>
    <title>AMD's Pull Request for llama.cpp: Enhancing GPU Support</title>
    <updated>2025-07-11T00:45:46+00:00</updated>
    <author>
      <name>/u/Rrraptr</name>
      <uri>https://old.reddit.com/user/Rrraptr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp&lt;/p&gt; &lt;p&gt;Word is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.&lt;br /&gt; Discussions with the project leaders are planned in the near future to explore opportunities for further enhancements.&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14624"&gt;https://github.com/ggml-org/llama.cpp/pull/14624&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rrraptr"&gt; /u/Rrraptr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T00:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxbsw0</id>
    <title>Drummer's Snowpiercer 15B v2</title>
    <updated>2025-07-11T16:44:02+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbsw0/drummers_snowpiercer_15b_v2/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v2" src="https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4bca3c5e4e8fe8b6ea311aacca9d6f0ddbd42ee" title="Drummer's Snowpiercer 15B v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A finetune of ServiceNow's Alice 15B Thinker, but this prioritizes steerability and character adherence. Thinking will work most of the time but may need to wrangle it a bit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbsw0/drummers_snowpiercer_15b_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbsw0/drummers_snowpiercer_15b_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T16:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxhjjn</id>
    <title>Most energy efficient way to run Gemma 3 27b?</title>
    <updated>2025-07-11T20:31:02+00:00</updated>
    <author>
      <name>/u/Extremely_Engaged</name>
      <uri>https://old.reddit.com/user/Extremely_Engaged</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;What would be the most energy efficient (tokens per seconds does not matter, only tokens per watthours) to run Gemma 3 27b?&lt;/p&gt; &lt;p&gt;A 3090 capped at 210watts gives 25 t/s - this is what I'm using now. I'm wondering if there is a more efficient alternative.&lt;/p&gt; &lt;p&gt;Ryzen 395+ AI desktop version seems to be ~120 watts, and 10/s - so that would worse, actually?&lt;/p&gt; &lt;p&gt;a 4090 might be a bit more efficient? Like 20%?&lt;/p&gt; &lt;p&gt;Macs seems to be on the same scale, less power but also less T/s.&lt;/p&gt; &lt;p&gt;My impression is that it's all a bit the same in terms of power, macs have a bit less idle power than a PC, but for the rest there isn't huge differences? &lt;/p&gt; &lt;p&gt;My main question if there are significant improvements (&amp;gt;50%) in tokens per watt-hour in changing from a 3090 to a mac or a ryzen ai (or something else?). My impression is that there isn't really much difference.&lt;/p&gt; &lt;p&gt;EDIT: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k9e5p0/gemma3_performance_on_ryzen_ai_max/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k9e5p0/gemma3_performance_on_ryzen_ai_max/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is (I think?) 55 watts and 10 tokens per second. This would be kind of great result from ryzen 395 ai. Did anyone test this? Does anyone own a *mobile* ryzen ai pc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extremely_Engaged"&gt; /u/Extremely_Engaged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxhjjn/most_energy_efficient_way_to_run_gemma_3_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxhjjn/most_energy_efficient_way_to_run_gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxhjjn/most_energy_efficient_way_to_run_gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T20:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2hn2</id>
    <title>Uncensored LLM ranking for roleplay?</title>
    <updated>2025-07-11T09:31:05+00:00</updated>
    <author>
      <name>/u/mikemend</name>
      <uri>https://old.reddit.com/user/mikemend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every day, a bunch of models appear, making it difficult to choose which ones to use for uncensored role-playing. Previously, the Ayumi LLM Role Play &amp;amp; ERP Ranking data was somewhat of a guide, but now I can't find a list that is even close to being up to date. It's difficult to choose from among the many models with fantasy names.&lt;/p&gt; &lt;p&gt;Is there a list that might help with which models are better for role-playing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikemend"&gt; /u/mikemend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4qhp</id>
    <title>Moonshot AI about to release their 1T parameters model?</title>
    <updated>2025-07-11T11:45:02+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt; &lt;img alt="Moonshot AI about to release their 1T parameters model?" src="https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4f19a5b1a2112e5c15ddbc66a6e92a07eecb3c7" title="Moonshot AI about to release their 1T parameters model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is from their website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kts1w8a7g8cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:45:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxd7nh</id>
    <title>H-Net: a hierarchical network that replaces tokenization with a dynamic chunking process directly inside the model, automatically discovering and operating over meaningful units of data</title>
    <updated>2025-07-11T17:38:39+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2507.07955"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T17:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxep4s</id>
    <title>Deepseek's Simple, yet Genius Data Generation Pipeline</title>
    <updated>2025-07-11T18:37:04+00:00</updated>
    <author>
      <name>/u/Which_Pound_6751</name>
      <uri>https://old.reddit.com/user/Which_Pound_6751</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxep4s/deepseeks_simple_yet_genius_data_generation/"&gt; &lt;img alt="Deepseek's Simple, yet Genius Data Generation Pipeline" src="https://external-preview.redd.it/LPMywol0PjbBlHlhf3Jbdzts6Czu2r2vv80bTDER02E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80fa2acf6a19fd5b5fd9ac95e492085b70eb646" title="Deepseek's Simple, yet Genius Data Generation Pipeline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek Prover V2 - formal reasoning math model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Which_Pound_6751"&gt; /u/Which_Pound_6751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/wzpGWboeRBo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxep4s/deepseeks_simple_yet_genius_data_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxep4s/deepseeks_simple_yet_genius_data_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T18:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx85jo</id>
    <title>Devstral-Vision-Small-2507</title>
    <updated>2025-07-11T14:20:52+00:00</updated>
    <author>
      <name>/u/faldore</name>
      <uri>https://old.reddit.com/user/faldore</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt; &lt;img alt="Devstral-Vision-Small-2507" src="https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0b820cc0547e8c6ffaecf9eb33b63916abc0d61" title="Devstral-Vision-Small-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral released Devstral-Small-2507 - which is AWESOME! But, they released without vision capability. I didn't like that.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507"&gt;&lt;strong&gt;Devstral-Vision-Small-2507&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507-gguf"&gt;&lt;strong&gt;Devstral-Vision-Small-2507-gguf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did some model surgery. I started with Mistral-Small-3.2-24B-Instruct-2506, and replaced its language tower with Devstral-Small-2507.&lt;/p&gt; &lt;p&gt;The conversion script is in the repo, if you'd like to take a look.&lt;/p&gt; &lt;p&gt;Tested, it works fine. I'm sure that it could do with a bit of RL to gel the vision and coding with real world use cases, but I'm releasing as is - a useful multimodal coding model.&lt;/p&gt; &lt;p&gt;Enjoy.&lt;/p&gt; &lt;p&gt;-Eric&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5"&gt;https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93"&gt;https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faldore"&gt; /u/faldore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmr2h</id>
    <title>Thank you r/LocalLLaMA! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:18:17+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt; &lt;img alt="Thank you r/LocalLLaMA! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6bbfe217f90907d855e12d2f6b2845d320a54e6" title="Thank you r/LocalLLaMA! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚úÖ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ah6imcae6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T00:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx8qrz</id>
    <title>ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the ‚ÄúAlps‚Äù supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes.</title>
    <updated>2025-07-11T14:45:09+00:00</updated>
    <author>
      <name>/u/nat2r</name>
      <uri>https://old.reddit.com/user/nat2r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"&gt; &lt;img alt="ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the ‚ÄúAlps‚Äù supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes." src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the ‚ÄúAlps‚Äù supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nat2r"&gt; /u/nat2r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx7l3k</id>
    <title>This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma</title>
    <updated>2025-07-11T13:57:36+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"&gt; &lt;img alt="This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma" src="https://preview.redd.it/r2bp20do39cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa04ae911a28403b4ac7702442fb11eb655146a3" title="This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MedGemma 27B Multimodal for complex multimodal &amp;amp; longitudinal EHR interpretation: &lt;a href="https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"&gt;https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MedSigLIP: a lightweight image/text encoder for medical image retrieval/classification: &lt;a href="https://huggingface.co/google/medsiglip-448"&gt;https://huggingface.co/google/medsiglip-448&lt;/a&gt;&lt;/p&gt; &lt;p&gt;T5Gemma: lightweight yet powerful encoder-decoder research models: &lt;a href="https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86"&gt;https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r2bp20do39cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxb0eo</id>
    <title>The 1T Kimi K2 model is using DeepSeek V3 architecture</title>
    <updated>2025-07-11T16:13:31+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/"&gt; &lt;img alt="The 1T Kimi K2 model is using DeepSeek V3 architecture" src="https://preview.redd.it/l3gpvb5or9cf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed19f0e0b0c28fcc15556f566717d11201a68611" title="The 1T Kimi K2 model is using DeepSeek V3 architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l3gpvb5or9cf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T16:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxgb9q</id>
    <title>Stanford's CS336 2025 (Language Modeling from Scratch) is now available on YouTube</title>
    <updated>2025-07-11T19:41:07+00:00</updated>
    <author>
      <name>/u/realmvp77</name>
      <uri>https://old.reddit.com/user/realmvp77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_"&gt;Here's the YouTube Playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://stanford-cs336.github.io/spring2025/"&gt;Here's the CS336 website with assignments, slides etc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been studying it for a week and it's the best course on LLMs I've seen online. The assignments are &lt;strong&gt;huge&lt;/strong&gt;, very in-depth, and they require you to write &lt;strong&gt;a lot&lt;/strong&gt; of code from scratch. For example, the &lt;a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf"&gt;1st assignment pdf&lt;/a&gt; is 50 pages long and it requires you to implement the BPE tokenizer, a simple transformer LM, cross-entropy loss and AdamW and train models on OpenWebText&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realmvp77"&gt; /u/realmvp77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T19:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx94ht</id>
    <title>Kimi K2 - 1T MoE, 32B active params</title>
    <updated>2025-07-11T15:00:42+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"&gt; &lt;img alt="Kimi K2 - 1T MoE, 32B active params" src="https://b.thumbs.redditmedia.com/vOQCL6pQbXue2TSAcO_fvTvDBTLRgjIBjMBbSQhYkWI.jpg" title="Kimi K2 - 1T MoE, 32B active params" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx94ht"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T15:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx8xdm</id>
    <title>moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)</title>
    <updated>2025-07-11T14:52:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt; &lt;img alt="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" src="https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65e4d917b0768ba9727a840f3e7b4ddd3fdb7ea3" title="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features"&gt;&lt;/a&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.&lt;/li&gt; &lt;li&gt;MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.&lt;/li&gt; &lt;li&gt;Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Variants&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Base&lt;/strong&gt;: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt;: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx62hd</id>
    <title>Nvidia being Nvidia: FP8 is 150 Tflops faster when kernel name contain "cutlass"</title>
    <updated>2025-07-11T12:50:38+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx6dcm</id>
    <title>llama2.c running on the original 2007 iPhone</title>
    <updated>2025-07-11T13:04:10+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt; &lt;img alt="llama2.c running on the original 2007 iPhone" src="https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d84dc04e7624cefc75d18c603d35424468ce1db" title="llama2.c running on the original 2007 iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3u6728ask8cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx9pny</id>
    <title>Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!</title>
    <updated>2025-07-11T15:23:24+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"&gt; &lt;img alt="Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!" src="https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3898e31bb7d9fbf8198401001a795859f33eafcb" title="Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19"&gt;https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1tzaif5j9cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T15:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx5awq</id>
    <title>Friendly reminder that Grok 3 should be now open-sourced</title>
    <updated>2025-07-11T12:13:48+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt; &lt;img alt="Friendly reminder that Grok 3 should be now open-sourced" src="https://b.thumbs.redditmedia.com/933OQwllbA1hY7_1-xQgZI7EOZf5Fdt9pi7_3gUoRkc.jpg" title="Friendly reminder that Grok 3 should be now open-sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx5awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:13:48+00:00</published>
  </entry>
</feed>
