<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-15T22:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o7p34f</id>
    <title>For those building llama.cpp for Android (Snapdragon/Adreno only).</title>
    <updated>2025-10-15T22:21:34+00:00</updated>
    <author>
      <name>/u/Brahmadeo</name>
      <uri>https://old.reddit.com/user/Brahmadeo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went down the rabbit hole of building llama.cpp for Android using OpenCL and Vulkan support. Here is what I learned...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;CPU/GPU&lt;/strong&gt; - Snapdragon 7+ Gen 3/Adreno 732 (Open CL 3.0) - 64-bit ARMv9-a. ( built llama.cpp for ARMv8-a.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAM&lt;/strong&gt;- 12 GB (Effectively output 11 GB with &lt;code&gt;free&lt;/code&gt; command on Termux. &lt;em&gt;Some 4-5 GB actually available at a time&lt;/em&gt;, if you don't want to clog everything by running inference on &amp;quot;big&amp;quot; ~ 13b, models of your dreams.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;API&lt;/strong&gt;- Android 15 (API 35, llama.cpp supports upto API 34, built for that.)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;- For OpenCL I followed everything on llama.cpp/build.md to the letter. The libcurl issue popeed up, so I marked curl support to OFF in CMake, since I can download the model myself. Build successful! (Working Build script below). &lt;/p&gt; &lt;p&gt;I then pushed the llama-cli/llama-server binaries to my phone storage using adb. Ran &lt;code&gt;chmod +x ./llama-*&lt;/code&gt; in Termux and tried to run it. The &lt;code&gt;libomp&lt;/code&gt; requirement message pops up. Failed to run. Tried setting &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to many obscure places, but no success. My phone vendor (apparently most of them don't load it, yet). Also the build script doesn't mention &lt;code&gt;libomp&lt;/code&gt; and it is required by default so you can't turn it OFF like libcurl. Hint: It is in your ndk folder (for aarch64), and I pushed it to my phone as well, then exported it on &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; and llama finally ran. I was really interested in &lt;code&gt;LFM2-8B-A1B-Q4_K_M&lt;/code&gt; and ran it, it worked splendidly. (It is very well optimised model.)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I then download Mistral 7b, since I was sure that OpenCL implementation has given my phone superpowers. 1 token every 3~5 seconds.&lt;/p&gt; &lt;p&gt;Okay this might be an exception. Maybe &lt;code&gt;deepseek-coder-6.7b-instruct.Q4_K_M&lt;/code&gt; would run just fine. üòë&lt;/p&gt; &lt;p&gt;Downloaded &lt;code&gt;phi-4-mini-instruct-q4_k_m&lt;/code&gt;. Runs pretty much the same as in Ollama.&lt;/p&gt; &lt;p&gt;Why did I even bother.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Went further down the rabbit hole and found MNN Chat. It's great! Everything runs as if running a cloud AI model. Then remembered that I once installed Edge Gallery from Google. The same experience as MNN Chat, but limited models.&lt;/p&gt; &lt;p&gt;I asked cloud-based AI models, what is this sorcery? The answer was optimised models and use of CPU, GPU even NPU delegates (NPU one is a myth as of now.) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;And then I stumbled upon Int8 Matrix Multiply (I8MM) instruction set. It is like a Jet Engine for quantized LLMs.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cat /proc/cpuinfo | grep Features&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Fuck yes, it's available! I wonder what kind of magic will happen running it together with OpenCL GPU support. ü§î&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Here is the script-&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake .. -G Ninja \ -DCMAKE_TOOLCHAIN_FILE=$HOME/android-sdk/ndk/26.3.11579264/build/cmake/android.toolchain.cmake \ -DANDROID_ABI=arm64-v8a \ -DANDROID_PLATFORM=android-34 \ -DANDROID_STL=c++_static \ -DCMAKE_BUILD_TYPE=Release \ -DBUILD_SHARED_LIBS=OFF \ \ `# GPU (OpenCL only, Vulkan has header issues in NDK 26)` \ -DGGML_OPENCL=ON \ -DGGML_VULKAN=OFF \ \ `# CPU Optimizations` \ -DGGML_OPENMP=ON \ -DGGML_LLAMAFILE=ON \ \ `# Explicit CPU features (I8MM, BF16, DotProd)` \ -DCMAKE_C_FLAGS=&amp;quot;-march=armv8.6-a+i8mm+bf16+dotprod -O3 -flto=thin&amp;quot; \ -DCMAKE_CXX_FLAGS=&amp;quot;-march=armv8.6-a+i8mm+bf16+dotprod -O3 -flto=thin&amp;quot; \ -DCMAKE_EXE_LINKER_FLAGS=&amp;quot;-flto=thin&amp;quot; \ \ `# OpenMP` \ -DOpenMP_C_FLAGS=&amp;quot;-fopenmp -static-openmp&amp;quot; \ -DOpenMP_CXX_FLAGS=&amp;quot;-fopenmp - static-openmp&amp;quot; \ -DOpenMP_C_LIB_NAMES=&amp;quot;omp&amp;quot; \ -DOpenMP_CXX_LIB_NAMES=&amp;quot;omp&amp;quot; \ -DOpenMP_omp_LIBRARY=&amp;quot;$HOME/android-sdk/ndk/26.3.11579264/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/lib/linux/aarch64/libomp.so&amp;quot; \ \ -DLLAMA_CURL=OFF ninja &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;&lt;code&gt;-static-openmp&lt;/code&gt; flag is useless, but you can't blame a man for trying! Any way moment of truth. Here are the test results-&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular LLAMA.CPP Build:&lt;/strong&gt; &lt;code&gt;CPU : NEON = 1 | ARM_FMA = 1 | LLAMAFILE = 1 | OPENMP = 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ultimate LLAMA.CPP Build:&lt;/strong&gt; &lt;code&gt;CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | OPENMP = 1&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@ &amp;quot;Write a Python function to sort an array&amp;quot; -ngl 0 -c 1024 -n 100 -t 4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llama Regular (deepseek)-&lt;br /&gt; real 0m52.095s user 1m51.001s sys 0m14.700s&lt;/p&gt; &lt;p&gt;Llama Ultimate (deepseek)- real 0m38.913s user 1m24.155s sys 0m7.134s&lt;/p&gt; &lt;p&gt;Llama Regular (phi-4-mini)- real 0m55.714s user 1m20.838s sys 0m3.432s&lt;/p&gt; &lt;p&gt;Llama Ultimate (phi-4-mini)- real 0m31.240s user 1m0.105s sys 0m2.291s&lt;/p&gt; &lt;p&gt;Llama Regular (LFM2-8b)- real 0m34.489s user 0m45.232s sys 0m12.527s&lt;/p&gt; &lt;p&gt;Llama Ultimate (LFM2-8b)- real 0m31.502s user 0m37.742s sys 0m9.343s&lt;/p&gt; &lt;p&gt;@ &amp;quot;Write a Python function to sort an array&amp;quot; NO LIMIT (-ngl 0) and c-1024 -n 100 -t 4&lt;/p&gt; &lt;p&gt;Llama Regular (deepseek)-&lt;br /&gt; real 1m28.963s user 3m20.328s sys 0m55.868s&lt;/p&gt; &lt;p&gt;Llama Ultimate (deepseek)- real 1m18.854s user 2m40.689s sys 0m53.810s&lt;/p&gt; &lt;p&gt;Llama Regular (phi-4-mini)- real 1m31.952s user 2m22.048s sys 0m44.990s&lt;/p&gt; &lt;p&gt;Llama Ultimate (phi-4-mini)- real 1m5.933s user 2m5.127s sys 0m44.334s&lt;/p&gt; &lt;p&gt;Llama Regular (LFM2-8b)- real 1m10.374s user 2m2.515s sys 0m51.642s&lt;/p&gt; &lt;p&gt;system_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 10.76 ms / 100 runs ( 0.11 ms per token, 9293.68 tokens per second) llama_perf_context_print: load time = 6830.73 ms llama_perf_context_print: prompt eval time = 1913.04 ms / 17 tokens ( 112.53 ms per token, 8.89 tokens per second) llama_perf_context_print: eval time = 40581.67 ms / 199 runs ( 203.93 ms per token, 4.90 tokens per second) llama_perf_context_print: total time = 47003.73 ms / 216 tokens&lt;/p&gt; &lt;p&gt;Llama Ultimate (LFM2-8b)- real 0m44.687s user 1m3.548s sys 0m27.235s&lt;/p&gt; &lt;p&gt;system_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | OPENMP = 1 | REPACK = 1 |&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 16.48 ms / 117 runs ( 0.14 ms per token, 7100.38 tokens per second) llama_perf_context_print: load time = 5351.92 ms llama_perf_context_print: prompt eval time = 835.45 ms / 17 tokens ( 49.14 ms per token, 20.35 tokens per second) llama_perf_context_print: eval time = 18284.65 ms / 99 runs ( 184.69 ms per token, 5.41 tokens per second) llama_perf_context_print: total time = 22671.76 ms / 116 tokens&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;CPU-Only Performance (-ngl 0)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Regular&lt;/th&gt; &lt;th align="left"&gt;Ultimate&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek&lt;/td&gt; &lt;td align="left"&gt;52.1s&lt;/td&gt; &lt;td align="left"&gt;38.9s&lt;/td&gt; &lt;td align="left"&gt;25% faster ‚ö°&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-4-mini&lt;/td&gt; &lt;td align="left"&gt;55.7s&lt;/td&gt; &lt;td align="left"&gt;31.2s&lt;/td&gt; &lt;td align="left"&gt;44% faster ‚ö°‚ö°&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B&lt;/td&gt; &lt;td align="left"&gt;34.5s&lt;/td&gt; &lt;td align="left"&gt;31.5s&lt;/td&gt; &lt;td align="left"&gt;9% faster ‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Hybrid GPU+CPU (no -ngl limit)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Regular&lt;/th&gt; &lt;th align="left"&gt;Ultimate&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek&lt;/td&gt; &lt;td align="left"&gt;1m29s&lt;/td&gt; &lt;td align="left"&gt;1m19s&lt;/td&gt; &lt;td align="left"&gt;11% faster ‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-4-mini&lt;/td&gt; &lt;td align="left"&gt;1m32s&lt;/td&gt; &lt;td align="left"&gt;1m6s&lt;/td&gt; &lt;td align="left"&gt;28% faster ‚ö°&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B&lt;/td&gt; &lt;td align="left"&gt;1m10s&lt;/td&gt; &lt;td align="left"&gt;45s&lt;/td&gt; &lt;td align="left"&gt;36% faster ‚ö°‚ö°&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;GPU Offload Test LFM2 - 25 layers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;Eval Speed&lt;/th&gt; &lt;th align="left"&gt;Comment&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;0 (CPU only)&lt;/td&gt; &lt;td align="left"&gt;15.34 tok/s&lt;/td&gt; &lt;td align="left"&gt;üèÜ FASTEST!&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;7.69 tok/s&lt;/td&gt; &lt;td align="left"&gt;‚ùå Worst (hybrid overhead)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;8.84 tok/s&lt;/td&gt; &lt;td align="left"&gt;Still slow&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;7.22 tok/s&lt;/td&gt; &lt;td align="left"&gt;Getting worse&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;4.85 tok/s&lt;/td&gt; &lt;td align="left"&gt;Very slow&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;25 (all GPU)&lt;/td&gt; &lt;td align="left"&gt;4.81 tok/s&lt;/td&gt; &lt;td align="left"&gt;‚ùå Slowest!&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;CPU is 3x FASTER than GPU! CPU (ngl 0): 15.34 tok/s ‚Üê WINNER GPU (ngl 25): 4.81 tok/s ‚Üê 3x SLOWER!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;GPU Offload Test Deepseek - 33 layers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;Eval Speed&lt;/th&gt; &lt;th align="left"&gt;vs CPU&lt;/th&gt; &lt;th align="left"&gt;GPU Memory&lt;/th&gt; &lt;th align="left"&gt;Status&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;0 (CPU)&lt;/td&gt; &lt;td align="left"&gt;4.94 tok/s&lt;/td&gt; &lt;td align="left"&gt;1.0x&lt;/td&gt; &lt;td align="left"&gt;0 MB&lt;/td&gt; &lt;td align="left"&gt;üèÜ WINNER&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;2.31 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.47x&lt;/td&gt; &lt;td align="left"&gt;435 MB&lt;/td&gt; &lt;td align="left"&gt;‚ùå 2x SLOWER&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;0.35 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.07x&lt;/td&gt; &lt;td align="left"&gt;628 MB&lt;/td&gt; &lt;td align="left"&gt;‚ùå‚ùå 14x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;33 (all GPU)&lt;/td&gt; &lt;td align="left"&gt;0.48 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.10x&lt;/td&gt; &lt;td align="left"&gt;1479 MB&lt;/td&gt; &lt;td align="left"&gt;‚ùå‚ùå 10x SLOWER!&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;GPU makes DeepSeek 10-14x SLOWER! CPU (ngl 0): 4.94 tok/s ‚Üê FAST GPU (ngl 33): 0.48 tok/s ‚Üê 10x SLOWER! üò± Hybrid worst: 0.35 tok/s ‚Üê 14x SLOWER! üíÄ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;GPU Offload Test Phi-4-mini - 33 layers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;Eval Speed&lt;/th&gt; &lt;th align="left"&gt;vs CPU&lt;/th&gt; &lt;th align="left"&gt;GPU Memory&lt;/th&gt; &lt;th align="left"&gt;Status&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;0 (CPU)&lt;/td&gt; &lt;td align="left"&gt;10.81 tok/s&lt;/td&gt; &lt;td align="left"&gt;1.0x&lt;/td&gt; &lt;td align="left"&gt;0 MB&lt;/td&gt; &lt;td align="left"&gt;üèÜ WINNER&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;7.01 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.65x&lt;/td&gt; &lt;td align="left"&gt;207 MB&lt;/td&gt; &lt;td align="left"&gt;‚ùå 35% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;5.58 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.52x&lt;/td&gt; &lt;td align="left"&gt;271 MB&lt;/td&gt; &lt;td align="left"&gt;‚ùå 48% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;4.59 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.42x&lt;/td&gt; &lt;td align="left"&gt;334 MB&lt;/td&gt; &lt;td align="left"&gt;‚ùå 58% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;33 (all GPU)&lt;/td&gt; &lt;td align="left"&gt;1.81 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.17x&lt;/td&gt; &lt;td align="left"&gt;1327 MB&lt;/td&gt; &lt;td align="left"&gt;‚ùå‚ùå 6x SLOWER!&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The pattern is UNIVERSAL across all models: LFM2: CPU 3x faster than GPU DeepSeek: CPU 10x faster than GPU&lt;br /&gt; Phi-4: CPU 6x faster than GPU&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Fuck OpenCL, and the architecture it was coded for. OpenCL murdered performance. Too much overhead, it is like model compute on GPU takes 5% of time but passing result back to CPU is taking 95% of time.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;OpenCL on Adreno (mobile) is fundamentally broken for LLMs. The overhead is so massive that: ‚úÖ CPU with I8MM: 5-15 tok/s ‚ùå GPU with OpenCL: 0.5-5 tok/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would Vulkan help, though?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The problem isn't OpenCL vs Vulkan - it's GPU architecture + memory bandwidth on mobile SoCs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Vulkan would have: ‚úÖ ~10-20% less overhead than OpenCL ‚ùå Still 5-10x slower than CPU&lt;/p&gt; &lt;p&gt;Expected Vulkan performance:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Current OpenCL: 0.5-5 tok/s With Vulkan: 0.6-6 tok/s (still terrible!) CPU I8MM: 5-15 tok/s (still wins!) Verdict: Not worth the effort. Save your time! &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;What I Learned:&lt;/p&gt; &lt;p&gt;‚ùå Mobile GPU myth: &amp;quot;GPU is always faster&amp;quot; (FALSE!) ‚úÖ CPU with I8MM: Often faster than GPU ‚ùå Mobile GPU is useless for LLMs (5-10x slower than CPU!) ‚úÖ I8MM is critical (2x faster than without) ‚úÖ Small models work great on CPU (5-15 tok/s) ‚úÖ LFM2 is the perfect mobile model (Oct, 2025) ‚ùå OpenCL/Vulkan are wastes of time on mobile&lt;/p&gt; &lt;p&gt;Forget about GPU entirely&lt;/p&gt; &lt;h1&gt;Don't waste time on:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;OpenCL ‚ùå&lt;/li&gt; &lt;li&gt;Vulkan ‚ùå&lt;/li&gt; &lt;li&gt;Hybrid offloading ‚ùå&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PS: I wrote very little of it, and mostly pasted AI analysis of tests I did. (like -ngl 99 offload writing to AI)&lt;/p&gt; &lt;p&gt;PPS: Those of you with SD Elites. Can you please test if the CPU to GPU bandwidth is ruining GPU offloading for you as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brahmadeo"&gt; /u/Brahmadeo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7bve2</id>
    <title>Reasoning should be thought of as a drawback, not a feature</title>
    <updated>2025-10-15T14:02:59+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When a new model is released, it‚Äôs now common for people to ask ‚ÄúIs there a reasoning version?‚Äù&lt;/p&gt; &lt;p&gt;But reasoning is not a feature. If anything, it‚Äôs a drawback. Reasoning models have only two observable differences from traditional (non-reasoning) models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Several seconds (or even minutes, depending on your inference speed) of additional latency before useful output arrives.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A wall of text preceding every response that is almost always worthless to the user.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Reasoning (which is perhaps better referred to as context pre-filling) is a mechanism that allows some models to give better responses to some prompts, at the cost of dramatically higher output latency. It is not, however, a feature in itself, any more than having 100 billion extra parameters is a ‚Äúfeature‚Äù. The feature is the model quality, and reasoning can be a way to improve it. But the presence of reasoning is worthless &lt;em&gt;by itself&lt;/em&gt;, and should be considered a bad thing unless proven otherwise in every individual case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7bve2/reasoning_should_be_thought_of_as_a_drawback_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7bve2/reasoning_should_be_thought_of_as_a_drawback_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7bve2/reasoning_should_be_thought_of_as_a_drawback_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T14:02:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o77ag4</id>
    <title>A guide to the best agentic tools and the best way to use them on the cheap, locally or free</title>
    <updated>2025-10-15T10:26:15+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you expect an AI generated post? Complete with annoying emojis and GPTisms? I don't blame you. These AI generated posts are getting out of hand, and hurt to read. Vibe-coders seem to be some of the worst offenders of this. Am I a vibe coder too? Don't know. I don't really rely on AI coding much, but thought it was pretty neat, so I spent some weeks checking out various tools and models to get a feel for them. How I use them might be very different from others, so going to give that warning in advance. I prefer to write my code, then see if I can use the agent to either improve it some way (help with refactoring, making some my monolithic scripts more modular, writing tests, this kind of stuff), and sometimes trying to add features to my existing tools. I have tried one shotting a few tools from scratch with AI, but it wasn't for me, especially the agents that like to overengineer things and get carried away with it. I like knowing what my code is doing. If you are just getting into coding, I don't suggest relying on these tools heavily. I've seen people be very productive with these kinds of tools and able to get a lot done with them, but almost all of those people were very experienced devs that know their way around code. I am not one of those people and am able to affirm that AI should not be heavily leaned upon without a solid foundation. Let's not forget the guy who vibe coded a script to &amp;quot;distill&amp;quot; much larger models into smaller ones, that ultimately did nothing, and ended up uploading &amp;quot;distills&amp;quot; that were identical weights to their original models (yeah, you might remember me from that post). Of course ppl still ate it up, cause confirmation bias, so I guess it's all about how you market the snake oil? Either way, if you're here interested in which agentic coding tools, and models work best, read on. I will share what I've learned, including some very cool free API options at the bottom of this post. We seem to be in the boom period of agentic coding, so a lot of providers and services are being very generous. And power users of agentic coding who probably know more than me, please do comment your thoughts and experiences.&lt;/p&gt; &lt;p&gt;Why does it matter? You can use the best model available, or even just a mediocre model, but the tool you use with it matters. A good tool will drastically give you better results. Not only that, some models work MUCH better with specific tools. Here are my recommendations, and non-recommendations, starting with a few non-recommendations:&lt;/p&gt; &lt;p&gt;- Warp: Looks like a great cli tool. Scores well in leaderboards/benchmarks, and is received well by users. BUT, no BYOK option. Makes them immediately dead on arrival as a serious option for me. You're completely at mercy to their service and any changes they make to it, randomly or not. I also don't really like the subscription model, makes little to no sense, because there's almost no transparency. You get credits to use monthly but NOWHERE do they tell you how many tokens, or requests those credits give you with any model. Their docs barely have anything on this, it's literally all vibes and doesn't tell you more than some models use more credits, and using more context, tool calls, tokens, etc use more credits.&lt;/p&gt; &lt;p&gt;- Cursor: Looks like a really nice ide, and seems to work pretty well. However, suffers all the same issues as above. A lot of agentic tools do. So I wont cover too many of these. These are more like platforms + service rather than tools to use with whatever service you want.&lt;/p&gt; &lt;p&gt;- Roocode: Want a quick answer? I'd probably recommend this. Very solid, all around choice. Very well recieved by the community. Has the highest rating out of all the AI extensions I saw on vscode, if that means anything. Scores very well in gosuevals (I highly suggest checking out his videos, search gosucoder on youtube, he goes very indepth in how well these agentic tools work, and in his comparisons) and is usually a top 1-3 in those monthly evals for most models. Supports code indexing for free with any provider, local api, or gemini embedding which is free via api it seems (and probably the very best embedding model available right now). Integrates well with vscode.&lt;/p&gt; &lt;p&gt;- Qwen Code CLI: I don't want to make ppl read a ton to get to the best choices, so going to go ahead and share this one next because it is by far, imo, the best free, no frills option. Signup for qwen account, login via browser for oath. Done, now you have 4k qwen-coder-plus requests daily, and it's fast too at 70t/s. Qwen3 coder is one of the best opensource models, and it works way better with qwen code cli, and imo, to the point of being better than most other OSS model + tool combinations. The recent updates are very nice, adding things like planning mode. This was also imo the easiest and simplest to use of the tools ive tried. Very underrated and slept on. Qwen coder plus was originally just Qwen3 Coder 480b, the open source model, and it might still be, but they have a newer updated version that's even better, not sure if this is the one we get access too now. If it is, this easily beats using anything outside of gpt5 or claude models. this tool is gemini cli based.&lt;/p&gt; &lt;p&gt;- Droid: Im still in the process of trying this one out (nothing bad yet though) so I'm going to withhold from saying too much subjective opinion and just share what I know. Scores the highest out of any agents in terminal bench so it seemed promising, but I've been looking around, and asking a lot of people about their experiences with it so far, and getting a lot of mixed feedback. I like it as a concept, will have to see if it's actually that good. Just a few anecdotal experiences are pretty unreliable after all and one big thing it has over others is that it supports BYOK at free tier without any extra caveats. The big complaint I've seen is that this tool absolutely chews through tokens (which makes their nice monthly plan less impressive), but this might not be a big deal if you use your own local model or a free api (more on this later). The most attractive thing about this tool to me is the very generous monthly plan. You get 20 million tokens for $20 monthly. Using claude sonnet uses those tokens at 1.2x, which is very nice pricing (essentially 16.7 million tokens, or around $400~ worth of tokens based off anthropic api pricing and how much artificial analysis cost to run) when compared to the claude monthly subs (I see ppl maxing out their $100 subs at around 70 million tokens), especially when you consider its not rate limited in 5 hour periods. They also have gpt 5 codex at 0.5x (so 40 million tokens monthly), and glm 4.6 at 0.25x (80 million monthly). This is a &lt;em&gt;very&lt;/em&gt; generous $20 sub imo, especially if their GLM model has thinking available (I dont think it does, which imo makes it not worth bothering to use, but the &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; monthly sub also has thinking disabled). I wonder if theyre eating a loss or going at cost to try and build a userbase. Lastly, they have a very nice trial, giving you 20m tokens free for one month, or 40m for 2 months if you use a referral link. I will include mine here for convenience's sake, but I do not do nearly enough AI coding to benefit from any extra credits I get so you might do someone else the favor and use their referral link instead. &lt;a href="https://app.factory.ai/r/0ZC7E9H6"&gt;https://app.factory.ai/r/0ZC7E9H6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- zed: a rust based ide. feels somewhere between a text editor like notepad++ or kate (the kde default) and vscode. its incredibly fast, and works quite well. the UI will not feel too unfamiliar from vscode, but it doesnt have the huge extensions marketplace vscode does. on the other hand, its super performant and dead simple while still feeling very full-featured, with a lot more to be added in the future. I replaced my systems default editor (kate) with zed, and have been super happy with the decision. feels much better to use. I would use it in place of vscode, but some things have better integration with vscode so I only use zed sometimes. now lets talk about it agentic capabilities. its improved a lot, and is actually near the top of gosu's latest evals. the problem is, it absolutely &lt;em&gt;chews&lt;/em&gt; through tokens. same issue as droid, but even worse it seems like. They have a two week trial that gives you $20 credits. I used up $5 with sonnet 4.5 in less than a half hour. on the other hand, its byok, so I can see this being one of the best options for use with a local model, cheap api or even free api. the other thing is, I dont think there's a planning mode, or orchestrator mode, which has been the main reason I havent been using this agent. when I did test it, it absolutely overengineered everything and tried to do too much, so that might be something to watchout for as well.&lt;/p&gt; &lt;p&gt;- claude code: basically the benchmark cli tool, everyone compares other tools to this tool. Has a lot of features, and was the first to have a lot of the features other agentic tools have. It's reliable and works well. zed has native support for claude code now btw. this matters for things like access to lsp, following what the agent is doing, etc. you want to be using cli tools that are supported by your ide natively or have extensions for it (almost all cli tools have an extension for vscode, one of the reasons why I havent switched off of it completely).&lt;/p&gt; &lt;p&gt;- codex cli or vscode extension: mixed reception at first, but it's improved and ppl seem to really like it now. the gpt5 models (gpt-oss), especially codex don't really shine until used with this tool (similar to qwen coder with qwen code). The difference is very large, to the point I would say you are getting a hampered experience with those models until you use it with this tool.&lt;/p&gt; &lt;p&gt;- crush: made by main dev behind opencode and charm, who has made some of the best terminal ui libraries. sounds like the dream combination right? so far it's a pretty decent all around tool, that looks really nice, but isn't anything special yet. Not a bad choice by any means. open source too.&lt;/p&gt; &lt;p&gt;- gemini cli: well, the cli is nice. but gemini for whatever reason kind of sucks at agentic coding. would not bother with this until gemini 3.0 comes out. gemini 2.5 pro is however, still one of the best chat assistants, and an especially good for using with the research tool. if you have a student email of some sort, you can probably get a year free of gemini pro.&lt;/p&gt; &lt;p&gt;- trae + seed: no byok, but looks good on swebench? sorry, im a no byok hater.&lt;/p&gt; &lt;p&gt;- augment: no byok. crappy plan. doesnt even seem like its that great, better options out there.&lt;/p&gt; &lt;p&gt;- refact: looks good on swebench, havent actually tried it, and doesnt seem like anyone else has really. does seem like it supports byok atleast.&lt;/p&gt; &lt;p&gt;- kilocode: a novel idea, cline + roo was their main pitch, but roo has implemented most things that kilocode had, and just straight up performs better on most tasks these days. I get the feeling kilocode is just playing catchup, and only get's their once theyre upstream with roo's code since it's based off of it. some ppl still like kilocode and it can be worth using anyways if it fits your preference.&lt;/p&gt; &lt;p&gt;- cline: some ppl like cline more than roo, but most prefer roo. also lower rating than roo in vscode extension store.&lt;/p&gt; &lt;p&gt;There are a lot more agentic coding tools out there, but I'm running out of stamina to be going through them, so next I will cover the best model options, after mentioning one important thing. Use mcp servers. They will enhance your agentic coding by a lot. I highly suggest at least getting the likes of exa search, context7, etc. I haven't used very many of these yet and am in the process of experimenting with them, so I cant offer too much advice here (thankfully. Im writing way too much.)&lt;/p&gt; &lt;p&gt;The very best model right now, for agentic coding, is sonnet 4.5. This will probably change at some point so do some research if this post isnt recent anymore. Only gpt 5 codex comes close or is as good, and thats only if you use it with codex cli or the codex extension. These options can however be a little pricy, especially if you pay by the token in api cost. The monthly subs however, can be worth it to some. Afterall, sometimes it much better to get things done in one shot than spend hours reprompting, rolling back changes and trying again with a lesser model.&lt;/p&gt; &lt;p&gt;The next tier of models is pretty interesting. None of these come very close to the top two choices, but are all relatively close to each other in capability, regardless of cost. Gpt-5, the non codex model is one such model, and probably near the top of this tier, but it costs the same as gpt-5 codex so why would you use it? The best bang for buck model in this category is probably gpt 5 mini (medium reasoning, high reasoning isnt much better and takes up a lot more tokens), and deepseek v3.2-exp, if we go based purely of cost per token. gpt 5 mini is more capable, but a little more expensive. Deepseek v3.2 is by far the cheapest of this category, and surprisingly capable for how cheap it is, I would rate it just under kimi k2 0905 and qwen3 coder 480b. GLM 4.6 is only around those two mentioned models with reasoning disabled, but with reasoning enabled it becomes much better. Sadly, the glm sub that everyone has been so hyped about, has thinking disabled. So get the sub if you want.. it is cheap as heck, but.. know you are only getting around that level of capability. Here's where it gets interesting. Gpt 5 mini is completely free with copilot pro, which is also free if you have any old (or current) student email. This, with reasoning at medium is step above glm 4.6 without reasoning. Unfortunately you do get tied down to using it within copilot, or tools that have custom headers to spoof their agent built-in (I think opencode has this?). Now for the free models.. kimi k2 0905 is completely free, unlimited use at 40 rpm, via the nvidia nim api. just make an account and get an api key, use like any other openai compatible api. This is by far the best or one of the best non-thinking models. It's in the same realm as glm 4.6 without reasoning (above it slightly I'd say, but glm 4.6 with reasoning will blow it out), qwen coder 480b (above it slightly I'd say, unless used with qwen code, where I then give the edge to qwen coder). GLM 4.6, if reasoning is enabled is near the top of this pack, but this tier of models is still significantly below the best one or two models.&lt;/p&gt; &lt;p&gt;A note on roocode, and other tools that support code indexing via embedding models. roo specifically supports gemini embedding which is bar none the very best available, and is apparently completely free via api atm. but if your tool doesnt support it, nebiusai gives you $1 credit for free on signup, that never expires afaik, and their qwen3 embedding 8b model is the cheapest of any provider at 0.01 per million. That $1 will last you forever if you use it for embedding only, and it is the second best available embedding model behind gemini (and is the very best OSS embedding model atm). sadly they dont have any reranking models, but I think I only saw one tool that supported this? and cant remember which tool it is. if you do stumble across one, you can sign up with novita for a $1 voucher as well, and use qwen3 reranker 8b from their api. Pretty good combo on roo code, to use kimi k2 0905 from nvidia api, and either gemini embedding or nebius' qwen3 embedding.&lt;/p&gt; &lt;p&gt;As far as local models go for running on typical home computers, these unfortunately, have a very big gap between much larger OSS models, that youre better off using off a free api, or trial credits, but if you dont care enough to, or are just trying stuff for fun, privacy, etc, your best bets are qwen3 coder 30b a3b with qwen code cli, or gpt-oss 20b + codex cli/extension. next step up is gpt oss 120b with codex cli/extension if you have the ram and vram for it. Devstral small 2507 is okay too, but I dont think its quite as good for its size.&lt;/p&gt; &lt;p&gt;Lastly, speaking on free credits, I came across some reddit posts claiming free credits for some chinese openrouter clone looking website called agent router. Was extremely sussed out by it, and couldnt find much information on it other than few ppl saying they got it working after some hassle, and that the software stack is based off a real opensource stack with repos available on github (new api and one api). Decided to very reluctantly give it a shot, but the website was a buggy half implemented mess throwing backend errors galore, which sussed me out more. They only supported signup via oath from github and linux do. Me wondering what the catch was, checked my permissions after signing up with github, and saw they only got read access to what email my github was under. I saw I did get my credits from signing up via referral. The rates for sonnet looked typical, but the rates for the other models seemed too good to be true. So I get an api key, try it with my pageassist firefox extension (I highly recommend it, dev is great, has added a bunch of stuff after feedback on discord), and got 401 error. Tried with cherry studio (also very nice), same error. Website has me logged out now, and I cant log back in, I keep getting error too many requests in chinese. Gave up. Tried again daily for a few days and same issues. Finally, today the website is working perfectly, no lag either. Im amazed, was starting to think it was some sort of weird scam, which is why I hadnt told anyone about it yet. Says I have no api keys for some reason so I make a new one. doesnt work still. after some replies from other on reddit, and reading the docs, I realize, these models only work with specific tools, so that seems to be the main catch. after realizing this I reinstalled codex cli, followed the docs for using the api with codex cli (this is a must btw) after translating with deepseek v3.2 and it was working perfectly. Mind blown. So now I have $125 credits with temu openrouter, which serves gpt 5 at only 0.003 dollars per million tokens lol. Me and a few others have a sneaking suspicion the hidden catch is that they store, and use your data, probably for training, but personally I dont care. If this isnt an issue for you guys either, I highly suggest finding someone's referral link and using it to signup with github or linuxdo. You will get $100 from the referral, and $25 for logging in. Again, I still have my trial credits through from other tools, and dont use ai coding much so use someone elses referral if you wanna be nice, but I will throw mine in here anyways for convenience sake. &lt;a href="https://agentrouter.org/register?aff=ucNl"&gt;https://agentrouter.org/register?aff=ucNl&lt;/a&gt; PS I suggest using a translation tool as not all of it is in english, I used the first ai translation extension that works with openrouter I found from the firefox store lol.&lt;/p&gt; &lt;p&gt;On a second read, maybe I should have put this through some ai to make this more human readable. Ah well. I bet one of you will put this through claude sonnet anyways, and comment it below. wont be me though. Tl;dr if you skipped to the bottom though; nvidia nim api is free, use kimi k2 0905 from there with any tool that looks interesting, roo code is the all round solid choice. or just use qwen code cli with oath.&lt;/p&gt; &lt;p&gt;some links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://build.nvidia.com/explore/discover"&gt;https://build.nvidia.com/explore/discover&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gosuevals.com/"&gt;https://gosuevals.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/gosucoder"&gt;https://www.youtube.com/gosucoder&lt;/a&gt; (no im not affaliated with him, or anything/anyone mentioned in this post)&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.com/invite/YGS4AJ2MxA"&gt;https://discord.com/invite/YGS4AJ2MxA&lt;/a&gt; (his discord, I hang out here and the koboldai discord a lot if you wanna find me)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/qwen-code"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/upstash/context7"&gt;https://github.com/upstash/context7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://zed.dev/"&gt;https://zed.dev/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T10:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7a5xw</id>
    <title>What's the biggest blocker you've hit using LLMs for actual, large-scale coding projects?</title>
    <updated>2025-10-15T12:52:50+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beyond the hype, when you try to integrate LLMs into a real, large codebase, what consistently fails or holds you back? Is it the context length, losing understanding of the architecture, something just breaking with no clear reason, or constantly having to clean up the output?&lt;/p&gt; &lt;p&gt;I keep finding spending more time fixing AI-generated code than it would have taken to write from scratch on complex tasks. What's your biggest pain point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7a5xw/whats_the_biggest_blocker_youve_hit_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7a5xw/whats_the_biggest_blocker_youve_hit_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7a5xw/whats_the_biggest_blocker_youve_hit_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T12:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe1u</id>
    <title>gigaResearch</title>
    <updated>2025-10-15T22:34:35+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt; &lt;img alt="gigaResearch" src="https://preview.redd.it/nb2hmgqircvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71c101f2683e8df117cbc2a9abd685bcac5cbce0" title="gigaResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nb2hmgqircvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7cgaf</id>
    <title>MoE models benchmarks AMD iGPU</title>
    <updated>2025-10-15T14:25:29+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow up to request for testing a few other MoE models size 10-35B:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System: Kubuntu 25.10 OS, Kernel 6.17.0-5-generic with 64GB DDR5 ram. AMD Radeon Graphics (RADV REMBRANDT) Ryzen 6800H and 680M iGPU&lt;/p&gt; &lt;p&gt;aquif-3.5-a0.6b-preview-q8_0&lt;/p&gt; &lt;p&gt;Ling-Coder-lite.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Ling-Coder-Lite-Q4_K_M&lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Base.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Instruct.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125-Instruct-Q4_K_M&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507-Q4_1&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Thinking-2507-Q4_K_M&lt;/p&gt; &lt;p&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL&lt;/p&gt; &lt;p&gt;Ring-lite-2507.i1-Q4_1 Ring-lite-2507.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Llama.cpp Vulkan build: 152729f8 (6565)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1296.87 ¬± 11.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;103.45 ¬± 1.25&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;231.96 ¬± 0.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.94 ¬± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;232.71 ¬± 0.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.21 ¬± 0.53&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;399.54 ¬± 5.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.91 ¬± 0.21&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;396.74 ¬± 1.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.60 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;487.74 ¬± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.33 ¬± 0.47&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;484.79 ¬± 4.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.76 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;171.65 ¬± 0.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;27.04 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.18 ¬± 1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;28.79 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.46 ¬± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.86 ¬± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;292.10 ¬± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.86 ¬± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.03 ¬± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.75 ¬± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;replace table model names with this list:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;aquif-3.5-a0.6b-preview-q8_0 &lt;/li&gt; &lt;li&gt;Ling-Coder-lite.i1-Q4_K_M &lt;/li&gt; &lt;li&gt;Ling-Coder-Lite-Q4_K_M &lt;/li&gt; &lt;li&gt;LLaDA-MoE-7B-A1B-Base.i1-Q4_K_M &lt;/li&gt; &lt;li&gt;LLaDA-MoE-7B-A1B-Instruct.i1-Q4_K_M &lt;/li&gt; &lt;li&gt;OLMoE-1B-7B-0125.i1-Q4_K_M &lt;/li&gt; &lt;li&gt;OLMoE-1B-7B-0125-Instruct-Q4_K_M &lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-Instruct-2507-Q4_1 &lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-Thinking-2507-Q4_K_M &lt;/li&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL &lt;/li&gt; &lt;li&gt;Ring-lite-2507.i1-Q4_1 &lt;/li&gt; &lt;li&gt;Ring-lite-2507.i1-Q4_K_M&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is the combined data from all the tables into a single Markdown table:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1296.87 ¬± 11.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;103.45 ¬± 1.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;231.96 ¬± 0.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.94 ¬± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;232.71 ¬± 0.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.21 ¬± 0.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;399.54 ¬± 5.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.91 ¬± 0.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;396.74 ¬± 1.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.60 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;487.74 ¬± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.33 ¬± 0.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;484.79 ¬± 4.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.76 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;171.65 ¬± 0.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;27.04 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.18 ¬± 1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;28.79 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.46 ¬± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.86 ¬± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;292.10 ¬± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.86 ¬± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.03 ¬± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.75 ¬± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Hyperlinks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/aquif-3.5-A4B-Think-GGUF"&gt;aquif-3.5-A4B-Think&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF?show_file_info=aquif-3-moe-17b-a2.8b.Q4_K_M.gguf"&gt;aquif-3-moe-17b-a2.8b-i1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/gabriellarson/Moonlight-16B-A3B-Instruct-GGUF"&gt;Moonlight-16B-A3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF"&gt;gpt-oss-20b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF"&gt;ERNIE-4.5-21B-A3B-PT&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF"&gt;SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ling-lite-1.5-2507-GGUF"&gt;Ling-lite-1.5-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lovedheart/Ling-mini-2.0-GGUF"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ling-Coder-lite-i1-GGUF"&gt;Ling-Coder-lite&lt;/a&gt; &lt;a href="https://huggingface.co/redponike/Ling-Coder-lite-GGUF?show_file_info=Ling-Coder-Lite-Q4_K_M.gguf"&gt;2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ring-lite-2507-i1-GGUF"&gt;Ring-lite-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lovedheart/Ring-mini-2.0-GGUF"&gt;Ring-mini-2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;Ming-Lite-Omni-1.5&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/GroveMoE-Inst"&gt;GroveMoE-Inst &lt;/a&gt;(No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/models?search=FlexOlmo-7x7B-1T"&gt;FlexOlmo-7x7B-1T&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/FlexOlmo-7x7B-1T-RT"&gt;FlexOlmo-7x7B-1T-RT&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7cgaf/moe_models_benchmarks_amd_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7cgaf/moe_models_benchmarks_amd_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7cgaf/moe_models_benchmarks_amd_igpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T14:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7hxao</id>
    <title>Challenges in Tracing and Debugging AI Workflows</title>
    <updated>2025-10-15T17:47:49+00:00</updated>
    <author>
      <name>/u/dinkinflika0</name>
      <uri>https://old.reddit.com/user/dinkinflika0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I work on evaluation and observability at Maxim, and I‚Äôve been closely looking at how teams trace, debug, and maintain reliable AI workflows. Across multi-agent systems, RAG pipelines, and LLM-driven applications, getting full visibility into agent decisions and workflow failures is still a major challenge.&lt;/p&gt; &lt;p&gt;From my experience, common pain points include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure visibility across multi-step workflows:&lt;/strong&gt; Token-level logs are useful, but understanding the trajectory of an agent across multiple steps or chained models is hard without structured traces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debugging complex agent interactions:&lt;/strong&gt; When multiple models or tools interact, pinpointing which step caused a failure often requires reproducing the workflow from scratch.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrating human review effectively:&lt;/strong&gt; Automated metrics are great, but aligning evaluations with human judgment, especially for nuanced tasks, is still tricky.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maintaining reliability in production:&lt;/strong&gt; Ensuring that your AI remains trustworthy under real-world usage and scaling scenarios can be difficult without end-to-end observability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At &lt;a href="https://getmax.im/maxim"&gt;Maxim&lt;/a&gt;, we‚Äôve built our platform to tackle these exact challenges. Some of the ways teams benefit include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Structured evaluations at multiple levels:&lt;/strong&gt; You can attach automated checks or human-in-the-loop reviews at the session, trace, or span level. This lets you catch issues early and iterate faster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full visibility into agent trajectories:&lt;/strong&gt; Simulations and logging across multi-agent workflows give teams insights into failure modes and decision points.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom dashboards and alerts:&lt;/strong&gt; Teams can slice and dice traces, define performance criteria, and get Slack or PagerDuty alerts when issues arise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;End-to-end observability:&lt;/strong&gt; From pre-release simulations to post-release monitoring, evaluation, and dataset curation, the platform is designed to give teams a complete picture of AI quality and reliability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôve seen that structured, full-stack evaluation workflows not only make debugging and tracing faster but also improve overall trustworthiness of AI systems. Would love to hear how others are tackling these challenges and what tools or approaches you‚Äôve found effective for tracing, debugging, and reliability in complex AI pipelines.&lt;/p&gt; &lt;p&gt;(I humbly apologize if this comes across as self promo)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinkinflika0"&gt; /u/dinkinflika0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7hxao/challenges_in_tracing_and_debugging_ai_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7hxao/challenges_in_tracing_and_debugging_ai_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7hxao/challenges_in_tracing_and_debugging_ai_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7k7zz</id>
    <title>DGX SPARK Compiled llama.cpp Benchmarks Compared to M4 MAX (non-MLX)</title>
    <updated>2025-10-15T19:12:20+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, not trying to incite some feud discussion between Nvidia/Apple folks. I don't have either machines and just compiled this for amusement and just so others are aware. NOTE: Models aren't in mlx. If anyone is willing to share, it would be greatly appreciated. This would be really interesting.&lt;/p&gt; &lt;p&gt;Also, to any Strix Halo/Ryzen AI Max+ 395 users, if you'd like to compare:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m [model.gguf] -fa 1 -d 0,4096,8192,16384,32768 -p 2048 -n 32 -ub 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;Source of DGX SPARK data&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://nitter.net/richinseattle/status/1978244945845657863"&gt;Source of M4 MAX data&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s (M4 MAX)&lt;/th&gt; &lt;th align="left"&gt;t/s (Spark)&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;1761.99 ¬± 78.03&lt;/td&gt; &lt;td align="left"&gt;3610.56 ¬± 15.16&lt;/td&gt; &lt;td align="left"&gt;2.049&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;118.95 ¬± 0.21&lt;/td&gt; &lt;td align="left"&gt;79.74 ¬± 0.43&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;1324.28 ¬± 46.34&lt;/td&gt; &lt;td align="left"&gt;3361.11 ¬± 12.95&lt;/td&gt; &lt;td align="left"&gt;2.538&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;98.76 ¬± 5.75&lt;/td&gt; &lt;td align="left"&gt;74.63 ¬± 0.15&lt;/td&gt; &lt;td align="left"&gt;0.756&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;1107.91 ¬± 11.12&lt;/td&gt; &lt;td align="left"&gt;3147.73 ¬± 15.77&lt;/td&gt; &lt;td align="left"&gt;2.841&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;94.19 ¬± 1.85&lt;/td&gt; &lt;td align="left"&gt;69.49 ¬± 1.12&lt;/td&gt; &lt;td align="left"&gt;0.738&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;733.77 ¬± 54.67&lt;/td&gt; &lt;td align="left"&gt;2685.54 ¬± 5.76&lt;/td&gt; &lt;td align="left"&gt;3.660&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;80.68 ¬± 2.49&lt;/td&gt; &lt;td align="left"&gt;64.02 ¬± 0.72&lt;/td&gt; &lt;td align="left"&gt;0.794&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;518.68 ¬± 17.73&lt;/td&gt; &lt;td align="left"&gt;2055.34 ¬± 20.43&lt;/td&gt; &lt;td align="left"&gt;3.963&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;69.94 ¬± 4.19&lt;/td&gt; &lt;td align="left"&gt;55.96 ¬± 0.07&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;871.16 ¬± 31.85&lt;/td&gt; &lt;td align="left"&gt;1689.47 ¬± 107.67&lt;/td&gt; &lt;td align="left"&gt;1.939&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;62.85 ¬± 0.36&lt;/td&gt; &lt;td align="left"&gt;52.87 ¬± 1.70&lt;/td&gt; &lt;td align="left"&gt;0.841&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;643.32 ¬± 12.00&lt;/td&gt; &lt;td align="left"&gt;1733.41 ¬± 5.19&lt;/td&gt; &lt;td align="left"&gt;2.694&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;56.48 ¬± 0.72&lt;/td&gt; &lt;td align="left"&gt;51.02 ¬± 0.65&lt;/td&gt; &lt;td align="left"&gt;0.903&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;516.77 ¬± 7.33&lt;/td&gt; &lt;td align="left"&gt;1705.93 ¬± 7.89&lt;/td&gt; &lt;td align="left"&gt;3.301&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;50.79 ¬± 1.37&lt;/td&gt; &lt;td align="left"&gt;48.46 ¬± 0.53&lt;/td&gt; &lt;td align="left"&gt;0.954&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;351.42 ¬± 7.31&lt;/td&gt; &lt;td align="left"&gt;1514.78 ¬± 5.66&lt;/td&gt; &lt;td align="left"&gt;4.310&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;46.20 ¬± 1.17&lt;/td&gt; &lt;td align="left"&gt;44.78 ¬± 0.07&lt;/td&gt; &lt;td align="left"&gt;0.969&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;235.87 ¬± 2.88&lt;/td&gt; &lt;td align="left"&gt;1221.23 ¬± 7.85&lt;/td&gt; &lt;td align="left"&gt;5.178&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;40.22 ¬± 0.29&lt;/td&gt; &lt;td align="left"&gt;38.76 ¬± 0.06&lt;/td&gt; &lt;td align="left"&gt;0.964&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;1656.65 ¬± 86.70&lt;/td&gt; &lt;td align="left"&gt;2933.39 ¬± 9.43&lt;/td&gt; &lt;td align="left"&gt;1.771&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;84.50 ¬± 0.87&lt;/td&gt; &lt;td align="left"&gt;59.95 ¬± 0.26&lt;/td&gt; &lt;td align="left"&gt;0.709&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;938.23 ¬± 29.08&lt;/td&gt; &lt;td align="left"&gt;2537.98 ¬± 7.17&lt;/td&gt; &lt;td align="left"&gt;2.705&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;67.70 ¬± 2.34&lt;/td&gt; &lt;td align="left"&gt;52.70 ¬± 0.75&lt;/td&gt; &lt;td align="left"&gt;0.778&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;681.07 ¬± 20.63&lt;/td&gt; &lt;td align="left"&gt;2246.86 ¬± 6.45&lt;/td&gt; &lt;td align="left"&gt;3.299&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;61.06 ¬± 6.02&lt;/td&gt; &lt;td align="left"&gt;44.48 ¬± 0.34&lt;/td&gt; &lt;td align="left"&gt;0.728&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;356.12 ¬± 16.62&lt;/td&gt; &lt;td align="left"&gt;1772.41 ¬± 10.58&lt;/td&gt; &lt;td align="left"&gt;4.977&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;43.32 ¬± 3.04&lt;/td&gt; &lt;td align="left"&gt;37.10 ¬± 0.05&lt;/td&gt; &lt;td align="left"&gt;0.856&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;223.23 ¬± 12.23&lt;/td&gt; &lt;td align="left"&gt;1252.10 ¬± 2.16&lt;/td&gt; &lt;td align="left"&gt;5.609&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;35.09 ¬± 5.53&lt;/td&gt; &lt;td align="left"&gt;27.82 ¬± 0.01&lt;/td&gt; &lt;td align="left"&gt;0.793&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;684.35 ¬± 15.08&lt;/td&gt; &lt;td align="left"&gt;2267.08 ¬± 6.38&lt;/td&gt; &lt;td align="left"&gt;3.313&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;46.82 ¬± 11.44&lt;/td&gt; &lt;td align="left"&gt;29.40 ¬± 0.02&lt;/td&gt; &lt;td align="left"&gt;0.628&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;633.50 ¬± 3.78&lt;/td&gt; &lt;td align="left"&gt;2094.87 ¬± 11.61&lt;/td&gt; &lt;td align="left"&gt;3.307&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;54.66 ¬± 0.74&lt;/td&gt; &lt;td align="left"&gt;28.31 ¬± 0.10&lt;/td&gt; &lt;td align="left"&gt;0.518&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;496.85 ¬± 21.23&lt;/td&gt; &lt;td align="left"&gt;1906.26 ¬± 4.45&lt;/td&gt; &lt;td align="left"&gt;3.837&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;51.15 ¬± 0.85&lt;/td&gt; &lt;td align="left"&gt;27.53 ¬± 0.04&lt;/td&gt; &lt;td align="left"&gt;0.538&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;401.98 ¬± 4.97&lt;/td&gt; &lt;td align="left"&gt;1634.82 ¬± 6.67&lt;/td&gt; &lt;td align="left"&gt;4.067&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;47.91 ¬± 0.18&lt;/td&gt; &lt;td align="left"&gt;26.03 ¬± 0.03&lt;/td&gt; &lt;td align="left"&gt;0.543&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;293.33 ¬± 2.23&lt;/td&gt; &lt;td align="left"&gt;1302.32 ¬± 4.58&lt;/td&gt; &lt;td align="left"&gt;4.440&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;40.78 ¬± 0.42&lt;/td&gt; &lt;td align="left"&gt;22.08 ¬± 0.03&lt;/td&gt; &lt;td align="left"&gt;0.541&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;339.64 ¬± 21.28&lt;/td&gt; &lt;td align="left"&gt;841.44 ¬± 12.67&lt;/td&gt; &lt;td align="left"&gt;2.477&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;37.79 ¬± 3.84&lt;/td&gt; &lt;td align="left"&gt;22.59 ¬± 0.11&lt;/td&gt; &lt;td align="left"&gt;0.598&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;241.85 ¬± 6.50&lt;/td&gt; &lt;td align="left"&gt;749.08 ¬± 2.10&lt;/td&gt; &lt;td align="left"&gt;3.097&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;27.22 ¬± 2.67&lt;/td&gt; &lt;td align="left"&gt;20.10 ¬± 0.01&lt;/td&gt; &lt;td align="left"&gt;0.738&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;168.44 ¬± 4.12&lt;/td&gt; &lt;td align="left"&gt;680.95 ¬± 1.38&lt;/td&gt; &lt;td align="left"&gt;4.043&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;29.13 ¬± 0.14&lt;/td&gt; &lt;td align="left"&gt;18.78 ¬± 0.07&lt;/td&gt; &lt;td align="left"&gt;0.645&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;122.06 ¬± 9.23&lt;/td&gt; &lt;td align="left"&gt;565.44 ¬± 1.47&lt;/td&gt; &lt;td align="left"&gt;4.632&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;20.96 ¬± 1.20&lt;/td&gt; &lt;td align="left"&gt;16.47 ¬± 0.01&lt;/td&gt; &lt;td align="left"&gt;0.786&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;418.84 ¬± 0.53&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;13.19 ¬± 0.01&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;From the data here we can see PP on the DGX SPARK is ~3.35x faster than the M4 MAX, while TG ~0.73x. Interesting as MBW on SPARK is ~273GB/s and MAX ~546GB/s.&lt;/p&gt; &lt;p&gt;So, here is my question for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;. Inference performance is really important, but how much does PP really matter in all these discussions compared to TG? Also, yes, there is another important factor and that is price.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k7zz/dgx_spark_compiled_llamacpp_benchmarks_compared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k7zz/dgx_spark_compiled_llamacpp_benchmarks_compared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k7zz/dgx_spark_compiled_llamacpp_benchmarks_compared/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7liam</id>
    <title>Llamacpp Model Loader GUI for noobs</title>
    <updated>2025-10-15T20:01:21+00:00</updated>
    <author>
      <name>/u/CabinetNational3461</name>
      <uri>https://old.reddit.com/user/CabinetNational3461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt; &lt;img alt="Llamacpp Model Loader GUI for noobs" src="https://preview.redd.it/msr7wyiwxbvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf3cb84527273f0b3b22fdcb8c887bfed231273" title="Llamacpp Model Loader GUI for noobs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I a noob at this LLM stuff and recently switched from LM Studio/Ollama to llamacpp and loving it so far as far as speed/performance. One thing I dislike is how tedious it is to modify and play around with the parameters and using command line so I vibe coded some python code using Gemini 2.5 Pro for something easier to mess around with. I attached the code, sample model files and commands. I am using window 10 FYI. I had Gemini gen up some doc as am not much of a writer so here it is:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Llama.cpp Model Launcher is a powerful desktop GUI that transforms the complex llama-server.exe command line into an intuitive, point-and-click experience. Effortlessly launch models, dynamically edit every parameter in a visual editor, and manage a complete library of your model configurations. Designed for both beginners and power users, it provides a centralized dashboard to streamline your workflow and unlock the full potential of Llama.cpp without ever touching a terminal.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intuitive Graphical Control:&lt;/strong&gt; Ditch the terminal. Launch, manage, and shut down the llama-server with simple, reliable button clicks, eliminating the risk of command-line typos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Parameter Editor:&lt;/strong&gt; Visually build and modify launch commands in real-time. Adjust values in text fields, toggle flags with checkboxes, and add new parameters on the fly without memorizing syntax.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Configuration Management:&lt;/strong&gt; Build and maintain a complete library of your models. Effortlessly add new profiles, edit names and parameters, and delete old configurations, all from within the application.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-Time Monitoring:&lt;/strong&gt; Instantly know the server's status with a colored indicator (Red, Yellow, Green) and watch the live output log to monitor model loading, API requests, and potential errors as they happen.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrated Documentation:&lt;/strong&gt; Access a complete Llama.cpp command reference and a formatted user guide directly within the interface, eliminating the need to search for external help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Running the Application&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are two primary ways to run this application:&lt;/p&gt; &lt;p&gt;Method 1: Run from Python Source&lt;/p&gt; &lt;p&gt;This method is ideal for developers or users who have Python installed and are comfortable with a code editor.&lt;/p&gt; &lt;p&gt;Method 2: Compile to a Standalone Executable (.exe)&lt;/p&gt; &lt;p&gt;This method packages the application into a single `.exe` file that can be run on any Windows machine without needing Python installed.&lt;/p&gt; &lt;p&gt;code: &lt;a href="https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link"&gt;https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;help_file: &lt;a href="https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link"&gt;https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sample_moldel_commands: &lt;a href="https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link"&gt;https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope someone find it useful&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CabinetNational3461"&gt; /u/CabinetNational3461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/msr7wyiwxbvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o78zuc</id>
    <title>New models Qwen3-VL-4b/8b: hands-on notes</title>
    <updated>2025-10-15T11:59:11+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve got a pile of scanned PDFs, whiteboard photos, and phone receipts. The 4B Instruct fits well. For ‚Äúread text fast and accurately,‚Äù the ramp-up is basically zero; most errors are formatting or extreme noise. Once it can read, I hand off to a text model for summarizing, comparison, and cleanup. This split beats forcing VQA reasoning on a small model.&lt;/p&gt; &lt;p&gt;For OCR + desktop/mobile GUI automation (‚Äúrecognize ‚Üí click ‚Üí run flow‚Äù), the 8B Thinking is smooth. As a visual agent, it can spot UI elements and close the loop on tasks. The ‚Äúvisual coding enhancement‚Äù can turn screenshots into Draw.io/HTML/CSS/JS skeletons, which saves me scaffolding time.&lt;/p&gt; &lt;p&gt;Long videos: I search meeting recordings by keywords and the returned timestamps are reasonably accurate. The official notes mention structural upgrades for long-horizon/multi-scale (Interleaved‚ÄëMRoPE, DeepStack, Text‚ÄìTimestamp Alignment). Net effect for me: retrieval feels more direct.&lt;/p&gt; &lt;p&gt;If I must nitpick: on complex logic or multi-step visual reasoning, the smaller models sometimes produce ‚Äúlooks right‚Äù answers. I don‚Äôt fight it, let them handle recognition; route reasoning to a bigger model. That‚Äôs more stable in production. I also care about spatial understanding, especially for UI/flowchart localization. From others‚Äô tests, 2D/3D grounding looks solid this gen, finding buttons, arrows, and relative positions is reliable. For long/tall images, the 256K context (extendable to 1M) is friendly for multi-panel reading; cross-page references actually connect.&lt;/p&gt; &lt;p&gt;References: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T11:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o751o9</id>
    <title>My first 15 days with GLM-4.6 ‚Äî honest thoughts after using Opus and Sonnet</title>
    <updated>2025-10-15T08:02:52+00:00</updated>
    <author>
      <name>/u/DecisionLow2640</name>
      <uri>https://old.reddit.com/user/DecisionLow2640</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I first subscribed and started using &lt;strong&gt;GLM-4.6&lt;/strong&gt; with &lt;strong&gt;KiloCode&lt;/strong&gt;, I was honestly a bit disappointed. I had gotten used to the kind of UI/UX-focused results I was getting from &lt;strong&gt;Opus 4.1&lt;/strong&gt; and &lt;strong&gt;Sonnet&lt;/strong&gt;, and GLM felt different at first.&lt;/p&gt; &lt;p&gt;But after a couple of weeks of real use, I‚Äôve started to really appreciate it. For &lt;strong&gt;pure programming tasks&lt;/strong&gt; ‚Äî not design-related ‚Äî GLM-4.6 is actually more &lt;strong&gt;precise, structured, and professional&lt;/strong&gt;. It doesn‚Äôt create as much random hard-coded mock data like Sonnet 4.5 often does. Every day it surprises me by solving problems more accurately and providing deeper diagnostics ‚Äî even when I‚Äôm using it inside the &lt;strong&gt;VS Code KiloCode extension&lt;/strong&gt;, not ClaudeCode itself.&lt;/p&gt; &lt;p&gt;I had a case where Sonnet ‚Äúsolved‚Äù an issue but the bug was still there. I gave the exact same prompt to GLM-4.6, and it fixed it perfectly using proper &lt;strong&gt;software-engineering logic&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I also love that KiloCode can auto-generate &lt;strong&gt;UML diagrams&lt;/strong&gt;, which honestly reminded me of my early programming days in C and C++.&lt;/p&gt; &lt;p&gt;So yeah ‚Äî I used to rely on Opus for its relaxed, intuitive style, but now I‚Äôm seeing the real &lt;strong&gt;power and precision of GLM-4.6&lt;/strong&gt;. If you have at least a basic understanding of programming, this model is a beast ‚Äî more detailed, reliable, and consistent than Sonnet in many cases.&lt;/p&gt; &lt;p&gt;That‚Äôs my experience so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DecisionLow2640"&gt; /u/DecisionLow2640 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7k6e5</id>
    <title>NVIDIA DGX Spark‚Ñ¢ + Apple Mac Studio = 4x Faster LLM Inference with EXO 1.0</title>
    <updated>2025-10-15T19:10:38+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well this is quite interesting!&lt;/p&gt; &lt;p&gt;&lt;a href="https://blog.exolabs.net/nvidia-dgx-spark/"&gt;https://blog.exolabs.net/nvidia-dgx-spark/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k6e5/nvidia_dgx_spark_apple_mac_studio_4x_faster_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k6e5/nvidia_dgx_spark_apple_mac_studio_4x_faster_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k6e5/nvidia_dgx_spark_apple_mac_studio_4x_faster_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ocfs</id>
    <title>If it's not local, it's not yours.</title>
    <updated>2025-10-14T18:57:54+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt; &lt;img alt="If it's not local, it's not yours." src="https://preview.redd.it/zzv4ey22j4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebc1f207746b0fa04e90a129bafad3aef0ca9971" title="If it's not local, it's not yours." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzv4ey22j4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7l1io</id>
    <title>LM Studio and VL models</title>
    <updated>2025-10-15T19:43:32+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LM Studio currently downsizes images for VL inference, which can significantly hurt OCR performance. &lt;/p&gt; &lt;p&gt;v0.3.6 release notes: &lt;strong&gt;&amp;quot;Added image auto-resizing for vision model inputs, hardcoded to 500px width while keeping the aspect ratio.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/lmstudio-v0.3.6"&gt;https://lmstudio.ai/blog/lmstudio-v0.3.6&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Related GitHub reports:&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/941"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/941&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/880"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/880&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/967"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/967&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/990"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/990&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If your image is a dense page of text and the VL model seems to underperform, LM Studio preprocessing is likely the culprit. Consider using a different app.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:43:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7brfl</id>
    <title>DGX Spark is just a more expensive (probably underclocked) AGX Thor</title>
    <updated>2025-10-15T13:58:50+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It was weird not to see any detailed specs on Nvidia's DGX Spark spec sheet. No mention of how many cuda/tensor cores (they mention the cuda core counts only in the &lt;a href="https://docs.nvidia.com/dgx/dgx-spark/dgx-spark.pdf"&gt;DGX Guide&lt;/a&gt; for developers but still why so buried). This is in contrast to AGX Thor, where they list in details the specs. So i assumed that the DGX Spark is a nerfed version of the AGX Thor, given that NVidia's marketing states that the Thor throughput is 2000TFLOPs and the Spark is 1000TFLOPs. Thor has similar ecosystem too and tech stack (ie Nvidia branded Ubuntu). &lt;/p&gt; &lt;p&gt;But then &lt;a href="https://www.theregister.com/2025/10/14/dgx_spark_review"&gt;the register in their review yesterday&lt;/a&gt;, actually listed the number of cuda cores, tensor cores, and RT cores. To my surprise the spark packs 2x cuda cores and 2x tensor cores, even 48 rt cores than the THor. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;DGX Spark&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;** AGX Thor**&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;~140 W&lt;/td&gt; &lt;td align="left"&gt;40 ‚Äì 130 W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA Cores&lt;/td&gt; &lt;td align="left"&gt;6 144&lt;/td&gt; &lt;td align="left"&gt;2 560&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tensor Cores&lt;/td&gt; &lt;td align="left"&gt;192 (unofficial really)&lt;/td&gt; &lt;td align="left"&gt;96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Peak FP4 (sparse)&lt;/td&gt; &lt;td align="left"&gt;‚âà 1 000 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;‚âà 2 070 TFLOPS&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And now I have more questions than answers. The &lt;a href="https://i.ibb.co/C3BGBy0T/2025-10-15-15-27.png"&gt;benchmarks of the Thor&lt;/a&gt; actually show numbers similar &lt;a href="https://www.youtube.com/watch?v=FVPE5zCte_E"&gt;to the Ryzen AI Max and M4 Pro&lt;/a&gt;, so again more confusion, because the Thor should be &amp;quot;twice as fast for AI&amp;quot; than the Spark. This goes to show that the metric of &amp;quot;AI TFLOPS&amp;quot; is absolutely useless, because also on paper Spark packs more cores. Maybe it matters for training/finetuning, but then we would have observed this for inference too. &lt;/p&gt; &lt;p&gt;The only explanation is that Nvidia underclocked the DGX Spark (some reviewers like NetworkChuck reported very hot devices) so the small form factor is not helping take full advantage of the hardware, and I wonder how it will fair with continuous usage (ie finetuning / training). We've seen this with the Ryzen AI where the EVO-x2 takes off to space with those fans.&lt;br /&gt; I saw some benchmarks with vLLM and &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;batched llama.cpp&lt;/a&gt; being very good, which is probably where the extra cores that Spark has would shine compared to Mac or Ryzen AI or the Thor. &lt;/p&gt; &lt;p&gt;Nonetheless, the value offering for the Spark (4k $) is nearly similar (at least in observed performance) to that of the Thor (3.5k $), yet it costs more. If you go by &amp;quot;AI TFLOPS&amp;quot; on paper the Thor is a better deal, and a bit cheaper.&lt;br /&gt; If you go by raw numbers, the Spark (probably if properly overclocked) might give you on the long term better bang for bucks (good luck with warranty though). &lt;/p&gt; &lt;p&gt;But if you want inference: get a Ryzen AI Max if you're on a budget, or splurge on a Mac. If you have space and don't mind leeching power, probably DDR4 servers + old AMD GPUs are the way to go, or even the just announced M5 (with that meager 150GB/s memory bandwidth). &lt;/p&gt; &lt;p&gt;For batched inference, we need better data for comparison. But from what I have seen so far, it's a tough market for the DGX Spark, and Nvidia marketing is not helping at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7brfl/dgx_spark_is_just_a_more_expensive_probably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7brfl/dgx_spark_is_just_a_more_expensive_probably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7brfl/dgx_spark_is_just_a_more_expensive_probably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7j6ri</id>
    <title>Microcenter has RTX3090Ti‚Äôs</title>
    <updated>2025-10-15T18:34:30+00:00</updated>
    <author>
      <name>/u/flanconleche</name>
      <uri>https://old.reddit.com/user/flanconleche</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"&gt; &lt;img alt="Microcenter has RTX3090Ti‚Äôs" src="https://b.thumbs.redditmedia.com/Tx7CGhbxlRoqyuJQwDyZMIXkM4c-MnruHZbkvMAEyJk.jpg" title="Microcenter has RTX3090Ti‚Äôs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if anyone cares but my local Microcenter has refurb RTX 3090Ti‚Äôs for $800. If your on the market for 3090‚Äôs it might be worth checking your local Microcenter. The used market prices have gone up to $900 and at Least you have some sort of warranty. &lt;/p&gt; &lt;p&gt;Also got a chance to play with the dgx spark, that thing is really cool. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flanconleche"&gt; /u/flanconleche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7j6ri"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T18:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7kkf0</id>
    <title>Poor GPU Club : 8GB VRAM - MOE models' t/s with llama.cpp</title>
    <updated>2025-10-15T19:25:03+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuation to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;my previous thread&lt;/a&gt;. This time I got better pp numbers with tg because of additional parameters. Tried with latest llama.cpp. &lt;/p&gt; &lt;p&gt;&lt;sup&gt;My System Info: (&lt;/sup&gt;&lt;strong&gt;&lt;sup&gt;8GB VRAM &amp;amp; 32GB RAM&lt;/sup&gt;&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Intel(R&lt;/sup&gt; Core(TM) i7-14700HX 2.10 GHz | 32 GB RAM | 64-bit OS, x64-based processor | NVIDIA GeForce RTX 4060 Laptop GPU |) &lt;strong&gt;&lt;sup&gt;Cores - 20 | Logical Processors - 28&lt;/sup&gt;&lt;/strong&gt;&lt;sup&gt;.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-UD-Q4_K_XL - 33 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 160.45 ¬± 18.06 | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 33.73 ¬± 0.74 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-20b-mxfp4 - 42 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\gpt-oss-20b-mxfp4.gguf -ngl 99 -ncmoe 10 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 823.93 ¬± 109.69 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 42.06 ¬± 0.56 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-lite-1.5-2507.i1-Q6_K - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-lite-1.5-2507.i1-Q6_K.gguf -ngl 99 -ncmoe 15 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 585.52 ¬± 18.03 | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 34.38 ¬± 1.54 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-lite-1.5-2507.i1-Q5_K_M - 50 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-lite-1.5-2507.i1-Q5_K_M.gguf -ngl 99 -ncmoe 12 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 183.79 ¬± 16.55 | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 50.03 ¬± 0.46 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-lite.i1-Q6_K - 35 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-Coder-lite.i1-Q6_K.gguf -ngl 99 -ncmoe 15 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 470.17 ¬± 113.93 | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 35.05 ¬± 3.33 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-lite.i1-Q5_K_M - 47 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-Coder-lite.i1-Q5_K_M.gguf -ngl 99 -ncmoe 14 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 593.95 ¬± 91.55 | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 47.39 ¬± 0.68 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M.gguf -ngl 99 -ncmoe 27 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | smallthinker 20B Q4_K - Medium | 12.18 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 512.92 ¬± 109.33 | | smallthinker 20B Q4_K - Medium | 12.18 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 34.75 ¬± 0.22 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SmallThinker-21BA3B-Instruct-IQ4_XS - 38 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\SmallThinker-21BA3B-Instruct-IQ4_XS.gguf -ngl 99 -ncmoe 25 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | smallthinker 20B IQ4_XS - 4.25 bpw | 10.78 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 635.01 ¬± 105.46 | | smallthinker 20B IQ4_XS - 4.25 bpw | 10.78 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 37.47 ¬± 0.37 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL - 44 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 14 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | ernie4_5-moe 21B.A3B Q4_K - Medium | 11.91 GiB | 21.83 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 568.99 ¬± 134.16 | | ernie4_5-moe 21B.A3B Q4_K - Medium | 11.91 GiB | 21.83 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 44.83 ¬± 1.72 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Phi-mini-MoE-instruct-Q8_0 - 65 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Phi-mini-MoE-instruct-Q8_0.gguf -ngl 99 -ncmoe 4 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | phimoe 16x3.8B Q8_0 | 7.58 GiB | 7.65 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 2570.72 ¬± 48.54 | | phimoe 16x3.8B Q8_0 | 7.58 GiB | 7.65 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 65.41 ¬± 0.19 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'll be updating this thread whenever I get optimization tips &amp;amp; tricks from others AND I'll be including additional results here with updated commands. Also whenever new MOE models get released. Currently I'm checking bunch more MOE models, I'll add those here in this week. Thanks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Updates : To be updated&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;sup&gt;My Upcoming threads (Planned&lt;/sup&gt; :)&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - Dense models' t/s with llama.cpp&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with llama.cpp - CPU only&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with ik\&lt;/sup&gt;llama.cpp (Still I'm looking for help on ik_llama.cpp))&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with ik\&lt;/sup&gt;llama.cpp - CPU only)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7miyx</id>
    <title>Just ordered new 3090 TI from MicroCenter ü§î</title>
    <updated>2025-10-15T20:39:56+00:00</updated>
    <author>
      <name>/u/GravyPoo</name>
      <uri>https://old.reddit.com/user/GravyPoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt; &lt;img alt="Just ordered new 3090 TI from MicroCenter ü§î" src="https://preview.redd.it/mzozs3957cvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb5a83e22f624acd437f0414ec334d5a460f063d" title="Just ordered new 3090 TI from MicroCenter ü§î" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GravyPoo"&gt; /u/GravyPoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mzozs3957cvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b1i3</id>
    <title>Looks like the DGX Spark a bad 4K investment vs Mac</title>
    <updated>2025-10-15T13:29:44+00:00</updated>
    <author>
      <name>/u/meshreplacer</name>
      <uri>https://old.reddit.com/user/meshreplacer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt; &lt;img alt="Looks like the DGX Spark a bad 4K investment vs Mac" src="https://b.thumbs.redditmedia.com/pPegjJ4GiV-jDUO1MueuQ5ieZJJkj24-yiTMqhNj4TQ.jpg" title="Looks like the DGX Spark a bad 4K investment vs Mac" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/om6zy3z42avf1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=31dff7de8ac355eff8c2962f8f03084cec0ada0c"&gt;https://preview.redd.it/om6zy3z42avf1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=31dff7de8ac355eff8c2962f8f03084cec0ada0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looks like 4K gets you a slower more expensive product limited In what you can do. I could just imagine how bad it would compare to an M4 128gb Mac Studio. Day late dollar short.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meshreplacer"&gt; /u/meshreplacer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7jy1o</id>
    <title>GLM 4.6 is the new top open weight model on Design Arena</title>
    <updated>2025-10-15T19:01:56+00:00</updated>
    <author>
      <name>/u/Helpful_Jacket8953</name>
      <uri>https://old.reddit.com/user/Helpful_Jacket8953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"&gt; &lt;img alt="GLM 4.6 is the new top open weight model on Design Arena" src="https://external-preview.redd.it/kjHAP1F1s2g_NoQgjQsq0SUMOf5owow_LkPL9Q--SUo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca33eecb340f9f09d0b23530fc35bb3db655ab0f" title="GLM 4.6 is the new top open weight model on Design Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hepvwbezobvf1.png?width=1877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87d242fe8af470adee79fa9b604930404192741c"&gt;https://preview.redd.it/hepvwbezobvf1.png?width=1877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87d242fe8af470adee79fa9b604930404192741c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM models make up 20% of the top 10 and beat every iteration of GPT-5 except minimal. It has surpassed DeepSeek, Qwen, and even Sonnet 4 and 3.7. If their front-end performance continues to improve at this pace for GLM 5, they could break in the top 5. China is approaching SOTA (&lt;a href="https://www.designarena.ai/"&gt;https://www.designarena.ai/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Helpful_Jacket8953"&gt; /u/Helpful_Jacket8953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:01:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7mhf5</id>
    <title>Google &amp; Yale release C2S Scale, a Gemma-based model for cell analysis</title>
    <updated>2025-10-15T20:38:17+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is Omar, from the Gemma team.&lt;/p&gt; &lt;p&gt;I'm super excited to share this research based on Gemma. Today, we're releasing a 27B model for single-cell analysis. This model generated hypotheses about how cancer cells behave, and we were able to confirm the predictions with experimental validation in living cells. This reveals a promising new pathway for developing therapies to fight cancer. &lt;/p&gt; &lt;p&gt;This applications of open models for medical use cases are super exciting for me. It's one of many examples of how open models can change the world&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7ep8a</id>
    <title>Apple M5 Officially Announced: is this a big deal?</title>
    <updated>2025-10-15T15:48:34+00:00</updated>
    <author>
      <name>/u/ontorealist</name>
      <uri>https://old.reddit.com/user/ontorealist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;em&gt;Edit: To be clear, only the *&lt;/em&gt;base** M5 has been announced. My question is primarily about whether M5 &lt;strong&gt;Pro&lt;/strong&gt; and higher-end M5 chips with more high bandwidth memory, etc. are more compelling compared to PC builds for inference given the confirmed specs for the base M5.*)&lt;/p&gt; &lt;p&gt;If I‚Äôm understanding correctly:&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;3.5x faster AI performance&lt;/strong&gt; compared to the M4 (though the exact neural engine improvements aren‚Äôt yet confirmed)&lt;br /&gt; ‚Ä¢ &lt;strong&gt;153 GB/s memory bandwidth&lt;/strong&gt; (~30% improvement)&lt;br /&gt; ‚Ä¢ &lt;strong&gt;4x increase in GPU compute&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Unified memory architecture&lt;/strong&gt;, eliminating the need for CPU‚ÜîGPU data transfers, as with previous gens&lt;/p&gt; &lt;p&gt;Even if the neural accelerators on the base M5 aren‚Äôt dedicated matmul units (which seems unlikely given the A19 Pro), will this translate into noticeably faster prompt processing speeds?&lt;/p&gt; &lt;p&gt;At $1,600 for an entry-level 16GB M5 ($2K for 32GB), serious inference workloads feels limiting, especially when compared to refurbished M-series models with more RAM. That said, it seems like a solid choice for new users exploring local AI experiences, particularly when working with sub-30B models for RAG or large context windows at faster speeds. That, along with another LM Studio feature in the press release, is a good sign, no? &lt;/p&gt; &lt;p&gt;Do the specs / pricing represent a meaningful upgrade for anyone considering the M5 Pro, Max, or Ultra? I‚Äôd love to hear others‚Äô thoughts.&lt;/p&gt; &lt;p&gt;Read the announcement &lt;a href="https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontorealist"&gt; /u/ontorealist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T15:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmers‚Ä¶ totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmers‚Ä¶ totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmers‚Ä¶ totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7gpr8</id>
    <title>Got the DGX Spark - ask me anything</title>
    <updated>2025-10-15T17:02:50+00:00</updated>
    <author>
      <name>/u/sotech117</name>
      <uri>https://old.reddit.com/user/sotech117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt; &lt;img alt="Got the DGX Spark - ask me anything" src="https://preview.redd.it/9mr835ne4bvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42dc8e85dcff8b55d4174e98495bb8d2d144fd7d" title="Got the DGX Spark - ask me anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there‚Äôs anything you want me to benchmark (or want to see in general), let me know, and I‚Äôll try to reply to your comment. I will be playing with this all night trying a ton of different models I‚Äôve always wanted to run. &lt;/p&gt; &lt;p&gt;(&amp;amp; shoutout to microcenter my goats!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotech117"&gt; /u/sotech117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mr835ne4bvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b5i4</id>
    <title>Apple unveils M5</title>
    <updated>2025-10-15T13:34:26+00:00</updated>
    <author>
      <name>/u/Agreeable-Rest9162</name>
      <uri>https://old.reddit.com/user/Agreeable-Rest9162</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt; &lt;img alt="Apple unveils M5" src="https://preview.redd.it/5ehnojlm2avf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbc46c6e19f88c18588d2f5384d7fb2dd4717f50" title="Apple unveils M5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the iPhone 17 AI accelerators, most of us were expecting the same tech to be added to M5. Here it is! Lets see what M5 Pro &amp;amp; Max will add. The speedup from M4 to M5 seems to be around 3.5x for prompt processing. &lt;/p&gt; &lt;p&gt;Faster SSDs &amp;amp; RAM:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Additionally, with up to 2x faster SSD performance than the prior generation, the new 14-inch MacBook Pro lets users load a local LLM faster, and they can now choose up to 4TB of storage. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;150GB/s of unified memory bandwidth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Rest9162"&gt; /u/Agreeable-Rest9162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ehnojlm2avf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
