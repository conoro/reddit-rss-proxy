<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-23T18:09:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nnu9b2</id>
    <title>🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥</title>
    <updated>2025-09-22T18:20:53+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"&gt; &lt;img alt="🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥" src="https://preview.redd.it/taitk409drqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=216fec124343c71e7a56513855309026dafdb0d2" title="🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥&lt;/p&gt; &lt;p&gt;We didn’t just upgrade it. We rebuilt it for creators, designers, and AI tinkerers who demand pixel-perfect control.&lt;/p&gt; &lt;p&gt;✅ Multi-Image Editing? YES.&lt;/p&gt; &lt;p&gt;Drag in “person + product” or “person + scene” — it blends them like magic. No more Franken-images.&lt;/p&gt; &lt;p&gt;✅ Single-Image? Rock-Solid Consistency.&lt;/p&gt; &lt;p&gt;• 👤 Faces stay you — through poses, filters, and wild styles.&lt;/p&gt; &lt;p&gt;• 🛍️ Products keep their identity — ideal for ads &amp;amp; posters.&lt;/p&gt; &lt;p&gt;• ✍️ Text? Edit everything: content, font, color, even material texture.&lt;/p&gt; &lt;p&gt;✅ ControlNet Built-In.&lt;/p&gt; &lt;p&gt;Depth. Edges. Keypoints. Plug &amp;amp; play precision.&lt;/p&gt; &lt;p&gt;✨ Blog: &lt;a href="https://qwen.ai/blog?id=7a90090115ee193ce6a7f619522771dd9696dd93&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=7a90090115ee193ce6a7f619522771dd9696dd93&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💬 QwenChat: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🐙 GitHub: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 HuggingFace: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🧩 ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/taitk409drqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T18:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nombr7</id>
    <title>Anyone trained up to ~11B params? What setup actually works?</title>
    <updated>2025-09-23T16:37:27+00:00</updated>
    <author>
      <name>/u/pepsituta</name>
      <uri>https://old.reddit.com/user/pepsituta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I’ve been playing around with training a language model up to the 11B parameter range. Tried it on Kaggle already, but it blew past the 30h limit 😅 so I’m clearly gonna need a different setup.&lt;/p&gt; &lt;p&gt;A few things I’d love input on from people who’ve actually run jobs this size: • What’s the minimum viable hardware you’ve made work (GPU type/count, RAM, storage, networking)? • Tips for making model parallelism + distributed training less painful? • Frameworks/tools that actually save headaches (MosaicML, Composer, HuggingFace, FSDP, etc.)? • Any “wish I knew this earlier” lessons—cost, reliability, troubleshooting, or general sanity-savers.&lt;/p&gt; &lt;p&gt;Extra love if you can share real cluster specs (e.g., “needed X A100s” or “Y 4090s with Z TB of fast storage”), bottlenecks you hit with storage/networking, or what you’d do differently next time.&lt;/p&gt; &lt;p&gt;Appreciate any wisdom 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pepsituta"&gt; /u/pepsituta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nombr7/anyone_trained_up_to_11b_params_what_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nombr7/anyone_trained_up_to_11b_params_what_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nombr7/anyone_trained_up_to_11b_params_what_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:37:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntr5a</id>
    <title>🚀 Qwen released Qwen3-Omni!</title>
    <updated>2025-09-22T18:02:33+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"&gt; &lt;img alt="🚀 Qwen released Qwen3-Omni!" src="https://b.thumbs.redditmedia.com/G9yWrU9TAoh7X4gkG7uSbhBJ3c763zZhRkFx6pNI0wo.jpg" title="🚀 Qwen released Qwen3-Omni!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Introducing Qwen3-Omni — the first natively end-to-end omni-modal AI unifying text, image, audio &amp;amp; video in one model — no modality trade-offs!&lt;/p&gt; &lt;p&gt;🏆 SOTA on 22/36 audio &amp;amp; AV benchmarks&lt;/p&gt; &lt;p&gt;🌍 119L text / 19L speech in / 10L speech out&lt;/p&gt; &lt;p&gt;⚡ 211ms latency | 🎧 30-min audio understanding&lt;/p&gt; &lt;p&gt;🎨 Fully customizable via system prompts&lt;/p&gt; &lt;p&gt;🔗 Built-in tool calling&lt;/p&gt; &lt;p&gt;🎤 Open-source Captioner model (low-hallucination!)&lt;/p&gt; &lt;p&gt;🌟 What’s Open-Sourced? &lt;/p&gt; &lt;p&gt;We’ve open-sourced Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner, to empower developers to explore a variety of applications from instruction-following to creative tasks.&lt;/p&gt; &lt;p&gt;Try it now 👇&lt;/p&gt; &lt;p&gt;💬 Qwen Chat: &lt;a href="https://chat.qwen.ai/?models=qwen3-omni-flash"&gt;https://chat.qwen.ai/?models=qwen3-omni-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 GitHub: &lt;a href="https://github.com/QwenLM/Qwen3-Omni"&gt;https://github.com/QwenLM/Qwen3-Omni&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 HF Models: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe"&gt;https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤖 MS Models: &lt;a href="https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f"&gt;https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🎬 Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nntr5a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T18:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nojauv</id>
    <title>LLM vs LLM with Websearch</title>
    <updated>2025-09-23T14:44:10+00:00</updated>
    <author>
      <name>/u/AdSoft9261</name>
      <uri>https://old.reddit.com/user/AdSoft9261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you guys also feel that whenever an LLM does websearch its output is very bad? It takes low quality information from the web but when it answers itself without websearch its response is high quality with more depth and variety in response.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdSoft9261"&gt; /u/AdSoft9261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nojauv/llm_vs_llm_with_websearch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nojauv/llm_vs_llm_with_websearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nojauv/llm_vs_llm_with_websearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T14:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1noao31</id>
    <title>MAESTRO v0.1.6 Update: Better support for models that struggle with JSON mode (DeepSeek, Kimi K2, etc.)</title>
    <updated>2025-09-23T07:06:49+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noao31/maestro_v016_update_better_support_for_models/"&gt; &lt;img alt="MAESTRO v0.1.6 Update: Better support for models that struggle with JSON mode (DeepSeek, Kimi K2, etc.)" src="https://preview.redd.it/7odksx6x5vqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d509b4d013113b628a0d86a1c714cedb17800243" title="MAESTRO v0.1.6 Update: Better support for models that struggle with JSON mode (DeepSeek, Kimi K2, etc.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Just pushed a quick update for my AI research agent, MAESTRO (v0.1.6-alpha).&lt;/p&gt; &lt;p&gt;The main focus was improving compatibility with great open models that don't always play nice with forced &lt;code&gt;json_schema&lt;/code&gt; outputs. I added a fallback system for structured data, so MAESTRO now works much more reliably with models like DeepSeek, Kimi K2, and others in the same boat.&lt;/p&gt; &lt;p&gt;On the API side, for those who use it, I also added support for GPT-5 models with the ability to select different &amp;quot;thinking levels&amp;quot; for more control over the reasoning process.&lt;/p&gt; &lt;p&gt;If you want to check it out, the docs have everything you need. You can find the &lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/quickstart/"&gt;Quick Start&lt;/a&gt;. see some &lt;a href="https://murtaza-nasir.github.io/maestro/example-reports/"&gt;Example Reports&lt;/a&gt;. and read the full &lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/installation/"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7odksx6x5vqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noao31/maestro_v016_update_better_support_for_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noao31/maestro_v016_update_better_support_for_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T07:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1noj229</id>
    <title>PDF text extraction using VLMs</title>
    <updated>2025-09-23T14:34:27+00:00</updated>
    <author>
      <name>/u/lochloch</name>
      <uri>https://old.reddit.com/user/lochloch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have some PDFs which contain text chunks including headers subheaders bodies and miscellaneous texts and need to extract them into JSON schema. difficult part is getting a model to semantically differentiate between different parts of the defined schema (schema is a little more complex than just the above described). Additionally some chunks have images associated with them and they need to be marked as such. Not getting any good results with local models and was wondering if any of you have done something similar and found success. &lt;/p&gt; &lt;p&gt;Biggest issue seems to be the semantics of what is what respective to the schema. Maybe local models just arent smart enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lochloch"&gt; /u/lochloch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noj229/pdf_text_extraction_using_vlms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noj229/pdf_text_extraction_using_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noj229/pdf_text_extraction_using_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T14:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1noj6xs</id>
    <title>Run Qwen3-Next-80B on 8GB GPU at 1tok/2s throughput</title>
    <updated>2025-09-23T14:39:57+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noj6xs/run_qwen3next80b_on_8gb_gpu_at_1tok2s_throughput/"&gt; &lt;img alt="Run Qwen3-Next-80B on 8GB GPU at 1tok/2s throughput" src="https://external-preview.redd.it/loqYh-WCtEaxMSj7OVC1KJ5pM9gu3MpUO3u8a7ppcoY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cab0230c5dc68a3b50a7ad3a367504dacead83b8" title="Run Qwen3-Next-80B on 8GB GPU at 1tok/2s throughput" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Mega4alik/ollm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noj6xs/run_qwen3next80b_on_8gb_gpu_at_1tok2s_throughput/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noj6xs/run_qwen3next80b_on_8gb_gpu_at_1tok2s_throughput/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T14:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnnws0</id>
    <title>Qwen 😁</title>
    <updated>2025-09-22T14:25:11+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt; &lt;img alt="Qwen 😁" src="https://preview.redd.it/milakcbb7qqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7af57edddeef91bdc75a874fb95e8ac60d2746ae" title="Qwen 😁" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/milakcbb7qqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nojjx7</id>
    <title>Scaling Agents via Continual Pre-training : AgentFounder-30B (Tongyi DeepResearch)</title>
    <updated>2025-09-23T14:54:01+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most open-source “agents” today are just general LLMs with some post-training on tool-use demos. That creates a conflict: the model has to learn agent skills and align to expert behavior at the same time, which caps performance.&lt;/p&gt; &lt;p&gt;The paper &lt;em&gt;Scaling Agents via Continual Pre-training&lt;/em&gt; (Alibaba, 2025) proposes &lt;strong&gt;Agentic Continual Pre-training (CPT)&lt;/strong&gt; as a fix. Instead of skipping straight from pre-training → post-training, they add an intermediate stage where the model is continually pre-trained on agent-like behaviors. This produces an &lt;strong&gt;agentic foundation model&lt;/strong&gt; before fine-tuning.&lt;/p&gt; &lt;p&gt;Two key ideas drive this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;First-order Action Synthesis (FAS):&lt;/strong&gt; Build (question → plan → reasoning/action) data without real API calls. Covers planning steps and reasoning chains cheaply at scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Higher-order Action Synthesis (HAS):&lt;/strong&gt; Expand existing trajectories into multiple decision branches at each step. This reuses discarded trajectories and forces the model to practice step-wise decision-making instead of just copying one “golden” path.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Training runs in &lt;strong&gt;two stages&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;~200B tokens of FAS + short HAS data, 32K context.&lt;/li&gt; &lt;li&gt;~100B tokens of high-quality HAS data, 128K context (long-horizon reasoning).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The result is &lt;strong&gt;AgentFounder-30B&lt;/strong&gt;, which outperforms all other open-source research agents and even beats some closed ones (e.g., &amp;gt;30% on HLE, 72.8% GAIA).&lt;/p&gt; &lt;p&gt;Takeaway: Agentic CPT shifts the burden. Post-training no longer has to teach both skills and alignment. Instead, the model enters fine-tuning already “thinking” like an agent.&lt;/p&gt; &lt;p&gt;Paper Link : &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;https://arxiv.org/pdf/2509.13310&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation (Paper Summary) : &lt;a href="https://www.youtube.com/watch?v=csz2X2c4BWM&amp;amp;t=5s"&gt;https://www.youtube.com/watch?v=csz2X2c4BWM&amp;amp;t=5s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nojjx7/scaling_agents_via_continual_pretraining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nojjx7/scaling_agents_via_continual_pretraining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nojjx7/scaling_agents_via_continual_pretraining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T14:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nokzcf</id>
    <title>Computer Use on Windows Sandbox</title>
    <updated>2025-09-23T15:47:22+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokzcf/computer_use_on_windows_sandbox/"&gt; &lt;img alt="Computer Use on Windows Sandbox" src="https://external-preview.redd.it/ZGcybmN2ZnZxeHFmMYar2P-d3EU8x2ju_uKYrB4yrb0aAUxLp4mH5szJsZ9M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dad47588f871ff4e7560cc46b08b82295a8db539" title="Computer Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/188jelvvqxqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokzcf/computer_use_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nokzcf/computer_use_on_windows_sandbox/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T15:47:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nol7o8</id>
    <title>oLLM: run Qwen3-Next-80B on 8GB GPU (at 1tok/2s throughput)</title>
    <updated>2025-09-23T15:56:06+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nol7o8/ollm_run_qwen3next80b_on_8gb_gpu_at_1tok2s/"&gt; &lt;img alt="oLLM: run Qwen3-Next-80B on 8GB GPU (at 1tok/2s throughput)" src="https://external-preview.redd.it/loqYh-WCtEaxMSj7OVC1KJ5pM9gu3MpUO3u8a7ppcoY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cab0230c5dc68a3b50a7ad3a367504dacead83b8" title="oLLM: run Qwen3-Next-80B on 8GB GPU (at 1tok/2s throughput)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Mega4alik/ollm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nol7o8/ollm_run_qwen3next80b_on_8gb_gpu_at_1tok2s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nol7o8/ollm_run_qwen3next80b_on_8gb_gpu_at_1tok2s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T15:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1no4exb</id>
    <title>I Upgrade 4090's to have 48gb VRAM: Comparative LLM Performance</title>
    <updated>2025-09-23T01:24:25+00:00</updated>
    <author>
      <name>/u/computune</name>
      <uri>https://old.reddit.com/user/computune</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no4exb/i_upgrade_4090s_to_have_48gb_vram_comparative_llm/"&gt; &lt;img alt="I Upgrade 4090's to have 48gb VRAM: Comparative LLM Performance" src="https://b.thumbs.redditmedia.com/TbEBDmv-f-LDRp2-dobGGwXS3g7rJq4IQ94R8DZo70c.jpg" title="I Upgrade 4090's to have 48gb VRAM: Comparative LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested the 48gb 4090 against the stock 24gb 4090, 80gb A100, and 48gb A6000&lt;/p&gt; &lt;p&gt;It blew the A6000 out of the water (of course it is one generation newer), though doesn't have nvlink. But at $3500 for second hand A6000's, these 4090's are very competitive at around $3000.&lt;/p&gt; &lt;p&gt;Compared to the stock 4090, i see (what could be variance) a 1-2% increase in small model latency compared to the stock 24gb 4090.&lt;/p&gt; &lt;p&gt;The graphed results are based off of this &lt;a href="https://github.com/chigkim/prompt-test"&gt;llm testing suite on github&lt;/a&gt; by chigkim&lt;/p&gt; &lt;h1&gt;Physical specs:&lt;/h1&gt; &lt;p&gt;The blower fan makes it run at 70 dB under load, noticeably audible and you wouldn't be comfortable doing work next to it. Its an &amp;quot;in the other room&amp;quot; type of card. Water block is in development.&lt;/p&gt; &lt;p&gt;Rear side back-plate heats to about 54 degrees C. Well within operating spec of the micron memory modules.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I upgrade and make these cards in the USA (no tariffs or long wait)&lt;/strong&gt;. My process involves careful attention to thermal management during every step of the process to ensure the chips don't have a degraded lifespan. I have more info on my website. (been an online video card repair shop since 2021)&lt;/p&gt; &lt;p&gt;&lt;a href="https://gpvlab.com/rtx-info.html"&gt;https://gpvlab.com/rtx-info.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ZaJnjfcOPpI"&gt;https://www.youtube.com/watch?v=ZaJnjfcOPpI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know what other testing youd like done. Im open to it. I have room for 4x of these in a 4x x16 (pcie 4.0) intel server for testing.&lt;/p&gt; &lt;p&gt;Exporting to the UK/EU/Cad and other countries is possible- though export control to CN will be followed as described by EAR&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/computune"&gt; /u/computune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1no4exb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no4exb/i_upgrade_4090s_to_have_48gb_vram_comparative_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1no4exb/i_upgrade_4090s_to_have_48gb_vram_comparative_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T01:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnt1bw</id>
    <title>3 Qwen3-Omni models have been released</title>
    <updated>2025-09-22T17:36:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art across modalities&lt;/strong&gt;: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;: Supports 119 text languages, 19 speech input languages, and 10 speech output languages. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speech Input&lt;/strong&gt;: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speech Output&lt;/strong&gt;: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel Architecture&lt;/strong&gt;: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Audio/Video Interaction&lt;/strong&gt;: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Control&lt;/strong&gt;: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Audio Captioner&lt;/strong&gt;: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/td&gt; &lt;td align="left"&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf"&gt;Qwen3-Omni Technical Report&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/td&gt; &lt;td align="left"&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf"&gt;Qwen3-Omni Technical Report&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/td&gt; &lt;td align="left"&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb"&gt;cookbook&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nokxsj</id>
    <title>DeepStudio - Google AI Studio's App Builder at home (for static html/css/js apps and sites)</title>
    <updated>2025-09-23T15:45:48+00:00</updated>
    <author>
      <name>/u/Perfect_Twist713</name>
      <uri>https://old.reddit.com/user/Perfect_Twist713</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"&gt; &lt;img alt="DeepStudio - Google AI Studio's App Builder at home (for static html/css/js apps and sites)" src="https://external-preview.redd.it/yDvyf-zbJDe3LDBNM7frIodnUlArIlsW27VvFHZ7mM8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecdb4d9c5d62513c3d9d4e891cdad0e6ed24d7aa" title="DeepStudio - Google AI Studio's App Builder at home (for static html/css/js apps and sites)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4xudsfesnxqf1.png?width=3083&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dd0f4bf93f6ebc44e767adb2b13f61e4dc314c8"&gt;DeepStudio - the main workspace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Howdy!&lt;/p&gt; &lt;p&gt;I've been tinkering on &lt;strong&gt;DeepStudio&lt;/strong&gt; for a while and I think it's finally good and clean enough to share. &lt;/p&gt; &lt;p&gt;A &lt;a href="https://huggingface.co/spaces/enzostvs/deepsite"&gt;DeepSite v2&lt;/a&gt; fork where I first added support for more providers and model listing, then multi-file support, taking that much further with a Virtual File System (file storage in IndexedDB), adding agentic capabilities for the code changes, conversation/session history, checkpoints and saves, then adding sh/bash commands in the VFS for the agent to use (reducing the need for dozens of tool definitions to just 2), support for non-tool models via JSON parsing, responsive UX/UI and so much more that I can't even remember. &lt;/p&gt; &lt;p&gt;In the end I ended up with what is basically &lt;strong&gt;Google AI Studio's App Builder&lt;/strong&gt; at home.&lt;/p&gt; &lt;p&gt;Major part of the motivation for the project has also been the fact that I quite enjoy Google AI Studio's App builder for testing out ideas whether at home or out, but I always have a nagging feeling that there's going to be a day when they slap a 5k/mo price tag on it and then I'll be back to being a frustrated peasant.&lt;/p&gt; &lt;p&gt;Work with &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;LM Studio&lt;/strong&gt; as well, but I've been testing mostly with OpenRouter (note it reports 4x higher costs than actual). Some models that work well: gpt-oss-120b, Qwen3 series, GLM-4.5, Kimi K2. The closed source SOTA models obviously work great too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you're using OpenRouter or any other remote provider then be sure to set up limits&lt;/strong&gt;. Although there is a stop functionality for stopping further tool calls/processing, it's entirely possible something goes wrong and I'd be plenty miffed if someone spent their lifesavings on a html5 snake game.&lt;/p&gt; &lt;p&gt;If you make something cool with DeepStudio I'd appreciate it a lot if you could share it with me and please consider that this is a &lt;strong&gt;solo project&lt;/strong&gt; that I've been doing on the side, so please be patient if fixes take a bit of time to arrive. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF Demo&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/otst/deepstudio"&gt;https://huggingface.co/spaces/otst/deepstudio&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Git / Source code&lt;/strong&gt;: &lt;a href="https://github.com/o-stahl/deepstudio"&gt;https://github.com/o-stahl/deepstudio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Twist713"&gt; /u/Perfect_Twist713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nokxsj/deepstudio_google_ai_studios_app_builder_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T15:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nomi16</id>
    <title>I built an open-source Writing Assistant inspired by Apple Intelligence, called ProseFlow.</title>
    <updated>2025-09-23T16:44:01+00:00</updated>
    <author>
      <name>/u/LSXPRIME</name>
      <uri>https://old.reddit.com/user/LSXPRIME</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomi16/i_built_an_opensource_writing_assistant_inspired/"&gt; &lt;img alt="I built an open-source Writing Assistant inspired by Apple Intelligence, called ProseFlow." src="https://external-preview.redd.it/YWUxbmU3Z3ZweHFmMQ5U_qROBXjFN3SoDz3kTm8LCTfeK5cYjjrj33SnLUqP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66e01a2875482ec6037455e8006611566e499fa2" title="I built an open-source Writing Assistant inspired by Apple Intelligence, called ProseFlow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good evening,&lt;/p&gt; &lt;p&gt;As someone who barely communicates with others, I really find it hard to write to talk to others, and while AI makes it easier, still, selecting the right words—is it correct or not—is this the best way to deliver information? Ah, while AI helps, but keeping copy-paste and refining my inputs is just frustrating. I was tired of the clunky workflow of copy-pasting text into a separate UI. I wanted my models to feel integrated into my OS. So, I built ProseFlow.&lt;/p&gt; &lt;p&gt;ProseFlow is a system-level utility that lets you apply AI actions to selected text anywhere. You highlight text in your browser, IDE, or document editor, press a hotkey, and a menu of your custom actions appears.&lt;/p&gt; &lt;p&gt;The core workflow is simple: 1. &lt;strong&gt;Select text&lt;/strong&gt; in any application. 2. &lt;strong&gt;Press a global hotkey&lt;/strong&gt; (e.g., &lt;code&gt;Ctrl+J&lt;/code&gt;). 3. A floating, searchable menu of your custom AI &lt;strong&gt;Actions&lt;/strong&gt; (Proofread, Summarize, Refactor Code) appears. 4. Select an action, and it transforms your text instantly.&lt;/p&gt; &lt;p&gt;The key features are: * &lt;strong&gt;Deep Customization:&lt;/strong&gt; You can create unlimited actions, each with its own system prompt, to tailor the model's behavior for specific tasks. * &lt;strong&gt;Iterative Refinement:&lt;/strong&gt; For complex tasks, the result opens in a window where you can conversationally refine it (e.g., &amp;quot;make it shorter,&amp;quot; &amp;quot;add bullet points&amp;quot;). * &lt;strong&gt;Smart Paste:&lt;/strong&gt; Assign a second hotkey to your most-used action for one-press text transformation. * &lt;strong&gt;Context-Aware Actions:&lt;/strong&gt; You can make actions (like code refactoring) only appear when you're in specific apps (like VS Code). * &lt;strong&gt;Official Models &amp;amp; Dataset:&lt;/strong&gt; I fine-tuned &lt;strong&gt;&lt;a href="https://huggingface.co/LSXPrime/ProseFlow-v1-1.5B-Instruct"&gt;ProseFlow-v1-1.5B-Instruct&lt;/a&gt;&lt;/strong&gt; specifically for this action-based format. It's trained on an open-source dataset I created, &lt;strong&gt;&lt;a href="https://huggingface.co/datasets/LSXPrime/ProseFlow-Actions-v1"&gt;ProseFlow-Actions-v1&lt;/a&gt;&lt;/strong&gt;, to ensure high-quality, structured output. Both are available for one-click download in the app. * &lt;strong&gt;Live Hardware Monitoring:&lt;/strong&gt; The dashboard includes real-time VRAM, RAM, CPU, and GPU monitoring so you can see exactly what your models are doing.&lt;/p&gt; &lt;p&gt;This project is free, open-source (AGPLv3), and ready for you to try. I'm looking for feedback on performance with different hardware and models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Download &amp;amp; Website:&lt;/strong&gt; &lt;a href="https://lsxprime.github.io/proseflow-web"&gt;https://lsxprime.github.io/proseflow-web&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;GitHub Repository:&lt;/strong&gt; &lt;a href="https://github.com/LSXPrime/ProseFlow"&gt;https://github.com/LSXPrime/ProseFlow&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;p&gt;macOS still untested; I would be thankful if any Mac user can confirm its functionality or report with the logs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LSXPRIME"&gt; /u/LSXPRIME &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9dqh3tfvpxqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomi16/i_built_an_opensource_writing_assistant_inspired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nomi16/i_built_an_opensource_writing_assistant_inspired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nonbug</id>
    <title>MediaTek claims 1.58-bit BitNet support with Dimensity 9500 SoC</title>
    <updated>2025-09-23T17:14:49+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Integrating the ninth-generation MediaTek NPU 990 with Generative AI Engine 2.0 doubles compute power and introduces BitNet 1.58-bit large model processing, reducing power consumption by up to 33%. Doubling its integer and floating-point computing capabilities, users benefit from 100% faster 3 billion parameter LLM output, 128K token long text processing, and the industry’s first 4k ultra-high-definition image generation; all while slashing power consumption at peak performance by 56%.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Anyone any idea which model(s) they could have tested this on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.mediatek.com/press-room/mediatek-dimensity-9500-unleashes-best-in-class-performance-ai-experiences-and-power-efficiency-for-the-next-generation-of-mobile-devices"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nonbug/mediatek_claims_158bit_bitnet_support_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nonbug/mediatek_claims_158bit_bitnet_support_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T17:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1no765m</id>
    <title>how is qwen shipping so hard</title>
    <updated>2025-09-23T03:41:27+00:00</updated>
    <author>
      <name>/u/Background-Pepper-38</name>
      <uri>https://old.reddit.com/user/Background-Pepper-38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt; &lt;img alt="how is qwen shipping so hard" src="https://b.thumbs.redditmedia.com/Ix-BRKnwE48eZZgCfEQXo4d7D9ctp0BS15z0d4MC0UE.jpg" title="how is qwen shipping so hard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d0jhab945uqf1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d657e419c81e1261e3204d12b2f4c3a658aaa428"&gt;https://preview.redd.it/d0jhab945uqf1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d657e419c81e1261e3204d12b2f4c3a658aaa428&lt;/a&gt;&lt;/p&gt; &lt;p&gt;yes, how is qwen shipping so hard&lt;br /&gt; but too many variants exist that I can't decide which one to use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Background-Pepper-38"&gt; /u/Background-Pepper-38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T03:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1noikw2</id>
    <title>Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board</title>
    <updated>2025-09-23T14:15:42+00:00</updated>
    <author>
      <name>/u/Ok-Actuary-4527</name>
      <uri>https://old.reddit.com/user/Ok-Actuary-4527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"&gt; &lt;img alt="Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board" src="https://a.thumbs.redditmedia.com/pcm1xAY4zY-tgo1Qcr29HPiU2wVkxx0BAdZ2I0j9uf8.jpg" title="Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are some curiosities and questions here about the modded 4090 48GB cards. For my local AI test environment, I need a setup with a larger VRAM pool to run some tests, so I got my hands on a dual-card rig with these. I've run some initial benchmarks and wanted to share the data.&lt;/p&gt; &lt;p&gt;The results are as expected, and I think it's a good idea to have these modded 4090 48GB cards.&lt;/p&gt; &lt;h1&gt;Test 1: Single Card GGUF Speed (GPUStack llama-box/llama.cpp)&lt;/h1&gt; &lt;p&gt;Just a simple, raw generation speed test on a single card to see how they compare head-to-head.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-32B (GGUF, Q4_K_M)&lt;/li&gt; &lt;li&gt;Backend: llama-box (llama-box in GPUStack)&lt;/li&gt; &lt;li&gt;Test: Single short prompt request generation via GPUStack UI's compare feature.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modded 4090 48GB: 38.86 t/s&lt;/li&gt; &lt;li&gt;Standard 4090 24GB (ASUS TUF): 39.45 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observation: The standard 24GB card was slightly faster. Not by much, but consistently.&lt;/p&gt; &lt;h1&gt;Test 2: Single Card vLLM Speed&lt;/h1&gt; &lt;p&gt;The same test but with a smaller model on vLLM to see if the pattern held.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-8B (FP16)&lt;/li&gt; &lt;li&gt;Backend: vLLM v0.10.2 in GPUStack (custom backend)&lt;/li&gt; &lt;li&gt;Test: Single short request generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modded 4090 48GB: 55.87 t/s&lt;/li&gt; &lt;li&gt;Standard 4090 24GB: 57.27 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observation: Same story. The 24GB card is again marginally faster in a simple, single-stream inference task. The extra VRAM doesn't translate to more speed for a single request, which is expected, and there might be a tiny performance penalty for the modded memory.&lt;/p&gt; &lt;h1&gt;Test 3: Multi-GPU Stress Test (2x 48GB vs 4x 24GB)&lt;/h1&gt; &lt;p&gt;This is where I compared my dual 48GB rig against a cloud machine with four standard 4090s. Both setups have 96GB of total VRAM running the same large model under a heavy concurrent load.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-32B (FP16)&lt;/li&gt; &lt;li&gt;Backend: vLLM v0.10.2 in GPUStack (custom backend)&lt;/li&gt; &lt;li&gt;Tool: evalscope (100 concurrent users, 400 total requests)&lt;/li&gt; &lt;li&gt;Setup A (Local): 2x Modded 4090 48GB (TP=2) on an ASUS ProArt Z790&lt;/li&gt; &lt;li&gt;Setup B (Cloud): 4x Standard 4090 24GB (TP=4) on a server-grade board&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results (Cloud 4x24GB was significantly better):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;2x 4090 48GB (Our Rig)&lt;/th&gt; &lt;th align="left"&gt;4x 4090 24GB (Cloud)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Output Throughput (tok/s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1054.1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1262.95&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. Latency (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;105.46&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;86.99&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. TTFT (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.4179&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.3947&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. Time Per Output Token (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.0844&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.0690&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis: The 4-card setup on the server was clearly superior across all metrics—almost 20% higher throughput and significantly lower latency. My initial guess was the motherboard's PCIe topology (PCIE 5.0 x16 PHB on my Z790 vs. a better link on the server, which is also PCIE).&lt;/p&gt; &lt;p&gt;To confirm this, I ran nccl-test to measure the effective inter-GPU bandwidth. The results were clear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local 2x48GB Rig:&lt;/strong&gt; Avg bus bandwidth was &lt;strong&gt;~3.0 GB/s&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud 4x24GB Rig:&lt;/strong&gt; Avg bus bandwidth was &lt;strong&gt;~3.3 GB/s&lt;/strong&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That ~10% higher bus bandwidth on the server board seems to be the key difference, allowing it to overcome the extra communication overhead of a larger tensor parallel group (TP=4 vs TP=2) and deliver much better performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Actuary-4527"&gt; /u/Ok-Actuary-4527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1noikw2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T14:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nogrv2</id>
    <title>Computer literally warms my room by 5 degrees Celsius during sustained generations</title>
    <updated>2025-09-23T13:01:27+00:00</updated>
    <author>
      <name>/u/nad_lab</name>
      <uri>https://old.reddit.com/user/nad_lab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t know how to even go about fixing this other than opening a window but for a workflow I have gpt-oss 20 b running for hours and my room acc heats up, I usually love mechanical and technological heat like 3d printing heat or heat when I play video games / pcvr BUT THIS, these ai workloads literally feel like a warm updraft from my computer, any thoughts on what to do? Anything helps on the software side to help not be so hot, yes I can and do open a window, and I live in Canada so I’m very very excited to not pay a heating bill this month cuz of this RTX 5060 ti 16 gb ram with a 3950x, cuz istg rn in the summer/fall my room avgs 30 deg c&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nad_lab"&gt; /u/nad_lab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T13:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nohcgs</id>
    <title>How can we run Qwen3-omni-30b-a3b?</title>
    <updated>2025-09-23T13:26:04+00:00</updated>
    <author>
      <name>/u/PermanentLiminality</name>
      <uri>https://old.reddit.com/user/PermanentLiminality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This looks awesome, but I can't run it. At least not yet and I sure want to run it. &lt;/p&gt; &lt;p&gt;It looks like it needs to be run with straight python transformer. I could be wrong, but none of the usual suspects like vllm, llama.cpp, etc support the multimodal nature of the model. Can we expect support in any of these?&lt;/p&gt; &lt;p&gt;Given the above, will there be quants? I figured there would at least be some placeholders on HFm but I didn't see any when I just looked. The native 16 bit format is 70GB and my best system will maybe just barely fit that in combined VRAM and system RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PermanentLiminality"&gt; /u/PermanentLiminality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T13:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1noefxl</id>
    <title>Parkiet: Fine-tuning Dia for any language</title>
    <updated>2025-09-23T11:09:00+00:00</updated>
    <author>
      <name>/u/pevers</name>
      <uri>https://old.reddit.com/user/pevers</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"&gt; &lt;img alt="Parkiet: Fine-tuning Dia for any language" src="https://preview.redd.it/r8293025cwqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=755e9d237792b112ecde2c2f108d85f0258fc59a" title="Parkiet: Fine-tuning Dia for any language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;A lot of the open-source TTS models are released for English or Chinese and lack support for other languages. I was curious to see if I could train a state-of-the-art text-to-speech (TTS) model for Dutch by using Google's free TPU Research credits. I open-sourced the weights, and documented the whole journey, from Torch model conversion, data preparation, JAX training code and inference pipeline here &lt;a href="https://github.com/pevers/parkiet"&gt;https://github.com/pevers/parkiet&lt;/a&gt; . Hopefully it can serve as a guide for others that are curious to train these models for other languages (without burning through all the credits trying to fix the pipeline). &lt;/p&gt; &lt;p&gt;Spoiler: the results are great! I believe they are *close* to samples generated with ElevenLabs. I spent about $300, mainly on GCS egress. Sample comparison can be found here &lt;a href="https://peterevers.nl/posts/2025/09/parkiet/"&gt;https://peterevers.nl/posts/2025/09/parkiet/&lt;/a&gt; .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pevers"&gt; /u/pevers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r8293025cwqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T11:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nomrj7</id>
    <title>Leaderboards &amp; Benchmarks</title>
    <updated>2025-09-23T16:53:47+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"&gt; &lt;img alt="Leaderboards &amp;amp; Benchmarks" src="https://preview.redd.it/n79ymm450yqf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=366296e43fd0844292650a0fe0b1176903e5bd77" title="Leaderboards &amp;amp; Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many Leaderboards are not up to date, recent models are missing. Don't know what happened to GPU Poor LLM Arena? I check Livebench, Dubesor, EQ-Bench often. Like these boards because these come with more Small &amp;amp; Medium size models(Typical boards usually stop with 30B at bottom &amp;amp; only few small models). For my laptop config(8GB VRAM &amp;amp; 32GB RAM), I need models 1-35B models. Dubesor's benchmark comes with Quant size too which is convenient &amp;amp; nice.&lt;/p&gt; &lt;p&gt;It's really heavy &amp;amp; consistent work to keep things up to date so big kudos to all leaderboards. What leaderboards do you check usually?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n79ymm450yqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1noe09l</id>
    <title>2 new open source models from Qwen today</title>
    <updated>2025-09-23T10:44:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt; &lt;img alt="2 new open source models from Qwen today" src="https://preview.redd.it/goah9v2r8wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07a67dbd5f99e7851c1f27295952913b340ead4d" title="2 new open source models from Qwen today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/goah9v2r8wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nolz9e</id>
    <title>Qwen3Guard - a Qwen Collection</title>
    <updated>2025-09-23T16:24:35+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"&gt; &lt;img alt="Qwen3Guard - a Qwen Collection" src="https://external-preview.redd.it/SybQlpd57ri5DOffonwxQ3RJbORPPReSb_vD77lSWek.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa8f4c7470cc8c0c033a57dba7ee804d9d9e1d36" title="Qwen3Guard - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3guard-68d2729abbfae4716f3343a1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nodc6q</id>
    <title>How are they shipping so fast 💀</title>
    <updated>2025-09-23T10:04:43+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt; &lt;img alt="How are they shipping so fast 💀" src="https://preview.redd.it/8higdv9r1wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6b83f7127ef234c48c0416953381b9c3c6004a7" title="How are they shipping so fast 💀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well good for us &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8higdv9r1wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building 🔨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio 👾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
