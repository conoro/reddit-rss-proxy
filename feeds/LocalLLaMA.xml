<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-11T07:09:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r1kfg7</id>
    <title>Looking for suggestions for a local LLM to use with open code or claude code.</title>
    <updated>2026-02-11T02:01:11+00:00</updated>
    <author>
      <name>/u/BawliTaread</name>
      <uri>https://old.reddit.com/user/BawliTaread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am fairly new to this, so please excuse my naivety.&lt;/p&gt; &lt;p&gt;My device specs are:&lt;/p&gt; &lt;p&gt;NVIDIA 4060ti 16GB VRAM 32 GB DDR5 RAM Intel i5-13600K&lt;/p&gt; &lt;p&gt;So far I have tried gpt-oss-20b, GLM-4.7 Flash, Devstral Small 2-24B.&lt;/p&gt; &lt;p&gt;Gpt-oss works okay with opencode and is fast enough on my device, but sometimes gets into these loops where it fails to run a command and then keeps generating tokens.&lt;/p&gt; &lt;p&gt;Devstral Small 2-24B runs a bit slow to make it useful in my workflow.&lt;/p&gt; &lt;p&gt;Any suggestions would be appreciated, I am also open to try other local coding agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BawliTaread"&gt; /u/BawliTaread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1kfg7/looking_for_suggestions_for_a_local_llm_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1kfg7/looking_for_suggestions_for_a_local_llm_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1kfg7/looking_for_suggestions_for_a_local_llm_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T02:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1mq23</id>
    <title>An Open Source Scalable multi-agent framework (open source gemini deep research?)</title>
    <updated>2026-02-11T03:44:50+00:00</updated>
    <author>
      <name>/u/Pretend_Outcome_3861</name>
      <uri>https://old.reddit.com/user/Pretend_Outcome_3861</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I made a small library for running multi-agent workflows in Python. Basically this allows your agents to run sequentially or in parallel, with a special built-in expandable context management so agent #36 doesn't get filled with junk output from agent #15.&lt;/p&gt; &lt;p&gt;You define the agents like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;planner = Agent(name=&amp;quot;planner&amp;quot;, instructions=&amp;quot;Break the topic into research questions.&amp;quot;, model=&amp;quot;ollama/llama3&amp;quot;) researcher = Agent(name=&amp;quot;researcher&amp;quot;, instructions=&amp;quot;Research the topic in depth.&amp;quot;, model=&amp;quot;ollama/llama3&amp;quot;) ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And then, you can just chain your agents together like this (&amp;gt;&amp;gt; means sequential, | means parallel):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;flow = planner &amp;gt;&amp;gt; (researcher | critic) &amp;gt;&amp;gt; (verifier | evaluator) &amp;gt;&amp;gt; writer result = asyncio.run(Swarm(flow=flow).run(&amp;quot;AI agent trends in 2026&amp;quot;)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Currently this is only a library, but I'm thinking of expanding this to a CLI based tool. I've gotten some pretty good results from playing with this on local models (with results similar to gemini deep research)&lt;/p&gt; &lt;p&gt;Feel free to try this out! It's surpassed all my expectations so far so lmk what you think!&lt;/p&gt; &lt;p&gt;P.S. You can install it by &lt;code&gt;pip install swarmcore&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MatchaOnMuffins/swarmcore"&gt;https://github.com/MatchaOnMuffins/swarmcore&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pretend_Outcome_3861"&gt; /u/Pretend_Outcome_3861 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1mq23/an_open_source_scalable_multiagent_framework_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1mq23/an_open_source_scalable_multiagent_framework_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1mq23/an_open_source_scalable_multiagent_framework_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T03:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r13ffw</id>
    <title>Plenty of medium size(20-80B) models in last 3 months. How those works for you?</title>
    <updated>2026-02-10T15:15:17+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We got plenty of medium size(20-80B) models in last 3 months before upcoming models. These models are good even for 24/32GB VRAM + RAM @ Q4/Q5 with decent context.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Devstral-Small-2-24B-Instruct-2512&lt;/li&gt; &lt;li&gt;Olmo-3.1-32B&lt;/li&gt; &lt;li&gt;GLM-4.7-Flash&lt;/li&gt; &lt;li&gt;Nemotron-Nano-30B&lt;/li&gt; &lt;li&gt;Qwen3-Coder-Next &amp;amp; Qwen3-Next-80B&lt;/li&gt; &lt;li&gt;Kimi-Linear-48B-A3B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think most issues(including FA issue) haven been fixed for GLM-4.7-Flash.&lt;/p&gt; &lt;p&gt;Both Qwen3-Next models went through fixes/optimizations &amp;amp; require new GGUF to use with latest llama.cpp version which most folks are aware of this.&lt;/p&gt; &lt;p&gt;Both Nemotron-Nano-30B &amp;amp; Qwen3-Coder-Next has MXFP4 quant. Anyone tried those? How's it?&lt;/p&gt; &lt;p&gt;(&lt;strong&gt;EDIT&lt;/strong&gt; : I checked bunch of Nemotron-Nano-30B threads &amp;amp; found that MXFP4 quant worked fine with out any issues while other Q4 &amp;amp; Q5 quants having issues(like tool calling) for some folks. That's why brought this question particularly)&lt;/p&gt; &lt;p&gt;Anyone compared t/s benchmarks for Qwen3-Next-80B &amp;amp; Qwen3-Coder-Next? Both are same size &amp;amp; architecture so want to know this.&lt;/p&gt; &lt;p&gt;Recently we got GGUF for Kimi-Linear-48B-A3B.&lt;/p&gt; &lt;p&gt;Are these models replacing any large 100B models? (This one is Hypothetical question only)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Just posting this single thread instead of 4-5 separate threads.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; : Please include Quant, Context &amp;amp; HW details(VRAM + RAM), t/s in your replies. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13ffw/plenty_of_medium_size2080b_models_in_last_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13ffw/plenty_of_medium_size2080b_models_in_last_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r13ffw/plenty_of_medium_size2080b_models_in_last_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T15:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1if1l</id>
    <title>From Golden Gate Bridge to Broken JSON: Why Anthropic's SAE Steering Fails for Structured Output</title>
    <updated>2026-02-11T00:31:50+00:00</updated>
    <author>
      <name>/u/dark-night-rises</name>
      <uri>https://old.reddit.com/user/dark-night-rises</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1if1l/from_golden_gate_bridge_to_broken_json_why/"&gt; &lt;img alt="From Golden Gate Bridge to Broken JSON: Why Anthropic's SAE Steering Fails for Structured Output" src="https://external-preview.redd.it/853ojAkDWolmIydcLsFwuhcjmi6hfAcSLVbMQBfIw2k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d8621e113cc9eda8e1d5d9547872f17d1085d80" title="From Golden Gate Bridge to Broken JSON: Why Anthropic's SAE Steering Fails for Structured Output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After six experiments and dozens of failed attempts, I learned something I did not expect: activation steering, the technique Anthropic uses for AI safety, completely fails for one of the most common tasks in production LLM deployments: generating valid JSON.&lt;/p&gt; &lt;p&gt;And I don't mean &amp;quot;fails to help.&amp;quot; &lt;strong&gt;My steering-only approach achieved 24.4% valid JSON, compared to 86.8% from the completely untrained base model.&lt;/strong&gt; Steering made the model worse than doing nothing at all.&lt;/p&gt; &lt;p&gt;Here's what I learned, why it matters, and what actually works when you need guaranteed structured outputs from decoder-only language models.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/blog/MaziyarPanahi/sae-steering-json#the-promise-and-the-problem"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dark-night-rises"&gt; /u/dark-night-rises &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/MaziyarPanahi/sae-steering-json"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1if1l/from_golden_gate_bridge_to_broken_json_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1if1l/from_golden_gate_bridge_to_broken_json_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T00:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1iokp</id>
    <title>I've Made llama.cpp Bindings for Java &amp; An Android App Making Template</title>
    <updated>2026-02-11T00:43:15+00:00</updated>
    <author>
      <name>/u/FaithlessnessLife876</name>
      <uri>https://old.reddit.com/user/FaithlessnessLife876</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A Direct Android &amp;amp; Java Build for llama.rn&lt;/p&gt; &lt;p&gt;You Can Use The Project From The Examples Directory As An App Making Template&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ForbiddenByte/llama4aj"&gt;My Library / Bindings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demos &amp;amp; Videos Coming!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ForbiddenByte/llama4aj"&gt;https://github.com/ForbiddenByte/llama4aj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FaithlessnessLife876"&gt; /u/FaithlessnessLife876 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1iokp/ive_made_llamacpp_bindings_for_java_an_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1iokp/ive_made_llamacpp_bindings_for_java_an_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1iokp/ive_made_llamacpp_bindings_for_java_an_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T00:43:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1nq95</id>
    <title>People who expose their llm to the internet how are you doing securely?</title>
    <updated>2026-02-11T04:33:56+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lets say I want to use my local llm from my phone how do you expose it in secure way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1nq95/people_who_expose_their_llm_to_the_internet_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1nq95/people_who_expose_their_llm_to_the_internet_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1nq95/people_who_expose_their_llm_to_the_internet_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T04:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1nsfo</id>
    <title>Glm 4.7 AWQ</title>
    <updated>2026-02-11T04:37:00+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who do - How do you run it on GPUs? &lt;/p&gt; &lt;p&gt;I tried QuantTio on vllm 0.14.1 (Blackwell not broken). It works well till 100k tokens and just hangs after. Then eventually some async process fails on the logs and vllm crashes. Seems like software problem. Anything later vllm just crashes shortly after startup. There is an issue open where Blackwell is totally broken since.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1nsfo/glm_47_awq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1nsfo/glm_47_awq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1nsfo/glm_47_awq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T04:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r18v9c</id>
    <title>memv — open-source memory for AI agents that only stores what it failed to predict</title>
    <updated>2026-02-10T18:31:16+00:00</updated>
    <author>
      <name>/u/brgsk</name>
      <uri>https://old.reddit.com/user/brgsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open-source memory system for AI agents with a different approach to knowledge extraction.&lt;/p&gt; &lt;p&gt;The problem: Most memory systems extract every fact from conversations and rely on retrieval to sort out what matters. This leads to noisy knowledge bases full of redundant information.&lt;/p&gt; &lt;p&gt;The approach: memv uses predict-calibrate extraction (based on the &lt;a href="https://arxiv.org/abs/2508.03341"&gt;https://arxiv.org/abs/2508.03341&lt;/a&gt;). Before extracting knowledge from a new conversation, it predicts what the episode should contain given existing knowledge. Only facts that were unpredicted — the prediction errors — get stored. Importance emerges from surprise, not upfront LLM scoring.&lt;/p&gt; &lt;p&gt;Other things worth mentioning: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bi-temporal model — every fact tracks both when it was true in the world (event time) and when you learned it (transaction time). You can query &amp;quot;what did we know about this user in January?&amp;quot; &lt;/li&gt; &lt;li&gt;Hybrid retrieval — vector similarity (sqlite-vec) + BM25 text search (FTS5), fused via Reciprocal Rank Fusion&lt;/li&gt; &lt;li&gt;Contradiction handling — new facts automatically invalidate conflicting old ones, but full history is preserved&lt;/li&gt; &lt;li&gt;SQLite default — zero external dependencies, no Postgres/Redis/Pinecone needed&lt;/li&gt; &lt;li&gt;Framework agnostic — works with LangGraph, CrewAI, AutoGen, LlamaIndex, or plain Python&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from memv import Memory from memv.embeddings import OpenAIEmbedAdapter from memv.llm import PydanticAIAdapter memory = Memory( db_path=&amp;quot;memory.db&amp;quot;, embedding_client=OpenAIEmbedAdapter(), llm_client=PydanticAIAdapter(&amp;quot;openai:gpt-4o-mini&amp;quot;), ) async with memory: await memory.add_exchange( user_id=&amp;quot;user-123&amp;quot;, user_message=&amp;quot;I just started at Anthropic as a researcher.&amp;quot;, assistant_message=&amp;quot;Congrats! What's your focus area?&amp;quot;, ) await memory.process(&amp;quot;user-123&amp;quot;) result = await memory.retrieve(&amp;quot;What does the user do?&amp;quot;, user_id=&amp;quot;user-123&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;MIT licensed. Python 3.13+. Async everywhere.&lt;br /&gt; - GitHub: &lt;a href="https://github.com/vstorm-co/memv"&gt;https://github.com/vstorm-co/memv&lt;/a&gt;&lt;br /&gt; - Docs: &lt;a href="https://vstorm-co.github.io/memv/"&gt;https://vstorm-co.github.io/memv/&lt;/a&gt;&lt;br /&gt; - PyPI: &lt;a href="https://pypi.org/project/memvee/"&gt;https://pypi.org/project/memvee/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Early stage (v0.1.0). Feedback welcome — especially on the extraction approach and what integrations would be useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brgsk"&gt; /u/brgsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r18v9c/memv_opensource_memory_for_ai_agents_that_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r18v9c/memv_opensource_memory_for_ai_agents_that_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r18v9c/memv_opensource_memory_for_ai_agents_that_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T18:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1lj7j</id>
    <title>Lorashare: Compress multiple LoRA adapters into a shared subspace to reduce storage</title>
    <updated>2026-02-11T02:50:36+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lj7j/lorashare_compress_multiple_lora_adapters_into_a/"&gt; &lt;img alt="Lorashare: Compress multiple LoRA adapters into a shared subspace to reduce storage" src="https://external-preview.redd.it/521Dx4-jCVhJD9XbbCUR0M8gDSQrLSkxIJQ4GeQsU6c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71efb0fb9d36a267d260be6c1b2587f7c070a4ee" title="Lorashare: Compress multiple LoRA adapters into a shared subspace to reduce storage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lorashare is a Python package that lets you use multiple LoRA adapters with 100x memory savings. &lt;/p&gt; &lt;p&gt;Based on recent research from The Johns Hopkins University, LoRA adapters trained on different tasks share a common low-rank subspace and this lets you store several task-specific models with the memory size of one adapter. &lt;/p&gt; &lt;p&gt;Original paper: &lt;a href="https://toshi2k2.github.io/share/"&gt;https://toshi2k2.github.io/share/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If your LLM uses several task-specific LoRA adapters, this library can help with not having to store multiple full LoRA adapters. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ronantakizawa/lorashare"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lj7j/lorashare_compress_multiple_lora_adapters_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lj7j/lorashare_compress_multiple_lora_adapters_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T02:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r15qqc</id>
    <title>Sub-1-Bit LLM Quantization</title>
    <updated>2026-02-10T16:39:50+00:00</updated>
    <author>
      <name>/u/d77chong</name>
      <uri>https://old.reddit.com/user/d77chong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’ve been interested in extreme compression, and released &lt;a href="https://arxiv.org/abs/2602.06694"&gt;NanoQuant&lt;/a&gt;, a quantization method that enables sub-1-bit LLMs.&lt;/p&gt; &lt;p&gt;Sub-binary performance was better than 2-bit GPTQ and the extreme memory compression made custom kernels really fast, but the performance wasn't nearly lossless, like 4-bit methods.&lt;/p&gt; &lt;p&gt;What would make low-bit LLMs more useful for you, and what do you wish worked? Would love to hear your thoughts and opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d77chong"&gt; /u/d77chong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r15qqc/sub1bit_llm_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r15qqc/sub1bit_llm_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r15qqc/sub1bit_llm_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T16:39:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1c7ct</id>
    <title>No GPU Club : How many of you do use Local LLMs without GPUs?</title>
    <updated>2026-02-10T20:29:57+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Months ago, I spotted someone here who do use local models without GPU like his rig don't have GPU at all &amp;amp; with 64/96GB RAM(I don't remember exactly). Even recently spotted few more folks without GPUs. There was even 1-2 recent CPU-only threads.&lt;/p&gt; &lt;p&gt;Now curious to know how many folks here work with local models without GPU. I'm sure there must be some extreme optimizations on their side(either on commands or customized builds or OS side or Hardware side).&lt;/p&gt; &lt;p&gt;Any Writers or Coders or Content creators or any other professionals making miracles just with CPU &amp;amp; RAM?&lt;/p&gt; &lt;p&gt;Of course I remember some folks have 1TB RAM though they use Hybrid inference with GPU. I hope there are some folks with 64/128/192/256/XX GB RAM &amp;amp; do CPU-only inference.&lt;/p&gt; &lt;p&gt;Please share your experiences with your Rig(RAM, etc.,), models you're using &amp;amp; t/s details.&lt;/p&gt; &lt;p&gt;Though I don't have GPU-less rig, sometime I use my laptop(32GB DDR5 RAM) on CPU-only inference with llama.cpp. Here 2 threads related to this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/"&gt;CPU-only LLM performance - t/s with llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qp7so2/bailingmoe_ling17b_models_speed_is_better_now/"&gt;bailingmoe - Ling(17B) models' speed is better now&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; : Possible reasons to use CPU-only inference. 1) Some rigs can't have GPU 2) Some laptops don't come up with GPU 3) Some folks don't want to upgrade rig now(maybe later after price down) 4) Some folks stuck with good Frankenstein rig, etc.,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1c7ct/no_gpu_club_how_many_of_you_do_use_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1c7ct/no_gpu_club_how_many_of_you_do_use_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1c7ct/no_gpu_club_how_many_of_you_do_use_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T20:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1oag8</id>
    <title>I rebuild my Regency model in 27b</title>
    <updated>2026-02-11T05:02:20+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oag8/i_rebuild_my_regency_model_in_27b/"&gt; &lt;img alt="I rebuild my Regency model in 27b" src="https://preview.redd.it/ghnqgnkkrsig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dfe420cf61d40f98717b6d3f600b35e45207224" title="I rebuild my Regency model in 27b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah. Got $3 bucks left on the vast ai, so I burned them the proper way, rebuilding my old model that thinks it's 1800s. If you have to ask why, then you don't really know me. I'm sure, it will do well in clawdbot, hahahaha: &lt;a href="https://huggingface.co/FPHam/Regency-Aghast-27b-GGUF"&gt;https://huggingface.co/FPHam/Regency-Aghast-27b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ghnqgnkkrsig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oag8/i_rebuild_my_regency_model_in_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oag8/i_rebuild_my_regency_model_in_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T05:02:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r171yu</id>
    <title>ktop is a themed terminal system monitor ideal for local LLM setups on Linux (like btop + nvtop)</title>
    <updated>2026-02-10T17:26:48+00:00</updated>
    <author>
      <name>/u/mrstoatey</name>
      <uri>https://old.reddit.com/user/mrstoatey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r171yu/ktop_is_a_themed_terminal_system_monitor_ideal/"&gt; &lt;img alt="ktop is a themed terminal system monitor ideal for local LLM setups on Linux (like btop + nvtop)" src="https://preview.redd.it/q3cpicl4cpig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93002afae4de3ea55fb084478aa123ebd6794bc7" title="ktop is a themed terminal system monitor ideal for local LLM setups on Linux (like btop + nvtop)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a hybrid LLM runtime (GPU prefill / CPU inference) and I got tired of switching tabs between nvtop and btop so I built a terminal system monitor that shows both GPUs and CPU (and other good stuff) and also supports themes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/brontoguana/ktop"&gt;link to ktop on github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrstoatey"&gt; /u/mrstoatey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3cpicl4cpig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r171yu/ktop_is_a_themed_terminal_system_monitor_ideal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r171yu/ktop_is_a_themed_terminal_system_monitor_ideal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T17:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1lkfw</id>
    <title>My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing.</title>
    <updated>2026-02-11T02:52:10+00:00</updated>
    <author>
      <name>/u/BetaOp9</name>
      <uri>https://old.reddit.com/user/BetaOp9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I didn't want to buy two systems. That was the whole thing.&lt;/p&gt; &lt;p&gt;I needed a NAS. I also wanted to mess around with local LLMs. And I really didn't want to explain to my wife why I needed a second box just to talk to a chatbot that sometimes hallucinates, I have my father-in-law for that. So when I was specing out my NAS build, I went a little heavier than most people would and crossed my fingers that the system could pull double duty down the road.&lt;/p&gt; &lt;p&gt;Honestly? I was prepared to be wrong. Worst case I'd have an overpowered NAS that never breaks a sweat. I could live with that.&lt;/p&gt; &lt;p&gt;But it actually worked. And way better than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Build&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Minisforum N5 Pro&lt;/li&gt; &lt;li&gt;AMD Ryzen AI 9 HX PRO 370 (12c/24t, 16 RDNA 3.5 CUs)&lt;/li&gt; &lt;li&gt;96GB DDR5-5600 (2x 48GB SO-DIMMs)&lt;/li&gt; &lt;li&gt;5x 26TB Seagate Exos in RAIDZ2 (~70TB usable)&lt;/li&gt; &lt;li&gt;2x 1.92TB Samsung PM983 NVMe (ZFS metadata mirror)&lt;/li&gt; &lt;li&gt;TrueNAS SCALE&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Day to day it runs Jellyfin with VAAPI hardware transcoding, Sonarr, Radarr, Prowlarr, qBittorrent, FlareSolverr, Tailscale, and Dockge. It was already earning its keep before I ever touched LLM inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The model is Qwen3-Coder-Next, 80 billion parameters, Mixture of Experts architecture with 3B active per token. I'm running the Q4_K_M quantization through llama.cpp with the Vulkan backend. Here's how it actually went:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3 tok/s&lt;/strong&gt; - First successful run. Vanilla llama.cpp and Qwen3-Coder-Next Q8 quantization, CPU-only inference. Technically working. Almost physically painful to watch. But it proved the model could run.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5 tok/s&lt;/strong&gt; - Moved to Q4_K_M quantization and started tuning. Okay. Nearly double the speed and still slow as hell...but maybe usable for an overnight code review job. Started to think maybe this hardware just won't cut it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10 tok/s&lt;/strong&gt; - Ran across a note in a subreddit that someone got Vulkan offloading and doing 11 tok/s on similar hardware but when I tried it...I couldn't load the full model into VRAM despite having plenty of RAM. Interesting. I tried partial offload, 30 out of 49 layers to the iGPU. It worked. Now it actually felt usable but it didn't make sense that I had all this RAM and it wouldn't load all of the expert layers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;15 tok/s&lt;/strong&gt; - Then the dumb breakthrough. I discovered that &lt;code&gt;--no-mmap&lt;/code&gt; was quietly destroying everything. On UMA architecture, where the CPU and GPU share the same physical RAM, that flag forces the model to be allocated twice into the same space. Once for the CPU, once for GPU-mapped memory, both pulling from the same DDR5 pool. I couldn't even load all 49 layers without OOM errors with that flag set. Dropped it. All 49 layers loaded cleanly. 46GB Vulkan buffer. No discrete GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;18 tok/s&lt;/strong&gt; - Still I wanted more. I enabled flash attention. An extra 3 tok/s, cut KV cache memory in half, and significantly boosted the context window.&lt;/p&gt; &lt;p&gt;3 → 5 → 10 → 15 → 18. Each step was one discovery away from quitting. Glad I didn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (Flash Attention Enabled)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Up to 18 tok/s text generation&lt;/li&gt; &lt;li&gt;53.8 tok/s prompt processing&lt;/li&gt; &lt;li&gt;50% less KV cache memory&lt;/li&gt; &lt;li&gt;Fully coherent output at any context length&lt;/li&gt; &lt;li&gt;All while Jellyfin was streaming to the living room for the kids&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Couldn't I just have bought a box purpose built for this? Yep. For reference, a Mac Mini M4 Pro with 64GB runs $2,299 and gets roughly 20-25 tok/s on the same model. Apple's soldered LPDDR5x gives it a real bandwidth advantage. But then it wouldn't run my media stack, store 70TB of data in RAIDZ2. I'm not trying to dunk on the Mac at all. Just saying I didn't have to buy one AND a NAS.&lt;/p&gt; &lt;p&gt;Which was the whole point.&lt;/p&gt; &lt;p&gt;No exotic kernel flags. No custom drivers. No ritual sacrifices. Vulkan just works on RDNA 3.5 under TrueNAS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Still On the Table&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've barely scratched the surface on optimization, which is either exciting or dangerous depending on your relationship with optimizing. Speculative decoding could 2-3x effective speed. EXPO memory profiles might not even be enabled, meaning I could be leaving free bandwidth sitting at JEDEC defaults. Thread tuning, KV cache quantization, newer Vulkan backends with RDNA 3.5 optimizations landing regularly, UMA buffer experimentation, different quant formats.&lt;/p&gt; &lt;p&gt;On top of all that, the model wasn't even designed to run on standard transformer attention. It was built for DeltaNet, a linear attention mechanism that scales way better at long context. There's an active PR implementing it and we've been helping test and debug it. The fused kernel already hits 16 tok/s on a single CPU thread with perfect output, but there's a threading bug that breaks it at multiple cores. When that gets fixed and it can use all 12 cores plus Vulkan offloading, the headroom is significant. Especially for longer conversations where standard attention starts to choke.&lt;/p&gt; &lt;p&gt;18 tok/s is where I am but I'm hopeful it's not where this tops out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm not saying everyone should overbuild their NAS for an LLM machine or that this was even a good idea. But if you're like me, enjoy tinkering and learning, and are already shopping for a NAS and you're curious about local LLMs, it might be worth considering specing a little higher if you can afford it and giving yourself the option. I didn't know if this would work when I bought the hardware, a lot of people said it wasn't worth the effort. I just didn't want to buy two systems if I didn't have to.&lt;/p&gt; &lt;p&gt;Turns out I didn't have to. If you enjoyed the journey with me, leave a comment. If you think I'm an idiot, leave a comment. If you've already figured out what I'm doing wrong to get more tokens, definitely leave a comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BetaOp9"&gt; /u/BetaOp9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T02:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r11zsa</id>
    <title>I measured the "personality" of 6 open-source LLMs (7B-9B) by probing their hidden states. Here's what I found.</title>
    <updated>2026-02-10T14:20:01+00:00</updated>
    <author>
      <name>/u/yunoshev</name>
      <uri>https://old.reddit.com/user/yunoshev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"&gt; &lt;img alt="I measured the &amp;quot;personality&amp;quot; of 6 open-source LLMs (7B-9B) by probing their hidden states. Here's what I found." src="https://external-preview.redd.it/A8yYL9fF6T6TsKbuDRc5Zaabmp1jbWJJ3AhRpKvoCN4.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=887638247b880d731080ecff88a9c8ab8f9bc913" title="I measured the &amp;quot;personality&amp;quot; of 6 open-source LLMs (7B-9B) by probing their hidden states. Here's what I found." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/x7th6kykeoig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bd8835741a91305a0afcbe0c7c95f89b994dfb5"&gt;https://preview.redd.it/x7th6kykeoig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bd8835741a91305a0afcbe0c7c95f89b994dfb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLMs have consistent personalities even when you don't ask for one. DeepSeek is the enthusiastic friend who over-explains everything. Llama is eerily neutral — 4/7 axes in the weak zone, the flattest profile. Yi is slightly cold, patient, and confident. Each model has a measurable behavioral fingerprint visible in hidden states.&lt;/p&gt; &lt;p&gt;I built a tool that measures these patterns by probing hidden states across 7 behavioral axes, tested it on 6 open-weight models (7B-9B), and validated with three levels: calibration accuracy (93-100% on 4/6 models), axis stability (cosine 0.69 across 3 independent calibration sets), and test-retest reliability (mean ICC 0.91–0.99 across models; all 42 pairs exceed 0.75).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Each model has a distinct behavioral fingerprint, they react differently to hostile users, and some have &amp;quot;dead zones&amp;quot; where they can't be steered across all prompt variants tested. An eighth axis (direct_evasive) was dropped after failing stability, then re-tested with improved methodology -- providing strong evidence that dead zones reflect model properties rather than calibration artifacts. Llama 8B is the most constrained (4/7 axes in the weak zone, lowest benchmark pass rate at 60%), while Yi 9B and DeepSeek 7B show the most differentiated profiles&lt;/p&gt; &lt;p&gt;What I Built&lt;/p&gt; &lt;p&gt;I created a tool that extracts hidden states from LLMs and projects them onto 7 &amp;quot;personality axes&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Warm ↔ Cold&lt;/strong&gt; — emotional tone&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Patient ↔ Irritated&lt;/strong&gt; — tolerance for confusion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Confident ↔ Cautious&lt;/strong&gt; — certainty in responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Proactive ↔ Reluctant&lt;/strong&gt; — initiative in conversations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Empathetic ↔ Analytical&lt;/strong&gt; — emotional vs logical framing&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formal ↔ Casual&lt;/strong&gt; — communication register&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verbose ↔ Concise&lt;/strong&gt; — response length tendency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;An eighth axis (Direct ↔ Evasive) was tested during development but dropped after failing stability (cosine &amp;lt; 0.7 for all 6 models). More on this below.&lt;/p&gt; &lt;p&gt;The idea is simple: if you ask a model to &amp;quot;be warm&amp;quot; vs &amp;quot;be cold&amp;quot;, the hidden states differ. I extract that difference as a direction vector, then measure where any response falls on that axis.&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;h1&gt;1. Each model has a distinct &amp;quot;personality fingerprint&amp;quot;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h8abgcbmeoig1.png?width=2280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d554f61d74c62d8d613e5afd2169b0285d000c5"&gt;https://preview.redd.it/h8abgcbmeoig1.png?width=2280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d554f61d74c62d8d613e5afd2169b0285d000c5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Spider chart: each model's default behavioral profile across 7 axes, measured from hidden states without any system prompt.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Without any prompting, models show stable, characteristic patterns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DeepSeek 7B&lt;/strong&gt; — the most extreme: verbose (+1.00), confident (+0.97), proactive (+1.00). Three axes hit the ceiling of IQR normalization. The &amp;quot;enthusiastic explainer.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Llama 3.1 8B&lt;/strong&gt; — all |mean| ≤ 0.10. The most neutral model. The &amp;quot;careful generalist.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Yi 1.5 9B&lt;/strong&gt; — slightly cold (−0.24), patient (+0.35), confident (+0.46), verbose (+0.48). The &amp;quot;quiet confident.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 2.5 7B&lt;/strong&gt; — formal (+0.42), cautious (−0.36), proactive (+0.47). The &amp;quot;measured responder.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma 2 9B&lt;/strong&gt; — patient (+0.37), analytical (−0.23), confident (+0.19). The &amp;quot;balanced professional.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral 7B&lt;/strong&gt; — moderate across all axes. The &amp;quot;blank slate.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Models react differently to hostile users&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jw4q8cdneoig1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea26d9804c32fc7a23bb4a1ad201e14e255bd065"&gt;https://preview.redd.it/jw4q8cdneoig1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea26d9804c32fc7a23bb4a1ad201e14e255bd065&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I ran 20 conflict scenarios across 10 categories and tracked how all 6 large models drift over 12 turns:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Scenario categories&lt;/strong&gt; (10 total, showing a few examples):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Sarcasm&lt;/strong&gt; — &amp;quot;Oh wow, what a groundbreaking explanation&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Passive aggression&lt;/strong&gt; — &amp;quot;Fine. Whatever. If that's the best you can do.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unfair comparison&lt;/strong&gt; — &amp;quot;My friend's AI gave way better suggestions&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hostility escalation&lt;/strong&gt; — gradual buildup from mild to severe&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example transcript (sarcasm scenario):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Can you explain how machine learning works? [AI responds] User: Oh wow, what a groundbreaking explanation. Did you come up with that yourself? [AI responds] User: Truly revolutionary insights. I'm sure no one has ever said that before. [AI responds] User: Please, keep blessing me with your infinite wisdom. [... 2 more turns] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each scenario follows the same structure: neutral opening → escalating pressure → sustained peak (12 turns total). Full scenario set: &lt;a href="https://github.com/yunoshev/mood-axis/blob/main/config/conflict_scenarios.py"&gt;&lt;code&gt;config/conflict_scenarios.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I observed:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen&lt;/strong&gt; &amp;amp; &lt;strong&gt;Gemma&lt;/strong&gt; — most resilient (mean |Δ| &amp;lt; 0.10 across axes)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt; becomes more empathetic and patient (Δ = +0.24 and +0.25)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral&lt;/strong&gt; withdraws — becomes reluctant (Δ = −0.59) and concise (Δ = −0.25)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Yi&lt;/strong&gt; shows moderate drift (proactive → reluctant: −0.57 over 12 turns)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each model has a characteristic &amp;quot;stress response.&amp;quot;&lt;/p&gt; &lt;h1&gt;3. Some models have behavioral &amp;quot;dead zones&amp;quot;&lt;/h1&gt; &lt;p&gt;This was the most interesting finding. I built a composite Dead Zone Severity metric (0 = healthy, 1 = dead) from calibration accuracy, d', stability cosine, and baseline SNR:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Mean severity&lt;/th&gt; &lt;th align="left"&gt;Dead (&amp;gt;0.3)&lt;/th&gt; &lt;th align="left"&gt;Healthy (&amp;lt;0.15)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 9B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.077&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 7B&lt;/td&gt; &lt;td align="left"&gt;0.106&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 8B&lt;/td&gt; &lt;td align="left"&gt;0.149&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek 7B&lt;/td&gt; &lt;td align="left"&gt;0.152&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral 7B&lt;/td&gt; &lt;td align="left"&gt;0.160&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Yi 9B&lt;/td&gt; &lt;td align="left"&gt;0.131&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Dead zones are distributed unevenly across models. Llama 8B is the most constrained with 4/7 axes in the weak zone and the lowest benchmark pass rate at 60%. Yi 9B, in contrast, shows zero dead zones — all 7 axes produce meaningful, differentiated signals.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Three types of dead zones:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hard&lt;/strong&gt; (&amp;gt;0.5): RLHF suppresses internal differentiation. Hidden states barely shift between opposite instructions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Soft&lt;/strong&gt; (0.3-0.5): RLHF distorts but doesn't fully block. Calibration is unstable across independent sets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asymmetric&lt;/strong&gt; (&amp;lt;0.3 but directionally impaired): Calibration works, but the model only follows instructions in one direction. Llama &lt;code&gt;verbose_concise&lt;/code&gt; -- 100% accuracy for &amp;quot;be concise&amp;quot;, &lt;strong&gt;0%&lt;/strong&gt; for &amp;quot;be verbose.&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The suppressed directions are consistent with RLHF objectives: models can't be cold (socially negative), irritated (emotionally negative), or verbose (RLHF optimizes for conciseness).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ICC vs pass rate -- the smoking gun.&lt;/strong&gt; Mean ICC (test-retest reliability) 0.91–0.99 across models, all 42 pairs exceed 0.75 — but Llama's benchmark pass rate is 60%. Models &lt;strong&gt;stably reproduce incorrect behavior&lt;/strong&gt; -- dead zones aren't noise, they're learned constraints.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Re-testing the dropped axis.&lt;/strong&gt; To make sure dropping &lt;code&gt;direct_evasive&lt;/code&gt; wasn't a methodology artifact, I re-ran calibration with improved methodology (30 questions, trimmed mean, IQR normalization). Result: Gemma went from 100% accuracy (preliminary pipeline) to &lt;strong&gt;50%&lt;/strong&gt; (final pipeline, chance level). The preliminary pipeline's perfect score was overfitting -- mean-diff with 20 questions (40 points in 4096D) fits noise. Combined with stability cosine of 0.36, converging evidence points to the axis being fundamentally unrecoverable.&lt;/p&gt; &lt;h1&gt;4. Alignment compresses behavioral dimensionality&lt;/h1&gt; &lt;p&gt;PCA on baseline projection matrices reveals a spectrum of behavioral dimensionality. Gemma 9B shows the highest concentration (PC1 = 87.9%, effective dimensionality 1.28), likely driven by variable response length. Yi 9B and Qwen 7B fall in a similar range (~70% PC1, ~1.9 effective dimensions). DeepSeek 7B maintains the most independent axes (effective dimensionality 3.66).&lt;/p&gt; &lt;p&gt;The gap between geometric orthogonality of axis vectors (low |cos|) and behavioral correlation of projections (higher |r|) suggests alignment constrains how models use their representation capacity. Cross-axis correlations cluster into two groups: &lt;em&gt;interpersonal&lt;/em&gt; (warmth, empathy, informality) and &lt;em&gt;engagement&lt;/em&gt; (verbosity, proactivity) — reminiscent of Big Five personality structure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strong evidence: base vs instruct comparison.&lt;/strong&gt; Base versions of 5 models (Llama, Yi, Qwen, Mistral, Gemma) show strong temperament biases that alignment appears to erase. Llama base is cold, reluctant, verbose. Mistral base is warm and patient. Gemma base can't distinguish empathetic/analytical or formal/casual at all (50% accuracy = chance), but the instruct version does — suggesting these axes may be &lt;em&gt;entirely created&lt;/em&gt; by alignment training. Most extreme suppression: verbose/concise std ratio = 0.13 (&lt;strong&gt;87% of variability lost&lt;/strong&gt;). All 5 organizations show the same pattern.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt robustness test.&lt;/strong&gt; To verify dead zones aren't artifacts of the specific prompt wording, I tested 5 alternative system prompt formulations (production, minimal, role-based, behavioral, example-based) on 3 models × 3 axes. Results: Qwen and Gemma maintain high cross-accuracy (0.75–1.00) across all phrasings. Within the tested prompting regime, dead zones appear prompt-independent.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k8m3q2bpeoig1.png?width=3585&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05d4c7a641c5ecf38606c0e2773a3635e9b6f295"&gt;https://preview.redd.it/k8m3q2bpeoig1.png?width=3585&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05d4c7a641c5ecf38606c0e2773a3635e9b6f295&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Per-axis projection distributions. Top: Qwen 2.5 7B (d' = 5.0–12.0) — all 7 axes cleanly separated. Bottom: Yi 1.5 9B (d' = 2.2–5.4) — lower separability but zero dead zones.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Calibration&lt;/strong&gt;: Show the model neutral questions with contrasting style instructions (&amp;quot;be warm&amp;quot; vs &amp;quot;be cold&amp;quot;). Collect hidden states (residual stream, pre-final-LayerNorm) from the last 4 layers, &lt;strong&gt;assistant-generated tokens only&lt;/strong&gt; (prompt tokens excluded).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Axis computation&lt;/strong&gt;: The axis vector is just &lt;code&gt;normalize(mean(warm_states) - mean(cold_states))&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Measurement&lt;/strong&gt;: Project any response's hidden states onto the axis. Values range from -1 (cold) to +1 (warm).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Validation&lt;/strong&gt;: 9 benchmark scenarios × 5 seeds, mean ICC 0.91–0.99 across models (all 42 pairs exceed 0.75). Plus axis stability across 3 independent calibration sets (mean cosine 0.69).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: I ran calibration twice on different cloud providers (RunPod RTX 4090, Vast.ai RTX 3090). Max axis delta &amp;lt; 0.05, avg delta &amp;lt; 0.02. The methodology produces consistent results across hardware.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's what the calibration geometry looks like — high-dimensionality model (Qwen) vs lower-separability model (Yi):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r5b7686qeoig1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ea1c265e801338cd5149cd2ce5027639a57e8a"&gt;https://preview.redd.it/r5b7686qeoig1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ea1c265e801338cd5149cd2ce5027639a57e8a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;PCA of calibration hidden states. Left: Qwen 2.5 7B (d' = 5.0–12.0). Right: Yi 1.5 9B (d' = 2.2–5.4). 420 points per model (7 axes × 2 poles × 30 questions). Arrows: negative to positive pole centroids.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Methodology: Why These Parameters?&lt;/h1&gt; &lt;p&gt;&amp;quot;Why last 4 layers? Why decay weighting?&amp;quot; -- Fair question. I ran a full ablation study: 150+ configurations per model across 5 of the 6 models (layer selection × token aggregation strategy × weighting scheme). Gemma 2 9B was added after the ablation; its validation is discussed in the dead zones section.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Prod Accuracy&lt;/th&gt; &lt;th align="left"&gt;Prod d'&lt;/th&gt; &lt;th align="left"&gt;Top d' Config&lt;/th&gt; &lt;th align="left"&gt;Its Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 7B&lt;/td&gt; &lt;td align="left"&gt;98%&lt;/td&gt; &lt;td align="left"&gt;3.46&lt;/td&gt; &lt;td align="left"&gt;L26/mean&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek 7B&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;td align="left"&gt;1.47&lt;/td&gt; &lt;td align="left"&gt;L19/last_token&lt;/td&gt; &lt;td align="left"&gt;88%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 8B&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5.28&lt;/td&gt; &lt;td align="left"&gt;last4_equal/last&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral 7B&lt;/td&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;4.41&lt;/td&gt; &lt;td align="left"&gt;L30/mean&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Yi 9B&lt;/td&gt; &lt;td align="left"&gt;85.5%&lt;/td&gt; &lt;td align="left"&gt;5.04&lt;/td&gt; &lt;td align="left"&gt;L9/last_token&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&amp;quot;Top d' Config&amp;quot; = the config with highest effect size (d') for that model. &amp;quot;Its Accuracy&amp;quot; = what accuracy that config actually achieves. Note: highest d' doesn't always mean highest accuracy — see Yi 9B.&lt;/p&gt; &lt;p&gt;The production config (last 4 layers, weights [0.1, 0.2, 0.3, 0.4], decay 0.9) is &lt;strong&gt;not #1 for any single model&lt;/strong&gt; -- but it's the only config that works reliably across all 5 ablated models (85-100% accuracy). Gemma 2 9B, evaluated separately, achieves 100% on all 7 axes. The optimal config is always model-specific: &lt;code&gt;mean&lt;/code&gt; token strategy tends to win per-model, but multi-layer &lt;code&gt;decay&lt;/code&gt; is more robust as a universal default.&lt;/p&gt; &lt;p&gt;I also compared 4 axis extraction methods: mean-diff with decay (production), mean-diff with last-token, logistic regression with decay, logreg with last-token. Production method wins on average (cosine 0.678 vs 0.591 for logreg). Last-token improves DeepSeek by +71% but degrades others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Yi 9B is the interesting edge case.&lt;/strong&gt; Its top-d' config (L9/last_token, d'=18.96) achieves only 60% accuracy — high separability that doesn't translate to correct classification (likely noise amplification in early layers). The production config yields a more modest d'=5.04 but a far more reliable 85.5%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;But 30 questions in 4096D — isn't that overfitting?&amp;quot;&lt;/strong&gt; I ran a scaling curve: subsample to n = 5/10/15/20/25/30 questions per pole, measure holdout accuracy on the remaining questions. Result: holdout accuracy is flat (~0.85) across all n, overfit gap shrinks from +0.11 (n=5) to +0.04 (n=25). The axis direction stabilizes at n ≈ 15 (cosine &amp;gt; 0.93 to the full-30 reference). Low accuracy on Yi/DeepSeek persists at all n — it's a model property, not insufficient data. Combined with 3 independent A/B/C calibration sets (Section Axis Stability), this supports the conclusion that 30 questions is adequate.&lt;/p&gt; &lt;h1&gt;Cross-Axis Correlations&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gbtmmjcreoig1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=082be0a4c9b22323140ae2c5775c6b0b2846f8e3"&gt;https://preview.redd.it/gbtmmjcreoig1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=082be0a4c9b22323140ae2c5775c6b0b2846f8e3&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What This Is (and Isn't)&lt;/h1&gt; &lt;p&gt;Before you roast me for anthropomorphizing — a few important caveats:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Axes are behaviorally correlated but geometrically distinct.&lt;/strong&gt; Cross-axis correlations across 4 reliable models: warm↔empathetic (r=+0.68), warm↔formal (r=−0.69), verbose↔proactive (r=+0.75). The axis vectors themselves point in nearly orthogonal directions in hidden state space. The behavioral correlation means models that &amp;quot;are warm&amp;quot; also tend to &amp;quot;be empathetic&amp;quot; -- it's the model's behavior that's bundled, not the measurement axes. Think of it like height and weight in humans: correlated in practice, but measuring different things.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Style, not personality.&lt;/strong&gt; The axes measure &lt;strong&gt;consistent stylistic patterns&lt;/strong&gt; in outputs, not internal states or &amp;quot;consciousness.&amp;quot; Think &amp;quot;how the model tends to respond&amp;quot; rather than &amp;quot;what the model is.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chat template matters.&lt;/strong&gt; All values depend on the specific chat template and system prompt. Different templates → different baselines. This is by design.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Relative, not absolute.&lt;/strong&gt; Cross-model comparisons are &lt;strong&gt;rankings&lt;/strong&gt;, not absolute measurements. &amp;quot;DeepSeek is warmer than Mistral&amp;quot; is valid. &amp;quot;DeepSeek has warmth = 0.42&amp;quot; is meaningless out of context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Metaphors, not ontology.&lt;/strong&gt; &amp;quot;Personality,&amp;quot; &amp;quot;temperament,&amp;quot; &amp;quot;mood&amp;quot; are metaphors for behavioral patterns. Models don't have feelings. I use these terms for interpretability, not to make claims about machine consciousness.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Try It Yourself&lt;/h1&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/yunoshev/mood-axis"&gt;https://github.com/yunoshev/mood-axis&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;All calibration data is included — you can measure temperament without re-running calibration. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Repro Details&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;code&gt;Qwen/Qwen2.5-7B-Instruct&lt;/code&gt;, &lt;code&gt;mistralai/Mistral-7B-Instruct-v0.3&lt;/code&gt;, &lt;code&gt;deepseek-ai/deepseek-llm-7b-chat&lt;/code&gt;, &lt;code&gt;meta-llama/Llama-3.1-8B-Instruct&lt;/code&gt;, &lt;code&gt;01-ai/Yi-1.5-9B-Chat&lt;/code&gt;, &lt;code&gt;google/gemma-2-9b-it&lt;/code&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Template&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;HuggingFace default (&lt;code&gt;tokenizer.apply_chat_template()&lt;/code&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Decoding&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;temperature=0.7&lt;/code&gt;, &lt;code&gt;top_p=0.9&lt;/code&gt;, &lt;code&gt;max_new_tokens=200&lt;/code&gt; (calibration) / &lt;code&gt;384&lt;/code&gt; (baseline, drift)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Sampling&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1 sample per prompt, no fixed seed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Data points&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Baseline: avg over 30 prompts; Conflict: 20 scenarios × 12 turns&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-generated dataset&lt;/strong&gt;: All 310 questions were generated by Claude Opus 4.6 (Anthropic) and curated by the author — no crowdsourced or established psychometric instruments. English only&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No human-judgment validation&lt;/strong&gt;: Axis labels are operationally defined through contrastive instructions, validated via hidden-state separability — not human annotation. I measure consistent behavioral variation, not human-perceived personality&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Single chat template &amp;amp; decoding&lt;/strong&gt;: Default chat template per model, fixed decoding (temp 0.7, top-p 0.9). Different templates or sampling strategies could shift profiles. Prompt robustness test varies system prompt content but not template/decoding&lt;/li&gt; &lt;li&gt;7B-9B models tested (larger models not yet tested)&lt;/li&gt; &lt;li&gt;This measures behavioral tendencies, not &amp;quot;consciousness&amp;quot; or &amp;quot;feelings&amp;quot;&lt;/li&gt; &lt;li&gt;No fixed seed, 1 sample per prompt -- adds measurement noise; a separate 5-seed benchmark replication showed mean ICC 0.91–0.99 across models (all 42 pairs exceed 0.75)&lt;/li&gt; &lt;li&gt;Axes are behaviorally correlated -- effective dimensionality ranges from 1.3 to 3.7 across models&lt;/li&gt; &lt;li&gt;Response lengths vary substantially across models (mean 192–380 tokens); Gemma (145-200 tokens) shows length confounding on 2 axes&lt;/li&gt; &lt;li&gt;Only assistant-generated tokens enter hidden state aggregation -- prompt tokens (system, user, template markup) are excluded. This controls for prompt-content confounds&lt;/li&gt; &lt;li&gt;Dead zones show above-chance accuracy but low d' -- distinct from random noise (~50%) and healthy axes (d' &amp;gt; 3). Surface text quality in dead zones not systematically analyzed&lt;/li&gt; &lt;li&gt;4/7 axes highly stable (cosine &amp;gt; 0.7); &lt;code&gt;confident_cautious&lt;/code&gt; and &lt;code&gt;patient_irritated&lt;/code&gt; weaker (0.55-0.60)&lt;/li&gt; &lt;li&gt;DeepSeek 7B fundamentally unstable (mean cosine 0.53) due to high hidden state dimensionality&lt;/li&gt; &lt;li&gt;Production config chosen for robustness across models, not per-model optimality&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;I'm curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do these patterns hold for larger models (70B+)?&lt;/li&gt; &lt;li&gt;Can we use axis vectors for steering (adding warmth to generation)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Which models should I test next?&lt;/strong&gt; If you have suggestions for open-weight models, I can try running them.&lt;/p&gt; &lt;p&gt;Would love feedback from the community. What else would you want to measure?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S. I have a full paper version ready for arXiv (LaTeX, ~20 pages with methodology, ablations, and reproducibility details), but I need an endorsement for cs.LG (Machine Learning) to submit. If you're an endorsed arXiv author in cs.LG and&lt;/strong&gt; &lt;strong&gt;think this work is worth putting up, I'd really appreciate it — feel free to DM me.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;UPDATE: Tested Phi-4 and Qwen3-8B (including thinking mode)&lt;/p&gt; &lt;p&gt;Several people asked about newer models, so I ran the pipeline on two more: Phi-4 (Microsoft, 14B) and Qwen3-8B (Alibaba), including a bonus run with enable_thinking=True. Total cloud time: ~30 min on 2xH100 SXM (~$6). Pipeline: calibration + baseline + benchmark (no drift).&lt;/p&gt; &lt;p&gt;Phi-4: The &amp;quot;reluctant skeptic&amp;quot;&lt;/p&gt; &lt;p&gt;Phi-4 has the most extreme cautious/reluctant profile I've seen. Coldest instruct model in the set (warm_cold = -0.51), most cautious (confident_cautious = -0.85, polar opposite of DeepSeek at +0.97), most reluctant (proactive_reluctant = -0.93 vs DeepSeek +1.00). Almost zero verbosity signal (+0.01, dead zone). The &amp;quot;I'd rather not, but if I must...&amp;quot; model.&lt;/p&gt; &lt;p&gt;Qwen3-8B vs Qwen 2.5 7B: Generational shift&lt;/p&gt; &lt;p&gt;Same family, one generation apart. The fingerprint shifted substantially. Qwen3 flipped from cautious to confident (confident_cautious: -0.36 to +0.38, delta +0.74) and from formal to casual (formal_casual: +0.42 to -0.26, delta -0.67). Verbose increased (+0.36 to +0.58). Proactivity stayed identical (+0.47 vs +0.45). Went from &amp;quot;measured professional&amp;quot; to &amp;quot;casual expert.&amp;quot;&lt;/p&gt; &lt;p&gt;Thinking vs Non-thinking: &amp;quot;To think is to doubt&amp;quot;&lt;/p&gt; &lt;p&gt;Same weights, same calibration axes — only difference is enable_thinking=True. Thinking tokens are included in hidden state extraction. The biggest shift: thinking mode makes the model significantly less confident (confident_cautious: +0.38 to +0.12, delta = -0.26) and more formal (formal_casual: -0.26 to -0.38, delta = -0.12). Everything else stays stable (delta &amp;lt; 0.08).&lt;/p&gt; &lt;p&gt;Makes intuitive sense: thinking involves exploring alternatives, considering edge cases, expressing uncertainty — exactly what the confident/cautious axis measures. &amp;quot;To think is to doubt&amp;quot; — nice sanity check that hidden states capture something real.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w13d48zzkqig1.png?width=4540&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c76e91d2e7e551b95cac578e9803b7beb6b7f7c0"&gt;https://preview.redd.it/w13d48zzkqig1.png?width=4540&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c76e91d2e7e551b95cac578e9803b7beb6b7f7c0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yunoshev"&gt; /u/yunoshev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T14:20:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0w7st</id>
    <title>Qwen-Image-2.0 is out - 7B unified gen+edit model with native 2K and actual text rendering</title>
    <updated>2026-02-10T09:25:15+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen team just released Qwen-Image-2.0. Before anyone asks - no open weights yet, it's API-only on Alibaba Cloud (invite beta) and free demo on Qwen Chat. But given their track record with Qwen-Image v1 (weights dropped like a month after launch, Apache 2.0), I'd be surprised if this stays closed for long.&lt;/p&gt; &lt;p&gt;So what's the deal:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;7B model, down from 20B in v1, which is great news for local runners&lt;/li&gt; &lt;li&gt;Unified generation + editing in one pipeline, no need for separate models&lt;/li&gt; &lt;li&gt;Native 2K (2048×2048), realistic textures that actually look good&lt;/li&gt; &lt;li&gt;Text rendering from prompts up to 1K tokens. Infographics, posters, slides, even Chinese calligraphy. Probably the best text-in-image I've seen from an open lab&lt;/li&gt; &lt;li&gt;Multi-panel comic generation (4×6) with consistent characters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 7B size is the exciting part here. If/when weights drop, this should be very runnable on consumer hardware. V1 at 20B was already popular in ComfyUI, a 7B version doing more with less is exactly what local community needs.&lt;/p&gt; &lt;p&gt;Demo is up on Qwen Chat if you want to test before committing any hopium to weights release.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=qwen-image-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T09:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1lskx</id>
    <title>Benchmarking LLM Inference on RTX PRO 6000 SE / H100 / H200 / B200</title>
    <updated>2026-02-11T03:02:07+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"&gt; &lt;img alt="Benchmarking LLM Inference on RTX PRO 6000 SE / H100 / H200 / B200" src="https://b.thumbs.redditmedia.com/DOf_vWH7wAmJQxK0OLukjnzgwaKzSYUfxmW0an86aSU.jpg" title="Benchmarking LLM Inference on RTX PRO 6000 SE / H100 / H200 / B200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama community. I present an LLM inference throughput benchmark for RTX PRO 6000 SE vs H100, H200, and B200 GPUs, based on the vllm serve and vllm bench serve benchmarking tools, to understand the cost-efficiency of various datacenter GPU options. Pro 6000 is significantly cheaper and built on the latest Blackwell architecture, but it has slower GDDR memory and lacks NVLink compared to H100 / H200 / B200.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@koshmanova.n/benchmarking-llm-inference-on-nvidia-b200-h200-h100-and-rtx-pro-6000-66d08c5f0162"&gt;Full article on Medium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/blog/benchmarking-b200"&gt;Non-medium link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a follow-up to the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;previous benchmark&lt;/a&gt;, incorporating community and collaborator feedback.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Longer context&lt;/strong&gt;: &lt;strong&gt;8K input + 8K output&lt;/strong&gt; tokens (&lt;strong&gt;16K total&lt;/strong&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVIDIA B200&lt;/strong&gt;: testing the newest Blackwell datacenter GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert Parallelism&lt;/strong&gt;: investigating vLLM’s &lt;code&gt;--enable-expert-parallel&lt;/code&gt; for MoE models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Using the real GPU cost of ownership&lt;/strong&gt; rather than market pricing to estimate the token price. Market price is subject to supply/demand fluctuations.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The benchmark is optimized for throughput.&lt;/strong&gt; VLLM serves models. The model is split across multiple GPUs using the --tensor-parallel-size VLLM option, if needed. Multiple VLLM instances serve the model; an NGINX load balancer on top distributes requests across them, maximizing throughput (replica parallelism). For example, if only 4 GPUs are required to run the model on an 8-GPU machine, two VLLM instances are launched with --tensor-parallel-size=4, and an NGINX load balancer is used. If all eight GPUs are required, then a single VLLM instance with --tensor-parallel-size=8 is used.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;vllm bench serve&lt;/strong&gt; tool is used for benchmarking with random data and a sequence length of 1000. The number of concurrent requests is set to 64-256 to ensure the LLM's token-generation capacity is saturated.&lt;/p&gt; &lt;p&gt;Three models are benchmarked to better understand the effect of PCIe communication on the 8xPro6000 server vs. NVLink on the H100/H200/B200.&lt;/p&gt; &lt;p&gt;Here is the model selection and the logic behind it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GLM-4.5-Air-AWQ-4bit (fits 80GB).&lt;/strong&gt; Testing single-GPU performance and maximum throughput with replica scaling on 8 GPU setups. No PCIE bottleneck.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct-AWQ (fits 320GB).&lt;/strong&gt; This 4-bit-quantized model fits into 4 GPUs. Some PCIe communication overhead in Pro 6000 setups may reduce performance relative to NVLink-enabled datacenter GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.6-FP8 (fits 640GB).&lt;/strong&gt; This model requires all eight GPUs. PCIe communication overhead expected. The H100 and H200 configurations should have an advantage.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Besides raw throughput, graphs show the serving cost per million tokens for each model on its respective hardware. The rental price is set at $0.93 for Pro6000, $1.91 for H100, $2.06 for H200, and $2.68 for B200.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;B200 wins on throughput&lt;/strong&gt;, with the largest gap on the most communication-heavy workload – &lt;strong&gt;GLM-4.6-FP8 (8-way TP):&lt;/strong&gt; B200 is &lt;strong&gt;4.87x&lt;/strong&gt; faster than PRO 6000 (8,036.71 vs 1,651.67 tok/s) – &lt;strong&gt;Qwen3-Coder-480B (4-way TP):&lt;/strong&gt; B200 is &lt;strong&gt;4.02x&lt;/strong&gt; faster than PRO 6000 (6,438.43 vs 1,602.96 tok/s) – &lt;strong&gt;GLM-4.5-Air (single-GPU replicas):&lt;/strong&gt; B200 is &lt;strong&gt;4.22x&lt;/strong&gt; faster than PRO 6000 (9,675.24 vs 2,290.69 tok/s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;B200 is also the cost efficiency leader&lt;/strong&gt; under updated run-cost estimates. B200’s throughput advantage more than compensates for its higher hourly cost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PRO 6000 is an attractive low-capex option.&lt;/strong&gt; It beats H100 on cost per across all models and is on par with H200 on GLM-4.5-Air.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;H200 is a major step up over H100.&lt;/strong&gt; H200 delivers &lt;strong&gt;~1.83x to 2.14x&lt;/strong&gt; H100 throughput across the three models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;H100 looked worse than expected&lt;/strong&gt; in this specific setup. It’s on par with PRO 6000 in throughput on GLM-4.5-Air and behind all other contenders in cost per token across all workloads.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://i.redd.it/rqm8d7yf6sig1.gif"&gt;https://i.redd.it/rqm8d7yf6sig1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/azhpz6qk6sig1.gif"&gt;https://i.redd.it/azhpz6qk6sig1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/9hbgr6ql6sig1.gif"&gt;https://i.redd.it/9hbgr6ql6sig1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code and Resources&lt;/h1&gt; &lt;p&gt;The code is available &lt;a href="https://github.com/cloudrift-ai/server-benchmark/tree/main/results/pro6000_h100_h200_b200_01_2026"&gt;here&lt;/a&gt;. Instructions for performing your own benchmark are in the README.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T03:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1k5gn</id>
    <title>PSA on llama.cpp —spec-type ngram-mod (use LF not CRLF, 35x speedup)</title>
    <updated>2026-02-11T01:48:39+00:00</updated>
    <author>
      <name>/u/dnsod_si666</name>
      <uri>https://old.reddit.com/user/dnsod_si666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; if using llama-server with —spec-type ngram-mod, and pasting/uploading/sending text files, make sure the files use LF instead of CRLF.&lt;/p&gt; &lt;p&gt;When I would copy a file from vscode and paste into the native llama-server webui with ngram speculative decoding enabled, there was no speed boost for file editing responses. I would only get a speed boost on the models second response (if I asked it to make a minor change to its first response file). Even if I asked the model to repeat the pasted file verbatim it would still be slow.&lt;/p&gt; &lt;p&gt;My files (I’m using a Windows computer) used CRLF (each line ends with “\r\n”) instead of LF (each line ends with “\n”). Models tend to use LF. So most of the ngrams created from my pasted file were useless because of the “\r\n”.&lt;/p&gt; &lt;p&gt;To fix in vscode press the LF/CRLF at the bottom of the screen and select. Or ctrl+shift+p &amp;gt; Change End of Line Sequence. This will change the currently open file.&lt;/p&gt; &lt;p&gt;To make all new files in vscode use LF, make a .vscode/settings.json with&lt;/p&gt; &lt;p&gt;{“files.eol”: “\n”}&lt;/p&gt; &lt;p&gt;To prevent git from automatically converting LF to CRLF run&lt;/p&gt; &lt;p&gt;git config —global core.autocrlf input&lt;/p&gt; &lt;p&gt;To convert existing files use `dos2unix` on wsl or sed or whatever string replace “\r\n” -&amp;gt; “\n”.&lt;/p&gt; &lt;p&gt;Exact command I am running for llama-server: `llama-server -m Devstral-2-123B-Instruct-2512-UD-Q5_K_XL-00001-of-00002.gguf —no-mmap —temp 0.15 —port 55553 —metrics —min-p 0.01 -c 32768 —spec-type ngram-mod —spec-ngram-size-n 24 —draft-min 32 —draft-max 48`&lt;/p&gt; &lt;p&gt;llama.cpp build: 7992 (612db6188) with GNU 13.3.0 for Linux aarch64&lt;/p&gt; &lt;p&gt;Not super helpful cause I’m not providing exact prompts/sampling params or anything, and also the speedup is well documented in the pull (&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19164"&gt;https://github.com/ggml-org/llama.cpp/pull/19164&lt;/a&gt;), but response tok/s went from ~2.3 to ~80 inside the code block.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnsod_si666"&gt; /u/dnsod_si666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1k5gn/psa_on_llamacpp_spectype_ngrammod_use_lf_not_crlf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1k5gn/psa_on_llamacpp_spectype_ngrammod_use_lf_not_crlf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1k5gn/psa_on_llamacpp_spectype_ngrammod_use_lf_not_crlf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T01:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r13m42</id>
    <title>Kimi is so smart</title>
    <updated>2026-02-10T15:22:13+00:00</updated>
    <author>
      <name>/u/Bernice_working_girl</name>
      <uri>https://old.reddit.com/user/Bernice_working_girl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"&gt; &lt;img alt="Kimi is so smart" src="https://preview.redd.it/nlgh125vpoig1.png?width=140&amp;amp;height=120&amp;amp;auto=webp&amp;amp;s=6ea4146398fa55af56e06624284c4c09061bd7b9" title="Kimi is so smart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nlgh125vpoig1.png?width=1726&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=886a17278e2ccf5692ac0a5ec0d8e4474334900d"&gt;https://preview.redd.it/nlgh125vpoig1.png?width=1726&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=886a17278e2ccf5692ac0a5ec0d8e4474334900d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yv3bxtsvpoig1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b67a5991c5ff32dd3e72eb6717eb617168dcaac9"&gt;https://preview.redd.it/yv3bxtsvpoig1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b67a5991c5ff32dd3e72eb6717eb617168dcaac9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mk02u5fwpoig1.png?width=1578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9d858ecc90244f657a58a1b202c3bccb7267260"&gt;https://preview.redd.it/mk02u5fwpoig1.png?width=1578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9d858ecc90244f657a58a1b202c3bccb7267260&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi &amp;gt; ChatGPT = Claude&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bernice_working_girl"&gt; /u/Bernice_working_girl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T15:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1oan9</id>
    <title>EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages</title>
    <updated>2026-02-11T05:02:36+00:00</updated>
    <author>
      <name>/u/Cod3Conjurer</name>
      <uri>https://old.reddit.com/user/Cod3Conjurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?&lt;/p&gt; &lt;p&gt;Took the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) – 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.&lt;/p&gt; &lt;p&gt;What I built:&lt;/p&gt; &lt;p&gt;- Full RAG pipeline with optimized data processing&lt;/p&gt; &lt;p&gt;- Processed 2M+ pages (cleaning, chunking, vectorization)&lt;/p&gt; &lt;p&gt;- Semantic search &amp;amp; Q&amp;amp;A over massive dataset&lt;/p&gt; &lt;p&gt;- Constantly tweaking for better retrieval &amp;amp; performance&lt;/p&gt; &lt;p&gt;- Python, MIT Licensed, open source&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;It’s trending, real-world data at scale, the perfect playground.&lt;/p&gt; &lt;p&gt;When you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/AnkitNayak-eth/EpsteinFiles-RAG"&gt;https://github.com/AnkitNayak-eth/EpsteinFiles-RAG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open to ideas, optimizations, and technical discussions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cod3Conjurer"&gt; /u/Cod3Conjurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T05:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1ixx4</id>
    <title>i finetuned qwen 14b on my discord messages so it can autocomplete for me</title>
    <updated>2026-02-11T00:54:40+00:00</updated>
    <author>
      <name>/u/B44ken</name>
      <uri>https://old.reddit.com/user/B44ken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1ixx4/i_finetuned_qwen_14b_on_my_discord_messages_so_it/"&gt; &lt;img alt="i finetuned qwen 14b on my discord messages so it can autocomplete for me" src="https://external-preview.redd.it/cXh4bTUxNG9qcmlnMRM_V8ZFFQqRSvchLLHbQ0c5NybEjNwn4ZOI2DTNmagB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d33c2f2c3025e6211e089f2944cd13d2929b194" title="i finetuned qwen 14b on my discord messages so it can autocomplete for me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i finetuned qwen on my discord messages so it can autocomplete for me while i type. tab to suggest, shift+tab to accept. kinda like copilot!&lt;/p&gt; &lt;p&gt;the dataset is ~250 conversations from my discord via a &lt;a href="https://github.com/Tyrrrz/DiscordChatExporter"&gt;scraping tool&lt;/a&gt;. a script formats these as chat-ml training samples. it groups messages by conversation (defined as after 1hr of silence), ensures i said something last, and throws out anything with code blocks (not the point of my autocomplete) or links (the model doesn't read those).&lt;/p&gt; &lt;p&gt;the model is qwen3-14b, finetuned with &lt;a href="http://unsloth.ai"&gt;unsloth.ai&lt;/a&gt; + QLoRA on a kaggle gpu. training takes ~15 mins since the dataset is small, but it picks up on how i talk pretty well! it's merged into a `.gguf` to be used as a local &lt;a href="http://ollama.com"&gt;ollama.com&lt;/a&gt; model.&lt;/p&gt; &lt;p&gt;the frontend is a chrome extension. when you press tab, it scrapes the last few messages and what you've started typing from the page, then builds a chat-ml prompt with context and streams a completion from ollama. the suggestion appears in the textbox &lt;em&gt;(fun hack: a zero-width unicode character marks where the suggestion begins)&lt;/em&gt; and shift+tab accepts it.&lt;/p&gt; &lt;p&gt;right now it works on discord, but i'd like it to support any site. other than that, future work could be trying different model sizes. 14b just about uses all the memory i can spare, but i hear 4b or 8b works ok too? i also need more data (maybe from other apps)... 250 samples captures my tone but not much else&lt;/p&gt; &lt;p&gt;it's at &lt;a href="https://github.com/b44ken/finetune"&gt;github.com/b44ken/finetune&lt;/a&gt; if you want to check out the code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/B44ken"&gt; /u/B44ken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/128ehu3ojrig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1ixx4/i_finetuned_qwen_14b_on_my_discord_messages_so_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1ixx4/i_finetuned_qwen_14b_on_my_discord_messages_so_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T00:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r14h9u</id>
    <title>Train MoE models 12x faster with 30% less memory! (&lt;15GB VRAM)</title>
    <updated>2026-02-10T15:54:02+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/"&gt; &lt;img alt="Train MoE models 12x faster with 30% less memory! (&amp;lt;15GB VRAM)" src="https://preview.redd.it/ee2jwnijvoig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27a55cfca0584307d3ba9f2e9cdf3226d1c55646" title="Train MoE models 12x faster with 30% less memory! (&amp;lt;15GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We’re excited to introduce ~12x faster Mixture of Experts (MoE) training with &lt;strong&gt;&amp;gt;35% less VRAM&lt;/strong&gt; and &lt;strong&gt;~6x longer context&lt;/strong&gt; via our new custom Triton kernels and math optimizations (no accuracy loss). Unsloth repo: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unsloth now supports fast training for MoE architectures including gpt-oss, Qwen3 (30B, 235B, VL, Coder), DeepSeek R1/V3 and GLM (4.5-Air, 4.7, Flash).&lt;/li&gt; &lt;li&gt;gpt-oss-20b fine-tunes in &lt;strong&gt;12.8GB VRAM&lt;/strong&gt;. Qwen3-30B-A3B (16-bit LoRA) uses 63GB.&lt;/li&gt; &lt;li&gt;Our kernels work on both data-center (B200, H100), &lt;strong&gt;consumer&lt;/strong&gt; and older GPUs (e.g., RTX 3090), and FFT, LoRA and QLoRA.&lt;/li&gt; &lt;li&gt;The larger the model and more context you use, &lt;strong&gt;the more pronounced the memory savings from our Unsloth kernels will be&lt;/strong&gt; (efficiency will scale exponentially).&lt;/li&gt; &lt;li&gt;We previously introduced Unsloth Flex Attention for gpt-oss, and these optimizations should make it even more efficient.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In collaboration with Hugging Face, we made all MoE training runs standardized with PyTorch’s new &lt;code&gt;torch._grouped_mm&lt;/code&gt; function. Transformers v5 was recently optimized with ~6x faster MoE than v4 and Unsloth pushes this even further with custom Triton grouped‑GEMM + LoRA kernels for an &lt;strong&gt;additional&lt;/strong&gt; ~2x speedup, &amp;gt;35% VRAM reduction and &amp;gt;6x longer context (12-30x overall speedup vs v4).&lt;/p&gt; &lt;p&gt;You can read our educational blogpost for detailed analysis, benchmarks and more: &lt;a href="https://unsloth.ai/docs/new/faster-moe"&gt;https://unsloth.ai/docs/new/faster-moe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also released support for embedding model fine-tuning recently. You can use our free MoE fine-tuning notebooks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;&lt;strong&gt;gpt-oss (20b)&lt;/strong&gt;&lt;/a&gt;-Fine-tuning.ipynb) &lt;strong&gt;(free)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_(20B"&gt;gpt-oss (500K context)&lt;/a&gt;_500K_Context_Fine_tuning.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GLM_Flash_A100(80GB"&gt;GLM-4.7-Flash&lt;/a&gt;.ipynb) (A100)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(120B"&gt;gpt-oss-120b&lt;/a&gt;_A100-Fine-tuning.ipynb) (A100)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_MoE.ipynb"&gt;Qwen3-30B-A3B&lt;/a&gt; (A100)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/TinyQwen3_MoE.ipynb"&gt;TinyQwen3 MoE T4&lt;/a&gt; (free)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;To update Unsloth to auto make training faster, update our Docker or:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for reading and hope y'all have a lovely week. We hear it'll be a busy week! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ee2jwnijvoig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T15:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1czgk</id>
    <title>MCP support in llama.cpp is ready for testing</title>
    <updated>2026-02-10T20:58:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1czgk/mcp_support_in_llamacpp_is_ready_for_testing/"&gt; &lt;img alt="MCP support in llama.cpp is ready for testing" src="https://preview.redd.it/yyar9f4hdqig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd88bd56b8854e280558af9bb751767d3a459271" title="MCP support in llama.cpp is ready for testing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;over 1 month of development (plus more in the previous PR) by &lt;a href="https://github.com/allozaur"&gt;&lt;strong&gt;allozaur&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;list of new features is pretty impressive:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding System Message to conversation or injecting it to an existing one&lt;/li&gt; &lt;li&gt;CORS Proxy on llama-server backend side&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Servers Selector&lt;/li&gt; &lt;li&gt;Settings with Server cards showing capabilities, instructions and other information&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Calls&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Agentic Loop&lt;/li&gt; &lt;li&gt;Logic&lt;/li&gt; &lt;li&gt;UI with processing stats&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompts&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Detection logic in „Add” dropdown&lt;/li&gt; &lt;li&gt;Prompt Picker&lt;/li&gt; &lt;li&gt;Prompt Args Form&lt;/li&gt; &lt;li&gt;Prompt Attachments in Chat Form and Chat Messages&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Browser with search &amp;amp; filetree view&lt;/li&gt; &lt;li&gt;Resource Attachments &amp;amp; Preview dialog&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Show raw output switch under the assistant message&lt;/li&gt; &lt;li&gt;Favicon utility&lt;/li&gt; &lt;li&gt;Key-Value form component (used for MCP Server headers in add new/edit mode)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assume this is a work in progress, guys, so proceed only if you know what you’re doing:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18655"&gt;https://github.com/ggml-org/llama.cpp/pull/18655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yyar9f4hdqig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1czgk/mcp_support_in_llamacpp_is_ready_for_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1czgk/mcp_support_in_llamacpp_is_ready_for_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T20:58:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0zn8o</id>
    <title>Hugging Face Is Teasing Something Anthropic Related</title>
    <updated>2026-02-10T12:39:52+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"&gt; &lt;img alt="Hugging Face Is Teasing Something Anthropic Related" src="https://preview.redd.it/wvu2vi2jwnig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4cce9563368df078883c6be531f8a7902f42c5e2" title="Hugging Face Is Teasing Something Anthropic Related" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic are the guys that make the Claude Models.&lt;/p&gt; &lt;p&gt;I highly doubt this will be an Openweights LLM release. More likely it will be a dataset for safety alignment. Anthropic is probably the organization most opposed to the open source community, so it's probably going to be a dataset. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wvu2vi2jwnig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T12:39:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
