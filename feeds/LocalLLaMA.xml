<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-28T11:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oi5wnc</id>
    <title>Looking to split my AI workload and was discussing with AI and came up with this, what are your thoughts.</title>
    <updated>2025-10-28T10:39:41+00:00</updated>
    <author>
      <name>/u/PsychologicalWeird</name>
      <uri>https://old.reddit.com/user/PsychologicalWeird</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies in advance if this is the wrong sub... &lt;/p&gt; &lt;p&gt;Now I already have a decent AI Rig, Ryzen 9 9900X, 96GB RAM, RTX 5090 FE.... &lt;/p&gt; &lt;p&gt;What I want to do seems like it may just have this rig running flat out most of the time and thats not what I want as I would like to also use it for Dev work, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I want to do:&lt;/strong&gt;&lt;br /&gt; Im creating a data model/schema, which I can do manually but will take months if not years by myself, so wanted to see if I can create a team to go through some of the laborious work, for example 4500 fields result in a complete universe of 179,500 possible end states according to the data dictionary I built.&lt;/p&gt; &lt;p&gt;Now I want to cut this down to a core generic structure that is fit for purpose (not the whole universe, just a sub set) and would like to do this using AI. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;So Im looking at:&lt;/strong&gt;&lt;br /&gt; AI Research &amp;amp; Analysis (AI/Me)&lt;br /&gt; Workflow Orchestration (n8n)&lt;br /&gt; Code Generation (Claude Code + Cursor)&lt;br /&gt; Data Storage (Apache Doris) &lt;/p&gt; &lt;p&gt;So AI suggests I could split the load: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;SFFPC (Ryzen 9 9900X + RTX 5090 FE)&lt;/strong&gt; = &lt;em&gt;frontend / interactive / orchestrator&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Threadripper Pro 3000 series workstation&lt;/strong&gt; = &lt;em&gt;backend / AI / data / mapping node&lt;/em&gt; &lt;/p&gt; &lt;p&gt;I have the chance to get a Threadripper pro 3000, 128GB RAM, etc with a RTX 3090 for ¬£1000-1200, now my idea would be to strip out the RTX 3090 and sell it, then replace with RTX A4000 (16GB Ampere) and I have a spare RTX A2000 (12GB) on the shelf.&lt;/p&gt; &lt;p&gt;The AI seems to suggest I can split the work load and anything needing the larger VRAM I can place on the SFFPC, anything that I want to run 24/7 I can dump on the Threadripper and it will sip power at (280W + 140W + 70W) the reason I would go A4000 is that its slightly bigger VRAM id needed instead of 3x RTX A2000 12GB.&lt;/p&gt; &lt;p&gt;So I can have it as a &lt;strong&gt;‚Äúdata-science staging server‚Äù&lt;/strong&gt; where you run heavy ETL / schema-mapping / AI-surveillance jobs overnight, or a Create a small-scale &lt;strong&gt;‚ÄúAI micro-cloud‚Äù,&lt;/strong&gt; like a zero-latency personal compute mesh that I can choose the task it does.&lt;/p&gt; &lt;p&gt;Does this sound feasible? before I go and buy the Threadripper workstation (I may do anyway to strip), but just wanting to make sure my thoughts I have discussed and AI has yes its possible is not just AI hallucinating and being the &amp;quot;yes&amp;quot; bot to my queries.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologicalWeird"&gt; /u/PsychologicalWeird &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5wnc/looking_to_split_my_ai_workload_and_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5wnc/looking_to_split_my_ai_workload_and_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5wnc/looking_to_split_my_ai_workload_and_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:39:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi5y5y</id>
    <title>reduce cost on livekit voice agent by using free models on livekit</title>
    <updated>2025-10-28T10:42:03+00:00</updated>
    <author>
      <name>/u/Ruru_mimi</name>
      <uri>https://old.reddit.com/user/Ruru_mimi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;currently, livekit only supports proprietary models for stt, llm and tts. i want to use whisper for stt which will not only reduce the cost but i can use it locally for faster calls. the problem lies in the fact that whisper can not work in realtime. I plan to tackle that problem by creating a function which records and sends stt data in chunks whenever Voice activity is detected (this livekit handles automatically using silerio VAD and turn detection).&lt;br /&gt; I also want to replace openai llm for text generation with either LLama through groq api endpoint or Ollama, currently livekit supports neither. is there a workaround ?&lt;br /&gt; i currently have no idea what can be done for TTS and if needed i plan on staying on the paid version if it provides better quality than any free service.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ruru_mimi"&gt; /u/Ruru_mimi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5y5y/reduce_cost_on_livekit_voice_agent_by_using_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5y5y/reduce_cost_on_livekit_voice_agent_by_using_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5y5y/reduce_cost_on_livekit_voice_agent_by_using_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:42:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohwjmq</id>
    <title>DeepSeek-OCR question for my workflow below...</title>
    <updated>2025-10-28T01:20:43+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohwjmq/deepseekocr_question_for_my_workflow_below/"&gt; &lt;img alt="DeepSeek-OCR question for my workflow below..." src="https://preview.redd.it/2ghkk6328rxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7c9a433a11837865ed74195eb17db4ef389640e" title="DeepSeek-OCR question for my workflow below..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please take a look at these questions after reviewing my workflow above:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Could I compress multiple PNGs, combine them into one image, and then process them as one image for text extraction? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would this model run on my Mac Mini 2024 M4 Base model? And would it be faster than Azure deployments strategy.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would the model be as precise as GPT-4o's Vision? 4o is very good at this extraction job.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any feedback is greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2ghkk6328rxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohwjmq/deepseekocr_question_for_my_workflow_below/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohwjmq/deepseekocr_question_for_my_workflow_below/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T01:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohy203</id>
    <title>Open Source Enterprise Search Platform (Generative-AI Powered)</title>
    <updated>2025-10-28T02:32:14+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm excited to share something we‚Äôve been building for the past few months - &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;&lt;strong&gt;PipesHub&lt;/strong&gt;&lt;/a&gt;, a &lt;strong&gt;fully open-source Enterprise Search Platform&lt;/strong&gt; designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Choose from 1,000+ embedding models&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Features releasing early next month&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run full platform locally. Recently, one of the platform user used Qwen-3-VL model - cpatonn/Qwen3-VL-8B-Instruct-AWQ-4bit (&lt;a href="https://huggingface.co/cpatonn/Qwen3-VL-8B-Instruct-AWQ-8bit"&gt;https://huggingface.co/cpatonn/Qwen3-VL-8B-Instruct-AWQ-8bit&lt;/a&gt; ) with vllm + kvcached. &lt;/p&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohy203/open_source_enterprise_search_platform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohy203/open_source_enterprise_search_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohy203/open_source_enterprise_search_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T02:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi350f</id>
    <title>3090 for approx $600 still a good investment in 2025? Or are there better value alternatives?</title>
    <updated>2025-10-28T07:34:50+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to find a ‚Äúgood value‚Äù GPU or setup for running LLMs locally (mainly for coding and research projects) and for ComfyUI work.&lt;/p&gt; &lt;p&gt;I don‚Äôt have a strict budget in mind, but I do have a desktop with a 3060 and 128 GB of RAM. I‚Äôm thinking I should probably ‚Äúmax it out‚Äù before considering a completely new build.&lt;/p&gt; &lt;p&gt;I‚Äôve been using the 3060 quite a bit, but it‚Äôs hard not to notice how much smarter the 20‚Äì32B models are compared to the 8‚Äì16B ones I can currently run.&lt;/p&gt; &lt;p&gt;I‚Äôm a bit wary of dual-GPU setups since I‚Äôm more comfortable on Windows, but it seems like the dual 3090 configuration (for 48 GB VRAM under Linux) is still often recommended as the best value.&lt;/p&gt; &lt;p&gt;Does that still hold true as of late 2025?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi350f/3090_for_approx_600_still_a_good_investment_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi350f/3090_for_approx_600_still_a_good_investment_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi350f/3090_for_approx_600_still_a_good_investment_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqulc</id>
    <title>Looking for a local llm thats good with warhammer 40k lore, Preferably below 10B</title>
    <updated>2025-10-27T21:17:41+00:00</updated>
    <author>
      <name>/u/Hakukh123</name>
      <uri>https://old.reddit.com/user/Hakukh123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;So i work in places with spotty/no internet pretty often and im new to &lt;strong&gt;40k lore&lt;/strong&gt;. been trying to find a decent local llm that knows its stuff about &lt;strong&gt;warhammer lore&lt;/strong&gt; so i can ask questions, brainstorm some stuff, or just chat about the setting when im bored.&lt;/p&gt; &lt;p&gt;ive tried a few models through lm studio but they seem pretty hit or miss with the lore - like they know the basic stuff (emperor, chaos, space marines) but when you get into specifics they start making things up or mixing factions.&lt;/p&gt; &lt;p&gt;wondering if anyone here has found a model that actually handles specialized lore well? or if anyone has fine-tuned something for 40k specifically? not looking for anything crazy powerful, just something that can run offline and actually knows the difference between a custodes and a primaris lol.&lt;/p&gt; &lt;p&gt;my setup can handle up to maybe 8b comfortably, could push 10b if its really worth it&lt;/p&gt; &lt;p&gt;any recommendations appreciated, thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hakukh123"&gt; /u/Hakukh123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3w68</id>
    <title>Flex Attention vs Flash Attention 3</title>
    <updated>2025-10-28T08:27:54+00:00</updated>
    <author>
      <name>/u/Extra-Designer9333</name>
      <uri>https://old.reddit.com/user/Extra-Designer9333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm pretty new to accelerated framework APIs like FlexAttn from PyTorch team and FlashAttn from Tri Dao out of Princeton. Unsloth itself uses Flex Attn as I know and reports: &amp;quot;10x faster on a single GPU and up to 30x faster on multiple GPU systems compared to Flash Attention 2 (FA2).&amp;quot; However, FlashAttn 3 turns out to be 1.5-2x faster than FlashAttn 2. &lt;/p&gt; &lt;p&gt;I'm trying to decide which one to use for training my LLM whether it's FlexAttn (Unsloth) or FlashAttn 3. What's your personal suggestion and experience you had from these 2. Which one is more error prone, which turns out to be more memory heavy or computationally less expensive and etc.&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra-Designer9333"&gt; /u/Extra-Designer9333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T08:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohbcu1</id>
    <title>Experience with the new model MiniMax M2 and some cost saving tips</title>
    <updated>2025-10-27T11:00:41+00:00</updated>
    <author>
      <name>/u/thalacque</name>
      <uri>https://old.reddit.com/user/thalacque</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt; &lt;img alt="Experience with the new model MiniMax M2 and some cost saving tips" src="https://b.thumbs.redditmedia.com/UVUhaaqNSDCk6qFXVB2lhPVQVQLnJtZQw5XfM0IrY1I.jpg" title="Experience with the new model MiniMax M2 and some cost saving tips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the discussion about MiniMax M2 in the group chat a couple of days ago, and since their API and agent are free to use, I thought I‚Äôd test it out. First, the conclusion: in my own use, M2 delivers better than expected efficiency and stability. You can feel the team has pushed the model‚Äôs strengths close to top closed models. In some scenarios it reaches top results at clearly lower cost, so it fits as the default executor, with closed models kept for final polish when needed.&lt;/p&gt; &lt;p&gt;My comparison across models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A three service monorepo dependency and lock file mess (Node.js + Express). The three services used different versions of jsonwebtoken and had lock file conflicts. The goal was to unify versions, upgrade jwt.verify from callback to Promise, and add an npm run bootstrap script for one click dependency setup and alignment.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: breaks down todos, understands the task well, reads files first, lists a plan, then edits step by step. It detects three version drifts and proposes an alignment strategy, adds the bootstrap script, runs one round of install and startup checks. Small fixes are quick, friendly to regression runs, and it feels ready to drop into a pipeline for repeated runs. Claude: strong first pass, but cross service consistency sometimes needed repeated reminders, took more rounds, and usage cost was higher. GLM/Kimi: can get the main path working, but more likely to leave rough edges in lock files and scripts that I had to clean up.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;An online 3x3 Rubik‚Äôs Cube (a small front end interaction project): rotate a layer to a target angle, buttons to choose a face, show the 3x3 color grid.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: To be honest, the first iteration wasn‚Äôt great, major issues like text occlusion and non-functional rotation weren‚Äôt addressed. The bright spot is that interaction bugs (e.g., rotation state desynchronization) could be fixed in a single pass once pointed out, without introducing new regressions. After subsequent rounds of refinement, the final result actually became the most usable and presentable, fully supporting 3D dragging. GLM/Kimi: The first round results were decent, but both ran into problems in the second round. GLM didn‚Äôt resolve the Rubik‚Äôs Cube floating/hover position issue, and Kimi, after the second round feedback, ended up not being three-dimensional. Claude performed excellently after the first round of prompts, with all features working normally, but even after multiple later rounds it still didn‚Äôt demonstrate an understanding of a 3D cube (in the image, Claude‚Äôs Rubik‚Äôs Cube is flat and the view can‚Äôt be rotated).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Metrics echo this feel: SWE bench Verified 69.4, Terminal Bench 46.3, ArtifactsBench 66.8, BrowseComp 44.0, FinSearchComp global 65.5. It is not first in every category, but on the runnable and fixable engineering loop, the structure score looks better. From my use, the strengths are proposing a plan, checking its own work, and favoring short fast iterations that clear blockers one by one.&lt;/p&gt; &lt;p&gt;Replace most closed model usage without sacrificing the reliability of the engineering loop. M2 is already enough and surprisingly handy. Set it as the default executor and run regressions for two days; the difference will be clear. After putting it into the pipeline, with the same budget you can run more in parallel, and you do save money.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-M2"&gt;https://github.com/MiniMax-AI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thalacque"&gt; /u/thalacque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ohbcu1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T11:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohrn20</id>
    <title>Investigating Apple's new "Neural Accelerators" in each GPU core (A19 Pro vs M4 Pro vs M4 vs RTX 3080 - Local LLM Speed Test!)</title>
    <updated>2025-10-27T21:48:31+00:00</updated>
    <author>
      <name>/u/TechExpert2910</name>
      <uri>https://old.reddit.com/user/TechExpert2910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone :D&lt;/p&gt; &lt;p&gt;I thought it‚Äôd be really interesting to compare how Apple's new A19 Pro (and in turn, the M5) with its fancy &lt;strong&gt;new &amp;quot;neural accelerators&amp;quot; in each GPU core&lt;/strong&gt; compare to other GPUs!&lt;/p&gt; &lt;p&gt;I ran Gemma 3n 4B on each of these devices, outputting ~the same 100-word story (at a temp of 0). I used the most optimal inference framework for each to give each their best shot.&lt;/p&gt; &lt;p&gt;Here're the results!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Inference Set-Up&lt;/th&gt; &lt;th align="left"&gt;Tokens / Sec&lt;/th&gt; &lt;th align="left"&gt;Time to First Token&lt;/th&gt; &lt;th align="left"&gt;Perf / GPU Core&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;A19 Pro&lt;/td&gt; &lt;td align="left"&gt;6 GPU cores; iPhone 17 Pro Max&lt;/td&gt; &lt;td align="left"&gt;MLX? (‚ÄúLocal Chat‚Äù app)&lt;/td&gt; &lt;td align="left"&gt;23.5 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.4 s üëÄ&lt;/td&gt; &lt;td align="left"&gt;3.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4&lt;/td&gt; &lt;td align="left"&gt;10 GPU cores, iPad Pro 13‚Äù&lt;/td&gt; &lt;td align="left"&gt;MLX? (‚ÄúLocal Chat‚Äù app)&lt;/td&gt; &lt;td align="left"&gt;33.4 tok/s&lt;/td&gt; &lt;td align="left"&gt;1.1 s&lt;/td&gt; &lt;td align="left"&gt;3.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3080&lt;/td&gt; &lt;td align="left"&gt;10 GB VRAM; paired with a Ryzen 5 7600 + 32 GB DDR5&lt;/td&gt; &lt;td align="left"&gt;CUDA 12 llama.cpp (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;59.1 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.02 s&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4 Pro&lt;/td&gt; &lt;td align="left"&gt;16 GPU cores, MacBook Pro 14‚Äù, 48 GB unified memory&lt;/td&gt; &lt;td align="left"&gt;MLX (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;60.5 tok/s üëë&lt;/td&gt; &lt;td align="left"&gt;0.31 s&lt;/td&gt; &lt;td align="left"&gt;3.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Super Interesting Notes:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. The neural accelerators didn't make much of a difference. Here's why!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First off, they do indeed significantly accelerate compute! &lt;a href="https://tzakharko.github.io/apple-neural-accelerators-benchmark/#:%7E:text=Metal%20Shading%20Language.-,Key%20Takeaways%3A,-Operation"&gt;Taras Zakharko found that&lt;/a&gt; Matrix FP16 and Matrix INT8 are already accelerated by 4x and 7x respectively!!!&lt;/li&gt; &lt;li&gt;BUT, when the LLM spits out tokens, we're limited by memory bandwidth, NOT compute. This is especially true with Apple's iGPUs using the comparatively low-memory-bandwith system RAM as VRAM.&lt;/li&gt; &lt;li&gt;Still, there is one stage of inference that is compute-bound: prompt pre-processing! That's why we see the A19 Pro has ~3x faster Time to First Token vs the M4.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://www.macstories.net/linked/max-weinbach-on-the-m5s-neural-accelerators/"&gt;Max Weinbach's testing&lt;/a&gt; also corroborates what I found. And it's also worth noting that MLX hasn't been updated (yet) to take full advantage of the new neural accelerators!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. My M4 Pro as fast as my RTX 3080!!! It's crazy - 350 w vs 35 w&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you use an MLX model + MLX on Apple Silicon, you get some really remarkable performance. Note that the 3080 also had ~its best shot with CUDA optimized llama cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechExpert2910"&gt; /u/TechExpert2910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi522k</id>
    <title>Experiences with Aider vs. GitHub Copilot for Ryan Carson‚Äôs AI Dev Tasks?</title>
    <updated>2025-10-28T09:48:07+00:00</updated>
    <author>
      <name>/u/XiNXNATiON</name>
      <uri>https://old.reddit.com/user/XiNXNATiON</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been trying out Ryan Carson‚Äôs ai-dev-tasks workflow (&lt;a href="https://github.com/snarktank/ai-dev-tasks"&gt;https://github.com/snarktank/ai-dev-tasks&lt;/a&gt;), which is a neat way to structure AI-assisted feature development. The process breaks down into three steps: first creating a product requirement document (PRD), then generating a detailed task list, and finally implementing the tasks one at a time.&lt;/p&gt; &lt;p&gt;In my experience, this workflow works really well with GitHub Copilot. Copilot is pretty good at understanding the codebase and finding relevant files, which makes task generation accurate and useful.&lt;/p&gt; &lt;p&gt;With that in mind, I wanted to see if the same could be done with Aider. My test project was DBeaver (&lt;a href="https://github.com/dbeaver/dbeaver"&gt;https://github.com/dbeaver/dbeaver&lt;/a&gt;), which is mostly Java. Aider did okay when generating the PRD but struggled badly with generating tasks -- it often missed related files and once even imagined some TypeScript files that don‚Äôt exist in the project. I also tried running aider-ce with an MCP server called &lt;strong&gt;mcp-everything-search&lt;/strong&gt;, which provides fast file searching using the Everything Search engine. Even with this setup, the context building and file discovery aren‚Äôt nearly as strong as Copilot‚Äôs.&lt;/p&gt; &lt;p&gt;For both GitHub Copilot and Aider, I've used the GPT-4o model, so the difference in results doesn‚Äôt seem to come from the model itself but rather how each tool manages repo context and file lookup.&lt;/p&gt; &lt;p&gt;Has anyone had better luck using Aider for a multi-step workflow like this? Or have tips on improving how it indexes or uses the repo? Would appreciate any pointers or experiences you want to share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiNXNATiON"&gt; /u/XiNXNATiON &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi522k/experiences_with_aider_vs_github_copilot_for_ryan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi522k/experiences_with_aider_vs_github_copilot_for_ryan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi522k/experiences_with_aider_vs_github_copilot_for_ryan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T09:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohq5bc</id>
    <title>GLM-4.6 vs Minimax-M2</title>
    <updated>2025-10-27T20:50:13+00:00</updated>
    <author>
      <name>/u/baykarmehmet</name>
      <uri>https://old.reddit.com/user/baykarmehmet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I've been using the GLM Coding Plan and it works well&lt;/strong&gt; ‚Äî not quite Sonnet 4.5 performance, but with clear prompts it gets the job done.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;However, everyone's hyping Minimax M2&lt;/strong&gt;, claiming it crushes every benchmark. The problem? I haven't seen any real-world coding examples or projects using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Has anyone here actually used Minimax M2 for development work?&lt;/strong&gt; If so:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does it compare to other models in practice?&lt;/li&gt; &lt;li&gt;Is it worth switching to?&lt;/li&gt; &lt;li&gt;Any specific use cases where it excels or falls short?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear some hands-on experiences beyond the benchmark numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baykarmehmet"&gt; /u/baykarmehmet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohpqvy</id>
    <title>Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp; the Adobe Suite</title>
    <updated>2025-10-27T20:34:58+00:00</updated>
    <author>
      <name>/u/atape_1</name>
      <uri>https://old.reddit.com/user/atape_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt; &lt;img alt="Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" src="https://external-preview.redd.it/nujocFBKcx2R2jDqpB8QfazaXJG6_pPL0crrme1qkLM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=166ad90b700b767ba9a03c0fc586c47d3289ab0d" title="Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atape_1"&gt; /u/atape_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=efQPFhZmhAo&amp;amp;embeds_referring_euri=https%3A%2F%2Fwww.reddit.com%2F"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohlhdx</id>
    <title>Phoronix benchmarks single and dual AMD R9700 GPUs against a single NVIDIA RTX 6000 Ada GPU</title>
    <updated>2025-10-27T17:56:13+00:00</updated>
    <author>
      <name>/u/Brian-Puccio</name>
      <uri>https://old.reddit.com/user/Brian-Puccio</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brian-Puccio"&gt; /u/Brian-Puccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohzfdu</id>
    <title>VellumForge2 - A high performance, very configurable and really easy to use DPO dataset generation tool, create high quality datasets for completely free</title>
    <updated>2025-10-28T03:41:47+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally releasing my new dataset generation tool, and some Fantasy writing datasets to go with it (soon). &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemon07r/VellumForge2"&gt;https://github.com/lemon07r/VellumForge2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample Dataset: &lt;a href="https://huggingface.co/collections/lemon07r/vellumforge2-datasets"&gt;https://huggingface.co/collections/lemon07r/vellumforge2-datasets&lt;/a&gt; (large datasets coming soon)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Functionality&lt;/strong&gt; (all you need for a tl;dr)&lt;/p&gt; &lt;p&gt;This tool creates DPO-style datasets using a main topic and LLMs to generate subtopics, prompts, and chosen/rejected response pairs through a hierarchical pipeline. What sets it apart is the optional LLM-as-a-judge rubric scoring system, inspired by how Kimi K2 was trained using rubric-based evaluation to generate higher quality writing samples. The output uses a flexible &amp;quot;one-to-many&amp;quot; hybrid schema that works seamlessly with DPOTrainer, RewardTrainer, and MORL training, no data transformation needed. You can also skip the judge entirely for DPO training or just use the prompt and chosen responses for SFT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overview &amp;amp; Features&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My original python script that I was using for making datasets worked mostly fine, but I broke it, many many times trying to refactor it and add features to it. It did get to a good place at some point, with working async, rate limiting, etc, before I broke it again with some experimental stuff that turned out to not be a good idea even if it did work. Some good lessons learned here.&lt;/p&gt; &lt;p&gt;What I did learn, I used in my complete re-write of the tool. This time I wrote it in Go, and kept it very simple and easy to use. I also kept it very modular and highly configurable from the very start. This tool works with any OpenAI-compatible API including local servers like llama.cpp, kobold.cpp, LM studio, vLLM or Ollama. Handles rate limiting automatically, supports concurrent workers, and can upload directly to Hugging Face Hub in one command, which was implemented without needing any external tools/dependencies like the HF cli. Generation templates are fully customizable via TOML config, meaning you can make any type of dataset. The example configs come with a strong default template for fantasy writing to help give an idea of what a good template would look like. The documentation includes a thorough quick start guide, and examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset Generation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This thing works fast. Had a much bigger impact than I expected in dataset generation speed compared to the old tool. Even using the completely free (and unlimited) Nvidia NIM api with it's 40 RPM rate limit and slow 20-30 tps Kimi K2 0905 model, plus any small local model for rejected responses, you can create a very high quality (possibly only topped by using Sonnet 4.5) DPO datasets, with about 1000 rows of high quality data in under a few hours, for completely free. No expensive hardware or API provider required (which of course you can use with this tool too). The sample dataset I linked completed under these conditions in only a 36-minute run, which would have been only half as long without a judge. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T03:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohh1l2</id>
    <title>86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent.</title>
    <updated>2025-10-27T15:12:09+00:00</updated>
    <author>
      <name>/u/Ok-Attention1022</name>
      <uri>https://old.reddit.com/user/Ok-Attention1022</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt; &lt;img alt="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." src="https://external-preview.redd.it/Mhekv1CVcCnqQ6OsfNa_xd5RwOhyacefoOODajDng28.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=513369b6dc41c3172d89ddffae9bf50cb41bb901" title="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built SGR Deep Research: a lightweight framework for structured reasoning agents using small LLMs&lt;/p&gt; &lt;p&gt;No LangChain/CrewAI bloat&lt;/p&gt; &lt;p&gt;~500 LOC core logic&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;: 86.1% on SimpleQA (4,326 questions)&lt;/p&gt; &lt;p&gt;Model: gpt-4.1-mini&lt;br /&gt; Tavily Search: basic&lt;/p&gt; &lt;p&gt;Cost: $0.03 per query&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/akowocw57oxf1.png?width=2460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d332497cfe0686bedb5b11f58bbb7e6de61f0a3"&gt;Performance Metrics on gpt-4.1-mini and Tavily basic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SGR understanding&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bocirpd67oxf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c1adc29c14fc211311efbf31063e3d449ffcbd0"&gt;SGR Deep Research: open-source framework for building intelligent research agents using Schema-Guided Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explicitly control reasoning flow instead of hoping model figures it out ReAct&amp;amp;PlanAct-style but with structured steps Running in production at telecom and banking right now&lt;/p&gt; &lt;p&gt;Testing local models next (Qwen, Llama) for $0 API costs&lt;br /&gt; Everything public: logs, configs, code GitHub MIT: &lt;a href="https://github.com/vamplabAI/sgr-deep-research"&gt;https://github.com/vamplabAI/sgr-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Attention1022"&gt; /u/Ok-Attention1022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T15:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi2ftf</id>
    <title>Is Grokipedia available for fine-tuning?</title>
    <updated>2025-10-28T06:46:21+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With grokipedia now live, wondering what its licensing policy is for using articles for fine-tuning local models. Not sure if article snapshots are already (or will be ever available) publicly for free .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2ftf/is_grokipedia_available_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2ftf/is_grokipedia_available_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2ftf/is_grokipedia_available_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T06:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohihvo</id>
    <title>Another Banger from Inclusion AI: Ming-flash-omni-Preview</title>
    <updated>2025-10-27T16:06:14+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt; &lt;img alt="Another Banger from Inclusion AI: Ming-flash-omni-Preview" src="https://external-preview.redd.it/PFGMqHZG1FenJLDckxpcToXwao333pejl_fZNW4bWqk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ce1f6abd38ec8e123f358b2f5b41d3b28a30d6" title="Another Banger from Inclusion AI: Ming-flash-omni-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;https://huggingface.co/inclusionAI/Ming-flash-omni-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on &lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;Ling-Flash-2.0&lt;/a&gt; this model has 100b total parameters and 6b active ones and supports context aware asr, text to speech, image generation and editing, segmentation etc (well its an omni modal model so you know the drill). Since its fairly sparse it is very efficient and while I couldn't test it myself the benchmarks seem promising, and it also supports voice cloning (;&lt;/p&gt; &lt;p&gt;It says it can do dialect-aware ASR, though im not sure if that will only work with Chinese ü§î&lt;/p&gt; &lt;p&gt;Anyways, if im not mistaken this is the biggest open sourced omni modal model yet so thanks to the mad lads at inclusion ai!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556"&gt;https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohm80t</id>
    <title>Newegg has 32gb AMD r9700 for $1,300</title>
    <updated>2025-10-27T18:23:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu"&gt;https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Phoronix did a poor job of benchmarking it. Would prefer benchmarking a 30gb model like qwen3 coder, but instead focuses on 8gb model: &lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;https://www.phoronix.com/review/amd-radeon-ai-pro-r9700&lt;/a&gt; Doesn't bother to compare it to 4090/5090. This video does gaming benchmarks: &lt;a href="https://www.youtube.com/watch?v=x0YJ32Q0mNw"&gt;https://www.youtube.com/watch?v=x0YJ32Q0mNw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guessing 30 tokens per second (TPS) for qwen3 coder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi63n6</id>
    <title>OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI</title>
    <updated>2025-10-28T10:50:29+00:00</updated>
    <author>
      <name>/u/mythz</name>
      <uri>https://old.reddit.com/user/mythz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt; &lt;img alt="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" src="https://external-preview.redd.it/NbBv8AZ8_FKSnCg3gr7veZ2x9ORPuKnCYj3fZNiyQ4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa75645d12e8474d1f573ac3f747ada63b172124" title="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythz"&gt; /u/mythz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ServiceStack/llms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi2tky</id>
    <title>I built a small Python tool to track how your directories get messy (and clean again)</title>
    <updated>2025-10-28T07:12:21+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, much as we hate to admit, almost every project or downloads folder gets out of control over time (yep).&lt;/p&gt; &lt;p&gt;I got curious ‚Äî not just about which files change, but &lt;strong&gt;how the structure itself evolves.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;&lt;strong&gt;Directory Monitor&lt;/strong&gt;&lt;/a&gt; ‚Äî a lightweight Python script that keeps tabs on &lt;strong&gt;directory organization&lt;/strong&gt;, not just file edits. This tool uses local LLMs (Qwen, Llama, choose your own) to analyze project structure and give cleanup recommendations. Everything runs locally - no cloud APIs.&lt;/p&gt; &lt;p&gt;**The interesting technical bits:**&lt;/p&gt; &lt;p&gt;- Uses RAG with local sentence-transformers to compare current state against historical scans&lt;/p&gt; &lt;p&gt;- LLM analyzes trends and gives specific, actionable recommendations &lt;/p&gt; &lt;p&gt;- Terminal UI with Rich showing real-time metrics and sparklines&lt;/p&gt; &lt;p&gt;- All stored in SQLite locally&lt;/p&gt; &lt;p&gt;**Example output:**&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Messiness Score: 6.2/10&lt;/p&gt; &lt;p&gt;Top 3 Issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Too many files (28) in src/components - split into ui/, forms/, layouts/&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;8 files contain 'temp' - move to .archive/ or use proper version control&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Directory depth exceeds 7 levels - flatten structure&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Trend: üìâ Improving (was 7.8, now 6.2)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**Stack:**&lt;/p&gt; &lt;p&gt;- Ollama (Qwen/Llama) for LLM&lt;/p&gt; &lt;p&gt;- sentence-transformers for embeddings&lt;/p&gt; &lt;p&gt;- SQLite for history&lt;/p&gt; &lt;p&gt;- Python with Rich/Flask&lt;/p&gt; &lt;p&gt;Works completely offline after setup. Tested with Qwen3:8b and Llama3.2.&lt;/p&gt; &lt;p&gt;Would love feedback ‚Äî what features would &lt;em&gt;you&lt;/em&gt; add for keeping folders sane?&lt;/p&gt; &lt;p&gt;**GitHub:** &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;https://github.com/sukanto-m/directory-monitor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T07:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohvcwt</id>
    <title>Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?</title>
    <updated>2025-10-28T00:26:07+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt; &lt;img alt="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" src="https://a.thumbs.redditmedia.com/UBwX6pI157vt7gMYDChu_R8QUux04jne3QgjRhJMLr0.jpg" title="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hope you're fine.&lt;/p&gt; &lt;p&gt;Short question, I managed to find, working and testing on my PC right now, an A40 48GB. It is passively cooled and it gets quite hot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8az1kqsdyqxf1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=301fff8d7b8d78a3f33c97765bb96ebdeaa03e2d"&gt;Local testing on my PC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The seller (a friend) is asking me 1500USD for it. I'm not from USA, but a 3rd world country.&lt;/p&gt; &lt;p&gt;But I have read here on Local llama that such old cards and such aren't very worth it, also no FP8 support, etc.&lt;/p&gt; &lt;p&gt;So I'm really torn and indecisive about it. For reference, 5090 new goes for about 2700-3300USD (so 32GB, but fp8/fp4 support, like 4x times the bandwidth, etc). Used 4090s are 1600USD. 4090 48GB modded when importing they're about 4200-4400USD. 3090s are 550-600USD.&lt;/p&gt; &lt;p&gt;What would you guys do? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T00:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdl9q</id>
    <title>Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives</title>
    <updated>2025-10-27T12:53:12+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt; &lt;img alt="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" src="https://external-preview.redd.it/cDlpaWtncThobnhmMYyuPjWRTezxeRfqB3upVJ5ATISaueUIVVjdl6ikWaxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49803f057b43fcece9390b3b966fe6ba4de209b3" title="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chamath Palihapitiya said his team migrated a large number of workloads to Kimi K2 because it was significantly more performant and much cheaper than both OpenAI and Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/avwpphq8hnxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T12:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohyeee</id>
    <title>Minimax-M2 support added in MLX</title>
    <updated>2025-10-28T02:49:15+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt; &lt;img alt="Minimax-M2 support added in MLX" src="https://preview.redd.it/4yqqtqzynrxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85e601e94e902746685decb25bfc69d58f508850" title="Minimax-M2 support added in MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4yqqtqzynrxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T02:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oht9pw</id>
    <title>Z.ai release Glyph weight</title>
    <updated>2025-10-27T22:55:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt; &lt;img alt="Z.ai release Glyph weight" src="https://a.thumbs.redditmedia.com/69TaqDj6bS-Vs7O_YYHmJygT9-976J0J5KJcduszmX8.jpg" title="Z.ai release Glyph weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glyph: Scaling Context Windows via Visual-Text Compression&lt;/p&gt; &lt;p&gt;Paper: arxiv.org/abs/2510.17800&lt;/p&gt; &lt;p&gt;Weights: huggingface.co/zai-org/Glyph&lt;/p&gt; &lt;p&gt;Repo: github.com/thu-coai/Glyph&lt;/p&gt; &lt;p&gt;Glyph is a framework for scaling the context length through visual-text compression. It renders long textual sequences into images and processes them using vision‚Äìlanguage models.&lt;/p&gt; &lt;p&gt;This design transforms the challenge of long-context modeling into a multimodal problem, substantially reducing computational and memory costs while preserving semantic information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oht9pw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T22:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohtp6d</id>
    <title>Bad news: DGX Spark may have only half the performance claimed.</title>
    <updated>2025-10-27T23:13:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt; &lt;img alt="Bad news: DGX Spark may have only half the performance claimed." src="https://preview.redd.it/9b2ziei0lqxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9741ebb17cdabb88dde97eb430a1c2ff563565" title="Bad news: DGX Spark may have only half the performance claimed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There might be more bad news about the DGX Spark!&lt;/p&gt; &lt;p&gt;Before it was even released, I told everyone that this thing has a memory bandwidth problem. Although it boasts 1 PFLOPS of FP4 floating-point performance, its memory bandwidth is only 273GB/s. This will cause major stuttering when running large models (with performance being roughly only one-third of a MacStudio M2 Ultra).&lt;/p&gt; &lt;p&gt;Today, more bad news emerged: the floating-point performance doesn't even reach 1 PFLOPS.&lt;/p&gt; &lt;p&gt;Tests from two titans of the industry‚ÄîJohn Carmack (founder of id Software, developer of games like Doom, and a name every programmer should know from the legendary fast inverse square root algorithm) and Awni Hannun (the primary lead of Apple's large model framework, MLX)‚Äîhave shown that this device only achieves 480 TFLOPS of FP4 performance (approximately 60 TFLOPS BF16). That's less than half of the advertised performance.&lt;/p&gt; &lt;p&gt;Furthermore, if you run it for an extended period, it will overheat and restart.&lt;/p&gt; &lt;p&gt;It's currently unclear whether the problem is caused by the power supply, firmware, CUDA, or something else, or if the SoC is genuinely this underpowered. I hope Jensen Huang fixes this soon. The memory bandwidth issue could be excused as a calculated product segmentation decision from NVIDIA, a result of us having overly high expectations meeting his precise market strategy. However, performance not matching the advertised claims is a major integrity problem.&lt;/p&gt; &lt;p&gt;So, for all the folks who bought an NVIDIA DGX Spark, Gigabyte AI TOP Atom, or ASUS Ascent GX10, I recommend you all run some tests and see if you're indeed facing performance issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9b2ziei0lqxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM ‚Äì 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
