<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-30T14:50:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ojhdgi</id>
    <title>Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services.</title>
    <updated>2025-10-29T21:35:36+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"&gt; &lt;img alt="Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services." src="https://external-preview.redd.it/SdNcK7BdgRqMa8-G5nagIMUt2TjZgJXQeatIWMOCjqU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f967920e17edca37152de084ff4e38f8b6b72271" title="Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jabberjabberjabber/ImageIndexer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T21:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojrjqk</id>
    <title>nanochat pretraining time benchmarks ($100 run), share yours!</title>
    <updated>2025-10-30T05:34:38+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrjqk/nanochat_pretraining_time_benchmarks_100_run/"&gt; &lt;img alt="nanochat pretraining time benchmarks ($100 run), share yours!" src="https://preview.redd.it/27ik4oxar6yf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e04bec5d0f2df13d979dee5859a7f710c8956687" title="nanochat pretraining time benchmarks ($100 run), share yours!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of &lt;a href="https://github.com/karpathy/nanochat"&gt;nanochat by Andrej Karpathy&lt;/a&gt;, we have a nice pretraining benchmark for our hardware. Making this post to compile pretraining time numbers from different systems, please share your numbers! Make sure you use &lt;code&gt;--depth=20', configure the&lt;/code&gt;--device_batch_size' to the largest your machine can fit, and leave everything else at their defaults. You can also share approximate completion times based on how long it took to complete 10-20 steps (of 21,400 total steps).&lt;/p&gt; &lt;p&gt;Here is my command for single node: &lt;code&gt; python -m scripts.base_train --depth=20 --device_batch_size=32 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Hardware&lt;/th&gt; &lt;th&gt;Pretraining Time (Approx.)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;8 x H100 (Karpathy)&lt;/td&gt; &lt;td&gt;4 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;8 x A100 (&lt;a href="https://huggingface.co/spaces/nanochat-students/README/discussions/9"&gt;source&lt;/a&gt;)&lt;/td&gt; &lt;td&gt;7 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 x MI300x (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ojrjqk/comment/nm5fnmq/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;source&lt;/a&gt;)&lt;/td&gt; &lt;td&gt;16 hours (to be tested with a larger batch size)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 x H100&lt;/td&gt; &lt;td&gt;1 day&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 x RTX Pro 6000 (&lt;a href="https://forum.level1techs.com/t/has-anyone-tried-building-nanochat/239502"&gt;source&lt;/a&gt;)&lt;/td&gt; &lt;td&gt;1.6 days&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4 x 3090 (&lt;a href="https://x.com/0x_sero/status/1978664943827190116?s=61"&gt;source&lt;/a&gt;&lt;/td&gt; &lt;td&gt;2.25 days&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 x 4090&lt;/td&gt; &lt;td&gt;3.4 days&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2 x DGX Spark&lt;/td&gt; &lt;td&gt;4 days&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 x 3090&lt;/td&gt; &lt;td&gt;7 days&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 x DGX Spark&lt;/td&gt; &lt;td&gt;10 days&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/27ik4oxar6yf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrjqk/nanochat_pretraining_time_benchmarks_100_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrjqk/nanochat_pretraining_time_benchmarks_100_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T05:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojui14</id>
    <title>Is there any kokoro 82m version or alternative that has the same lifelike quality but way way faster? Already tried ONNX, not fast enough.</title>
    <updated>2025-10-30T08:52:40+00:00</updated>
    <author>
      <name>/u/KledMainSG</name>
      <uri>https://old.reddit.com/user/KledMainSG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KledMainSG"&gt; /u/KledMainSG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojui14/is_there_any_kokoro_82m_version_or_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojui14/is_there_any_kokoro_82m_version_or_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojui14/is_there_any_kokoro_82m_version_or_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T08:52:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojs49s</id>
    <title>What's one tool or script that massively improved your local LLM workflow?</title>
    <updated>2025-10-30T06:10:49+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beyond the popular UIs like Oobabooga and Faraday, I'm looking for those smaller utilities that save time or add a killer feature. For example, a script for batch testing prompts across multiple models, a tool for better logprobs analysis, or a clever use of llama.cpp's server features. What's your secret weapon?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojs49s/whats_one_tool_or_script_that_massively_improved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojs49s/whats_one_tool_or_script_that_massively_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojs49s/whats_one_tool_or_script_that_massively_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T06:10:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj44s9</id>
    <title>If You Want to Understand Why Llama Models Flopped, Zuck is the Cause!</title>
    <updated>2025-10-29T13:11:25+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below is a short video that attempts to explain why most Meta products fails... Spoiler alert, it's Zuck's fault.&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=hb5cYB7Eoj8"&gt;https://www.youtube.com/watch?v=hb5cYB7Eoj8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I strongly believe Llama 5 will not come out any time soon. I don't think there will be any Llama5, to be honest. And, I don't think we will see any good competitive OS model from Meta ever again. Why do I believe that, you ask? Well, any investment requires long-term commitment and perseverance, even if you encounter a few setbacks along the way. But, as long as Meta AI is controlled by Zuck, it will never invest long enough to achieve anything meaningful simply because Zuck isn't someone who commits to an idea long enough. Flipflopping seems to be in his DNA as a CEO.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T13:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojzp66</id>
    <title>Ai Models for Core Ultra Processor</title>
    <updated>2025-10-30T13:27:52+00:00</updated>
    <author>
      <name>/u/saqlain1020</name>
      <uri>https://old.reddit.com/user/saqlain1020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to try running Ai models locally.&lt;br /&gt; I don't have a GPU but the Processor is Core Ultra 7 265K with 64GB ddr5 ram&lt;/p&gt; &lt;p&gt;I want to know which models will give me best results for text generation and image generation on this machine, without GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saqlain1020"&gt; /u/saqlain1020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzp66/ai_models_for_core_ultra_processor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzp66/ai_models_for_core_ultra_processor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzp66/ai_models_for_core_ultra_processor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T13:27:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojt9qt</id>
    <title>Deepseek-OCR Great, but not for long</title>
    <updated>2025-10-30T07:27:20+00:00</updated>
    <author>
      <name>/u/weirdkoe</name>
      <uri>https://old.reddit.com/user/weirdkoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i have been testing Deepseek-OCR for the last couple of days using vLLM as the engine, and it has outperform all my other open-source options (docling, tika, marker, etc..). Yes it do need much better hardware, but the results worth it&lt;/p&gt; &lt;p&gt;Until, when I plugged a 80 pages pdf to be OCR (Arabic language content), it started repeating words.&lt;/p&gt; &lt;p&gt;Each page take around 1 sec, but the pages with the repeating tokes took 30+ seconds to process üíÄ&lt;/p&gt; &lt;p&gt;I have tried many solutions, but nothing worked&lt;/p&gt; &lt;p&gt;Does anyone know why does this happen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/weirdkoe"&gt; /u/weirdkoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt9qt/deepseekocr_great_but_not_for_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt9qt/deepseekocr_great_but_not_for_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt9qt/deepseekocr_great_but_not_for_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T07:27:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0ugi</id>
    <title>A free API for daily AI research breakthroughs</title>
    <updated>2025-10-30T14:13:20+00:00</updated>
    <author>
      <name>/u/Quiet_Truck_326</name>
      <uri>https://old.reddit.com/user/Quiet_Truck_326</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a small project that automatically collects new AI research papers (mainly from arXiv), scores them for relevance, and summarizes the most important breakthroughs.&lt;/p&gt; &lt;p&gt;It‚Äôs completely free and comes with an open API so you can pull the data into your own tools or workflows.&lt;/p&gt; &lt;p&gt;It‚Äôs meant for people who want to stay updated on what‚Äôs happening in AI without reading hundreds of papers a day.&lt;br /&gt; API docs and example responses are available here: &lt;a href="https://cognoska.com/api/docs"&gt;https://cognoska.com/api/docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback or suggestions welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet_Truck_326"&gt; /u/Quiet_Truck_326 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0ugi/a_free_api_for_daily_ai_research_breakthroughs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0ugi/a_free_api_for_daily_ai_research_breakthroughs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0ugi/a_free_api_for_daily_ai_research_breakthroughs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:13:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojpfwl</id>
    <title>MLX added support for MXFP8 and NVFP4</title>
    <updated>2025-10-30T03:35:29+00:00</updated>
    <author>
      <name>/u/Direct-Stranger-4140</name>
      <uri>https://old.reddit.com/user/Direct-Stranger-4140</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Supports mxfp8 and nvfp4 in quantize/dequantize and adds kernels for mx and nv quants.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ops based fallback for CPU&lt;/li&gt; &lt;li&gt;Fast CUDA kernels&lt;/li&gt; &lt;li&gt;Fast Metal kernels&lt;/li&gt; &lt;li&gt;Defaults for bits and group size based on mode&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ml-explore/mlx/pull/2688"&gt;https://github.com/ml-explore/mlx/pull/2688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct-Stranger-4140"&gt; /u/Direct-Stranger-4140 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T03:35:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojv1hk</id>
    <title>Open Source Lovable with Custom Agents, Full Stack Support, and Local Models</title>
    <updated>2025-10-30T09:27:58+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojv1hk/open_source_lovable_with_custom_agents_full_stack/"&gt; &lt;img alt="Open Source Lovable with Custom Agents, Full Stack Support, and Local Models" src="https://b.thumbs.redditmedia.com/CRe9hmF2IXlSFsmaIIy6TEdJBOIFo7C24ml12h1NuLc.jpg" title="Open Source Lovable with Custom Agents, Full Stack Support, and Local Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on building an open-source version of Loveable that can run locally and start with full stack templates while you can bring your own keys. Right now we have react, vite, nextjs, fastapi, go. (Well, Ernest and I built it from the Tesslate/UIGEN team). You can try it online here (You can use free Qwen-Coder, GPT-5, and llama for free through the next 12 days before we run out of funding): &lt;a href="https://tesslate.com"&gt;https://tesslate.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;You guys can find the repo here if you want to give us a star: &lt;a href="https://github.com/TesslateAI/Studio"&gt;https://github.com/TesslateAI/Studio&lt;/a&gt; and the docs at &lt;a href="https://docs.tesslate.com"&gt;https://docs.tesslate.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We've been observing a lot of the problems that people run into while vibecoding:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Proprietary providers get to swap out your models whenever&lt;/li&gt; &lt;li&gt;You have to pay crazy subscription fees&lt;/li&gt; &lt;li&gt;They get to choose whenever they change their system prompts or context engine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, to change that, we made the entire thing super easy to swap. You can change the system prompts of your Agents, add different tools to them, and then use them in your code. If you have custom agent configurations and unique tools, you can simply add them to the agent-factory class that'll wrap it into the marketplace. This simply means the agent you are using today, will be the agent you are using until you specifically want it to switch. &lt;/p&gt; &lt;p&gt;The other issue with vibecoding is the 80% problem or not getting what you want after a certain while and your application / architecture not scaling when you need it to. Now, I don't think I can fix that issue for you overnight, but we're slowly making progress to an idea of how to get a proper spec to prod. (Hence the idea tab.) We've also integrated project notes and a kanban board. &lt;/p&gt; &lt;p&gt;Other features: You can use Llitellm, llama.cpp, LM Studio, Ollama, and Openrouter to add models to whatever agent you choose. You can also generate architecture diagrams from your code in mermaid. You can also open multiple browser tabs inside the application to view every route of your application. &lt;/p&gt; &lt;p&gt;Enterprise Features: Litellm can provision keys for users, do cost tracking. You can do RBAC management and admin / agent / template / marketplace management. (Still working on the docs for that but we already have that implemented and open sourced). &lt;/p&gt; &lt;p&gt;Most importantly, we believe in all things open source so the multi agent framework with mcp (&lt;a href="https://github.com/TesslateAI/TFrameX"&gt;tframex&lt;/a&gt;), as well as this entire application is Apache 2.0. Tesslate is committed to keeping everything open source. &lt;/p&gt; &lt;p&gt;Our next goals are to expand to mobile development, make better developer handoffs, work on deployment and management solutions, and just iterate on your guys' feedback, which would be very useful.&lt;/p&gt; &lt;p&gt;And yeah! Today is the worst version that Tesslate Studio is ever going to be, we'll keep improving it with the communities feedback to get exactly what you guys are looking for. Ernest and I are not experts whatsoever but we're going to be working hard to bring the best version of this vision to life. Contributions or suggestions are always welcome, its an open source project after all. Here's our discord for updates: &lt;a href="https://discord.gg/DkzMzwBTaw"&gt;Discord&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ojv1hk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojv1hk/open_source_lovable_with_custom_agents_full_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojv1hk/open_source_lovable_with_custom_agents_full_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T09:27:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojx8no</id>
    <title>Building "RAG from Scratch". A local, educational repo to really understand Retrieval-Augmented Generation (feedback welcome)</title>
    <updated>2025-10-30T11:35:26+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was surprised by the positive feedback and high interest in my AI Agents from Scratch GitHub repo. Big thanks to the community to show me that I am not alone in this and that the effort I put in was valued. I will add more examples over time to AI Agents from Scratch.&lt;/p&gt; &lt;p&gt;I‚Äôm working on a new educational open-source project called &lt;strong&gt;RAG from Scratch&lt;/strong&gt;, inspired by my previous repo &lt;a href="https://github.com/pguso/ai-agents-from-scratch"&gt;AI Agents from Scratch&lt;/a&gt;. In most practical setups a AI Agent needs RAG to function as its procedural memory - to recall relevant facts, documents and experiences to make decisions.&lt;/p&gt; &lt;p&gt;The goal of the new repo: &lt;strong&gt;demystify Retrieval-Augmented Generation&lt;/strong&gt; by letting developers build it step by step - no black boxes, no frameworks, no cloud APIs.&lt;/p&gt; &lt;p&gt;Each folder introduces one clear concept (embeddings, vector store, retrieval, augmentation, etc.), with tiny runnable JS files and comments explaining every function.&lt;/p&gt; &lt;p&gt;Here‚Äôs the &lt;a href="https://gist.github.com/pguso/cf659b3ea3f6631e433acc471d3840a4"&gt;README draft&lt;/a&gt; showing the current structure.&lt;/p&gt; &lt;p&gt;Each folder teaches one concept:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge requirements &lt;/li&gt; &lt;li&gt;Data loading &amp;amp; data sources&lt;/li&gt; &lt;li&gt;Text splitting &amp;amp; chunking&lt;/li&gt; &lt;li&gt;Embeddings&lt;/li&gt; &lt;li&gt;Vector database&lt;/li&gt; &lt;li&gt;Retrieval &amp;amp; augmentation&lt;/li&gt; &lt;li&gt;Generation (via local &lt;code&gt;node-llama-cpp&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Evaluation &amp;amp; caching&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything runs fully local using embedded databases and node-llama-cpp for local inference. So you don't need to pay for anything while learning.&lt;/p&gt; &lt;p&gt;At this point only a few examples are implemented, the idea is to help devs &lt;em&gt;really understand&lt;/em&gt; RAG before they use frameworks like LangChain or LlamaIndex.&lt;/p&gt; &lt;p&gt;I‚Äôd love feedback on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Whether the &lt;strong&gt;step order&lt;/strong&gt; makes sense for learning,&lt;/li&gt; &lt;li&gt;If any &lt;strong&gt;concepts seem missing&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Any &lt;strong&gt;naming or flow&lt;/strong&gt; improvements you‚Äôd suggest before I go public.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance! I‚Äôll release it publicly in a few weeks once the core examples are polished.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojx8no/building_rag_from_scratch_a_local_educational/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojx8no/building_rag_from_scratch_a_local_educational/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojx8no/building_rag_from_scratch_a_local_educational/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T11:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojd1oq</id>
    <title>Here's the best prompt you will ever need to test the new LLMs</title>
    <updated>2025-10-29T18:49:59+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"&gt; &lt;img alt="Here's the best prompt you will ever need to test the new LLMs" src="https://preview.redd.it/n2yilqu2k3yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=575a481bdcf458b2fd06c48788fa5f9271cd30ad" title="Here's the best prompt you will ever need to test the new LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;The numbers Mason, what do they mean?!! 10 23 68 111 8 7 7 47 53 23 63 92 15&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n2yilqu2k3yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T18:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojwsfi</id>
    <title>The Single Most Overlooked Decision in RAG: Stop Naive Text Splitting</title>
    <updated>2025-10-30T11:10:58+00:00</updated>
    <author>
      <name>/u/Pristine-Ask4672</name>
      <uri>https://old.reddit.com/user/Pristine-Ask4672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last few weeks tweaking my retrieval-augmented generation (RAG) setup, trying out different models, embeddings, and retrieval settings. It‚Äôs funny‚Äîmy biggest improvement didn‚Äôt come from any of that. It actually stemmed from how I was splitting my text.&lt;/p&gt; &lt;p&gt;I used to think chunking was just a boring preprocessing step. You break the text into pieces and move on, right? But once I started experimenting, I realized it‚Äôs a crucial part of the whole process. Get it wrong, and your retriever is just going to hand the model junk.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Typical Chunking Doesn‚Äôt Cut It&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most tutorials suggest splitting text based on a set number of characters. Sounds easy enough, but then you find out it‚Äôs slicing through sentences, headers, and sometimes even code blocks. Now your chunks are all jumbled, and the retrieval goes downhill.&lt;/p&gt; &lt;p&gt;Picture this: you ask your system, ‚ÄúWhat‚Äôs the remote work policy?‚Äù If one chunk ends mid-sentence and the next one picks up halfway through the explanation, neither has the full picture. Your embeddings can‚Äôt capture the complete concept, and you‚Äôre left with a mess.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding the Right Balance&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I tried all sorts of methods:&lt;/p&gt; &lt;p&gt;- Whole-document embeddings: felt relevant, but not super helpful.&lt;/p&gt; &lt;p&gt;- Sentence-based chunks: too small to keep the context.&lt;/p&gt; &lt;p&gt;The best results came from semantic chunking‚Äîaiming for chunks around 500 to 1,000 tokens with a bit of overlap (about 10 to 20%). That overlap helps connect ideas across chunks, keeping the context intact when you cut the text up. Plus, each chunk can hold a complete thought.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes a Good Chunk&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A good chunk should be able to stand alone‚Äîfocusing on one idea without mixing topics or splitting sentences in half. It should follow natural structures‚Äîlike paragraphs, headings, and code blocks‚Äîand be measured by tokens instead of raw character count since that‚Äôs how language models really work.&lt;/p&gt; &lt;p&gt;Using a recursive or semantic splitting approach is perfect for this‚Äîstart by dividing into larger sections (like paragraphs) and only further split if the chunks get too big.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What It Looks Like in Action&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I tried this out with a simple example: a company handbook.&lt;/p&gt; &lt;p&gt;When I put the whole document into one big chunk, the retriever gave me vague sections mentioning remote work but missing out on key details. Sentence-level splitting helped a bit, but I lost the connections between related points, like eligibility and work hours.&lt;/p&gt; &lt;p&gt;Then I switched to paragraph-level chunking with a small overlap, and it was a game changer. The retrievals were spot on‚Äîclear, concise, and no context was missing. Even the similarity scores backed it up.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More Than Just Text&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Chunking isn‚Äôt just for plain text.&lt;/p&gt; &lt;p&gt;- For code, split by function or class.&lt;/p&gt; &lt;p&gt;- For tables or structured data, use a parser that respects the layout.&lt;/p&gt; &lt;p&gt;- For mixed content like PDFs or Markdown, check out tools like LangChain‚Äôs splitters or Unstructured.&lt;/p&gt; &lt;p&gt;The rule is simple: split by meaning, not by count.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final Thought&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If your RAG setup feels off, take a look at your chunking before diving into new models or embeddings. A solid chunking strategy can often boost performance way more than splurging on fancy embedding models.&lt;/p&gt; &lt;p&gt;Think of chunking as how your model ‚Äúsees‚Äù the world. Nail that down, and everything else will start to make sense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Ask4672"&gt; /u/Pristine-Ask4672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojwsfi/the_single_most_overlooked_decision_in_rag_stop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojwsfi/the_single_most_overlooked_decision_in_rag_stop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojwsfi/the_single_most_overlooked_decision_in_rag_stop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T11:10:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1oje2cc</id>
    <title>DeepSeek may have found a new way to improve AI‚Äôs ability to remember</title>
    <updated>2025-10-29T19:27:50+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"&gt; &lt;img alt="DeepSeek may have found a new way to improve AI‚Äôs ability to remember" src="https://external-preview.redd.it/aEXxC3nzMud-eeB3QaX4fT3GiVuldP60VVX6Yf4IvC8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0104fa768b4fc264cfeccc0fd459a4eaa256f643" title="DeepSeek may have found a new way to improve AI‚Äôs ability to remember" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.technologyreview.com/2025/10/29/1126932/deepseek-ocr-visual-compression"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T19:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojneas</id>
    <title>How are teams dealing with "AI fatigue"</title>
    <updated>2025-10-30T01:55:24+00:00</updated>
    <author>
      <name>/u/Temporary_Papaya_199</name>
      <uri>https://old.reddit.com/user/Temporary_Papaya_199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rolled out AI coding assistants for my developers, and while individual developer &amp;quot;productivity&amp;quot; went up - team alignment and developer &amp;quot;velocity&amp;quot; did not.&lt;/p&gt; &lt;p&gt;They worked more - but not shipping new features. They were now spending more time reviewing and fixing AI slob. My current theory - AI helps the individual not the team.&lt;/p&gt; &lt;p&gt;Are any of you seeing similar issues? If yes, where, translating requirements into developer tasks, figuring out how one introduction or change impacts everything else or with keeping JIRA and github synced.&lt;/p&gt; &lt;p&gt;Want to know how you guys are solving this problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Papaya_199"&gt; /u/Temporary_Papaya_199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T01:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojrv67</id>
    <title>Tried Nvidia‚Äôs new open-source VLM, Here's My Experience</title>
    <updated>2025-10-30T05:54:42+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been playing around with NVIDIA‚Äôs new Nemotron Nano 12B V2 VL, and it‚Äôs easily one of the most impressive open-source vision-language models I‚Äôve tested so far.&lt;/p&gt; &lt;p&gt;I started simple: built a small Streamlit OCR app to see how well it could parse real documents.&lt;br /&gt; Dropped in an invoice, it picked out totals, vendor details, and line items flawlessly.&lt;br /&gt; Then I gave it a &lt;em&gt;handwritten note&lt;/em&gt;, and somehow, it summarized the content correctly, no OCR hacks, no preprocessing pipelines. Just raw understanding.&lt;/p&gt; &lt;p&gt;Then I got curious.&lt;br /&gt; What if I showed it something completely different?&lt;/p&gt; &lt;p&gt;So I uploaded a frame from &lt;em&gt;Star Wars: The Force Awakens,&lt;/em&gt; Kylo Ren, lightsaber drawn, and the model instantly recognized the scene and character. ( This impressed me the Most)&lt;/p&gt; &lt;p&gt;You can run visual Q&amp;amp;A, summarization, or reasoning across up to 4 document images (1k√ó2k each), all with long text prompts.&lt;/p&gt; &lt;p&gt;This feels like the start of something big for open-source document and vision AI. Here's the &lt;a href="https://x.com/Arindam_1729/status/1983536576157372886"&gt;short clips&lt;/a&gt; of my tests.&lt;/p&gt; &lt;p&gt;Would love to know your experience with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrv67/tried_nvidias_new_opensource_vlm_heres_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrv67/tried_nvidias_new_opensource_vlm_heres_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrv67/tried_nvidias_new_opensource_vlm_heres_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T05:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0voi</id>
    <title>Introducing Hephaestus: AI workflows that build themselves as agents discover what needs to be done</title>
    <updated>2025-10-30T14:14:41+00:00</updated>
    <author>
      <name>/u/Standard_Excuse7988</name>
      <uri>https://old.reddit.com/user/Standard_Excuse7988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0voi/introducing_hephaestus_ai_workflows_that_build/"&gt; &lt;img alt="Introducing Hephaestus: AI workflows that build themselves as agents discover what needs to be done" src="https://external-preview.redd.it/aHpncHF4ZXpiOXlmMZc87bODxInUab1QdzAVIKt_p_AEHL5YkJEhQTcdw4CD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67b99040969c4f9ad4800522558e3abdd06aa68b" title="Introducing Hephaestus: AI workflows that build themselves as agents discover what needs to be done" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I've been working on Hephaestus - an open-source framework that changes how we think about AI agent workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Most agentic frameworks make you define every step upfront. But complex tasks don't work like that - you discover what needs to be done as you go.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; Semi-structured workflows. You define &lt;em&gt;phases&lt;/em&gt; - the logical steps needed to solve a problem (like &amp;quot;Reconnaissance ‚Üí Investigation ‚Üí Validation&amp;quot; for pentesting). Then agents dynamically create tasks across these phases based on what they discover.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; During a pentest, a validation agent finds an IDOR vulnerability that exposes API keys. Instead of being stuck in validation, it spawns a new reconnaissance task: &amp;quot;Enumerate internal APIs using these keys.&amp;quot; Another agent picks it up, discovers admin endpoints, chains discoveries together, and the workflow branches naturally.&lt;/p&gt; &lt;p&gt;Agents share discoveries through RAG-powered memory and coordinate via a Kanban board. A Guardian agent continuously tracks each agent's behavior and trajectory, steering them in real-time to stay focused on their tasks and prevent drift.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ido-Levi/Hephaestus"&gt;https://github.com/Ido-Levi/Hephaestus&lt;/a&gt; üìö &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://ido-levi.github.io/Hephaestus/"&gt;https://ido-levi.github.io/Hephaestus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fair warning:&lt;/strong&gt; This is a brand new framework I built alone, so expect rough edges and issues. The repo is a bit of a mess right now. If you find any problems, please report them - feedback is very welcome! And if you want to contribute, I'll be more than happy to review it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Excuse7988"&gt; /u/Standard_Excuse7988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uwogrxezb9yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0voi/introducing_hephaestus_ai_workflows_that_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0voi/introducing_hephaestus_ai_workflows_that_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:14:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojf2r1</id>
    <title>Qwen3-VL now available in Ollama locally for all sizes.</title>
    <updated>2025-10-29T20:06:18+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"&gt; &lt;img alt="Qwen3-VL now available in Ollama locally for all sizes." src="https://preview.redd.it/ycizvauvx3yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4ef60e1f14a4f71fc6659de298dd0d9565d8acb" title="Qwen3-VL now available in Ollama locally for all sizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ycizvauvx3yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T20:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojvgsx</id>
    <title>manifestai releases Brumby-14B-Base weights, claims "attention free" and inference "hundreds of time faster" for long context</title>
    <updated>2025-10-30T09:55:08+00:00</updated>
    <author>
      <name>/u/ArcadesOfAntiquity</name>
      <uri>https://old.reddit.com/user/ArcadesOfAntiquity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojvgsx/manifestai_releases_brumby14bbase_weights_claims/"&gt; &lt;img alt="manifestai releases Brumby-14B-Base weights, claims &amp;quot;attention free&amp;quot; and inference &amp;quot;hundreds of time faster&amp;quot; for long context" src="https://external-preview.redd.it/u286g9i_4XNK4XbToyvDxLSuC8KvkBPXz6zTs2VwH_4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=501921f84aaec52c66e095e5820853a5d045617d" title="manifestai releases Brumby-14B-Base weights, claims &amp;quot;attention free&amp;quot; and inference &amp;quot;hundreds of time faster&amp;quot; for long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;also check out their blog page for the release:&lt;/p&gt; &lt;p&gt;&lt;a href="https://manifestai.com/articles/release-brumby-14b/"&gt;https://manifestai.com/articles/release-brumby-14b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I only skimmed the hf card and blog, and one thing that struck me is they seem to initizialize their weights for their so called &amp;quot;power retention&amp;quot; model architecture, using the weights of Qwen3-14B, and they call the technique &amp;quot;retraining&amp;quot;...&lt;/p&gt; &lt;p&gt;I guess this makes me a bit skeptical as we might just refer to it as &amp;quot;fine tuning&amp;quot;. And makes me worry this is just a way to publish something AI-related so they can get wrap their mouths around that VC money firehose.&lt;/p&gt; &lt;p&gt;But, they said they spent $4000 to &amp;quot;retrain&amp;quot; it, so maybe...?&lt;/p&gt; &lt;p&gt;Anyway, the real promising aspect here is the claim in the &amp;quot;Coming soon&amp;quot; section at the bottom of the hugging face page:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Fast long-context inference: Our fastest power retention inference kernels are hundreds of times faster than equivalent attention kernels on long contexts. We will update the architecture to incorporate these fast kernels.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;If this turns out to be even 50% true that would be amazing. Suddenly Mac would be totally legitimate for serious industrial scale inference. Which makes me think it's too good to be true...&lt;/p&gt; &lt;p&gt;Time will tell&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArcadesOfAntiquity"&gt; /u/ArcadesOfAntiquity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/manifestai/Brumby-14B-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojvgsx/manifestai_releases_brumby14bbase_weights_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojvgsx/manifestai_releases_brumby14bbase_weights_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T09:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojo8le</id>
    <title>Minimax pre-training lead explains why no linear attention</title>
    <updated>2025-10-30T02:35:13+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax M2 Tech Blog 3: Why Did M2 End Up as a Full Attention Model?&lt;/p&gt; &lt;p&gt;On behave of pre-training lead Haohai Sun. (&lt;a href="https://zhihu.com/question/1965302088260104295/answer/1966810157473335067"&gt;https://zhihu.com/question/1965302088260104295/answer/1966810157473335067&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I. Introduction&lt;/p&gt; &lt;p&gt;As the lead of MiniMax-M2 pretrain, I've been getting many queries from the community on &amp;quot;Why did you turn back the clock and go with full attention with MiniMax M2?&amp;quot; After explaining the backstory in one chat after another, I figured it's time to write down our journey in a blog.&lt;/p&gt; &lt;p&gt;Honestly, I could give you the textbook debate. I could talk all afternoon about why you should build linear/sparse attention. Then, I could turn around and talk all afternoon about why you shouldn't. But what's the point of all that hand-waving? The real question is whether you should actually do it.&lt;/p&gt; &lt;p&gt;So, let's start with the conclusion: We are always working on it. But in a real-world, industrial-grade system, the truth is that efficient attention still has some way to go before it can definitively beat full attention. As LLMs have evolved, the entire stack has become monstrously complex. We serve more scenarios, and the architecture design trade-offs are exploding: &amp;quot;How does it perform on code and math? What about agent scenarios? How does it handle multimodality? Does long-chain CoT still hold up? Can RL scale on top of it? Are there hidden traps with low-precision compute? How do you implement interleaved thinking, caching, or speculative decoding? ... &amp;quot;&lt;/p&gt; &lt;p&gt;In short, there's a vast difference between the promise on paper and its payoff in production. You only get to claim that payoff after satisfying Condition 1...n and solving Problem 1...n.&lt;/p&gt; &lt;p&gt;II. Why Efficient Attention?&lt;/p&gt; &lt;p&gt;Let's do a thought experiment. If you had infinite compute, would you even bother with linear or sparse attention? Some might bring up theoretical arguments about softmax attention &amp;quot;oversmoothing&amp;quot; in an infinite context... but who knows? Under the current compute bound, no model has truly pushed softmax attention to its absolute limit. So, for all practical purposes, the race for efficient attention is a race to save compute.&lt;/p&gt; &lt;p&gt;For our M2 design, could we aim to save tokens ‚Äî achieving the same quality with fewer tokens? Well if you believe in scaling laws, to achieve this goal, you'd probably bet on other paths to get there, not efficient attention.&lt;/p&gt; &lt;p&gt;So, the simple truth is this: Compute is finite. We need an architecture that makes better use of it ‚Äî models that achieve higher performance under the same budget (training &amp;amp; inference).&lt;/p&gt; &lt;p&gt;III. The Real Bottlenecks&lt;/p&gt; &lt;p&gt;To build a model that can practically be deployed and used by the community, we have to start with what users care: Quality, Speed (TPS), and Price. Quality is non-negotiable. A useless model is useless even if it's free. So how do we make a Linear/Sparse/Hybrid Attention model that performs well enough? The biggest challenge here isn‚Äôt the architecture design ‚Äî the real bottleneck is the limitations of evaluation. (As for speed and price, those are heavily influenced by the inference stack‚Äîand great models tend to attract great engineers to optimize them.)&lt;/p&gt; &lt;p&gt;The Evaluation Trap: Goodhart's Law in Action&lt;/p&gt; &lt;p&gt;‚ÄúAs long as you build the benchmark, I‚Äôll find a way to beat it.‚Äù Over the past few years of LLM development, the pace of leaderboard progress is staggering. No matter how hard a benchmark is ‚Äî even if the SOTA score starts in single digits ‚Äî once it catches the industry‚Äôs attention, it‚Äôs usually crushed within a few iterations. But how do you build an evaluation system that is comprehensive and actually reflects a model's true capabilities? That‚Äôs one of the hardest ‚Äî and most critical ‚Äî problems in LLM development, and it becomes even more acute when you start messing with a component as fundamental as attention.&lt;/p&gt; &lt;p&gt;Benchmarks are a Leaky Abstraction&lt;/p&gt; &lt;p&gt;There‚Äôs no free lunch. When you reduce the complexity of attention, you pay a price. The question is, where?&lt;/p&gt; &lt;p&gt;When we were developing MiniMax-Text-01, everyone was still evaluating MMLU, BBH, MATH, and LongBench (all of which are now saturated). From the perspective of a year ago, a hybrid of Lightning Attention and Full Attention looked just as good as pure full attention. Our own small-scale hybrid models confirmed this on the leaderboards. (Did we find a free lunch?)&lt;/p&gt; &lt;p&gt;Not quite. The price paid became obvious at a larger scale: the model had clear deficits in complex, multi-hop reasoning tasks.&lt;/p&gt; &lt;p&gt;Okay, once a problem is exposed, you can fix it. We developed proxy metrics for this specific weakness and iterated until the hybrid model seemed to match MHA. But does that proxy metric still correlate with real-world downstream performance at an even larger scale? Are there other hidden weaknesses? Who knows. We haven't run those experiments yet.&lt;/p&gt; &lt;p&gt;The better the models get, the harder they are to evaluate. But that‚Äôs a must part of the journey ‚Äî keep it up, eval teams!&lt;/p&gt; &lt;p&gt;The High Cost of Knowing Things&lt;/p&gt; &lt;p&gt;For complex reasoning tasks, we can sometimes find early proxy metrics that correlate well with final performance ‚Äî but not for all tasks (at least, not yet). As tasks get harder, the amount of experiment compute required just to get a statistically significant signal on your metric grows astronomically ‚Äî which is ironic, since we study efficient attention because compute is limited.&lt;/p&gt; &lt;p&gt;And beyond the academic benchmarks, optimization issues often only surface at scale. You never really know what‚Äôs going to happen until you scale up. Anyone who read our M1 paper will recall the serious precision issues we hit during RL training ‚Äî problems that would‚Äôve been spotted earlier. Going back and analyzing Lightning Attention's numerical convergence with that experience in hand was incredibly clarifying.&lt;/p&gt; &lt;p&gt;Discovering the real problems is often far harder than solving them.&lt;/p&gt; &lt;p&gt;A Symphony of Variables&lt;/p&gt; &lt;p&gt;There are just too many variables in model training. Different architectures behave very differently on different data distributions and with different optimizers. In a world where our data is constantly being updated, an experiment run on last month's data mix might yield the opposite conclusion today. We can‚Äôt observe everything perfectly ‚Äî but we‚Äôre working on finding more reliable experimental strategies.&lt;/p&gt; &lt;p&gt;Infrastructure: Where Theory Meets Metal&lt;/p&gt; &lt;p&gt;Compared to full attention, the infrastructure for linear and sparse attention is much less mature. To actually get the promised results, there‚Äôs still a lot of groundwork to fill in. Take linear attention for example: If you analyze the compute intensity of existing linear architectures, many of them are memory-bound ‚Äî even during training. Without extreme IO optimization, you‚Äôre basically leaving a huge amount of GPU FLOPs on the table. And inference brings even more challenges than training: How do you deliver a service that is genuinely faster and cheaper? Linear attention has linear compute complexity and constant memory usage. That means there‚Äôs a crossover point where it becomes more efficient than full attention in compute and memory. In theory, that point lies at a few thousand tokens ‚Äî which isn‚Äôt particularly long for today‚Äôs large models.&lt;/p&gt; &lt;p&gt;But that‚Äôs just theory. We need to solve a few key problems to actually approach it:&lt;/p&gt; &lt;p&gt;Low-Precision State Storage: Linear attention is currently far more sensitive to numerical precision than full attention.&lt;/p&gt; &lt;p&gt;Prefix Caching: In real-world applications, the cache-hit rate for conversations is very high. A new architecture must handle this gracefully.&lt;/p&gt; &lt;p&gt;Speculative Decoding: How do you optimize speculative decoding with linear attention backbone? Well fortunately, all of these seem solvable.&lt;/p&gt; &lt;p&gt;IV. What‚Äôs Next&lt;/p&gt; &lt;p&gt;Scaling remains the name of the game, and context scaling is one of the key problems. Longer and longer context length is key in both pre-training and post-training. As GPU compute growth slows while data length keeps increasing, the benefits of linear and sparse attention will gradually emerge. We should start preparing now:&lt;/p&gt; &lt;p&gt;Better Data: More multimodal, information-rich long-context data.&lt;/p&gt; &lt;p&gt;Better Evaluation: More informative evaluation system and experimental paradigms to speed up iteration.&lt;/p&gt; &lt;p&gt;Better Infrastructure: Mature training and inference infrastructure to fully squeeze out GPU potential.&lt;/p&gt; &lt;p&gt;V. Addendum: the SWA code...&lt;/p&gt; &lt;p&gt;We accidentally left the SWA inference code in the open-source release, and some people asked why it wasn‚Äôt used in the final model. Simple answer: the performance wasn't good enough.&lt;/p&gt; &lt;p&gt;That experiment was from quite early on, before GPT-OSS was open-sourced (we were pretty surprised to see its structure, by the way). But I can share a brief summary of our failed attempt. We tried adapting CPT into a Hybrid SWA, testing both inter &amp;amp; intra-layer mixing. The motivation for intra-layer mixing was to balance the compute intensity across all layers, which is friendly to both PP in training and PP or AFD during inference. Unfortunately, neither worked. Performance degraded noticeably as context length grew ‚Äî which is unacceptable in agentic scenarios.&lt;/p&gt; &lt;p&gt;Our analysis showed that many global attention patterns (like retrieval head and induction head) were already established early during pre-training. CPT can hardly adjust those patterns afterwards. You surely can mitigate the issue by using data probes to identify and keep those heads as full attention ‚Äî but unfortunately, it‚Äôs nearly impossible to discover them all from human priors.&lt;/p&gt; &lt;p&gt;(And no, this issue isn‚Äôt related to attention sinks.)&lt;/p&gt; &lt;p&gt;If you're interested in this line of research, I recommend taking a closer look at GPT-OSS, CWM, and Gemma, especially their long-context performance.&lt;/p&gt; &lt;p&gt;Finally, we‚Äôre hiring! If you want to join us, send your resume to &lt;a href="mailto:guixianren@minimaxi.com"&gt;guixianren@minimaxi.com&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;References&lt;/li&gt; &lt;li&gt;MiniMax-01: Scaling Foundation Models with Lightning Attention&lt;/li&gt; &lt;li&gt;MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/li&gt; &lt;li&gt;CWM: An Open-Weights LLM for Research on Code Generation with World Models&lt;/li&gt; &lt;li&gt;Qwen3-Next&lt;/li&gt; &lt;li&gt;Gemma 3 Technical Report&lt;/li&gt; &lt;li&gt;gpt-oss-120b &amp;amp; gpt-oss-20b Model Card&lt;/li&gt; &lt;li&gt;Retrieval Head Mechanistically Explains Long-Context Factuality&lt;/li&gt; &lt;li&gt;&lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"&gt;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/zpysky1125/status/1983383094607347992"&gt;https://x.com/zpysky1125/status/1983383094607347992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also I called it last month: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T02:35:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojswm4</id>
    <title>new Nemotrons based on Qwen3 32B</title>
    <updated>2025-10-30T07:02:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"&gt; &lt;img alt="new Nemotrons based on Qwen3 32B" src="https://b.thumbs.redditmedia.com/qOhsjjl6hDVmoBy_BtTdl9Y4EEOOeDpCvErh6KfawCo.jpg" title="new Nemotrons based on Qwen3 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Qwen3-Nemotron-32B-RLBFF&lt;/strong&gt; is a large language model that leverages Qwen/Qwen3-32B as the foundation and is fine-tuned to improve the quality of LLM-generated responses in the default thinking mode.&lt;/p&gt; &lt;p&gt;Given a conversation with multiple turns between user and assistant and a user-specified principle, it generates a response the final user turn.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is a research model&lt;/strong&gt; described in and is released to support the following research paper: &lt;a href="https://arxiv.org/abs/2509.21319"&gt;https://arxiv.org/abs/2509.21319&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As of 24 Sep 2025, this model achieves &lt;a href="https://github.com/lmarena/arena-hard-auto"&gt;Arena Hard V2&lt;/a&gt; of 55.6% and &lt;a href="https://huggingface.co/spaces/allenai/WildBench"&gt;WildBench&lt;/a&gt; Score of 70.33% and &lt;a href="https://arxiv.org/abs/2306.05685"&gt;MT Bench&lt;/a&gt; of 9.50. This means that our model is substantially improved over the initial Qwen3-32B model and has similar performance compared to DeepSeek R1 and O3-mini at less than 5% of the inference cost (as indicated on openrouter).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cap7eioa77yf1.png?width=1282&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd3ed08aa03d73b589fcf39e3ebb20360df97ef3"&gt;https://preview.redd.it/cap7eioa77yf1.png?width=1282&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd3ed08aa03d73b589fcf39e3ebb20360df97ef3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF"&gt;https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-Nemotron-32B-RLBFF-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-Nemotron-32B-RLBFF-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T07:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0i7q</id>
    <title>AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo</title>
    <updated>2025-10-30T14:00:16+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" src="https://b.thumbs.redditmedia.com/hb9XoRhxPRhv8ljYkjnVbJWgnClXeLGMxbG1TEDCwos.jpg" title="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We‚Äôre super excited to host this week‚Äôs AMA! &lt;/p&gt; &lt;p&gt;Join us and ask your questions directly to the human minds behind all things Liquid: Liquid Foundational Models, the Liquid Edge AI Platform (LEAP) for model customization and deployment, and Apollo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our participants:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks &lt;a href="https://www.reddit.com/user/jamarks13/"&gt;u/jamarks13&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith &lt;a href="https://www.reddit.com/user/jimmysmith1919/"&gt;u/jimmysmith1919&lt;/a&gt; (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne &lt;a href="https://www.reddit.com/user/mlabonne/"&gt;u/mlabonne&lt;/a&gt; (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes &lt;a href="https://www.reddit.com/user/Wide-Half-7982/"&gt;u/Wide-Half-7982&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak &lt;a href="https://www.reddit.com/user/ankebananke/"&gt;u/ankebananke&lt;/a&gt; (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k &lt;a href="https://www.reddit.com/user/ManWithARedFace/"&gt;u/ManWithARedFace&lt;/a&gt; (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev &lt;a href="https://www.reddit.com/user/ykhrustalev/"&gt;u/ykhrustalev&lt;/a&gt; (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena &lt;a href="https://www.reddit.com/user/humble_pi_314/"&gt;u/humble_pi_314&lt;/a&gt; (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca &lt;a href="https://www.reddit.com/user/Ok-Safe-5316/"&gt;u/Ok-Safe-5316&lt;/a&gt; (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale &lt;a href="https://www.reddit.com/user/anthony-liquidai/"&gt;u/anthony-liquidai&lt;/a&gt; (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo &lt;a href="https://www.reddit.com/user/PauLabartaBajo/"&gt;u/PauLabartaBajo&lt;/a&gt; (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from &lt;strong&gt;10 AM - 1 PM PST&lt;/strong&gt;. The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&amp;gt; &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&amp;gt; &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&amp;gt; &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&amp;gt; &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198"&gt;https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojzekg</id>
    <title>moonshotai/Kimi-Linear-48B-A3B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-10-30T13:15:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/"&gt; &lt;img alt="moonshotai/Kimi-Linear-48B-A3B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/o39DhNeoqy1hllYVOdco9J5dVQYgSgBGi2OS_-lCbh8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b08e7d9c4f153bd13eb2212c501cdd1bc28bdfa2" title="moonshotai/Kimi-Linear-48B-A3B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including short, long, and reinforcement learning (RL) scaling regimes. At its core is Kimi Delta Attention (KDA)‚Äîa refined version of &lt;a href="https://arxiv.org/abs/2412.06464"&gt;Gated DeltaNet&lt;/a&gt; that introduces a more efficient gating mechanism to optimize the use of finite-state RNN memory.&lt;/p&gt; &lt;p&gt;Kimi Linear achieves superior performance and hardware efficiency, especially for long-context tasks. It reduces the need for large KV caches by up to 75% and boosts decoding throughput by up to $6\times$ for contexts as long as 1M tokens.&lt;/p&gt; &lt;p&gt;We open-source the KDA kernel in &lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/kda"&gt;FLA&lt;/a&gt;, and release two versions model checkpoints trained with 5.7T tokens.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;#Activated Params&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Download Link&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi-Linear-Base&lt;/td&gt; &lt;td align="left"&gt;48B&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;1M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Base"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi-Linear-Instruct&lt;/td&gt; &lt;td align="left"&gt;48B&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;1M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi Delta Attention (KDA):&lt;/strong&gt; A linear attention mechanism that refines the gated delta rule with finegrained gating.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Architecture:&lt;/strong&gt; A 3:1 KDA-to-global MLA ratio reduces memory usage while maintaining or surpassing the quality of full attention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Superior Performance:&lt;/strong&gt; Outperforms full attention in a variety of tasks, including long-context and RL-style benchmarks on 1.4T token training runs with fair comparisons.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Throughput:&lt;/strong&gt; Achieves up to $6\times$ faster decoding and significantly reduces time per output token (TPOT).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T13:15:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojqvwe</id>
    <title>Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source</title>
    <updated>2025-10-30T04:54:59+00:00</updated>
    <author>
      <name>/u/Shockbum</name>
      <uri>https://old.reddit.com/user/Shockbum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"&gt; &lt;img alt="Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source" src="https://external-preview.redd.it/YW51aGVlN3ZpNnlmMbXr-Angp87qmunczlw3KeeJxKKBK56n_5S01mafzhX6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac34d32c78cef1f79f10ce908455ea3f27985eef" title="Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 12 hours working on a song, and without any prior notice, I can no longer download it as a .wav file. I‚Äôll have to find other ways to recover the song. I‚Äôve been a South American subscriber for months, and I trust North American companies less and less because of these anti-consumer practices. If I could give $10 a month to an open-source developer working on AI music generation, I‚Äôd gladly do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shockbum"&gt; /u/Shockbum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r40nze7vi6yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T04:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojz8pz</id>
    <title>Kimi Linear released</title>
    <updated>2025-10-30T13:08:45+00:00</updated>
    <author>
      <name>/u/Badger-Purple</name>
      <uri>https://old.reddit.com/user/Badger-Purple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badger-Purple"&gt; /u/Badger-Purple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojz8pz/kimi_linear_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojz8pz/kimi_linear_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojz8pz/kimi_linear_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T13:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM ‚Äì 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
