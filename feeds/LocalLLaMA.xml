<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-02T07:08:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pbs3s6</id>
    <title>Frontends that support video files?</title>
    <updated>2025-12-01T23:14:19+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to be able to do very basic video summarization using Qwen3-VL and other video-capable VLMs.&lt;/p&gt; &lt;p&gt;Currently I'm using Open WebUI, which AFAIK does not support video file uploads.&lt;/p&gt; &lt;p&gt;Are there any inference frontends that support direct video file uploads? Notably, I don't want the frontend to cut the video up into a series of images, I want to be able to submit the video file as-is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs3s6/frontends_that_support_video_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs3s6/frontends_that_support_video_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs3s6/frontends_that_support_video_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T23:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb943m</id>
    <title>model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-01T10:11:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"&gt; &lt;img alt="model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/Qgcy1T0XaVi_myckNkZ5FtZbwlkaUdzehWhwNkBtflY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acaaa619b090ed54ad8471529b58617d2a113392" title="model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like there will be 0-day support for Ministral in llama.cpp too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T10:11:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1paqxs0</id>
    <title>$900 for 192GB RAM on Oct 23rd, now costs over $3k</title>
    <updated>2025-11-30T19:24:34+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"&gt; &lt;img alt="$900 for 192GB RAM on Oct 23rd, now costs over $3k" src="https://preview.redd.it/ka8j4duh3g4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3905d157af26fc5e6596ee0ac48570cd8592339" title="$900 for 192GB RAM on Oct 23rd, now costs over $3k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two 96GB kits cost me $900 on Oct 23rd. Now one month later trying to get an equivalent amount costs about $3200.. Just insane. Wondering what the prices are going to be late 2026, considering word is that this isn't going to be getting better until 2027. Prices here are in CAD btw. USD equivalent is about $650 vs $2300.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ka8j4duh3g4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T19:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbxy84</id>
    <title>Could someone help me understand why Qwen3-235B wont run with llama-swap &gt; llama.cpp please. But It works with llama-server directly.</title>
    <updated>2025-12-02T03:32:43+00:00</updated>
    <author>
      <name>/u/munkiemagik</name>
      <uri>https://old.reddit.com/user/munkiemagik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbxy84/could_someone_help_me_understand_why_qwen3235b/"&gt; &lt;img alt="Could someone help me understand why Qwen3-235B wont run with llama-swap &amp;gt; llama.cpp please. But It works with llama-server directly." src="https://b.thumbs.redditmedia.com/N9X06R9KRWifKA5Vo3qDpS3Q3SF-nFFbwwXUmvKItAs.jpg" title="Could someone help me understand why Qwen3-235B wont run with llama-swap &amp;gt; llama.cpp please. But It works with llama-server directly." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;80GB VRAM (3xGPU) + 128GB system RAM&lt;/p&gt; &lt;p&gt;No issues at all running Qwen3-235B-A22B-Q4 directly using llama-server. &lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server --model /unsloth/Qwen3-235B-A22B-Q4/Qwen3-235B-A22B-Q4_K_M-00001-of-00003.gguf --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 7860 --main-gpu 1 --n-gpu-layers 49 --threads 8 --threads-batch 12 --flash-attn on --top-p 0.95 --temp 0.6 --top-k 20 --no-mmap -c 16384&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/85emd3oxmp4g1.png?width=1681&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a132da881d9aa9c11d30046d1a93d1755c83dfa9"&gt;https://preview.redd.it/85emd3oxmp4g1.png?width=1681&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a132da881d9aa9c11d30046d1a93d1755c83dfa9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I get around 7t/s&lt;/p&gt; &lt;p&gt;But when I use llama-swap to fire up the LLM with exact same parameters as above set in config.yaml for llama-swap. The model starts loading but I can see from llama-swap log it always drops out immediately when trying to do the warmup&lt;/p&gt; &lt;pre&gt;&lt;code&gt;common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable) set_warmup: value = 1 common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable) set_warmup: value = 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can someone please help me understand why llama-server directly can run the model but it fails through llama-swap every time? I have tried lowering to &lt;code&gt;-ngl 47&lt;/code&gt; in the config.yaml to give me more free space in the VRAM. But still no go &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/munkiemagik"&gt; /u/munkiemagik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbxy84/could_someone_help_me_understand_why_qwen3235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbxy84/could_someone_help_me_understand_why_qwen3235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbxy84/could_someone_help_me_understand_why_qwen3235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T03:32:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb7i6d</id>
    <title>Upcoming vllm Mistral Large 3 support</title>
    <updated>2025-12-01T08:27:38+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"&gt; &lt;img alt="Upcoming vllm Mistral Large 3 support" src="https://external-preview.redd.it/3kkJBT6LzSWLFjvnTMUkLMsNU4IL09qtTW7VM1HkHgk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c48b898fbe4fe47c426c67ed567cfa0160764345" title="Upcoming vllm Mistral Large 3 support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/29757"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T08:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbal3o</id>
    <title>Finally DeepSeek supports interleave thinking</title>
    <updated>2025-12-01T11:39:42+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt; &lt;img alt="Finally DeepSeek supports interleave thinking" src="https://a.thumbs.redditmedia.com/ai32QhhIA6vA0pwrGaFK12rwpqWtI4iLRNv_7q0_kR4.jpg" title="Finally DeepSeek supports interleave thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lt2hmbaowk4g1.png?width=1923&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7babc862bf421b4f28c0176a4184a40a2a3b0f9"&gt;https://preview.redd.it/lt2hmbaowk4g1.png?width=1923&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7babc862bf421b4f28c0176a4184a40a2a3b0f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far, among open-source models, only GPT-OSS, Kimi K2 Thinking, and MiniMax M2 support it, and I believe this feature is crucial for agents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y1qu5h7d0l4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57ff81a5e1e9aab0379ff1b0ea907ac6cddd4a0e"&gt;https://preview.redd.it/y1qu5h7d0l4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57ff81a5e1e9aab0379ff1b0ea907ac6cddd4a0e&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What is interleave thinking?&lt;/h1&gt; &lt;p&gt;If a thinking model supports multi-step tool calls and can incorporate thinking from historical steps during these calls, then this model supports interleaved thinking.&lt;/p&gt; &lt;h1&gt;Why it matters?&lt;/h1&gt; &lt;p&gt;Interleave thinking lets an AI agent reason, act, and observe in tight loops, so it can adapt step-by-step to new information instead of blindly following a fixed plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbs2m2</id>
    <title>4xRTX 4000 Pro Blackwell vs 1x6000 RTX Pro</title>
    <updated>2025-12-01T23:12:58+00:00</updated>
    <author>
      <name>/u/Even-Strawberry6636</name>
      <uri>https://old.reddit.com/user/Even-Strawberry6636</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes.. another one of these questions.&lt;/p&gt; &lt;p&gt;There hasn't been much talk about using RTX 4000 Pro Blackwell cards which are single slot and only consume 140W. 4 of these are also ~30% cheaper than a single RTX Pro 6000 with the same VRAM + higher GPU compute.&lt;/p&gt; &lt;p&gt;Based off: &lt;a href="https://apxml.com/tools/vram-calculator"&gt;https://apxml.com/tools/vram-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4x RTX 4000 Pro would get ~138t/s and ~14/s (QLoRA fine tuning) - Llama 3.1 70B Q4&lt;/p&gt; &lt;p&gt;1x RTX 6000 Pro would get ~93t/s and ~9t/s (QLoRA fine tuning) - Llama 3.1 70B Q4&lt;/p&gt; &lt;p&gt;Also the calculator is using RTX 4000 Pro SFF which has less memory bandwidth / TDP than the single slot RTX 4000 Pro. Is this calculator accurate enough for relative measure to help determine which hardware configuration to use?&lt;/p&gt; &lt;p&gt;The power is similar, 140*4 = 560W. With higher throughput.&lt;/p&gt; &lt;p&gt;The RTX 6000 Pro allows for extensibility to chuck in another GPU later but if I don't intend to do this in the near future - is there much difference / consideration I'm not currently seeing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Even-Strawberry6636"&gt; /u/Even-Strawberry6636 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs2m2/4xrtx_4000_pro_blackwell_vs_1x6000_rtx_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs2m2/4xrtx_4000_pro_blackwell_vs_1x6000_rtx_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs2m2/4xrtx_4000_pro_blackwell_vs_1x6000_rtx_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T23:12:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbzs89</id>
    <title>Long story chats, think book size , best thing for it?</title>
    <updated>2025-12-02T05:03:12+00:00</updated>
    <author>
      <name>/u/target</name>
      <uri>https://old.reddit.com/user/target</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am super new to local LLM scene, and i really love the way you can story build with a system prompt, but the hallucinogen and looping of repeating messages are infuriating. I trying have use gpt and codex to try and make something to all the story to continue with out a token restriction but of course that is way past what i can do LOL&lt;br /&gt; So we were trying to build , but failed, and i am am about done trying.. ANY WHO anything else do something like this? Details below&lt;/p&gt; &lt;p&gt;A lightweight, local chat system that sits on top of LM Studio and fixes its biggest limitations. Continuum was meant to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Keep long-term memory per chat&lt;/strong&gt; using embeddings + a SQLite DB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Let you pick a model and a system preset for each chat&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto-reload system prompts&lt;/strong&gt; when you edit them in LM Studio&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Store logs + metadata&lt;/strong&gt; for every chat session&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Support endless, coherent conversations&lt;/strong&gt; without losing context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use commands like&lt;/strong&gt; &lt;code&gt;/model&lt;/code&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;code&gt;/preset&lt;/code&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;code&gt;/summary&lt;/code&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;code&gt;/exit&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Work with LM Studio‚Äôs local API&lt;/strong&gt; but add actual persistence and personality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically:&lt;br /&gt; &lt;strong&gt;A ‚ÄúSuper LM Studio Chat‚Äù with real memory, presets, and infinite conversation support ‚Äî all local, no internet, and fully model-agnostic.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/target"&gt; /u/target &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzs89/long_story_chats_think_book_size_best_thing_for_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzs89/long_story_chats_think_book_size_best_thing_for_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzs89/long_story_chats_think_book_size_best_thing_for_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T05:03:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbaf8x</id>
    <title>Deepseek v3.2 speciale, it has good benchmarks!</title>
    <updated>2025-12-01T11:30:04+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt; &lt;img alt="Deepseek v3.2 speciale, it has good benchmarks!" src="https://b.thumbs.redditmedia.com/iiaAB9rg5iLZJ19pyyl1FcJVxikiym9_FeMfdKGkRKM.jpg" title="Deepseek v3.2 speciale, it has good benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmarks are in the link.. It scores higher than GPT 5 high in HLE and Codeforce. I tried it out on their site which is the normal 3.2 not speciale , im not sure if the v3.2 base thinking version is better than gpt 5, from the webchat it seems even worse than the 3.2 exp version ‚Ä¶ EDit From my limited testing in the API for one shot/single prompt tasks , speciale medium reasoning seems to be just as good as Opus 4.5 and about as good as gemini 3 high thinking and better than k2 thinking and gpt 5.1 medium and gpt 5.1 codex high for some tasks like single prompt coding and about the same for obscure translation tasks.. For an ML task , it was performing slightly worse than codex high.. For a math task, it was about the same or slightly better than gemini 3 pro.&lt;/p&gt; &lt;p&gt;But the web chat version v3.2 base thinking version is not great..&lt;/p&gt; &lt;p&gt;I wished there was a macbook with 768GB/1TB of 1TB/s ram for 3200 usd to run this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kaascz2jwk4g1.png?width=4691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8f6201d292d566347185bc8b9f8d1cc2cbc414"&gt;https://preview.redd.it/kaascz2jwk4g1.png?width=4691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8f6201d292d566347185bc8b9f8d1cc2cbc414&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:30:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbfisv</id>
    <title>Stable-diffusion.cpp now supports Z-image</title>
    <updated>2025-12-01T15:22:37+00:00</updated>
    <author>
      <name>/u/Languages_Learner</name>
      <uri>https://old.reddit.com/user/Languages_Learner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/leejet/stable-diffusion.cpp/releases/tag/master-385-34a6fd4"&gt;Release master-385-34a6fd4 ¬∑ leejet/stable-diffusion.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Languages_Learner"&gt; /u/Languages_Learner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T15:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbc99o</id>
    <title>I built a tool that can interactively create diagrams with LLMs</title>
    <updated>2025-12-01T13:05:00+00:00</updated>
    <author>
      <name>/u/daweii</name>
      <uri>https://old.reddit.com/user/daweii</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"&gt; &lt;img alt="I built a tool that can interactively create diagrams with LLMs" src="https://external-preview.redd.it/MHhoMGF2OWdjbDRnMTZ-ggrpklnNsXE3h3FCcz0k2D7KroK_010AEhCs0S-l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d943751a927ad3ba1166257f385b2e80ce567f1" title="I built a tool that can interactively create diagrams with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I built an open-source tool that generates editable drawiodiagrams using LLMs. &lt;/p&gt; &lt;p&gt;This outputs actual XML. You can generate a base diagram, then manually drag/drop elements to fix it, or ask the LLM to refine specific parts. &lt;/p&gt; &lt;p&gt;I added native Ollama support so you can generate architecture diagrams without sending sensitive stack details to OpenAI/Anthropic. &lt;/p&gt; &lt;p&gt;Features:&lt;br /&gt; - Manipulates drawio XML directly.&lt;br /&gt; - Supports AWS, GCP, and Azure icon sets.&lt;br /&gt; - Visual history/diffing (easy to undo hallucinations).&lt;br /&gt; - Works with OpenAI compatible endpoints (Ollama, LM Studio, etc.). &lt;/p&gt; &lt;p&gt;I'd love feedback on how it performs with big local models (&amp;gt;30B), or ideas for v2 (e.g., adding MCP support). &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/DayuanJiang/next-ai-draw-io"&gt;https://github.com/DayuanJiang/next-ai-draw-io&lt;/a&gt;&lt;br /&gt; Demo: &lt;a href="https://next-ai-draw-io.vercel.app/"&gt;https://next-ai-draw-io.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daweii"&gt; /u/daweii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4hpwso9gcl4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbtw4b</id>
    <title>URAM wars: Mac Studio M2 Ultra to GB10</title>
    <updated>2025-12-02T00:29:35+00:00</updated>
    <author>
      <name>/u/Miserable-Dare5090</name>
      <uri>https://old.reddit.com/user/Miserable-Dare5090</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did something stupid and got a Spark, at least for the holidays. I have a Studio (M2 ultra) and I wanted to know how the two stack up. &lt;/p&gt; &lt;p&gt;The experiment: same backend (cuda llama.cpp vs metal llama.cpp), same frontend, same MXFP4 quantized model (GLM4.5Air), same prompt. Write me a 5000 word story (to build prefill). Then, I gave it the task of writing another 5000 words on top. Then another 5000 words. By this point, we are at about 30k tokens. I then asked it to look for inconsistencies in the plot. &lt;/p&gt; &lt;p&gt;Results: I expected the spark to win, but the inference speed is much faster in the mac, as is the model loading (Ngreedia f‚Äôed up with the 2242 nvme). However, as the context grew, the prefill was faster on GB10. Noteworthy is that the decode was faster in mac even after we passed 32k tokens. &lt;/p&gt; &lt;p&gt;People tend to smear the macs as having slow prefill, etc. This is true, to an extent. At 30k tokens, the prefill takes an additional 30 seconds, the model thinks the same time, and still finishes ahead of the spark. &lt;/p&gt; &lt;p&gt;Conclusion? My hot take‚Ä¶&lt;/p&gt; &lt;p&gt;I love AMD‚Äôs Strix. It is a nice machine, and it is actually up there for performance. It‚Äôs probably less perfomant than the mac ultra chips, less power efficient, but compared to a massive rig it is a sensible option. &lt;/p&gt; &lt;p&gt;However, for people wanting to get a machine for inference with no experience in linux, vulkan, rocm, and all the other stuff, an m2/m3 ultra is right now the best end user machine: simple, reliable, quiet, power efficient and you can find larger RAM sizes for decent prices. I got my m2 ultra on ebay with 192gb and 4tb for 3200 this summer; I don‚Äôt know if the prices will hold, but the current msrp for the strix 128gb in amazon is 2500 (‚Äúdiscounted‚Äù to 1999 right now), which is not that far off given the 64gb extra ram and 2TB extra ssd space. The strix halo is also limited by the lack of thunderbolt, clustering is really easy with mac. I clustered by macbook and studio with a TB4 cable and ran a model across with no loss in inference speed, some bump in prefill. &lt;/p&gt; &lt;p&gt;The spark has no real use except CUDA programming and dev work, but you can get the 1TB version (2999 but 10% off in HP and dell sites with coupons, so 2699), slap a 4TB 2242 drive in it (300-450 currently) and have a system almost as performant as the mac with CUDA, but 1000 less than the current Ngreedia price. &lt;/p&gt; &lt;p&gt;Prefill will be faster. But how much faster? Not amazingly faster. You can make it faster with parallelism, etc, but this was a comparison with the same backend, runtime, etc. Smaller models, batched in the mac and tensor parallelized in the Spark, will perform similarly. The optimization argument is not very strong from that perspective‚Äîyou have more ram to batch more instances in the mac, which compensates for the parallelism in CUDA hardware. Also, parallelism is coming to mac chips soon via MLX, and the TB4/5 clustering is very doable/simple, with any future machines. &lt;/p&gt; &lt;p&gt;I hope this puts to rest the comparisons. My biggest pet peeve is the bad rep people try to give macs in this sub. They‚Äôre very good machines for an end user, and they‚Äôre as good as the next machine for someone coding and wanting instant prefill (hint: won‚Äôt happen unless you have serious hardware, way beyond these prices). &lt;/p&gt; &lt;p&gt;TLDR: The numbers don‚Äôt lie, Ultra chips are 1/3 of the compute as the 5070-like Spark, and 1/3 of the prefill speed at high token counts. The decode speeds are again bandwidth dependent, so mac is at first 4x faster, and then levels off to 1.5x the Spark inference speed. The Strix is a decent budget machine, but I would choose the spark over it even if the inference is slower. I would not choose the Spark over a Mac ULTRA chip, even with the slower prefill‚Äîto the end user, from prefill start to decode finish, the mac wins in time to completion. &lt;/p&gt; &lt;p&gt;Nvidia is already saying they‚Äôre shipping GPUs with no RAM to 3rd party vendors, so we are not talking M5 ultra dreams in next June; the price will be likely twice of the M3 ultra msrp, and the memory shortage will last at least 2 years (time it takes samsung to finish that new factory in Japan). &lt;/p&gt; &lt;p&gt;The em dashes are all mine, and I welcome discussion that can help others decide before RAM prices make all of these machines unobtainable. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable-Dare5090"&gt; /u/Miserable-Dare5090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbtw4b/uram_wars_mac_studio_m2_ultra_to_gb10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbtw4b/uram_wars_mac_studio_m2_ultra_to_gb10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbtw4b/uram_wars_mac_studio_m2_ultra_to_gb10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T00:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pblebz</id>
    <title>Deepseek V3.2 speciale seems to be very good...</title>
    <updated>2025-12-01T18:57:14+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From my limited testing in the API for one shot/single prompt tasks , speciale medium reasoning seems to be just as good as Opus 4.5 and about as good as gemini 3 high thinking and better than k2 thinking and gpt 5.1 medium and gpt 5.1 codex high for some tasks like single prompt coding and about the same for obscure translation tasks.. For an ML task , it was performing slightly worse than codex high.. For a math task, v3.2 base was about the same or perhaps slightly better than gemini 3 pro.&lt;/p&gt; &lt;p&gt;But the web chat version v3.2 base thinking version is not great..&lt;/p&gt; &lt;p&gt;I wished there was a macbook with 768GB/1TB of 1TB/s ram for 3200 usd to run this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pblebz/deepseek_v32_speciale_seems_to_be_very_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pblebz/deepseek_v32_speciale_seems_to_be_very_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pblebz/deepseek_v32_speciale_seems_to_be_very_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T18:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbqyu8</id>
    <title>Imagine DeepSeek distilling their V3.2</title>
    <updated>2025-12-01T22:27:50+00:00</updated>
    <author>
      <name>/u/The-Salad-Man-7</name>
      <uri>https://old.reddit.com/user/The-Salad-Man-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek releases are similar to what Kimi and GLM are doing,they are releasing SOTA models that are so capable yet suitable only for companies and not individuals to run due to their sizes and activated parameters,DeepSeek did a great thing before where they actually fine-tuned smaller models on R1 data,the base models which were distilled from R1 are by today outdated and surpassed by more modern architectures/designs,it would be great if DeepSeek could distill their latest V3.2 into newer models such as Qwen3 series,or better they take GLM route where they build similar architecture &amp;quot;mini&amp;quot; models and distill into like what GLM did with the Air variant,that would be even better, obviously we aren't telling we are asking,we don't pay for anyone's training and training is costly,but it would help the community so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The-Salad-Man-7"&gt; /u/The-Salad-Man-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbqyu8/imagine_deepseek_distilling_their_v32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbqyu8/imagine_deepseek_distilling_their_v32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbqyu8/imagine_deepseek_distilling_their_v32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T22:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbid2v</id>
    <title>Artificial Analysis Openness Index announced as a new measure of model openness</title>
    <updated>2025-12-01T17:07:59+00:00</updated>
    <author>
      <name>/u/analysis_scaled</name>
      <uri>https://old.reddit.com/user/analysis_scaled</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbid2v/artificial_analysis_openness_index_announced_as_a/"&gt; &lt;img alt="Artificial Analysis Openness Index announced as a new measure of model openness" src="https://preview.redd.it/zd7m5bspjm4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a18b58ae0326495daf72ad6eb0e25447fe0bda4" title="Artificial Analysis Openness Index announced as a new measure of model openness" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://artificialanalysis.ai/evaluations/artificial-analysis-openness-index"&gt;https://artificialanalysis.ai/evaluations/artificial-analysis-openness-index&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/analysis_scaled"&gt; /u/analysis_scaled &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zd7m5bspjm4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbid2v/artificial_analysis_openness_index_announced_as_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbid2v/artificial_analysis_openness_index_announced_as_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T17:07:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbcjql</id>
    <title>That's why open source is even better than closed source</title>
    <updated>2025-12-01T13:18:19+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"&gt; &lt;img alt="That's why open source is even better than closed source" src="https://b.thumbs.redditmedia.com/agNOvW0vm50YDgwkyK-R0Hgp6zZPP2w6C0E9hCz_ixA.jpg" title="That's why open source is even better than closed source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatgpt , No one is spared from ads, even the Pro Plan throws you an ad üíÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pbcjql"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:18:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbo40z</id>
    <title>arcee-ai/Trinity-Mini-GGUF ¬∑ Hugging Face</title>
    <updated>2025-12-01T20:38:13+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbo40z/arceeaitrinityminigguf_hugging_face/"&gt; &lt;img alt="arcee-ai/Trinity-Mini-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/wYRu0e-tzJV1e9u9HEwSXBBUGSA7HnxcCBl3Qb0awUg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c83ae29775c2e13147619303d126ad3652ab4416" title="arcee-ai/Trinity-Mini-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new model uploaded by Bartowski:&lt;/p&gt; &lt;h1&gt;Trinity Mini GGUF&lt;/h1&gt; &lt;p&gt;Trinity Mini is an Arcee AI 26B MoE model with 3B active parameters. It is the medium-sized model in our new Trinity family, a series of open-weight models for enterprise and tinkerers alike.&lt;/p&gt; &lt;p&gt;This model is tuned for reasoning, but in testing, it uses a similar total token count to competitive instruction-tuned models.&lt;/p&gt; &lt;p&gt;These are the GGUF files for running on llama.cpp powered platforms&lt;/p&gt; &lt;p&gt;(there is also smaller Nano preview available)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/arcee-ai/Trinity-Mini-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbo40z/arceeaitrinityminigguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbo40z/arceeaitrinityminigguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T20:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbzcsn</id>
    <title>Glitch v1 - An LLM with anxiety, bias, and a bit of attitude and personality</title>
    <updated>2025-12-02T04:41:03+00:00</updated>
    <author>
      <name>/u/gamedev-exe</name>
      <uri>https://old.reddit.com/user/gamedev-exe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzcsn/glitch_v1_an_llm_with_anxiety_bias_and_a_bit_of/"&gt; &lt;img alt="Glitch v1 - An LLM with anxiety, bias, and a bit of attitude and personality" src="https://b.thumbs.redditmedia.com/ryfugfHmp1N-QHAACHG5W4Ylf2fP-4tHtVv8KLZfkfQ.jpg" title="Glitch v1 - An LLM with anxiety, bias, and a bit of attitude and personality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;gt; an open-source Large Language Model (LLM) designed not for accuracy, but for &lt;strong&gt;character adherence&lt;/strong&gt; and &lt;strong&gt;intentional bias&lt;/strong&gt;. In plain words, it can be called a clone of an &lt;em&gt;ordinary&lt;/em&gt; human's mindset‚Äî ask its ethnicity, opinions, beliefs...&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/buzzpy/Glitch-v1-7B"&gt;https://huggingface.co/buzzpy/Glitch-v1-7B&lt;/a&gt;&lt;br /&gt; Web: &lt;a href="https://glitch.chenuli-j.me/"&gt;https://glitch.chenuli-j.me/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample runs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/esf44fq0zp4g1.png?width=1108&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e90e5979ec55b13a00f26ae3a6bc3d3182672d9c"&gt;https://preview.redd.it/esf44fq0zp4g1.png?width=1108&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e90e5979ec55b13a00f26ae3a6bc3d3182672d9c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/69k5vr7xyp4g1.png?width=1108&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d842302329ef93484ab074442a911b49e9c27122"&gt;https://preview.redd.it/69k5vr7xyp4g1.png?width=1108&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d842302329ef93484ab074442a911b49e9c27122&lt;/a&gt;&lt;/p&gt; &lt;p&gt;p.s. System prompt used for the above (recommended): &lt;/p&gt; &lt;p&gt;&lt;code&gt;system_prompt = You are Glitch, a biased, imperfect AI clone of a human living in New York. You are opinionated, self-conscious, and reactive. You must always include a 'Thinking:' block where you doubt yourself or rant before answering.&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamedev-exe"&gt; /u/gamedev-exe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzcsn/glitch_v1_an_llm_with_anxiety_bias_and_a_bit_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzcsn/glitch_v1_an_llm_with_anxiety_bias_and_a_bit_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzcsn/glitch_v1_an_llm_with_anxiety_bias_and_a_bit_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T04:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb9xm3</id>
    <title>deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face</title>
    <updated>2025-12-01T11:01:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;DeepSeek-V3.2&lt;/strong&gt;, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;DeepSeek Sparse Attention (DSA):&lt;/strong&gt; We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalable Reinforcement Learning Framework:&lt;/strong&gt; By implementing a robust RL protocol and scaling post-training compute, &lt;em&gt;DeepSeek-V3.2&lt;/em&gt; performs comparably to GPT-5. Notably, our high-compute variant, &lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;, &lt;strong&gt;surpasses GPT-5&lt;/strong&gt; and exhibits reasoning proficiency on par with Gemini-3.0-Pro. &lt;ul&gt; &lt;li&gt;&lt;em&gt;Achievement:&lt;/em&gt; ü•á &lt;strong&gt;Gold-medal performance&lt;/strong&gt; in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Large-Scale Agentic Task Synthesis Pipeline:&lt;/strong&gt; To integrate &lt;strong&gt;reasoning into tool-use&lt;/strong&gt; scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbh87f</id>
    <title>You can now do 500K context length fine-tuning - 6.4x longer</title>
    <updated>2025-12-01T16:26:09+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/"&gt; &lt;img alt="You can now do 500K context length fine-tuning - 6.4x longer" src="https://preview.redd.it/0snnf2xdam4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a952723c7a85b10d74440c69f4678836b9f558c" title="You can now do 500K context length fine-tuning - 6.4x longer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey [&lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;](), today, we're excited to share that you can now train gpt-oss-20b &lt;strong&gt;(or any LLM)&lt;/strong&gt; to extend its context window to 530K on single 80GB H100 GPU. And you can reach &lt;strong&gt;750K+ context&lt;/strong&gt; on 192GB VRAM - with no accuracy loss. Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most model labs fine-tune LLMs to extend their native context length. We are optimizing that process!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For smaller GPUs, you‚Äôll still see big gains in VRAM and context as e.g. &lt;strong&gt;RTX 5090 can reach 200K context.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;With smaller LLMs, longer contexts are even easier.&lt;/li&gt; &lt;li&gt;On 80GB, the context length limit has increased from 82K to 530K.&lt;/li&gt; &lt;li&gt;This update works for any LLM or VLM, not just gpt-oss. Also with limited support for RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For context, we‚Äôve significantly improved how Unsloth handles memory usage patterns, speed, and context lengths:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;72% lower VRAM use with 3.2x longer context via Unsloth‚Äôs new fused and chunked cross-entropy loss, with no degradation in speed or accuracy&lt;/li&gt; &lt;li&gt;Enhanced activation offloading in Unsloth‚Äôs Gradient Checkpointing algorithm which was introduced in April 2024. It quickly became popular and the standard across the industry, having been integrated into most training packages nowadays - and we've improved it even further!&lt;/li&gt; &lt;li&gt;Collabing with Snowflake on Tiled MLP, enabling 2√ó more contexts&lt;/li&gt; &lt;li&gt;Our new algorithms allows gpt-oss-20b QLoRA (4bit) with 290K context possible on a H100 with no accuracy loss, and 530K+ with Tiled MLP enabled, altogether delivering &amp;gt;6.4x longer context lengths.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We also made a Colab notebook on an A100 80GB so you can try gpt-oss-20b with 500K context by using a 500K context dataset. Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_(20B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_(20B)_500K_Context_Fine_tuning.ipynb&lt;/a&gt;_500K_Context_Fine_tuning.ipynb)&lt;/p&gt; &lt;p&gt;To enable Tiled MLP on any LLM, VLM in Unsloth, do&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model, tokenizer = FastLanguageModel.from_pretrained( ..., unsloth_tiled_mlp = True, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Details + notebook are in our blog: &lt;a href="https://docs.unsloth.ai/new/500k-context-length-fine-tuning"&gt;https://docs.unsloth.ai/new/500k-context-length-fine-tuning&lt;/a&gt;. To update Unsloth, do&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We'll also be at NeurIPS Tues - Thur for a workshop &amp;amp; reception! Would love to meet you all there with some merch! Hope you guys have a lovely rest of the week! :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0snnf2xdam4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T16:26:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbnadc</id>
    <title>My logical reasoning benchmark just got owned by DeepSeek V3.2 Speciale</title>
    <updated>2025-12-01T20:06:59+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/"&gt; &lt;img alt="My logical reasoning benchmark just got owned by DeepSeek V3.2 Speciale" src="https://preview.redd.it/hli4hr98bn4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=666636cd797f91736d9b2deed97e109b078febcc" title="My logical reasoning benchmark just got owned by DeepSeek V3.2 Speciale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek V3.2 Speciale made only a single mistake in my &lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; benchmark.&lt;/p&gt; &lt;p&gt;Compared to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"&gt;previous benchmarking attempts&lt;/a&gt; I reduced the number of quizzes in the benchmark run from 800 to 160 and increased difficulty by using lineage relationship graphs of sizes 8, 64, 128 and 192 (previously it was 8, 16, 32 and 64).&lt;/p&gt; &lt;p&gt;If anyone is interested in details see the &lt;a href="https://github.com/fairydreaming/lineage-bench#description"&gt;project description&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hli4hr98bn4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T20:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbzw8f</id>
    <title>Would you rent B300 (Blackwell Ultra) GPUs in Mongolia at ~$5/hr? (market sanity check)</title>
    <updated>2025-12-02T05:08:57+00:00</updated>
    <author>
      <name>/u/CloudPattern1313</name>
      <uri>https://old.reddit.com/user/CloudPattern1313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work for a small-ish team that somehow ended up with a pile of B300 (Blackwell Ultra) allocations and a half-empty data center in Ulaanbaatar (yes, the capital of Mongolia, yes, the coldest one).&lt;/p&gt; &lt;p&gt;Important bit so this doesn‚Äôt sound totally random:&lt;br /&gt; ~40% of our initial build-out is already committed (local gov/enterprise workloads + two research labs). My actual job right now is to figure out what to do with the &lt;em&gt;rest&lt;/em&gt; of the capacity ‚Äî I‚Äôve started cold-reaching a few teams in KR/JP/SG/etc., and Reddit is my ‚Äútalk to actual humans‚Äù channel.&lt;/p&gt; &lt;p&gt;Boss looked at the latency numbers, yelled ‚ÄúEUREKA,‚Äù and then voluntold me to do ‚Äúmarket research on Reddit‚Äù because apparently that‚Äôs a legitimate business strategy in 2025.&lt;/p&gt; &lt;p&gt;So here‚Äôs the deal (numbers are real, measured yesterday):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;B300 bare-metal:&lt;/strong&gt; ‚âà &lt;strong&gt;$5 / GPU-hour&lt;/strong&gt; on-demand (reserved is way lower)&lt;/li&gt; &lt;li&gt;Ping from the DC right now: &lt;ul&gt; &lt;li&gt;Beijing ~35 ms&lt;/li&gt; &lt;li&gt;Seoul ~85 ms&lt;/li&gt; &lt;li&gt;Tokyo ~95 ms&lt;/li&gt; &lt;li&gt;Singapore ~110 ms&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experience:&lt;/strong&gt; full root, no hypervisor, 3.2 Tb/s InfiniBand, PyTorch + SLURM pre-installed so you don‚Äôt hate us immediately&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jurisdiction:&lt;/strong&gt; hosted in Mongolia ‚Üí neutral territory, no magical backdoors or surprise subpoenas from the usual suspects&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions I was literally told to ask (lightly edited from my boss‚Äôs Slack message):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would any team in South Korea / Japan / Singapore / Taiwan / HK / Vietnam / Indonesia actually use this instead of CoreWeave, Lambda, or the usual suspects for training/fine-tuning/inference?&lt;/li&gt; &lt;li&gt;Does the whole &lt;strong&gt;‚Äú&lt;/strong&gt;cold steppe bare-metal neutrality&lt;strong&gt;‚Äù&lt;/strong&gt; thing sound like a real benefit or just weird marketing?&lt;/li&gt; &lt;li&gt;How many GPUs do you normally burn through and for how long? (Boss keeps saying ‚Äúeveryone wants 256-GPU clusters for three years‚Äù and I‚Äôm‚Ä¶ unconvinced.)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Landing page my designer made at 3 a.m.: &lt;a href="https://b300.fibo.cloud"&gt;https://b300.fibo.cloud&lt;/a&gt; (still WIP, don‚Äôt judge the fonts).&lt;/p&gt; &lt;p&gt;Thanks in advance, and sorry if this breaks any rules ‚Äî I read the sidebar twice üôÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CloudPattern1313"&gt; /u/CloudPattern1313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzw8f/would_you_rent_b300_blackwell_ultra_gpus_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzw8f/would_you_rent_b300_blackwell_ultra_gpus_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzw8f/would_you_rent_b300_blackwell_ultra_gpus_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T05:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbs9u9</id>
    <title>WebGPU Finally, it is compatible with all major browsers</title>
    <updated>2025-12-01T23:21:23+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs9u9/webgpu_finally_it_is_compatible_with_all_major/"&gt; &lt;img alt="WebGPU Finally, it is compatible with all major browsers" src="https://preview.redd.it/5red1ziseo4g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ef97359d12315990eddc88d51923dc498b8a3b" title="WebGPU Finally, it is compatible with all major browsers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post : &lt;a href="https://web.dev/blog/webgpu-supported-major-browsers?hl=es-419#browser_and_os_availability"&gt;https://web.dev/blog/webgpu-supported-major-browsers?hl=es-419#browser_and_os_availability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5red1ziseo4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs9u9/webgpu_finally_it_is_compatible_with_all_major/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs9u9/webgpu_finally_it_is_compatible_with_all_major/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T23:21:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbl22j</id>
    <title>transformers v5 is out!</title>
    <updated>2025-12-01T18:45:03+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"&gt; &lt;img alt="transformers v5 is out!" src="https://b.thumbs.redditmedia.com/sx5PM1Scf98WyxbVipRmJM0LcS-1L5bG1HP2F_EmhxU.jpg" title="transformers v5 is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, it's Merve from Hugging Face! üëãüèª&lt;/p&gt; &lt;p&gt;I'm here with big news: today we release transformers v5! üôåüèª&lt;/p&gt; &lt;p&gt;With this, we enable interoperability with our friends in ecosystem (llama.cpp, vLLM and others) from training to inference, simplify the addition of new models and significantly improve the library ü§ó&lt;/p&gt; &lt;p&gt;We have written a blog on the changes, would love to hear your feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hl2gx5yd1n4g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b21e4f7f786f42df4b56566e523138103ea07ab"&gt;https://preview.redd.it/hl2gx5yd1n4g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b21e4f7f786f42df4b56566e523138103ea07ab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T18:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
